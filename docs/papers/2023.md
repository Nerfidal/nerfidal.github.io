---
layout: default
title: 2023
parent: Papers
nav_order: 2
---
<!---metadata--->

## Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with  Generative Diffusion Models

nerf{: .label .label-blue }

2023-12-30 | Han Jiang, Haosen Sun, Ruoxuan Li, Chi-Keung Tang, Yu-Wing Tai | cs.CV | [PDF](http://arxiv.org/pdf/2401.00208v1){: .btn .btn-green }

**Abstract**: Current Neural Radiance Fields (NeRF) can generate photorealistic novel
views. For editing 3D scenes represented by NeRF, with the advent of generative
models, this paper proposes Inpaint4DNeRF to capitalize on state-of-the-art
stable diffusion models (e.g., ControlNet) for direct generation of the
underlying completed background content, regardless of static or dynamic. The
key advantages of this generative approach for NeRF inpainting are twofold.
First, after rough mask propagation, to complete or fill in previously occluded
content, we can individually generate a small subset of completed images with
plausible content, called seed images, from which simple 3D geometry proxies
can be derived. Second and the remaining problem is thus 3D multiview
consistency among all completed images, now guided by the seed images and their
3D proxies. Without other bells and whistles, our generative Inpaint4DNeRF
baseline framework is general which can be readily extended to 4D dynamic
NeRFs, where temporal consistency can be naturally handled in a similar way as
our multiview consistency.

---

## PlanarNeRF: Online Learning of Planar Primitives with Neural Radiance  Fields

nerf{: .label .label-blue }

2023-12-30 | Zheng Chen, Qingan Yan, Huangying Zhan, Changjiang Cai, Xiangyu Xu, Yuzhong Huang, Weihan Wang, Ziyue Feng, Lantao Liu, Yi Xu | cs.CV | [PDF](http://arxiv.org/pdf/2401.00871v1){: .btn .btn-green }

**Abstract**: Identifying spatially complete planar primitives from visual data is a
crucial task in computer vision. Prior methods are largely restricted to either
2D segment recovery or simplifying 3D structures, even with extensive plane
annotations. We present PlanarNeRF, a novel framework capable of detecting
dense 3D planes through online learning. Drawing upon the neural field
representation, PlanarNeRF brings three major contributions. First, it enhances
3D plane detection with concurrent appearance and geometry knowledge. Second, a
lightweight plane fitting module is proposed to estimate plane parameters.
Third, a novel global memory bank structure with an update mechanism is
introduced, ensuring consistent cross-frame correspondence. The flexible
architecture of PlanarNeRF allows it to function in both 2D-supervised and
self-supervised solutions, in each of which it can effectively learn from
sparse training signals, significantly improving training efficiency. Through
extensive experiments, we demonstrate the effectiveness of PlanarNeRF in
various scenarios and remarkable improvement over existing works.

---

## Informative Rays Selection for Few-Shot Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-29 | Marco Orsingher, Anthony Dell'Eva, Paolo Zani, Paolo Medici, Massimo Bertozzi | cs.CV | [PDF](http://arxiv.org/pdf/2312.17561v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have recently emerged as a powerful method for
image-based 3D reconstruction, but the lengthy per-scene optimization limits
their practical usage, especially in resource-constrained settings. Existing
approaches solve this issue by reducing the number of input views and
regularizing the learned volumetric representation with either complex losses
or additional inputs from other modalities. In this paper, we present KeyNeRF,
a simple yet effective method for training NeRF in few-shot scenarios by
focusing on key informative rays. Such rays are first selected at camera level
by a view selection algorithm that promotes baseline diversity while
guaranteeing scene coverage, then at pixel level by sampling from a probability
distribution based on local image entropy. Our approach performs favorably
against state-of-the-art methods, while requiring minimal changes to existing
NeRF codebases.

Comments:
- To appear at VISAPP 2024

---

## DreamGaussian4D: Generative 4D Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-12-28 | Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.17142v2){: .btn .btn-green }

**Abstract**: Remarkable progress has been made in 4D content generation recently. However,
existing methods suffer from long optimization time, lack of motion
controllability, and a low level of detail. In this paper, we introduce
DreamGaussian4D, an efficient 4D generation framework that builds on 4D
Gaussian Splatting representation. Our key insight is that the explicit
modeling of spatial transformations in Gaussian Splatting makes it more
suitable for the 4D generation setting compared with implicit representations.
DreamGaussian4D reduces the optimization time from several hours to just a few
minutes, allows flexible control of the generated 3D motion, and produces
animated meshes that can be efficiently rendered in 3D engines.

Comments:
- Technical report. Project page is at
  https://jiawei-ren.github.io/projects/dreamgaussian4d Code is at
  https://github.com/jiawei-ren/dreamgaussian4d

---

## City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web

nerf{: .label .label-blue }

2023-12-27 | Kaiwen Song, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2312.16457v1){: .btn .btn-green }

**Abstract**: NeRF has significantly advanced 3D scene reconstruction, capturing intricate
details across various environments. Existing methods have successfully
leveraged radiance field baking to facilitate real-time rendering of small
scenes. However, when applied to large-scale scenes, these techniques encounter
significant challenges, struggling to provide a seamless real-time experience
due to limited resources in computation, memory, and bandwidth. In this paper,
we propose City-on-Web, which represents the whole scene by partitioning it
into manageable blocks, each with its own Level-of-Detail, ensuring high
fidelity, efficient memory management and fast rendering. Meanwhile, we
carefully design the training and inference process such that the final
rendering result on web is consistent with training. Thanks to our novel
representation and carefully designed training/inference process, we are the
first to achieve real-time rendering of large-scale scenes in
resource-constrained environments. Extensive experimental results demonstrate
that our method facilitates real-time rendering of large-scale scenes on a web
platform, achieving 32FPS at 1080P resolution with an RTX 3060 GPU, while
simultaneously achieving a quality that closely rivals that of state-of-the-art
methods. Project page: https://ustc3dv.github.io/City-on-Web/

Comments:
- Project page: https://ustc3dv.github.io/City-on-Web/

---

## DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision

nerf{: .label .label-blue }

2023-12-26 | Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong, Gang Hua, Tianyi Zhang, Bedrich Benes, Aniket Bera | cs.CV | [PDF](http://arxiv.org/pdf/2312.16256v2){: .btn .btn-green }

**Abstract**: We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.

---

## Pano-NeRF: Synthesizing High Dynamic Range Novel Views with Geometry  from Sparse Low Dynamic Range Panoramic Images

nerf{: .label .label-blue }

2023-12-26 | Zhan Lu, Qian Zheng, Boxin Shi, Xudong Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2312.15942v1){: .btn .btn-green }

**Abstract**: Panoramic imaging research on geometry recovery and High Dynamic Range (HDR)
reconstruction becomes a trend with the development of Extended Reality (XR).
Neural Radiance Fields (NeRF) provide a promising scene representation for both
tasks without requiring extensive prior data. However, in the case of inputting
sparse Low Dynamic Range (LDR) panoramic images, NeRF often degrades with
under-constrained geometry and is unable to reconstruct HDR radiance from LDR
inputs. We observe that the radiance from each pixel in panoramic images can be
modeled as both a signal to convey scene lighting information and a light
source to illuminate other pixels. Hence, we propose the irradiance fields from
sparse LDR panoramic images, which increases the observation counts for
faithful geometry recovery and leverages the irradiance-radiance attenuation
for HDR reconstruction. Extensive experiments demonstrate that the irradiance
fields outperform state-of-the-art methods on both geometry recovery and HDR
reconstruction and validate their effectiveness. Furthermore, we show a
promising byproduct of spatially-varying lighting estimation. The code is
available at https://github.com/Lu-Zhan/Pano-NeRF.

---

## LangSplat: 3D Language Gaussian Splatting

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-26 | Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister | cs.CV | [PDF](http://arxiv.org/pdf/2312.16084v1){: .btn .btn-green }

**Abstract**: Human lives in a 3D world and commonly uses natural language to interact with
a 3D scene. Modeling a 3D language field to support open-ended language queries
in 3D has gained increasing attention recently. This paper introduces
LangSplat, which constructs a 3D language field that enables precise and
efficient open-vocabulary querying within 3D spaces. Unlike existing methods
that ground CLIP language embeddings in a NeRF model, LangSplat advances the
field by utilizing a collection of 3D Gaussians, each encoding language
features distilled from CLIP, to represent the language field. By employing a
tile-based splatting technique for rendering language features, we circumvent
the costly rendering process inherent in NeRF. Instead of directly learning
CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and
then learns language features on the scene-specific latent space, thereby
alleviating substantial memory demands imposed by explicit modeling. Existing
methods struggle with imprecise and vague 3D language fields, which fail to
discern clear boundaries between objects. We delve into this issue and propose
to learn hierarchical semantics using SAM, thereby eliminating the need for
extensively querying the language field across various scales and the
regularization of DINO features. Extensive experiments on open-vocabulary 3D
object localization and semantic segmentation demonstrate that LangSplat
significantly outperforms the previous state-of-the-art method LERF by a large
margin. Notably, LangSplat is extremely efficient, achieving a {\speed}
$\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We
strongly recommend readers to check out our video results at
https://langsplat.github.io

Comments:
- Project Page: https://langsplat.github.io

---

## 2D-Guided 3D Gaussian Segmentation

nerf{: .label .label-blue }

2023-12-26 | Kun Lan, Haoran Li, Haolin Shi, Wenjun Wu, Yong Liao, Lin Wang, Pengyuan Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2312.16047v1){: .btn .btn-green }

**Abstract**: Recently, 3D Gaussian, as an explicit 3D representation method, has
demonstrated strong competitiveness over NeRF (Neural Radiance Fields) in terms
of expressing complex scenes and training duration. These advantages signal a
wide range of applications for 3D Gaussians in 3D understanding and editing.
Meanwhile, the segmentation of 3D Gaussians is still in its infancy. The
existing segmentation methods are not only cumbersome but also incapable of
segmenting multiple objects simultaneously in a short amount of time. In
response, this paper introduces a 3D Gaussian segmentation method implemented
with 2D segmentation as supervision. This approach uses input 2D segmentation
maps to guide the learning of the added 3D Gaussian semantic information, while
nearest neighbor clustering and statistical filtering refine the segmentation
results. Experiments show that our concise method can achieve comparable
performances on mIOU and mAcc for multi-object segmentation as previous
single-object segmentation methods.

---

## Neural BSSRDF: Object Appearance Representation Including Heterogeneous  Subsurface Scattering



2023-12-25 | Thomson TG, Jeppe Revall Frisvad, Ravi Ramamoorthi, Henrik Wann Jensen | cs.GR | [PDF](http://arxiv.org/pdf/2312.15711v1){: .btn .btn-green }

**Abstract**: Monte Carlo rendering of translucent objects with heterogeneous scattering
properties is often expensive both in terms of memory and computation. If we do
path tracing and use a high dynamic range lighting environment, the rendering
becomes computationally heavy. We propose a compact and efficient neural method
for representing and rendering the appearance of heterogeneous translucent
objects. The neural representation function resembles a bidirectional
scattering-surface reflectance distribution function (BSSRDF). However,
conventional BSSRDF models assume a planar half-space medium and only surface
variation of the material, which is often not a good representation of the
appearance of real-world objects. Our method represents the BSSRDF of a full
object taking its geometry and heterogeneities into account. This is similar to
a neural radiance field, but our representation works for an arbitrary distant
lighting environment. In a sense, we present a version of neural precomputed
radiance transfer that captures all-frequency relighting of heterogeneous
translucent objects. We use a multi-layer perceptron (MLP) with skip
connections to represent the appearance of an object as a function of spatial
position, direction of observation, and direction of incidence. The latter is
considered a directional light incident across the entire non-self-shadowed
part of the object. We demonstrate the ability of our method to store highly
complex materials while having high accuracy when comparing to reference images
of the represented object in unseen lighting environments. As compared with
path tracing of a heterogeneous light scattering volume behind a refractive
interface, our method more easily enables importance sampling of the directions
of incidence and can be integrated into existing rendering frameworks while
achieving interactive frame rates.

---

## SUNDIAL: 3D Satellite Understanding through Direct, Ambient, and Complex  Lighting Decomposition

nerf{: .label .label-blue }

2023-12-24 | Nikhil Behari, Akshat Dave, Kushagra Tiwary, William Yang, Ramesh Raskar | cs.CV | [PDF](http://arxiv.org/pdf/2312.16215v1){: .btn .btn-green }

**Abstract**: 3D modeling from satellite imagery is essential in areas of environmental
science, urban planning, agriculture, and disaster response. However,
traditional 3D modeling techniques face unique challenges in the remote sensing
context, including limited multi-view baselines over extensive regions, varying
direct, ambient, and complex illumination conditions, and time-varying scene
changes across captures. In this work, we introduce SUNDIAL, a comprehensive
approach to 3D reconstruction of satellite imagery using neural radiance
fields. We jointly learn satellite scene geometry, illumination components, and
sun direction in this single-model approach, and propose a secondary shadow ray
casting technique to 1) improve scene geometry using oblique sun angles to
render shadows, 2) enable physically-based disentanglement of scene albedo and
illumination, and 3) determine the components of illumination from direct,
ambient (sky), and complex sources. To achieve this, we incorporate lighting
cues and geometric priors from remote sensing literature in a neural rendering
approach, modeling physical properties of satellite scenes such as shadows,
scattered sky illumination, and complex illumination and shading of vegetation
and water. We evaluate the performance of SUNDIAL against existing NeRF-based
techniques for satellite scene modeling and demonstrate improved scene and
lighting disentanglement, novel view and lighting rendering, and geometry and
sun direction estimation on challenging scenes with small baselines, sparse
inputs, and variable illumination.

Comments:
- 8 pages, 6 figures

---

## Human101: Training 100+FPS Human Gaussians in 100s from 1 View

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-23 | Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2312.15258v1){: .btn .btn-green }

**Abstract**: Reconstructing the human body from single-view videos plays a pivotal role in
the virtual reality domain. One prevalent application scenario necessitates the
rapid reconstruction of high-fidelity 3D digital humans while simultaneously
ensuring real-time rendering and interaction. Existing methods often struggle
to fulfill both requirements. In this paper, we introduce Human101, a novel
framework adept at producing high-fidelity dynamic 3D human reconstructions
from 1-view videos by training 3D Gaussians in 100 seconds and rendering in
100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which
provides an explicit and efficient representation of 3D humans. Standing apart
from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric
Forward Gaussian Animation method to deform the parameters of 3D Gaussians,
thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an
impressive 60+ FPS and rendering 512-resolution images at 100+ FPS).
Experimental results indicate that our approach substantially eclipses current
methods, clocking up to a 10 times surge in frames per second and delivering
comparable or superior rendering quality. Code and demos will be released at
https://github.com/longxiang-ai/Human101.

Comments:
- Website: https://github.com/longxiang-ai/Human101

---

## Efficient Deformable Tissue Reconstruction via Orthogonal Neural Plane

nerf{: .label .label-blue }

2023-12-23 | Chen Yang, Kailing Wang, Yuehao Wang, Qi Dou, Xiaokang Yang, Wei Shen | cs.CV | [PDF](http://arxiv.org/pdf/2312.15253v1){: .btn .btn-green }

**Abstract**: Intraoperative imaging techniques for reconstructing deformable tissues in
vivo are pivotal for advanced surgical systems. Existing methods either
compromise on rendering quality or are excessively computationally intensive,
often demanding dozens of hours to perform, which significantly hinders their
practical application. In this paper, we introduce Fast Orthogonal Plane
(Forplane), a novel, efficient framework based on neural radiance fields (NeRF)
for the reconstruction of deformable tissues. We conceptualize surgical
procedures as 4D volumes, and break them down into static and dynamic fields
comprised of orthogonal neural planes. This factorization iscretizes the
four-dimensional space, leading to a decreased memory usage and faster
optimization. A spatiotemporal importance sampling scheme is introduced to
improve performance in regions with tool occlusion as well as large motions and
accelerate training. An efficient ray marching method is applied to skip
sampling among empty regions, significantly improving inference speed. Forplane
accommodates both binocular and monocular endoscopy videos, demonstrating its
extensive applicability and flexibility. Our experiments, carried out on two in
vivo datasets, the EndoNeRF and Hamlyn datasets, demonstrate the effectiveness
of our framework. In all cases, Forplane substantially accelerates both the
optimization process (by over 100 times) and the inference process (by over 15
times) while maintaining or even improving the quality across a variety of
non-rigid deformations. This significant performance improvement promises to be
a valuable asset for future intraoperative surgical applications. The code of
our project is now available at https://github.com/Loping151/ForPlane.

---

## CaLDiff: Camera Localization in NeRF via Pose Diffusion

nerf{: .label .label-blue }

2023-12-23 | Rashik Shrestha, Bishad Koju, Abhigyan Bhusal, Danda Pani Paudel, François Rameau | cs.CV | [PDF](http://arxiv.org/pdf/2312.15242v1){: .btn .btn-green }

**Abstract**: With the widespread use of NeRF-based implicit 3D representation, the need
for camera localization in the same representation becomes manifestly apparent.
Doing so not only simplifies the localization process -- by avoiding an
outside-the-NeRF-based localization -- but also has the potential to offer the
benefit of enhanced localization. This paper studies the problem of localizing
cameras in NeRF using a diffusion model for camera pose adjustment. More
specifically, given a pre-trained NeRF model, we train a diffusion model that
iteratively updates randomly initialized camera poses, conditioned upon the
image to be localized. At test time, a new camera is localized in two steps:
first, coarse localization using the proposed pose diffusion process, followed
by local refinement steps of a pose inversion process in NeRF. In fact, the
proposed camera localization by pose diffusion (CaLDiff) method also integrates
the pose inversion steps within the diffusion process. Such integration offers
significantly better localization, thanks to our downstream refinement-aware
diffusion process. Our exhaustive experiments on challenging real-world data
validate our method by providing significantly better results than the compared
methods and the established baselines. Our source code will be made publicly
available.

---

## INFAMOUS-NeRF: ImproviNg FAce MOdeling Using Semantically-Aligned  Hypernetworks with Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-23 | Andrew Hou, Feng Liu, Zhiyuan Ren, Michel Sarkis, Ning Bi, Yiying Tong, Xiaoming Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.16197v1){: .btn .btn-green }

**Abstract**: We propose INFAMOUS-NeRF, an implicit morphable face model that introduces
hypernetworks to NeRF to improve the representation power in the presence of
many training subjects. At the same time, INFAMOUS-NeRF resolves the classic
hypernetwork tradeoff of representation power and editability by learning
semantically-aligned latent spaces despite the subject-specific models, all
without requiring a large pretrained model. INFAMOUS-NeRF further introduces a
novel constraint to improve NeRF rendering along the face boundary. Our
constraint can leverage photometric surface rendering and multi-view
supervision to guide surface color prediction and improve rendering near the
surface. Finally, we introduce a novel, loss-guided adaptive sampling method
for more effective NeRF training by reducing the sampling redundancy. We show
quantitatively and qualitatively that our method achieves higher representation
power than prior face modeling methods in both controlled and in-the-wild
settings. Code and models will be released upon publication.

---

## Deformable 3D Gaussian Splatting for Animatable Human Avatars

gaussian splatting{: .label .label-blue }

2023-12-22 | HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam | cs.CV | [PDF](http://arxiv.org/pdf/2312.15059v1){: .btn .btn-green }

**Abstract**: Recent advances in neural radiance fields enable novel view synthesis of
photo-realistic images in dynamic settings, which can be applied to scenarios
with human animation. Commonly used implicit backbones to establish accurate
models, however, require many input views and additional annotations such as
human masks, UV maps and depth maps. In this work, we propose ParDy-Human
(Parameterized Dynamic Human Avatar), a fully explicit approach to construct a
digital avatar from as little as a single monocular sequence. ParDy-Human
introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D
Gaussians are deformed by a human pose model to animate the avatar. Our method
is composed of two parts: A first module that deforms canonical 3D Gaussians
according to SMPL vertices and a consecutive module that further takes their
designed joint encodings and predicts per Gaussian deformations to deal with
dynamics beyond SMPL vertex deformations. Images are then synthesized by a
rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic
human avatars which requires significantly fewer training views and images. Our
avatars learning is free of additional annotations such as masks and can be
trained with variable backgrounds while inferring full-resolution images
efficiently even on consumer hardware. We provide experimental evidence to show
that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and
THUman4.0 datasets both quantitatively and visually.

---

## PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF

nerf{: .label .label-blue }

2023-12-22 | Mohsen Gholami, Rabab Ward, Z. Jane Wang | cs.CV | [PDF](http://arxiv.org/pdf/2312.14915v1){: .btn .btn-green }

**Abstract**: This paper proposes an end-to-end framework for generating 3D human pose
datasets using Neural Radiance Fields (NeRF). Public datasets generally have
limited diversity in terms of human poses and camera viewpoints, largely due to
the resource-intensive nature of collecting 3D human pose data. As a result,
pose estimators trained on public datasets significantly underperform when
applied to unseen out-of-distribution samples. Previous works proposed
augmenting public datasets by generating 2D-3D pose pairs or rendering a large
amount of random data. Such approaches either overlook image rendering or
result in suboptimal datasets for pre-trained models. Here we propose PoseGen,
which learns to generate a dataset (human 3D poses and images) with a feedback
loss from a given pre-trained pose estimator. In contrast to prior art, our
generated data is optimized to improve the robustness of the pre-trained model.
The objective of PoseGen is to learn a distribution of data that maximizes the
prediction error of a given pre-trained model. As the learned data distribution
contains OOD samples of the pre-trained model, sampling data from such a
distribution for further fine-tuning a pre-trained model improves the
generalizability of the model. This is the first work that proposes NeRFs for
3D human data generation. NeRFs are data-driven and do not require 3D scans of
humans. Therefore, using NeRF for data generation is a new direction for
convenient user-specific data generation. Our extensive experiments show that
the proposed PoseGen improves two baseline models (SPIN and HybrIK) on four
datasets with an average 6% relative improvement.

---

## Density Uncertainty Quantification with NeRF-Ensembles: Impact of Data  and Scene Constraints

nerf{: .label .label-blue }

2023-12-22 | Miriam Jäger, Steven Landgraf, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2312.14664v1){: .btn .btn-green }

**Abstract**: In the fields of computer graphics, computer vision and photogrammetry,
Neural Radiance Fields (NeRFs) are a major topic driving current research and
development. However, the quality of NeRF-generated 3D scene reconstructions
and subsequent surface reconstructions, heavily relies on the network output,
particularly the density. Regarding this critical aspect, we propose to utilize
NeRF-Ensembles that provide a density uncertainty estimate alongside the mean
density. We demonstrate that data constraints such as low-quality images and
poses lead to a degradation of the training process, increased density
uncertainty and decreased predicted density. Even with high-quality input data,
the density uncertainty varies based on scene constraints such as acquisition
constellations, occlusions and material properties. NeRF-Ensembles not only
provide a tool for quantifying the uncertainty but exhibit two promising
advantages: Enhanced robustness and artifact removal. Through the utilization
of NeRF-Ensembles instead of single NeRFs, small outliers are removed, yielding
a smoother output with improved completeness of structures. Furthermore,
applying percentile-based thresholds on density uncertainty outliers proves to
be effective for the removal of large (foggy) artifacts in post-processing. We
conduct our methodology on 3 different datasets: (i) synthetic benchmark
dataset, (ii) real benchmark dataset, (iii) real data under realistic recording
conditions and sensors.

Comments:
- 21 pages, 12 figures, 5 tables

---

## SyncDreamer for 3D Reconstruction of Endangered Animal Species with NeRF  and NeuS

nerf{: .label .label-blue }

2023-12-21 | Ahmet Haydar Ornek, Deniz Sen, Esmanur Civil | cs.CV | [PDF](http://arxiv.org/pdf/2312.13832v1){: .btn .btn-green }

**Abstract**: The main aim of this study is to demonstrate how innovative view synthesis
and 3D reconstruction techniques can be used to create models of endangered
species using monocular RGB images. To achieve this, we employed SyncDreamer to
produce unique perspectives and NeuS and NeRF to reconstruct 3D
representations. We chose four different animals, including the oriental stork,
frog, dragonfly, and tiger, as our subjects for this study. Our results show
that the combination of SyncDreamer, NeRF, and NeuS techniques can successfully
create 3D models of endangered animals. However, we also observed that NeuS
produced blurry images, while NeRF generated sharper but noisier images. This
study highlights the potential of modeling endangered animals and offers a new
direction for future research in this field. By showcasing the effectiveness of
these advanced techniques, we hope to encourage further exploration and
development of techniques for preserving and studying endangered species.

Comments:
- 8 figures

---

## Visual Tomography: Physically Faithful Volumetric Models of Partially  Translucent Objects

nerf{: .label .label-blue }

2023-12-21 | David Nakath, Xiangyu Weng, Mengkun She, Kevin Köser | cs.CV | [PDF](http://arxiv.org/pdf/2312.13494v1){: .btn .btn-green }

**Abstract**: When created faithfully from real-world data, Digital 3D representations of
objects can be useful for human or computer-assisted analysis. Such models can
also serve for generating training data for machine learning approaches in
settings where data is difficult to obtain or where too few training data
exists, e.g. by providing novel views or images in varying conditions. While
the vast amount of visual 3D reconstruction approaches focus on non-physical
models, textured object surfaces or shapes, in this contribution we propose a
volumetric reconstruction approach that obtains a physical model including the
interior of partially translucent objects such as plankton or insects. Our
technique photographs the object under different poses in front of a bright
white light source and computes absorption and scattering per voxel. It can be
interpreted as visual tomography that we solve by inverse raytracing. We
additionally suggest a method to convert non-physical NeRF media into a
physically-based volumetric grid for initialization and illustrate the
usefulness of the approach using two real-world plankton validation sets, the
lab-scanned models being finally also relighted and virtually submerged in a
scenario with augmented medium and illumination conditions. Please visit the
project homepage at www.marine.informatik.uni-kiel.de/go/vito

Comments:
- Accepted for publication at 3DV '24

---

## Gaussian Splatting with NeRF-based Color and Opacity

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-21 | Dawid Malarz, Weronika Smolak, Jacek Tabor, Sławomir Tadeja, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2312.13729v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of
neural networks to capture the intricacies of 3D objects. By encoding the shape
and color information within neural network weights, NeRFs excel at producing
strikingly sharp novel views of 3D objects. Recently, numerous generalizations
of NeRFs utilizing generative models have emerged, expanding its versatility.
In contrast, Gaussian Splatting (GS) offers a similar renders quality with
faster training and inference as it does not need neural networks to work. We
encode information about the 3D objects in the set of Gaussian distributions
that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are
difficult to condition since they usually require circa hundred thousand
Gaussian components. To mitigate the caveats of both models, we propose a
hybrid model that uses GS representation of the 3D object's shape and
NeRF-based encoding of color and opacity. Our model uses Gaussian distributions
with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of
Gaussian), color and opacity, and neural network, which takes parameters of
Gaussian and viewing direction to produce changes in color and opacity.
Consequently, our model better describes shadows, light reflections, and
transparency of 3D objects.

---

## Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed  Diffusion Models

gaussian splatting{: .label .label-blue }

2023-12-21 | Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis | cs.CV | [PDF](http://arxiv.org/pdf/2312.13763v2){: .btn .btn-green }

**Abstract**: Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.

Comments:
- Project page:
  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/

---

## DyBluRF: Dynamic Deblurring Neural Radiance Fields for Blurry Monocular  Video

nerf{: .label .label-blue }

2023-12-21 | Minh-Quan Viet Bui, Jongmin Park, Jihyong Oh, Munchurl Kim | cs.CV | [PDF](http://arxiv.org/pdf/2312.13528v1){: .btn .btn-green }

**Abstract**: Video view synthesis, allowing for the creation of visually appealing frames
from arbitrary viewpoints and times, offers immersive viewing experiences.
Neural radiance fields, particularly NeRF, initially developed for static
scenes, have spurred the creation of various methods for video view synthesis.
However, the challenge for video view synthesis arises from motion blur, a
consequence of object or camera movement during exposure, which hinders the
precise synthesis of sharp spatio-temporal views. In response, we propose a
novel dynamic deblurring NeRF framework for blurry monocular video, called
DyBluRF, consisting of an Interleave Ray Refinement (IRR) stage and a Motion
Decomposition-based Deblurring (MDD) stage. Our DyBluRF is the first that
addresses and handles the novel view synthesis for blurry monocular video. The
IRR stage jointly reconstructs dynamic 3D scenes and refines the inaccurate
camera pose information to combat imprecise pose information extracted from the
given blurry frames. The MDD stage is a novel incremental latent sharp-rays
prediction (ILSP) approach for the blurry monocular video frames by decomposing
the latent sharp rays into global camera motion and local object motion
components. Extensive experimental results demonstrate that our DyBluRF
outperforms qualitatively and quantitatively the very recent state-of-the-art
methods. Our project page including source codes and pretrained model are
publicly available at https://kaist-viclab.github.io/dyblurf-site/.

Comments:
- The first three authors contributed equally to this work. Please
  visit our project page at https://kaist-viclab.github.io/dyblurf-site/

---

## Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion  Models with RL Finetuning

nerf{: .label .label-blue }

2023-12-21 | Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Sören Pirk, Arie E. Kaufman | cs.CV | [PDF](http://arxiv.org/pdf/2312.13980v1){: .btn .btn-green }

**Abstract**: Recent advancements in the text-to-3D task leverage finetuned text-to-image
diffusion models to generate multi-view images, followed by NeRF
reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still
suffer from multi-view inconsistency and the resulting NeRF artifacts. Although
training longer with SFT improves consistency, it also causes distribution
shift, which reduces diversity and realistic details. We argue that the SFT of
multi-view diffusion models resembles the instruction finetuning stage of the
LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods.
Essentially, RLFT methods optimize models beyond their SFT data distribution by
using their own outputs, effectively mitigating distribution shift. To this
end, we introduce Carve3D, a RLFT method coupled with the Multi-view
Reconstruction Consistency (MRC) metric, to improve the consistency of
multi-view diffusion models. To compute MRC on a set of multi-view images, we
compare them with their corresponding renderings of the reconstructed NeRF at
the same viewpoints. We validate the robustness of MRC with extensive
experiments conducted under controlled inconsistency levels. We enhance the
base RLFT algorithm to stabilize the training process, reduce distribution
shift, and identify scaling laws. Through qualitative and quantitative
experiments, along with a user study, we demonstrate Carve3D's improved
multi-view consistency, the resulting superior NeRF reconstruction quality, and
minimal distribution shift compared to longer SFT. Project webpage:
https://desaixie.github.io/carve-3d.

Comments:
- Project webpage: https://desaixie.github.io/carve-3d

---

## Virtual Pets: Animatable Animal Generation in 3D Scenes

nerf{: .label .label-blue }

2023-12-21 | Yen-Chi Cheng, Chieh Hubert Lin, Chaoyang Wang, Yash Kant, Sergey Tulyakov, Alexander Schwing, Liangyan Gui, Hsin-Ying Lee | cs.CV | [PDF](http://arxiv.org/pdf/2312.14154v1){: .btn .btn-green }

**Abstract**: Toward unlocking the potential of generative models in immersive 4D
experiences, we introduce Virtual Pet, a novel pipeline to model realistic and
diverse motions for target animal species within a 3D environment. To
circumvent the limited availability of 3D motion data aligned with
environmental geometry, we leverage monocular internet videos and extract
deformable NeRF representations for the foreground and static NeRF
representations for the background. For this, we develop a reconstruction
strategy, encompassing species-level shared template learning and per-video
fine-tuning. Utilizing the reconstructed data, we then train a conditional 3D
motion model to learn the trajectory and articulation of foreground animals in
the context of 3D backgrounds. We showcase the efficacy of our pipeline with
comprehensive qualitative and quantitative evaluations using cat videos. We
also demonstrate versatility across unseen cats and indoor environments,
producing temporally coherent 4D outputs for enriched virtual experiences.

Comments:
- Preprint. Project page: https://yccyenchicheng.github.io/VirtualPets/

---

## PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce  Lidar

nerf{: .label .label-blue }

2023-12-21 | Tzofi Klinghoffer, Xiaoyu Xiang, Siddharth Somasundaram, Yuchen Fan, Christian Richardt, Ramesh Raskar, Rakesh Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2312.14239v1){: .btn .btn-green }

**Abstract**: 3D reconstruction from a single-view is challenging because of the ambiguity
from monocular cues and lack of information about occluded regions. Neural
radiance fields (NeRF), while popular for view synthesis and 3D reconstruction,
are typically reliant on multi-view images. Existing methods for single-view 3D
reconstruction with NeRF rely on either data priors to hallucinate views of
occluded regions, which may not be physically accurate, or shadows observed by
RGB cameras, which are difficult to detect in ambient light and low albedo
backgrounds. We propose using time-of-flight data captured by a single-photon
avalanche diode to overcome these limitations. Our method models two-bounce
optical paths with NeRF, using lidar transient data for supervision. By
leveraging the advantages of both NeRF and two-bounce light measured by lidar,
we demonstrate that we can reconstruct visible and occluded geometry without
data priors or reliance on controlled ambient lighting or scene albedo. In
addition, we demonstrate improved generalization under practical constraints on
sensor spatial- and temporal-resolution. We believe our method is a promising
direction as single-photon lidars become ubiquitous on consumer devices, such
as phones, tablets, and headsets.

Comments:
- Project Page: https://platonerf.github.io/

---

## Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance  Generation



2023-12-21 | Philipp Schröppel, Christopher Wewer, Jan Eric Lenssen, Eddy Ilg, Thomas Brox | cs.CV | [PDF](http://arxiv.org/pdf/2312.14124v1){: .btn .btn-green }

**Abstract**: Controllable generation of 3D assets is important for many practical
applications like content creation in movies, games and engineering, as well as
in AR/VR. Recently, diffusion models have shown remarkable results in
generation quality of 3D objects. However, none of the existing models enable
disentangled generation to control the shape and appearance separately. For the
first time, we present a suitable representation for 3D diffusion models to
enable such disentanglement by introducing a hybrid point cloud and neural
radiance field approach. We model a diffusion process over point positions
jointly with a high-dimensional feature space for a local density and radiance
decoder. While the point positions represent the coarse shape of the object,
the point features allow modeling the geometry and appearance details. This
disentanglement enables us to sample both independently and therefore to
control both separately. Our approach sets a new state of the art in generation
compared to previous disentanglement-capable methods by reduced FID scores of
30-90% and is on-par with other non disentanglement-capable state-of-the art
methods.

---

## Splatter Image: Ultra-Fast Single-View 3D Reconstruction

gaussian splatting{: .label .label-blue }

2023-12-20 | Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi | cs.CV | [PDF](http://arxiv.org/pdf/2312.13150v1){: .btn .btn-green }

**Abstract**: We introduce the Splatter Image, an ultra-fast approach for monocular 3D
object reconstruction which operates at 38 FPS. Splatter Image is based on
Gaussian Splatting, which has recently brought real-time rendering, fast
training, and excellent scaling to multi-view reconstruction. For the first
time, we apply Gaussian Splatting in a monocular reconstruction setting. Our
approach is learning-based, and, at test time, reconstruction only requires the
feed-forward evaluation of a neural network. The main innovation of Splatter
Image is the surprisingly straightforward design: it uses a 2D image-to-image
network to map the input image to one 3D Gaussian per pixel. The resulting
Gaussians thus have the form of an image, the Splatter Image. We further extend
the method to incorporate more than one image as input, which we do by adding
cross-view attention. Owning to the speed of the renderer (588 FPS), we can use
a single GPU for training while generating entire images at each iteration in
order to optimize perceptual metrics like LPIPS. On standard benchmarks, we
demonstrate not only fast reconstruction but also better results than recent
and much more expensive baselines in terms of PSNR, LPIPS, and other metrics.

Comments:
- Project page: https://szymanowiczs.github.io/splatter-image.html .
  Code: https://github.com/szymanowiczs/splatter-image

---

## Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form  Color Estimation Method

nerf{: .label .label-blue }

2023-12-20 | Qihang Fang, Yafei Song, Keqiang Li, Liefeng Bo | cs.CV | [PDF](http://arxiv.org/pdf/2312.12726v1){: .btn .btn-green }

**Abstract**: Neural radiance field (NeRF) enables the synthesis of cutting-edge realistic
novel view images of a 3D scene. It includes density and color fields to model
the shape and radiance of a scene, respectively. Supervised by the photometric
loss in an end-to-end training manner, NeRF inherently suffers from the
shape-radiance ambiguity problem, i.e., it can perfectly fit training views but
does not guarantee decoupling the two fields correctly. To deal with this
issue, existing works have incorporated prior knowledge to provide an
independent supervision signal for the density field, including total variation
loss, sparsity loss, distortion loss, etc. These losses are based on general
assumptions about the density field, e.g., it should be smooth, sparse, or
compact, which are not adaptive to a specific scene. In this paper, we propose
a more adaptive method to reduce the shape-radiance ambiguity. The key is a
rendering method that is only based on the density field. Specifically, we
first estimate the color field based on the density field and posed images in a
closed form. Then NeRF's rendering process can proceed. We address the problems
in estimating the color field, including occlusion and non-uniformly
distributed views. Afterward, it is applied to regularize NeRF's density field.
As our regularization is guided by photometric loss, it is more adaptive
compared to existing ones. Experimental results show that our method improves
the density field of NeRF both qualitatively and quantitatively. Our code is
available at https://github.com/qihangGH/Closed-form-color-field.

Comments:
- This work has been published in NeurIPS 2023

---

## SWAGS: Sampling Windows Adaptively for Dynamic 3D Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-12-20 | Richard Shaw, Jifei Song, Arthur Moreau, Michal Nazarczuk, Sibi Catley-Chandar, Helisa Dhamo, Eduardo Perez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2312.13308v1){: .btn .btn-green }

**Abstract**: Novel view synthesis has shown rapid progress recently, with methods capable
of producing evermore photo-realistic results. 3D Gaussian Splatting has
emerged as a particularly promising method, producing high-quality renderings
of static scenes and enabling interactive viewing at real-time frame rates.
However, it is currently limited to static scenes only. In this work, we extend
3D Gaussian Splatting to reconstruct dynamic scenes. We model the dynamics of a
scene using a tunable MLP, which learns the deformation field from a canonical
space to a set of 3D Gaussians per frame. To disentangle the static and dynamic
parts of the scene, we learn a tuneable parameter for each Gaussian, which
weighs the respective MLP parameters to focus attention on the dynamic parts.
This improves the model's ability to capture dynamics in scenes with an
imbalance of static to dynamic regions. To handle scenes of arbitrary length
whilst maintaining high rendering quality, we introduce an adaptive window
sampling strategy to partition the sequence into windows based on the amount of
movement in the sequence. We train a separate dynamic Gaussian Splatting model
for each window, allowing the canonical representation to change, thus enabling
the reconstruction of scenes with significant geometric or topological changes.
Temporal consistency is enforced using a fine-tuning step with self-supervising
consistency loss on randomly sampled novel views. As a result, our method
produces high-quality renderings of general dynamic scenes with competitive
quantitative performance, which can be viewed in real-time with our dynamic
interactive viewer.

---

## ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors

nerf{: .label .label-blue }

2023-12-20 | Weijia Mao, Yan-Pei Cao, Jia-Wei Liu, Zhongcong Xu, Mike Zheng Shou | cs.CV | [PDF](http://arxiv.org/pdf/2312.13324v1){: .btn .btn-green }

**Abstract**: We introduce ShowRoom3D, a three-stage approach for generating high-quality
3D room-scale scenes from texts. Previous methods using 2D diffusion priors to
optimize neural radiance fields for generating room-scale scenes have shown
unsatisfactory quality. This is primarily attributed to the limitations of 2D
priors lacking 3D awareness and constraints in the training methodology. In
this paper, we utilize a 3D diffusion prior, MVDiffusion, to optimize the 3D
room-scale scene. Our contributions are in two aspects. Firstly, we propose a
progressive view selection process to optimize NeRF. This involves dividing the
training process into three stages, gradually expanding the camera sampling
scope. Secondly, we propose the pose transformation method in the second stage.
It will ensure MVDiffusion provide the accurate view guidance. As a result,
ShowRoom3D enables the generation of rooms with improved structural integrity,
enhanced clarity from any view, reduced content repetition, and higher
consistency across different perspectives. Extensive experiments demonstrate
that our method, significantly outperforms state-of-the-art approaches by a
large margin in terms of user study.

---

## SpecNeRF: Gaussian Directional Encoding for Specular Reflections

nerf{: .label .label-blue }

2023-12-20 | Li Ma, Vasu Agrawal, Haithem Turki, Changil Kim, Chen Gao, Pedro Sander, Michael Zollhöfer, Christian Richardt | cs.CV | [PDF](http://arxiv.org/pdf/2312.13102v1){: .btn .btn-green }

**Abstract**: Neural radiance fields have achieved remarkable performance in modeling the
appearance of 3D scenes. However, existing approaches still struggle with the
view-dependent appearance of glossy surfaces, especially under complex lighting
of indoor environments. Unlike existing methods, which typically assume distant
lighting like an environment map, we propose a learnable Gaussian directional
encoding to better model the view-dependent effects under near-field lighting
conditions. Importantly, our new directional encoding captures the
spatially-varying nature of near-field lighting and emulates the behavior of
prefiltered environment maps. As a result, it enables the efficient evaluation
of preconvolved specular color at any 3D location with varying roughness
coefficients. We further introduce a data-driven geometry prior that helps
alleviate the shape radiance ambiguity in reflection modeling. We show that our
Gaussian directional encoding and geometry prior significantly improve the
modeling of challenging specular reflections in neural radiance fields, which
helps decompose appearance into more physically meaningful components.

Comments:
- Project page: https://limacv.github.io/SpecNeRF_web/

---

## Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM

nerf{: .label .label-blue }

2023-12-20 | Junru Lin, Asen Nachkov, Songyou Peng, Luc Van Gool, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2312.13332v2){: .btn .btn-green }

**Abstract**: The opacity of rigid 3D scenes with opaque surfaces is considered to be of a
binary type. However, we observed that this property is not followed by the
existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this
prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization
through the volumetric rendering function does not facilitate easy integration
of the desired prior. Instead, we observed that the opacity of ternary-type
(TT) is well supported. In this work, we study why ternary-type opacity is
well-suited and desired for the task at hand. In particular, we provide
theoretical insights into the process of jointly optimizing radiance and
opacity through the volumetric rendering process. Through exhaustive
experiments on benchmark datasets, we validate our claim and provide insights
into the optimization process, which we believe will unleash the potential of
RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple
yet novel visual odometry scheme that uses a hybrid combination of volumetric
and warping-based image renderings. More specifically, the proposed hybrid
odometry (HO) additionally uses image warping-based coarse odometry, leading up
to an order of magnitude final speed-up. Furthermore, we show that the proposed
TT and HO well complement each other, offering state-of-the-art results on
benchmark datasets in terms of both speed and accuracy.

---

## Deep Learning on 3D Neural Fields



2023-12-20 | Pierluigi Zama Ramirez, Luca De Luigi, Daniele Sirocchi, Adriano Cardace, Riccardo Spezialetti, Francesco Ballerini, Samuele Salti, Luigi Di Stefano | cs.CV | [PDF](http://arxiv.org/pdf/2312.13277v1){: .btn .btn-green }

**Abstract**: In recent years, Neural Fields (NFs) have emerged as an effective tool for
encoding diverse continuous signals such as images, videos, audio, and 3D
shapes. When applied to 3D data, NFs offer a solution to the fragmentation and
limitations associated with prevalent discrete representations. However, given
that NFs are essentially neural networks, it remains unclear whether and how
they can be seamlessly integrated into deep learning pipelines for solving
downstream tasks. This paper addresses this research problem and introduces
nf2vec, a framework capable of generating a compact latent representation for
an input NF in a single inference pass. We demonstrate that nf2vec effectively
embeds 3D objects represented by the input NFs and showcase how the resulting
embeddings can be employed in deep learning pipelines to successfully address
various tasks, all while processing exclusively NFs. We test this framework on
several NFs used to represent 3D surfaces, such as unsigned/signed distance and
occupancy fields. Moreover, we demonstrate the effectiveness of our approach
with more complex NFs that encompass both geometry and appearance of 3D objects
such as neural radiance fields.

Comments:
- Extended version of the paper "Deep Learning on Implicit Neural
  Representations of Shapes" that was presented at ICLR 2023. arXiv admin note:
  text overlap with arXiv:2302.05438

---

## UniSDF: Unifying Neural Representations for High-Fidelity 3D  Reconstruction of Complex Scenes with Reflections

nerf{: .label .label-blue }

2023-12-20 | Fangjinhua Wang, Marie-Julie Rakotosaona, Michael Niemeyer, Richard Szeliski, Marc Pollefeys, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2312.13285v1){: .btn .btn-green }

**Abstract**: Neural 3D scene representations have shown great potential for 3D
reconstruction from 2D images. However, reconstructing real-world captures of
complex scenes still remains a challenge. Existing generic 3D reconstruction
methods often struggle to represent fine geometric details and do not
adequately model reflective surfaces of large-scale scenes. Techniques that
explicitly focus on reflective surfaces can model complex and detailed
reflections by exploiting better reflection parameterizations. However, we
observe that these methods are often not robust in real unbounded scenarios
where non-reflective as well as reflective components are present. In this
work, we propose UniSDF, a general purpose 3D reconstruction method that can
reconstruct large complex scenes with reflections. We investigate both
view-based as well as reflection-based color prediction parameterization
techniques and find that explicitly blending these representations in 3D space
enables reconstruction of surfaces that are more geometrically accurate,
especially for reflective surfaces. We further combine this representation with
a multi-resolution grid backbone that is trained in a coarse-to-fine manner,
enabling faster reconstructions than prior methods. Extensive experiments on
object-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF
360 and Ref-NeRF real demonstrate that our method is able to robustly
reconstruct complex large-scale scenes with fine details and reflective
surfaces. Please see our project page at
https://fangjinhuawang.github.io/UniSDF.

Comments:
- Project page: https://fangjinhuawang.github.io/UniSDF

---

## NeRF-VO: Real-Time Sparse Visual Odometry with Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-20 | Jens Naumann, Binbin Xu, Stefan Leutenegger, Xingxing Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2312.13471v1){: .btn .btn-green }

**Abstract**: We introduce a novel monocular visual odometry (VO) system, NeRF-VO, that
integrates learning-based sparse visual odometry for low-latency camera
tracking and a neural radiance scene representation for sophisticated dense
reconstruction and novel view synthesis. Our system initializes camera poses
using sparse visual odometry and obtains view-dependent dense geometry priors
from a monocular depth prediction network. We harmonize the scale of poses and
dense geometry, treating them as supervisory cues to train a neural implicit
scene representation. NeRF-VO demonstrates exceptional performance in both
photometric and geometric fidelity of the scene representation by jointly
optimizing a sliding window of keyframed poses and the underlying dense
geometry, which is accomplished through training the radiance field with volume
rendering. We surpass state-of-the-art methods in pose estimation accuracy,
novel view synthesis fidelity, and dense reconstruction quality across a
variety of synthetic and real-world datasets, while achieving a higher camera
tracking frequency and consuming less GPU memory.

Comments:
- 10 tables, 4 figures

---

## Compact 3D Scene Representation via Self-Organizing Gaussian Grids

gaussian splatting{: .label .label-blue }

2023-12-19 | Wieland Morgenstern, Florian Barthel, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2312.13299v1){: .btn .btn-green }

**Abstract**: 3D Gaussian Splatting has recently emerged as a highly promising technique
for modeling of static 3D scenes. In contrast to Neural Radiance Fields, it
utilizes efficient rasterization allowing for very fast rendering at
high-quality. However, the storage size is significantly higher, which hinders
practical deployment, e.g.~on resource constrained devices. In this paper, we
introduce a compact scene representation organizing the parameters of 3D
Gaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a
drastic reduction in storage requirements without compromising visual quality
during rendering. Central to our idea is the explicit exploitation of
perceptual redundancies present in natural scenes. In essence, the inherent
nature of a scene allows for numerous permutations of Gaussian parameters to
equivalently represent it. To this end, we propose a novel highly parallel
algorithm that regularly arranges the high-dimensional Gaussian parameters into
a 2D grid while preserving their neighborhood structure. During training, we
further enforce local smoothness between the sorted parameters in the grid. The
uncompressed Gaussians use the same structure as 3DGS, ensuring a seamless
integration with established renderers. Our method achieves a reduction factor
of 8x to 26x in size for complex scenes with no increase in training time,
marking a substantial leap forward in the domain of 3D scene distribution and
consumption. Additional information can be found on our project page:
https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/

---

## pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable  Generalizable 3D Reconstruction

gaussian splatting{: .label .label-blue }

2023-12-19 | David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann | cs.CV | [PDF](http://arxiv.org/pdf/2312.12337v2){: .btn .btn-green }

**Abstract**: We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D
radiance fields parameterized by 3D Gaussian primitives from pairs of images.
Our model features real-time and memory-efficient rendering for scalable
training as well as fast 3D reconstruction at inference time. To overcome local
minima inherent to sparse and locally supported representations, we predict a
dense probability distribution over 3D and sample Gaussian means from that
probability distribution. We make this sampling operation differentiable via a
reparameterization trick, allowing us to back-propagate gradients through the
Gaussian splatting representation. We benchmark our method on wide-baseline
novel view synthesis on the real-world RealEstate10k and ACID datasets, where
we outperform state-of-the-art light field transformers and accelerate
rendering by 2.5 orders of magnitude while reconstructing an interpretable and
editable 3D radiance field.

Comments:
- Project page: https://dcharatan.github.io/pixelsplat

---

## ZS-SRT: An Efficient Zero-Shot Super-Resolution Training Method for  Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-19 | Xiang Feng, Yongbo He, Yubo Wang, Chengkai Wang, Zhenzhong Kuang, Jiajun Ding, Feiwei Qin, Jun Yu, Jianping Fan | cs.CV | [PDF](http://arxiv.org/pdf/2312.12122v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have achieved great success in the task of
synthesizing novel views that preserve the same resolution as the training
views. However, it is challenging for NeRF to synthesize high-quality
high-resolution novel views with low-resolution training data. To solve this
problem, we propose a zero-shot super-resolution training framework for NeRF.
This framework aims to guide the NeRF model to synthesize high-resolution novel
views via single-scene internal learning rather than requiring any external
high-resolution training data. Our approach consists of two stages. First, we
learn a scene-specific degradation mapping by performing internal learning on a
pretrained low-resolution coarse NeRF. Second, we optimize a super-resolution
fine NeRF by conducting inverse rendering with our mapping function so as to
backpropagate the gradients from low-resolution 2D space into the
super-resolution 3D sampling space. Then, we further introduce a temporal
ensemble strategy in the inference phase to compensate for the scene estimation
errors. Our method is featured on two points: (1) it does not consume
high-resolution views or additional scene data to train super-resolution NeRF;
(2) it can speed up the training process by adopting a coarse-to-fine strategy.
By conducting extensive experiments on public datasets, we have qualitatively
and quantitatively demonstrated the effectiveness of our method.

---

## LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks  in Cluttered Tabletop Environments

nerf{: .label .label-blue }

2023-12-19 | Federico Ceola, Lorenzo Natale, Niko Sünderhauf, Krishan Rana | cs.RO | [PDF](http://arxiv.org/pdf/2312.12036v2){: .btn .btn-green }

**Abstract**: Instructing a robot to complete an everyday task within our homes has been a
long-standing challenge for robotics. While recent progress in
language-conditioned imitation learning and offline reinforcement learning has
demonstrated impressive performance across a wide range of tasks, they are
typically limited to short-horizon tasks -- not reflective of those a home
robot would be expected to complete. While existing architectures have the
potential to learn these desired behaviours, the lack of the necessary
long-horizon, multi-step datasets for real robotic systems poses a significant
challenge. To this end, we present the Long-Horizon Manipulation (LHManip)
dataset comprising 200 episodes, demonstrating 20 different manipulation tasks
via real robot teleoperation. The tasks entail multiple sub-tasks, including
grasping, pushing, stacking and throwing objects in highly cluttered
environments. Each task is paired with a natural language instruction and
multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the
dataset comprises 176,278 observation-action pairs which form part of the Open
X-Embodiment dataset. The full LHManip dataset is made publicly available at
https://github.com/fedeceola/LHManip.

Comments:
- Submitted to IJRR

---

## MixRT: Mixed Neural Representations For Real-Time NeRF Rendering

nerf{: .label .label-blue }

2023-12-19 | Chaojian Li, Bichen Wu, Peter Vajda,  Yingyan,  Lin | cs.CV | [PDF](http://arxiv.org/pdf/2312.11841v3){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) has emerged as a leading technique for novel
view synthesis, owing to its impressive photorealistic reconstruction and
rendering capability. Nevertheless, achieving real-time NeRF rendering in
large-scale scenes has presented challenges, often leading to the adoption of
either intricate baked mesh representations with a substantial number of
triangles or resource-intensive ray marching in baked representations. We
challenge these conventions, observing that high-quality geometry, represented
by meshes with substantial triangles, is not necessary for achieving
photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF
representation that includes a low-quality mesh, a view-dependent displacement
map, and a compressed NeRF model. This design effectively harnesses the
capabilities of existing graphics hardware, thus enabling real-time NeRF
rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering
framework, our proposed MixRT attains real-time rendering speeds on edge
devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),
better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360
datasets), and a smaller storage size (less than 80% compared to
state-of-the-art methods).

Comments:
- Accepted by 3DV'24. Project Page: https://licj15.github.io/MixRT/

---

## Text-Image Conditioned Diffusion for Consistent Text-to-3D Generation

nerf{: .label .label-blue }

2023-12-19 | Yuze He, Yushi Bai, Matthieu Lin, Jenny Sheng, Yubin Hu, Qi Wang, Yu-Hui Wen, Yong-Jin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.11774v1){: .btn .btn-green }

**Abstract**: By lifting the pre-trained 2D diffusion models into Neural Radiance Fields
(NeRFs), text-to-3D generation methods have made great progress. Many
state-of-the-art approaches usually apply score distillation sampling (SDS) to
optimize the NeRF representations, which supervises the NeRF optimization with
pre-trained text-conditioned 2D diffusion models such as Imagen. However, the
supervision signal provided by such pre-trained diffusion models only depends
on text prompts and does not constrain the multi-view consistency. To inject
the cross-view consistency into diffusion priors, some recent works finetune
the 2D diffusion model with multi-view data, but still lack fine-grained view
coherence. To tackle this challenge, we incorporate multi-view image conditions
into the supervision signal of NeRF optimization, which explicitly enforces
fine-grained view consistency. With such stronger supervision, our proposed
text-to-3D method effectively mitigates the generation of floaters (due to
excessive densities) and completely empty spaces (due to insufficient
densities). Our quantitative evaluations on the T$^3$Bench dataset demonstrate
that our method achieves state-of-the-art performance over existing text-to-3D
methods. We will make the code publicly available.

---

## GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View  Synthesis

gaussian splatting{: .label .label-blue }

2023-12-18 | Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2312.11458v1){: .btn .btn-green }

**Abstract**: We propose a method for dynamic scene reconstruction using deformable 3D
Gaussians that is tailored for monocular video. Building upon the efficiency of
Gaussian splatting, our approach extends the representation to accommodate
dynamic elements via a deformable set of Gaussians residing in a canonical
space, and a time-dependent deformation field defined by a multi-layer
perceptron (MLP). Moreover, under the assumption that most natural scenes have
large regions that remain static, we allow the MLP to focus its
representational power by additionally including a static Gaussian point cloud.
The concatenated dynamic and static point clouds form the input for the
Gaussian Splatting rasterizer, enabling real-time rendering. The differentiable
pipeline is optimized end-to-end with a self-supervised rendering loss. Our
method achieves results that are comparable to state-of-the-art dynamic neural
radiance field methods while allowing much faster optimization and rendering.
Project website: https://lynl7130.github.io/gaufre/index.html

Comments:
- 10 pages, 8 figures, 4 tables

---

## AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head  Synthesis

nerf{: .label .label-blue }

2023-12-18 | Dongze Li, Kang Zhao, Wei Wang, Bo Peng, Yingya Zhang, Jing Dong, Tieniu Tan | cs.CV | [PDF](http://arxiv.org/pdf/2312.10921v1){: .btn .btn-green }

**Abstract**: Audio-driven talking head synthesis is a promising topic with wide
applications in digital human, film making and virtual reality. Recent
NeRF-based approaches have shown superiority in quality and fidelity compared
to previous studies. However, when it comes to few-shot talking head
generation, a practical scenario where only few seconds of talking video is
available for one identity, two limitations emerge: 1) they either have no base
model, which serves as a facial prior for fast convergence, or ignore the
importance of audio when building the prior; 2) most of them overlook the
degree of correlation between different face regions and audio, e.g., mouth is
audio related, while ear is audio independent. In this paper, we present Audio
Enhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can
generate realistic portraits of a new speaker with fewshot dataset.
Specifically, we introduce an Audio Aware Aggregation module into the feature
fusion stage of the reference scheme, where the weight is determined by the
similarity of audio between reference and target image. Then, an Audio-Aligned
Face Generation strategy is proposed to model the audio related and audio
independent regions respectively, with a dual-NeRF framework. Extensive
experiments have shown AE-NeRF surpasses the state-of-the-art on image
fidelity, audio-lip synchronization, and generalization ability, even in
limited training set or training iterations.

Comments:
- Accepted by AAAI 2024

---

## GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-18 | Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, Umar Iqbal | cs.CV | [PDF](http://arxiv.org/pdf/2312.11461v1){: .btn .btn-green }

**Abstract**: Gaussian splatting has emerged as a powerful 3D representation that harnesses
the advantages of both explicit (mesh) and implicit (NeRF) 3D representations.
In this paper, we seek to leverage Gaussian splatting to generate realistic
animatable avatars from textual descriptions, addressing the limitations (e.g.,
flexibility and efficiency) imposed by mesh or NeRF-based representations.
However, a naive application of Gaussian splatting cannot generate high-quality
animatable avatars and suffers from learning instability; it also cannot
capture fine avatar geometries and often leads to degenerate body parts. To
tackle these problems, we first propose a primitive-based 3D Gaussian
representation where Gaussians are defined inside pose-driven primitives to
facilitate animation. Second, to stabilize and amortize the learning of
millions of Gaussians, we propose to use neural implicit fields to predict the
Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries
and extract detailed meshes, we propose a novel SDF-based implicit mesh
learning approach for 3D Gaussians that regularizes the underlying geometries
and extracts highly detailed textured meshes. Our proposed method, GAvatar,
enables the large-scale generation of diverse animatable avatars using only
text prompts. GAvatar significantly surpasses existing methods in terms of both
appearance and geometry quality, and achieves extremely fast rendering (100
fps) at 1K resolution.

Comments:
- Project website: https://nvlabs.github.io/GAvatar

---

## PNeRFLoc: Visual Localization with Point-based Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-17 | Boming Zhao, Luwei Yang, Mao Mao, Hujun Bao, Zhaopeng Cui | cs.CV | [PDF](http://arxiv.org/pdf/2312.10649v1){: .btn .btn-green }

**Abstract**: Due to the ability to synthesize high-quality novel views, Neural Radiance
Fields (NeRF) have been recently exploited to improve visual localization in a
known environment. However, the existing methods mostly utilize NeRFs for data
augmentation to improve the regression model training, and the performance on
novel viewpoints and appearances is still limited due to the lack of geometric
constraints. In this paper, we propose a novel visual localization framework,
\ie, PNeRFLoc, based on a unified point-based representation. On the one hand,
PNeRFLoc supports the initial pose estimation by matching 2D and 3D feature
points as traditional structure-based methods; on the other hand, it also
enables pose refinement with novel view synthesis using rendering-based
optimization. Specifically, we propose a novel feature adaption module to close
the gaps between the features for visual localization and neural rendering. To
improve the efficacy and efficiency of neural rendering-based optimization, we
also develop an efficient rendering-based framework with a warping loss
function. Furthermore, several robustness techniques are developed to handle
illumination changes and dynamic objects for outdoor scenarios. Experiments
demonstrate that PNeRFLoc performs the best on synthetic data when the NeRF
model can be well learned and performs on par with the SOTA method on the
visual localization benchmark datasets.

Comments:
- Accepted to AAAI 2024

---

## Learning Dense Correspondence for NeRF-Based Face Reenactment

nerf{: .label .label-blue }

2023-12-16 | Songlin Yang, Wei Wang, Yushi Lan, Xiangyu Fan, Bo Peng, Lei Yang, Jing Dong | cs.CV | [PDF](http://arxiv.org/pdf/2312.10422v2){: .btn .btn-green }

**Abstract**: Face reenactment is challenging due to the need to establish dense
correspondence between various face representations for motion transfer. Recent
studies have utilized Neural Radiance Field (NeRF) as fundamental
representation, which further enhanced the performance of multi-view face
reenactment in photo-realism and 3D consistency. However, establishing dense
correspondence between different face NeRFs is non-trivial, because implicit
representations lack ground-truth correspondence annotations like mesh-based 3D
parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning
3DMM space with NeRF-based face representations can realize motion control, it
is sub-optimal for their limited face-only modeling and low identity fidelity.
Therefore, we are inspired to ask: Can we learn the dense correspondence
between different NeRF-based face representations without a 3D parametric model
prior? To address this challenge, we propose a novel framework, which adopts
tri-planes as fundamental NeRF representation and decomposes face tri-planes
into three components: canonical tri-planes, identity deformations, and motion.
In terms of motion control, our key contribution is proposing a Plane
Dictionary (PlaneDict) module, which efficiently maps the motion conditions to
a linear weighted addition of learnable orthogonal plane bases. To the best of
our knowledge, our framework is the first method that achieves one-shot
multi-view face reenactment without a 3D parametric model prior. Extensive
experiments demonstrate that we produce better results in fine-grained motion
control and identity preservation than previous methods.

Comments:
- Accepted by Proceedings of the AAAI Conference on Artificial
  Intelligence, 2024

---

## RANRAC: Robust Neural Scene Representations via Random Ray Consensus



2023-12-15 | Benno Buschmann, Andreea Dogaru, Elmar Eisemann, Michael Weinmann, Bernhard Egger | cs.CV | [PDF](http://arxiv.org/pdf/2312.09780v1){: .btn .btn-green }

**Abstract**: We introduce RANRAC, a robust reconstruction algorithm for 3D objects
handling occluded and distracted images, which is a particularly challenging
scenario that prior robust reconstruction methods cannot deal with. Our
solution supports single-shot reconstruction by involving light-field networks,
and is also applicable to photo-realistic, robust, multi-view reconstruction
from real-world images based on neural radiance fields. While the algorithm
imposes certain limitations on the scene representation and, thereby, the
supported scene types, it reliably detects and excludes inconsistent
perspectives, resulting in clean images without floating artifacts. Our
solution is based on a fuzzy adaption of the random sample consensus paradigm,
enabling its application to large scale models. We interpret the minimal number
of samples to determine the model parameters as a tunable hyperparameter. This
is applicable, as a cleaner set of samples improves reconstruction quality.
Further, this procedure also handles outliers. Especially for conditioned
models, it can result in the same local minimum in the latent space as would be
obtained with a completely clean set. We report significant improvements for
novel-view synthesis in occluded scenarios, of up to 8dB PSNR compared to the
baseline.

---

## Towards Transferable Targeted 3D Adversarial Attack in the Physical  World

nerf{: .label .label-blue }

2023-12-15 | Yao Huang, Yinpeng Dong, Shouwei Ruan, Xiao Yang, Hang Su, Xingxing Wei | cs.CV | [PDF](http://arxiv.org/pdf/2312.09558v1){: .btn .btn-green }

**Abstract**: Compared with transferable untargeted attacks, transferable targeted
adversarial attacks could specify the misclassification categories of
adversarial samples, posing a greater threat to security-critical tasks. In the
meanwhile, 3D adversarial samples, due to their potential of multi-view
robustness, can more comprehensively identify weaknesses in existing deep
learning systems, possessing great application value. However, the field of
transferable targeted 3D adversarial attacks remains vacant. The goal of this
work is to develop a more effective technique that could generate transferable
targeted 3D adversarial examples, filling the gap in this field. To achieve
this goal, we design a novel framework named TT3D that could rapidly
reconstruct from few multi-view images into Transferable Targeted 3D textured
meshes. While existing mesh-based texture optimization methods compute
gradients in the high-dimensional mesh space and easily fall into local optima,
leading to unsatisfactory transferability and distinct distortions, TT3D
innovatively performs dual optimization towards both feature grid and
Multi-layer Perceptron (MLP) parameters in the grid-based NeRF space, which
significantly enhances black-box transferability while enjoying naturalness.
Experimental results show that TT3D not only exhibits superior cross-model
transferability but also maintains considerable adaptability across different
renders and vision tasks. More importantly, we produce 3D adversarial examples
with 3D printing techniques in the real world and verify their robust
performance under various scenarios.

Comments:
- 11 pages, 7 figures

---

## Exploring the Feasibility of Generating Realistic 3D Models of  Endangered Species Using DreamGaussian: An Analysis of Elevation Angle's  Impact on Model Generation

gaussian splatting{: .label .label-blue }

2023-12-15 | Selcuk Anil Karatopak, Deniz Sen | cs.CV | [PDF](http://arxiv.org/pdf/2312.09682v1){: .btn .btn-green }

**Abstract**: Many species face the threat of extinction. It's important to study these
species and gather information about them as much as possible to preserve
biodiversity. Due to the rarity of endangered species, there is a limited
amount of data available, making it difficult to apply data requiring
generative AI methods to this domain. We aim to study the feasibility of
generating consistent and real-like 3D models of endangered animals using
limited data. Such a phenomenon leads us to utilize zero-shot stable diffusion
models that can generate a 3D model out of a single image of the target
species. This paper investigates the intricate relationship between elevation
angle and the output quality of 3D model generation, focusing on the innovative
approach presented in DreamGaussian. DreamGaussian, a novel framework utilizing
Generative Gaussian Splatting along with novel mesh extraction and refinement
algorithms, serves as the focal point of our study. We conduct a comprehensive
analysis, analyzing the effect of varying elevation angles on DreamGaussian's
ability to reconstruct 3D scenes accurately. Through an empirical evaluation,
we demonstrate how changes in elevation angle impact the generated images'
spatial coherence, structural integrity, and perceptual realism. We observed
that giving a correct elevation angle with the input image significantly
affects the result of the generated 3D model. We hope this study to be
influential for the usability of AI to preserve endangered animals; while the
penultimate aim is to obtain a model that can output biologically consistent 3D
models via small samples, the qualitative interpretation of an existing
state-of-the-art model such as DreamGaussian will be a step forward in our
goal.

---

## SLS4D: Sparse Latent Space for 4D Novel View Synthesis

nerf{: .label .label-blue }

2023-12-15 | Qi-Yuan Feng, Hao-Xiang Chen, Qun-Ce Xu, Tai-Jiang Mu | cs.CV | [PDF](http://arxiv.org/pdf/2312.09743v1){: .btn .btn-green }

**Abstract**: Neural radiance field (NeRF) has achieved great success in novel view
synthesis and 3D representation for static scenarios. Existing dynamic NeRFs
usually exploit a locally dense grid to fit the deformation field; however,
they fail to capture the global dynamics and concomitantly yield models of
heavy parameters. We observe that the 4D space is inherently sparse. Firstly,
the deformation field is sparse in spatial but dense in temporal due to the
continuity of of motion. Secondly, the radiance field is only valid on the
surface of the underlying scene, usually occupying a small fraction of the
whole space. We thus propose to represent the 4D scene using a learnable sparse
latent space, a.k.a. SLS4D. Specifically, SLS4D first uses dense learnable time
slot features to depict the temporal space, from which the deformation field is
fitted with linear multi-layer perceptions (MLP) to predict the displacement of
a 3D position at any time. It then learns the spatial features of a 3D position
using another sparse latent space. This is achieved by learning the adaptive
weights of each latent code with the attention mechanism. Extensive experiments
demonstrate the effectiveness of our SLS4D: it achieves the best 4D novel view
synthesis using only about $6\%$ parameters of the most recent work.

Comments:
- 10 pages, 6 figures

---

## LAENeRF: Local Appearance Editing for Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-15 | Lukas Radl, Michael Steiner, Andreas Kurz, Markus Steinberger | cs.CV | [PDF](http://arxiv.org/pdf/2312.09913v1){: .btn .btn-green }

**Abstract**: Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest
towards editable implicit 3D representations has surged over the last years.
However, editing implicit or hybrid representations as used for NeRFs is
difficult due to the entanglement of appearance and geometry encoded in the
model parameters. Despite these challenges, recent research has shown first
promising steps towards photorealistic and non-photorealistic appearance edits.
The main open issues of related work include limited interactivity, a lack of
support for local edits and large memory requirements, rendering them less
useful in practice. We address these limitations with LAENeRF, a unified
framework for photorealistic and non-photorealistic appearance editing of
NeRFs. To tackle local editing, we leverage a voxel grid as starting point for
region selection. We learn a mapping from expected ray terminations to final
output color, which can optionally be supervised by a style loss, resulting in
a framework which can perform photorealistic and non-photorealistic appearance
editing of selected regions. Relying on a single point per ray for our mapping,
we limit memory requirements and enable fast optimization. To guarantee
interactivity, we compose the output color using a set of learned, modifiable
base colors, composed with additive layer mixing. Compared to concurrent work,
LAENeRF enables recoloring and stylization while keeping processing time low.
Furthermore, we demonstrate that our approach surpasses baseline methods both
quantitatively and qualitatively.

Comments:
- Project website: https://r4dl.github.io/LAENeRF/

---

## SlimmeRF: Slimmable Radiance Fields

nerf{: .label .label-blue }

2023-12-15 | Shiran Yuan, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2312.10034v1){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) and its variants have recently emerged as
successful methods for novel view synthesis and 3D scene reconstruction.
However, most current NeRF models either achieve high accuracy using large
model sizes, or achieve high memory-efficiency by trading off accuracy. This
limits the applicable scope of any single model, since high-accuracy models
might not fit in low-memory devices, and memory-efficient models might not
satisfy high-quality requirements. To this end, we present SlimmeRF, a model
that allows for instant test-time trade-offs between model size and accuracy
through slimming, thus making the model simultaneously suitable for scenarios
with different computing budgets. We achieve this through a newly proposed
algorithm named Tensorial Rank Incrementation (TRaIn) which increases the rank
of the model's tensorial representation gradually during training. We also
observe that our model allows for more effective trade-offs in sparse-view
scenarios, at times even achieving higher accuracy after being slimmed. We
credit this to the fact that erroneous information such as floaters tend to be
stored in components corresponding to higher ranks. Our implementation is
available at https://github.com/Shiran-Yuan/SlimmeRF.

Comments:
- 3DV 2024 Oral, Project Page: https://shiran-yuan.github.io/SlimmeRF/,
  Code: https://github.com/Shiran-Yuan/SlimmeRF/

---

## Customize-It-3D: High-Quality 3D Creation from A Single Image Using  Subject-Specific Knowledge Prior

nerf{: .label .label-blue }

2023-12-15 | Nan Huang, Ting Zhang, Yuhui Yuan, Dong Chen, Shanghang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2312.11535v2){: .btn .btn-green }

**Abstract**: In this paper, we present a novel two-stage approach that fully utilizes the
information provided by the reference image to establish a customized knowledge
prior for image-to-3D generation. While previous approaches primarily rely on a
general diffusion prior, which struggles to yield consistent results with the
reference image, we propose a subject-specific and multi-modal diffusion model.
This model not only aids NeRF optimization by considering the shading mode for
improved geometry but also enhances texture from the coarse results to achieve
superior refinement. Both aspects contribute to faithfully aligning the 3D
content with the subject. Extensive experiments showcase the superiority of our
method, Customize-It-3D, outperforming previous works by a substantial margin.
It produces faithful 360-degree reconstructions with impressive visual quality,
making it well-suited for various applications, including text-to-3D creation.

Comments:
- Project Page: https://nnanhuang.github.io/projects/customize-it-3d/

---

## FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple  Super-Resolution Pipeline

nerf{: .label .label-blue }

2023-12-15 | Chien-Yu Lin, Qichen Fu, Thomas Merth, Karren Yang, Anurag Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2312.11537v2){: .btn .btn-green }

**Abstract**: Super-resolution (SR) techniques have recently been proposed to upscale the
outputs of neural radiance fields (NeRF) and generate high-quality images with
enhanced inference speeds. However, existing NeRF+SR methods increase training
overhead by using extra input features, loss functions, and/or expensive
training procedures such as knowledge distillation. In this paper, we aim to
leverage SR for efficiency gains without costly training or architectural
changes. Specifically, we build a simple NeRF+SR pipeline that directly
combines existing modules, and we propose a lightweight augmentation technique,
random patch sampling, for training. Compared to existing NeRF+SR methods, our
pipeline mitigates the SR computing overhead and can be trained up to 23x
faster, making it feasible to run on consumer devices such as the Apple
MacBook. Experiments show our pipeline can upscale NeRF outputs by 2-4x while
maintaining high quality, increasing inference speeds by up to 18x on an NVIDIA
V100 GPU and 12.8x on an M1 Pro chip. We conclude that SR can be a simple but
effective technique for improving the efficiency of NeRF models for consumer
devices.

Comments:
- WACV 2024 (Oral)

---

## SpectralNeRF: Physically Based Spectral Rendering with Neural Radiance  Field

nerf{: .label .label-blue }

2023-12-14 | Ru Li, Jia Liu, Guanghui Liu, Shengping Zhang, Bing Zeng, Shuaicheng Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.08692v1){: .btn .btn-green }

**Abstract**: In this paper, we propose SpectralNeRF, an end-to-end Neural Radiance Field
(NeRF)-based architecture for high-quality physically based rendering from a
novel spectral perspective. We modify the classical spectral rendering into two
main steps, 1) the generation of a series of spectrum maps spanning different
wavelengths, 2) the combination of these spectrum maps for the RGB output. Our
SpectralNeRF follows these two steps through the proposed multi-layer
perceptron (MLP)-based architecture (SpectralMLP) and Spectrum Attention UNet
(SAUNet). Given the ray origin and the ray direction, the SpectralMLP
constructs the spectral radiance field to obtain spectrum maps of novel views,
which are then sent to the SAUNet to produce RGB images of white-light
illumination. Applying NeRF to build up the spectral rendering is a more
physically-based way from the perspective of ray-tracing. Further, the spectral
radiance fields decompose difficult scenes and improve the performance of
NeRF-based methods. Comprehensive experimental results demonstrate the proposed
SpectralNeRF is superior to recent NeRF-based methods when synthesizing new
views on synthetic and real datasets. The codes and datasets are available at
https://github.com/liru0126/SpectralNeRF.

Comments:
- Accepted by AAAI 2024

---

## CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental  Learning

nerf{: .label .label-blue }

2023-12-14 | Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, Fei Deng | cs.CV | [PDF](http://arxiv.org/pdf/2312.08760v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated impressive performance in
novel view synthesis. However, NeRF and most of its variants still rely on
traditional complex pipelines to provide extrinsic and intrinsic camera
parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF,
directly treat camera parameters as learnable and estimate them through
differential volume rendering. However, these methods work for forward-looking
scenes with slight motions and fail to tackle the rotation scenario in
practice. To overcome this limitation, we propose a novel \underline{c}amera
parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally
reconstructs 3D representations and recovers the camera parameters inspired by
incremental structure from motion (SfM). Given a sequence of images, CF-NeRF
estimates the camera parameters of images one by one and reconstructs the scene
through initialization, implicit localization, and implicit optimization. To
evaluate our method, we use a challenging real-world dataset NeRFBuster which
provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF
is robust to camera rotation and achieves state-of-the-art results without
providing prior information and constraints.

Comments:
- Accepted at the Thirty-Eighth AAAI Conference on Artificial
  Intelligence (AAAI24)

---

## VaLID: Variable-Length Input Diffusion for Novel View Synthesis



2023-12-14 | Shijie Li, Farhad G. Zanjani, Haitam Ben Yahia, Yuki M. Asano, Juergen Gall, Amirhossein Habibian | cs.CV | [PDF](http://arxiv.org/pdf/2312.08892v1){: .btn .btn-green }

**Abstract**: Novel View Synthesis (NVS), which tries to produce a realistic image at the
target view given source view images and their corresponding poses, is a
fundamental problem in 3D Vision. As this task is heavily under-constrained,
some recent work, like Zero123, tries to solve this problem with generative
modeling, specifically using pre-trained diffusion models. Although this
strategy generalizes well to new scenes, compared to neural radiance
field-based methods, it offers low levels of flexibility. For example, it can
only accept a single-view image as input, despite realistic applications often
offering multiple input images. This is because the source-view images and
corresponding poses are processed separately and injected into the model at
different stages. Thus it is not trivial to generalize the model into
multi-view source images, once they are available. To solve this issue, we try
to process each pose image pair separately and then fuse them as a unified
visual representation which will be injected into the model to guide image
synthesis at the target-views. However, inconsistency and computation costs
increase as the number of input source-view images increases. To solve these
issues, the Multi-view Cross Former module is proposed which maps
variable-length input data to fix-size output data. A two-stage training
strategy is introduced to further improve the efficiency during training time.
Qualitative and quantitative evaluation over multiple datasets demonstrates the
effectiveness of the proposed method against previous approaches. The code will
be released according to the acceptance.

Comments:
- paper and supplementary material

---

## Scene 3-D Reconstruction System in Scattering Medium

nerf{: .label .label-blue }

2023-12-14 | Zhuoyifan Zhang, Lu Zhang, Liang Wang, Haoming Wu | cs.CV | [PDF](http://arxiv.org/pdf/2312.09005v1){: .btn .btn-green }

**Abstract**: The research on neural radiance fields for new view synthesis has experienced
explosive growth with the development of new models and extensions. The NERF
algorithm, suitable for underwater scenes or scattering media, is also
evolving. Existing underwater 3D reconstruction systems still face challenges
such as extensive training time and low rendering efficiency. This paper
proposes an improved underwater 3D reconstruction system to address these
issues and achieve rapid, high-quality 3D reconstruction.To begin with, we
enhance underwater videos captured by a monocular camera to correct the poor
image quality caused by the physical properties of the water medium while
ensuring consistency in enhancement across adjacent frames. Subsequently, we
perform keyframe selection on the video frames to optimize resource utilization
and eliminate the impact of dynamic objects on the reconstruction results. The
selected keyframes, after pose estimation using COLMAP, undergo a
three-dimensional reconstruction improvement process using neural radiance
fields based on multi-resolution hash coding for model construction and
rendering.

---

## iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via  Comparing and Matching

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-14 | Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, Fei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2312.09031v1){: .btn .btn-green }

**Abstract**: We present a method named iComMa to address the 6D pose estimation problem in
computer vision. The conventional pose estimation methods typically rely on the
target's CAD model or necessitate specific network training tailored to
particular object classes. Some existing methods address mesh-free 6D pose
estimation by employing the inversion of a Neural Radiance Field (NeRF), aiming
to overcome the aforementioned constraints. However, it still suffers from
adverse initializations. By contrast, we model the pose estimation as the
problem of inverting the 3D Gaussian Splatting (3DGS) with both the comparing
and matching loss. In detail, a render-and-compare strategy is adopted for the
precise estimation of poses. Additionally, a matching module is designed to
enhance the model's robustness against adverse initializations by minimizing
the distances between 2D keypoints. This framework systematically incorporates
the distinctive characteristics and inherent rationale of render-and-compare
and matching-based approaches. This comprehensive consideration equips the
framework to effectively address a broader range of intricate and challenging
scenarios, including instances with substantial angular deviations, all while
maintaining a high level of prediction accuracy. Experimental results
demonstrate the superior precision and robustness of our proposed jointly
optimized framework when evaluated on synthetic and complex real-world data in
challenging scenarios.

Comments:
- 10 pages, 5 figures

---

## Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption

nerf{: .label .label-blue }

2023-12-14 | Ziteng Cui, Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada | cs.CV | [PDF](http://arxiv.org/pdf/2312.09093v2){: .btn .btn-green }

**Abstract**: The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered
methodology, entangling the aspects of illumination and material reflectance
into emission solely from 3D points. This simplified rendering approach
presents challenges in accurately modeling images captured under adverse
lighting conditions, such as low light or over-exposure. Motivated by the
ancient Greek emission theory that posits visual perception as a result of rays
emanating from the eyes, we slightly refine the conventional NeRF framework to
train NeRF under challenging light conditions and generate normal-light
condition novel views unsupervised. We introduce the concept of a "Concealing
Field," which assigns transmittance values to the surrounding air to account
for illumination effects. In dark scenarios, we assume that object emissions
maintain a standard lighting level but are attenuated as they traverse the air
during the rendering process. Concealing Field thus compel NeRF to learn
reasonable density and colour estimations for objects even in dimly lit
situations. Similarly, the Concealing Field can mitigate over-exposed emissions
during the rendering stage. Furthermore, we present a comprehensive multi-view
dataset captured under challenging illumination conditions for evaluation. Our
code and dataset available at https://github.com/cuiziteng/Aleth-NeRF

Comments:
- AAAI 2024, code available at https://github.com/cuiziteng/Aleth-NeRF
  Modified version of previous paper arXiv:2303.05807

---

## ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance  Field

nerf{: .label .label-blue }

2023-12-14 | Zhangkai Ni, Peiqi Yang, Wenhan Yang, Hanli Wang, Lin Ma, Sam Kwong | cs.CV | [PDF](http://arxiv.org/pdf/2312.09095v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated impressive potential in
synthesizing novel views from dense input, however, their effectiveness is
challenged when dealing with sparse input. Existing approaches that incorporate
additional depth or semantic supervision can alleviate this issue to an extent.
However, the process of supervision collection is not only costly but also
potentially inaccurate, leading to poor performance and generalization ability
in diverse scenarios. In our work, we introduce a novel model: the
Collaborative Neural Radiance Fields (ColNeRF) designed to work with sparse
input. The collaboration in ColNeRF includes both the cooperation between
sparse input images and the cooperation between the output of the neural
radiation field. Through this, we construct a novel collaborative module that
aligns information from various views and meanwhile imposes self-supervised
constraints to ensure multi-view consistency in both geometry and appearance. A
Collaborative Cross-View Volume Integration module (CCVI) is proposed to
capture complex occlusions and implicitly infer the spatial location of
objects. Moreover, we introduce self-supervision of target rays projected in
multiple directions to ensure geometric and color consistency in adjacent
regions. Benefiting from the collaboration at the input and output ends,
ColNeRF is capable of capturing richer and more generalized scene
representation, thereby facilitating higher-quality results of the novel view
synthesis. Extensive experiments demonstrate that ColNeRF outperforms
state-of-the-art sparse input generalizable NeRF methods. Furthermore, our
approach exhibits superiority in fine-tuning towards adapting to new scenes,
achieving competitive performance compared to per-scene optimized NeRF-based
methods while significantly reducing computational costs. Our code is available
at: https://github.com/eezkni/ColNeRF.

---

## Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D  Reconstruction with Transformers

gaussian splatting{: .label .label-blue }

2023-12-14 | Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2312.09147v2){: .btn .btn-green }

**Abstract**: Recent advancements in 3D reconstruction from single images have been driven
by the evolution of generative models. Prominent among these are methods based
on Score Distillation Sampling (SDS) and the adaptation of diffusion models in
the 3D domain. Despite their progress, these techniques often face limitations
due to slow optimization or rendering processes, leading to extensive training
and optimization times. In this paper, we introduce a novel approach for
single-view reconstruction that efficiently generates a 3D model from a single
image via feed-forward inference. Our method utilizes two transformer-based
networks, namely a point decoder and a triplane decoder, to reconstruct 3D
objects using a hybrid Triplane-Gaussian intermediate representation. This
hybrid representation strikes a balance, achieving a faster rendering speed
compared to implicit representations while simultaneously delivering superior
rendering quality than explicit representations. The point decoder is designed
for generating point clouds from single images, offering an explicit
representation which is then utilized by the triplane decoder to query Gaussian
features for each point. This design choice addresses the challenges associated
with directly regressing explicit 3D Gaussian attributes characterized by their
non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to
enable rapid rendering through splatting. Both decoders are built upon a
scalable, transformer-based architecture and have been efficiently trained on
large-scale 3D datasets. The evaluations conducted on both synthetic datasets
and real-world images demonstrate that our method not only achieves higher
quality but also ensures a faster runtime in comparison to previous
state-of-the-art techniques. Please see our project page at
https://zouzx.github.io/TriplaneGaussian/.

Comments:
- Project Page: https://zouzx.github.io/TriplaneGaussian/

---

## 3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-14 | Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2312.09228v2){: .btn .btn-green }

**Abstract**: We introduce an approach that creates animatable human avatars from monocular
videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural
radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image
synthesis but often require days of training, and are extremely slow at
inference time. Recently, the community has explored fast grid structures for
efficient training of clothed avatars. Albeit being extremely fast at training,
these methods can barely achieve an interactive rendering frame rate with
around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a
non-rigid deformation network to reconstruct animatable clothed human avatars
that can be trained within 30 minutes and rendered at real-time frame rates
(50+ FPS). Given the explicit nature of our representation, we further
introduce as-isometric-as-possible regularizations on both the Gaussian mean
vectors and the covariance matrices, enhancing the generalization of our model
on highly articulated unseen poses. Experimental results show that our method
achieves comparable and even better performance compared to state-of-the-art
approaches on animatable avatar creation from a monocular input, while being
400x and 250x faster in training and inference, respectively.

Comments:
- Project page: https://neuralbodies.github.io/3DGS-Avatar

---

## OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural  Radiance Fields

nerf{: .label .label-blue }

2023-12-14 | Chubin Zhang, Juncheng Yan, Yi Wei, Jiaxin Li, Li Liu, Yansong Tang, Yueqi Duan, Jiwen Lu | cs.CV | [PDF](http://arxiv.org/pdf/2312.09243v1){: .btn .btn-green }

**Abstract**: As a fundamental task of vision-based perception, 3D occupancy prediction
reconstructs 3D structures of surrounding environments. It provides detailed
information for autonomous driving planning and navigation. However, most
existing methods heavily rely on the LiDAR point clouds to generate occupancy
ground truth, which is not available in the vision-based system. In this paper,
we propose an OccNeRF method for self-supervised multi-camera occupancy
prediction. Different from bounded 3D occupancy labels, we need to consider
unbounded scenes with raw image supervision. To solve the issue, we
parameterize the reconstructed occupancy fields and reorganize the sampling
strategy. The neural rendering is adopted to convert occupancy fields to
multi-camera depth maps, supervised by multi-frame photometric consistency.
Moreover, for semantic occupancy prediction, we design several strategies to
polish the prompts and filter the outputs of a pretrained open-vocabulary 2D
segmentation model. Extensive experiments for both self-supervised depth
estimation and semantic occupancy prediction tasks on nuScenes dataset
demonstrate the effectiveness of our method.

Comments:
- Code: https://github.com/LinShan-Bin/OccNeRF

---

## ZeroRF: Fast Sparse View 360° Reconstruction with Zero Pretraining

nerf{: .label .label-blue }

2023-12-14 | Ruoxi Shi, Xinyue Wei, Cheng Wang, Hao Su | cs.CV | [PDF](http://arxiv.org/pdf/2312.09249v1){: .btn .btn-green }

**Abstract**: We present ZeroRF, a novel per-scene optimization method addressing the
challenge of sparse view 360{\deg} reconstruction in neural field
representations. Current breakthroughs like Neural Radiance Fields (NeRF) have
demonstrated high-fidelity image synthesis but struggle with sparse input
views. Existing methods, such as Generalizable NeRFs and per-scene optimization
approaches, face limitations in data dependency, computational cost, and
generalization across diverse scenarios. To overcome these challenges, we
propose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into
a factorized NeRF representation. Unlike traditional methods, ZeroRF
parametrizes feature grids with a neural network generator, enabling efficient
sparse view 360{\deg} reconstruction without any pretraining or additional
regularization. Extensive experiments showcase ZeroRF's versatility and
superiority in terms of both quality and speed, achieving state-of-the-art
results on benchmark datasets. ZeroRF's significance extends to applications in
3D content generation and editing. Project page:
https://sarahweiii.github.io/zerorf/

Comments:
- Project page: https://sarahweiii.github.io/zerorf/

---

## Stable Score Distillation for High-Quality 3D Generation

nerf{: .label .label-blue }

2023-12-14 | Boshi Tang, Jianan Wang, Zhiyong Wu, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2312.09305v1){: .btn .btn-green }

**Abstract**: Score Distillation Sampling (SDS) has exhibited remarkable performance in
conditional 3D content generation. However, a comprehensive understanding of
the SDS formulation is still lacking, hindering the development of 3D
generation. In this work, we present an interpretation of SDS as a combination
of three functional components: mode-disengaging, mode-seeking and
variance-reducing terms, and analyze the properties of each. We show that
problems such as over-smoothness and color-saturation result from the intrinsic
deficiency of the supervision terms and reveal that the variance-reducing term
introduced by SDS is sub-optimal. Additionally, we shed light on the adoption
of large Classifier-Free Guidance (CFG) scale for 3D generation. Based on the
analysis, we propose a simple yet effective approach named Stable Score
Distillation (SSD) which strategically orchestrates each term for high-quality
3D generation. Extensive experiments validate the efficacy of our approach,
demonstrating its ability to generate high-fidelity 3D content without
succumbing to issues such as over-smoothness and over-saturation, even under
low CFG conditions with the most challenging NeRF representation.

---

## LatentEditor: Text Driven Local Editing of 3D Scenes

nerf{: .label .label-blue }

2023-12-14 | Umar Khalid, Hasan Iqbal, Nazmul Karim, Jing Hua, Chen Chen | cs.CV | [PDF](http://arxiv.org/pdf/2312.09313v2){: .btn .btn-green }

**Abstract**: While neural fields have made significant strides in view synthesis and scene
reconstruction, editing them poses a formidable challenge due to their implicit
encoding of geometry and texture information from multi-view inputs. In this
paper, we introduce \textsc{LatentEditor}, an innovative framework designed to
empower users with the ability to perform precise and locally controlled
editing of neural fields using text prompts. Leveraging denoising diffusion
models, we successfully embed real-world scenes into the latent space,
resulting in a faster and more adaptable NeRF backbone for editing compared to
traditional methods. To enhance editing precision, we introduce a delta score
to calculate the 2D mask in the latent space that serves as a guide for local
modifications while preserving irrelevant regions. Our novel pixel-level
scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the
disparity between IP2P conditional and unconditional noise predictions in the
latent space. The edited latents conditioned on the 2D masks are then
iteratively updated in the training set to achieve 3D local editing. Our
approach achieves faster editing speeds and superior output quality compared to
existing 3D editing models, bridging the gap between textual instructions and
high-quality 3D scene editing in latent space. We show the superiority of our
approach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.

Comments:
- Project Page: https://latenteditor.github.io/

---

## ProNeRF: Learning Efficient Projection-Aware Ray Sampling for  Fine-Grained Implicit Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-13 | Juan Luis Gonzalez Bello, Minh-Quan Viet Bui, Munchurl Kim | cs.CV | [PDF](http://arxiv.org/pdf/2312.08136v1){: .btn .btn-green }

**Abstract**: Recent advances in neural rendering have shown that, albeit slow, implicit
compact models can learn a scene's geometries and view-dependent appearances
from multiple views. To maintain such a small memory footprint but achieve
faster inference times, recent works have adopted `sampler' networks that
adaptively sample a small subset of points along each ray in the implicit
neural radiance fields. Although these methods achieve up to a 10$\times$
reduction in rendering time, they still suffer from considerable quality
degradation compared to the vanilla NeRF. In contrast, we propose ProNeRF,
which provides an optimal trade-off between memory footprint (similar to NeRF),
speed (faster than HyperReel), and quality (better than K-Planes). ProNeRF is
equipped with a novel projection-aware sampling (PAS) network together with a
new training strategy for ray exploration and exploitation, allowing for
efficient fine-grained particle sampling. Our ProNeRF yields state-of-the-art
metrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding
0.95dB higher PSNR than the best published sampler-based method, HyperReel. Our
exploration and exploitation training strategy allows ProNeRF to learn the full
scenes' color and density distributions while also learning efficient ray
sampling focused on the highest-density regions. We provide extensive
experimental results that support the effectiveness of our method on the widely
adopted forward-facing and 360 datasets, LLFF and Blender, respectively.

Comments:
- Visit our project website at
  https://kaist-viclab.github.io/pronerf-site/

---

## Neural Radiance Fields for Transparent Object Using Visual Hull

nerf{: .label .label-blue }

2023-12-13 | Heechan Yoon, Seungkyu Lee | cs.CV | [PDF](http://arxiv.org/pdf/2312.08118v1){: .btn .btn-green }

**Abstract**: Unlike opaque object, novel view synthesis of transparent object is a
challenging task, because transparent object refracts light of background
causing visual distortions on the transparent object surface along the
viewpoint change. Recently introduced Neural Radiance Fields (NeRF) is a view
synthesis method. Thanks to its remarkable performance improvement, lots of
following applications based on NeRF in various topics have been developed.
However, if an object with a different refractive index is included in a scene
such as transparent object, NeRF shows limited performance because refracted
light ray at the surface of the transparent object is not appropriately
considered. To resolve the problem, we propose a NeRF-based method consisting
of the following three steps: First, we reconstruct a three-dimensional shape
of a transparent object using visual hull. Second, we simulate the refraction
of the rays inside of the transparent object according to Snell's law. Last, we
sample points through refracted rays and put them into NeRF. Experimental
evaluation results demonstrate that our method addresses the limitation of
conventional NeRF with transparent objects.

---

## 3DGEN: A GAN-based approach for generating novel 3D models from image  data



2023-12-13 | Antoine Schnepf, Flavian Vasile, Ugo Tanielian | cs.CV | [PDF](http://arxiv.org/pdf/2312.08094v1){: .btn .btn-green }

**Abstract**: The recent advances in text and image synthesis show a great promise for the
future of generative models in creative fields. However, a less explored area
is the one of 3D model generation, with a lot of potential applications to game
design, video production, and physical product design. In our paper, we present
3DGEN, a model that leverages the recent work on both Neural Radiance Fields
for object reconstruction and GAN-based image generation. We show that the
proposed architecture can generate plausible meshes for objects of the same
category as the training images and compare the resulting meshes with the
state-of-the-art baselines, leading to visible uplifts in generation quality.

Comments:
- Submitted to NeurIPS 2022 Machine Learning for Creativity and Design
  Workshop

---

## uSF: Learning Neural Semantic Field with Uncertainty

nerf{: .label .label-blue }

2023-12-13 | Vsevolod Skorokhodov, Darya Drozdova, Dmitry Yudin | cs.CV | [PDF](http://arxiv.org/pdf/2312.08012v1){: .btn .btn-green }

**Abstract**: Recently, there has been an increased interest in NeRF methods which
reconstruct differentiable representation of three-dimensional scenes. One of
the main limitations of such methods is their inability to assess the
confidence of the model in its predictions. In this paper, we propose a new
neural network model for the formation of extended vector representations,
called uSF, which allows the model to predict not only color and semantic label
of each point, but also estimate the corresponding values of uncertainty. We
show that with a small number of images available for training, a model
quantifying uncertainty performs better than a model without such
functionality. Code of the uSF approach is publicly available at
https://github.com/sevashasla/usf/.

Comments:
- 12 pages, 4 figures

---

## DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic  Autonomous Driving Scenes

gaussian splatting{: .label .label-blue }

2023-12-13 | Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2312.07920v1){: .btn .btn-green }

**Abstract**: We present DrivingGaussian, an efficient and effective framework for
surrounding dynamic autonomous driving scenes. For complex scenes with moving
objects, we first sequentially and progressively model the static background of
the entire scene with incremental static 3D Gaussians. We then leverage a
composite dynamic Gaussian graph to handle multiple moving objects,
individually reconstructing each object and restoring their accurate positions
and occlusion relationships within the scene. We further use a LiDAR prior for
Gaussian Splatting to reconstruct scenes with greater details and maintain
panoramic consistency. DrivingGaussian outperforms existing methods in driving
scene reconstruction and enables photorealistic surround-view synthesis with
high-fidelity and multi-camera consistency. The source code and trained models
will be released.

---

## COLMAP-Free 3D Gaussian Splatting

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-12 | Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2312.07504v1){: .btn .btn-green }

**Abstract**: While neural rendering has led to impressive advances in scene reconstruction
and novel view synthesis, it relies heavily on accurately pre-computed camera
poses. To relax this constraint, multiple efforts have been made to train
Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the
implicit representations of NeRFs provide extra challenges to optimize the 3D
structure and camera poses at the same time. On the other hand, the recently
proposed 3D Gaussian Splatting provides new opportunities given its explicit
point cloud representations. This paper leverages both the explicit geometric
representation and the continuity of the input video stream to perform novel
view synthesis without any SfM preprocessing. We process the input frames in a
sequential manner and progressively grow the 3D Gaussians set by taking one
input frame at a time, without the need to pre-compute the camera poses. Our
method significantly improves over previous approaches in view synthesis and
camera pose estimation under large motion changes. Our project page is
https://oasisyang.github.io/colmap-free-3dgs

Comments:
- Project Page: https://oasisyang.github.io/colmap-free-3dgs

---

## Unifying Correspondence, Pose and NeRF for Pose-Free Novel View  Synthesis from Stereo Pairs

nerf{: .label .label-blue }

2023-12-12 | Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jiaolong Yang, Seungryong Kim, Chong Luo | cs.CV | [PDF](http://arxiv.org/pdf/2312.07246v1){: .btn .btn-green }

**Abstract**: This work delves into the task of pose-free novel view synthesis from stereo
pairs, a challenging and pioneering task in 3D vision. Our innovative
framework, unlike any before, seamlessly integrates 2D correspondence matching,
camera pose estimation, and NeRF rendering, fostering a synergistic enhancement
of these tasks. We achieve this through designing an architecture that utilizes
a shared representation, which serves as a foundation for enhanced 3D geometry
understanding. Capitalizing on the inherent interplay between the tasks, our
unified framework is trained end-to-end with the proposed training strategy to
improve overall model accuracy. Through extensive evaluations across diverse
indoor and outdoor scenes from two real-world datasets, we demonstrate that our
approach achieves substantial improvement over previous methodologies,
especially in scenarios characterized by extreme viewpoint changes and the
absence of accurate camera poses.

Comments:
- Project page: https://ku-cvlab.github.io/CoPoNeRF/

---

## WaterHE-NeRF: Water-ray Tracing Neural Radiance Fields for Underwater  Scene Reconstruction

nerf{: .label .label-blue }

2023-12-12 | Jingchun Zhou, Tianyu Liang, Zongxin He, Dehuan Zhang, Weishi Zhang, Xianping Fu, Chongyi Li | cs.CV | [PDF](http://arxiv.org/pdf/2312.06946v1){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) technology demonstrates immense potential in
novel viewpoint synthesis tasks, due to its physics-based volumetric rendering
process, which is particularly promising in underwater scenes. Addressing the
limitations of existing underwater NeRF methods in handling light attenuation
caused by the water medium and the lack of real Ground Truth (GT) supervision,
this study proposes WaterHE-NeRF. We develop a new water-ray tracing field by
Retinex theory that precisely encodes color, density, and illuminance
attenuation in three-dimensional space. WaterHE-NeRF, through its illuminance
attenuation mechanism, generates both degraded and clear multi-view images and
optimizes image restoration by combining reconstruction loss with Wasserstein
distance. Additionally, the use of histogram equalization (HE) as pseudo-GT
enhances the network's accuracy in preserving original details and color
distribution. Extensive experiments on real underwater datasets and synthetic
datasets validate the effectiveness of WaterHE-NeRF. Our code will be made
publicly available.

---

## Creating Visual Effects with Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-11 | Cyrus Vachha | cs.CV | [PDF](http://arxiv.org/pdf/2401.08633v1){: .btn .btn-green }

**Abstract**: We present a pipeline for integrating NeRFs into traditional compositing VFX
pipelines using Nerfstudio, an open-source framework for training and rendering
NeRFs. Our approach involves using Blender, a widely used open-source 3D
creation software, to align camera paths and composite NeRF renders with meshes
and other NeRFs, allowing for seamless integration of NeRFs into traditional
VFX pipelines. Our NeRF Blender add-on allows for more controlled camera
trajectories of photorealistic scenes, compositing meshes and other
environmental effects with NeRFs, and compositing multiple NeRFs in a single
scene.This approach of generating NeRF aligned camera paths can be adapted to
other 3D tool sets and workflows, enabling a more seamless integration of NeRFs
into visual effects and film production. Documentation can be found here:
https://docs.nerf.studio/extensions/blender_addon.html

Comments:
- 2 pages, 4 figures

---

## DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior

nerf{: .label .label-blue }

2023-12-11 | Tianyu Huang, Yihan Zeng, Zhilu Zhang, Wan Xu, Hang Xu, Songcen Xu, Rynson W. H. Lau, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2312.06439v1){: .btn .btn-green }

**Abstract**: 3D generation has raised great attention in recent years. With the success of
text-to-image diffusion models, the 2D-lifting technique becomes a promising
route to controllable 3D generation. However, these methods tend to present
inconsistent geometry, which is also known as the Janus problem. We observe
that the problem is caused mainly by two aspects, i.e., viewpoint bias in 2D
diffusion models and overfitting of the optimization objective. To address it,
we propose a two-stage 2D-lifting framework, namely DreamControl, which
optimizes coarse NeRF scenes as 3D self-prior and then generates fine-grained
objects with control-based score distillation. Specifically, adaptive viewpoint
sampling and boundary integrity metric are proposed to ensure the consistency
of generated priors. The priors are then regarded as input conditions to
maintain reasonable geometries, in which conditional LoRA and weighted score
are further proposed to optimize detailed textures. DreamControl can generate
high-quality 3D content in terms of both geometry consistency and texture
fidelity. Moreover, our control-based optimization guidance is applicable to
more downstream tasks, including user-guided generation and 3D animation. The
project page is available at https://github.com/tyhuang0428/DreamControl.

---

## Gaussian Splatting SLAM

gaussian splatting{: .label .label-blue }

2023-12-11 | Hidenobu Matsuki, Riku Murai, Paul H. J. Kelly, Andrew J. Davison | cs.CV | [PDF](http://arxiv.org/pdf/2312.06741v1){: .btn .btn-green }

**Abstract**: We present the first application of 3D Gaussian Splatting to incremental 3D
reconstruction using a single moving monocular or RGB-D camera. Our
Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps,
utilises Gaussians as the only 3D representation, unifying the required
representation for accurate, efficient tracking, mapping, and high-quality
rendering. Several innovations are required to continuously reconstruct 3D
scenes with high fidelity from a live camera. First, to move beyond the
original 3DGS algorithm, which requires accurate poses from an offline
Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using
direct optimisation against the 3D Gaussians, and show that this enables fast
and robust tracking with a wide basin of convergence. Second, by utilising the
explicit nature of the Gaussians, we introduce geometric verification and
regularisation to handle the ambiguities occurring in incremental 3D dense
reconstruction. Finally, we introduce a full SLAM system which not only
achieves state-of-the-art results in novel view synthesis and trajectory
estimation, but also reconstruction of tiny and even transparent objects.

Comments:
- First two authors contributed equally to this work. Project Page:
  https://rmurai.co.uk/projects/GaussianSplattingSLAM/ Video:
  https://www.youtube.com/watch?v=x604ghp9R_Q&ab_channel=DysonRoboticsLaboratoryatImperialCollege

---

## CorresNeRF: Image Correspondence Priors for Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-11 | Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2312.06642v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have achieved impressive results in novel view
synthesis and surface reconstruction tasks. However, their performance suffers
under challenging scenarios with sparse input views. We present CorresNeRF, a
novel method that leverages image correspondence priors computed by
off-the-shelf methods to supervise NeRF training. We design adaptive processes
for augmentation and filtering to generate dense and high-quality
correspondences. The correspondences are then used to regularize NeRF training
via the correspondence pixel reprojection and depth loss terms. We evaluate our
methods on novel view synthesis and surface reconstruction tasks with
density-based and SDF-based NeRF models on different datasets. Our method
outperforms previous methods in both photometric and geometric metrics. We show
that this simple yet effective technique of using correspondence priors can be
applied as a plug-and-play module across different NeRF variants. The project
page is at https://yxlao.github.io/corres-nerf.

---

## Learning Naturally Aggregated Appearance for Efficient 3D Editing



2023-12-11 | Ka Leong Cheng, Qiuyu Wang, Zifan Shi, Kecheng Zheng, Yinghao Xu, Hao Ouyang, Qifeng Chen, Yujun Shen | cs.CV | [PDF](http://arxiv.org/pdf/2312.06657v1){: .btn .btn-green }

**Abstract**: Neural radiance fields, which represent a 3D scene as a color field and a
density field, have demonstrated great progress in novel view synthesis yet are
unfavorable for editing due to the implicitness. In view of such a deficiency,
we propose to replace the color field with an explicit 2D appearance
aggregation, also called canonical image, with which users can easily customize
their 3D editing via 2D image processing. To avoid the distortion effect and
facilitate convenient editing, we complement the canonical image with a
projection field that maps 3D points onto 2D pixels for texture lookup. This
field is carefully initialized with a pseudo canonical camera model and
optimized with offset regularity to ensure naturalness of the aggregated
appearance. Extensive experimental results on three datasets suggest that our
representation, dubbed AGAP, well supports various ways of 3D editing (e.g.,
stylization, interactive drawing, and content extraction) with no need of
re-optimization for each case, demonstrating its generalizability and
efficiency. Project page is available at https://felixcheng97.github.io/AGAP/.

Comments:
- Project Webpage: https://felixcheng97.github.io/AGAP/, Code:
  https://github.com/felixcheng97/AGAP

---

## Nuvo: Neural UV Mapping for Unruly 3D Representations



2023-12-11 | Pratul P. Srinivasan, Stephan J. Garbin, Dor Verbin, Jonathan T. Barron, Ben Mildenhall | cs.CV | [PDF](http://arxiv.org/pdf/2312.05283v1){: .btn .btn-green }

**Abstract**: Existing UV mapping algorithms are designed to operate on well-behaved
meshes, instead of the geometry representations produced by state-of-the-art 3D
reconstruction and generation techniques. As such, applying these methods to
the volume densities recovered by neural radiance fields and related techniques
(or meshes triangulated from such fields) results in texture atlases that are
too fragmented to be useful for tasks such as view synthesis or appearance
editing. We present a UV mapping method designed to operate on geometry
produced by 3D reconstruction and generation techniques. Instead of computing a
mapping defined on a mesh's vertices, our method Nuvo uses a neural field to
represent a continuous UV mapping, and optimizes it to be a valid and
well-behaved mapping for just the set of visible points, i.e. only points that
affect the scene's appearance. We show that our model is robust to the
challenges posed by ill-behaved geometry, and that it produces editable UV
mappings that can represent detailed appearance.

Comments:
- Project page at https://pratulsrinivasan.github.io/nuvo

---

## TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint  Video

nerf{: .label .label-blue }

2023-12-10 | Minye Wu, Zehao Wang, Georgios Kouros, Tinne Tuytelaars | cs.CV | [PDF](http://arxiv.org/pdf/2312.06713v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) revolutionize the realm of visual media by
providing photorealistic Free-Viewpoint Video (FVV) experiences, offering
viewers unparalleled immersion and interactivity. However, the technology's
significant storage requirements and the computational complexity involved in
generation and rendering currently limit its broader application. To close this
gap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel
technology that significantly reduces the storage size for Free-Viewpoint Video
(FVV) while maintaining low-cost generation and rendering. TeTriRF introduces a
hybrid representation with tri-planes and voxel grids to support scaling up to
long-duration sequences and scenes with complex motions or rapid changes. We
propose a group training scheme tailored to achieving high training efficiency
and yielding temporally consistent, low-entropy scene representations.
Leveraging these properties of the representations, we introduce a compression
pipeline with off-the-shelf video codecs, achieving an order of magnitude less
storage size compared to the state-of-the-art. Our experiments demonstrate that
TeTriRF can achieve competitive quality with a higher compression rate.

Comments:
- 13 pages, 11 figures

---

## ASH: Animatable Gaussian Splats for Efficient and Photoreal Human  Rendering

gaussian splatting{: .label .label-blue }

2023-12-10 | Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann | cs.CV | [PDF](http://arxiv.org/pdf/2312.05941v1){: .btn .btn-green }

**Abstract**: Real-time rendering of photorealistic and controllable human avatars stands
as a cornerstone in Computer Vision and Graphics. While recent advances in
neural implicit rendering have unlocked unprecedented photorealism for digital
avatars, real-time performance has mostly been demonstrated for static scenes
only. To address this, we propose ASH, an animatable Gaussian splatting
approach for photorealistic rendering of dynamic humans in real-time. We
parameterize the clothed human as animatable 3D Gaussians, which can be
efficiently splatted into image space to generate the final rendering. However,
naively learning the Gaussian parameters in 3D space poses a severe challenge
in terms of compute. Instead, we attach the Gaussians onto a deformable
character model, and learn their parameters in 2D texture space, which allows
leveraging efficient 2D convolutional architectures that easily scale with the
required number of Gaussians. We benchmark ASH with competing methods on
pose-controllable avatars, demonstrating that our method outperforms existing
real-time methods by a large margin and shows comparable or even better results
than offline methods.

Comments:
- 13 pages, 7 figures. For project page, see
  https://vcai.mpi-inf.mpg.de/projects/ash/

---

## Learning for CasADi: Data-driven Models in Numerical Optimization



2023-12-10 | Tim Salzmann, Jon Arrizabalaga, Joel Andersson, Marco Pavone, Markus Ryll | eess.SY | [PDF](http://arxiv.org/pdf/2312.05873v1){: .btn .btn-green }

**Abstract**: While real-world problems are often challenging to analyze analytically, deep
learning excels in modeling complex processes from data. Existing optimization
frameworks like CasADi facilitate seamless usage of solvers but face challenges
when integrating learned process models into numerical optimizations. To
address this gap, we present the Learning for CasADi (L4CasADi) framework,
enabling the seamless integration of PyTorch-learned models with CasADi for
efficient and potentially hardware-accelerated numerical optimization. The
applicability of L4CasADi is demonstrated with two tutorial examples: First, we
optimize a fish's trajectory in a turbulent river for energy efficiency where
the turbulent flow is represented by a PyTorch model. Second, we demonstrate
how an implicit Neural Radiance Field environment representation can be easily
leveraged for optimal control with L4CasADi. L4CasADi, along with examples and
documentation, is available under MIT license at
https://github.com/Tim-Salzmann/l4casadi

---

## NeVRF: Neural Video-based Radiance Fields for Long-duration Sequences

nerf{: .label .label-blue }

2023-12-10 | Minye Wu, Tinne Tuytelaars | cs.CV | [PDF](http://arxiv.org/pdf/2312.05855v1){: .btn .btn-green }

**Abstract**: Adopting Neural Radiance Fields (NeRF) to long-duration dynamic sequences has
been challenging. Existing methods struggle to balance between quality and
storage size and encounter difficulties with complex scene changes such as
topological changes and large motions. To tackle these issues, we propose a
novel neural video-based radiance fields (NeVRF) representation. NeVRF marries
neural radiance field with image-based rendering to support photo-realistic
novel view synthesis on long-duration dynamic inward-looking scenes. We
introduce a novel multi-view radiance blending approach to predict radiance
directly from multi-view videos. By incorporating continual learning
techniques, NeVRF can efficiently reconstruct frames from sequential data
without revisiting previous frames, enabling long-duration free-viewpoint
video. Furthermore, with a tailored compression approach, NeVRF can compactly
represent dynamic scenes, making dynamic radiance fields more practical in
real-world scenarios. Our extensive experiments demonstrate the effectiveness
of NeVRF in enabling long-duration sequence rendering, sequential data
reconstruction, and compact data storage.

Comments:
- 11 pages, 12 figures

---

## IL-NeRF: Incremental Learning for Neural Radiance Fields with Camera  Pose Alignment

nerf{: .label .label-blue }

2023-12-10 | Letian Zhang, Ming Li, Chen Chen, Jie Xu | cs.CV | [PDF](http://arxiv.org/pdf/2312.05748v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) is a promising approach for generating
photorealistic images and representing complex scenes. However, when processing
data sequentially, it can suffer from catastrophic forgetting, where previous
data is easily forgotten after training with new data. Existing incremental
learning methods using knowledge distillation assume that continuous data
chunks contain both 2D images and corresponding camera pose parameters,
pre-estimated from the complete dataset. This poses a paradox as the necessary
camera pose must be estimated from the entire dataset, even though the data
arrives sequentially and future chunks are inaccessible. In contrast, we focus
on a practical scenario where camera poses are unknown. We propose IL-NeRF, a
novel framework for incremental NeRF training, to address this challenge.
IL-NeRF's key idea lies in selecting a set of past camera poses as references
to initialize and align the camera poses of incoming image data. This is
followed by a joint optimization of camera poses and replay-based NeRF
distillation. Our experiments on real-world indoor and outdoor scenes show that
IL-NeRF handles incremental NeRF training and outperforms the baselines by up
to $54.04\%$ in rendering quality.

---

## R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid  Landmarks Encoding and Progressive Multilayer Conditioning

nerf{: .label .label-blue }

2023-12-09 | Zhiling Ye, LiangGuo Zhang, Dingheng Zeng, Quan Lu, Ning Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2312.05572v1){: .btn .btn-green }

**Abstract**: Dynamic NeRFs have recently garnered growing attention for 3D talking
portrait synthesis. Despite advances in rendering speed and visual quality,
challenges persist in enhancing efficiency and effectiveness. We present
R2-Talker, an efficient and effective framework enabling realistic real-time
talking head synthesis. Specifically, using multi-resolution hash grids, we
introduce a novel approach for encoding facial landmarks as conditional
features. This approach losslessly encodes landmark structures as conditional
features, decoupling input diversity, and conditional spaces by mapping
arbitrary landmarks to a unified feature space. We further propose a scheme of
progressive multilayer conditioning in the NeRF rendering pipeline for
effective conditional feature fusion. Our new approach has the following
advantages as demonstrated by extensive experiments compared with the
state-of-the-art works: 1) The lossless input encoding enables acquiring more
precise features, yielding superior visual quality. The decoupling of inputs
and conditional spaces improves generalizability. 2) The fusing of conditional
features and MLP outputs at each MLP layer enhances conditional impact,
resulting in more accurate lip synthesis and better visual quality. 3) It
compactly structures the fusion of conditional features, significantly
enhancing computational efficiency.

---

## Robo360: A 3D Omnispective Multi-Material Robotic Manipulation Dataset

nerf{: .label .label-blue }

2023-12-09 | Litian Liang, Liuyu Bian, Caiwei Xiao, Jialin Zhang, Linghao Chen, Isabella Liu, Fanbo Xiang, Zhiao Huang, Hao Su | cs.CV | [PDF](http://arxiv.org/pdf/2312.06686v1){: .btn .btn-green }

**Abstract**: Building robots that can automate labor-intensive tasks has long been the
core motivation behind the advancements in computer vision and the robotics
community. Recent interest in leveraging 3D algorithms, particularly neural
fields, has led to advancements in robot perception and physical understanding
in manipulation scenarios. However, the real world's complexity poses
significant challenges. To tackle these challenges, we present Robo360, a
dataset that features robotic manipulation with a dense view coverage, which
enables high-quality 3D neural representation learning, and a diverse set of
objects with various physical and optical properties and facilitates research
in various object manipulation and physical world modeling tasks. We confirm
the effectiveness of our dataset using existing dynamic NeRF and evaluate its
potential in learning multi-view policies. We hope that Robo360 can open new
research directions yet to be explored at the intersection of understanding the
physical world in 3D and robot control.

---

## CoGS: Controllable Gaussian Splatting

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-09 | Heng Yu, Joel Julin, Zoltán Á. Milacski, Koichiro Niinuma, László A. Jeni | cs.CV | [PDF](http://arxiv.org/pdf/2312.05664v1){: .btn .btn-green }

**Abstract**: Capturing and re-animating the 3D structure of articulated objects present
significant barriers. On one hand, methods requiring extensively calibrated
multi-view setups are prohibitively complex and resource-intensive, limiting
their practical applicability. On the other hand, while single-camera Neural
Radiance Fields (NeRFs) offer a more streamlined approach, they have excessive
training and rendering costs. 3D Gaussian Splatting would be a suitable
alternative but for two reasons. Firstly, existing methods for 3D dynamic
Gaussians require synchronized multi-view cameras, and secondly, the lack of
controllability in dynamic scenarios. We present CoGS, a method for
Controllable Gaussian Splatting, that enables the direct manipulation of scene
elements, offering real-time control of dynamic scenes without the prerequisite
of pre-computing control signals. We evaluated CoGS using both synthetic and
real-world datasets that include dynamic objects that differ in degree of
difficulty. In our evaluations, CoGS consistently outperformed existing dynamic
and controllable neural representations in terms of visual fidelity.

Comments:
- 10 pages, in submission

---

## Multi-view Inversion for 3D-aware Generative Adversarial Networks

nerf{: .label .label-blue }

2023-12-08 | Florian Barthel, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2312.05330v1){: .btn .btn-green }

**Abstract**: Current 3D GAN inversion methods for human heads typically use only one
single frontal image to reconstruct the whole 3D head model. This leaves out
meaningful information when multi-view data or dynamic videos are available.
Our method builds on existing state-of-the-art 3D GAN inversion techniques to
allow for consistent and simultaneous inversion of multiple views of the same
subject. We employ a multi-latent extension to handle inconsistencies present
in dynamic face videos to re-synthesize consistent 3D representations from the
sequence. As our method uses additional information about the target subject,
we observe significant enhancements in both geometric accuracy and image
quality, particularly when rendering from wide viewing angles. Moreover, we
demonstrate the editability of our inverted 3D renderings, which distinguishes
them from NeRF-based scene reconstructions.

---

## 360° Volumetric Portrait Avatar



2023-12-08 | Jalees Nehvi, Berna Kabadayi, Julien Valentin, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2312.05311v1){: .btn .btn-green }

**Abstract**: We propose 360{\deg} Volumetric Portrait (3VP) Avatar, a novel method for
reconstructing 360{\deg} photo-realistic portrait avatars of human subjects
solely based on monocular video inputs. State-of-the-art monocular avatar
reconstruction methods rely on stable facial performance capturing. However,
the common usage of 3DMM-based facial tracking has its limits; side-views can
hardly be captured and it fails, especially, for back-views, as required inputs
like facial landmarks or human parsing masks are missing. This results in
incomplete avatar reconstructions that only cover the frontal hemisphere. In
contrast to this, we propose a template-based tracking of the torso, head and
facial expressions which allows us to cover the appearance of a human subject
from all sides. Thus, given a sequence of a subject that is rotating in front
of a single camera, we train a neural volumetric representation based on neural
radiance fields. A key challenge to construct this representation is the
modeling of appearance changes, especially, in the mouth region (i.e., lips and
teeth). We, therefore, propose a deformation-field-based blend basis which
allows us to interpolate between different appearance states. We evaluate our
approach on captured real-world data and compare against state-of-the-art
monocular reconstruction methods. In contrast to those, our method is the first
monocular technique that reconstructs an entire 360{\deg} avatar.

Comments:
- Project page: https://jalees018.github.io/3VP-Avatar/

---

## SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational  Score Distillation



2023-12-08 | Thuan Hoang Nguyen, Anh Tran | cs.CV | [PDF](http://arxiv.org/pdf/2312.05239v1){: .btn .btn-green }

**Abstract**: Despite their ability to generate high-resolution and diverse images from
text prompts, text-to-image diffusion models often suffer from slow iterative
sampling processes. Model distillation is one of the most effective directions
to accelerate these models. However, previous distillation methods fail to
retain the generation quality while requiring a significant amount of images
for training, either from real data or synthetically generated by the teacher
model. In response to this limitation, we present a novel image-free
distillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration from
text-to-3D synthesis, in which a 3D neural radiance field that aligns with the
input prompt can be obtained from a 2D text-to-image diffusion prior via a
specialized loss without the use of any 3D data ground-truth, our approach
re-purposes that same loss for distilling a pretrained multi-step text-to-image
model to a student network that can generate high-fidelity images with just a
single inference step. In spite of its simplicity, our model stands as one of
the first one-step text-to-image generators that can produce images of
comparable quality to Stable Diffusion without reliance on any training image
data. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and a
CLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive
results or even substantially surpassing existing state-of-the-art distillation
techniques.

Comments:
- Project Page: https://thuanz123.github.io/swiftbrush/

---

## TriHuman : A Real-time and Controllable Tri-plane Representation for  Detailed Human Geometry and Appearance Synthesis

nerf{: .label .label-blue }

2023-12-08 | Heming Zhu, Fangneng Zhan, Christian Theobalt, Marc Habermann | cs.CV | [PDF](http://arxiv.org/pdf/2312.05161v1){: .btn .btn-green }

**Abstract**: Creating controllable, photorealistic, and geometrically detailed digital
doubles of real humans solely from video data is a key challenge in Computer
Graphics and Vision, especially when real-time performance is required. Recent
methods attach a neural radiance field (NeRF) to an articulated structure,
e.g., a body model or a skeleton, to map points into a pose canonical space
while conditioning the NeRF on the skeletal pose. These approaches typically
parameterize the neural field with a multi-layer perceptron (MLP) leading to a
slow runtime. To address this drawback, we propose TriHuman a novel
human-tailored, deformable, and efficient tri-plane representation, which
achieves real-time performance, state-of-the-art pose-controllable geometry
synthesis as well as photorealistic rendering quality. At the core, we
non-rigidly warp global ray samples into our undeformed tri-plane texture
space, which effectively addresses the problem of global points being mapped to
the same tri-plane locations. We then show how such a tri-plane feature
representation can be conditioned on the skeletal motion to account for dynamic
appearance and geometry changes. Our results demonstrate a clear step towards
higher quality in terms of geometry and appearance modeling of humans as well
as runtime performance.

---

## Learn to Optimize Denoising Scores for 3D Generation: A Unified and  Improved Diffusion Prior on NeRF and 3D Gaussian Splatting

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-08 | Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2312.04820v1){: .btn .btn-green }

**Abstract**: We propose a unified framework aimed at enhancing the diffusion priors for 3D
generation tasks. Despite the critical importance of these tasks, existing
methodologies often struggle to generate high-caliber results. We begin by
examining the inherent limitations in previous diffusion priors. We identify a
divergence between the diffusion priors and the training procedures of
diffusion models that substantially impairs the quality of 3D generation. To
address this issue, we propose a novel, unified framework that iteratively
optimizes both the 3D model and the diffusion prior. Leveraging the different
learnable parameters of the diffusion prior, our approach offers multiple
configurations, affording various trade-offs between performance and
implementation complexity. Notably, our experimental results demonstrate that
our method markedly surpasses existing techniques, establishing new
state-of-the-art in the realm of text-to-3D generation. Furthermore, our
approach exhibits impressive performance on both NeRF and the newly introduced
3D Gaussian Splatting backbones. Additionally, our framework yields insightful
contributions to the understanding of recent score distillation methods, such
as the VSD and DDS loss.

---

## Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular  Video

nerf{: .label .label-blue }

2023-12-08 | Yuchen Rao, Eduardo Perez Pellitero, Benjamin Busam, Yiren Zhou, Jifei Song | cs.CV | [PDF](http://arxiv.org/pdf/2312.04784v1){: .btn .btn-green }

**Abstract**: Recent advancements in 3D avatar generation excel with multi-view supervision
for photorealistic models. However, monocular counterparts lag in quality
despite broader applicability. We propose ReCaLab to close this gap. ReCaLab is
a fully-differentiable pipeline that learns high-fidelity 3D human avatars from
just a single RGB video. A pose-conditioned deformable NeRF is optimized to
volumetrically represent a human subject in canonical T-pose. The canonical
representation is then leveraged to efficiently associate viewpoint-agnostic
textures using 2D-3D correspondences. This enables to separately generate
albedo and shading which jointly compose an RGB prediction. The design allows
to control intermediate results for human pose, body shape, texture, and
lighting with text prompts. An image-conditioned diffusion model thereby helps
to animate appearance and pose of the 3D avatar to create video sequences with
previously unseen human motion. Extensive experiments show that ReCaLab
outperforms previous monocular approaches in terms of image quality for image
synthesis tasks. ReCaLab even outperforms multi-view methods that leverage up
to 19x more synchronized videos for the task of novel pose rendering. Moreover,
natural language offers an intuitive user interface for creative manipulation
of 3D human avatars.

Comments:
- Video link: https://youtu.be/Oz83z1es2J4

---

## Correspondences of the Third Kind: Camera Pose Estimation from Object  Reflection

nerf{: .label .label-blue }

2023-12-07 | Kohei Yamashita, Vincent Lepetit, Ko Nishino | cs.CV | [PDF](http://arxiv.org/pdf/2312.04527v1){: .btn .btn-green }

**Abstract**: Computer vision has long relied on two kinds of correspondences: pixel
correspondences in images and 3D correspondences on object surfaces. Is there
another kind, and if there is, what can they do for us? In this paper, we
introduce correspondences of the third kind we call reflection correspondences
and show that they can help estimate camera pose by just looking at objects
without relying on the background. Reflection correspondences are point
correspondences in the reflected world, i.e., the scene reflected by the object
surface. The object geometry and reflectance alters the scene geometrically and
radiometrically, respectively, causing incorrect pixel correspondences.
Geometry recovered from each image is also hampered by distortions, namely
generalized bas-relief ambiguity, leading to erroneous 3D correspondences. We
show that reflection correspondences can resolve the ambiguities arising from
these distortions. We introduce a neural correspondence estimator and a RANSAC
algorithm that fully leverages all three kinds of correspondences for robust
and accurate joint camera pose and object shape estimation just from the object
appearance. The method expands the horizon of numerous downstream tasks,
including camera pose estimation for appearance modeling (e.g., NeRF) and
motion estimation of reflective objects (e.g., cars on the road), to name a
few, as it relieves the requirement of overlapping background.

---

## Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial  Reconstruction

nerf{: .label .label-blue }

2023-12-07 | Jiayi Kong, Baixin Xu, Xurui Song, Chen Qian, Jun Luo, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2312.04106v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) typically require a complete set of images
taken from multiple camera perspectives to accurately reconstruct geometric
details. However, this approach raise significant privacy concerns in the
context of facial reconstruction. The critical need for privacy protection
often leads invidividuals to be reluctant in sharing their facial images, due
to fears of potential misuse or security risks. Addressing these concerns, we
propose a method that leverages privacy-preserving images for reconstructing 3D
head geometry within the NeRF framework. Our method stands apart from
traditional facial reconstruction techniques as it does not depend on RGB
information from images containing sensitive facial data. Instead, it
effectively generates plausible facial geometry using a series of
identity-obscured inputs, thereby protecting facial privacy.

---

## Towards 4D Human Video Stylization

nerf{: .label .label-blue }

2023-12-07 | Tiantian Wang, Xinxin Zuo, Fangzhou Mu, Jian Wang, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2312.04143v1){: .btn .btn-green }

**Abstract**: We present a first step towards 4D (3D and time) human video stylization,
which addresses style transfer, novel view synthesis and human animation within
a unified framework. While numerous video stylization methods have been
developed, they are often restricted to rendering images in specific viewpoints
of the input video, lacking the capability to generalize to novel views and
novel poses in dynamic scenes. To overcome these limitations, we leverage
Neural Radiance Fields (NeRFs) to represent videos, conducting stylization in
the rendered feature space. Our innovative approach involves the simultaneous
representation of both the human subject and the surrounding scene using two
NeRFs. This dual representation facilitates the animation of human subjects
across various poses and novel viewpoints. Specifically, we introduce a novel
geometry-guided tri-plane representation, significantly enhancing feature
representation robustness compared to direct tri-plane optimization. Following
the video reconstruction, stylization is performed within the NeRFs' rendered
feature space. Extensive experiments demonstrate that the proposed method
strikes a superior balance between stylized textures and temporal coherence,
surpassing existing approaches. Furthermore, our framework uniquely extends its
capabilities to accommodate novel poses and viewpoints, making it a versatile
tool for creative human video stylization.

Comments:
- Under Review

---

## Multi-View Unsupervised Image Generation with Cross Attention Guidance

nerf{: .label .label-blue }

2023-12-07 | Llukman Cerkezi, Aram Davtyan, Sepehr Sameni, Paolo Favaro | cs.CV | [PDF](http://arxiv.org/pdf/2312.04337v1){: .btn .btn-green }

**Abstract**: The growing interest in novel view synthesis, driven by Neural Radiance Field
(NeRF) models, is hindered by scalability issues due to their reliance on
precisely annotated multi-view images. Recent models address this by
fine-tuning large text2image diffusion models on synthetic multi-view data.
Despite robust zero-shot generalization, they may need post-processing and can
face quality issues due to the synthetic-real domain gap. This paper introduces
a novel pipeline for unsupervised training of a pose-conditioned diffusion
model on single-category datasets. With the help of pretrained self-supervised
Vision Transformers (DINOv2), we identify object poses by clustering the
dataset through comparing visibility and locations of specific object parts.
The pose-conditioned diffusion model, trained on pose labels, and equipped with
cross-frame attention at inference time ensures cross-view consistency, that is
further aided by our novel hard-attention guidance. Our model, MIRAGE,
surpasses prior work in novel view synthesis on real images. Furthermore,
MIRAGE is robust to diverse textures and geometries, as demonstrated with our
experiments on synthetic images generated with pretrained Stable Diffusion.

---

## NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion



2023-12-07 | Savva Ignatyev, Daniil Selikhanovych, Oleg Voynov, Yiqun Wang, Peter Wonka, Stamatios Lefkimmiatis, Evgeny Burnaev | cs.CV | [PDF](http://arxiv.org/pdf/2312.04654v1){: .btn .btn-green }

**Abstract**: We present a novel method for 3D surface reconstruction from multiple images
where only a part of the object of interest is captured. Our approach builds on
two recent developments: surface reconstruction using neural radiance fields
for the reconstruction of the visible parts of the surface, and guidance of
pre-trained 2D diffusion models in the form of Score Distillation Sampling
(SDS) to complete the shape in unobserved regions in a plausible manner. We
introduce three components. First, we suggest employing normal maps as a pure
geometric representation for SDS instead of color renderings which are
entangled with the appearance information. Second, we introduce the freezing of
the SDS noise during training which results in more coherent gradients and
better convergence. Third, we propose Multi-View SDS as a way to condition the
generation of the non-observable part of the surface without fine-tuning or
making changes to the underlying 2D Stable Diffusion model. We evaluate our
approach on the BlendedMVS dataset demonstrating significant qualitative and
quantitative improvements over competing methods.

---

## MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar

gaussian splatting{: .label .label-blue }

2023-12-07 | Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.04558v1){: .btn .btn-green }

**Abstract**: The ability to animate photo-realistic head avatars reconstructed from
monocular portrait video sequences represents a crucial step in bridging the
gap between the virtual and real worlds. Recent advancements in head avatar
techniques, including explicit 3D morphable meshes (3DMM), point clouds, and
neural implicit representation have been exploited for this ongoing research.
However, 3DMM-based methods are constrained by their fixed topologies,
point-based approaches suffer from a heavy training burden due to the extensive
quantity of points involved, and the last ones suffer from limitations in
deformation flexibility and rendering efficiency. In response to these
challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head
Avatar), a novel approach that harnesses 3D Gaussian point representation
coupled with a Gaussian deformation field to learn explicit head avatars from
monocular portrait videos. We define our head avatars with Gaussian points
characterized by adaptable shapes, enabling flexible topology. These points
exhibit movement with a Gaussian deformation field in alignment with the target
pose and expression of a person, facilitating efficient deformation.
Additionally, the Gaussian points have controllable shape, size, color, and
opacity combined with Gaussian splatting, allowing for efficient training and
rendering. Experiments demonstrate the superior performance of our method,
which achieves state-of-the-art results among previous methods.

Comments:
- The link to our projectpage is
  https://yufan1012.github.io/MonoGaussianAvatar

---

## EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-07 | Sharath Girish, Kamal Gupta, Abhinav Shrivastava | cs.CV | [PDF](http://arxiv.org/pdf/2312.04564v1){: .btn .btn-green }

**Abstract**: Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view
scene synthesis. It addresses the challenges of lengthy training times and slow
rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,
differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time
rendering and accelerated training. They, however, demand substantial memory
resources for both training and storage, as they require millions of Gaussians
in their point cloud representation for each scene. We present a technique
utilizing quantized embeddings to significantly reduce memory storage
requirements and a coarse-to-fine training strategy for a faster and more
stable optimization of the Gaussian point clouds. Our approach results in scene
representations with fewer Gaussians and quantized representations, leading to
faster training times and rendering speeds for real-time rendering of high
resolution scenes. We reduce memory by more than an order of magnitude all
while maintaining the reconstruction quality. We validate the effectiveness of
our approach on a variety of datasets and scenes preserving the visual quality
while consuming 10-20x less memory and faster training/inference speed. Project
page and code is available https://efficientgaussian.github.io

Comments:
- Website: https://efficientgaussian.github.io Code:
  https://github.com/Sharath-girish/efficientgaussian

---

## MuRF: Multi-Baseline Radiance Fields

nerf{: .label .label-blue }

2023-12-07 | Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, Fisher Yu | cs.CV | [PDF](http://arxiv.org/pdf/2312.04565v1){: .btn .btn-green }

**Abstract**: We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward
approach to solving sparse view synthesis under multiple different baseline
settings (small and large baselines, and different number of input views). To
render a target novel view, we discretize the 3D space into planes parallel to
the target image plane, and accordingly construct a target view frustum volume.
Such a target volume representation is spatially aligned with the target view,
which effectively aggregates relevant information from the input views for
high-quality rendering. It also facilitates subsequent radiance field
regression with a convolutional network thanks to its axis-aligned nature. The
3D context modeled by the convolutional network enables our method to synthesis
sharper scene structures than prior works. Our MuRF achieves state-of-the-art
performance across multiple different baseline settings and diverse scenarios
ranging from simple objects (DTU) to complex indoor and outdoor scenes
(RealEstate10K and LLFF). We also show promising zero-shot generalization
abilities on the Mip-NeRF 360 dataset, demonstrating the general applicability
of MuRF.

Comments:
- Project page: https://haofeixu.github.io/murf/

---

## VOODOO 3D: Volumetric Portrait Disentanglement for One-Shot 3D Head  Reenactment



2023-12-07 | Phong Tran, Egor Zakharov, Long-Nhat Ho, Anh Tuan Tran, Liwen Hu, Hao Li | cs.CV | [PDF](http://arxiv.org/pdf/2312.04651v1){: .btn .btn-green }

**Abstract**: We present a 3D-aware one-shot head reenactment method based on a fully
volumetric neural disentanglement framework for source appearance and driver
expressions. Our method is real-time and produces high-fidelity and
view-consistent output, suitable for 3D teleconferencing systems based on
holographic displays. Existing cutting-edge 3D-aware reenactment methods often
use neural radiance fields or 3D meshes to produce view-consistent appearance
encoding, but, at the same time, they rely on linear face models, such as 3DMM,
to achieve its disentanglement with facial expressions. As a result, their
reenactment results often exhibit identity leakage from the driver or have
unnatural expressions. To address these problems, we propose a neural
self-supervised disentanglement approach that lifts both the source image and
driver video frame into a shared 3D volumetric representation based on
tri-planes. This representation can then be freely manipulated with expression
tri-planes extracted from the driving images and rendered from an arbitrary
view using neural radiance fields. We achieve this disentanglement via
self-supervised learning on a large in-the-wild video dataset. We further
introduce a highly effective fine-tuning approach to improve the
generalizability of the 3D lifting using the same real-world data. We
demonstrate state-of-the-art performance on a wide range of datasets, and also
showcase high-quality 3D-aware head reenactment on highly challenging and
diverse subjects, including non-frontal head poses and complex expressions for
both source and driver.

---

## Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion

nerf{: .label .label-blue }

2023-12-06 | Kira Prabhu, Jane Wu, Lynn Tsai, Peter Hedman, Dan B Goldman, Ben Poole, Michael Broxton | cs.CV | [PDF](http://arxiv.org/pdf/2312.03869v1){: .btn .btn-green }

**Abstract**: This paper presents a novel approach to inpainting 3D regions of a scene,
given masked multi-view images, by distilling a 2D diffusion model into a
learned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods
that explicitly condition the diffusion model on camera pose or multi-view
information, our diffusion model is conditioned only on a single masked 2D
image. Nevertheless, we show that this 2D diffusion model can still serve as a
generative prior in a 3D multi-view reconstruction problem where we optimize a
NeRF using a combination of score distillation sampling and NeRF reconstruction
losses. Predicted depth is used as additional supervision to encourage accurate
geometry. We compare our approach to 3D inpainting methods that focus on object
removal. Because our method can generate content to fill any 3D masked region,
we additionally demonstrate 3D object completion, 3D object replacement, and 3D
scene completion.

---

## HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian  Splatting



2023-12-06 | Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2312.03461v2){: .btn .btn-green }

**Abstract**: We have recently seen tremendous progress in photo-real human modeling and
rendering. Yet, efficiently rendering realistic human performance and
integrating it into the rasterization pipeline remains challenging. In this
paper, we present HiFi4G, an explicit and compact Gaussian-based approach for
high-fidelity human performance rendering from dense footage. Our core
intuition is to marry the 3D Gaussian representation with non-rigid tracking,
achieving a compact and compression-friendly representation. We first propose a
dual-graph mechanism to obtain motion priors, with a coarse deformation graph
for effective initialization and a fine-grained Gaussian graph to enforce
subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with
adaptive spatial-temporal regularizers to effectively balance the non-rigid
prior and Gaussian updating. We also present a companion compression scheme
with residual compensation for immersive experiences on various platforms. It
achieves a substantial compression rate of approximately 25 times, with less
than 2MB of storage per frame. Extensive experiments demonstrate the
effectiveness of our approach, which significantly outperforms existing
approaches in terms of optimization speed, rendering quality, and storage
overhead.

---

## Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-06 | Youtian Lin, Zuozhuo Dai, Siyu Zhu, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2312.03431v1){: .btn .btn-green }

**Abstract**: We introduce Gaussian-Flow, a novel point-based approach for fast dynamic
scene reconstruction and real-time rendering from both multi-view and monocular
videos. In contrast to the prevalent NeRF-based approaches hampered by slow
training and rendering speeds, our approach harnesses recent advancements in
point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain
Deformation Model (DDDM) is proposed to explicitly model attribute deformations
of each Gaussian point, where the time-dependent residual of each attribute is
captured by a polynomial fitting in the time domain, and a Fourier series
fitting in the frequency domain. The proposed DDDM is capable of modeling
complex scene deformations across long video footage, eliminating the need for
training separate 3DGS for each frame or introducing an additional implicit
neural field to model 3D dynamics. Moreover, the explicit deformation modeling
for discretized Gaussian points ensures ultra-fast training and rendering of a
4D scene, which is comparable to the original 3DGS designed for static 3D
reconstruction. Our proposed approach showcases a substantial efficiency
improvement, achieving a $5\times$ faster training speed compared to the
per-frame 3DGS modeling. In addition, quantitative results demonstrate that the
proposed Gaussian-Flow significantly outperforms previous leading methods in
novel view rendering quality. Project page:
https://nju-3dv.github.io/projects/Gaussian-Flow

---

## Artist-Friendly Relightable and Animatable Neural Heads

nerf{: .label .label-blue }

2023-12-06 | Yingyan Xu, Prashanth Chandran, Sebastian Weiss, Markus Gross, Gaspard Zoss, Derek Bradley | cs.CV | [PDF](http://arxiv.org/pdf/2312.03420v1){: .btn .btn-green }

**Abstract**: An increasingly common approach for creating photo-realistic digital avatars
is through the use of volumetric neural fields. The original neural radiance
field (NeRF) allowed for impressive novel view synthesis of static heads when
trained on a set of multi-view images, and follow up methods showed that these
neural representations can be extended to dynamic avatars. Recently, new
variants also surpassed the usual drawback of baked-in illumination in neural
representations, showing that static neural avatars can be relit in any
environment. In this work we simultaneously tackle both the motion and
illumination problem, proposing a new method for relightable and animatable
neural heads. Our method builds on a proven dynamic avatar approach based on a
mixture of volumetric primitives, combined with a recently-proposed lightweight
hardware setup for relightable neural fields, and includes a novel architecture
that allows relighting dynamic neural avatars performing unseen expressions in
any environment, even with nearfield illumination and viewpoints.

---

## Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-12-06 | Vladimir Yugay, Yue Li, Theo Gevers, Martin R. Oswald | cs.CV | [PDF](http://arxiv.org/pdf/2312.10070v1){: .btn .btn-green }

**Abstract**: We present a new dense simultaneous localization and mapping (SLAM) method
that uses Gaussian splats as a scene representation. The new representation
enables interactive-time reconstruction and photo-realistic rendering of
real-world and synthetic scenes. We propose novel strategies for seeding and
optimizing Gaussian splats to extend their use from multiview offline scenarios
to sequential monocular RGBD input data setups. In addition, we extend Gaussian
splats to encode geometry and experiment with tracking against this scene
representation. Our method achieves state-of-the-art rendering quality on both
real-world and synthetic datasets while being competitive in reconstruction
performance and runtime.

---

## Evaluating the point cloud of individual trees generated from images  based on Neural Radiance fields (NeRF) method

nerf{: .label .label-blue }

2023-12-06 | Hongyu Huang, Guoji Tian, Chongcheng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2312.03372v1){: .btn .btn-green }

**Abstract**: Three-dimensional (3D) reconstruction of trees has always been a key task in
precision forestry management and research. Due to the complex branch
morphological structure of trees themselves and the occlusions from tree stems,
branches and foliage, it is difficult to recreate a complete three-dimensional
tree model from a two-dimensional image by conventional photogrammetric
methods. In this study, based on tree images collected by various cameras in
different ways, the Neural Radiance Fields (NeRF) method was used for
individual tree reconstruction and the exported point cloud models are compared
with point cloud derived from photogrammetric reconstruction and laser scanning
methods. The results show that the NeRF method performs well in individual tree
3D reconstruction, as it has higher successful reconstruction rate, better
reconstruction in the canopy area, it requires less amount of images as input.
Compared with photogrammetric reconstruction method, NeRF has significant
advantages in reconstruction efficiency and is adaptable to complex scenes, but
the generated point cloud tends to be noisy and low resolution. The accuracy of
tree structural parameters (tree height and diameter at breast height)
extracted from the photogrammetric point cloud is still higher than those of
derived from the NeRF point cloud. The results of this study illustrate the
great potential of NeRF method for individual tree reconstruction, and it
provides new ideas and research directions for 3D reconstruction and
visualization of complex forest scenes.

Comments:
- 25 pages; 6 figures

---

## RING-NeRF: A Versatile Architecture based on Residual Implicit Neural  Grids

nerf{: .label .label-blue }

2023-12-06 | Doriand Petit, Steve Bourgeois, Dumitru Pavel, Vincent Gay-Bellile, Florian Chabot, Loic Barthe | cs.CV | [PDF](http://arxiv.org/pdf/2312.03357v1){: .btn .btn-green }

**Abstract**: Since their introduction, Neural Fields have become very popular for 3D
reconstruction and new view synthesis. Recent researches focused on
accelerating the process, as well as improving the robustness to variation of
the observation distance and limited number of supervised viewpoints. However,
those approaches often led to dedicated solutions that cannot be easily
combined. To tackle this issue, we introduce a new simple but efficient
architecture named RING-NeRF, based on Residual Implicit Neural Grids, that
provides a control on the level of detail of the mapping function between the
scene and the latent spaces. Associated with a distance-aware forward mapping
mechanism and a continuous coarse-to-fine reconstruction process, our versatile
architecture demonstrates both fast training and state-of-the-art performances
in terms of: (1) anti-aliased rendering, (2) reconstruction quality from few
supervised viewpoints, and (3) robustness in the absence of appropriate
scene-specific initialization for SDF-based NeRFs. We also demonstrate that our
architecture can dynamically add grids to increase the details of the
reconstruction, opening the way to adaptive reconstruction.

---

## SO-NeRF: Active View Planning for NeRF using Surrogate Objectives

nerf{: .label .label-blue }

2023-12-06 | Keifer Lee, Shubham Gupta, Sunglyoung Kim, Bhargav Makwana, Chao Chen, Chen Feng | cs.CV | [PDF](http://arxiv.org/pdf/2312.03266v1){: .btn .btn-green }

**Abstract**: Despite the great success of Neural Radiance Fields (NeRF), its
data-gathering process remains vague with only a general rule of thumb of
sampling as densely as possible. The lack of understanding of what actually
constitutes good views for NeRF makes it difficult to actively plan a sequence
of views that yield the maximal reconstruction quality. We propose Surrogate
Objectives for Active Radiance Fields (SOAR), which is a set of interpretable
functions that evaluates the goodness of views using geometric and photometric
visual cues - surface coverage, geometric complexity, textural complexity, and
ray diversity. Moreover, by learning to infer the SOAR scores from a deep
network, SOARNet, we are able to effectively select views in mere seconds
instead of hours, without the need for prior visits to all the candidate views
or training any radiance field during such planning. Our experiments show
SOARNet outperforms the baselines with $\sim$80x speed-up while achieving
better or comparable reconstruction qualities. We finally show that SOAR is
model-agnostic, thus it generalizes across fully neural-implicit to fully
explicit approaches.

Comments:
- 13 pages

---

## Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled  Feature Fields

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-06 | Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, Achuta Kadambi | cs.CV | [PDF](http://arxiv.org/pdf/2312.03203v1){: .btn .btn-green }

**Abstract**: 3D scene representations have gained immense popularity in recent years.
Methods that use Neural Radiance fields are versatile for traditional tasks
such as novel view synthesis. In recent times, some work has emerged that aims
to extend the functionality of NeRF beyond view synthesis, for semantically
aware tasks such as editing and segmentation using 3D feature field
distillation from 2D foundation models. However, these methods have two major
limitations: (a) they are limited by the rendering speed of NeRF pipelines, and
(b) implicitly represented feature fields suffer from continuity artifacts
reducing feature quality. Recently, 3D Gaussian Splatting has shown
state-of-the-art performance on real-time radiance field rendering. In this
work, we go one step further: in addition to radiance field rendering, we
enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D
foundation model distillation. This translation is not straightforward: naively
incorporating feature fields in the 3DGS framework leads to warp-level
divergence. We propose architectural and training changes to efficiently avert
this problem. Our proposed method is general, and our experiments showcase
novel view semantic segmentation, language-guided editing and segment anything
through learning feature fields from state-of-the-art 2D foundation models such
as SAM and CLIP-LSeg. Across experiments, our distillation method is able to
provide comparable or better results, while being significantly faster to both
train and render. Additionally, to the best of our knowledge, we are the first
method to enable point and bounding-box prompting for radiance field
manipulation, by leveraging the SAM model. Project website at:
https://feature-3dgs.github.io/

---

## MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human  Captures

nerf{: .label .label-blue }

2023-12-05 | Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, Xiaoguang Han | cs.CV | [PDF](http://arxiv.org/pdf/2312.02963v1){: .btn .btn-green }

**Abstract**: In this era, the success of large language models and text-to-image models
can be attributed to the driving force of large-scale datasets. However, in the
realm of 3D vision, while remarkable progress has been made with models trained
on large-scale synthetic and real-captured object data like Objaverse and
MVImgNet, a similar level of progress has not been observed in the domain of
human-centric tasks partially due to the lack of a large-scale human dataset.
Existing datasets of high-fidelity 3D human capture continue to be mid-sized
due to the significant challenges in acquiring large-scale high-quality 3D
human data. To bridge this gap, we present MVHumanNet, a dataset that comprises
multi-view human action sequences of 4,500 human identities. The primary focus
of our work is on collecting human data that features a large number of diverse
identities and everyday clothing using a multi-view human capture system, which
facilitates easily scalable data collection. Our dataset contains 9,000 daily
outfits, 60,000 motion sequences and 645 million frames with extensive
annotations, including human masks, camera parameters, 2D and 3D keypoints,
SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the
potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot
studies on view-consistent action recognition, human NeRF reconstruction,
text-driven view-unconstrained human image generation, as well as 2D
view-unconstrained human image and 3D avatar generation. Extensive experiments
demonstrate the performance improvements and effective applications enabled by
the scale provided by MVHumanNet. As the current largest-scale 3D human
dataset, we hope that the release of MVHumanNet data with annotations will
foster further innovations in the domain of 3D human-centric tasks at scale.

Comments:
- Project page: https://x-zhangyang.github.io/MVHumanNet/

---

## FINER: Flexible spectral-bias tuning in Implicit NEural Representation  by Variable-periodic Activation Functions



2023-12-05 | Zhen Liu, Hao Zhu, Qi Zhang, Jingde Fu, Weibing Deng, Zhan Ma, Yanwen Guo, Xun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2312.02434v1){: .btn .btn-green }

**Abstract**: Implicit Neural Representation (INR), which utilizes a neural network to map
coordinate inputs to corresponding attributes, is causing a revolution in the
field of signal processing. However, current INR techniques suffer from a
restricted capability to tune their supported frequency set, resulting in
imperfect performance when representing complex signals with multiple
frequencies. We have identified that this frequency-related problem can be
greatly alleviated by introducing variable-periodic activation functions, for
which we propose FINER. By initializing the bias of the neural network within
different ranges, sub-functions with various frequencies in the
variable-periodic function are selected for activation. Consequently, the
supported frequency set of FINER can be flexibly tuned, leading to improved
performance in signal representation. We demonstrate the capabilities of FINER
in the contexts of 2D image fitting, 3D signed distance field representation,
and 5D neural radiance fields optimization, and we show that it outperforms
existing INRs.

Comments:
- 10 pages, 9 figures

---

## HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-12-05 | Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2312.02902v1){: .btn .btn-green }

**Abstract**: 3D head animation has seen major quality and runtime improvements over the
last few years, particularly empowered by the advances in differentiable
rendering and neural radiance fields. Real-time rendering is a highly desirable
goal for real-world applications. We propose HeadGaS, the first model to use 3D
Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper
we introduce a hybrid model that extends the explicit representation from 3DGS
with a base of learnable latent features, which can be linearly blended with
low-dimensional parameters from parametric head models to obtain
expression-dependent final color and opacity values. We demonstrate that
HeadGaS delivers state-of-the-art results in real-time inference frame rates,
which surpasses baselines by up to ~2dB, while accelerating rendering speed by
over x10.

---

## Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent

nerf{: .label .label-blue }

2023-12-05 | Jianmeng Liu, Yuyao Zhang, Zeyuan Meng, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2312.02568v1){: .btn .btn-green }

**Abstract**: This paper explores promptable NeRF generation (e.g., text prompt or single
image prompt) for direct conditioning and fast generation of NeRF parameters
for the underlying 3D scenes, thus undoing complex intermediate steps while
providing full 3D generation with conditional control. Unlike previous
diffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,
Prompt2NeRF-PIL is capable of generating a variety of 3D objects with a single
forward pass, leveraging a pre-trained implicit latent space of NeRF
parameters. Furthermore, in zero-shot tasks, our experiments demonstrate that
the NeRFs produced by our method serve as semantically informative
initializations, significantly accelerating the inference process of existing
prompt-to-NeRF methods. Specifically, we will show that our approach speeds up
the text-to-NeRF model DreamFusion and the 3D reconstruction speed of the
image-to-NeRF method Zero-1-to-3 by 3 to 5 times.

---

## Alchemist: Parametric Control of Material Properties with Diffusion  Models

nerf{: .label .label-blue }

2023-12-05 | Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T. Freeman, Mark Matthews | cs.CV | [PDF](http://arxiv.org/pdf/2312.02970v1){: .btn .btn-green }

**Abstract**: We propose a method to control material attributes of objects like roughness,
metallic, albedo, and transparency in real images. Our method capitalizes on
the generative prior of text-to-image models known for photorealism, employing
a scalar value and instructions to alter low-level material properties.
Addressing the lack of datasets with controlled material attributes, we
generated an object-centric synthetic dataset with physically-based materials.
Fine-tuning a modified pre-trained text-to-image model on this synthetic
dataset enables us to edit material properties in real-world images while
preserving all other attributes. We show the potential application of our model
to material edited NeRFs.

---

## HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces

nerf{: .label .label-blue }

2023-12-05 | Haithem Turki, Vasu Agrawal, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollhöfer, Christian Richardt | cs.CV | [PDF](http://arxiv.org/pdf/2312.03160v1){: .btn .btn-green }

**Abstract**: Neural radiance fields provide state-of-the-art view synthesis quality but
tend to be slow to render. One reason is that they make use of volume
rendering, thus requiring many samples (and model queries) per ray at render
time. Although this representation is flexible and easy to optimize, most
real-world objects can be modeled more efficiently with surfaces instead of
volumes, requiring far fewer samples per ray. This observation has spurred
considerable progress in surface representations such as signed distance
functions, but these may struggle to model semi-opaque and thin structures. We
propose a method, HybridNeRF, that leverages the strengths of both
representations by rendering most objects as surfaces while modeling the
(typically) small fraction of challenging regions volumetrically. We evaluate
HybridNeRF against the challenging Eyeful Tower dataset along with other
commonly used view synthesis datasets. When comparing to state-of-the-art
baselines, including recent rasterization-based approaches, we improve error
rates by 15-30% while achieving real-time framerates (at least 36 FPS) for
virtual-reality resolutions (2Kx2K).

Comments:
- Project page: https://haithemturki.com/hybrid-nerf/

---

## GauHuman: Articulated Gaussian Splatting from Monocular Human Videos

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-05 | Shoukang Hu, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.02973v1){: .btn .btn-green }

**Abstract**: We present, GauHuman, a 3D human model with Gaussian Splatting for both fast
training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with
existing NeRF-based implicit representation modelling frameworks demanding
hours of training and seconds of rendering per frame. Specifically, GauHuman
encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians
from canonical space to posed space with linear blend skinning (LBS), in which
effective pose and LBS refinement modules are designed to learn fine details of
3D humans under negligible computational cost. Moreover, to enable fast
optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human
prior, while splitting/cloning via KL divergence guidance, along with a novel
merge operation for further speeding up. Extensive experiments on ZJU_Mocap and
MonoCap datasets demonstrate that GauHuman achieves state-of-the-art
performance quantitatively and qualitatively with fast training and real-time
rendering speed. Notably, without sacrificing rendering quality, GauHuman can
fast model the 3D human performer with ~13k 3D Gaussians.

Comments:
- project page: https://skhu101.github.io/GauHuman/; code:
  https://github.com/skhu101/GauHuman

---

## C-NERF: Representing Scene Changes as Directional Consistency  Difference-based NeRF

nerf{: .label .label-blue }

2023-12-05 | Rui Huang, Binbin Jiang, Qingyi Zhao, William Wang, Yuxiang Zhang, Qing Guo | cs.CV | [PDF](http://arxiv.org/pdf/2312.02751v2){: .btn .btn-green }

**Abstract**: In this work, we aim to detect the changes caused by object variations in a
scene represented by the neural radiance fields (NeRFs). Given an arbitrary
view and two sets of scene images captured at different timestamps, we can
predict the scene changes in that view, which has significant potential
applications in scene monitoring and measuring. We conducted preliminary
studies and found that such an exciting task cannot be easily achieved by
utilizing existing NeRFs and 2D change detection methods with many false or
missing detections. The main reason is that the 2D change detection is based on
the pixel appearance difference between spatial-aligned image pairs and
neglects the stereo information in the NeRF. To address the limitations, we
propose the C-NERF to represent scene changes as directional consistency
difference-based NeRF, which mainly contains three modules. We first perform
the spatial alignment of two NeRFs captured before and after changes. Then, we
identify the change points based on the direction-consistent constraint; that
is, real change points have similar change representations across view
directions, but fake change points do not. Finally, we design the change map
rendering process based on the built NeRFs and can generate the change map of
an arbitrarily specified view direction. To validate the effectiveness, we
build a new dataset containing ten scenes covering diverse scenarios with
different changing objects. Our approach surpasses state-of-the-art 2D change
detection and NeRF-based methods by a significant margin.

---

## ReconFusion: 3D Reconstruction with Diffusion Priors

nerf{: .label .label-blue }

2023-12-05 | Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, Aleksander Holynski | cs.CV | [PDF](http://arxiv.org/pdf/2312.02981v1){: .btn .btn-green }

**Abstract**: 3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at
rendering photorealistic novel views of complex scenes. However, recovering a
high-quality NeRF typically requires tens to hundreds of input images,
resulting in a time-consuming capture process. We present ReconFusion to
reconstruct real-world scenes using only a few photos. Our approach leverages a
diffusion prior for novel view synthesis, trained on synthetic and multiview
datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel
camera poses beyond those captured by the set of input images. Our method
synthesizes realistic geometry and texture in underconstrained regions while
preserving the appearance of observed regions. We perform an extensive
evaluation across various real-world datasets, including forward-facing and
360-degree scenes, demonstrating significant performance improvements over
previous few-view NeRF reconstruction approaches.

Comments:
- Project page: https://reconfusion.github.io/

---

## Mathematical Supplement for the $\texttt{gsplat}$ Library

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-04 | Vickie Ye, Angjoo Kanazawa | cs.MS | [PDF](http://arxiv.org/pdf/2312.02121v1){: .btn .btn-green }

**Abstract**: This report provides the mathematical details of the gsplat library, a
modular toolbox for efficient differentiable Gaussian splatting, as proposed by
Kerbl et al. It provides a self-contained reference for the computations
involved in the forward and backward passes of differentiable Gaussian
splatting. To facilitate practical usage and development, we provide a user
friendly Python API that exposes each component of the forward and backward
passes in rasterization at github.com/nerfstudio-project/gsplat .

Comments:
- Find the library at: https://docs.gsplat.studio/

---

## Customize your NeRF: Adaptive Source Driven 3D Scene Editing via  Local-Global Iterative Training

nerf{: .label .label-blue }

2023-12-04 | Runze He, Shaofei Huang, Xuecheng Nie, Tianrui Hui, Luoqi Liu, Jiao Dai, Jizhong Han, Guanbin Li, Si Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.01663v1){: .btn .btn-green }

**Abstract**: In this paper, we target the adaptive source driven 3D scene editing task by
proposing a CustomNeRF model that unifies a text description or a reference
image as the editing prompt. However, obtaining desired editing results
conformed with the editing prompt is nontrivial since there exist two
significant challenges, including accurate editing of only foreground regions
and multi-view consistency given a single-view reference image. To tackle the
first challenge, we propose a Local-Global Iterative Editing (LGIE) training
scheme that alternates between foreground region editing and full-image
editing, aimed at foreground-only manipulation while preserving the background.
For the second challenge, we also design a class-guided regularization that
exploits class priors within the generation model to alleviate the
inconsistency problem among different views in image-driven editing. Extensive
experiments show that our CustomNeRF produces precise editing results under
various real scenes for both text- and image-driven settings.

Comments:
- 14 pages, 13 figures, project website: https://customnerf.github.io/

---

## Fast and accurate sparse-view CBCT reconstruction using meta-learned  neural attenuation field and hash-encoding regularization



2023-12-04 | Heejun Shin, Taehee Kim, Jongho Lee, Se Young Chun, Seungryung Cho, Dongmyung Shin | eess.IV | [PDF](http://arxiv.org/pdf/2312.01689v2){: .btn .btn-green }

**Abstract**: Cone beam computed tomography (CBCT) is an emerging medical imaging technique
to visualize the internal anatomical structures of patients. During a CBCT
scan, several projection images of different angles or views are collectively
utilized to reconstruct a tomographic image. However, reducing the number of
projections in a CBCT scan while preserving the quality of a reconstructed
image is challenging due to the nature of an ill-posed inverse problem.
Recently, a neural attenuation field (NAF) method was proposed by adopting a
neural radiance field algorithm as a new way for CBCT reconstruction,
demonstrating fast and promising results using only 50 views. However,
decreasing the number of projections is still preferable to reduce potential
radiation exposure, and a faster reconstruction time is required considering a
typical scan time. In this work, we propose a fast and accurate sparse-view
CBCT reconstruction (FACT) method to provide better reconstruction quality and
faster optimization speed in the minimal number of view acquisitions ($<$ 50
views). In the FACT method, we meta-trained a neural network and a hash-encoder
using a few scans (= 15), and a new regularization technique is utilized to
reconstruct the details of an anatomical structure. In conclusion, we have
shown that the FACT method produced better, and faster reconstruction results
over the other conventional algorithms based on CBCT scans of different body
parts (chest, head, and abdomen) and CT vendors (Siemens, Phillips, and GE).

---

## SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes

gaussian splatting{: .label .label-blue }

2023-12-04 | Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi | cs.CV | [PDF](http://arxiv.org/pdf/2312.14937v2){: .btn .btn-green }

**Abstract**: Novel view synthesis for dynamic scenes is still a challenging problem in
computer vision and graphics. Recently, Gaussian splatting has emerged as a
robust technique to represent static scenes and enable high-quality and
real-time novel view synthesis. Building upon this technique, we propose a new
representation that explicitly decomposes the motion and appearance of dynamic
scenes into sparse control points and dense Gaussians, respectively. Our key
idea is to use sparse control points, significantly fewer in number than the
Gaussians, to learn compact 6 DoF transformation bases, which can be locally
interpolated through learned interpolation weights to yield the motion field of
3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF
transformations for each control point, which reduces learning complexities,
enhances learning abilities, and facilitates obtaining temporal and spatial
coherent motion patterns. Then, we jointly learn the 3D Gaussians, the
canonical space locations of control points, and the deformation MLP to
reconstruct the appearance, geometry, and dynamics of 3D scenes. During
learning, the location and number of control points are adaptively adjusted to
accommodate varying motion complexities in different regions, and an ARAP loss
following the principle of as rigid as possible is developed to enforce spatial
continuity and local rigidity of learned motions. Finally, thanks to the
explicit sparse motion representation and its decomposition from appearance,
our method can enable user-controlled motion editing while retaining
high-fidelity appearances. Extensive experiments demonstrate that our approach
outperforms existing approaches on novel view synthesis with a high rendering
speed and enables novel appearance-preserved motion editing applications.
Project page: https://yihua7.github.io/SC-GS-web/

Comments:
- Code link: https://github.com/yihua7/SC-GS

---

## ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence  Colonoscopy Reconstruction

nerf{: .label .label-blue }

2023-12-04 | Yufei Shi, Beijia Lu, Jia-Wei Liu, Ming Li, Mike Zheng Shou | cs.CV | [PDF](http://arxiv.org/pdf/2312.02015v1){: .btn .btn-green }

**Abstract**: Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.
However, accurate long-sequence colonoscopy reconstruction faces three major
challenges: (1) dissimilarity among segments of the colon due to its meandering
and convoluted shape; (2) co-existence of simple and intricately folded
geometry structures; (3) sparse viewpoints due to constrained camera
trajectories. To tackle these challenges, we introduce a new reconstruction
framework based on neural radiance field (NeRF), named ColonNeRF, which
leverages neural rendering for novel view synthesis of long-sequence
colonoscopy. Specifically, to reconstruct the entire colon in a piecewise
manner, our ColonNeRF introduces a region division and integration module,
effectively reducing shape dissimilarity and ensuring geometric consistency in
each segment. To learn both the simple and complex geometry in a unified
framework, our ColonNeRF incorporates a multi-level fusion module that
progressively models the colon regions from easy to hard. Additionally, to
overcome the challenges from sparse views, we devise a DensiNet module for
densifying camera poses under the guidance of semantic consistency. We conduct
extensive experiments on both synthetic and real-world datasets to evaluate our
ColonNeRF. Quantitatively, our ColonNeRF outperforms existing methods on two
benchmarks over four evaluation metrics. Notably, our LPIPS-ALEX scores exhibit
a substantial increase of about 67%-85% on the SimCol-to-3D dataset.
Qualitatively, our reconstruction visualizations show much clearer textures and
more accurate geometric details. These sufficiently demonstrate our superior
performance over the state-of-the-art methods.

Comments:
- for Project Page, see https://showlab.github.io/ColonNeRF/

---

## GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians



2023-12-04 | Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2312.02069v1){: .btn .btn-green }

**Abstract**: We introduce GaussianAvatars, a new method to create photorealistic head
avatars that are fully controllable in terms of expression, pose, and
viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian
splats that are rigged to a parametric morphable face model. This combination
facilitates photorealistic rendering while allowing for precise animation
control via the underlying parametric model, e.g., through expression transfer
from a driving sequence or by manually changing the morphable model parameters.
We parameterize each splat by a local coordinate frame of a triangle and
optimize for explicit displacement offset to obtain a more accurate geometric
representation. During avatar reconstruction, we jointly optimize for the
morphable model parameters and Gaussian splat parameters in an end-to-end
fashion. We demonstrate the animation capabilities of our photorealistic avatar
in several challenging scenarios. For instance, we show reenactments from a
driving video, where our method outperforms existing works by a significant
margin.

Comments:
- Project page: https://shenhanqian.github.io/gaussian-avatars

---

## PointNeRF++: A multi-scale, point-based Neural Radiance Field

nerf{: .label .label-blue }

2023-12-04 | Weiwei Sun, Eduard Trulls, Yang-Che Tseng, Sneha Sambandam, Gopal Sharma, Andrea Tagliasacchi, Kwang Moo Yi | cs.CV | [PDF](http://arxiv.org/pdf/2312.02362v1){: .btn .btn-green }

**Abstract**: Point clouds offer an attractive source of information to complement images
in neural scene representations, especially when few images are available.
Neural rendering methods based on point clouds do exist, but they do not
perform well when the point cloud quality is low -- e.g., sparse or incomplete,
which is often the case with real-world data. We overcome these problems with a
simple representation that aggregates point clouds at multiple scale levels
with sparse voxel grids at different resolutions. To deal with point cloud
sparsity, we average across multiple scale levels -- but only among those that
are valid, i.e., that have enough neighboring points in proximity to the ray of
a pixel. To help model areas without points, we add a global voxel at the
coarsest scale, thus unifying "classical" and point-based NeRF formulations. We
validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,
outperforming the state of the art by a significant margin.

---

## Fast View Synthesis of Casual Videos



2023-12-04 | Yao-Chih Lee, Zhoutong Zhang, Kevin Blackburn-Matzen, Simon Niklaus, Jianming Zhang, Jia-Bin Huang, Feng Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.02135v1){: .btn .btn-green }

**Abstract**: Novel view synthesis from an in-the-wild video is difficult due to challenges
like scene dynamics and lack of parallax. While existing methods have shown
promising results with implicit neural radiance fields, they are slow to train
and render. This paper revisits explicit video representations to synthesize
high-quality novel views from a monocular video efficiently. We treat static
and dynamic video content separately. Specifically, we build a global static
scene model using an extended plane-based scene representation to synthesize
temporally coherent novel video. Our plane-based scene representation is
augmented with spherical harmonics and displacement maps to capture
view-dependent effects and model non-planar complex surface geometry. We opt to
represent the dynamic content as per-frame point clouds for efficiency. While
such representations are inconsistency-prone, minor temporal inconsistencies
are perceptually masked due to motion. We develop a method to quickly estimate
such a hybrid video representation and render novel views in real time. Our
experiments show that our method can render high-quality novel views from an
in-the-wild video with comparable quality to state-of-the-art methods while
being 100x faster in training and enabling real-time rendering.

Comments:
- Project page: https://casual-fvs.github.io/

---

## MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D  Gaussians

gaussian splatting{: .label .label-blue }

2023-12-04 | Chandradeep Pokhariya, Ishaan N Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar | cs.CV | [PDF](http://arxiv.org/pdf/2312.02137v1){: .btn .btn-green }

**Abstract**: Understanding how we grasp objects with our hands has important applications
in areas like robotics and mixed reality. However, this challenging problem
requires accurate modeling of the contact between hands and objects. To capture
grasps, existing methods use skeletons, meshes, or parametric models that can
cause misalignments resulting in inaccurate contacts. We present MANUS, a
method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians.
We build a novel articulated 3D Gaussians representation that extends 3D
Gaussian splatting for high-fidelity representation of articulating hands.
Since our representation uses Gaussian primitives, it enables us to efficiently
and accurately estimate contacts between the hand and the object. For the most
accurate results, our method requires tens of camera views that current
datasets do not provide. We therefore build MANUS-Grasps, a new dataset that
contains hand-object grasps viewed from 53 cameras across 30+ scenes, 3
subjects, and comprising over 7M frames. In addition to extensive qualitative
results, we also show that our method outperforms others on a quantitative
contact evaluation method that uses paint transfer from the object to the hand.

---

## GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for  Real-time Human Novel View Synthesis

gaussian splatting{: .label .label-blue }

2023-12-04 | Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2312.02155v1){: .btn .btn-green }

**Abstract**: We present a new approach, termed GPS-Gaussian, for synthesizing novel views
of a character in a real-time manner. The proposed method enables 2K-resolution
rendering under a sparse-view camera setting. Unlike the original Gaussian
Splatting or neural implicit rendering methods that necessitate per-subject
optimizations, we introduce Gaussian parameter maps defined on the source views
and regress directly Gaussian Splatting properties for instant novel view
synthesis without any fine-tuning or optimization. To this end, we train our
Gaussian parameter regression module on a large amount of human scan data,
jointly with a depth estimation module to lift 2D parameter maps to 3D space.
The proposed framework is fully differentiable and experiments on several
datasets demonstrate that our method outperforms state-of-the-art methods while
achieving an exceeding rendering speed.

Comments:
- The link to our projectpage is https://shunyuanzheng.github.io

---

## Mesh-Guided Neural Implicit Field Editing



2023-12-04 | Can Wang, Mingming He, Menglei Chai, Dongdong Chen, Jing Liao | cs.CV | [PDF](http://arxiv.org/pdf/2312.02157v1){: .btn .btn-green }

**Abstract**: Neural implicit fields have emerged as a powerful 3D representation for
reconstructing and rendering photo-realistic views, yet they possess limited
editability. Conversely, explicit 3D representations, such as polygonal meshes,
offer ease of editing but may not be as suitable for rendering high-quality
novel views. To harness the strengths of both representations, we propose a new
approach that employs a mesh as a guiding mechanism in editing the neural
radiance field. We first introduce a differentiable method using marching
tetrahedra for polygonal mesh extraction from the neural implicit field and
then design a differentiable color extractor to assign colors obtained from the
volume renderings to this extracted mesh. This differentiable colored mesh
allows gradient back-propagation from the explicit mesh to the implicit fields,
empowering users to easily manipulate the geometry and color of neural implicit
fields. To enhance user control from coarse-grained to fine-grained levels, we
introduce an octree-based structure into its optimization. This structure
prioritizes the edited regions and the surface part, making our method achieve
fine-grained edits to the neural implicit field and accommodate various user
modifications, including object additions, component removals, specific area
deformations, and adjustments to local and global colors. Through extensive
experiments involving diverse scenes and editing operations, we have
demonstrated the capabilities and effectiveness of our method. Our project page
is: \url{https://cassiepython.github.io/MNeuEdit/}

Comments:
- Project page: https://cassiepython.github.io/MNeuEdit/

---

## Calibrated Uncertainties for Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-04 | Niki Amini-Naieni, Tomas Jakab, Andrea Vedaldi, Ronald Clark | cs.CV | [PDF](http://arxiv.org/pdf/2312.02350v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields have achieved remarkable results for novel view
synthesis but still lack a crucial component: precise measurement of
uncertainty in their predictions. Probabilistic NeRF methods have tried to
address this, but their output probabilities are not typically accurately
calibrated, and therefore do not capture the true confidence levels of the
model. Calibration is a particularly challenging problem in the sparse-view
setting, where additional held-out data is unavailable for fitting a calibrator
that generalizes to the test distribution. In this paper, we introduce the
first method for obtaining calibrated uncertainties from NeRF models. Our
method is based on a robust and efficient metric to calculate per-pixel
uncertainties from the predictive posterior distribution. We propose two
techniques that eliminate the need for held-out data. The first, based on patch
sampling, involves training two NeRF models for each scene. The second is a
novel meta-calibrator that only requires the training of one NeRF model. Our
proposed approach for obtaining calibrated uncertainties achieves
state-of-the-art uncertainty in the sparse-view setting while maintaining image
quality. We further demonstrate our method's effectiveness in applications such
as view enhancement and next-best view selection.

---

## Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields  through Novel Views Synthesis

nerf{: .label .label-blue }

2023-12-04 | Felix Tristram, Stefano Gasperini, Federico Tombari, Nassir Navab, Benjamin Busam | cs.CV | [PDF](http://arxiv.org/pdf/2312.02255v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis
capabilities even in large-scale, unbounded scenes, albeit requiring hundreds
of views or introducing artifacts in sparser settings. Their optimization
suffers from shape-radiance ambiguities wherever only a small visual overlap is
available. This leads to erroneous scene geometry and artifacts. In this paper,
we propose Re-Nerfing, a simple and general multi-stage approach that leverages
NeRF's own view synthesis to address these limitations. With Re-Nerfing, we
increase the scene's coverage and enhance the geometric consistency of novel
views as follows: First, we train a NeRF with the available views. Then, we use
the optimized NeRF to synthesize pseudo-views next to the original ones to
simulate a stereo or trifocal setup. Finally, we train a second NeRF with both
original and pseudo views while enforcing structural, epipolar constraints via
the newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset
show the effectiveness of Re-Nerfing across denser and sparser input scenarios,
bringing improvements to the state-of-the-art Zip-NeRF, even when trained with
all views.

Comments:
- Code will be released upon acceptance

---

## SANeRF-HQ: Segment Anything for NeRF in High Quality

nerf{: .label .label-blue }

2023-12-03 | Yichen Liu, Benran Hu, Chi-Keung Tang, Yu-Wing Tai | cs.CV | [PDF](http://arxiv.org/pdf/2312.01531v1){: .btn .btn-green }

**Abstract**: Recently, the Segment Anything Model (SAM) has showcased remarkable
capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has
gained popularity as a method for various 3D problems beyond novel view
synthesis. Though there exist initial attempts to incorporate these two methods
into 3D segmentation, they face the challenge of accurately and consistently
segmenting objects in complex scenarios. In this paper, we introduce the
Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality
3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for
open-world object segmentation guided by user-supplied prompts, while
leveraging NeRF to aggregate information from different viewpoints. To overcome
the aforementioned challenges, we employ density field and RGB similarity to
enhance the accuracy of segmentation boundary during the aggregation.
Emphasizing on segmentation accuracy, we evaluate our method quantitatively on
multiple NeRF datasets where high-quality ground-truths are available or
manually annotated. SANeRF-HQ shows a significant quality improvement over
previous state-of-the-art methods in NeRF object segmentation, provides higher
flexibility for object localization, and enables more consistent object
segmentation across multiple views. Additional information can be found at
https://lyclyc52.github.io/SANeRF-HQ/.

---

## WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance  Fields

nerf{: .label .label-blue }

2023-12-03 | Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull | cs.CV | [PDF](http://arxiv.org/pdf/2312.02218v1){: .btn .btn-green }

**Abstract**: Dynamic Neural Radiance Fields (Dynamic NeRF) enhance NeRF technology to
model moving scenes. However, they are resource intensive and challenging to
compress. To address this issue, this paper presents WavePlanes, a fast and
more compact explicit model. We propose a multi-scale space and space-time
feature plane representation using N-level 2-D wavelet coefficients. The
inverse discrete wavelet transform reconstructs N feature signals at varying
detail, which are linearly decoded to approximate the color and density of
volumes in a 4-D grid. Exploiting the sparsity of wavelet coefficients, we
compress a Hash Map containing only non-zero coefficients and their locations
on each plane. This results in a compressed model size of ~12 MB. Compared with
state-of-the-art plane-based models, WavePlanes is up to 15x smaller, less
computationally demanding and achieves comparable results in as little as one
hour of training - without requiring custom CUDA code or high performance
computing resources. Additionally, we propose new feature fusion schemes that
work as well as previously proposed schemes while providing greater
interpretability. Our code is available at:
https://github.com/azzarelli/waveplanes/

---

## VideoRF: Rendering Dynamic Radiance Fields as 2D Feature Video Streams

nerf{: .label .label-blue }

2023-12-03 | Liao Wang, Kaixin Yao, Chengcheng Guo, Zhirui Zhang, Qiang Hu, Jingyi Yu, Lan Xu, Minye Wu | cs.CV | [PDF](http://arxiv.org/pdf/2312.01407v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) excel in photorealistically rendering static
scenes. However, rendering dynamic, long-duration radiance fields on ubiquitous
devices remains challenging, due to data storage and computational constraints.
In this paper, we introduce VideoRF, the first approach to enable real-time
streaming and rendering of dynamic radiance fields on mobile platforms. At the
core is a serialized 2D feature image stream representing the 4D radiance field
all in one. We introduce a tailored training scheme directly applied to this 2D
domain to impose the temporal and spatial redundancy of the feature image
stream. By leveraging the redundancy, we show that the feature image stream can
be efficiently compressed by 2D video codecs, which allows us to exploit video
hardware accelerators to achieve real-time decoding. On the other hand, based
on the feature image stream, we propose a novel rendering pipeline for VideoRF,
which has specialized space mappings to query radiance properties efficiently.
Paired with a deferred shading model, VideoRF has the capability of real-time
rendering on mobile devices thanks to its efficiency. We have developed a
real-time interactive player that enables online streaming and rendering of
dynamic scenes, offering a seamless and immersive free-viewpoint experience
across a range of devices, from desktops to mobile phones.

Comments:
- Project page, see https://aoliao12138.github.io/VideoRF

---

## Self-Evolving Neural Radiance Fields

nerf{: .label .label-blue }

2023-12-02 | Jaewoo Jung, Jisang Han, Jiwon Kang, Seongchan Kim, Min-Seop Kwak, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2312.01003v2){: .btn .btn-green }

**Abstract**: Recently, neural radiance field (NeRF) has shown remarkable performance in
novel view synthesis and 3D reconstruction. However, it still requires abundant
high-quality images, limiting its applicability in real-world scenarios. To
overcome this limitation, recent works have focused on training NeRF only with
sparse viewpoints by giving additional regularizations, often called few-shot
NeRF. We observe that due to the under-constrained nature of the task, solely
using additional regularization is not enough to prevent the model from
overfitting to sparse viewpoints. In this paper, we propose a novel framework,
dubbed Self-Evolving Neural Radiance Fields (SE-NeRF), that applies a
self-training framework to NeRF to address these problems. We formulate
few-shot NeRF into a teacher-student framework to guide the network to learn a
more robust representation of the scene by training the student with additional
pseudo labels generated from the teacher. By distilling ray-level pseudo labels
using distinct distillation schemes for reliable and unreliable rays obtained
with our novel reliability estimation method, we enable NeRF to learn a more
accurate and robust geometry of the 3D scene. We show and evaluate that
applying our self-training framework to existing models improves the quality of
the rendered images and achieves state-of-the-art performance in multiple
settings.

Comments:
- 34 pages, 21 figures Our project page can be found at :
  https://ku-cvlab.github.io/SE-NeRF/

---

## StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D

nerf{: .label .label-blue }

2023-12-02 | Pengsheng Guo, Hans Hao, Adam Caccavale, Zhongzheng Ren, Edward Zhang, Qi Shan, Aditya Sankar, Alexander G. Schwing, Alex Colburn, Fangchang Ma | cs.CV | [PDF](http://arxiv.org/pdf/2312.02189v1){: .btn .btn-green }

**Abstract**: In the realm of text-to-3D generation, utilizing 2D diffusion models through
score distillation sampling (SDS) frequently leads to issues such as blurred
appearances and multi-faced geometry, primarily due to the intrinsically noisy
nature of the SDS loss. Our analysis identifies the core of these challenges as
the interaction among noise levels in the 2D diffusion process, the
architecture of the diffusion network, and the 3D model representation. To
overcome these limitations, we present StableDreamer, a methodology
incorporating three advances. First, inspired by InstructNeRF2NeRF, we
formalize the equivalence of the SDS generative prior and a simple supervised
L2 reconstruction loss. This finding provides a novel tool to debug SDS, which
we use to show the impact of time-annealing noise levels on reducing
multi-faced geometries. Second, our analysis shows that while image-space
diffusion contributes to geometric precision, latent-space diffusion is crucial
for vivid color rendition. Based on this observation, StableDreamer introduces
a two-stage training strategy that effectively combines these aspects,
resulting in high-fidelity 3D models. Third, we adopt an anisotropic 3D
Gaussians representation, replacing Neural Radiance Fields (NeRFs), to enhance
the overall quality, reduce memory usage during training, and accelerate
rendering speeds, and better capture semi-transparent objects. StableDreamer
reduces multi-face geometries, generates fine details, and converges stably.

---

## Volumetric Rendering with Baked Quadrature Fields

nerf{: .label .label-blue }

2023-12-02 | Gopal Sharma, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi | cs.GR | [PDF](http://arxiv.org/pdf/2312.02202v1){: .btn .btn-green }

**Abstract**: We propose a novel Neural Radiance Field (NeRF) representation for non-opaque
scenes that allows fast inference by utilizing textured polygons. Despite the
high-quality novel view rendering that NeRF provides, a critical limitation is
that it relies on volume rendering that can be computationally expensive and
does not utilize the advancements in modern graphics hardware. Existing methods
for this problem fall short when it comes to modelling volumetric effects as
they rely purely on surface rendering. We thus propose to model the scene with
polygons, which can then be used to obtain the quadrature points required to
model volumetric effects, and also their opacity and colour from the texture.
To obtain such polygonal mesh, we train a specialized field whose
zero-crossings would correspond to the quadrature points when volume rendering,
and perform marching cubes on this field. We then rasterize the polygons and
utilize the fragment shaders to obtain the final colour image. Our method
allows rendering on various devices and easy integration with existing graphics
frameworks while keeping the benefits of volume rendering alive.

---

## DISTWAR: Fast Differentiable Rendering on Raster-based Rendering  Pipelines

gaussian splatting{: .label .label-blue }

2023-12-01 | Sankeerth Durvasula, Adrian Zhao, Fan Chen, Ruofan Liang, Pawan Kumar Sanjaya, Nandita Vijaykumar | cs.CV | [PDF](http://arxiv.org/pdf/2401.05345v1){: .btn .btn-green }

**Abstract**: Differentiable rendering is a technique used in an important emerging class
of visual computing applications that involves representing a 3D scene as a
model that is trained from 2D images using gradient descent. Recent works (e.g.
3D Gaussian Splatting) use a rasterization pipeline to enable rendering high
quality photo-realistic imagery at high speeds from these learned 3D models.
These methods have been demonstrated to be very promising, providing
state-of-art quality for many important tasks. However, training a model to
represent a scene is still a time-consuming task even when using powerful GPUs.
In this work, we observe that the gradient computation phase during training is
a significant bottleneck on GPUs due to the large number of atomic operations
that need to be processed. These atomic operations overwhelm atomic units in
the L2 partitions causing stalls. To address this challenge, we leverage the
observations that during the gradient computation: (1) for most warps, all
threads atomically update the same memory locations; and (2) warps generate
varying amounts of atomic traffic (since some threads may be inactive). We
propose DISTWAR, a software-approach to accelerate atomic operations based on
two key ideas: First, we enable warp-level reduction of threads at the SM
sub-cores using registers to leverage the locality in intra-warp atomic
updates. Second, we distribute the atomic computation between the warp-level
reduction at the SM and the L2 atomic units to increase the throughput of
atomic computation. Warps with many threads performing atomic updates to the
same memory locations are scheduled at the SM, and the rest using L2 atomic
units. We implement DISTWAR using existing warp-level primitives. We evaluate
DISTWAR on widely used raster-based differentiable rendering workloads. We
demonstrate significant speedups of 2.44x on average (up to 5.7x).

---

## Segment Any 3D Gaussians

gaussian splatting{: .label .label-blue }

2023-12-01 | Jiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian | cs.CV | [PDF](http://arxiv.org/pdf/2312.00860v1){: .btn .btn-green }

**Abstract**: Interactive 3D segmentation in radiance fields is an appealing task since its
importance in 3D scene understanding and manipulation. However, existing
methods face challenges in either achieving fine-grained, multi-granularity
segmentation or contending with substantial computational overhead, inhibiting
real-time interaction. In this paper, we introduce Segment Any 3D GAussians
(SAGA), a novel 3D interactive segmentation approach that seamlessly blends a
2D segmentation foundation model with 3D Gaussian Splatting (3DGS), a recent
breakthrough of radiance fields. SAGA efficiently embeds multi-granularity 2D
segmentation results generated by the segmentation foundation model into 3D
Gaussian point features through well-designed contrastive training. Evaluation
on existing benchmarks demonstrates that SAGA can achieve competitive
performance with state-of-the-art methods. Moreover, SAGA achieves
multi-granularity segmentation and accommodates various prompts, including
points, scribbles, and 2D masks. Notably, SAGA can finish the 3D segmentation
within milliseconds, achieving nearly 1000x acceleration compared to previous
SOTA. The project page is at https://jumpat.github.io/SAGA.

Comments:
- Work in progress. Project page: https://jumpat.github.io/SAGA

---

## Gaussian Grouping: Segment and Edit Anything in 3D Scenes

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-01 | Mingqiao Ye, Martin Danelljan, Fisher Yu, Lei Ke | cs.CV | [PDF](http://arxiv.org/pdf/2312.00732v1){: .btn .btn-green }

**Abstract**: The recent Gaussian Splatting achieves high-quality and real-time novel-view
synthesis of the 3D scenes. However, it is solely concentrated on the
appearance and geometry modeling, while lacking in fine-grained object-level
scene understanding. To address this issue, we propose Gaussian Grouping, which
extends Gaussian Splatting to jointly reconstruct and segment anything in
open-world 3D scenes. We augment each Gaussian with a compact Identity
Encoding, allowing the Gaussians to be grouped according to their object
instance or stuff membership in the 3D scene. Instead of resorting to expensive
3D labels, we supervise the Identity Encodings during the differentiable
rendering by leveraging the 2D mask predictions by SAM, along with introduced
3D spatial consistency regularization. Comparing to the implicit NeRF
representation, we show that the discrete and grouped 3D Gaussians can
reconstruct, segment and edit anything in 3D with high visual quality, fine
granularity and efficiency. Based on Gaussian Grouping, we further propose a
local Gaussian Editing scheme, which shows efficacy in versatile scene editing
applications, including 3D object removal, inpainting, colorization and scene
recomposition. Our code and models will be at
https://github.com/lkeab/gaussian-grouping.

Comments:
- We propose Gaussian Grouping, which extends Gaussian Splatting to
  fine-grained open-world 3D scene understanding. Github:
  https://github.com/lkeab/gaussian-grouping

---

## FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-12-01 | Zehao Zhu, Zhiwen Fan, Yifan Jiang, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2312.00451v1){: .btn .btn-green }

**Abstract**: Novel view synthesis from limited observations remains an important and
persistent task. However, high efficiency in existing NeRF-based few-shot view
synthesis is often compromised to obtain an accurate 3D representation. To
address this challenge, we propose a few-shot view synthesis framework based on
3D Gaussian Splatting that enables real-time and photo-realistic view synthesis
with as few as three training views. The proposed method, dubbed FSGS, handles
the extremely sparse initialized SfM points with a thoughtfully designed
Gaussian Unpooling process. Our method iteratively distributes new Gaussians
around the most representative locations, subsequently infilling local details
in vacant areas. We also integrate a large-scale pre-trained monocular depth
estimator within the Gaussians optimization process, leveraging online
augmented views to guide the geometric optimization towards an optimal
solution. Starting from sparse points observed from limited input viewpoints,
our FSGS can accurately grow into unseen regions, comprehensively covering the
scene and boosting the rendering quality of novel views. Overall, FSGS achieves
state-of-the-art performance in both accuracy and rendering efficiency across
diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website:
https://zehaozhu.github.io/FSGS/.

Comments:
- Project page: https://zehaozhu.github.io/FSGS/

---

## NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting  Guidance

gaussian splatting{: .label .label-blue }

2023-12-01 | Hanlin Chen, Chen Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2312.00846v1){: .btn .btn-green }

**Abstract**: Existing neural implicit surface reconstruction methods have achieved
impressive performance in multi-view 3D reconstruction by leveraging explicit
geometry priors such as depth maps or point clouds as regularization. However,
the reconstruction results still lack fine details because of the over-smoothed
depth map or sparse point cloud. In this work, we propose a neural implicit
surface reconstruction pipeline with guidance from 3D Gaussian Splatting to
recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is
that it can generate dense point clouds with detailed structure. Nonetheless, a
naive adoption of 3D Gaussian Splatting can fail since the generated points are
the centers of 3D Gaussians that do not necessarily lie on the surface. We thus
introduce a scale regularizer to pull the centers close to the surface by
enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine
the point cloud from 3D Gaussians Splatting with the normal priors from the
surface predicted by neural implicit models instead of using a fixed set of
points as guidance. Consequently, the quality of surface reconstruction
improves from the guidance of the more accurate 3D Gaussian splatting. By
jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our
approach benefits from both representations and generates complete surfaces
with intricate details. Experiments on Tanks and Temples verify the
effectiveness of our proposed method.

---

## Contrastive Denoising Score for Text-guided Latent Diffusion Image  Editing

nerf{: .label .label-blue }

2023-11-30 | Hyelin Nam, Gihyun Kwon, Geon Yeong Park, Jong Chul Ye | cs.CV | [PDF](http://arxiv.org/pdf/2311.18608v1){: .btn .btn-green }

**Abstract**: With the remarkable advent of text-to-image diffusion models, image editing
methods have become more diverse and continue to evolve. A promising recent
approach in this realm is Delta Denoising Score (DDS) - an image editing
technique based on Score Distillation Sampling (SDS) framework that leverages
the rich generative prior of text-to-image diffusion models. However, relying
solely on the difference between scoring functions is insufficient for
preserving specific structural elements from the original image, a crucial
aspect of image editing. Inspired by the similarity and importance differences
between DDS and the contrastive learning for unpaired image-to-image
translation (CUT), here we present an embarrassingly simple yet very powerful
modification of DDS, called Contrastive Denoising Score (CDS), for latent
diffusion models (LDM). Specifically, to enforce structural correspondence
between the input and output while maintaining the controllability of contents,
we introduce a straightforward approach to regulate structural consistency
using CUT loss within the DDS framework. To calculate this loss, instead of
employing auxiliary networks, we utilize the intermediate features of LDM, in
particular, those from the self-attention layers, which possesses rich spatial
information. Our approach enables zero-shot image-to-image translation and
neural radiance field (NeRF) editing, achieving a well-balanced interplay
between maintaining the structural details and transforming content.
Qualitative results and comparisons demonstrates the effectiveness of our
proposed method. Project page with code is available at
https://hyelinnam.github.io/CDS/.

Comments:
- Project page: https://hyelinnam.github.io/CDS/

---

## Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector  Quantization

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-30 | KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, Hamed Pirsiavash | cs.CV | [PDF](http://arxiv.org/pdf/2311.18159v1){: .btn .btn-green }

**Abstract**: 3D Gaussian Splatting is a new method for modeling and rendering 3D radiance
fields that achieves much faster learning and rendering time compared to SOTA
NeRF methods. However, it comes with a drawback in the much larger storage
demand compared to NeRF methods since it needs to store the parameters for
several 3D Gaussians. We notice that many Gaussians may share similar
parameters, so we introduce a simple vector quantization method based on
\kmeans algorithm to quantize the Gaussian parameters. Then, we store the small
codebook along with the index of the code for each Gaussian. Moreover, we
compress the indices further by sorting them and using a method similar to
run-length encoding. We do extensive experiments on standard benchmarks as well
as a new benchmark which is an order of magnitude larger than the standard
benchmarks. We show that our simple yet effective method can reduce the storage
cost for the original 3D Gaussian Splatting method by a factor of almost
$20\times$ with a very small drop in the quality of rendered images.

Comments:
- Code is available at https://github.com/UCDvision/compact3d

---

## CosAvatar: Consistent and Animatable Portrait Video Tuning with Text  Prompt

nerf{: .label .label-blue }

2023-11-30 | Haiyao Xiao, Chenglai Zhong, Xuan Gao, Yudong Guo, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2311.18288v1){: .btn .btn-green }

**Abstract**: Recently, text-guided digital portrait editing has attracted more and more
attentions. However, existing methods still struggle to maintain consistency
across time, expression, and view or require specific data prerequisites. To
solve these challenging problems, we propose CosAvatar, a high-quality and
user-friendly framework for portrait tuning. With only monocular video and text
instructions as input, we can produce animatable portraits with both temporal
and 3D consistency. Different from methods that directly edit in the 2D domain,
we employ a dynamic NeRF-based 3D portrait representation to model both the
head and torso. We alternate between editing the video frames' dataset and
updating the underlying 3D portrait until the edited frames reach 3D
consistency. Additionally, we integrate the semantic portrait priors to enhance
the edited results, allowing precise modifications in specified semantic areas.
Extensive results demonstrate that our proposed method can not only accurately
edit portrait styles or local attributes based on text instructions but also
support expressive animation driven by a source video.

Comments:
- Project page: https://ustc3dv.github.io/CosAvatar/

---

## ZeST-NeRF: Using temporal aggregation for Zero-Shot Temporal NeRFs

nerf{: .label .label-blue }

2023-11-30 | Violeta Menéndez González, Andrew Gilbert, Graeme Phillipson, Stephen Jolly, Simon Hadfield | cs.CV | [PDF](http://arxiv.org/pdf/2311.18491v1){: .btn .btn-green }

**Abstract**: In the field of media production, video editing techniques play a pivotal
role. Recent approaches have had great success at performing novel view image
synthesis of static scenes. But adding temporal information adds an extra layer
of complexity. Previous models have focused on implicitly representing static
and dynamic scenes using NeRF. These models achieve impressive results but are
costly at training and inference time. They overfit an MLP to describe the
scene implicitly as a function of position. This paper proposes ZeST-NeRF, a
new approach that can produce temporal NeRFs for new scenes without retraining.
We can accurately reconstruct novel views using multi-view synthesis techniques
and scene flow-field estimation, trained only with unrelated scenes. We
demonstrate how existing state-of-the-art approaches from a range of fields
cannot adequately solve this new task and demonstrate the efficacy of our
solution. The resulting network improves quantitatively by 15% and produces
significantly better visual results.

Comments:
- VUA BMVC 2023

---

## Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and  Real-time Rendering

gaussian splatting{: .label .label-blue }

2023-11-30 | Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2311.18561v1){: .btn .btn-green }

**Abstract**: Modeling dynamic, large-scale urban scenes is challenging due to their highly
intricate geometric structures and unconstrained dynamics in both space and
time. Prior methods often employ high-level architectural priors, separating
static and dynamic elements, resulting in suboptimal capture of their
synergistic interactions. To address this challenge, we present a unified
representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon
the efficient 3D Gaussian splatting technique, originally designed for static
scene representation, by introducing periodic vibration-based temporal
dynamics. This innovation enables PVG to elegantly and uniformly represent the
characteristics of various objects and elements in dynamic urban scenes. To
enhance temporally coherent representation learning with sparse training data,
we introduce a novel flow-based temporal smoothing mechanism and a
position-aware adaptive control strategy. Extensive experiments on Waymo Open
Dataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art
alternatives in both reconstruction and novel view synthesis for both dynamic
and static scenes. Notably, PVG achieves this without relying on manually
labeled object bounding boxes or expensive optical flow estimation. Moreover,
PVG exhibits 50/6000-fold acceleration in training/rendering over the best
alternative.

Comments:
- Project page: https://fudan-zvg.github.io/PVG/

---

## Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and  Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-30 | Hartmut Surmann, Niklas Digakis, Jan-Nicklas Kremer, Julien Meine, Max Schulte, Niklas Voigt | cs.CV | [PDF](http://arxiv.org/pdf/2401.06143v1){: .btn .btn-green }

**Abstract**: In the realm of digital situational awareness during disaster situations,
accurate digital representations, like 3D models, play an indispensable role.
To ensure the safety of rescue teams, robotic platforms are often deployed to
generate these models. In this paper, we introduce an innovative approach that
synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller
than 30 cm, equipped with 360 degree cameras and the advances of Neural
Radiance Fields (NeRFs). A NeRF, a specialized neural network, can deduce a 3D
representation of any scene using 2D images and then synthesize it from various
angles upon request. This method is especially tailored for urban environments
which have experienced significant destruction, where the structural integrity
of buildings is compromised to the point of barring entry-commonly observed
post-earthquakes and after severe fires. We have tested our approach through
recent post-fire scenario, underlining the efficacy of NeRFs even in
challenging outdoor environments characterized by water, snow, varying light
conditions, and reflective surfaces.

Comments:
- 6 pages, published at IEEE International Symposium on
  Safety,Security,and Rescue Robotics SSRR2023 in FUKUSHIMA, November 13-15
  2023

---

## Anisotropic Neural Representation Learning for High-Quality Neural  Rendering

nerf{: .label .label-blue }

2023-11-30 | Y. Wang, J. Xu, Y. Zeng, Y. Gong | cs.CV | [PDF](http://arxiv.org/pdf/2311.18311v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) have achieved impressive view synthesis
results by learning an implicit volumetric representation from multi-view
images. To project the implicit representation into an image, NeRF employs
volume rendering that approximates the continuous integrals of rays as an
accumulation of the colors and densities of the sampled points. Although this
approximation enables efficient rendering, it ignores the direction information
in point intervals, resulting in ambiguous features and limited reconstruction
quality. In this paper, we propose an anisotropic neural representation
learning method that utilizes learnable view-dependent features to improve
scene representation and reconstruction. We model the volumetric function as
spherical harmonic (SH)-guided anisotropic features, parameterized by
multilayer perceptrons, facilitating ambiguity elimination while preserving the
rendering efficiency. To achieve robust scene reconstruction without anisotropy
overfitting, we regularize the energy of the anisotropic features during
training. Our method is flexiable and can be plugged into NeRF-based
frameworks. Extensive experiments show that the proposed representation can
boost the rendering quality of various NeRFs and achieve state-of-the-art
rendering performance on both synthetic and real-world scenes.

---

## Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering

gaussian splatting{: .label .label-blue }

2023-11-30 | Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2312.00109v1){: .btn .btn-green }

**Abstract**: Neural rendering methods have significantly advanced photo-realistic 3D scene
rendering in various academic and industrial applications. The recent 3D
Gaussian Splatting method has achieved the state-of-the-art rendering quality
and speed combining the benefits of both primitive-based representations and
volumetric representations. However, it often leads to heavily redundant
Gaussians that try to fit every training view, neglecting the underlying scene
geometry. Consequently, the resulting model becomes less robust to significant
view changes, texture-less area and lighting effects. We introduce Scaffold-GS,
which uses anchor points to distribute local 3D Gaussians, and predicts their
attributes on-the-fly based on viewing direction and distance within the view
frustum. Anchor growing and pruning strategies are developed based on the
importance of neural Gaussians to reliably improve the scene coverage. We show
that our method effectively reduces redundant Gaussians while delivering
high-quality rendering. We also demonstrates an enhanced capability to
accommodate scenes with varying levels-of-detail and view-dependent
observations, without sacrificing the rendering speed.

Comments:
- Project page: https://city-super.github.io/scaffold-gs/

---

## LucidDreaming: Controllable Object-Centric 3D Generation

nerf{: .label .label-blue }

2023-11-30 | Zhaoning Wang, Ming Li, Chen Chen | cs.CV | [PDF](http://arxiv.org/pdf/2312.00588v1){: .btn .btn-green }

**Abstract**: With the recent development of generative models, Text-to-3D generations have
also seen significant growth. Nonetheless, achieving precise control over 3D
generation continues to be an arduous task, as using text to control often
leads to missing objects and imprecise locations. Contemporary strategies for
enhancing controllability in 3D generation often entail the introduction of
additional parameters, such as customized diffusion models. This often induces
hardness in adapting to different diffusion models or creating distinct
objects.
  In this paper, we present LucidDreaming as an effective pipeline capable of
fine-grained control over 3D generation. It requires only minimal input of 3D
bounding boxes, which can be deduced from a simple text prompt using a Large
Language Model. Specifically, we propose clipped ray sampling to separately
render and optimize objects with user specifications. We also introduce
object-centric density blob bias, fostering the separation of generated
objects. With individual rendering and optimizing of objects, our method excels
not only in controlled content generation from scratch but also within the
pre-trained NeRF scenes. In such scenarios, existing generative approaches
often disrupt the integrity of the original scene, and current editing methods
struggle to synthesize new content in empty spaces. We show that our method
exhibits remarkable adaptability across a spectrum of mainstream Score
Distillation Sampling-based 3D generation frameworks, and achieves superior
alignment of 3D content when compared to baseline approaches. We also provide a
dataset of prompts with 3D bounding boxes, benchmarking 3D spatial
controllability.

---

## MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly  Deformable Scenes

gaussian splatting{: .label .label-blue }

2023-11-30 | Bardienus P. Duisterhof, Zhao Mandi, Yunchao Yao, Jia-Wei Liu, Mike Zheng Shou, Shuran Song, Jeffrey Ichnowski | cs.CV | [PDF](http://arxiv.org/pdf/2312.00583v1){: .btn .btn-green }

**Abstract**: Accurate 3D tracking in highly deformable scenes with occlusions and shadows
can facilitate new applications in robotics, augmented reality, and generative
AI. However, tracking under these conditions is extremely challenging due to
the ambiguity that arises with large deformations, shadows, and occlusions. We
introduce MD-Splatting, an approach for simultaneous 3D tracking and novel view
synthesis, using video captures of a dynamic scene from various camera poses.
MD-Splatting builds on recent advances in Gaussian splatting, a method that
learns the properties of a large number of Gaussians for state-of-the-art and
fast novel view synthesis. MD-Splatting learns a deformation function to
project a set of Gaussians with non-metric, thus canonical, properties into
metric space. The deformation function uses a neural-voxel encoding and a
multilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow
scalar. We enforce physics-inspired regularization terms based on local
rigidity, conservation of momentum, and isometry, which leads to trajectories
with smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking
on highly deformable scenes with shadows and occlusions. Compared to
state-of-the-art, we improve 3D tracking by an average of 23.9 %, while
simultaneously achieving high-quality novel view synthesis. With sufficient
texture such as in scene 6, MD-Splatting achieves a median tracking error of
3.39 mm on a cloth of 1 x 1 meters in size. Project website:
https://md-splatting.github.io/.

---

## DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis  with 3D Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-11-30 | Agelos Kratimenos, Jiahui Lei, Kostas Daniilidis | cs.CV | [PDF](http://arxiv.org/pdf/2312.00112v1){: .btn .btn-green }

**Abstract**: Accurately and efficiently modeling dynamic scenes and motions is considered
so challenging a task due to temporal dynamics and motion complexity. To
address these challenges, we propose DynMF, a compact and efficient
representation that decomposes a dynamic scene into a few neural trajectories.
We argue that the per-point motions of a dynamic scene can be decomposed into a
small set of explicit or learned trajectories. Our carefully designed neural
framework consisting of a tiny set of learned basis queried only in time allows
for rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, while
at the same time, requiring only double the storage compared to static scenes.
Our neural representation adequately constrains the inherently underconstrained
motion field of a dynamic scene leading to effective and fast optimization.
This is done by biding each point to motion coefficients that enforce the
per-point sharing of basis trajectories. By carefully applying a sparsity loss
to the motion coefficients, we are able to disentangle the motions that
comprise the scene, independently control them, and generate novel motion
combinations that have never been seen before. We can reach state-of-the-art
render quality within just 5 minutes of training and in less than half an hour,
we can synthesize novel views of dynamic scenes with superior photorealistic
quality. Our representation is interpretable, efficient, and expressive enough
to offer real-time view synthesis of complex dynamic scene motions, in
monocular and multi-view scenarios.

Comments:
- Project page: https://agelosk.github.io/dynmf/

---

## SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian  Splatting

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-30 | Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, Achuta Kadambi | cs.CV | [PDF](http://arxiv.org/pdf/2312.00206v1){: .btn .btn-green }

**Abstract**: The problem of novel view synthesis has grown significantly in popularity
recently with the introduction of Neural Radiance Fields (NeRFs) and other
implicit scene representation methods. A recent advance, 3D Gaussian Splatting
(3DGS), leverages an explicit representation to achieve real-time rendering
with high-quality results. However, 3DGS still requires an abundance of
training views to generate a coherent scene representation. In few shot
settings, similar to NeRF, 3DGS tends to overfit to training views, causing
background collapse and excessive floaters, especially as the number of
training views are reduced. We propose a method to enable training coherent
3DGS-based radiance fields of 360 scenes from sparse training views. We find
that using naive depth priors is not sufficient and integrate depth priors with
generative and explicit constraints to reduce background collapse, remove
floaters, and enhance consistency from unseen viewpoints. Experiments show that
our method outperforms base 3DGS by up to 30.5% and NeRF-based methods by up to
15.6% in LPIPS on the MipNeRF-360 dataset with substantially less training and
inference cost.

Comments:
- The main text spans eight pages, followed by two pages of references
  and four pages of supplementary materials

---

## PyNeRF: Pyramidal Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-30 | Haithem Turki, Michael Zollhöfer, Christian Richardt, Deva Ramanan | cs.CV | [PDF](http://arxiv.org/pdf/2312.00252v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) can be dramatically accelerated by spatial
grid representations. However, they do not explicitly reason about scale and so
introduce aliasing artifacts when reconstructing scenes captured at different
camera distances. Mip-NeRF and its extensions propose scale-aware renderers
that project volumetric frustums rather than point samples but such approaches
rely on positional encodings that are not readily compatible with grid methods.
We propose a simple modification to grid-based models by training model heads
at different spatial grid resolutions. At render time, we simply use coarser
grids to render samples that cover larger volumes. Our method can be easily
applied to existing accelerated NeRF methods and significantly improves
rendering quality (reducing error rates by 20-90% across synthetic and
unbounded real-world scenes) while incurring minimal performance overhead (as
each model head is quick to evaluate). Compared to Mip-NeRF, we reduce error
rates by 20% while training over 60x faster.

Comments:
- Neurips 2023 Project page: https://haithemturki.com/pynerf/

---

## GaussianShader: 3D Gaussian Splatting with Shading Functions for  Reflective Surfaces

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-29 | Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, Yuexin Ma | cs.CV | [PDF](http://arxiv.org/pdf/2311.17977v1){: .btn .btn-green }

**Abstract**: The advent of neural 3D Gaussians has recently brought about a revolution in
the field of neural rendering, facilitating the generation of high-quality
renderings at real-time speeds. However, the explicit and discrete
representation encounters challenges when applied to scenes featuring
reflective surfaces. In this paper, we present GaussianShader, a novel method
that applies a simplified shading function on 3D Gaussians to enhance the
neural rendering in scenes with reflective surfaces while preserving the
training and rendering efficiency. The main challenge in applying the shading
function lies in the accurate normal estimation on discrete 3D Gaussians.
Specifically, we proposed a novel normal estimation framework based on the
shortest axis directions of 3D Gaussians with a delicately designed loss to
make the consistency between the normals and the geometries of Gaussian
spheres. Experiments show that GaussianShader strikes a commendable balance
between efficiency and visual quality. Our method surpasses Gaussian Splatting
in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When
compared to prior works handling reflective surfaces, such as Ref-NeRF, our
optimization time is significantly accelerated (23h vs. 0.58h). Please click on
our project website to see more results.

Comments:
- 13 pages, 11 figures, refrences added

---

## NeRFTAP: Enhancing Transferability of Adversarial Patches on Face  Recognition using Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-29 | Xiaoliang Liu, Furao Shen, Feng Han, Jian Zhao, Changhai Nie | cs.CV | [PDF](http://arxiv.org/pdf/2311.17332v1){: .btn .btn-green }

**Abstract**: Face recognition (FR) technology plays a crucial role in various
applications, but its vulnerability to adversarial attacks poses significant
security concerns. Existing research primarily focuses on transferability to
different FR models, overlooking the direct transferability to victim's face
images, which is a practical threat in real-world scenarios. In this study, we
propose a novel adversarial attack method that considers both the
transferability to the FR model and the victim's face image, called NeRFTAP.
Leveraging NeRF-based 3D-GAN, we generate new view face images for the source
and target subjects to enhance transferability of adversarial patches. We
introduce a style consistency loss to ensure the visual similarity between the
adversarial UV map and the target UV map under a 0-1 mask, enhancing the
effectiveness and naturalness of the generated adversarial face images.
Extensive experiments and evaluations on various FR models demonstrate the
superiority of our approach over existing attack techniques. Our work provides
valuable insights for enhancing the robustness of FR systems in practical
adversarial settings.

---

## SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis

nerf{: .label .label-blue }

2023-11-29 | Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan | cs.CV | [PDF](http://arxiv.org/pdf/2311.17590v1){: .btn .btn-green }

**Abstract**: Achieving high synchronization in the synthesis of realistic, speech-driven
talking head videos presents a significant challenge. Traditional Generative
Adversarial Networks (GAN) struggle to maintain consistent facial identity,
while Neural Radiance Fields (NeRF) methods, although they can address this
issue, often produce mismatched lip movements, inadequate facial expressions,
and unstable head poses. A lifelike talking head requires synchronized
coordination of subject identity, lip movements, facial expressions, and head
poses. The absence of these synchronizations is a fundamental flaw, leading to
unrealistic and artificial outcomes. To address the critical issue of
synchronization, identified as the "devil" in creating realistic talking heads,
we introduce SyncTalk. This NeRF-based method effectively maintains subject
identity, enhancing synchronization and realism in talking head synthesis.
SyncTalk employs a Face-Sync Controller to align lip movements with speech and
innovatively uses a 3D facial blendshape model to capture accurate facial
expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more
natural head movements. The Portrait-Sync Generator restores hair details and
blends the generated head with the torso for a seamless visual experience.
Extensive experiments and user studies demonstrate that SyncTalk outperforms
state-of-the-art methods in synchronization and realism. We recommend watching
the supplementary video: https://ziqiaopeng.github.io/synctalk

Comments:
- 11 pages, 5 figures

---

## Cinematic Behavior Transfer via NeRF-based Differentiable Filming

nerf{: .label .label-blue }

2023-11-29 | Xuekun Jiang, Anyi Rao, Jingbo Wang, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2311.17754v1){: .btn .btn-green }

**Abstract**: In the evolving landscape of digital media and video production, the precise
manipulation and reproduction of visual elements like camera movements and
character actions are highly desired. Existing SLAM methods face limitations in
dynamic scenes and human pose estimation often focuses on 2D projections,
neglecting 3D statuses. To address these issues, we first introduce a reverse
filming behavior estimation technique. It optimizes camera trajectories by
leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then
introduce a cinematic transfer pipeline that is able to transfer various shot
types to a new 2D video or a 3D virtual environment. The incorporation of 3D
engine workflow enables superior rendering and control abilities, which also
achieves a higher rating in the user study.

Comments:
- Project Page:
  https://virtualfilmstudio.github.io/projects/cinetransfer

---

## HUGS: Human Gaussian Splats

gaussian splatting{: .label .label-blue }

2023-11-29 | Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2311.17910v1){: .btn .btn-green }

**Abstract**: Recent advances in neural rendering have improved both training and rendering
times by orders of magnitude. While these methods demonstrate state-of-the-art
quality and speed, they are designed for photogrammetry of static scenes and do
not generalize well to freely moving humans in the environment. In this work,
we introduce Human Gaussian Splats (HUGS) that represents an animatable human
together with the scene using 3D Gaussian Splatting (3DGS). Our method takes
only a monocular video with a small number of (50-100) frames, and it
automatically learns to disentangle the static scene and a fully animatable
human avatar within 30 minutes. We utilize the SMPL body model to initialize
the human Gaussians. To capture details that are not modeled by SMPL (e.g.
cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.
Utilizing 3D Gaussians for animated humans brings new challenges, including the
artifacts created when articulating the Gaussians. We propose to jointly
optimize the linear blend skinning weights to coordinate the movements of
individual Gaussians during animation. Our approach enables novel-pose
synthesis of human and novel view synthesis of both the human and the scene. We
achieve state-of-the-art rendering quality with a rendering speed of 60 FPS
while being ~100x faster to train over previous work. Our code will be
announced here: https://github.com/apple/ml-hugs

---

## FisherRF: Active View Selection and Uncertainty Quantification for  Radiance Fields using Fisher Information

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-29 | Wen Jiang, Boshu Lei, Kostas Daniilidis | cs.CV | [PDF](http://arxiv.org/pdf/2311.17874v1){: .btn .btn-green }

**Abstract**: This study addresses the challenging problem of active view selection and
uncertainty quantification within the domain of Radiance Fields. Neural
Radiance Fields (NeRF) have greatly advanced image rendering and
reconstruction, but the limited availability of 2D images poses uncertainties
stemming from occlusions, depth ambiguities, and imaging errors. Efficiently
selecting informative views becomes crucial, and quantifying NeRF model
uncertainty presents intricate challenges. Existing approaches either depend on
model architecture or are based on assumptions regarding density distributions
that are not generally applicable. By leveraging Fisher Information, we
efficiently quantify observed information within Radiance Fields without ground
truth data. This can be used for the next best view selection and pixel-wise
uncertainty quantification. Our method overcomes existing limitations on model
architecture and effectiveness, achieving state-of-the-art results in both view
selection and uncertainty quantification, demonstrating its potential to
advance the field of Radiance Fields. Our method with the 3D Gaussian Splatting
backend could perform view selections at 70 fps.

Comments:
- Project page: https://jiangwenpl.github.io/FisherRF/

---

## AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text

nerf{: .label .label-blue }

2023-11-29 | Jianfeng Zhang, Xuanmeng Zhang, Huichao Zhang, Jun Hao Liew, Chenxu Zhang, Yi Yang, Jiashi Feng | cs.GR | [PDF](http://arxiv.org/pdf/2311.17917v1){: .btn .btn-green }

**Abstract**: We study the problem of creating high-fidelity and animatable 3D avatars from
only textual descriptions. Existing text-to-avatar methods are either limited
to static avatars which cannot be animated or struggle to generate animatable
avatars with promising quality and precise pose control. To address these
limitations, we propose AvatarStudio, a coarse-to-fine generative model that
generates explicit textured 3D meshes for animatable human avatars.
Specifically, AvatarStudio begins with a low-resolution NeRF-based
representation for coarse generation, followed by incorporating SMPL-guided
articulation into the explicit mesh representation to support avatar animation
and high resolution rendering. To ensure view consistency and pose
controllability of the resulting avatars, we introduce a 2D diffusion model
conditioned on DensePose for Score Distillation Sampling supervision. By
effectively leveraging the synergy between the articulated mesh representation
and the DensePose-conditional diffusion model, AvatarStudio can create
high-quality avatars from text that are ready for animation, significantly
outperforming previous methods. Moreover, it is competent for many
applications, e.g., multimodal avatar animations and style-guided avatar
creation. For more results, please refer to our project page:
http://jeff95.me/projects/avatarstudio.html

Comments:
- Project page at http://jeff95.me/projects/avatarstudio.html

---

## CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-11-29 | Alexander Vilesov, Pradyumna Chari, Achuta Kadambi | cs.CV | [PDF](http://arxiv.org/pdf/2311.17907v1){: .btn .btn-green }

**Abstract**: With the onset of diffusion-based generative models and their ability to
generate text-conditioned images, content generation has received a massive
invigoration. Recently, these models have been shown to provide useful guidance
for the generation of 3D graphics assets. However, existing work in
text-conditioned 3D generation faces fundamental constraints: (i) inability to
generate detailed, multi-object scenes, (ii) inability to textually control
multi-object configurations, and (iii) physically realistic scene composition.
In this work, we propose CG3D, a method for compositionally generating scalable
3D assets that resolves these constraints. We find that explicit Gaussian
radiance fields, parameterized to allow for compositions of objects, possess
the capability to enable semantically and physically consistent scenes. By
utilizing a guidance framework built around this explicit representation, we
show state of the art results, capable of even exceeding the guiding diffusion
model in terms of object combinations and physics accuracy.

---

## Point'n Move: Interactive Scene Object Manipulation on Gaussian  Splatting Radiance Fields

gaussian splatting{: .label .label-blue }

2023-11-28 | Jiajun Huang, Hongchuan Yu | cs.CV | [PDF](http://arxiv.org/pdf/2311.16737v1){: .btn .btn-green }

**Abstract**: We propose Point'n Move, a method that achieves interactive scene object
manipulation with exposed region inpainting. Interactivity here further comes
from intuitive object selection and real-time editing. To achieve this, we
adopt Gaussian Splatting Radiance Field as the scene representation and fully
leverage its explicit nature and speed advantage. Its explicit representation
formulation allows us to devise a 2D prompt points to 3D mask dual-stage
self-prompting segmentation algorithm, perform mask refinement and merging,
minimize change as well as provide good initialization for scene inpainting and
perform editing in real-time without per-editing training, all leads to
superior quality and performance. We test our method by performing editing on
both forward-facing and 360 scenes. We also compare our method against existing
scene object removal methods, showing superior quality despite being more
capable and having a speed advantage.

---

## Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-28 | Zhiwen Yan, Weng Fei Low, Yu Chen, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2311.17089v1){: .btn .btn-green }

**Abstract**: 3D Gaussians have recently emerged as a highly efficient representation for
3D reconstruction and rendering. Despite its high rendering quality and speed
at high resolutions, they both deteriorate drastically when rendered at lower
resolutions or from far away camera position. During low resolution or far away
rendering, the pixel size of the image can fall below the Nyquist frequency
compared to the screen size of each splatted 3D Gaussian and leads to aliasing
effect. The rendering is also drastically slowed down by the sequential alpha
blending of more splatted Gaussians per pixel. To address these issues, we
propose a multi-scale 3D Gaussian splatting algorithm, which maintains
Gaussians at different scales to represent the same scene. Higher-resolution
images are rendered with more small Gaussians, and lower-resolution images are
rendered with fewer larger Gaussians. With similar training time, our algorithm
can achieve 13\%-66\% PSNR and 160\%-2400\% rendering speed improvement at
4$\times$-128$\times$ scale rendering on Mip-NeRF360 dataset compared to the
single scale 3D Gaussian splatting.

---

## RGBGrasp: Image-based Object Grasping by Capturing Multiple Views during  Robot Arm Movement with Neural Radiance Fields



2023-11-28 | Chang Liu, Kejian Shi, Kaichen Zhou, Haoxiao Wang, Jiyao Zhang, Hao Dong | cs.RO | [PDF](http://arxiv.org/pdf/2311.16592v1){: .btn .btn-green }

**Abstract**: Robotic research encounters a significant hurdle when it comes to the
intricate task of grasping objects that come in various shapes, materials, and
textures. Unlike many prior investigations that heavily leaned on specialized
point-cloud cameras or abundant RGB visual data to gather 3D insights for
object-grasping missions, this paper introduces a pioneering approach called
RGBGrasp. This method depends on a limited set of RGB views to perceive the 3D
surroundings containing transparent and specular objects and achieve accurate
grasping. Our method utilizes pre-trained depth prediction models to establish
geometry constraints, enabling precise 3D structure estimation, even under
limited view conditions. Finally, we integrate hash encoding and a proposal
sampler strategy to significantly accelerate the 3D reconstruction process.
These innovations significantly enhance the adaptability and effectiveness of
our algorithm in real-world scenarios. Through comprehensive experimental
validation, we demonstrate that RGBGrasp achieves remarkable success across a
wide spectrum of object-grasping scenarios, establishing it as a promising
solution for real-world robotic manipulation tasks. The demo of our method can
be found on: https://sites.google.com/view/rgbgrasp

---

## SCALAR-NeRF: SCAlable LARge-scale Neural Radiance Fields for Scene  Reconstruction

nerf{: .label .label-blue }

2023-11-28 | Yu Chen, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2311.16657v1){: .btn .btn-green }

**Abstract**: In this work, we introduce SCALAR-NeRF, a novel framework tailored for
scalable large-scale neural scene reconstruction. We structure the neural
representation as an encoder-decoder architecture, where the encoder processes
3D point coordinates to produce encoded features, and the decoder generates
geometric values that include volume densities of signed distances and colors.
Our approach first trains a coarse global model on the entire image dataset.
Subsequently, we partition the images into smaller blocks using KMeans with
each block being modeled by a dedicated local model. We enhance the overlapping
regions across different blocks by scaling up the bounding boxes of each local
block. Notably, the decoder from the global model is shared across distinct
blocks and therefore promoting alignment in the feature space of local
encoders. We propose an effective and efficient methodology to fuse the outputs
from these local models to attain the final reconstruction. Employing this
refined coarse-to-fine strategy, our method outperforms state-of-the-art NeRF
methods and demonstrates scalability for large-scale scene reconstruction. The
code will be available on our project page at
https://aibluefisher.github.io/SCALAR-NeRF/

Comments:
- Project Page: https://aibluefisher.github.io/SCALAR-NeRF

---

## SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry,  Illumination, and Material Estimation

nerf{: .label .label-blue }

2023-11-28 | Jesus Zarzar, Bernard Ghanem | cs.CV | [PDF](http://arxiv.org/pdf/2311.16671v1){: .btn .btn-green }

**Abstract**: We present a novel approach for digitizing real-world objects by estimating
their geometry, material properties, and environmental lighting from a set of
posed images with fixed lighting. Our method incorporates into Neural Radiance
Field (NeRF) pipelines the split sum approximation used with image-based
lighting for real-time physical-based rendering. We propose modeling the
scene's lighting with a single scene-specific MLP representing pre-integrated
image-based lighting at arbitrary resolutions. We achieve accurate modeling of
pre-integrated lighting by exploiting a novel regularizer based on efficient
Monte Carlo sampling. Additionally, we propose a new method of supervising
self-occlusion predictions by exploiting a similar regularizer based on Monte
Carlo sampling. Experimental results demonstrate the efficiency and
effectiveness of our approach in estimating scene geometry, material
properties, and lighting. Our method is capable of attaining state-of-the-art
relighting quality after only ${\sim}1$ hour of training in a single NVIDIA
A100 GPU.

---

## Human Gaussian Splatting: Real-time Rendering of Animatable Avatars

gaussian splatting{: .label .label-blue }

2023-11-28 | Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2311.17113v1){: .btn .btn-green }

**Abstract**: This work addresses the problem of real-time rendering of photorealistic
human body avatars learned from multi-view videos. While the classical
approaches to model and render virtual humans generally use a textured mesh,
recent research has developed neural body representations that achieve
impressive visual quality. However, these models are difficult to render in
real-time and their quality degrades when the character is animated with body
poses different than the training observations. We propose the first animatable
human model based on 3D Gaussian Splatting, that has recently emerged as a very
efficient alternative to neural radiance fields. Our body is represented by a
set of gaussian primitives in a canonical space which are deformed in a coarse
to fine approach that combines forward skinning and local non-rigid refinement.
We describe how to learn our Human Gaussian Splatting (\OURS) model in an
end-to-end fashion from multi-view observations, and evaluate it against the
state-of-the-art approaches for novel pose synthesis of clothed body. Our
method presents a PSNR 1.5dbB better than the state-of-the-art on THuman4
dataset while being able to render at 20fps or more.

---

## REF$^2$-NeRF: Reflection and Refraction aware Neural Radiance Field

nerf{: .label .label-blue }

2023-11-28 | Wooseok Kim, Taiki Fukiage, Takeshi Oishi | cs.CV | [PDF](http://arxiv.org/pdf/2311.17116v3){: .btn .btn-green }

**Abstract**: Recently, significant progress has been made in the study of methods for 3D
reconstruction from multiple images using implicit neural representations,
exemplified by the neural radiance field (NeRF) method. Such methods, which are
based on volume rendering, can model various light phenomena, and various
extended methods have been proposed to accommodate different scenes and
situations. However, when handling scenes with multiple glass objects, e.g.,
objects in a glass showcase, modeling the target scene accurately has been
challenging due to the presence of multiple reflection and refraction effects.
Thus, this paper proposes a NeRF-based modeling method for scenes containing a
glass case. In the proposed method, refraction and reflection are modeled using
elements that are dependent and independent of the viewer's perspective. This
approach allows us to estimate the surfaces where refraction occurs, i.e.,
glass surfaces, and enables the separation and modeling of both direct and
reflected light components. Compared to existing methods, the proposed method
enables more accurate modeling of both glass refraction and the overall scene.

Comments:
- 10 pages, 8 figures, 2 tables

---

## DGNR: Density-Guided Neural Point Rendering of Large Driving Scenes

nerf{: .label .label-blue }

2023-11-28 | Zhuopeng Li, Chenming Wu, Liangjun Zhang, Jianke Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2311.16664v1){: .btn .btn-green }

**Abstract**: Despite the recent success of Neural Radiance Field (NeRF), it is still
challenging to render large-scale driving scenes with long trajectories,
particularly when the rendering quality and efficiency are in high demand.
Existing methods for such scenes usually involve with spatial warping,
geometric supervision from zero-shot normal or depth estimation, or scene
division strategies, where the synthesized views are often blurry or fail to
meet the requirement of efficient rendering. To address the above challenges,
this paper presents a novel framework that learns a density space from the
scenes to guide the construction of a point-based renderer, dubbed as DGNR
(Density-Guided Neural Rendering). In DGNR, geometric priors are no longer
needed, which can be intrinsically learned from the density space through
volumetric rendering. Specifically, we make use of a differentiable renderer to
synthesize images from the neural density features obtained from the learned
density space. A density-based fusion module and geometric regularization are
proposed to optimize the density space. By conducting experiments on a widely
used autonomous driving dataset, we have validated the effectiveness of DGNR in
synthesizing photorealistic driving scenes and achieving real-time capable
rendering.

---

## Continuous Pose for Monocular Cameras in Neural Implicit Representation

nerf{: .label .label-blue }

2023-11-28 | Qi Ma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2311.17119v1){: .btn .btn-green }

**Abstract**: In this paper, we showcase the effectiveness of optimizing monocular camera
poses as a continuous function of time. The camera poses are represented using
an implicit neural function which maps the given time to the corresponding
camera pose. The mapped camera poses are then used for the downstream tasks
where joint camera pose optimization is also required. While doing so, the
network parameters -- that implicitly represent camera poses -- are optimized.
We exploit the proposed method in four diverse experimental settings, namely,
(1) NeRF from noisy poses; (2) NeRF from asynchronous Events; (3) Visual
Simultaneous Localization and Mapping (vSLAM); and (4) vSLAM with IMUs. In all
four settings, the proposed method performs significantly better than the
compared baselines and the state-of-the-art methods. Additionally, using the
assumption of continuous motion, changes in pose may actually live in a
manifold that has lower than 6 degrees of freedom (DOF) is also realized. We
call this low DOF motion representation as the \emph{intrinsic motion} and use
the approach in vSLAM settings, showing impressive camera tracking performance.

---

## Rethinking Directional Integration in Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-28 | Congyue Deng, Jiawei Yang, Leonidas Guibas, Yue Wang | cs.CV | [PDF](http://arxiv.org/pdf/2311.16504v1){: .btn .btn-green }

**Abstract**: Recent works use the Neural radiance field (NeRF) to perform multi-view 3D
reconstruction, providing a significant leap in rendering photorealistic
scenes. However, despite its efficacy, NeRF exhibits limited capability of
learning view-dependent effects compared to light field rendering or
image-based view synthesis. To that end, we introduce a modification to the
NeRF rendering equation which is as simple as a few lines of code change for
any NeRF variations, while greatly improving the rendering quality of
view-dependent effects. By swapping the integration operator and the direction
decoder network, we only integrate the positional features along the ray and
move the directional terms out of the integration, resulting in a
disentanglement of the view-dependent and independent components. The modified
equation is equivalent to the classical volumetric rendering in ideal cases on
object surfaces with Dirac densities. Furthermore, we prove that with the
errors caused by network approximation and numerical integration, our rendering
equation exhibits better convergence properties with lower error accumulations
compared to the classical NeRF. We also show that the modified equation can be
interpreted as light field rendering with learned ray embeddings. Experiments
on different NeRF variations show consistent improvements in the quality of
view-dependent effects with our simple modification.

---

## LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and  200+ FPS

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-28 | Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2311.17245v3){: .btn .btn-green }

**Abstract**: Recent advancements in real-time neural rendering using point-based
techniques have paved the way for the widespread adoption of 3D
representations. However, foundational approaches like 3D Gaussian Splatting
come with a substantial storage overhead caused by growing the SfM points to
millions, often demanding gigabyte-level disk space for a single unbounded
scene, posing significant scalability challenges and hindering the splatting
efficiency.
  To address this challenge, we introduce LightGaussian, a novel method
designed to transform 3D Gaussians into a more efficient and compact format.
Drawing inspiration from the concept of Network Pruning, LightGaussian
identifies Gaussians that are insignificant in contributing to the scene
reconstruction and adopts a pruning and recovery process, effectively reducing
redundancy in Gaussian counts while preserving visual effects. Additionally,
LightGaussian employs distillation and pseudo-view augmentation to distill
spherical harmonics to a lower degree, allowing knowledge transfer to more
compact representations while maintaining reflectance. Furthermore, we propose
a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in
lower bitwidth representations with minimal accuracy losses.
  In summary, LightGaussian achieves an averaged compression rate over 15x
while boosting the FPS from 139 to 215, enabling an efficient representation of
complex scenes on Mip-NeRF 360, Tank and Temple datasets.
  Project website: https://lightgaussian.github.io/

Comments:
- 16pages, 8figures

---

## HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-11-28 | Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2311.17061v1){: .btn .btn-green }

**Abstract**: Realistic 3D human generation from text prompts is a desirable yet
challenging task. Existing methods optimize 3D representations like mesh or
neural fields via score distillation sampling (SDS), which suffers from
inadequate fine details or excessive training time. In this paper, we propose
an efficient yet effective framework, HumanGaussian, that generates
high-quality 3D humans with fine-grained geometry and realistic appearance. Our
key insight is that 3D Gaussian Splatting is an efficient renderer with
periodic Gaussian shrinkage or growing, where such adaptive density control can
be naturally guided by intrinsic human structures. Specifically, 1) we first
propose a Structure-Aware SDS that simultaneously optimizes human appearance
and geometry. The multi-modal score function from both RGB and depth space is
leveraged to distill the Gaussian densification and pruning process. 2)
Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS
into a noisier generative score and a cleaner classifier score, which well
addresses the over-saturation issue. The floating artifacts are further
eliminated based on Gaussian size in a prune-only phase to enhance generation
smoothness. Extensive experiments demonstrate the superior efficiency and
competitive quality of our framework, rendering vivid 3D humans under diverse
scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian

Comments:
- Project Page: https://alvinliu0.github.io/projects/HumanGaussian

---

## A Unified Approach for Text- and Image-guided 4D Scene Generation



2023-11-28 | Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Karsten Kreis, Otmar Hilliges, Shalini De Mello | cs.CV | [PDF](http://arxiv.org/pdf/2311.16854v2){: .btn .btn-green }

**Abstract**: Large-scale diffusion generative models are greatly simplifying image, video
and 3D asset creation from user-provided text prompts and images. However, the
challenging problem of text-to-4D dynamic 3D scene generation with diffusion
guidance remains largely unexplored. We propose Dream-in-4D, which features a
novel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2D
diffusion guidance to effectively learn a high-quality static 3D asset in the
first stage; (2) a deformable neural radiance field that explicitly
disentangles the learned static asset from its deformation, preserving quality
during motion learning; and (3) a multi-resolution feature grid for the
deformation field with a displacement total variation loss to effectively learn
motion with video diffusion guidance in the second stage. Through a user
preference study, we demonstrate that our approach significantly advances image
and motion quality, 3D consistency and text fidelity for text-to-4D generation
compared to baseline approaches. Thanks to its motion-disentangled
representation, Dream-in-4D can also be easily adapted for controllable
generation where appearance is defined by one or multiple images, without the
need to modify the motion learning stage. Thus, our method offers, for the
first time, a unified approach for text-to-4D, image-to-4D and personalized 4D
generation tasks.

Comments:
- Project page: https://research.nvidia.com/labs/nxp/dream-in-4d/

---

## UC-NeRF: Neural Radiance Field for Under-Calibrated Multi-view Cameras  in Autonomous Driving

nerf{: .label .label-blue }

2023-11-28 | Kai Cheng, Xiaoxiao Long, Wei Yin, Jin Wang, Zhiqiang Wu, Yuexin Ma, Kaixuan Wang, Xiaozhi Chen, Xuejin Chen | cs.CV | [PDF](http://arxiv.org/pdf/2311.16945v2){: .btn .btn-green }

**Abstract**: Multi-camera setups find widespread use across various applications, such as
autonomous driving, as they greatly expand sensing capabilities. Despite the
fast development of Neural radiance field (NeRF) techniques and their wide
applications in both indoor and outdoor scenes, applying NeRF to multi-camera
systems remains very challenging. This is primarily due to the inherent
under-calibration issues in multi-camera setup, including inconsistent imaging
effects stemming from separately calibrated image signal processing units in
diverse cameras, and system errors arising from mechanical vibrations during
driving that affect relative camera poses. In this paper, we present UC-NeRF, a
novel method tailored for novel view synthesis in under-calibrated multi-view
camera systems. Firstly, we propose a layer-based color correction to rectify
the color inconsistency in different image regions. Second, we propose virtual
warping to generate more viewpoint-diverse but color-consistent virtual views
for color correction and 3D recovery. Finally, a spatiotemporally constrained
pose refinement is designed for more robust and accurate pose calibration in
multi-camera systems. Our method not only achieves state-of-the-art performance
of novel view synthesis in multi-camera setups, but also effectively
facilitates depth estimation in large-scale outdoor scenes with the synthesized
novel views.

Comments:
- See the project page for code, data:
  https://kcheng1021.github.io/ucnerf.github.io

---

## The Sky's the Limit: Re-lightable Outdoor Scenes via a Sky-pixel  Constrained Illumination Prior and Outside-In Visibility

nerf{: .label .label-blue }

2023-11-28 | James A. D. Gardner, Evgenii Kashin, Bernhard Egger, William A. P. Smith | cs.CV | [PDF](http://arxiv.org/pdf/2311.16937v1){: .btn .btn-green }

**Abstract**: Inverse rendering of outdoor scenes from unconstrained image collections is a
challenging task, particularly illumination/albedo ambiguities and occlusion of
the illumination environment (shadowing) caused by geometry. However, there are
many cues in an image that can aid in the disentanglement of geometry, albedo
and shadows. We exploit the fact that any sky pixel provides a direct
measurement of distant lighting in the corresponding direction and, via a
neural illumination prior, a statistical cue as to the remaining illumination
environment. We also introduce a novel `outside-in' method for computing
differentiable sky visibility based on a neural directional distance function.
This is efficient and can be trained in parallel with the neural scene
representation, allowing gradients from appearance loss to flow from shadows to
influence estimation of illumination and geometry. Our method estimates
high-quality albedo, geometry, illumination and sky visibility, achieving
state-of-the-art results on the NeRF-OSR relighting benchmark. Our code and
models can be found https://github.com/JADGardner/neusky

---

## Animatable Gaussians: Learning Pose-dependent Gaussian Maps for  High-fidelity Human Avatar Modeling

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-27 | Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2311.16096v1){: .btn .btn-green }

**Abstract**: Modeling animatable human avatars from RGB videos is a long-standing and
challenging problem. Recent works usually adopt MLP-based neural radiance
fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to
regress pose-dependent garment details. To this end, we introduce Animatable
Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D
Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians
with the animatable avatar, we learn a parametric template from the input
videos, and then parameterize the template on two front \& back canonical
Gaussian maps where each pixel represents a 3D Gaussian. The learned template
is adaptive to the wearing garments for modeling looser clothes like dresses.
Such template-guided 2D parameterization enables us to employ a powerful
StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling
detailed dynamic appearances. Furthermore, we introduce a pose projection
strategy for better generalization given novel poses. Overall, our method can
create lifelike avatars with dynamic, realistic and generalized appearances.
Experiments show that our method outperforms other state-of-the-art approaches.
Code: https://github.com/lizhe00/AnimatableGaussians

Comments:
- Projectpage: https://animatable-gaussians.github.io/, Code:
  https://github.com/lizhe00/AnimatableGaussians

---

## GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-27 | Jiemin Fang, Junjie Wang, Xiaopeng Zhang, Lingxi Xie, Qi Tian | cs.CV | [PDF](http://arxiv.org/pdf/2311.16037v1){: .btn .btn-green }

**Abstract**: Recently, impressive results have been achieved in 3D scene editing with text
instructions based on a 2D diffusion model. However, current diffusion models
primarily generate images by predicting noise in the latent space, and the
editing is usually applied to the whole image, which makes it challenging to
perform delicate, especially localized, editing for 3D scenes. Inspired by
recent 3D Gaussian splatting, we propose a systematic framework, named
GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text
instructions. Benefiting from the explicit property of 3D Gaussians, we design
a series of techniques to achieve delicate editing. Specifically, we first
extract the region of interest (RoI) corresponding to the text instruction,
aligning it to 3D Gaussians. The Gaussian RoI is further used to control the
editing process. Our framework can achieve more delicate and precise editing of
3D scenes than previous methods while enjoying much faster training speed, i.e.
within 20 minutes on a single V100 GPU, more than twice as fast as
Instruct-NeRF2NeRF (45 minutes -- 2 hours).

Comments:
- Project page: https://GaussianEditor.github.io

---

## Deceptive-Human: Prompt-to-NeRF 3D Human Generation with 3D-Consistent  Synthetic Images

nerf{: .label .label-blue }

2023-11-27 | Shiu-hong Kao, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2311.16499v1){: .btn .btn-green }

**Abstract**: This paper presents Deceptive-Human, a novel Prompt-to-NeRF framework
capitalizing state-of-the-art control diffusion models (e.g., ControlNet) to
generate a high-quality controllable 3D human NeRF. Different from direct 3D
generative approaches, e.g., DreamFusion and DreamHuman, Deceptive-Human
employs a progressive refinement technique to elevate the reconstruction
quality. This is achieved by utilizing high-quality synthetic human images
generated through the ControlNet with view-consistent loss. Our method is
versatile and readily extensible, accommodating multimodal inputs, including a
text prompt and additional data such as 3D mesh, poses, and seed images. The
resulting 3D human NeRF model empowers the synthesis of highly photorealistic
novel views from 360-degree perspectives. The key to our Deceptive-Human for
hallucinating multi-view consistent synthetic human images lies in our
progressive finetuning strategy. This strategy involves iteratively enhancing
views using the provided multimodal inputs at each intermediate step to improve
the human NeRF model. Within this iterative refinement process, view-dependent
appearances are systematically eliminated to prevent interference with the
underlying density estimation. Extensive qualitative and quantitative
experimental comparison shows that our deceptive human models achieve
state-of-the-art application quality.

Comments:
- Github project: https://github.com/DanielSHKao/DeceptiveHuman

---

## SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using  Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-27 | Quentin Herau, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux | cs.CV | [PDF](http://arxiv.org/pdf/2311.15803v2){: .btn .btn-green }

**Abstract**: In rapidly-evolving domains such as autonomous driving, the use of multiple
sensors with different modalities is crucial to ensure high operational
precision and stability. To correctly exploit the provided information by each
sensor in a single common frame, it is essential for these sensors to be
accurately calibrated. In this paper, we leverage the ability of Neural
Radiance Fields (NeRF) to represent different sensors modalities in a common
volumetric representation to achieve robust and accurate spatio-temporal sensor
calibration. By designing a partitioning approach based on the visible part of
the scene for each sensor, we formulate the calibration problem using only the
overlapping areas. This strategy results in a more robust and accurate
calibration that is less prone to failure. We demonstrate that our approach
works on outdoor urban scenes by validating it on multiple established driving
datasets. Results show that our method is able to get better accuracy and
robustness compared to existing methods.

Comments:
- Paper + Supplementary, under review. Project page:
  https://qherau.github.io/SOAC/

---

## Mip-Splatting: Alias-free 3D Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-11-27 | Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, Andreas Geiger | cs.CV | [PDF](http://arxiv.org/pdf/2311.16493v1){: .btn .btn-green }

**Abstract**: Recently, 3D Gaussian Splatting has demonstrated impressive novel view
synthesis results, reaching high fidelity and efficiency. However, strong
artifacts can be observed when changing the sampling rate, \eg, by changing
focal length or camera distance. We find that the source for this phenomenon
can be attributed to the lack of 3D frequency constraints and the usage of a 2D
dilation filter. To address this problem, we introduce a 3D smoothing filter
which constrains the size of the 3D Gaussian primitives based on the maximal
sampling frequency induced by the input views, eliminating high-frequency
artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip
filter, which simulates a 2D box filter, effectively mitigates aliasing and
dilation issues. Our evaluation, including scenarios such a training on
single-scale images and testing on multiple scales, validates the effectiveness
of our approach.

Comments:
- Project page: https://niujinshuchong.github.io/mip-splatting/

---

## PaintNeSF: Artistic Creation of Stylized Scenes with Vectorized 3D  Strokes



2023-11-27 | Hao-Bin Duan, Miao Wang, Yan-Xun Li, Yong-Liang Yang | cs.CV | [PDF](http://arxiv.org/pdf/2311.15637v1){: .btn .btn-green }

**Abstract**: We present Paint Neural Stroke Field (PaintNeSF), a novel technique to
generate stylized images of a 3D scene at arbitrary novel views from multi-view
2D images. Different from existing methods which apply stylization to trained
neural radiance fields at the voxel level, our approach draws inspiration from
image-to-painting methods, simulating the progressive painting process of human
artwork with vector strokes. We develop a palette of stylized 3D strokes from
basic primitives and splines, and consider the 3D scene stylization task as a
multi-view reconstruction process based on these 3D stroke primitives. Instead
of directly searching for the parameters of these 3D strokes, which would be
too costly, we introduce a differentiable renderer that allows optimizing
stroke parameters using gradient descent, and propose a training scheme to
alleviate the vanishing gradient issue. The extensive evaluation demonstrates
that our approach effectively synthesizes 3D scenes with significant geometric
and aesthetic stylization while maintaining a consistent appearance across
different views. Our method can be further integrated with style loss and
image-text contrastive models to extend its applications, including color
transfer and text-driven 3D scene drawing.

---

## Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple  Human Avatars



2023-11-27 | Yang Liu, Xiang Huang, Minghan Qin, Qinwei Lin, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2311.16482v2){: .btn .btn-green }

**Abstract**: Neural radiance fields are capable of reconstructing high-quality drivable
human avatars but are expensive to train and render. To reduce consumption, we
propose Animatable 3D Gaussian, which learns human avatars from input images
and poses. We extend 3D Gaussians to dynamic human scenes by modeling a set of
skinned 3D Gaussians and a corresponding skeleton in canonical space and
deforming 3D Gaussians to posed space according to the input poses. We
introduce hash-encoded shape and appearance to speed up training and propose
time-dependent ambient occlusion to achieve high-quality reconstructions in
scenes containing complex motions and dynamic shadows. On both novel view
synthesis and novel pose synthesis tasks, our method outperforms existing
methods in terms of training time, rendering speed, and reconstruction quality.
Our method can be easily extended to multi-human scenes and achieve comparable
novel view synthesis results on a scene with ten people in only 25 seconds of
training.

---

## CaesarNeRF: Calibrated Semantic Representation for Few-shot  Generalizable Neural Rendering

nerf{: .label .label-blue }

2023-11-27 | Haidong Zhu, Tianyu Ding, Tianyi Chen, Ilya Zharkov, Ram Nevatia, Luming Liang | cs.CV | [PDF](http://arxiv.org/pdf/2311.15510v1){: .btn .btn-green }

**Abstract**: Generalizability and few-shot learning are key challenges in Neural Radiance
Fields (NeRF), often due to the lack of a holistic understanding in pixel-level
rendering. We introduce CaesarNeRF, an end-to-end approach that leverages
scene-level CAlibratEd SemAntic Representation along with pixel-level
representations to advance few-shot, generalizable neural rendering,
facilitating a holistic understanding without compromising high-quality
details. CaesarNeRF explicitly models pose differences of reference views to
combine scene-level semantic representations, providing a calibrated holistic
understanding. This calibration process aligns various viewpoints with precise
location and is further enhanced by sequential refinement to capture varying
details. Extensive experiments on public datasets, including LLFF, Shiny,
mip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-art
performance across varying numbers of reference views, proving effective even
with a single reference image. The project page of this work can be found at
https://haidongz-usc.github.io/project/caesarnerf.

---

## Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF  Decomposition and Ray Tracing

gaussian splatting{: .label .label-blue }

2023-11-27 | Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2311.16043v1){: .btn .btn-green }

**Abstract**: We present a novel differentiable point-based rendering framework for
material and lighting decomposition from multi-view images, enabling editing,
ray-tracing, and real-time relighting of the 3D point cloud. Specifically, a 3D
scene is represented as a set of relightable 3D Gaussian points, where each
point is additionally associated with a normal direction, BRDF parameters, and
incident lights from different directions. To achieve robust lighting
estimation, we further divide incident lights of each point into global and
local components, as well as view-dependent visibilities. The 3D scene is
optimized through the 3D Gaussian Splatting technique while BRDF and lighting
are decomposed by physically-based differentiable rendering. Moreover, we
introduce an innovative point-based ray-tracing approach based on the bounding
volume hierarchy for efficient visibility baking, enabling real-time rendering
and relighting of 3D Gaussian points with accurate shadow effects. Extensive
experiments demonstrate improved BRDF estimation and novel view rendering
results compared to state-of-the-art material estimation approaches. Our
framework showcases the potential to revolutionize the mesh-based graphics
pipeline with a relightable, traceable, and editable rendering pipeline solely
based on point cloud. Project
page:https://nju-3dv.github.io/projects/Relightable3DGaussian/.

---

## GS-IR: 3D Gaussian Splatting for Inverse Rendering

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-26 | Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, Kui Jia | cs.CV | [PDF](http://arxiv.org/pdf/2311.16473v2){: .btn .btn-green }

**Abstract**: We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian
Splatting (GS) that leverages forward mapping volume rendering to achieve
photorealistic novel view synthesis and relighting results. Unlike previous
works that use implicit neural representations and volume rendering (e.g.
NeRF), which suffer from low expressive power and high computational
complexity, we extend GS, a top-performance representation for novel view
synthesis, to estimate scene geometry, surface material, and environment
illumination from multi-view images captured under unknown lighting conditions.
There are two main problems when introducing GS to inverse rendering: 1) GS
does not support producing plausible normal natively; 2) forward mapping (e.g.
rasterization and splatting) cannot trace the occlusion like backward mapping
(e.g. ray tracing). To address these challenges, our GS-IR proposes an
efficient optimization scheme that incorporates a depth-derivation-based
regularization for normal estimation and a baking-based occlusion to model
indirect lighting. The flexible and expressive GS representation allows us to
achieve fast and compact geometry reconstruction, photorealistic novel view
synthesis, and effective physically-based rendering. We demonstrate the
superiority of our method over baseline methods through qualitative and
quantitative evaluations on various challenging scenes.

---

## NeuRAD: Neural Rendering for Autonomous Driving

nerf{: .label .label-blue }

2023-11-26 | Adam Tonderski, Carl Lindström, Georg Hess, William Ljungbergh, Lennart Svensson, Christoffer Petersson | cs.CV | [PDF](http://arxiv.org/pdf/2311.15260v2){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) have gained popularity in the autonomous
driving (AD) community. Recent methods show NeRFs' potential for closed-loop
simulation, enabling testing of AD systems, and as an advanced training data
augmentation technique. However, existing methods often require long training
times, dense semantic supervision, or lack generalizability. This, in turn,
hinders the application of NeRFs for AD at scale. In this paper, we propose
NeuRAD, a robust novel view synthesis method tailored to dynamic AD data. Our
method features simple network design, extensive sensor modeling for both
camera and lidar -- including rolling shutter, beam divergence and ray dropping
-- and is applicable to multiple datasets out of the box. We verify its
performance on five popular AD datasets, achieving state-of-the-art performance
across the board. To encourage further development, we will openly release the
NeuRAD source code. See https://github.com/georghess/NeuRAD .

---

## Obj-NeRF: Extract Object NeRFs from Multi-view Images

nerf{: .label .label-blue }

2023-11-26 | Zhiyi Li, Lihe Ding, Tianfan Xue | cs.CV | [PDF](http://arxiv.org/pdf/2311.15291v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have demonstrated remarkable effectiveness in
novel view synthesis within 3D environments. However, extracting a radiance
field of one specific object from multi-view images encounters substantial
challenges due to occlusion and background complexity, thereby presenting
difficulties in downstream applications such as NeRF editing and 3D mesh
extraction. To solve this problem, in this paper, we propose Obj-NeRF, a
comprehensive pipeline that recovers the 3D geometry of a specific object from
multi-view images using a single prompt. This method combines the 2D
segmentation capabilities of the Segment Anything Model (SAM) in conjunction
with the 3D reconstruction ability of NeRF. Specifically, we first obtain
multi-view segmentation for the indicated object using SAM with a single
prompt. Then, we use the segmentation images to supervise NeRF construction,
integrating several effective techniques. Additionally, we construct a large
object-level NeRF dataset containing diverse objects, which can be useful in
various downstream tasks. To demonstrate the practicality of our method, we
also apply Obj-NeRF to various applications, including object removal,
rotation, replacement, and recoloring.

---

## Efficient Encoding of Graphics Primitives with Simplex-based Structures

nerf{: .label .label-blue }

2023-11-26 | Yibo Wen, Yunfan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2311.15439v1){: .btn .btn-green }

**Abstract**: Grid-based structures are commonly used to encode explicit features for
graphics primitives such as images, signed distance functions (SDF), and neural
radiance fields (NeRF) due to their simple implementation. However, in
$n$-dimensional space, calculating the value of a sampled point requires
interpolating the values of its $2^n$ neighboring vertices. The exponential
scaling with dimension leads to significant computational overheads. To address
this issue, we propose a simplex-based approach for encoding graphics
primitives. The number of vertices in a simplex-based structure increases
linearly with dimension, making it a more efficient and generalizable
alternative to grid-based representations. Using the non-axis-aligned
simplicial structure property, we derive and prove a coordinate transformation,
simplicial subdivision, and barycentric interpolation scheme for efficient
sampling, which resembles transformation procedures in the simplex noise
algorithm. Finally, we use hash tables to store multiresolution features of all
interest points in the simplicial grid, which are passed into a tiny fully
connected neural network to parameterize graphics primitives. We implemented a
detailed simplex-based structure encoding algorithm in C++ and CUDA using the
methods outlined in our approach. In the 2D image fitting task, the proposed
method is capable of fitting a giga-pixel image with 9.4% less time compared to
the baseline method proposed by instant-ngp, while maintaining the same quality
and compression rate. In the volumetric rendering setup, we observe a maximum
41.2% speedup when the samples are dense enough.

Comments:
- 10 pages, 8 figures

---

## GaussianEditor: Swift and Controllable 3D Editing with Gaussian  Splatting

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-24 | Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2311.14521v4){: .btn .btn-green }

**Abstract**: 3D editing plays a crucial role in many areas such as gaming and virtual
reality. Traditional 3D editing methods, which rely on representations like
meshes and point clouds, often fall short in realistically depicting complex
scenes. On the other hand, methods based on implicit 3D representations, like
Neural Radiance Field (NeRF), render complex scenes effectively but suffer from
slow processing speeds and limited control over specific scene areas. In
response to these challenges, our paper presents GaussianEditor, an innovative
and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D
representation. GaussianEditor enhances precision and control in editing
through our proposed Gaussian semantic tracing, which traces the editing target
throughout the training process. Additionally, we propose Hierarchical Gaussian
splatting (HGS) to achieve stabilized and fine results under stochastic
generative guidance from 2D diffusion models. We also develop editing
strategies for efficient object removal and integration, a challenging task for
existing methods. Our comprehensive experiments demonstrate GaussianEditor's
superior control, efficacy, and rapid performance, marking a significant
advancement in 3D editing. Project Page:
https://buaacyw.github.io/gaussian-editor/

Comments:
- Project Page: https://buaacyw.github.io/gaussian-editor/ Code:
  https://github.com/buaacyw/GaussianEditor

---

## Animate124: Animating One Image to 4D Dynamic Scene

nerf{: .label .label-blue }

2023-11-24 | Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2311.14603v1){: .btn .btn-green }

**Abstract**: We introduce Animate124 (Animate-one-image-to-4D), the first work to animate
a single in-the-wild image into 3D video through textual motion descriptions,
an underexplored problem with significant applications. Our 4D generation
leverages an advanced 4D grid dynamic Neural Radiance Field (NeRF) model,
optimized in three distinct stages using multiple diffusion priors. Initially,
a static model is optimized using the reference image, guided by 2D and 3D
diffusion priors, which serves as the initialization for the dynamic NeRF.
Subsequently, a video diffusion model is employed to learn the motion specific
to the subject. However, the object in the 3D videos tends to drift away from
the reference image over time. This drift is mainly due to the misalignment
between the text prompt and the reference image in the video diffusion model.
In the final stage, a personalized diffusion prior is therefore utilized to
address the semantic drift. As the pioneering image-text-to-4D generation
framework, our method demonstrates significant advancements over existing
baselines, evidenced by comprehensive quantitative and qualitative assessments.

Comments:
- Project Page: https://animate124.github.io

---

## ECRF: Entropy-Constrained Neural Radiance Fields Compression with  Frequency Domain Optimization

nerf{: .label .label-blue }

2023-11-23 | Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge | cs.CV | [PDF](http://arxiv.org/pdf/2311.14208v1){: .btn .btn-green }

**Abstract**: Explicit feature-grid based NeRF models have shown promising results in terms
of rendering quality and significant speed-up in training. However, these
methods often require a significant amount of data to represent a single scene
or object. In this work, we present a compression model that aims to minimize
the entropy in the frequency domain in order to effectively reduce the data
size. First, we propose using the discrete cosine transform (DCT) on the
tensorial radiance fields to compress the feature-grid. This feature-grid is
transformed into coefficients, which are then quantized and entropy encoded,
following a similar approach to the traditional video coding pipeline.
Furthermore, to achieve a higher level of sparsity, we propose using an entropy
parameterization technique for the frequency domain, specifically for DCT
coefficients of the feature-grid. Since the transformed coefficients are
optimized during the training phase, the proposed model does not require any
fine-tuning or additional information. Our model only requires a lightweight
compression pipeline for encoding and decoding, making it easier to apply
volumetric radiance field methods for real-world applications. Experimental
results demonstrate that our proposed frequency domain entropy model can
achieve superior compression performance across various datasets. The source
code will be made publicly available.

Comments:
- 10 pages, 6 figures, 4 tables

---

## Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC  using Tube-Guided Data Augmentation and NeRFs

nerf{: .label .label-blue }

2023-11-23 | Andrea Tagliabue, Jonathan P. How | cs.RO | [PDF](http://arxiv.org/pdf/2311.14153v1){: .btn .btn-green }

**Abstract**: Imitation learning (IL) can train computationally-efficient sensorimotor
policies from a resource-intensive Model Predictive Controller (MPC), but it
often requires many samples, leading to long training times or limited
robustness. To address these issues, we combine IL with a variant of robust MPC
that accounts for process and sensing uncertainties, and we design a data
augmentation (DA) strategy that enables efficient learning of vision-based
policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance
Fields (NeRFs) to generate novel synthetic images, and uses properties of the
robust MPC (the tube) to select relevant views and to efficiently compute the
corresponding actions. We tailor our approach to the task of localization and
trajectory tracking on a multirotor, by learning a visuomotor policy that
generates control actions using images from the onboard camera as only source
of horizontal position. Our evaluations numerically demonstrate learning of a
robust visuomotor policy with an 80-fold increase in demonstration efficiency
and a 50% reduction in training time over current IL methods. Additionally, our
policies successfully transfer to a real multirotor, achieving accurate
localization and low tracking errors despite large disturbances, with an
onboard inference time of only 1.5 ms.

Comments:
- Video: https://youtu.be/_W5z33ZK1m4. Evolved paper from our previous
  work: arXiv:2210.10127

---

## Posterior Distillation Sampling



2023-11-23 | Juil Koo, Chanho Park, Minhyuk Sung | cs.CV | [PDF](http://arxiv.org/pdf/2311.13831v1){: .btn .btn-green }

**Abstract**: We introduce Posterior Distillation Sampling (PDS), a novel optimization
method for parametric image editing based on diffusion models. Existing
optimization-based methods, which leverage the powerful 2D prior of diffusion
models to handle various parametric images, have mainly focused on generation.
Unlike generation, editing requires a balance between conforming to the target
attribute and preserving the identity of the source content. Recent 2D image
editing methods have achieved this balance by leveraging the stochastic latent
encoded in the generative process of diffusion models. To extend the editing
capabilities of diffusion models shown in pixel space to parameter space, we
reformulate the 2D image editing method into an optimization form named PDS.
PDS matches the stochastic latents of the source and the target, enabling the
sampling of targets in diverse parameter spaces that align with a desired
attribute while maintaining the source's identity. We demonstrate that this
optimization resembles running a generative process with the target attribute,
but aligning this process with the trajectory of the source's generative
process. Extensive editing results in Neural Radiance Fields and Scalable
Vector Graphics representations demonstrate that PDS is capable of sampling
targets to fulfill the aforementioned balance across various parameter spaces.

Comments:
- Project page: https://posterior-distillation-sampling.github.io/

---

## Towards Transferable Multi-modal Perception Representation Learning for  Autonomy: NeRF-Supervised Masked AutoEncoder

nerf{: .label .label-blue }

2023-11-23 | Xiaohao Xu | cs.CV | [PDF](http://arxiv.org/pdf/2311.13750v2){: .btn .btn-green }

**Abstract**: This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. We hope
this study can inspire exploration of more general multi-modal representation
learning for autonomous agents.

---

## Compact 3D Gaussian Representation for Radiance Field

nerf{: .label .label-blue }

2023-11-22 | Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2311.13681v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have demonstrated remarkable potential in
capturing complex 3D scenes with high fidelity. However, one persistent
challenge that hinders the widespread adoption of NeRFs is the computational
bottleneck due to the volumetric rendering. On the other hand, 3D Gaussian
splatting (3DGS) has recently emerged as an alternative representation that
leverages a 3D Gaussisan-based representation and adopts the rasterization
pipeline to render the images rather than volumetric rendering, achieving very
fast rendering speed and promising image quality. However, a significant
drawback arises as 3DGS entails a substantial number of 3D Gaussians to
maintain the high fidelity of the rendered images, which requires a large
amount of memory and storage. To address this critical issue, we place a
specific emphasis on two key objectives: reducing the number of Gaussian points
without sacrificing performance and compressing the Gaussian attributes, such
as view-dependent color and covariance. To this end, we propose a learnable
mask strategy that significantly reduces the number of Gaussians while
preserving high performance. In addition, we propose a compact but effective
representation of view-dependent color by employing a grid-based neural field
rather than relying on spherical harmonics. Finally, we learn codebooks to
compactly represent the geometric attributes of Gaussian by vector
quantization. In our extensive experiments, we consistently show over
10$\times$ reduced storage and enhanced rendering speed, while maintaining the
quality of the scene representation, compared to 3DGS. Our work provides a
comprehensive framework for 3D scene representation, achieving high
performance, fast training, compactness, and real-time rendering. Our project
page is available at https://maincold2.github.io/c3dgs/.

Comments:
- Project page: http://maincold2.github.io/c3dgs/

---

## Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions

nerf{: .label .label-blue }

2023-11-22 | Keyang Ye, Tianjia Shao, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2311.13404v2){: .btn .btn-green }

**Abstract**: We present a novel animatable 3D Gaussian model for rendering high-fidelity
free-view human motions in real time. Compared to existing NeRF-based methods,
the model owns better capability in synthesizing high-frequency details without
the jittering problem across video frames. The core of our model is a novel
augmented 3D Gaussian representation, which attaches each Gaussian with a
learnable code. The learnable code serves as a pose-dependent appearance
embedding for refining the erroneous appearance caused by geometric
transformation of Gaussians, based on which an appearance refinement model is
learned to produce residual Gaussian properties to match the appearance in
target pose. To force the Gaussians to learn the foreground human only without
background interference, we further design a novel alpha loss to explicitly
constrain the Gaussians within the human body. We also propose to jointly
optimize the human joint parameters to improve the appearance accuracy. The
animatable 3D Gaussian model can be learned with shallow MLPs, so new human
motions can be synthesized in real time (66 fps on avarage). Experiments show
that our model has superior performance over NeRF-based methods.

Comments:
- Some experiment data is wrong. The expression of the paper in
  introduction and abstract is incorrect. Some graphs have inappropriate
  descriptions

---

## LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes

gaussian splatting{: .label .label-blue }

2023-11-22 | Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, Kyoung Mu Lee | cs.CV | [PDF](http://arxiv.org/pdf/2311.13384v2){: .btn .btn-green }

**Abstract**: With the widespread usage of VR devices and contents, demands for 3D scene
generation techniques become more popular. Existing 3D scene generation models,
however, limit the target scene to specific domain, primarily due to their
training strategies using 3D scan dataset that is far from the real-world. To
address such limitation, we propose LucidDreamer, a domain-free scene
generation pipeline by fully leveraging the power of existing large-scale
diffusion-based generative model. Our LucidDreamer has two alternate steps:
Dreaming and Alignment. First, to generate multi-view consistent images from
inputs, we set the point cloud as a geometrical guideline for each image
generation. Specifically, we project a portion of point cloud to the desired
view and provide the projection as a guidance for inpainting using the
generative model. The inpainted images are lifted to 3D space with estimated
depth maps, composing a new points. Second, to aggregate the new points into
the 3D scene, we propose an aligning algorithm which harmoniously integrates
the portions of newly generated 3D scenes. The finally obtained 3D scene serves
as initial points for optimizing Gaussian splats. LucidDreamer produces
Gaussian splats that are highly-detailed compared to the previous 3D scene
generation methods, with no constraint on domain of the target scene. Project
page: https://luciddreamer-cvlab.github.io/

Comments:
- Project page: https://luciddreamer-cvlab.github.io/

---

## Retargeting Visual Data with Deformation Fields



2023-11-22 | Tim Elsner, Julia Berger, Tong Wu, Victor Czech, Lin Gao, Leif Kobbelt | cs.CV | [PDF](http://arxiv.org/pdf/2311.13297v1){: .btn .btn-green }

**Abstract**: Seam carving is an image editing method that enable content-aware resizing,
including operations like removing objects. However, the seam-finding strategy
based on dynamic programming or graph-cut limits its applications to broader
visual data formats and degrees of freedom for editing. Our observation is that
describing the editing and retargeting of images more generally by a
displacement field yields a generalisation of content-aware deformations. We
propose to learn a deformation with a neural network that keeps the output
plausible while trying to deform it only in places with low information
content. This technique applies to different kinds of visual data, including
images, 3D scenes given as neural radiance fields, or even polygon meshes.
Experiments conducted on different visual data show that our method achieves
better content-aware retargeting compared to previous methods.

---

## Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to  3D Prior with Progressive Learning

nerf{: .label .label-blue }

2023-11-22 | Kai Yu, Jinlin Liu, Mengyang Feng, Miaomiao Cui, Xuansong Xie | cs.CV | [PDF](http://arxiv.org/pdf/2311.13617v1){: .btn .btn-green }

**Abstract**: We present Boosting3D, a multi-stage single image-to-3D generation method
that can robustly generate reasonable 3D objects in different data domains. The
point of this work is to solve the view consistency problem in single
image-guided 3D generation by modeling a reasonable geometric structure. For
this purpose, we propose to utilize better 3D prior to training the NeRF. More
specifically, we train an object-level LoRA for the target object using
original image and the rendering output of NeRF. And then we train the LoRA and
NeRF using a progressive training strategy. The LoRA and NeRF will boost each
other while training. After the progressive training, the LoRA learns the 3D
information of the generated object and eventually turns to an object-level 3D
prior. In the final stage, we extract the mesh from the trained NeRF and use
the trained LoRA to optimize the structure and appearance of the mesh. The
experiments demonstrate the effectiveness of the proposed method. Boosting3D
learns object-specific 3D prior which is beyond the ability of pre-trained
diffusion priors and achieves state-of-the-art performance in the single
image-to-3d generation task.

Comments:
- 8 pages, 7 figures, 1 table

---

## 3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh  Rasterization

nerf{: .label .label-blue }

2023-11-22 | Jianwei Feng, Prateek Singhal | cs.CV | [PDF](http://arxiv.org/pdf/2311.13168v1){: .btn .btn-green }

**Abstract**: Style transfer for human face has been widely researched in recent years.
Majority of the existing approaches work in 2D image domain and have 3D
inconsistency issue when applied on different viewpoints of the same face. In
this paper, we tackle the problem of 3D face style transfer which aims at
generating stylized novel views of a 3D human face with multi-view consistency.
We propose to use a neural radiance field (NeRF) to represent 3D human face and
combine it with 2D style transfer to stylize the 3D face. We find that directly
training a NeRF on stylized images from 2D style transfer brings in 3D
inconsistency issue and causes blurriness. On the other hand, training a NeRF
jointly with 2D style transfer objectives shows poor convergence due to the
identity and head pose gap between style image and content image. It also poses
challenge in training time and memory due to the need of volume rendering for
full image to apply style transfer loss functions. We therefore propose a
hybrid framework of NeRF and mesh rasterization to combine the benefits of high
fidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our
framework consists of three stages: 1. Training a NeRF model on input face
images to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF
model and optimizing it with style transfer objectives via differentiable
rasterization; 3. Training a new color network in NeRF conditioned on a style
embedding to enable arbitrary style transfer to the 3D face. Experiment results
show that our approach generates high quality face style transfer with great 3D
consistency, while also enabling a flexible style control.

---

## PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF

nerf{: .label .label-blue }

2023-11-22 | Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, Yin Yang | cs.CV | [PDF](http://arxiv.org/pdf/2311.13099v1){: .btn .btn-green }

**Abstract**: We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.

---

## Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot  Images

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-22 | Jaeyoung Chung, Jeongtaek Oh, Kyoung Mu Lee | cs.CV | [PDF](http://arxiv.org/pdf/2311.13398v3){: .btn .btn-green }

**Abstract**: In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images. Project page: robot0321.github.io/DepthRegGS

Comments:
- 10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS

---

## SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh  Reconstruction and High-Quality Mesh Rendering

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-21 | Antoine Guédon, Vincent Lepetit | cs.GR | [PDF](http://arxiv.org/pdf/2311.12775v3){: .btn .btn-green }

**Abstract**: We propose a method to allow precise and extremely fast mesh extraction from
3D Gaussian Splatting. Gaussian Splatting has recently become very popular as
it yields realistic rendering while being significantly faster to train than
NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D
gaussians as these gaussians tend to be unorganized after optimization and no
method has been proposed so far. Our first key contribution is a regularization
term that encourages the gaussians to align well with the surface of the scene.
We then introduce a method that exploits this alignment to extract a mesh from
the Gaussians using Poisson reconstruction, which is fast, scalable, and
preserves details, in contrast to the Marching Cubes algorithm usually applied
to extract meshes from Neural SDFs. Finally, we introduce an optional
refinement strategy that binds gaussians to the surface of the mesh, and
jointly optimizes these Gaussians and the mesh through Gaussian splatting
rendering. This enables easy editing, sculpting, rigging, animating,
compositing and relighting of the Gaussians using traditional softwares by
manipulating the mesh instead of the gaussians themselves. Retrieving such an
editable mesh for realistic rendering is done within minutes with our method,
compared to hours with the state-of-the-art methods on neural SDFs, while
providing a better rendering quality. Our project page is the following:
https://anttwo.github.io/sugar/

Comments:
- We identified a minor typographical error in Equation 6; We updated
  the paper accordingly. Project Webpage: https://anttwo.github.io/sugar/

---

## Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-21 | Yifan Wang, Yi Gong, Yuan Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2311.12490v1){: .btn .btn-green }

**Abstract**: Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity
scene reconstruction for novel view synthesis. However, NeRF requires hundreds
of network evaluations per pixel to approximate a volume rendering integral,
making it slow to train. Caching NeRFs into explicit data structures can
effectively enhance rendering speed but at the cost of higher memory usage. To
address these issues, we present Hyb-NeRF, a novel neural radiance field with a
multi-resolution hybrid encoding that achieves efficient neural modeling and
fast rendering, which also allows for high-quality novel view synthesis. The
key idea of Hyb-NeRF is to represent the scene using different encoding
strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits
memory-efficiency learnable positional features at coarse resolutions and the
fast optimization speed and local details of hash-based feature grids at fine
resolutions. In addition, to further boost performance, we embed cone
tracing-based features in our learnable positional encoding that eliminates
encoding ambiguity and reduces aliasing artifacts. Extensive experiments on
both synthetic and real-world datasets show that Hyb-NeRF achieves faster
rendering speed with better rending quality and even a lower memory footprint
in comparison to previous state-of-the-art methods.

Comments:
- WACV2024

---

## An Efficient 3D Gaussian Representation for Monocular/Multi-view Dynamic  Scenes



2023-11-21 | Kai Katsumata, Duc Minh Vo, Hideki Nakayama | cs.GR | [PDF](http://arxiv.org/pdf/2311.12897v1){: .btn .btn-green }

**Abstract**: In novel view synthesis of scenes from multiple input views, 3D Gaussian
splatting emerges as a viable alternative to existing radiance field
approaches, delivering great visual quality and real-time rendering. While
successful in static scenes, the present advancement of 3D Gaussian
representation, however, faces challenges in dynamic scenes in terms of memory
consumption and the need for numerous observations per time step, due to the
onus of storing 3D Gaussian parameters per time step. In this study, we present
an efficient 3D Gaussian representation tailored for dynamic scenes in which we
define positions and rotations as functions of time while leaving other
time-invariant properties of the static 3D Gaussian unchanged. Notably, our
representation reduces memory usage, which is consistent regardless of the
input sequence length. Additionally, it mitigates the risk of overfitting
observed frames by accounting for temporal changes. The optimization of our
Gaussian representation based on image and flow reconstruction results in a
powerful framework for dynamic scene view synthesis in both monocular and
multi-view cases. We obtain the highest rendering speed of $118$ frames per
second (FPS) at a resolution of $1352 \times 1014$ with a single GPU, showing
the practical usability and effectiveness of our proposed method in dynamic
scene rendering scenarios.

Comments:
- 10 pages, 10 figures

---

## GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-11-20 | Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li | cs.CV | [PDF](http://arxiv.org/pdf/2311.11700v3){: .btn .btn-green }

**Abstract**: In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
soon.

---

## GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene  Understanding

nerf{: .label .label-blue }

2023-11-20 | Hao Li, Dingwen Zhang, Yalun Dai, Nian Liu, Lechao Cheng, Jingfeng Li, Jingdong Wang, Junwei Han | cs.CV | [PDF](http://arxiv.org/pdf/2311.11863v1){: .btn .btn-green }

**Abstract**: Applying NeRF to downstream perception tasks for scene understanding and
representation is becoming increasingly popular. Most existing methods treat
semantic prediction as an additional rendering task, \textit{i.e.}, the "label
rendering" task, to build semantic NeRFs. However, by rendering
semantic/instance labels per pixel without considering the contextual
information of the rendered image, these methods usually suffer from unclear
boundary segmentation and abnormal segmentation of pixels within an object. To
solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel
pipeline that makes the widely used segmentation model and NeRF work compatibly
under a unified framework, for facilitating context-aware 3D scene perception.
To accomplish this goal, we introduce transformers to aggregate radiance as
well as semantic embedding fields jointly for novel views and facilitate the
joint volumetric rendering of both fields. In addition, we propose two
self-distillation mechanisms, i.e., the Semantic Distill Loss and the
Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality
of the semantic field and the maintenance of geometric consistency. In
evaluation, we conduct experimental comparisons under two perception tasks
(\textit{i.e.} semantic and instance segmentation) using both synthetic and
real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\%,
11.76\%, and 8.47\% on generalized semantic segmentation, finetuning semantic
segmentation, and instance segmentation, respectively.

---

## Entangled View-Epipolar Information Aggregation for Generalizable Neural  Radiance Fields

nerf{: .label .label-blue }

2023-11-20 | Zhiyuan Min, Yawei Luo, Wei Yang, Yuesong Wang, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2311.11845v1){: .btn .btn-green }

**Abstract**: Generalizable NeRF can directly synthesize novel views across new scenes,
eliminating the need for scene-specific retraining in vanilla NeRF. A critical
enabling factor in these approaches is the extraction of a generalizable 3D
representation by aggregating source-view features. In this paper, we propose
an Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF.
Different from existing methods that consider cross-view and along-epipolar
information independently, EVE-NeRF conducts the view-epipolar feature
aggregation in an entangled manner by injecting the scene-invariant appearance
continuity and geometry consistency priors to the aggregation process. Our
approach effectively mitigates the potential lack of inherent geometric and
appearance constraint resulting from one-dimensional interactions, thus further
boosting the 3D representation generalizablity. EVE-NeRF attains
state-of-the-art performance across various evaluation scenarios. Extensive
experiments demonstate that, compared to prevailing single-dimensional
aggregation, the entangled network excels in the accuracy of 3D scene geometry
and appearance reconstruction.Our project page is
https://github.com/tatakai1/EVENeRF.

---

## GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion  Probabilistic Models with Structured Noise

gaussian splatting{: .label .label-blue } nerf{: .label .label-blue }

2023-11-19 | Xinhai Li, Huaibin Wang, Kuo-Kun Tseng | cs.CV | [PDF](http://arxiv.org/pdf/2311.11221v1){: .btn .btn-green }

**Abstract**: Text-to-3D, known for its efficient generation methods and expansive creative
potential, has garnered significant attention in the AIGC domain. However, the
amalgamation of Nerf and 2D diffusion models frequently yields oversaturated
images, posing severe limitations on downstream industrial applications due to
the constraints of pixelwise rendering method. Gaussian splatting has recently
superseded the traditional pointwise sampling technique prevalent in NeRF-based
methodologies, revolutionizing various aspects of 3D reconstruction. This paper
introduces a novel text to 3D content generation framework based on Gaussian
splatting, enabling fine control over image saturation through individual
Gaussian sphere transparencies, thereby producing more realistic images. The
challenge of achieving multi-view consistency in 3D generation significantly
impedes modeling complexity and accuracy. Taking inspiration from SJC, we
explore employing multi-view noise distributions to perturb images generated by
3D Gaussian splatting, aiming to rectify inconsistencies in multi-view
geometry. We ingeniously devise an efficient method to generate noise that
produces Gaussian noise from diverse viewpoints, all originating from a shared
noise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap
models in local minima, causing artifacts like floaters, burrs, or
proliferative elements. To mitigate these issues, we propose the variational
Gaussian splatting technique to enhance the quality and stability of 3D
appearance. To our knowledge, our approach represents the first comprehensive
utilization of Gaussian splatting across the entire spectrum of 3D content
generation processes.

---

## LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval  Score Matching

gaussian splatting{: .label .label-blue }

2023-11-19 | Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2311.11284v3){: .btn .btn-green }

**Abstract**: The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.

Comments:
- The first two authors contributed equally to this work. Our code will
  be available at: https://github.com/EnVision-Research/LucidDreamer

---

## Towards Function Space Mesh Watermarking: Protecting the Copyright of  Signed Distance Fields

nerf{: .label .label-blue }

2023-11-18 | Xingyu Zhu, Guanhui Ye, Chengdong Dong, Xiapu Luo, Xuetao Wei | cs.CV | [PDF](http://arxiv.org/pdf/2311.12059v1){: .btn .btn-green }

**Abstract**: The signed distance field (SDF) represents 3D geometries in continuous
function space. Due to its continuous nature, explicit 3D models (e.g., meshes)
can be extracted from it at arbitrary resolution, which means losing the SDF is
equivalent to losing the mesh. Recent research has shown meshes can also be
extracted from SDF-enhanced neural radiance fields (NeRF). Such a signal raises
an alarm that any implicit neural representation with SDF enhancement can
extract the original mesh, which indicates identifying the SDF's intellectual
property becomes an urgent issue. This paper proposes FuncMark, a robust and
invisible watermarking method to protect the copyright of signed distance
fields by leveraging analytic on-surface deformations to embed binary watermark
messages. Such deformation can survive isosurfacing and thus be inherited by
the extracted meshes for further watermark message decoding. Our method can
recover the message with high-resolution meshes extracted from SDFs and detect
the watermark even when mesh vertices are extremely sparse. Furthermore, our
method is robust even when various distortions (including remeshing) are
encountered. Extensive experiments demonstrate that our \tool significantly
outperforms state-of-the-art approaches and the message is still detectable
even when only 50 vertex samples are given.

---

## SNI-SLAM: Semantic Neural Implicit SLAM

nerf{: .label .label-blue }

2023-11-18 | Siting Zhu, Guangming Wang, Hermann Blum, Jiuming Liu, Liang Song, Marc Pollefeys, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2311.11016v1){: .btn .btn-green }

**Abstract**: We propose SNI-SLAM, a semantic SLAM system utilizing neural implicit
representation, that simultaneously performs accurate semantic mapping,
high-quality surface reconstruction, and robust camera tracking. In this
system, we introduce hierarchical semantic representation to allow multi-level
semantic comprehension for top-down structured semantic mapping of the scene.
In addition, to fully utilize the correlation between multiple attributes of
the environment, we integrate appearance, geometry and semantic features
through cross-attention for feature collaboration. This strategy enables a more
multifaceted understanding of the environment, thereby allowing SNI-SLAM to
remain robust even when single attribute is defective. Then, we design an
internal fusion-based decoder to obtain semantic, RGB, Truncated Signed
Distance Field (TSDF) values from multi-level features for accurate decoding.
Furthermore, we propose a feature loss to update the scene representation at
the feature level. Compared with low-level losses such as RGB loss and depth
loss, our feature loss is capable of guiding the network optimization on a
higher-level. Our SNI-SLAM method demonstrates superior performance over all
recent NeRF-based SLAM methods in terms of mapping and tracking accuracy on
Replica and ScanNet datasets, while also showing excellent capabilities in
accurate semantic segmentation and real-time semantic mapping.

---

## Structure-Aware Sparse-View X-ray 3D Reconstruction

nerf{: .label .label-blue }

2023-11-18 | Yuanhao Cai, Jiahao Wang, Alan Yuille, Zongwei Zhou, Angtian Wang | eess.IV | [PDF](http://arxiv.org/pdf/2311.10959v1){: .btn .btn-green }

**Abstract**: X-ray, known for its ability to reveal internal structures of objects, is
expected to provide richer information for 3D reconstruction than visible
light. Yet, existing neural radiance fields (NeRF) algorithms overlook this
important nature of X-ray, leading to their limitations in capturing structural
contents of imaged objects. In this paper, we propose a framework,
Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view
X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer
(Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal
structures of objects in 3D space by modeling the dependencies within each line
segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray
sampling strategy to extract contextual and geometric information in 2D
projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray
applications. Experiments on X3D show that SAX-NeRF surpasses previous
NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT
reconstruction. Code, models, and data will be released at
https://github.com/caiyuanhao1998/SAX-NeRF

---

## Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis

gaussian splatting{: .label .label-blue }

2023-11-17 | Simon Niedermayr, Josef Stumpfegger, Rüdiger Westermann | cs.CV | [PDF](http://arxiv.org/pdf/2401.02436v1){: .btn .btn-green }

**Abstract**: Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian
splat representation has been introduced for novel view synthesis from sparse
image sets. Making such representations suitable for applications like network
streaming and rendering on low-power devices requires significantly reduced
memory consumption as well as improved rendering efficiency. We propose a
compressed 3D Gaussian splat representation that utilizes sensitivity-aware
vector clustering with quantization-aware training to compress directional
colors and Gaussian parameters. The learned codebooks have low bitrates and
achieve a compression rate of up to $31\times$ on real-world scenes with only
minimal degradation of visual quality. We demonstrate that the compressed splat
representation can be efficiently rendered with hardware rasterization on
lightweight GPUs at up to $4\times$ higher framerates than reported via an
optimized GPU compute pipeline. Extensive experiments across multiple datasets
demonstrate the robustness and rendering speed of the proposed approach.

---

## Removing Adverse Volumetric Effects From Trained Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-17 | Andreas L. Teigen, Mauhing Yip, Victor P. Hamran, Vegard Skui, Annette Stahl, Rudolf Mester | cs.CV | [PDF](http://arxiv.org/pdf/2311.10523v1){: .btn .btn-green }

**Abstract**: While the use of neural radiance fields (NeRFs) in different challenging
settings has been explored, only very recently have there been any
contributions that focus on the use of NeRF in foggy environments. We argue
that the traditional NeRF models are able to replicate scenes filled with fog
and propose a method to remove the fog when synthesizing novel views. By
calculating the global contrast of a scene, we can estimate a density threshold
that, when applied, removes all visible fog. This makes it possible to use NeRF
as a way of rendering clear views of objects of interest located in fog-filled
environments. Additionally, to benchmark performance on such scenes, we
introduce a new dataset that expands some of the original synthetic NeRF scenes
through the addition of fog and natural environments. The code, dataset, and
video results can be found on our project page: https://vegardskui.com/fognerf/

Comments:
- This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible

---

## SplatArmor: Articulated Gaussian splatting for animatable humans from  monocular RGB videos

gaussian splatting{: .label .label-blue }

2023-11-17 | Rohit Jena, Ganesh Subramanian Iyer, Siddharth Choudhary, Brandon Smith, Pratik Chaudhari, James Gee | cs.CV | [PDF](http://arxiv.org/pdf/2311.10812v1){: .btn .btn-green }

**Abstract**: We propose SplatArmor, a novel approach for recovering detailed and
animatable human models by `armoring' a parameterized body model with 3D
Gaussians. Our approach represents the human as a set of 3D Gaussians within a
canonical space, whose articulation is defined by extending the skinning of the
underlying SMPL geometry to arbitrary locations in the canonical space. To
account for pose-dependent effects, we introduce a SE(3) field, which allows us
to capture both the location and anisotropy of the Gaussians. Furthermore, we
propose the use of a neural color field to provide color regularization and 3D
supervision for the precise positioning of these Gaussians. We show that
Gaussian splatting provides an interesting alternative to neural rendering
based methods by leverging a rasterization primitive without facing any of the
non-differentiability and optimization challenges typically faced in such
approaches. The rasterization paradigms allows us to leverage forward skinning,
and does not suffer from the ambiguities associated with inverse skinning and
warping. We show compelling results on the ZJU MoCap and People Snapshot
datasets, which underscore the effectiveness of our method for controllable
human synthesis.

---

## Adaptive Shells for Efficient Neural Radiance Field Rendering



2023-11-16 | Zian Wang, Tianchang Shen, Merlin Nimier-David, Nicholas Sharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas Müller, Zan Gojcic | cs.CV | [PDF](http://arxiv.org/pdf/2311.10091v1){: .btn .btn-green }

**Abstract**: Neural radiance fields achieve unprecedented quality for novel view
synthesis, but their volumetric formulation remains expensive, requiring a huge
number of samples to render high-resolution images. Volumetric encodings are
essential to represent fuzzy geometry such as foliage and hair, and they are
well-suited for stochastic optimization. Yet, many scenes ultimately consist
largely of solid surfaces which can be accurately rendered by a single sample
per pixel. Based on this insight, we propose a neural radiance formulation that
smoothly transitions between volumetric- and surface-based rendering, greatly
accelerating rendering speed and even improving visual fidelity. Our method
constructs an explicit mesh envelope which spatially bounds a neural volumetric
representation. In solid regions, the envelope nearly converges to a surface
and can often be rendered with a single sample. To this end, we generalize the
NeuS formulation with a learned spatially-varying kernel size which encodes the
spread of the density, fitting a wide kernel to volume-like regions and a tight
kernel to surface-like regions. We then extract an explicit mesh of a narrow
band around the surface, with width determined by the kernel size, and
fine-tune the radiance field within this band. At inference time, we cast rays
against the mesh and evaluate the radiance field only within the enclosed
region, greatly reducing the number of samples required. Experiments show that
our approach enables efficient rendering at very high fidelity. We also
demonstrate that the extracted envelope enables downstream applications such as
animation and simulation.

Comments:
- SIGGRAPH Asia 2023. Project page:
  research.nvidia.com/labs/toronto-ai/adaptive-shells/

---

## EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction  on Mobile Devices

nerf{: .label .label-blue }

2023-11-16 | Jingnan Gao, Zhuo Chen, Yichao Yan, Bowen Pan, Zhe Wang, Jiangjing Lyu, Xiaokang Yang | cs.CV | [PDF](http://arxiv.org/pdf/2311.09806v2){: .btn .btn-green }

**Abstract**: Reconstructing real-world 3D objects has numerous applications in computer
vision, such as virtual reality, video games, and animations. Ideally, 3D
reconstruction methods should generate high-fidelity results with 3D
consistency in real-time. Traditional methods match pixels between images using
photo-consistency constraints or learned features, while differentiable
rendering methods like Neural Radiance Fields (NeRF) use differentiable volume
rendering or surface-based representation to generate high-fidelity scenes.
However, these methods require excessive runtime for rendering, making them
impractical for daily applications. To address these challenges, we present
$\textbf{EvaSurf}$, an $\textbf{E}$fficient $\textbf{V}$iew-$\textbf{A}$ware
implicit textured $\textbf{Surf}$ace reconstruction method on mobile devices.
In our method, we first employ an efficient surface-based model with a
multi-view supervision module to ensure accurate mesh reconstruction. To enable
high-fidelity rendering, we learn an implicit texture embedded with a set of
Gaussian lobes to capture view-dependent information. Furthermore, with the
explicit geometry and the implicit texture, we can employ a lightweight neural
shader to reduce the expense of computation and further support real-time
rendering on common mobile devices. Extensive experiments demonstrate that our
method can reconstruct high-quality appearance and accurate mesh on both
synthetic and real-world datasets. Moreover, our method can be trained in just
1-2 hours using a single GPU and run on mobile devices at over 40 FPS (Frames
Per Second), with a final package required for rendering taking up only 40-50
MB.

Comments:
- Project Page: http://g-1nonly.github.io/EvaSurf-Website/

---

## Reconstructing Continuous Light Field From Single Coded Image

nerf{: .label .label-blue }

2023-11-16 | Yuya Ishikawa, Keita Takahashi, Chihiro Tsutake, Toshiaki Fujii | cs.CV | [PDF](http://arxiv.org/pdf/2311.09646v1){: .btn .btn-green }

**Abstract**: We propose a method for reconstructing a continuous light field of a target
scene from a single observed image. Our method takes the best of two worlds:
joint aperture-exposure coding for compressive light-field acquisition, and a
neural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding
implemented in a camera enables effective embedding of 3-D scene information
into an observed image, but in previous works, it was used only for
reconstructing discretized light-field views. NeRF-based neural rendering
enables high quality view synthesis of a 3-D scene from continuous viewpoints,
but when only a single image is given as the input, it struggles to achieve
satisfactory quality. Our method integrates these two techniques into an
efficient and end-to-end trainable pipeline. Trained on a wide variety of
scenes, our method can reconstruct continuous light fields accurately and
efficiently without any test time optimization. To our knowledge, this is the
first work to bridge two worlds: camera design for efficiently acquiring 3-D
information and neural rendering.

---

## Single-Image 3D Human Digitization with Shape-Guided Diffusion

nerf{: .label .label-blue }

2023-11-15 | Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, Jia-Bin Huang | cs.CV | [PDF](http://arxiv.org/pdf/2311.09221v1){: .btn .btn-green }

**Abstract**: We present an approach to generate a 360-degree view of a person with a
consistent, high-resolution appearance from a single input image. NeRF and its
variants typically require videos or images from different viewpoints. Most
existing approaches taking monocular input either rely on ground-truth 3D scans
for supervision or lack 3D consistency. While recent 3D generative models show
promise of 3D consistent human digitization, these approaches do not generalize
well to diverse clothing appearances, and the results lack photorealism. Unlike
existing work, we utilize high-capacity 2D diffusion models pretrained for
general image synthesis tasks as an appearance prior of clothed humans. To
achieve better 3D consistency while retaining the input identity, we
progressively synthesize multiple views of the human in the input image by
inpainting missing regions with shape-guided diffusion conditioned on
silhouette and surface normal. We then fuse these synthesized multi-view images
via inverse rendering to obtain a fully textured high-resolution 3D mesh of the
given person. Experiments show that our approach outperforms prior methods and
achieves photorealistic 360-degree synthesis of a wide range of clothed humans
with complex textures from a single image.

Comments:
- SIGGRAPH Asia 2023. Project website: https://human-sgd.github.io/

---

## DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction  Model

nerf{: .label .label-blue }

2023-11-15 | Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, Kai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2311.09217v1){: .btn .btn-green }

**Abstract**: We propose \textbf{DMV3D}, a novel 3D generation approach that uses a
transformer-based 3D large reconstruction model to denoise multi-view
diffusion. Our reconstruction model incorporates a triplane NeRF representation
and can denoise noisy multi-view images via NeRF reconstruction and rendering,
achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train
\textbf{DMV3D} on large-scale multi-view image datasets of highly diverse
objects using only image reconstruction losses, without accessing 3D assets. We
demonstrate state-of-the-art results for the single-image reconstruction
problem where probabilistic modeling of unseen object parts is required for
generating diverse reconstructions with sharp textures. We also show
high-quality text-to-3D generation results outperforming previous 3D diffusion
models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .

Comments:
- Project Page: https://justimyhxu.github.io/projects/dmv3d/

---

## Spiking NeRF: Representing the Real-World Geometry by a Discontinuous  Representation

nerf{: .label .label-blue }

2023-11-15 | Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan | cs.CV | [PDF](http://arxiv.org/pdf/2311.09077v2){: .btn .btn-green }

**Abstract**: A crucial reason for the success of existing NeRF-based methods is to build a
neural density field for the geometry representation via multiple perceptron
layers (MLPs). MLPs are continuous functions, however, real geometry or density
field is frequently discontinuous at the interface between the air and the
surface. Such a contrary brings the problem of unfaithful geometry
representation. To this end, this paper proposes spiking NeRF, which leverages
spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural
Network (SNN) framework to build a discontinuous density field for faithful
geometry representation. Specifically, we first demonstrate the reason why
continuous density fields will bring inaccuracy. Then, we propose to use the
spiking neurons to build a discontinuous density field. We conduct a
comprehensive analysis for the problem of existing spiking neuron models and
then provide the numerical relationship between the parameter of the spiking
neuron and the theoretical accuracy of geometry. Based on this, we propose a
bounded spiking neuron to build the discontinuous density field. Our method
achieves SOTA performance. The source code and the supplementary material are
available at https://github.com/liaozhanfeng/Spiking-NeRF.

---

## Drivable 3D Gaussian Avatars

gaussian splatting{: .label .label-blue }

2023-11-14 | Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollhöfer, Justus Thies, Javier Romero | cs.CV | [PDF](http://arxiv.org/pdf/2311.08581v1){: .btn .btn-green }

**Abstract**: We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable
model for human bodies rendered with Gaussian splats. Current photorealistic
drivable avatars require either accurate 3D registrations during training,
dense input images during testing, or both. The ones based on neural radiance
fields also tend to be prohibitively slow for telepresence applications. This
work uses the recently presented 3D Gaussian Splatting (3DGS) technique to
render realistic humans at real-time framerates, using dense calibrated
multi-view videos as input. To deform those primitives, we depart from the
commonly used point deformation method of linear blend skinning (LBS) and use a
classic volumetric deformation method: cage deformations. Given their smaller
size, we drive these deformations with joint angles and keypoints, which are
more suitable for communication applications. Our experiments on nine subjects
with varied body shapes, clothes, and motions obtain higher-quality results
than state-of-the-art methods when using the same training and test data.

Comments:
- Website: https://zielon.github.io/d3ga/

---

## $L_0$-Sampler: An $L_{0}$ Model Guided Volume Sampling for NeRF

nerf{: .label .label-blue }

2023-11-13 | Liangchen Li, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2311.07044v1){: .btn .btn-green }

**Abstract**: Since being proposed, Neural Radiance Fields (NeRF) have achieved great
success in related tasks, mainly adopting the hierarchical volume sampling
(HVS) strategy for volume rendering. However, the HVS of NeRF approximates
distributions using piecewise constant functions, which provides a relatively
rough estimation. Based on the observation that a well-trained weight function
$w(t)$ and the $L_0$ distance between points and the surface have very high
similarity, we propose $L_0$-Sampler by incorporating the $L_0$ model into
$w(t)$ to guide the sampling process. Specifically, we propose to use piecewise
exponential functions rather than piecewise constant functions for
interpolation, which can not only approximate quasi-$L_0$ weight distributions
along rays quite well but also can be easily implemented with few lines of code
without additional computational burden. Stable performance improvements can be
achieved by applying $L_0$-Sampler to NeRF and its related tasks like 3D
reconstruction. Code is available at https://ustc3dv.github.io/L0-Sampler/ .

Comments:
- Project page: https://ustc3dv.github.io/L0-Sampler/

---

## Aria-NeRF: Multimodal Egocentric View Synthesis

nerf{: .label .label-blue }

2023-11-11 | Jiankai Sun, Jianing Qiu, Chuanyang Zheng, John Tucker, Javier Yu, Mac Schwager | cs.CV | [PDF](http://arxiv.org/pdf/2311.06455v1){: .btn .btn-green }

**Abstract**: We seek to accelerate research in developing rich, multimodal scene models
trained from egocentric data, based on differentiable volumetric ray-tracing
inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like
model from an egocentric image sequence plays a pivotal role in understanding
human behavior and holds diverse applications within the realms of VR/AR. Such
egocentric NeRF-like models may be used as realistic simulations, contributing
significantly to the advancement of intelligent agents capable of executing
tasks in the real-world. The future of egocentric view synthesis may lead to
novel environment representations going beyond today's NeRFs by augmenting
visual data with multimodal sensors such as IMU for egomotion tracking, audio
sensors to capture surface texture and human language context, and eye-gaze
trackers to infer human attention patterns in the scene. To support and
facilitate the development and evaluation of egocentric multimodal scene
modeling, we present a comprehensive multimodal egocentric video dataset. This
dataset offers a comprehensive collection of sensory data, featuring RGB
images, eye-tracking camera footage, audio recordings from a microphone,
atmospheric pressure readings from a barometer, positional coordinates from
GPS, connectivity details from Wi-Fi and Bluetooth, and information from
dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The
dataset was collected with the Meta Aria Glasses wearable device platform. The
diverse data modalities and the real-world context captured within this dataset
serve as a robust foundation for furthering our understanding of human behavior
and enabling more immersive and intelligent experiences in the realms of VR,
AR, and robotics.

---

## Instant3D: Fast Text-to-3D with Sparse-View Generation and Large  Reconstruction Model

nerf{: .label .label-blue }

2023-11-10 | Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, Sai Bi | cs.CV | [PDF](http://arxiv.org/pdf/2311.06214v2){: .btn .btn-green }

**Abstract**: Text-to-3D with diffusion models has achieved remarkable progress in recent
years. However, existing methods either rely on score distillation-based
optimization which suffer from slow inference, low diversity and Janus
problems, or are feed-forward methods that generate low-quality results due to
the scarcity of 3D training data. In this paper, we propose Instant3D, a novel
method that generates high-quality and diverse 3D assets from text prompts in a
feed-forward manner. We adopt a two-stage paradigm, which first generates a
sparse set of four structured and consistent views from text in one shot with a
fine-tuned 2D text-to-image diffusion model, and then directly regresses the
NeRF from the generated images with a novel transformer-based sparse-view
reconstructor. Through extensive experiments, we demonstrate that our method
can generate diverse 3D assets of high visual quality within 20 seconds, which
is two orders of magnitude faster than previous optimization-based methods that
can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.

Comments:
- Project webpage: https://jiahao.ai/instant3d/

---

## ASSIST: Interactive Scene Nodes for Scalable and Realistic Indoor  Simulation



2023-11-10 | Zhide Zhong, Jiakai Cao, Songen Gu, Sirui Xie, Weibo Gao, Liyi Luo, Zike Yan, Hao Zhao, Guyue Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2311.06211v1){: .btn .btn-green }

**Abstract**: We present ASSIST, an object-wise neural radiance field as a panoptic
representation for compositional and realistic simulation. Central to our
approach is a novel scene node data structure that stores the information of
each object in a unified fashion, allowing online interaction in both intra-
and cross-scene settings. By incorporating a differentiable neural network
along with the associated bounding box and semantic features, the proposed
structure guarantees user-friendly interaction on independent objects to scale
up novel view simulation. Objects in the scene can be queried, added,
duplicated, deleted, transformed, or swapped simply through mouse/keyboard
controls or language instructions. Experiments demonstrate the efficacy of the
proposed method, where scaled realistic simulation can be achieved through
interactive editing and compositional rendering, with color images, depth
images, and panoptic segmentation masks generated in a 3D consistent manner.

---

## A Neural Height-Map Approach for the Binocular Photometric Stereo  Problem

nerf{: .label .label-blue }

2023-11-10 | Fotios Logothetis, Ignas Budvytis, Roberto Cipolla | cs.CV | [PDF](http://arxiv.org/pdf/2311.05958v1){: .btn .btn-green }

**Abstract**: In this work we propose a novel, highly practical, binocular photometric
stereo (PS) framework, which has same acquisition speed as single view PS,
however significantly improves the quality of the estimated geometry.
  As in recent neural multi-view shape estimation frameworks such as NeRF,
SIREN and inverse graphics approaches to multi-view photometric stereo (e.g.
PS-NeRF) we formulate shape estimation task as learning of a differentiable
surface and texture representation by minimising surface normal discrepancy for
normals estimated from multiple varying light images for two views as well as
discrepancy between rendered surface intensity and observed images. Our method
differs from typical multi-view shape estimation approaches in two key ways.
First, our surface is represented not as a volume but as a neural heightmap
where heights of points on a surface are computed by a deep neural network.
Second, instead of predicting an average intensity as PS-NeRF or introducing
lambertian material assumptions as Guo et al., we use a learnt BRDF and perform
near-field per point intensity rendering.
  Our method achieves the state-of-the-art performance on the DiLiGenT-MV
dataset adapted to binocular stereo setup as well as a new binocular
photometric stereo dataset - LUCES-ST.

Comments:
- WACV 2024

---

## UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical  Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-10 | Jing Hu, Qinrui Fan, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang | eess.IV | [PDF](http://arxiv.org/pdf/2311.05836v4){: .btn .btn-green }

**Abstract**: In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.

---

## BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis

nerf{: .label .label-blue }

2023-11-09 | Hao-Bin Duan, Miao Wang, Jin-Chuan Shi, Xu-Chuan Chen, Yan-Pei Cao | cs.GR | [PDF](http://arxiv.org/pdf/2311.05521v2){: .btn .btn-green }

**Abstract**: Synthesizing photorealistic 4D human head avatars from videos is essential
for VR/AR, telepresence, and video game applications. Although existing Neural
Radiance Fields (NeRF)-based methods achieve high-fidelity results, the
computational expense limits their use in real-time applications. To overcome
this limitation, we introduce BakedAvatar, a novel representation for real-time
neural head avatar synthesis, deployable in a standard polygon rasterization
pipeline. Our approach extracts deformable multi-layer meshes from learned
isosurfaces of the head and computes expression-, pose-, and view-dependent
appearances that can be baked into static textures for efficient rasterization.
We thus propose a three-stage pipeline for neural head avatar synthesis, which
includes learning continuous deformation, manifold, and radiance fields,
extracting layered meshes and textures, and fine-tuning texture details with
differential rasterization. Experimental results demonstrate that our
representation generates synthesis results of comparable quality to other
state-of-the-art methods while significantly reducing the inference time
required. We further showcase various head avatar synthesis results from
monocular videos, including view synthesis, face reenactment, expression
editing, and pose editing, all at interactive frame rates.

Comments:
- ACM Transactions on Graphics (SIGGRAPH Asia 2023). Project Page:
  https://buaavrcg.github.io/BakedAvatar

---

## Control3D: Towards Controllable Text-to-3D Generation

nerf{: .label .label-blue }

2023-11-09 | Yang Chen, Yingwei Pan, Yehao Li, Ting Yao, Tao Mei | cs.CV | [PDF](http://arxiv.org/pdf/2311.05461v1){: .btn .btn-green }

**Abstract**: Recent remarkable advances in large-scale text-to-image diffusion models have
inspired a significant breakthrough in text-to-3D generation, pursuing 3D
content creation solely from a given text prompt. However, existing text-to-3D
techniques lack a crucial ability in the creative process: interactively
control and shape the synthetic 3D contents according to users' desired
specifications (e.g., sketch). To alleviate this issue, we present the first
attempt for text-to-3D generation conditioning on the additional hand-drawn
sketch, namely Control3D, which enhances controllability for users. In
particular, a 2D conditioned diffusion model (ControlNet) is remoulded to guide
the learning of 3D scene parameterized as NeRF, encouraging each view of 3D
scene aligned with the given text prompt and hand-drawn sketch. Moreover, we
exploit a pre-trained differentiable photo-to-sketch model to directly estimate
the sketch of the rendered image over synthetic 3D scene. Such estimated sketch
along with each sampled view is further enforced to be geometrically consistent
with the given sketch, pursuing better controllable text-to-3D generation.
Through extensive experiments, we demonstrate that our proposal can generate
accurate and faithful 3D scenes that align closely with the input text prompts
and sketches.

Comments:
- ACM Multimedia 2023

---

## VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for  Enhanced Indoor View Synthesis

nerf{: .label .label-blue }

2023-11-09 | Sen Wang, Wei Zhang, Stefano Gasperini, Shun-Cheng Wu, Nassir Navab | cs.CV | [PDF](http://arxiv.org/pdf/2311.05289v1){: .btn .btn-green }

**Abstract**: Creating high-quality view synthesis is essential for immersive applications
but continues to be problematic, particularly in indoor environments and for
real-time deployment. Current techniques frequently require extensive
computational time for both training and rendering, and often produce
less-than-ideal 3D representations due to inadequate geometric structuring. To
overcome this, we introduce VoxNeRF, a novel approach that leverages volumetric
representations to enhance the quality and efficiency of indoor view synthesis.
Firstly, VoxNeRF constructs a structured scene geometry and converts it into a
voxel-based representation. We employ multi-resolution hash grids to adaptively
capture spatial features, effectively managing occlusions and the intricate
geometry of indoor scenes. Secondly, we propose a unique voxel-guided efficient
sampling technique. This innovation selectively focuses computational resources
on the most relevant portions of ray segments, substantially reducing
optimization time. We validate our approach against three public indoor
datasets and demonstrate that VoxNeRF outperforms state-of-the-art methods.
Remarkably, it achieves these gains while reducing both training and rendering
times, surpassing even Instant-NGP in speed and bringing the technology closer
to real-time.

Comments:
- 8 pages, 4 figures

---

## ConRad: Image Constrained Radiance Fields for 3D Generation from a  Single Image



2023-11-09 | Senthil Purushwalkam, Nikhil Naik | cs.CV | [PDF](http://arxiv.org/pdf/2311.05230v1){: .btn .btn-green }

**Abstract**: We present a novel method for reconstructing 3D objects from a single RGB
image. Our method leverages the latest image generation models to infer the
hidden 3D structure while remaining faithful to the input image. While existing
methods obtain impressive results in generating 3D models from text prompts,
they do not provide an easy approach for conditioning on input RGB data.
Na\"ive extensions of these methods often lead to improper alignment in
appearance between the input image and the 3D reconstructions. We address these
challenges by introducing Image Constrained Radiance Fields (ConRad), a novel
variant of neural radiance fields. ConRad is an efficient 3D representation
that explicitly captures the appearance of an input image in one viewpoint. We
propose a training algorithm that leverages the single RGB image in conjunction
with pretrained Diffusion Models to optimize the parameters of a ConRad
representation. Extensive experiments show that ConRad representations can
simplify preservation of image details while producing a realistic 3D
reconstruction. Compared to existing state-of-the-art baselines, we show that
our 3D reconstructions remain more faithful to the input and produce more
consistent 3D models while demonstrating significantly improved quantitative
performance on a ShapeNet object benchmark.

Comments:
- Advances in Neural Information Processing Systems (NeurIPS 2023)

---

## LRM: Large Reconstruction Model for Single Image to 3D

nerf{: .label .label-blue }

2023-11-08 | Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, Hao Tan | cs.CV | [PDF](http://arxiv.org/pdf/2311.04400v1){: .btn .btn-green }

**Abstract**: We propose the first Large Reconstruction Model (LRM) that predicts the 3D
model of an object from a single input image within just 5 seconds. In contrast
to many previous methods that are trained on small-scale datasets such as
ShapeNet in a category-specific fashion, LRM adopts a highly scalable
transformer-based architecture with 500 million learnable parameters to
directly predict a neural radiance field (NeRF) from the input image. We train
our model in an end-to-end manner on massive multi-view data containing around
1 million objects, including both synthetic renderings from Objaverse and real
captures from MVImgNet. This combination of a high-capacity model and
large-scale training data empowers our model to be highly generalizable and
produce high-quality 3D reconstructions from various testing inputs including
real-world in-the-wild captures and images from generative models. Video demos
and interactable 3D meshes can be found on this website:
https://yiconghong.me/LRM/.

Comments:
- 23 pages

---

## Learning Robust Multi-Scale Representation for Neural Radiance Fields  from Unposed Images



2023-11-08 | Nishant Jain, Suryansh Kumar, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2311.04521v1){: .btn .btn-green }

**Abstract**: We introduce an improved solution to the neural image-based rendering problem
in computer vision. Given a set of images taken from a freely moving camera at
train time, the proposed approach could synthesize a realistic image of the
scene from a novel viewpoint at test time. The key ideas presented in this
paper are (i) Recovering accurate camera parameters via a robust pipeline from
unposed day-to-day images is equally crucial in neural novel view synthesis
problem; (ii) It is rather more practical to model object's content at
different resolutions since dramatic camera motion is highly likely in
day-to-day unposed images. To incorporate the key ideas, we leverage the
fundamentals of scene rigidity, multi-scale neural scene representation, and
single-image depth prediction. Concretely, the proposed approach makes the
camera parameters as learnable in a neural fields-based modeling framework. By
assuming per view depth prediction is given up to scale, we constrain the
relative pose between successive frames. From the relative poses, absolute
camera pose estimation is modeled via a graph-neural network-based multiple
motion averaging within the multi-scale neural-fields network, leading to a
single loss function. Optimizing the introduced loss function provides camera
intrinsic, extrinsic, and image rendering from unposed images. We demonstrate,
with examples, that for a unified framework to accurately model multiscale
neural scene representation from day-to-day acquired unposed multi-view images,
it is equally essential to have precise camera-pose estimates within the scene
representation framework. Without considering robustness measures in the camera
pose estimation pipeline, modeling for multi-scale aliasing artifacts can be
counterproductive. We present extensive experiments on several benchmark
datasets to demonstrate the suitability of our approach.

Comments:
- Accepted for publication at International Journal of Computer Vision
  (IJCV). Draft info: 22 pages, 12 figures and 14 tables

---

## High-fidelity 3D Reconstruction of Plants using Neural Radiance Field

nerf{: .label .label-blue }

2023-11-07 | Kewei Hu, Ying Wei, Yaoqiang Pan, Hanwen Kang, Chao Chen | cs.CV | [PDF](http://arxiv.org/pdf/2311.04154v1){: .btn .btn-green }

**Abstract**: Accurate reconstruction of plant phenotypes plays a key role in optimising
sustainable farming practices in the field of Precision Agriculture (PA).
Currently, optical sensor-based approaches dominate the field, but the need for
high-fidelity 3D reconstruction of crops and plants in unstructured
agricultural environments remains challenging. Recently, a promising
development has emerged in the form of Neural Radiance Field (NeRF), a novel
method that utilises neural density fields. This technique has shown impressive
performance in various novel vision synthesis tasks, but has remained
relatively unexplored in the agricultural context. In our study, we focus on
two fundamental tasks within plant phenotyping: (1) the synthesis of 2D
novel-view images and (2) the 3D reconstruction of crop and plant models. We
explore the world of neural radiance fields, in particular two SOTA methods:
Instant-NGP, which excels in generating high-quality images with impressive
training and inference speed, and Instant-NSR, which improves the reconstructed
geometry by incorporating the Signed Distance Function (SDF) during training.
In particular, we present a novel plant phenotype dataset comprising real plant
images from production environments. This dataset is a first-of-its-kind
initiative aimed at comprehensively exploring the advantages and limitations of
NeRF in agricultural contexts. Our experimental results show that NeRF
demonstrates commendable performance in the synthesis of novel-view images and
is able to achieve reconstruction results that are competitive with Reality
Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based
reconstruction. However, our study also highlights certain drawbacks of NeRF,
including relatively slow training speeds, performance limitations in cases of
insufficient sampling, and challenges in obtaining geometry quality in complex
setups.

---

## Fast Sun-aligned Outdoor Scene Relighting based on TensoRF

nerf{: .label .label-blue }

2023-11-07 | Yeonjin Chang, Yearim Kim, Seunghyeon Seo, Jung Yi, Nojun Kwak | cs.CV | [PDF](http://arxiv.org/pdf/2311.03965v1){: .btn .btn-green }

**Abstract**: In this work, we introduce our method of outdoor scene relighting for Neural
Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF).
SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun,
thereby achieving a simplified workflow that eliminates the need for
environment maps. Our sun-alignment strategy is motivated by the insight that
shadows, unlike viewpoint-dependent albedo, are determined by light direction.
We directly use the sun direction as an input during shadow generation,
simplifying the requirements of the inference process significantly. Moreover,
SR-TensoRF leverages the training efficiency of TensoRF by incorporating our
proposed cubemap concept, resulting in notable acceleration in both training
and rendering processes compared to existing methods.

Comments:
- WACV 2024

---

## UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-07 | Injae Kim, Minhyuk Choi, Hyunwoo J. Kim | cs.CV | [PDF](http://arxiv.org/pdf/2311.03784v2){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) has enabled novel view synthesis with high
fidelity given images and camera poses. Subsequent works even succeeded in
eliminating the necessity of pose priors by jointly optimizing NeRF and camera
pose. However, these works are limited to relatively simple settings such as
photometrically consistent and occluder-free image collections or a sequence of
images from a video. So they have difficulty handling unconstrained images with
varying illumination and transient occluders. In this paper, we propose
$\textbf{UP-NeRF}$ ($\textbf{U}$nconstrained $\textbf{P}$ose-prior-free
$\textbf{Ne}$ural $\textbf{R}$adiance $\textbf{F}$ields) to optimize NeRF with
unconstrained image collections without camera pose prior. We tackle these
challenges with surrogate tasks that optimize color-insensitive feature fields
and a separate module for transient occluders to block their influence on pose
estimation. In addition, we introduce a candidate head to enable more robust
pose estimation and transient-aware depth supervision to minimize the effect of
incorrect prior. Our experiments verify the superior performance of our method
compared to the baselines including BARF and its variants in a challenging
internet photo collection, $\textit{Phototourism}$ dataset.

Comments:
- Neural Information Processing Systems (NeurIPS), 2023. The code is
  available at https://github.com/mlvlab/UP-NeRF

---

## ADFactory: An Effective Framework for Generalizing Optical Flow with  Nerf

nerf{: .label .label-blue }

2023-11-07 | Han Ling | cs.CV | [PDF](http://arxiv.org/pdf/2311.04246v2){: .btn .btn-green }

**Abstract**: A significant challenge facing current optical flow methods is the difficulty
in generalizing them well to the real world. This is mainly due to the high
cost of hand-crafted datasets, and existing self-supervised methods are limited
by indirect loss and occlusions, resulting in fuzzy outcomes. To address this
challenge, we introduce a novel optical flow training framework: automatic data
factory (ADF). ADF only requires RGB images as input to effectively train the
optical flow network on the target data domain. Specifically, we use advanced
Nerf technology to reconstruct scenes from photo groups collected by a
monocular camera, and then calculate optical flow labels between camera pose
pairs based on the rendering results. To eliminate erroneous labels caused by
defects in the scene reconstructed by Nerf, we screened the generated labels
from multiple aspects, such as optical flow matching accuracy, radiation field
confidence, and depth consistency. The filtered labels can be directly used for
network supervision. Experimentally, the generalization ability of ADF on KITTI
surpasses existing self-supervised optical flow and monocular scene flow
algorithms. In addition, ADF achieves impressive results in real-world
zero-point generalization evaluations and surpasses most supervised methods.

Comments:
- 8 pages

---

## Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM  and Next Best View Planning

nerf{: .label .label-blue }

2023-11-06 | Rowan Border, Nived Chebrolu, Yifu Tao, Jonathan D. Gammell, Maurice Fallon | cs.RO | [PDF](http://arxiv.org/pdf/2311.03484v1){: .btn .btn-green }

**Abstract**: Aerial mapping systems are important for many surveying applications (e.g.,
industrial inspection or agricultural monitoring). Semi-autonomous mapping with
GPS-guided aerial platforms that fly preplanned missions is already widely
available but fully autonomous systems can significantly improve efficiency.
Autonomously mapping complex 3D structures requires a system that performs
online mapping and mission planning. This paper presents Osprey, an autonomous
aerial mapping system with state-of-the-art multi-session mapping capabilities.
It enables a non-expert operator to specify a bounded target area that the
aerial platform can then map autonomously, over multiple flights if necessary.
Field experiments with Osprey demonstrate that this system can achieve greater
map coverage of large industrial sites than manual surveys with a pilot-flown
aerial platform or a terrestrial laser scanner (TLS). Three sites, with a total
ground coverage of $7085$ m$^2$ and a maximum height of $27$ m, were mapped in
separate missions using $112$ minutes of autonomous flight time. True colour
maps were created from images captured by Osprey using pointcloud and NeRF
reconstruction methods. These maps provide useful data for structural
inspection tasks.

Comments:
- Submitted to Field Robotics, Manuscript #FR-23-0016. 25 pages, 15
  figures, 3 tables. Video available at
  https://www.youtube.com/watch?v=CVIXu2qUQJ8

---

## Long-Term Invariant Local Features via Implicit Cross-Domain  Correspondences



2023-11-06 | Zador Pataki, Mohammad Altillawi, Menelaos Kanakis, Rémi Pautrat, Fengyi Shen, Ziyuan Liu, Luc Van Gool, Marc Pollefeys | cs.CV | [PDF](http://arxiv.org/pdf/2311.03345v1){: .btn .btn-green }

**Abstract**: Modern learning-based visual feature extraction networks perform well in
intra-domain localization, however, their performance significantly declines
when image pairs are captured across long-term visual domain variations, such
as different seasonal and daytime variations. In this paper, our first
contribution is a benchmark to investigate the performance impact of long-term
variations on visual localization. We conduct a thorough analysis of the
performance of current state-of-the-art feature extraction networks under
various domain changes and find a significant performance gap between intra-
and cross-domain localization. We investigate different methods to close this
gap by improving the supervision of modern feature extractor networks. We
propose a novel data-centric method, Implicit Cross-Domain Correspondences
(iCDC). iCDC represents the same environment with multiple Neural Radiance
Fields, each fitting the scene under individual visual domains. It utilizes the
underlying 3D representations to generate accurate correspondences across
different long-term visual conditions. Our proposed method enhances
cross-domain localization performance, significantly reducing the performance
gap. When evaluated on popular long-term localization benchmarks, our trained
networks consistently outperform existing methods. This work serves as a
substantial stride toward more robust visual localization pipelines for
long-term deployments, and opens up research avenues in the development of
long-term invariant descriptors.

Comments:
- 14 pages + 5 pages appendix, 13 figures

---

## Animating NeRFs from Texture Space: A Framework for Pose-Dependent  Rendering of Human Performances

nerf{: .label .label-blue }

2023-11-06 | Paul Knoll, Wieland Morgenstern, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2311.03140v1){: .btn .btn-green }

**Abstract**: Creating high-quality controllable 3D human models from multi-view RGB videos
poses a significant challenge. Neural radiance fields (NeRFs) have demonstrated
remarkable quality in reconstructing and free-viewpoint rendering of static as
well as dynamic scenes. The extension to a controllable synthesis of dynamic
human performances poses an exciting research question. In this paper, we
introduce a novel NeRF-based framework for pose-dependent rendering of human
performances. In our approach, the radiance field is warped around an SMPL body
mesh, thereby creating a new surface-aligned representation. Our representation
can be animated through skeletal joint parameters that are provided to the NeRF
in addition to the viewpoint for pose dependent appearances. To achieve this,
our representation includes the corresponding 2D UV coordinates on the mesh
texture map and the distance between the query point and the mesh. To enable
efficient learning despite mapping ambiguities and random visual variations, we
introduce a novel remapping process that refines the mapped coordinates.
Experiments demonstrate that our approach results in high-quality renderings
for novel-view and novel-pose synthesis.

---

## Consistent4D: Consistent 360° Dynamic Object Generation from  Monocular Video

nerf{: .label .label-blue }

2023-11-06 | Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2311.02848v1){: .btn .btn-green }

**Abstract**: In this paper, we present Consistent4D, a novel approach for generating 4D
dynamic objects from uncalibrated monocular videos. Uniquely, we cast the
360-degree dynamic object reconstruction as a 4D generation problem,
eliminating the need for tedious multi-view data collection and camera
calibration. This is achieved by leveraging the object-level 3D-aware image
diffusion model as the primary supervision signal for training Dynamic Neural
Radiance Fields (DyNeRF). Specifically, we propose a Cascade DyNeRF to
facilitate stable convergence and temporal continuity under the supervision
signal which is discrete along the time axis. To achieve spatial and temporal
consistency, we further introduce an Interpolation-driven Consistency Loss. It
is optimized by minimizing the discrepancy between rendered frames from DyNeRF
and interpolated frames from a pre-trained video interpolation model. Extensive
experiments show that our Consistent4D can perform competitively to prior art
alternatives, opening up new possibilities for 4D dynamic object generation
from monocular videos, whilst also demonstrating advantage for conventional
text-to-3D generation tasks. Our project page is
https://consistent4d.github.io/.

Comments:
- Technique report. Project page: https://consistent4d.github.io/

---

## InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image

nerf{: .label .label-blue }

2023-11-06 | Jianhui Li, Shilong Liu, Zidong Liu, Yikai Wang, Kaiwen Zheng, Jinghui Xu, Jianmin Li, Jun Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2311.02826v1){: .btn .btn-green }

**Abstract**: With the success of Neural Radiance Field (NeRF) in 3D-aware portrait
editing, a variety of works have achieved promising results regarding both
quality and 3D consistency. However, these methods heavily rely on per-prompt
optimization when handling natural language as editing instructions. Due to the
lack of labeled human face 3D datasets and effective architectures, the area of
human-instructed 3D-aware editing for open-world portraits in an end-to-end
manner remains under-explored. To solve this problem, we propose an end-to-end
diffusion-based framework termed InstructPix2NeRF, which enables instructed
3D-aware portrait editing from a single open-world image with human
instructions. At its core lies a conditional latent 3D diffusion process that
lifts 2D editing to 3D space by learning the correlation between the paired
images' difference and the instructions via triplet data. With the help of our
proposed token position randomization strategy, we could even achieve
multi-semantic editing through one single pass with the portrait identity
well-preserved. Besides, we further propose an identity consistency module that
directly modulates the extracted identity signals into our diffusion process,
which increases the multi-view 3D identity consistency. Extensive experiments
verify the effectiveness of our method and show its superiority against strong
baselines quantitatively and qualitatively.

Comments:
- https://github.com/mybabyyh/InstructPix2NeRF

---

## VR-NeRF: High-Fidelity Virtualized Walkable Spaces

nerf{: .label .label-blue }

2023-11-05 | Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, Aljaž Božič, Dahua Lin, Michael Zollhöfer, Christian Richardt | cs.CV | [PDF](http://arxiv.org/pdf/2311.02542v1){: .btn .btn-green }

**Abstract**: We present an end-to-end system for the high-fidelity capture, model
reconstruction, and real-time rendering of walkable spaces in virtual reality
using neural radiance fields. To this end, we designed and built a custom
multi-camera rig to densely capture walkable spaces in high fidelity and with
multi-view high dynamic range images in unprecedented quality and density. We
extend instant neural graphics primitives with a novel perceptual color space
for learning accurate HDR appearance, and an efficient mip-mapping mechanism
for level-of-detail rendering with anti-aliasing, while carefully optimizing
the trade-off between quality and speed. Our multi-GPU renderer enables
high-fidelity volume rendering of our neural radiance field model at the full
VR resolution of dual 2K$\times$2K at 36 Hz on our custom demo machine. We
demonstrate the quality of our results on our challenging high-fidelity
datasets, and compare our method and datasets to existing baselines. We release
our dataset on our project website.

Comments:
- SIGGRAPH Asia 2023; Project page: https://vr-nerf.github.io

---

## A Neural Radiance Field-Based Architecture for Intelligent Multilayered  View Synthesis



2023-11-03 | D. Dhinakaran, S. M. Udhaya Sankar, G. Elumalai, N. Jagadish kumar | cs.NI | [PDF](http://arxiv.org/pdf/2311.01842v1){: .btn .btn-green }

**Abstract**: A mobile ad hoc network is made up of a number of wireless portable nodes
that spontaneously come together en route for establish a transitory network
with no need for any central management. A mobile ad hoc network (MANET) is
made up of a sizable and reasonably dense community of mobile nodes that travel
across any terrain and rely solely on wireless interfaces for communication,
not on any well before centralized management. Furthermore, routing be supposed
to offer a method for instantly delivering data across a network between any
two nodes. Finding the best packet routing from across infrastructure is the
major issue, though. The proposed protocol's major goal is to identify the
least-expensive nominal capacity acquisition that assures the transportation of
realistic transport that ensures its durability in the event of any node
failure. This study suggests the Optimized Route Selection via Red Imported
Fire Ants (RIFA) Strategy as a way to improve on-demand source routing systems.
Predicting Route Failure and energy Utilization is used to pick the path during
the routing phase. Proposed work assess the results of the comparisons based on
performance parameters like as energy usage, packet delivery rate (PDR), and
end-to-end (E2E) delay. The outcome demonstrates that the proposed strategy is
preferable and increases network lifetime while lowering node energy
consumption and typical E2E delay under the majority of network performance
measures and factors.

---

## Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural  Radiance Fields

nerf{: .label .label-blue }

2023-11-03 | Jianxiong Shen, Ruijie Ren, Adria Ruiz, Francesc Moreno-Noguer | cs.CV | [PDF](http://arxiv.org/pdf/2311.01815v2){: .btn .btn-green }

**Abstract**: Current methods based on Neural Radiance Fields (NeRF) significantly lack the
capacity to quantify uncertainty in their predictions, particularly on the
unseen space including the occluded and outside scene content. This limitation
hinders their extensive applications in robotics, where the reliability of
model predictions has to be considered for tasks such as robotic exploration
and planning in unknown environments. To address this, we propose a novel
approach to estimate a 3D Uncertainty Field based on the learned incomplete
scene geometry, which explicitly identifies these unseen regions. By
considering the accumulated transmittance along each camera ray, our
Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for
rays directly casting towards occluded or outside the scene content. To
quantify the uncertainty on the learned surface, we model a stochastic radiance
field. Our experiments demonstrate that our approach is the only one that can
explicitly reason about high uncertainty both on 3D unseen regions and its
involved 2D rendered pixels, compared with recent methods. Furthermore, we
illustrate that our designed uncertainty field is ideally suited for real-world
robotics tasks, such as next-best-view selection.

---

## PDF: Point Diffusion Implicit Function for Large-scale Scene Neural  Representation

nerf{: .label .label-blue }

2023-11-03 | Yuhan Ding, Fukun Yin, Jiayuan Fan, Hui Li, Xin Chen, Wen Liu, Chongshan Lu, Gang YU, Tao Chen | cs.CV | [PDF](http://arxiv.org/pdf/2311.01773v1){: .btn .btn-green }

**Abstract**: Recent advances in implicit neural representations have achieved impressive
results by sampling and fusing individual points along sampling rays in the
sampling space. However, due to the explosively growing sampling space, finely
representing and synthesizing detailed textures remains a challenge for
unbounded large-scale outdoor scenes. To alleviate the dilemma of using
individual points to perceive the entire colossal space, we explore learning
the surface distribution of the scene to provide structural priors and reduce
the samplable space and propose a Point Diffusion implicit Function, PDF, for
large-scale scene neural representation. The core of our method is a
large-scale point cloud super-resolution diffusion module that enhances the
sparse point cloud reconstructed from several training images into a dense
point cloud as an explicit prior. Then in the rendering stage, only sampling
points with prior points within the sampling radius are retained. That is, the
sampling space is reduced from the unbounded space to the scene surface.
Meanwhile, to fill in the background of the scene that cannot be provided by
point clouds, the region sampling based on Mip-NeRF 360 is employed to model
the background representation. Expensive experiments have demonstrated the
effectiveness of our method for large-scale scene novel view synthesis, which
outperforms relevant state-of-the-art baselines.

Comments:
- Accepted to NeurIPS 2023

---

## Efficient Cloud Pipelines for Neural Radiance Fields

nerf{: .label .label-blue }

2023-11-03 | Derek Jacoby, Donglin Xu, Weder Ribas, Minyi Xu, Ting Liu, Vishwanath Jayaraman, Mengdi Wei, Emma De Blois, Yvonne Coady | cs.CV | [PDF](http://arxiv.org/pdf/2311.01659v1){: .btn .btn-green }

**Abstract**: Since their introduction in 2020, Neural Radiance Fields (NeRFs) have taken
the computer vision community by storm. They provide a multi-view
representation of a scene or object that is ideal for eXtended Reality (XR)
applications and for creative endeavors such as virtual production, as well as
change detection operations in geospatial analytics. The computational cost of
these generative AI models is quite high, however, and the construction of
cloud pipelines to generate NeRFs is neccesary to realize their potential in
client applications. In this paper, we present pipelines on a high performance
academic computing cluster and compare it with a pipeline implemented on
Microsoft Azure. Along the way, we describe some uses of NeRFs in enabling
novel user interaction scenarios.

---

## INeAT: Iterative Neural Adaptive Tomography



2023-11-03 | Bo Xiong, Changqing Su, Zihan Lin, You Zhou, Zhaofei Yu | eess.IV | [PDF](http://arxiv.org/pdf/2311.01653v1){: .btn .btn-green }

**Abstract**: Computed Tomography (CT) with its remarkable capability for three-dimensional
imaging from multiple projections, enjoys a broad range of applications in
clinical diagnosis, scientific observation, and industrial detection. Neural
Adaptive Tomography (NeAT) is a recently proposed 3D rendering method based on
neural radiance field for CT, and it demonstrates superior performance compared
to traditional methods. However, it still faces challenges when dealing with
the substantial perturbations and pose shifts encountered in CT scanning
processes. Here, we propose a neural rendering method for CT reconstruction,
named Iterative Neural Adaptive Tomography (INeAT), which incorporates
iterative posture optimization to effectively counteract the influence of
posture perturbations in data, particularly in cases involving significant
posture variations. Through the implementation of a posture feedback
optimization strategy, INeAT iteratively refines the posture corresponding to
the input images based on the reconstructed 3D volume. We demonstrate that
INeAT achieves artifact-suppressed and resolution-enhanced reconstruction in
scenarios with significant pose disturbances. Furthermore, we show that our
INeAT maintains comparable reconstruction performance to stable-state
acquisitions even using data from unstable-state acquisitions, which
significantly reduces the time required for CT scanning and relaxes the
stringent requirements on imaging hardware systems, underscoring its immense
potential for applications in short-time and low-cost CT technology.

---

## Novel View Synthesis from a Single RGBD Image for Indoor Scenes

nerf{: .label .label-blue }

2023-11-02 | Congrui Hetang, Yuping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2311.01065v1){: .btn .btn-green }

**Abstract**: In this paper, we propose an approach for synthesizing novel view images from
a single RGBD (Red Green Blue-Depth) input. Novel view synthesis (NVS) is an
interesting computer vision task with extensive applications. Methods using
multiple images has been well-studied, exemplary ones include training
scene-specific Neural Radiance Fields (NeRF), or leveraging multi-view stereo
(MVS) and 3D rendering pipelines. However, both are either computationally
intensive or non-generalizable across different scenes, limiting their
practical value. Conversely, the depth information embedded in RGBD images
unlocks 3D potential from a singular view, simplifying NVS. The widespread
availability of compact, affordable stereo cameras, and even LiDARs in
contemporary devices like smartphones, makes capturing RGBD images more
accessible than ever. In our method, we convert an RGBD image into a point
cloud and render it from a different viewpoint, then formulate the NVS task
into an image translation problem. We leveraged generative adversarial networks
to style-transfer the rendered image, achieving a result similar to a
photograph taken from the new perspective. We explore both unsupervised
learning using CycleGAN and supervised learning with Pix2Pix, and demonstrate
the qualitative results. Our method circumvents the limitations of traditional
multi-image techniques, holding significant promise for practical, real-time
applications in NVS.

Comments:
- 2nd International Conference on Image Processing, Computer Vision and
  Machine Learning, November 2023

---

## FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance  Fields by Analyzing and Enhancing Fourier PlenOctrees

nerf{: .label .label-blue }

2023-10-31 | Saskia Rabich, Patrick Stotko, Reinhard Klein | cs.CV | [PDF](http://arxiv.org/pdf/2310.20710v1){: .btn .btn-green }

**Abstract**: Fourier PlenOctrees have shown to be an efficient representation for
real-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many
advantages, this method suffers from artifacts introduced by the involved
compression when combining it with recent state-of-the-art techniques for
training the static per-frame NeRF models. In this paper, we perform an
in-depth analysis of these artifacts and leverage the resulting insights to
propose an improved representation. In particular, we present a novel density
encoding that adapts the Fourier-based compression to the characteristics of
the transfer function used by the underlying volume rendering procedure and
leads to a substantial reduction of artifacts in the dynamic model.
Furthermore, we show an augmentation of the training data that relaxes the
periodicity assumption of the compression. We demonstrate the effectiveness of
our enhanced Fourier PlenOctrees in the scope of quantitative and qualitative
evaluations on synthetic and real-world scenes.

---

## NeRF Revisited: Fixing Quadrature Instability in Volume Rendering

nerf{: .label .label-blue }

2023-10-31 | Mikaela Angelina Uy, Kiyohiro Nakayama, Guandao Yang, Rahul Krishna Thomas, Leonidas Guibas, Ke Li | cs.CV | [PDF](http://arxiv.org/pdf/2310.20685v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) rely on volume rendering to synthesize novel
views. Volume rendering requires evaluating an integral along each ray, which
is numerically approximated with a finite sum that corresponds to the exact
integral along the ray under piecewise constant volume density. As a
consequence, the rendered result is unstable w.r.t. the choice of samples along
the ray, a phenomenon that we dub quadrature instability. We propose a
mathematically principled solution by reformulating the sample-based rendering
equation so that it corresponds to the exact integral under piecewise linear
volume density. This simultaneously resolves multiple issues: conflicts between
samples along different rays, imprecise hierarchical sampling, and
non-differentiability of quantiles of ray termination distances w.r.t. model
parameters. We demonstrate several benefits over the classical sample-based
rendering equation, such as sharper textures, better geometric reconstruction,
and stronger depth supervision. Our proposed formulation can be also be used as
a drop-in replacement to the volume rendering equation of existing NeRF-based
methods. Our project page can be found at pl-nerf.github.io.

Comments:
- Neurips 2023

---

## Dynamic Gaussian Splatting from Markerless Motion Capture can  Reconstruct Infants Movements

gaussian splatting{: .label .label-blue }

2023-10-30 | R. James Cotton, Colleen Peyton | cs.CV | [PDF](http://arxiv.org/pdf/2310.19441v1){: .btn .btn-green }

**Abstract**: Easy access to precise 3D tracking of movement could benefit many aspects of
rehabilitation. A challenge to achieving this goal is that while there are many
datasets and pretrained algorithms for able-bodied adults, algorithms trained
on these datasets often fail to generalize to clinical populations including
people with disabilities, infants, and neonates. Reliable movement analysis of
infants and neonates is important as spontaneous movement behavior is an
important indicator of neurological function and neurodevelopmental disability,
which can help guide early interventions. We explored the application of
dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our
approach leverages semantic segmentation masks to focus on the infant,
significantly improving the initialization of the scene. Our results
demonstrate the potential of this method in rendering novel views of scenes and
tracking infant movements. This work paves the way for advanced movement
analysis tools that can be applied to diverse clinical populations, with a
particular emphasis on early detection in infants.

---

## Generative Neural Fields by Mixtures of Neural Implicit Functions

nerf{: .label .label-blue }

2023-10-30 | Tackgeun You, Mijeong Kim, Jungtaek Kim, Bohyung Han | cs.LG | [PDF](http://arxiv.org/pdf/2310.19464v1){: .btn .btn-green }

**Abstract**: We propose a novel approach to learning the generative neural fields
represented by linear combinations of implicit basis networks. Our algorithm
learns basis networks in the form of implicit neural representations and their
coefficients in a latent space by either conducting meta-learning or adopting
auto-decoding paradigms. The proposed method easily enlarges the capacity of
generative neural fields by increasing the number of basis networks while
maintaining the size of a network for inference to be small through their
weighted model averaging. Consequently, sampling instances using the model is
efficient in terms of latency and memory footprint. Moreover, we customize
denoising diffusion probabilistic model for a target task to sample latent
mixture coefficients, which allows our final model to generate unseen data
effectively. Experiments show that our approach achieves competitive generation
performance on diverse benchmarks for images, voxel data, and NeRF scenes
without sophisticated designs for specific modalities and domains.

---

## SeamlessNeRF: Stitching Part NeRFs with Gradient Propagation

nerf{: .label .label-blue }

2023-10-30 | Bingchen Gong, Yuehao Wang, Xiaoguang Han, Qi Dou | cs.CV | [PDF](http://arxiv.org/pdf/2311.16127v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as promising digital mediums of
3D objects and scenes, sparking a surge in research to extend the editing
capabilities in this domain. The task of seamless editing and merging of
multiple NeRFs, resembling the ``Poisson blending'' in 2D image editing,
remains a critical operation that is under-explored by existing work. To fill
this gap, we propose SeamlessNeRF, a novel approach for seamless appearance
blending of multiple NeRFs. In specific, we aim to optimize the appearance of a
target radiance field in order to harmonize its merge with a source field. We
propose a well-tailored optimization procedure for blending, which is
constrained by 1) pinning the radiance color in the intersecting boundary area
between the source and target fields and 2) maintaining the original gradient
of the target. Extensive experiments validate that our approach can effectively
propagate the source appearance from the boundary area to the entire target
field through the gradients. To the best of our knowledge, SeamlessNeRF is the
first work that introduces gradient-guided appearance editing to radiance
fields, offering solutions for seamless stitching of 3D objects represented in
NeRFs.

Comments:
- To appear in SIGGRAPH Asia 2023. Project website is accessible at
  https://sites.google.com/view/seamlessnerf

---

## DynPoint: Dynamic Neural Point For View Synthesis



2023-10-29 | Kaichen Zhou, Jia-Xing Zhong, Sangyun Shin, Kai Lu, Yiyuan Yang, Andrew Markham, Niki Trigoni | cs.CV | [PDF](http://arxiv.org/pdf/2310.18999v2){: .btn .btn-green }

**Abstract**: The introduction of neural radiance fields has greatly improved the
effectiveness of view synthesis for monocular videos. However, existing
algorithms face difficulties when dealing with uncontrolled or lengthy
scenarios, and require extensive training time specific to each new scenario.
To tackle these limitations, we propose DynPoint, an algorithm designed to
facilitate the rapid synthesis of novel views for unconstrained monocular
videos. Rather than encoding the entirety of the scenario information into a
latent representation, DynPoint concentrates on predicting the explicit 3D
correspondence between neighboring frames to realize information aggregation.
Specifically, this correspondence prediction is achieved through the estimation
of consistent depth and scene flow information across frames. Subsequently, the
acquired correspondence is utilized to aggregate information from multiple
reference frames to a target frame, by constructing hierarchical neural point
clouds. The resulting framework enables swift and accurate view synthesis for
desired views of target frames. The experimental results obtained demonstrate
the considerable acceleration of training time achieved - typically an order of
magnitude - by our proposed method while yielding comparable outcomes compared
to prior approaches. Furthermore, our method exhibits strong robustness in
handling long-duration videos without learning a canonical representation of
video content.

---

## TiV-NeRF: Tracking and Mapping via Time-Varying Representation with  Dynamic Neural Radiance Fields

nerf{: .label .label-blue }

2023-10-29 | Chengyao Duan, Zhiliu Yang | cs.CV | [PDF](http://arxiv.org/pdf/2310.18917v2){: .btn .btn-green }

**Abstract**: Previous attempts to integrate Neural Radiance Fields (NeRF) into
Simultaneous Localization and Mapping (SLAM) framework either rely on the
assumption of static scenes or treat dynamic objects as outliers. However, most
of real-world scenarios is dynamic. In this paper, we propose a time-varying
representation to track and reconstruct the dynamic scenes. Our system
simultaneously maintains two processes, tracking process and mapping process.
For tracking process, the entire input images are uniformly sampled and
training of the RGB images are self-supervised. For mapping process, we
leverage know masks to differentiate dynamic objects and static backgrounds,
and we apply distinct sampling strategies for two types of areas. The
parameters optimization for both processes are made up by two stages, the first
stage associates time with 3D positions to convert the deformation field to the
canonical field. And the second associates time with 3D positions in canonical
field to obtain colors and Signed Distance Function (SDF). Besides, We propose
a novel keyframe selection strategy based on the overlapping rate. We evaluate
our approach on two publicly available synthetic datasets and validate that our
method is more effective compared to current state-of-the-art dynamic mapping
methods.

---

## INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings

nerf{: .label .label-blue }

2023-10-28 | Amirhossein Kazerouni, Reza Azad, Alireza Hosseini, Dorit Merhof, Ulas Bagci | cs.CV | [PDF](http://arxiv.org/pdf/2310.18846v1){: .btn .btn-green }

**Abstract**: Implicit Neural Representations (INRs) have revolutionized signal
representation by leveraging neural networks to provide continuous and smooth
representations of complex data. However, existing INRs face limitations in
capturing fine-grained details, handling noise, and adapting to diverse signal
types. To address these challenges, we introduce INCODE, a novel approach that
enhances the control of the sinusoidal-based activation function in INRs using
deep prior knowledge. INCODE comprises a harmonizer network and a composer
network, where the harmonizer network dynamically adjusts key parameters of the
activation function. Through a task-specific pre-trained model, INCODE adapts
the task-specific parameters to optimize the representation process. Our
approach not only excels in representation, but also extends its prowess to
tackle complex tasks such as audio, image, and 3D shape reconstructions, as
well as intricate challenges such as neural radiance fields (NeRFs), and
inverse problems, including denoising, super-resolution, inpainting, and CT
reconstruction. Through comprehensive experiments, INCODE demonstrates its
superiority in terms of robustness, accuracy, quality, and convergence rate,
broadening the scope of signal representation. Please visit the project's
website for details on the proposed method and access to the code.

Comments:
- Accepted at WACV 2024 conference

---

## ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image

nerf{: .label .label-blue }

2023-10-27 | Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, Jiajun Wu | cs.CV | [PDF](http://arxiv.org/pdf/2310.17994v1){: .btn .btn-green }

**Abstract**: We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view
synthesis for in-the-wild scenes. While existing methods are designed for
single objects with masked backgrounds, we propose new techniques to address
challenges introduced by in-the-wild multi-object scenes with complex
backgrounds. Specifically, we train a generative prior on a mixture of data
sources that capture object-centric, indoor, and outdoor scenes. To address
issues from data mixture such as depth-scale ambiguity, we propose a novel
camera conditioning parameterization and normalization scheme. Further, we
observe that Score Distillation Sampling (SDS) tends to truncate the
distribution of complex backgrounds during distillation of 360-degree scenes,
and propose "SDS anchoring" to improve the diversity of synthesized novel
views. Our model sets a new state-of-the-art result in LPIPS on the DTU dataset
in the zero-shot setting, even outperforming methods specifically trained on
DTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmark
for single-image novel view synthesis, and demonstrate strong performance in
this setting. Our code and data are at http://kylesargent.github.io/zeronvs/

Comments:
- 17 pages

---

## Reconstructive Latent-Space Neural Radiance Fields for Efficient 3D  Scene Representations

nerf{: .label .label-blue }

2023-10-27 | Tristan Aumentado-Armstrong, Ashkan Mirzaei, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2310.17880v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have proven to be powerful 3D representations,
capable of high quality novel view synthesis of complex scenes. While NeRFs
have been applied to graphics, vision, and robotics, problems with slow
rendering speed and characteristic visual artifacts prevent adoption in many
use cases. In this work, we investigate combining an autoencoder (AE) with a
NeRF, in which latent features (instead of colours) are rendered and then
convolutionally decoded. The resulting latent-space NeRF can produce novel
views with higher quality than standard colour-space NeRFs, as the AE can
correct certain visual artifacts, while rendering over three times faster. Our
work is orthogonal to other techniques for improving NeRF efficiency. Further,
we can control the tradeoff between efficiency and image quality by shrinking
the AE architecture, achieving over 13 times faster rendering with only a small
drop in performance. We hope that our approach can form the basis of an
efficient, yet high-fidelity, 3D scene representation for downstream tasks,
especially when retaining differentiability is useful, as in many robotics
scenarios requiring continual learning.

---

## HyperFields: Towards Zero-Shot Generation of NeRFs from Text

nerf{: .label .label-blue }

2023-10-26 | Sudarshan Babu, Richard Liu, Avery Zhou, Michael Maire, Greg Shakhnarovich, Rana Hanocka | cs.CV | [PDF](http://arxiv.org/pdf/2310.17075v2){: .btn .btn-green }

**Abstract**: We introduce HyperFields, a method for generating text-conditioned Neural
Radiance Fields (NeRFs) with a single forward pass and (optionally) some
fine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns
a smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF
distillation training, which distills scenes encoded in individual NeRFs into
one dynamic hypernetwork. These techniques enable a single network to fit over
a hundred unique scenes. We further demonstrate that HyperFields learns a more
general map between text and NeRFs, and consequently is capable of predicting
novel in-distribution and out-of-distribution scenes -- either zero-shot or
with a few finetuning steps. Finetuning HyperFields benefits from accelerated
convergence thanks to the learned general map, and is capable of synthesizing
novel scenes 5 to 10 times faster than existing neural optimization-based
methods. Our ablation experiments show that both the dynamic architecture and
NeRF distillation are critical to the expressivity of HyperFields.

Comments:
- Project page: https://threedle.github.io/hyperfields/

---

## Open-NeRF: Towards Open Vocabulary NeRF Decomposition

nerf{: .label .label-blue }

2023-10-25 | Hao Zhang, Fang Li, Narendra Ahuja | cs.CV | [PDF](http://arxiv.org/pdf/2310.16383v1){: .btn .btn-green }

**Abstract**: In this paper, we address the challenge of decomposing Neural Radiance Fields
(NeRF) into objects from an open vocabulary, a critical task for object
manipulation in 3D reconstruction and view synthesis. Current techniques for
NeRF decomposition involve a trade-off between the flexibility of processing
open-vocabulary queries and the accuracy of 3D segmentation. We present,
Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage
large-scale, off-the-shelf, segmentation models like the Segment Anything Model
(SAM) and introduce an integrate-and-distill paradigm with hierarchical
embeddings to achieve both the flexibility of open-vocabulary querying and 3D
segmentation accuracy. Open-NeRF first utilizes large-scale foundation models
to generate hierarchical 2D mask proposals from varying viewpoints. These
proposals are then aligned via tracking approaches and integrated within the 3D
space and subsequently distilled into the 3D field. This process ensures
consistent recognition and granularity of objects from different viewpoints,
even in challenging scenarios involving occlusion and indistinct features. Our
experimental results show that the proposed Open-NeRF outperforms
state-of-the-art methods such as LERF \cite{lerf} and FFD \cite{ffd} in
open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF
decomposition, guided by open-vocabulary queries, enabling novel applications
in robotics and vision-language interaction in open-world 3D scenes.

Comments:
- Accepted by WACV 2024

---

## UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception

nerf{: .label .label-blue }

2023-10-25 | Christopher Maxey, Jaehoon Choi, Hyungtae Lee, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2310.16255v1){: .btn .btn-green }

**Abstract**: Tremendous variations coupled with large degrees of freedom in UAV-based
imaging conditions lead to a significant lack of data in adequately learning
UAV-based perception models. Using various synthetic renderers in conjunction
with perception models is prevalent to create synthetic data to augment the
learning in the ground-based imaging domain. However, severe challenges in the
austere UAV-based domain require distinctive solutions to image synthesis for
data augmentation. In this work, we leverage recent advancements in neural
rendering to improve static and dynamic novelview UAV-based image synthesis,
especially from high altitudes, capturing salient scene attributes. Finally, we
demonstrate a considerable performance boost is achieved when a state-ofthe-art
detection model is optimized primarily on hybrid sets of real and synthetic
data instead of the real or synthetic data separately.

Comments:
- Video Link: https://www.youtube.com/watch?v=ucPzbPLqqpI

---

## PERF: Panoramic Neural Radiance Field from a Single Panorama

nerf{: .label .label-blue }

2023-10-25 | Guangcong Wang, Peng Wang, Zhaoxi Chen, Wenping Wang, Chen Change Loy, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2310.16831v2){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) has achieved substantial progress in novel view
synthesis given multi-view images. Recently, some works have attempted to train
a NeRF from a single image with 3D priors. They mainly focus on a limited field
of view with a few occlusions, which greatly limits their scalability to
real-world 360-degree panoramic scenarios with large-size occlusions. In this
paper, we present PERF, a 360-degree novel view synthesis framework that trains
a panoramic neural radiance field from a single panorama. Notably, PERF allows
3D roaming in a complex scene without expensive and tedious image collection.
To achieve this goal, we propose a novel collaborative RGBD inpainting method
and a progressive inpainting-and-erasing method to lift up a 360-degree 2D
scene to a 3D scene. Specifically, we first predict a panoramic depth map as
initialization given a single panorama and reconstruct visible 3D regions with
volume rendering. Then we introduce a collaborative RGBD inpainting approach
into a NeRF for completing RGB images and depth maps from random views, which
is derived from an RGB Stable Diffusion model and a monocular depth estimator.
Finally, we introduce an inpainting-and-erasing strategy to avoid inconsistent
geometry between a newly-sampled view and reference views. The two components
are integrated into the learning of NeRFs in a unified optimization framework
and achieve promising results. Extensive experiments on Replica and a new
dataset PERF-in-the-wild demonstrate the superiority of our PERF over
state-of-the-art methods. Our PERF can be widely used for real-world
applications, such as panorama-to-3D, text-to-3D, and 3D scene stylization
applications. Project page and code are available at
https://perf-project.github.io/ and https://github.com/perf-project/PeRF.

Comments:
- Project Page: https://perf-project.github.io/ , Code:
  https://github.com/perf-project/PeRF

---

## LightSpeed: Light and Fast Neural Light Fields on Mobile Devices

nerf{: .label .label-blue }

2023-10-25 | Aarush Gupta, Junli Cao, Chaoyang Wang, Ju Hu, Sergey Tulyakov, Jian Ren, László A Jeni | cs.CV | [PDF](http://arxiv.org/pdf/2310.16832v2){: .btn .btn-green }

**Abstract**: Real-time novel-view image synthesis on mobile devices is prohibitive due to
the limited computational power and storage. Using volumetric rendering
methods, such as NeRF and its derivatives, on mobile devices is not suitable
due to the high computational cost of volumetric rendering. On the other hand,
recent advances in neural light field representations have shown promising
real-time view synthesis results on mobile devices. Neural light field methods
learn a direct mapping from a ray representation to the pixel color. The
current choice of ray representation is either stratified ray sampling or
Plucker coordinates, overlooking the classic light slab (two-plane)
representation, the preferred representation to interpolate between light field
views. In this work, we find that using the light slab representation is an
efficient representation for learning a neural light field. More importantly,
it is a lower-dimensional ray representation enabling us to learn the 4D ray
space using feature grids which are significantly faster to train and render.
Although mostly designed for frontal views, we show that the light-slab
representation can be further extended to non-frontal scenes using a
divide-and-conquer strategy. Our method offers superior rendering quality
compared to previous light field methods and achieves a significantly improved
trade-off between rendering quality and speed.

Comments:
- Project Page: http://lightspeed-r2l.github.io/ . Add camera ready
  version

---

## 4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance  Fields via Semantic Distillation

nerf{: .label .label-blue }

2023-10-25 | Dadong Jiang, Zhihui Ke, Xiaobo Zhou, Xidong Shi | cs.CV | [PDF](http://arxiv.org/pdf/2310.16858v2){: .btn .btn-green }

**Abstract**: This paper targets interactive object-level editing (e.g., deletion,
recoloring, transformation, composition) in dynamic scenes. Recently, some
methods aiming for flexible editing static scenes represented by neural
radiance field (NeRF) have shown impressive synthesis quality, while similar
capabilities in time-variant dynamic scenes remain limited. To solve this
problem, we propose 4D-Editor, an interactive semantic-driven editing
framework, allowing editing multiple objects in a dynamic NeRF with user
strokes on a single frame. We propose an extension to the original dynamic NeRF
by incorporating a hybrid semantic feature distillation to maintain
spatial-temporal consistency after editing. In addition, we design Recursive
Selection Refinement that significantly boosts object segmentation accuracy
within a dynamic NeRF to aid the editing process. Moreover, we develop
Multi-view Reprojection Inpainting to fill holes caused by incomplete scene
capture after editing. Extensive experiments and editing examples on real-world
demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs.
Project page: https://patrickddj.github.io/4D-Editor

Comments:
- Project page: https://patrickddj.github.io/4D-Editor

---

## Cross-view Self-localization from Synthesized Scene-graphs

nerf{: .label .label-blue }

2023-10-24 | Ryogo Yamamoto, Kanji Tanaka | cs.CV | [PDF](http://arxiv.org/pdf/2310.15504v1){: .btn .btn-green }

**Abstract**: Cross-view self-localization is a challenging scenario of visual place
recognition in which database images are provided from sparse viewpoints.
Recently, an approach for synthesizing database images from unseen viewpoints
using NeRF (Neural Radiance Fields) technology has emerged with impressive
performance. However, synthesized images provided by these techniques are often
of lower quality than the original images, and furthermore they significantly
increase the storage cost of the database. In this study, we explore a new
hybrid scene model that combines the advantages of view-invariant appearance
features computed from raw images and view-dependent spatial-semantic features
computed from synthesized images. These two types of features are then fused
into scene graphs, and compressively learned and recognized by a graph neural
network. The effectiveness of the proposed method was verified using a novel
cross-view self-localization dataset with many unseen views generated using a
photorealistic Habitat simulator.

Comments:
- 5 pages, 5 figures, technical report

---

## VQ-NeRF: Vector Quantization Enhances Implicit Neural Representations

nerf{: .label .label-blue }

2023-10-23 | Yiying Yang, Wen Liu, Fukun Yin, Xin Chen, Gang Yu, Jiayuan Fan, Tao Chen | cs.CV | [PDF](http://arxiv.org/pdf/2310.14487v1){: .btn .btn-green }

**Abstract**: Recent advancements in implicit neural representations have contributed to
high-fidelity surface reconstruction and photorealistic novel view synthesis.
However, the computational complexity inherent in these methodologies presents
a substantial impediment, constraining the attainable frame rates and
resolutions in practical applications. In response to this predicament, we
propose VQ-NeRF, an effective and efficient pipeline for enhancing implicit
neural representations via vector quantization. The essence of our method
involves reducing the sampling space of NeRF to a lower resolution and
subsequently reinstating it to the original size utilizing a pre-trained VAE
decoder, thereby effectively mitigating the sampling time bottleneck
encountered during rendering. Although the codebook furnishes representative
features, reconstructing fine texture details of the scene remains challenging
due to high compression rates. To overcome this constraint, we design an
innovative multi-scale NeRF sampling scheme that concurrently optimizes the
NeRF model at both compressed and original scales to enhance the network's
ability to preserve fine details. Furthermore, we incorporate a semantic loss
function to improve the geometric fidelity and semantic coherence of our 3D
reconstructions. Extensive experiments demonstrate the effectiveness of our
model in achieving the optimal trade-off between rendering quality and
efficiency. Evaluation on the DTU, BlendMVS, and H3DS datasets confirms the
superior performance of our approach.

Comments:
- Submitted to the 38th Annual AAAI Conference on Artificial
  Intelligence

---

## CAwa-NeRF: Instant Learning of Compression-Aware NeRF Features

nerf{: .label .label-blue }

2023-10-23 | Omnia Mahmoud, Théo Ladune, Matthieu Gendrin | cs.CV | [PDF](http://arxiv.org/pdf/2310.14695v1){: .btn .btn-green }

**Abstract**: Modeling 3D scenes by volumetric feature grids is one of the promising
directions of neural approximations to improve Neural Radiance Fields (NeRF).
Instant-NGP (INGP) introduced multi-resolution hash encoding from a lookup
table of trainable feature grids which enabled learning high-quality neural
graphics primitives in a matter of seconds. However, this improvement came at
the cost of higher storage size. In this paper, we address this challenge by
introducing instant learning of compression-aware NeRF features (CAwa-NeRF),
that allows exporting the zip compressed feature grids at the end of the model
training with a negligible extra time overhead without changing neither the
storage architecture nor the parameters used in the original INGP paper.
Nonetheless, the proposed method is not limited to INGP but could also be
adapted to any model. By means of extensive simulations, our proposed instant
learning pipeline can achieve impressive results on different kinds of static
scenes such as single object masked background scenes and real-life scenes
captured in our studio. In particular, for single object masked background
scenes CAwa-NeRF compresses the feature grids down to 6% (1.2 MB) of the
original size without any loss in the PSNR (33 dB) or down to 2.4% (0.53 MB)
with a slight virtual loss (32.31 dB).

Comments:
- 10 pages, 9 figures

---

## ManifoldNeRF: View-dependent Image Feature Supervision for Few-shot  Neural Radiance Fields

nerf{: .label .label-blue }

2023-10-20 | Daiju Kanaoka, Motoharu Sonogashira, Hakaru Tamukoh, Yasutomo Kawanishi | cs.CV | [PDF](http://arxiv.org/pdf/2310.13670v1){: .btn .btn-green }

**Abstract**: Novel view synthesis has recently made significant progress with the advent
of Neural Radiance Fields (NeRF). DietNeRF is an extension of NeRF that aims to
achieve this task from only a few images by introducing a new loss function for
unknown viewpoints with no input images. The loss function assumes that a
pre-trained feature extractor should output the same feature even if input
images are captured at different viewpoints since the images contain the same
object. However, while that assumption is ideal, in reality, it is known that
as viewpoints continuously change, also feature vectors continuously change.
Thus, the assumption can harm training. To avoid this harmful training, we
propose ManifoldNeRF, a method for supervising feature vectors at unknown
viewpoints using interpolated features from neighboring known viewpoints. Since
the method provides appropriate supervision for each unknown viewpoint by the
interpolated features, the volume representation is learned better than
DietNeRF. Experimental results show that the proposed method performs better
than others in a complex scene. We also experimented with several subsets of
viewpoints from a set of viewpoints and identified an effective set of
viewpoints for real environments. This provided a basic policy of viewpoint
patterns for real-world application. The code is available at
https://github.com/haganelego/ManifoldNeRF_BMVC2023

Comments:
- Accepted by BMVC2023

---

## Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos

nerf{: .label .label-blue }

2023-10-20 | Seoha Kim, Jeongmin Bae, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2310.13356v2){: .btn .btn-green }

**Abstract**: Recent advancements in 4D scene reconstruction using neural radiance fields
(NeRF) have demonstrated the ability to represent dynamic scenes from
multi-view videos. However, they fail to reconstruct the dynamic scenes and
struggle to fit even the training views in unsynchronized settings. It happens
because they employ a single latent embedding for a frame while the multi-view
images at the same frame were actually captured at different moments. To
address this limitation, we introduce time offsets for individual
unsynchronized videos and jointly optimize the offsets with NeRF. By design,
our method is applicable for various baselines and improves them with large
margins. Furthermore, finding the offsets naturally works as synchronizing the
videos without manual effort. Experiments are conducted on the common Plenoptic
Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to
verify the performance of our method. Project page:
https://seoha-kim.github.io/sync-nerf

Comments:
- AAAI 2024, Project page: https://seoha-kim.github.io/sync-nerf

---

## UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale  Scene

nerf{: .label .label-blue }

2023-10-20 | Jiaming Gu, Minchao Jiang, Hongsheng Li, Xiaoyuan Lu, Guangming Zhu, Syed Afaq Ali Shah, Liang Zhang, Mohammed Bennamoun | cs.CV | [PDF](http://arxiv.org/pdf/2310.13263v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) is a novel implicit 3D reconstruction method
that shows immense potential and has been gaining increasing attention. It
enables the reconstruction of 3D scenes solely from a set of photographs.
However, its real-time rendering capability, especially for interactive
real-time rendering of large-scale scenes, still has significant limitations.
To address these challenges, in this paper, we propose a novel neural rendering
system called UE4-NeRF, specifically designed for real-time rendering of
large-scale scenes. We partitioned each large scene into different sub-NeRFs.
In order to represent the partitioned independent scene, we initialize
polygonal meshes by constructing multiple regular octahedra within the scene
and the vertices of the polygonal faces are continuously optimized during the
training process. Drawing inspiration from Level of Detail (LOD) techniques, we
trained meshes of varying levels of detail for different observation levels.
Our approach combines with the rasterization pipeline in Unreal Engine 4 (UE4),
achieving real-time rendering of large-scale scenes at 4K resolution with a
frame rate of up to 43 FPS. Rendering within UE4 also facilitates scene editing
in subsequent stages. Furthermore, through experiments, we have demonstrated
that our method achieves rendering quality comparable to state-of-the-art
approaches. Project page: https://jamchaos.github.io/UE4-NeRF/.

Comments:
- Accepted by NeurIPS2023

---

## VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector  Quantization

nerf{: .label .label-blue }

2023-10-18 | Hongliang Zhong, Jingbo Zhang, Jing Liao | cs.CV | [PDF](http://arxiv.org/pdf/2310.11864v3){: .btn .btn-green }

**Abstract**: We propose VQ-NeRF, a two-branch neural network model that incorporates
Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes.
Conventional neural reflectance fields use only continuous representations to
model 3D scenes, despite the fact that objects are typically composed of
discrete materials in reality. This lack of discretization can result in noisy
material decomposition and complicated material editing. To address these
limitations, our model consists of a continuous branch and a discrete branch.
The continuous branch follows the conventional pipeline to predict decomposed
materials, while the discrete branch uses the VQ mechanism to quantize
continuous materials into individual ones. By discretizing the materials, our
model can reduce noise in the decomposition process and generate a segmentation
map of discrete materials. Specific materials can be easily selected for
further editing by clicking on the corresponding area of the segmentation
outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy
to predict the number of materials in a scene, which reduces redundancy in the
material segmentation process. To improve usability, we also develop an
interactive interface to further assist material editing. We evaluate our model
on both computer-generated and real-world scenes, demonstrating its superior
performance. To the best of our knowledge, our model is the first to enable
discrete material editing in 3D scenes.

Comments:
- Accepted by TVCG. Project Page:
  https://jtbzhl.github.io/VQ-NeRF.github.io/

---

## Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos  using NeRFs

nerf{: .label .label-blue }

2023-10-18 | Khoa Tuan Nguyen, Francesca Tozzi, Nikdokht Rashidian, Wouter Willaert, Joris Vankerschaver, Wesley De Neve | cs.CV | [PDF](http://arxiv.org/pdf/2310.11645v1){: .btn .btn-green }

**Abstract**: Given that a conventional laparoscope only provides a two-dimensional (2-D)
view, the detection and diagnosis of medical ailments can be challenging. To
overcome the visual constraints associated with laparoscopy, the use of
laparoscopic images and videos to reconstruct the three-dimensional (3-D)
anatomical structure of the abdomen has proven to be a promising approach.
Neural Radiance Fields (NeRFs) have recently gained attention thanks to their
ability to generate photorealistic images from a 3-D static scene, thus
facilitating a more comprehensive exploration of the abdomen through the
synthesis of new views. This distinguishes NeRFs from alternative methods such
as Simultaneous Localization and Mapping (SLAM) and depth estimation. In this
paper, we present a comprehensive examination of NeRFs in the context of
laparoscopy surgical videos, with the goal of rendering abdominal scenes in
3-D. Although our experimental results are promising, the proposed approach
encounters substantial challenges, which require further exploration in future
research.

Comments:
- The Version of Record of this contribution is published in MLMI 2023
  Part I, and is available online at
  https://doi.org/10.1007/978-3-031-45673-2_9

---

## TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through  Neural Radiance Fields

nerf{: .label .label-blue }

2023-10-16 | Leif Van Holland, Ruben Bliersbach, Jan U. Müller, Patrick Stotko, Reinhard Klein | cs.CV | [PDF](http://arxiv.org/pdf/2310.10650v1){: .btn .btn-green }

**Abstract**: Implicit representations like Neural Radiance Fields (NeRF) showed impressive
results for photorealistic rendering of complex scenes with fine details.
However, ideal or near-perfectly specular reflecting objects such as mirrors,
which are often encountered in various indoor scenes, impose ambiguities and
inconsistencies in the representation of the reconstructed scene leading to
severe artifacts in the synthesized renderings. In this paper, we present a
novel reflection tracing method tailored for the involved volume rendering
within NeRF that takes these mirror-like objects into account while avoiding
the cost of straightforward but expensive extensions through standard path
tracing. By explicitly modeling the reflection behavior using physically
plausible materials and estimating the reflected radiance with Monte-Carlo
methods within the volume rendering formulation, we derive efficient strategies
for importance sampling and the transmittance computation along rays from only
few samples. We show that our novel method enables the training of consistent
representations of such challenging scenes and achieves superior results in
comparison to previous state-of-the-art approaches.

---

## Real-time Photorealistic Dynamic Scene Representation and Rendering with  4D Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-10-16 | Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2310.10642v2){: .btn .btn-green }

**Abstract**: Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time is challenging due to scene complexity and temporal dynamics. Despite
advancements in neural implicit models, limitations persist: (i) Inadequate
Scene Structure: Existing methods struggle to reveal the spatial and temporal
structure of dynamic scenes from directly learning the complex 6D plenoptic
function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element
deformation becomes impractical for complex dynamics. To address these issues,
we consider the spacetime as an entirety and propose to approximate the
underlying spatio-temporal 4D volume of a dynamic scene by optimizing a
collection of 4D primitives, with explicit geometry and appearance modeling.
Learning to optimize the 4D primitives enables us to synthesize novel views at
any desired time with our tailored rendering routine. Our model is conceptually
simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that
can rotate arbitrarily in space and time, as well as view-dependent and
time-evolved appearance represented by the coefficient of 4D spherindrical
harmonics. This approach offers simplicity, flexibility for variable-length
video and end-to-end training, and efficient real-time rendering, making it
suitable for capturing complex dynamic scene motions. Experiments across
various benchmarks, including monocular and multi-view scenarios, demonstrate
our 4DGS model's superior visual quality and efficiency.

Comments:
- ICLR 2024

---

## DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and  View-Change Human-Centric Video Editing

nerf{: .label .label-blue }

2023-10-16 | Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou | cs.CV | [PDF](http://arxiv.org/pdf/2310.10624v2){: .btn .btn-green }

**Abstract**: Despite recent progress in diffusion-based video editing, existing methods
are limited to short-length videos due to the contradiction between long-range
consistency and frame-wise editing. Prior attempts to address this challenge by
introducing video-2D representations encounter significant difficulties with
large-scale motion- and view-change videos, especially in human-centric
scenarios. To overcome this, we propose to introduce the dynamic Neural
Radiance Fields (NeRF) as the innovative video representation, where the
editing can be performed in the 3D spaces and propagated to the entire video
via the deformation field. To provide consistent and controllable editing, we
propose the image-based video-NeRF editing pipeline with a set of innovative
designs, including multi-view multi-pose Score Distillation Sampling (SDS) from
both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction
losses, text-guided local parts super-resolution, and style transfer. Extensive
experiments demonstrate that our method, dubbed as DynVideo-E, significantly
outperforms SOTA approaches on two challenging datasets by a large margin of
50% ~ 95% for human preference. Code will be released at
https://showlab.github.io/DynVideo-E/.

Comments:
- Project Page: https://showlab.github.io/DynVideo-E/

---

## Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion  Generation Model

nerf{: .label .label-blue }

2023-10-16 | Junpeng Tan, Xin Zhang, Yao Lv, Xiangmin Xu, Gang Li | eess.IV | [PDF](http://arxiv.org/pdf/2310.10209v1){: .btn .btn-green }

**Abstract**: Although the use of multiple stacks can handle slice-to-volume motion
correction and artifact removal problems, there are still several problems: 1)
The slice-to-volume method usually uses slices as input, which cannot solve the
problem of uniform intensity distribution and complementarity in regions of
different fetal MRI stacks; 2) The integrity of 3D space is not considered,
which adversely affects the discrimination and generation of globally
consistent information in fetal MRI; 3) Fetal MRI with severe motion artifacts
in the real-world cannot achieve high-quality super-resolution reconstruction.
To address these issues, we propose a novel fetal brain MRI high-quality volume
reconstruction method, called the Radiation Diffusion Generation Model (RDGM).
It is a self-supervised generation method, which incorporates the idea of
Neural Radiation Field (NeRF) based on the coordinate generation and diffusion
model based on super-resolution generation. To solve regional intensity
heterogeneity in different directions, we use a pre-trained transformer model
for slice registration, and then, a new regionally Consistent Implicit Neural
Representation (CINR) network sub-module is proposed. CINR can generate the
initial volume by combining a coordinate association map of two different
coordinate mapping spaces. To enhance volume global consistency and
discrimination, we introduce the Volume Diffusion Super-resolution Generation
(VDSG) mechanism. The global intensity discriminant generation from
volume-to-volume is carried out using the idea of diffusion generation, and
CINR becomes the deviation intensity generation network of the volume-to-volume
diffusion model. Finally, the experimental results on real-world fetal brain
MRI stacks demonstrate the state-of-the-art performance of our method.

---

## ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context

nerf{: .label .label-blue }

2023-10-15 | Binglun Wang, Niladri Shekhar Dutt, Niloy J. Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2310.09965v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have recently emerged as a popular option for
photo-realistic object capture due to their ability to faithfully capture
high-fidelity volumetric content even from handheld video input. Although much
research has been devoted to efficient optimization leading to real-time
training and rendering, options for interactive editing NeRFs remain limited.
We present a very simple but effective neural network architecture that is fast
and efficient while maintaining a low memory footprint. This architecture can
be incrementally guided through user-friendly image-based edits. Our
representation allows straightforward object selection via semantic feature
distillation at the training stage. More importantly, we propose a local
3D-aware image context to facilitate view-consistent image editing that can
then be distilled into fine-tuned NeRFs, via geometric and appearance
adjustments. We evaluate our setup on a variety of examples to demonstrate
appearance and geometric edits and report 10-30x speedup over concurrent work
focusing on text-guided NeRF editing. Video results can be seen on our project
webpage at https://proteusnerf.github.io.

---

## Active Perception using Neural Radiance Fields

nerf{: .label .label-blue }

2023-10-15 | Siming He, Christopher D. Hsu, Dexter Ong, Yifei Simon Shao, Pratik Chaudhari | cs.RO | [PDF](http://arxiv.org/pdf/2310.09892v1){: .btn .btn-green }

**Abstract**: We study active perception from first principles to argue that an autonomous
agent performing active perception should maximize the mutual information that
past observations posses about future ones. Doing so requires (a) a
representation of the scene that summarizes past observations and the ability
to update this representation to incorporate new observations (state estimation
and mapping), (b) the ability to synthesize new observations of the scene (a
generative model), and (c) the ability to select control trajectories that
maximize predictive information (planning). This motivates a neural radiance
field (NeRF)-like representation which captures photometric, geometric and
semantic properties of the scene grounded. This representation is well-suited
to synthesizing new observations from different viewpoints. And thereby, a
sampling-based planner can be used to calculate the predictive information from
synthetic observations along dynamically-feasible trajectories. We use active
perception for exploring cluttered indoor environments and employ a notion of
semantic uncertainty to check for the successful completion of an exploration
task. We demonstrate these ideas via simulation in realistic 3D indoor
environments.

---

## CBARF: Cascaded Bundle-Adjusting Neural Radiance Fields from Imperfect  Camera Poses

nerf{: .label .label-blue }

2023-10-15 | Hongyu Fu, Xin Yu, Lincheng Li, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2310.09776v1){: .btn .btn-green }

**Abstract**: Existing volumetric neural rendering techniques, such as Neural Radiance
Fields (NeRF), face limitations in synthesizing high-quality novel views when
the camera poses of input images are imperfect. To address this issue, we
propose a novel 3D reconstruction framework that enables simultaneous
optimization of camera poses, dubbed CBARF (Cascaded Bundle-Adjusting NeRF).In
a nutshell, our framework optimizes camera poses in a coarse-to-fine manner and
then reconstructs scenes based on the rectified poses. It is observed that the
initialization of camera poses has a significant impact on the performance of
bundle-adjustment (BA). Therefore, we cascade multiple BA modules at different
scales to progressively improve the camera poses. Meanwhile, we develop a
neighbor-replacement strategy to further optimize the results of BA in each
stage. In this step, we introduce a novel criterion to effectively identify
poorly estimated camera poses. Then we replace them with the poses of
neighboring cameras, thus further eliminating the impact of inaccurate camera
poses. Once camera poses have been optimized, we employ a density voxel grid to
generate high-quality 3D reconstructed scenes and images in novel views.
Experimental results demonstrate that our CBARF model achieves state-of-the-art
performance in both pose optimization and novel view synthesis, especially in
the existence of large camera pose noise.

---

## GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging  2D and 3D Diffusion Models

gaussian splatting{: .label .label-blue }

2023-10-12 | Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2310.08529v2){: .btn .btn-green }

**Abstract**: In recent times, the generation of 3D assets from text prompts has shown
impressive results. Both 2D and 3D diffusion models can help generate decent 3D
objects based on prompts. 3D diffusion models have good 3D consistency, but
their quality and generalization are limited as trainable 3D data is expensive
and hard to obtain. 2D diffusion models enjoy strong abilities of
generalization and fine generation, but 3D consistency is hard to guarantee.
This paper attempts to bridge the power from the two types of diffusion models
via the recent explicit and efficient 3D Gaussian splatting representation. A
fast 3D object generation framework, named as GaussianDreamer, is proposed,
where the 3D diffusion model provides priors for initialization and the 2D
diffusion model enriches the geometry and appearance. Operations of noisy point
growing and color perturbation are introduced to enhance the initialized
Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D
avatar within 15 minutes on one GPU, much faster than previous methods, while
the generated instances can be directly rendered in real time. Demos and code
are available at https://taoranyi.com/gaussiandreamer/.

Comments:
- Project page: https://taoranyi.com/gaussiandreamer/

---

## 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering

gaussian splatting{: .label .label-blue }

2023-10-12 | Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2310.08528v2){: .btn .btn-green }

**Abstract**: Representing and rendering dynamic scenes has been an important but
challenging task. Especially, to accurately model complex motions, high
efficiency is usually hard to guarantee. To achieve real-time dynamic scene
rendering while also enjoying high training and storage efficiency, we propose
4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes
rather than applying 3D-GS for each individual frame. In 4D-GS, a novel
explicit representation containing both 3D Gaussians and 4D neural voxels is
proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is
proposed to efficiently build Gaussian features from 4D neural voxels and then
a lightweight MLP is applied to predict Gaussian deformations at novel
timestamps. Our 4D-GS method achieves real-time rendering under high
resolutions, 82 FPS at an 800$\times$800 resolution on an RTX 3090 GPU while
maintaining comparable or better quality than previous state-of-the-art
methods. More demos and code are available at
https://guanjunwu.github.io/4dgs/.

Comments:
- Project page: https://guanjunwu.github.io/4dgs/

---

## rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera

nerf{: .label .label-blue }

2023-10-11 | Tongtong Zhang, Yuanxiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2310.07179v1){: .btn .btn-green }

**Abstract**: Novel view synthesis of satellite images holds a wide range of practical
applications. While recent advances in the Neural Radiance Field have
predominantly targeted pin-hole cameras, and models for satellite cameras often
demand sufficient input views. This paper presents rpcPRF, a Multiplane Images
(MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC).
Unlike coordinate-based neural radiance fields in need of sufficient views of
one scene, our model is applicable to single or few inputs and performs well on
images from unseen scenes. To enable generalization across scenes, we propose
to use reprojection supervision to induce the predicted MPI to learn the
correct geometry between the 3D coordinates and the images. Moreover, we remove
the stringent requirement of dense depth supervision from deep
multiview-stereo-based methods by introducing rendering techniques of radiance
fields. rpcPRF combines the superiority of implicit representations and the
advantages of the RPC model, to capture the continuous altitude space while
learning the 3D structure. Given an RGB image and its corresponding RPC, the
end-to-end model learns to synthesize the novel view with a new RPC and
reconstruct the altitude of the scene. When multiple views are provided as
inputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC
dataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF
outperforms state-of-the-art nerf-based methods by a significant margin in
terms of image fidelity, reconstruction accuracy, and efficiency, for both
single-view and multiview task.

---

## PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction

nerf{: .label .label-blue }

2023-10-11 | Jia-Wang Bian, Wenjing Bian, Victor Adrian Prisacariu, Philip Torr | cs.CV | [PDF](http://arxiv.org/pdf/2310.07449v2){: .btn .btn-green }

**Abstract**: Neural surface reconstruction is sensitive to the camera pose noise, even if
state-of-the-art pose estimators like COLMAP or ARKit are used. More
importantly, existing Pose-NeRF joint optimisation methods have struggled to
improve pose accuracy in challenging real-world scenarios. To overcome the
challenges, we introduce the pose residual field (\textbf{PoRF}), a novel
implicit representation that uses an MLP for regressing pose updates. This is
more robust than the conventional pose parameter optimisation due to parameter
sharing that leverages global information over the entire sequence.
Furthermore, we propose an epipolar geometry loss to enhance the supervision
that leverages the correspondences exported from COLMAP results without the
extra computational overhead. Our method yields promising results. On the DTU
dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the
decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the
MobileBrick dataset that contains casually captured unbounded 360-degree
videos, our method refines ARKit poses and improves the reconstruction F1 score
from 69.18 to 75.67, outperforming that with the dataset provided ground-truth
pose (75.14). These achievements demonstrate the efficacy of our approach in
refining camera poses and improving the accuracy of neural surface
reconstruction in real-world scenarios.

Comments:
- Under review

---

## Dynamic Appearance Particle Neural Radiance Field

nerf{: .label .label-blue }

2023-10-11 | Ancheng Lin, Jun Li | cs.CV | [PDF](http://arxiv.org/pdf/2310.07916v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D
scenes. Dynamic NeRFs extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic NeRFs employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-NeRF), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists
of superposition of a static field and a dynamic field. The dynamic field is
quantised as a collection of {\em appearance particles}, which carries the
visual information of a small dynamic element in the scene and is equipped with
a motion model. All components, including the static field, the visual features
and motion models of the particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
dataset to evaluate motion modelling. Experimental results show that DAP-NeRF
is an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene.

---

## Leveraging Neural Radiance Fields for Uncertainty-Aware Visual  Localization

nerf{: .label .label-blue }

2023-10-10 | Le Chen, Weirong Chen, Rui Wang, Marc Pollefeys | cs.CV | [PDF](http://arxiv.org/pdf/2310.06984v1){: .btn .btn-green }

**Abstract**: As a promising fashion for visual localization, scene coordinate regression
(SCR) has seen tremendous progress in the past decade. Most recent methods
usually adopt neural networks to learn the mapping from image pixels to 3D
scene coordinates, which requires a vast amount of annotated training data. We
propose to leverage Neural Radiance Fields (NeRF) to generate training samples
for SCR. Despite NeRF's efficiency in rendering, many of the rendered data are
polluted by artifacts or only contain minimal information gain, which can
hinder the regression accuracy or bring unnecessary computational costs with
redundant data. These challenges are addressed in three folds in this paper:
(1) A NeRF is designed to separately predict uncertainties for the rendered
color and depth images, which reveal data reliability at the pixel level. (2)
SCR is formulated as deep evidential learning with epistemic uncertainty, which
is used to evaluate information gain and scene coordinate quality. (3) Based on
the three arts of uncertainties, a novel view selection policy is formed that
significantly improves data efficiency. Experiments on public datasets
demonstrate that our method could select the samples that bring the most
information gain and promote the performance with the highest efficiency.

Comments:
- 8 pages, 5 figures

---

## High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying  Expression Conditioned Neural Radiance Field

nerf{: .label .label-blue }

2023-10-10 | Minghan Qin, Yifan Liu, Yuelang Xu, Xiaochen Zhao, Yebin Liu, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2310.06275v1){: .btn .btn-green }

**Abstract**: One crucial aspect of 3D head avatar reconstruction lies in the details of
facial expressions. Although recent NeRF-based photo-realistic 3D head avatar
methods achieve high-quality avatar rendering, they still encounter challenges
retaining intricate facial expression details because they overlook the
potential of specific expression variations at different spatial positions when
conditioning the radiance field. Motivated by this observation, we introduce a
novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained
by a simple MLP-based generation network, encompassing both spatial positional
features and global expression information. Benefiting from rich and diverse
information of the SVE at different positions, the proposed SVE-conditioned
neural radiance field can deal with intricate facial expressions and achieve
realistic rendering and geometry details of high-fidelity 3D head avatars.
Additionally, to further elevate the geometric and rendering quality, we
introduce a new coarse-to-fine training strategy, including a geometry
initialization strategy at the coarse stage and an adaptive importance sampling
strategy at the fine stage. Extensive experiments indicate that our method
outperforms other state-of-the-art (SOTA) methods in rendering and geometry
quality on mobile phone-collected and public datasets.

Comments:
- 9 pages, 5 figures

---

## A Real-time Method for Inserting Virtual Objects into Neural Radiance  Fields

nerf{: .label .label-blue }

2023-10-09 | Keyang Ye, Hongzhi Wu, Xin Tong, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2310.05837v1){: .btn .btn-green }

**Abstract**: We present the first real-time method for inserting a rigid virtual object
into a neural radiance field, which produces realistic lighting and shadowing
effects, as well as allows interactive manipulation of the object. By
exploiting the rich information about lighting and geometry in a NeRF, our
method overcomes several challenges of object insertion in augmented reality.
For lighting estimation, we produce accurate, robust and 3D spatially-varying
incident lighting that combines the near-field lighting from NeRF and an
environment lighting to account for sources not covered by the NeRF. For
occlusion, we blend the rendered virtual object with the background scene using
an opacity map integrated from the NeRF. For shadows, with a precomputed field
of spherical signed distance field, we query the visibility term for any point
around the virtual object, and cast soft, detailed shadows onto 3D surfaces.
Compared with state-of-the-art techniques, our approach can insert virtual
object into scenes with superior fidelity, and has a great potential to be
further applied to augmented reality systems.

---

## Neural Impostor: Editing Neural Radiance Fields with Explicit Shape  Manipulation

nerf{: .label .label-blue }

2023-10-09 | Ruiyang Liu, Jinxu Xiang, Bowen Zhao, Ran Zhang, Jingyi Yu, Changxi Zheng | cs.GR | [PDF](http://arxiv.org/pdf/2310.05391v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have significantly advanced the generation of
highly realistic and expressive 3D scenes. However, the task of editing NeRF,
particularly in terms of geometry modification, poses a significant challenge.
This issue has obstructed NeRF's wider adoption across various applications. To
tackle the problem of efficiently editing neural implicit fields, we introduce
Neural Impostor, a hybrid representation incorporating an explicit tetrahedral
mesh alongside a multigrid implicit field designated for each tetrahedron
within the explicit mesh. Our framework bridges the explicit shape manipulation
and the geometric editing of implicit fields by utilizing multigrid barycentric
coordinate encoding, thus offering a pragmatic solution to deform, composite,
and generate neural implicit fields while maintaining a complex volumetric
appearance. Furthermore, we propose a comprehensive pipeline for editing neural
implicit fields based on a set of explicit geometric editing operations. We
show the robustness and adaptability of our system through diverse examples and
experiments, including the editing of both synthetic objects and real captured
data. Finally, we demonstrate the authoring process of a hybrid
synthetic-captured object utilizing a variety of editing operations,
underlining the transformative potential of Neural Impostor in the field of 3D
content creation and manipulation.

Comments:
- Accepted at Pacific Graphics 2023 and Computer Graphics Forum

---

## LocoNeRF: A NeRF-based Approach for Local Structure from Motion for  Precise Localization

nerf{: .label .label-blue }

2023-10-08 | Artem Nenashev, Mikhail Kurenkov, Andrei Potapov, Iana Zhura, Maksim Katerishich, Dzmitry Tsetserukou | cs.CV | [PDF](http://arxiv.org/pdf/2310.05134v1){: .btn .btn-green }

**Abstract**: Visual localization is a critical task in mobile robotics, and researchers
are continuously developing new approaches to enhance its efficiency. In this
article, we propose a novel approach to improve the accuracy of visual
localization using Structure from Motion (SfM) techniques. We highlight the
limitations of global SfM, which suffers from high latency, and the challenges
of local SfM, which requires large image databases for accurate reconstruction.
To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as
opposed to image databases, to cut down on the space required for storage. We
suggest that sampling reference images around the prior query position can lead
to further improvements. We evaluate the accuracy of our proposed method
against ground truth obtained using LIDAR and Advanced Lidar Odometry and
Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM
with COLMAP in the conducted experiments. Our proposed method achieves an
accuracy of 0.068 meters compared to the ground truth, which is slightly lower
than the most advanced method COLMAP, which has an accuracy of 0.022 meters.
However, the size of the database required for COLMAP is 400 megabytes, whereas
the size of our NeRF model is only 160 megabytes. Finally, we perform an
ablation study to assess the impact of using reference images from the NeRF
reconstruction.

---

## Geometry Aware Field-to-field Transformations for 3D Semantic  Segmentation

nerf{: .label .label-blue }

2023-10-08 | Dominik Hollidt, Clinton Wang, Polina Golland, Marc Pollefeys | cs.CV | [PDF](http://arxiv.org/pdf/2310.05133v1){: .btn .btn-green }

**Abstract**: We present a novel approach to perform 3D semantic segmentation solely from
2D supervision by leveraging Neural Radiance Fields (NeRFs). By extracting
features along a surface point cloud, we achieve a compact representation of
the scene which is sample-efficient and conducive to 3D reasoning. Learning
this feature space in an unsupervised manner via masked autoencoding enables
few-shot segmentation. Our method is agnostic to the scene parameterization,
working on scenes fit with any type of NeRF.

Comments:
- 8 pages

---

## Improving Neural Radiance Field using Near-Surface Sampling with Point  Cloud Generation

nerf{: .label .label-blue }

2023-10-06 | Hye Bin Yoo, Hyun Min Han, Sung Soo Hwang, Il Yong Chun | cs.CV | [PDF](http://arxiv.org/pdf/2310.04152v1){: .btn .btn-green }

**Abstract**: Neural radiance field (NeRF) is an emerging view synthesis method that
samples points in a three-dimensional (3D) space and estimates their existence
and color probabilities. The disadvantage of NeRF is that it requires a long
training time since it samples many 3D points. In addition, if one samples
points from occluded regions or in the space where an object is unlikely to
exist, the rendering quality of NeRF can be degraded. These issues can be
solved by estimating the geometry of 3D scene. This paper proposes a
near-surface sampling framework to improve the rendering quality of NeRF. To
this end, the proposed method estimates the surface of a 3D object using depth
images of the training set and sampling is performed around there only. To
obtain depth information on a novel view, the paper proposes a 3D point cloud
generation method and a simple refining method for projected depth from a point
cloud. Experimental results show that the proposed near-surface sampling NeRF
framework can significantly improve the rendering quality, compared to the
original NeRF and a state-of-the-art depth-based NeRF method. In addition, one
can significantly accelerate the training time of a NeRF model with the
proposed near-surface sampling framework.

Comments:
- 13 figures, 2 tables

---

## Point-Based Radiance Fields for Controllable Human Motion Synthesis



2023-10-05 | Haitao Yu, Deheng Zhang, Peiyuan Xie, Tianyi Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2310.03375v1){: .btn .btn-green }

**Abstract**: This paper proposes a novel controllable human motion synthesis method for
fine-level deformation based on static point-based radiance fields. Although
previous editable neural radiance field methods can generate impressive results
on novel-view synthesis and allow naive deformation, few algorithms can achieve
complex 3D human editing such as forward kinematics. Our method exploits the
explicit point cloud to train the static 3D scene and apply the deformation by
encoding the point cloud translation using a deformation MLP. To make sure the
rendering result is consistent with the canonical space training, we estimate
the local rotation using SVD and interpolate the per-point rotation to the
query view direction of the pre-trained radiance field. Extensive experiments
show that our approach can significantly outperform the state-of-the-art on
fine-level complex deformation which can be generalized to other 3D characters
besides humans.

---

## Targeted Adversarial Attacks on Generalizable Neural Radiance Fields

nerf{: .label .label-blue }

2023-10-05 | Andras Horvath, Csaba M. Jozsa | cs.LG | [PDF](http://arxiv.org/pdf/2310.03578v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for
3D scene representation and rendering. These data-driven models can learn to
synthesize high-quality images from sparse 2D observations, enabling realistic
and interactive scene reconstructions. However, the growing usage of NeRFs in
critical applications such as augmented reality, robotics, and virtual
environments could be threatened by adversarial attacks.
  In this paper we present how generalizable NeRFs can be attacked by both
low-intensity adversarial attacks and adversarial patches, where the later
could be robust enough to be used in real world applications. We also
demonstrate targeted attacks, where a specific, predefined output scene is
generated by these attack with success.

---

## BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance  Fields

nerf{: .label .label-blue }

2023-10-05 | Ágoston István Csehi, Csaba Máté Józsa | cs.CV | [PDF](http://arxiv.org/pdf/2310.03563v1){: .btn .btn-green }

**Abstract**: We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which
defines the image pose estimation problem as a NeRF based iterative linear
optimization. NeRFs are novel neural space representation models that can
synthesize photorealistic novel views of real-world scenes or objects. Our
contributions are as follows: we extend the localization optimization objective
with a depth-based loss function, we introduce a multi-image based loss
function where a sequence of images with known relative poses are used without
increasing the computational complexity, we omit hierarchical sampling during
volumetric rendering, meaning only the coarse model is used for pose
estimation, and we how that by extending the sampling interval convergence can
be achieved even or higher initial pose estimate errors. With the proposed
modifications the convergence speed is significantly improved, and the basin of
convergence is substantially extended.

Comments:
- Accepted to Nerf4ADR workshop of ICCV23 conference

---

## Shielding the Unseen: Privacy Protection through Poisoning NeRF with  Spatial Deformation

nerf{: .label .label-blue }

2023-10-04 | Yihan Wu, Brandon Y. Feng, Heng Huang | cs.CV | [PDF](http://arxiv.org/pdf/2310.03125v1){: .btn .btn-green }

**Abstract**: In this paper, we introduce an innovative method of safeguarding user privacy
against the generative capabilities of Neural Radiance Fields (NeRF) models.
Our novel poisoning attack method induces changes to observed views that are
imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to
accurately reconstruct a 3D scene. To achieve this, we devise a bi-level
optimization algorithm incorporating a Projected Gradient Descent (PGD)-based
spatial deformation. We extensively test our approach on two common NeRF
benchmark datasets consisting of 29 real-world scenes with high-quality images.
Our results compellingly demonstrate that our privacy-preserving method
significantly impairs NeRF's performance across these benchmark datasets.
Additionally, we show that our method is adaptable and versatile, functioning
across various perturbation strengths and NeRF architectures. This work offers
valuable insights into NeRF's vulnerabilities and emphasizes the need to
account for such potential privacy risks when developing robust 3D scene
reconstruction algorithms. Our study contributes to the larger conversation
surrounding responsible AI and generative machine learning, aiming to protect
user privacy and respect creative ownership in the digital age.

---

## Efficient-3DiM: Learning a Generalizable Single-image Novel-view  Synthesizer in One Day



2023-10-04 | Yifan Jiang, Hao Tang, Jen-Hao Rick Chang, Liangchen Song, Zhangyang Wang, Liangliang Cao | cs.CV | [PDF](http://arxiv.org/pdf/2310.03015v1){: .btn .btn-green }

**Abstract**: The task of novel view synthesis aims to generate unseen perspectives of an
object or scene from a limited set of input images. Nevertheless, synthesizing
novel views from a single image still remains a significant challenge in the
realm of computer vision. Previous approaches tackle this problem by adopting
mesh prediction, multi-plain image construction, or more advanced techniques
such as neural radiance fields. Recently, a pre-trained diffusion model that is
specifically designed for 2D image synthesis has demonstrated its capability in
producing photorealistic novel views, if sufficiently optimized on a 3D
finetuning task. Although the fidelity and generalizability are greatly
improved, training such a powerful diffusion model requires a vast volume of
training data and model parameters, resulting in a notoriously long time and
high computational costs. To tackle this issue, we propose Efficient-3DiM, a
simple but effective framework to learn a single-image novel-view synthesizer.
Motivated by our in-depth analysis of the inference process of diffusion
models, we propose several pragmatic strategies to reduce the training overhead
to a manageable scale, including a crafted timestep sampling strategy, a
superior 3D feature extractor, and an enhanced training scheme. When combined,
our framework is able to reduce the total training time from 10 days to less
than 1 day, significantly accelerating the training process under the same
computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive
experiments are conducted to demonstrate the efficiency and generalizability of
our proposed method.

---

## T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation

nerf{: .label .label-blue }

2023-10-04 | Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, Yong-Jin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2310.02977v1){: .btn .btn-green }

**Abstract**: Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.

Comments:
- 16 pages, 11 figures

---

## ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space  NeRF

nerf{: .label .label-blue }

2023-10-04 | Jangho Park, Gihyun Kwon, Jong Chul Ye | cs.CV | [PDF](http://arxiv.org/pdf/2310.02712v1){: .btn .btn-green }

**Abstract**: Recently, there has been a significant advancement in text-to-image diffusion
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into NeRF editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing NeRF editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding
real-world scenes into the latent space of the latent diffusion model (LDM)
through a unique refinement layer. This approach enables us to obtain a NeRF
backbone that is not only faster but also more amenable to editing compared to
traditional image space NeRF editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-NeRF achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.

---

## USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields

nerf{: .label .label-blue }

2023-10-04 | Moyang Li, Peng Wang, Lingzhe Zhao, Bangyan Liao, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2310.02687v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) has received much attention recently due to its
impressive capability to represent 3D scene and synthesize novel view images.
Existing works usually assume that the input images are captured by a global
shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied
to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter
effect would also affect the accuracy of the camera pose estimation (e.g. via
COLMAP), which further prevents the success of NeRF algorithm with RS images.
In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance
Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and
recover accurate camera motion trajectory simultaneously under the framework of
NeRF, by modeling the physical image formation process of a RS camera.
Experimental results demonstrate that USB-NeRF achieves better performance
compared to prior works, in terms of RS effect removal, novel view image
synthesis as well as camera motion estimation. Furthermore, our algorithm can
also be used to recover high-fidelity high frame-rate global shutter video from
a sequence of RS images.

---

## Adaptive Multi-NeRF: Exploit Efficient Parallelism in Adaptive Multiple  Scale Neural Radiance Field Rendering

nerf{: .label .label-blue }

2023-10-03 | Tong Wang, Shuichi Kurabayashi | cs.CV | [PDF](http://arxiv.org/pdf/2310.01881v1){: .btn .btn-green }

**Abstract**: Recent advances in Neural Radiance Fields (NeRF) have demonstrated
significant potential for representing 3D scene appearances as implicit neural
networks, enabling the synthesis of high-fidelity novel views. However, the
lengthy training and rendering process hinders the widespread adoption of this
promising technique for real-time rendering applications. To address this
issue, we present an effective adaptive multi-NeRF method designed to
accelerate the neural rendering process for large scenes with unbalanced
workloads due to varying scene complexities.
  Our method adaptively subdivides scenes into axis-aligned bounding boxes
using a tree hierarchy approach, assigning smaller NeRFs to different-sized
subspaces based on the complexity of each scene portion. This ensures the
underlying neural representation is specific to a particular part of the scene.
We optimize scene subdivision by employing a guidance density grid, which
balances representation capability for each Multilayer Perceptron (MLP).
Consequently, samples generated by each ray can be sorted and collected for
parallel inference, achieving a balanced workload suitable for small MLPs with
consistent dimensions for regular and GPU-friendly computations. We aosl
demonstrated an efficient NeRF sampling strategy that intrinsically adapts to
increase parallelism, utilization, and reduce kernel calls, thereby achieving
much higher GPU utilization and accelerating the rendering process.

---

## MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural  Radiance Fields

nerf{: .label .label-blue }

2023-10-03 | Takuhiro Kaneko | cs.CV | [PDF](http://arxiv.org/pdf/2310.01821v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) have shown impressive results for novel view
synthesis. However, they depend on the repetitive use of a single-input
single-output multilayer perceptron (SISO MLP) that maps 3D coordinates and
view direction to the color and volume density in a sample-wise manner, which
slows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF)
that reduces the number of MLPs running by replacing the SISO MLP with a MIMO
MLP and conducting mappings in a group-wise manner. One notable challenge with
this approach is that the color and volume density of each point can differ
according to a choice of input coordinates in a group, which can lead to some
notable ambiguity. We also propose a self-supervised learning method that
regularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this
ambiguity without using pretrained models. The results of a comprehensive
experimental evaluation including comparative and ablation studies are
presented to show that MIMO-NeRF obtains a good trade-off between speed and
quality with a reasonable training time. We then demonstrate that MIMO-NeRF is
compatible with and complementary to previous advancements in NeRFs by applying
it to two representative fast NeRFs, i.e., a NeRF with sample reduction
(DONeRF) and a NeRF with alternative representations (TensoRF).

Comments:
- Accepted to ICCV 2023. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/mimo-nerf/

---

## EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields

nerf{: .label .label-blue }

2023-10-03 | Anish Bhattacharya, Ratnesh Madaan, Fernando Cladera, Sai Vemprala, Rogerio Bonatti, Kostas Daniilidis, Ashish Kapoor, Vijay Kumar, Nikolai Matni, Jayesh K. Gupta | cs.CV | [PDF](http://arxiv.org/pdf/2310.02437v2){: .btn .btn-green }

**Abstract**: We present EvDNeRF, a pipeline for generating event data and training an
event-based dynamic NeRF, for the purpose of faithfully reconstructing
eventstreams on scenes with rigid and non-rigid deformations that may be too
fast to capture with a standard camera. Event cameras register asynchronous
per-pixel brightness changes at MHz rates with high dynamic range, making them
ideal for observing fast motion with almost no motion blur. Neural radiance
fields (NeRFs) offer visual-quality geometric-based learnable rendering, but
prior work with events has only considered reconstruction of static scenes. Our
EvDNeRF can predict eventstreams of dynamic scenes from a static or moving
viewpoint between any desired timestamps, thereby allowing it to be used as an
event-based simulator for a given scene. We show that by training on varied
batch sizes of events, we can improve test-time predictions of events at fine
time resolutions, outperforming baselines that pair standard dynamic NeRFs with
event generators. We release our simulated and real datasets, as well as code
for multi-view event-based data generation and the training and evaluation of
EvDNeRF models (https://github.com/anish-bhattacharya/EvDNeRF).

Comments:
- 16 pages, 20 figures, 2 tables

---

## PC-NeRF: Parent-Child Neural Radiance Fields under Partial Sensor Data  Loss in Autonomous Driving Environments

nerf{: .label .label-blue }

2023-10-02 | Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma | cs.CV | [PDF](http://arxiv.org/pdf/2310.00874v1){: .btn .btn-green }

**Abstract**: Reconstructing large-scale 3D scenes is essential for autonomous vehicles,
especially when partial sensor data is lost. Although the recently developed
neural radiance fields (NeRF) have shown compelling results in implicit
representations, the large-scale 3D scene reconstruction using partially lost
LiDAR point cloud data still needs to be explored. To bridge this gap, we
propose a novel 3D scene reconstruction framework called parent-child neural
radiance field (PC-NeRF). The framework comprises two modules, the parent NeRF
and the child NeRF, to simultaneously optimize scene-level, segment-level, and
point-level scene representations. Sensor data can be utilized more efficiently
by leveraging the segment-level representation capabilities of child NeRFs, and
an approximate volumetric representation of the scene can be quickly obtained
even with limited observations. With extensive experiments, our proposed
PC-NeRF is proven to achieve high-precision 3D reconstruction in large-scale
scenes. Moreover, PC-NeRF can effectively tackle situations where partial
sensor data is lost and has high deployment efficiency with limited training
time. Our approach implementation and the pre-trained models will be available
at https://github.com/biter0088/pc-nerf.

---

## How Many Views Are Needed to Reconstruct an Unknown Object Using NeRF?

nerf{: .label .label-blue }

2023-10-01 | Sicong Pan, Liren Jin, Hao Hu, Marija Popović, Maren Bennewitz | cs.RO | [PDF](http://arxiv.org/pdf/2310.00684v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) are gaining significant interest for online
active object reconstruction due to their exceptional memory efficiency and
requirement for only posed RGB inputs. Previous NeRF-based view planning
methods exhibit computational inefficiency since they rely on an iterative
paradigm, consisting of (1) retraining the NeRF when new images arrive; and (2)
planning a path to the next best view only. To address these limitations, we
propose a non-iterative pipeline based on the Prediction of the Required number
of Views (PRV). The key idea behind our approach is that the required number of
views to reconstruct an object depends on its complexity. Therefore, we design
a deep neural network, named PRVNet, to predict the required number of views,
allowing us to tailor the data acquisition based on the object complexity and
plan a globally shortest path. To train our PRVNet, we generate supervision
labels using the ShapeNet dataset. Simulated experiments show that our
PRV-based view planning method outperforms baselines, achieving good
reconstruction quality while significantly reducing movement cost and planning
time. We further justify the generalization ability of our approach in a
real-world experiment.

Comments:
- Submitted to ICRA 2024

---

## Multi-tiling Neural Radiance Field (NeRF) -- Geometric Assessment on  Large-scale Aerial Datasets

nerf{: .label .label-blue }

2023-10-01 | Ningli Xu, Rongjun Qin, Debao Huang, Fabio Remondino | cs.CV | [PDF](http://arxiv.org/pdf/2310.00530v3){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) offer the potential to benefit 3D
reconstruction tasks, including aerial photogrammetry. However, the scalability
and accuracy of the inferred geometry are not well-documented for large-scale
aerial assets,since such datasets usually result in very high memory
consumption and slow convergence.. In this paper, we aim to scale the NeRF on
large-scael aerial datasets and provide a thorough geometry assessment of NeRF.
Specifically, we introduce a location-specific sampling technique as well as a
multi-camera tiling (MCT) strategy to reduce memory consumption during image
loading for RAM, representation training for GPU memory, and increase the
convergence rate within tiles. MCT decomposes a large-frame image into multiple
tiled images with different camera models, allowing these small-frame images to
be fed into the training process as needed for specific locations without a
loss of accuracy. We implement our method on a representative approach,
Mip-NeRF, and compare its geometry performance with threephotgrammetric MVS
pipelines on two typical aerial datasets against LiDAR reference data. Both
qualitative and quantitative results suggest that the proposed NeRF approach
produces better completeness and object details than traditional approaches,
although as of now, it still falls short in terms of accuracy.

Comments:
- 9 Figure

---

## MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane  Images Blending

nerf{: .label .label-blue }

2023-09-30 | Yuze He, Peng Wang, Yubin Hu, Wang Zhao, Ran Yi, Yong-Jin Liu, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2310.00249v1){: .btn .btn-green }

**Abstract**: This paper presents a flexible representation of neural radiance fields based
on multi-plane images (MPI), for high-quality view synthesis of complex scenes.
MPI with Normalized Device Coordinate (NDC) parameterization is widely used in
NeRF learning for its simple definition, easy calculation, and powerful ability
to represent unbounded scenes. However, existing NeRF works that adopt MPI
representation for novel view synthesis can only handle simple forward-facing
unbounded scenes, where the input cameras are all observing in similar
directions with small relative translations. Hence, extending these MPI-based
methods to more complex scenes like large-range or even 360-degree scenes is
very challenging. In this paper, we explore the potential of MPI and show that
MPI can synthesize high-quality novel views of complex scenes with diverse
camera distributions and view directions, which are not only limited to simple
forward-facing scenes. Our key idea is to encode the neural radiance field with
multiple MPIs facing different directions and blend them with an adaptive
blending operation. For each region of the scene, the blending operation gives
larger blending weights to those advantaged MPIs with stronger local
representation abilities while giving lower weights to those with weaker
representation abilities. Such blending operation automatically modulates the
multiple MPIs to appropriately represent the diverse local density and color
information. Experiments on the KITTI dataset and ScanNet dataset demonstrate
that our proposed MMPI synthesizes high-quality images from diverse camera pose
distributions and is fast to train, outperforming the previous fast-training
NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode
extremely long trajectories and produce novel view renderings, demonstrating
its potential in applications like autonomous driving.

---

## Multi-task View Synthesis with Neural Radiance Fields

nerf{: .label .label-blue }

2023-09-29 | Shuhong Zheng, Zhipeng Bao, Martial Hebert, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2309.17450v1){: .btn .btn-green }

**Abstract**: Multi-task visual learning is a critical aspect of computer vision. Current
research, however, predominantly concentrates on the multi-task dense
prediction setting, which overlooks the intrinsic 3D world and its multi-view
consistent structures, and lacks the capability for versatile imagination. In
response to these limitations, we present a novel problem setting -- multi-task
view synthesis (MTVS), which reinterprets multi-task prediction as a set of
novel-view synthesis tasks for multiple scene properties, including RGB. To
tackle the MTVS problem, we propose MuvieNeRF, a framework that incorporates
both multi-task and cross-view knowledge to simultaneously synthesize multiple
scene properties. MuvieNeRF integrates two key modules, the Cross-Task
Attention (CTA) and Cross-View Attention (CVA) modules, enabling the efficient
use of information across multiple views and tasks. Extensive evaluation on
both synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable
of simultaneously synthesizing different scene properties with promising visual
quality, even outperforming conventional discriminative models in various
settings. Notably, we show that MuvieNeRF exhibits universal applicability
across a range of NeRF backbones. Our code is available at
https://github.com/zsh2000/MuvieNeRF.

Comments:
- ICCV 2023, Website: https://zsh2000.github.io/mtvs.github.io/

---

## Forward Flow for Novel View Synthesis of Dynamic Scenes

nerf{: .label .label-blue }

2023-09-29 | Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, Xiaoqing Ye, Xiao Tan, Errui Ding, Yumeng Zhang, Jingdong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2309.17390v1){: .btn .btn-green }

**Abstract**: This paper proposes a neural radiance field (NeRF) approach for novel view
synthesis of dynamic scenes using forward warping. Existing methods often adopt
a static NeRF to represent the canonical space, and render dynamic images at
other time steps by mapping the sampled 3D points back to the canonical space
with the learned backward flow field. However, this backward flow field is
non-smooth and discontinuous, which is difficult to be fitted by commonly used
smooth motion models. To address this problem, we propose to estimate the
forward flow field and directly warp the canonical radiance field to other time
steps. Such forward flow field is smooth and continuous within the object
region, which benefits the motion model learning. To achieve this goal, we
represent the canonical radiance field with voxel grids to enable efficient
forward warping, and propose a differentiable warping process, including an
average splatting operation and an inpaint network, to resolve the many-to-one
and one-to-many mapping issues. Thorough experiments show that our method
outperforms existing methods in both novel view rendering and motion modeling,
demonstrating the effectiveness of our forward flow motion modeling. Project
page: https://npucvr.github.io/ForwardFlowDNeRF

Comments:
- Accepted by ICCV2023 as oral. Project page:
  https://npucvr.github.io/ForwardFlowDNeRF

---

## HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural  Radiance Field

nerf{: .label .label-blue }

2023-09-29 | Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, Hongwen Zhang, Jinli Suo, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2309.17128v1){: .btn .btn-green }

**Abstract**: The problem of modeling an animatable 3D human head avatar under light-weight
setups is of significant importance but has not been well solved. Existing 3D
representations either perform well in the realism of portrait images synthesis
or the accuracy of expression control, but not both. To address the problem, we
introduce a novel hybrid explicit-implicit 3D representation, Facial Model
Conditioned Neural Radiance Field, which integrates the expressiveness of NeRF
and the prior information from the parametric template. At the core of our
representation, a synthetic-renderings-based condition method is proposed to
fuse the prior information from the parametric model into the implicit field
without constraining its topological flexibility. Besides, based on the hybrid
representation, we properly overcome the inconsistent shape issue presented in
existing methods and improve the animation stability. Moreover, by adopting an
overall GAN-based architecture using an image-to-image translation network, we
achieve high-resolution, realistic and view-consistent synthesis of dynamic
head appearance. Experiments demonstrate that our method can achieve
state-of-the-art performance for 3D head avatar animation compared with
previous methods.

---

## MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering  and Beyond

nerf{: .label .label-blue }

2023-09-28 | Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2309.16553v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) and its subsequent variants have led to
remarkable progress in neural rendering. While most of recent neural rendering
works focus on objects and small-scale scenes, developing neural rendering
methods for city-scale scenes is of great potential in many real-world
applications. However, this line of research is impeded by the absence of a
comprehensive and high-quality dataset, yet collecting such a dataset over real
city-scale scenes is costly, sensitive, and technically difficult. To this end,
we build a large-scale, comprehensive, and high-quality synthetic dataset for
city-scale neural rendering researches. Leveraging the Unreal Engine 5 City
Sample project, we develop a pipeline to easily collect aerial and street city
views, accompanied by ground-truth camera poses and a range of additional data
modalities. Flexible controls over environmental factors like light, weather,
human and car crowd are also available in our pipeline, supporting the need of
various tasks covering city-scale neural rendering and beyond. The resulting
pilot dataset, MatrixCity, contains 67k aerial images and 452k street images
from two city maps of total size $28km^2$. On top of MatrixCity, a thorough
benchmark is also conducted, which not only reveals unique challenges of the
task of city-scale neural rendering, but also highlights potential improvements
for future works. The dataset and code will be publicly available at our
project page: https://city-super.github.io/matrixcity/.

Comments:
- Accepted to ICCV 2023. Project page:
  $\href{https://city-super.github.io/matrixcity/}{this\, https\, URL}$

---

## Learning Effective NeRFs and SDFs Representations with 3D Generative  Adversarial Networks for 3D Object Generation: Technical Report for ICCV 2023  OmniObject3D Challenge

nerf{: .label .label-blue }

2023-09-28 | Zheyuan Yang, Yibo Liu, Guile Wu, Tongtong Cao, Yuan Ren, Yang Liu, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2309.16110v1){: .btn .btn-green }

**Abstract**: In this technical report, we present a solution for 3D object generation of
ICCV 2023 OmniObject3D Challenge. In recent years, 3D object generation has
made great process and achieved promising results, but it remains a challenging
task due to the difficulty of generating complex, textured and high-fidelity
results. To resolve this problem, we study learning effective NeRFs and SDFs
representations with 3D Generative Adversarial Networks (GANs) for 3D object
generation. Specifically, inspired by recent works, we use the efficient
geometry-aware 3D GANs as the backbone incorporating with label embedding and
color mapping, which enables to train the model on different taxonomies
simultaneously. Then, through a decoder, we aggregate the resulting features to
generate Neural Radiance Fields (NeRFs) based representations for rendering
high-fidelity synthetic images. Meanwhile, we optimize Signed Distance
Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we
observe that this model can be effectively trained with only a few images of
each object from a variety of classes, instead of using a great number of
images per object or training one model per class. With this pipeline, we can
optimize an effective model for 3D object generation. This solution is one of
the final top-3-place solutions in the ICCV 2023 OmniObject3D Challenge.

---

## FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for  Independence-Assumption-Free Uncertainty Estimation

nerf{: .label .label-blue }

2023-09-28 | Songlin Wei, Jiazhao Zhang, Yang Wang, Fanbo Xiang, Hao Su, He Wang | cs.CV | [PDF](http://arxiv.org/pdf/2309.16364v2){: .btn .btn-green }

**Abstract**: Neural radiance fields with stochasticity have garnered significant interest
by enabling the sampling of plausible radiance fields and quantifying
uncertainty for downstream tasks. Existing works rely on the independence
assumption of points in the radiance field or the pixels in input views to
obtain tractable forms of the probability density function. However, this
assumption inadvertently impacts performance when dealing with intricate
geometry and texture. In this work, we propose an independence-assumption-free
probabilistic neural radiance field based on Flow-GAN. By combining the
generative capability of adversarial learning and the powerful expressivity of
normalizing flow, our method explicitly models the density-radiance
distribution of the whole scene. We represent our probabilistic NeRF as a
mean-shifted probabilistic residual neural model. Our model is trained without
an explicit likelihood function, thereby avoiding the independence assumption.
Specifically, We downsample the training images with different strides and
centers to form fixed-size patches which are used to train the generator with
patch-based adversarial learning. Through extensive experiments, our method
demonstrates state-of-the-art performance by predicting lower rendering errors
and more reliable uncertainty on both synthetic and real-world datasets.

---

## DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content  Creation

gaussian splatting{: .label .label-blue }

2023-09-28 | Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, Gang Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2309.16653v1){: .btn .btn-green }

**Abstract**: Recent advances in 3D content creation mostly leverage optimization-based 3D
generation via score distillation sampling (SDS). Though promising results have
been exhibited, these methods often suffer from slow per-sample optimization,
limiting their practical usage. In this paper, we propose DreamGaussian, a
novel 3D content generation framework that achieves both efficiency and quality
simultaneously. Our key insight is to design a generative 3D Gaussian Splatting
model with companioned mesh extraction and texture refinement in UV space. In
contrast to the occupancy pruning used in Neural Radiance Fields, we
demonstrate that the progressive densification of 3D Gaussians converges
significantly faster for 3D generative tasks. To further enhance the texture
quality and facilitate downstream applications, we introduce an efficient
algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning
stage to refine the details. Extensive experiments demonstrate the superior
efficiency and competitive generation quality of our proposed approach.
Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes
from a single-view image, achieving approximately 10 times acceleration
compared to existing methods.

Comments:
- project page: https://dreamgaussian.github.io/

---

## Augmenting Heritage: An Open-Source Multiplatform AR Application

nerf{: .label .label-blue }

2023-09-28 | Corrie Green | cs.HC | [PDF](http://arxiv.org/pdf/2310.13700v1){: .btn .btn-green }

**Abstract**: AI NeRF algorithms, capable of cloud processing, have significantly reduced
hardware requirements and processing efficiency in photogrammetry pipelines.
This accessibility has unlocked the potential for museums, charities, and
cultural heritage sites worldwide to leverage mobile devices for artifact
scanning and processing. However, the adoption of augmented reality platforms
often necessitates the installation of proprietary applications on users'
mobile devices, which adds complexity to development and limits global
availability. This paper presents a case study that demonstrates a
cost-effective pipeline for visualizing scanned museum artifacts using mobile
augmented reality, leveraging an open-source embedded solution on a website.

---

## Text-to-3D using Gaussian Splatting

gaussian splatting{: .label .label-blue }

2023-09-28 | Zilong Chen, Feng Wang, Huaping Liu | cs.CV | [PDF](http://arxiv.org/pdf/2309.16585v3){: .btn .btn-green }

**Abstract**: In this paper, we present Gaussian Splatting based text-to-3D generation
(GSGEN), a novel approach for generating high-quality 3D objects. Previous
methods suffer from inaccurate geometry and limited fidelity due to the absence
of 3D prior and proper representation. We leverage 3D Gaussian Splatting, a
recent state-of-the-art representation, to address existing shortcomings by
exploiting the explicit nature that enables the incorporation of 3D prior.
Specifically, our method adopts a progressive optimization strategy, which
includes a geometry optimization stage and an appearance refinement stage. In
geometry optimization, a coarse representation is established under a 3D
geometry prior along with the ordinary 2D SDS loss, ensuring a sensible and
3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an
iterative refinement to enrich details. In this stage, we increase the number
of Gaussians by compactness-based densification to enhance continuity and
improve fidelity. With these designs, our approach can generate 3D content with
delicate details and more accurate geometry. Extensive evaluations demonstrate
the effectiveness of our method, especially for capturing high-frequency
components. Video results are provided at https://gsgen3d.github.io. Our code
is available at https://github.com/gsgen3d/gsgen

Comments:
- Project page: https://gsgen3d.github.io. Code:
  https://github.com/gsgen3d/gsgen

---

## Preface: A Data-driven Volumetric Prior for Few-shot Ultra  High-resolution Face Synthesis

nerf{: .label .label-blue }

2023-09-28 | Marcel C. Bühler, Kripasindhu Sarkar, Tanmay Shah, Gengyan Li, Daoye Wang, Leonhard Helminger, Sergio Orts-Escolano, Dmitry Lagun, Otmar Hilliges, Thabo Beeler, Abhimitra Meka | cs.CV | [PDF](http://arxiv.org/pdf/2309.16859v1){: .btn .btn-green }

**Abstract**: NeRFs have enabled highly realistic synthesis of human faces including
complex appearance and reflectance effects of hair and skin. These methods
typically require a large number of multi-view input images, making the process
hardware intensive and cumbersome, limiting applicability to unconstrained
settings. We propose a novel volumetric human face prior that enables the
synthesis of ultra high-resolution novel views of subjects that are not part of
the prior's training distribution. This prior model consists of an
identity-conditioned NeRF, trained on a dataset of low-resolution multi-view
images of diverse humans with known camera calibration. A simple sparse
landmark-based 3D alignment of the training dataset allows our model to learn a
smooth latent space of geometry and appearance despite a limited number of
training identities. A high-quality volumetric representation of a novel
subject can be obtained by model fitting to 2 or 3 camera views of arbitrary
resolution. Importantly, our method requires as few as two views of casually
captured images as input at inference time.

Comments:
- In Proceedings of the IEEE/CVF International Conference on Computer
  Vision, 2023

---

## P2I-NET: Mapping Camera Pose to Image via Adversarial Learning for New  View Synthesis in Real Indoor Environments

nerf{: .label .label-blue }

2023-09-27 | Xujie Kang, Kanglin Liu, Jiang Duan, Yuanhao Gong, Guoping Qiu | cs.CV | [PDF](http://arxiv.org/pdf/2309.15526v1){: .btn .btn-green }

**Abstract**: Given a new $6DoF$ camera pose in an indoor environment, we study the
challenging problem of predicting the view from that pose based on a set of
reference RGBD views. Existing explicit or implicit 3D geometry construction
methods are computationally expensive while those based on learning have
predominantly focused on isolated views of object categories with regular
geometric structure. Differing from the traditional \textit{render-inpaint}
approach to new view synthesis in the real indoor environment, we propose a
conditional generative adversarial neural network (P2I-NET) to directly predict
the new view from the given pose. P2I-NET learns the conditional distribution
of the images of the environment for establishing the correspondence between
the camera pose and its view of the environment, and achieves this through a
number of innovative designs in its architecture and training lost function.
Two auxiliary discriminator constraints are introduced for enforcing the
consistency between the pose of the generated image and that of the
corresponding real world image in both the latent feature space and the real
world pose space. Additionally a deep convolutional neural network (CNN) is
introduced to further reinforce this consistency in the pixel space. We have
performed extensive new view synthesis experiments on real indoor datasets.
Results show that P2I-NET has superior performance against a number of NeRF
based strong baseline models. In particular, we show that P2I-NET is 40 to 100
times faster than these competitor techniques while synthesising similar
quality images. Furthermore, we contribute a new publicly available indoor
environment dataset containing 22 high resolution RGBD videos where each frame
also has accurate camera pose parameters.

---

## NeuRBF: A Neural Fields Representation with Adaptive Radial Basis  Functions



2023-09-27 | Zhang Chen, Zhong Li, Liangchen Song, Lele Chen, Jingyi Yu, Junsong Yuan, Yi Xu | cs.CV | [PDF](http://arxiv.org/pdf/2309.15426v1){: .btn .btn-green }

**Abstract**: We present a novel type of neural fields that uses general radial bases for
signal representation. State-of-the-art neural fields typically rely on
grid-based representations for storing local neural features and N-dimensional
linear kernels for interpolating features at continuous query points. The
spatial positions of their neural features are fixed on grid nodes and cannot
well adapt to target signals. Our method instead builds upon general radial
bases with flexible kernel position and shape, which have higher spatial
adaptivity and can more closely fit target signals. To further improve the
channel-wise capacity of radial basis functions, we propose to compose them
with multi-frequency sinusoid functions. This technique extends a radial basis
to multiple Fourier radial bases of different frequency bands without requiring
extra parameters, facilitating the representation of details. Moreover, by
marrying adaptive radial bases with grid-based ones, our hybrid combination
inherits both adaptivity and interpolation smoothness. We carefully designed
weighting schemes to let radial bases adapt to different types of signals
effectively. Our experiments on 2D image and 3D signed distance field
representation demonstrate the higher accuracy and compactness of our method
than prior arts. When applied to neural radiance field reconstruction, our
method achieves state-of-the-art rendering quality, with small model size and
comparable training speed.

Comments:
- Accepted to ICCV 2023 Oral. Project page:
  https://oppo-us-research.github.io/NeuRBF-website/

---

## BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction  using Neural Radiance Fields

nerf{: .label .label-blue }

2023-09-27 | Shreya Saha, Sainan Liu, Shan Lin, Jingpei Lu, Michael Yip | cs.CV | [PDF](http://arxiv.org/pdf/2309.15329v1){: .btn .btn-green }

**Abstract**: Reconstruction of deformable scenes from endoscopic videos is important for
many applications such as intraoperative navigation, surgical visual
perception, and robotic surgery. It is a foundational requirement for realizing
autonomous robotic interventions for minimally invasive surgery. However,
previous approaches in this domain have been limited by their modular nature
and are confined to specific camera and scene settings. Our work adopts the
Neural Radiance Fields (NeRF) approach to learning 3D implicit representations
of scenes that are both dynamic and deformable over time, and furthermore with
unknown camera poses. We demonstrate this approach on endoscopic surgical
scenes from robotic surgery. This work removes the constraints of known camera
poses and overcomes the drawbacks of the state-of-the-art unstructured dynamic
scene reconstruction technique, which relies on the static part of the scene
for accurate reconstruction. Through several experimental datasets, we
demonstrate the versatility of our proposed model to adapt to diverse camera
and scene settings, and show its promise for both current and future robotic
surgical systems.

---

## 3D Density-Gradient based Edge Detection on Neural Radiance Fields  (NeRFs) for Geometric Reconstruction

nerf{: .label .label-blue }

2023-09-26 | Miriam Jäger, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2309.14800v1){: .btn .btn-green }

**Abstract**: Generating geometric 3D reconstructions from Neural Radiance Fields (NeRFs)
is of great interest. However, accurate and complete reconstructions based on
the density values are challenging. The network output depends on input data,
NeRF network configuration and hyperparameter. As a result, the direct usage of
density values, e.g. via filtering with global density thresholds, usually
requires empirical investigations. Under the assumption that the density
increases from non-object to object area, the utilization of density gradients
from relative values is evident. As the density represents a position-dependent
parameter it can be handled anisotropically, therefore processing of the
voxelized 3D density field is justified. In this regard, we address geometric
3D reconstructions based on density gradients, whereas the gradients result
from 3D edge detection filters of the first and second derivatives, namely
Sobel, Canny and Laplacian of Gaussian. The gradients rely on relative
neighboring density values in all directions, thus are independent from
absolute magnitudes. Consequently, gradient filters are able to extract edges
along a wide density range, almost independent from assumptions and empirical
investigations. Our approach demonstrates the capability to achieve geometric
3D reconstructions with high geometric accuracy on object surfaces and
remarkable object completeness. Notably, Canny filter effectively eliminates
gaps, delivers a uniform point density, and strikes a favorable balance between
correctness and completeness across the scenes.

Comments:
- 8 pages, 4 figures, 2 tables. Will be published in the ISPRS The
  International Archives of Photogrammetry, Remote Sensing and Spatial
  Information Sciences

---

## NAS-NeRF: Generative Neural Architecture Search for Neural Radiance  Fields

nerf{: .label .label-blue }

2023-09-25 | Saeejith Nair, Yuhao Chen, Mohammad Javad Shafiee, Alexander Wong | cs.CV | [PDF](http://arxiv.org/pdf/2309.14293v3){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.

Comments:
- 8 pages

---

## Variational Inference for Scalable 3D Object-centric Learning

nerf{: .label .label-blue }

2023-09-25 | Tianyu Wang, Kee Siong Ng, Miaomiao Liu | cs.CV | [PDF](http://arxiv.org/pdf/2309.14010v1){: .btn .btn-green }

**Abstract**: We tackle the task of scalable unsupervised object-centric representation
learning on 3D scenes. Existing approaches to object-centric representation
learning show limitations in generalizing to larger scenes as their learning
processes rely on a fixed global coordinate system. In contrast, we propose to
learn view-invariant 3D object representations in localized object coordinate
systems. To this end, we estimate the object pose and appearance representation
separately and explicitly map object representations across views while
maintaining object identities. We adopt an amortized variational inference
pipeline that can process sequential input and scalably update object latent
distributions online. To handle large-scale scenes with a varying number of
objects, we further introduce a Cognitive Map that allows the registration and
query of objects on a per-scene global map to achieve scalable representation
learning. We explore the object-centric neural radiance field (NeRF) as our 3D
scene representation, which is jointly modeled within our unsupervised
object-centric learning framework. Experimental results on synthetic and real
datasets show that our proposed method can infer and maintain object-centric
representations of 3D scenes and outperforms previous models.

---

## Tiled Multiplane Images for Practical 3D Photography



2023-09-25 | Numair Khan, Douglas Lanman, Lei Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2309.14291v1){: .btn .btn-green }

**Abstract**: The task of synthesizing novel views from a single image has useful
applications in virtual reality and mobile computing, and a number of
approaches to the problem have been proposed in recent years. A Multiplane
Image (MPI) estimates the scene as a stack of RGBA layers, and can model
complex appearance effects, anti-alias depth errors and synthesize soft edges
better than methods that use textured meshes or layered depth images. And
unlike neural radiance fields, an MPI can be efficiently rendered on graphics
hardware. However, MPIs are highly redundant and require a large number of
depth layers to achieve plausible results. Based on the observation that the
depth complexity in local image regions is lower than that over the entire
image, we split an MPI into many small, tiled regions, each with only a few
depth planes. We call this representation a Tiled Multiplane Image (TMPI). We
propose a method for generating a TMPI with adaptive depth planes for
single-view 3D photography in the wild. Our synthesized results are comparable
to state-of-the-art single-view MPI methods while having lower computational
overhead.

Comments:
- ICCV 2023

---

## MM-NeRF: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance  Field

nerf{: .label .label-blue }

2023-09-24 | Zijiang Yang, Zhongwei Qiu, Chang Xu, Dongmei Fu | cs.CV | [PDF](http://arxiv.org/pdf/2309.13607v2){: .btn .btn-green }

**Abstract**: 3D style transfer aims to generate stylized views of 3D scenes with specified
styles, which requires high-quality generating and keeping multi-view
consistency. Existing methods still suffer the challenges of high-quality
stylization with texture details and stylization with multimodal guidance. In
this paper, we reveal that the common training method of stylization with NeRF,
which generates stylized multi-view supervision by 2D style transfer models,
causes the same object in supervision to show various states (color tone,
details, etc.) in different views, leading NeRF to tend to smooth the texture
details, further resulting in low-quality rendering for 3D multi-style
transfer. To tackle these problems, we propose a novel Multimodal-guided 3D
Multi-style transfer of NeRF, termed MM-NeRF. First, MM-NeRF projects
multimodal guidance into a unified space to keep the multimodal styles
consistency and extracts multimodal features to guide the 3D stylization.
Second, a novel multi-head learning scheme is proposed to relieve the
difficulty of learning multi-style transfer, and a multi-view style consistent
loss is proposed to track the inconsistency of multi-view supervision data.
Finally, a novel incremental learning mechanism to generalize MM-NeRF to any
new style with small costs. Extensive experiments on several real-world
datasets show that MM-NeRF achieves high-quality 3D multi-style stylization
with multimodal guidance, and keeps multi-view consistency and style
consistency between multimodal guidance. Codes will be released.

---

## NeRF-Enhanced Outpainting for Faithful Field-of-View Extrapolation

nerf{: .label .label-blue }

2023-09-23 | Rui Yu, Jiachen Liu, Zihan Zhou, Sharon X. Huang | cs.CV | [PDF](http://arxiv.org/pdf/2309.13240v1){: .btn .btn-green }

**Abstract**: In various applications, such as robotic navigation and remote visual
assistance, expanding the field of view (FOV) of the camera proves beneficial
for enhancing environmental perception. Unlike image outpainting techniques
aimed solely at generating aesthetically pleasing visuals, these applications
demand an extended view that faithfully represents the scene. To achieve this,
we formulate a new problem of faithful FOV extrapolation that utilizes a set of
pre-captured images as prior knowledge of the scene. To address this problem,
we present a simple yet effective solution called NeRF-Enhanced Outpainting
(NEO) that uses extended-FOV images generated through NeRF to train a
scene-specific image outpainting model. To assess the performance of NEO, we
conduct comprehensive evaluations on three photorealistic datasets and one
real-world dataset. Extensive experiments on the benchmark datasets showcase
the robustness and potential of our method in addressing this challenge. We
believe our work lays a strong foundation for future exploration within the
research community.

---

## NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular  Objects with Neural Refractive-Reflective Fields

nerf{: .label .label-blue }

2023-09-22 | Xiaoxue Chen, Junchen Liu, Hao Zhao, Guyue Zhou, Ya-Qin Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2309.13039v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) have revolutionized the field of image-based
view synthesis. However, NeRF uses straight rays and fails to deal with
complicated light path changes caused by refraction and reflection. This
prevents NeRF from successfully synthesizing transparent or specular objects,
which are ubiquitous in real-world robotics and A/VR applications. In this
paper, we introduce the refractive-reflective field. Taking the object
silhouette as input, we first utilize marching tetrahedra with a progressive
encoding to reconstruct the geometry of non-Lambertian objects and then model
refraction and reflection effects of the object in a unified framework using
Fresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, we
propose a virtual cone supersampling technique. We benchmark our method on
different shapes, backgrounds and Fresnel terms on both real-world and
synthetic datasets. We also qualitatively and quantitatively benchmark the
rendering results of various editing applications, including material editing,
object replacement/insertion, and environment illumination estimation. Codes
and data are publicly available at https://github.com/dawning77/NeRRF.

---

## Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene  Reconstruction



2023-09-22 | Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang Jin | cs.CV | [PDF](http://arxiv.org/pdf/2309.13101v2){: .btn .btn-green }

**Abstract**: Implicit neural representation has paved the way for new approaches to
dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic
neural rendering methods rely heavily on these implicit representations, which
frequently struggle to capture the intricate details of objects in the scene.
Furthermore, implicit methods have difficulty achieving real-time rendering in
general dynamic scenes, limiting their use in a variety of tasks. To address
the issues, we propose a deformable 3D Gaussians Splatting method that
reconstructs scenes using 3D Gaussians and learns them in canonical space with
a deformation field to model monocular dynamic scenes. We also introduce an
annealing smoothing training mechanism with no extra overhead, which can
mitigate the impact of inaccurate poses on the smoothness of time interpolation
tasks in real-world datasets. Through a differential Gaussian rasterizer, the
deformable 3D Gaussians not only achieve higher rendering quality but also
real-time rendering speed. Experiments show that our method outperforms
existing methods significantly in terms of both rendering quality and speed,
making it well-suited for tasks such as novel-view synthesis, time
interpolation, and real-time rendering.

---

## RHINO: Regularizing the Hash-based Implicit Neural Representation



2023-09-22 | Hao Zhu, Fengyi Liu, Qi Zhang, Xun Cao, Zhan Ma | cs.CV | [PDF](http://arxiv.org/pdf/2309.12642v1){: .btn .btn-green }

**Abstract**: The use of Implicit Neural Representation (INR) through a hash-table has
demonstrated impressive effectiveness and efficiency in characterizing
intricate signals. However, current state-of-the-art methods exhibit
insufficient regularization, often yielding unreliable and noisy results during
interpolations. We find that this issue stems from broken gradient flow between
input coordinates and indexed hash-keys, where the chain rule attempts to model
discrete hash-keys, rather than the continuous coordinates. To tackle this
concern, we introduce RHINO, in which a continuous analytical function is
incorporated to facilitate regularization by connecting the input coordinate
and the network additionally without modifying the architecture of current
hash-based INRs. This connection ensures a seamless backpropagation of
gradients from the network's output back to the input coordinates, thereby
enhancing regularization. Our experimental results not only showcase the
broadened regularization capability across different hash-based INRs like DINER
and Instant NGP, but also across a variety of tasks such as image fitting,
representation of signed distance functions, and optimization of 5D static / 6D
dynamic neural radiance fields. Notably, RHINO outperforms current
state-of-the-art techniques in both quality and speed, affirming its
superiority.

Comments:
- 17 pages, 11 figures

---

## MarkNerf:Watermarking for Neural Radiance Field

nerf{: .label .label-blue }

2023-09-21 | Lifeng Chen, Jia Liu, Yan Ke, Wenquan Sun, Weina Dong, Xiaozhong Pan | cs.CR | [PDF](http://arxiv.org/pdf/2309.11747v1){: .btn .btn-green }

**Abstract**: A watermarking algorithm is proposed in this paper to address the copyright
protection issue of implicit 3D models. The algorithm involves embedding
watermarks into the images in the training set through an embedding network,
and subsequently utilizing the NeRF model for 3D modeling. A copyright verifier
is employed to generate a backdoor image by providing a secret perspective as
input to the neural radiation field. Subsequently, a watermark extractor is
devised using the hyperparameterization method of the neural network to extract
the embedded watermark image from that perspective. In a black box scenario, if
there is a suspicion that the 3D model has been used without authorization, the
verifier can extract watermarks from a secret perspective to verify network
copyright. Experimental results demonstrate that the proposed algorithm
effectively safeguards the copyright of 3D models. Furthermore, the extracted
watermarks exhibit favorable visual effects and demonstrate robust resistance
against various types of noise attacks.

---

## Rendering stable features improves sampling-based localisation with  Neural radiance fields

nerf{: .label .label-blue }

2023-09-21 | Boxuan Zhang, Lindsay Kleeman, Michael Burke | cs.RO | [PDF](http://arxiv.org/pdf/2309.11698v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) are a powerful tool for implicit scene
representations, allowing for differentiable rendering and the ability to make
predictions about previously unseen viewpoints. From a robotics perspective,
there has been growing interest in object and scene-based localisation using
NeRFs, with a number of recent works relying on sampling-based or Monte-Carlo
localisation schemes. Unfortunately, these can be extremely computationally
expensive, requiring multiple network forward passes to infer camera or object
pose. To alleviate this, a variety of sampling strategies have been applied,
many relying on keypoint recognition techniques from classical computer vision.
This work conducts a systematic empirical comparison of these approaches and
shows that in contrast to conventional feature matching approaches for
geometry-based localisation, sampling-based localisation using NeRFs benefits
significantly from stable features. Results show that rendering stable features
can result in a tenfold reduction in the number of forward passes required, a
significant speed improvement.

---

## NeuralLabeling: A versatile toolset for labeling vision datasets using  Neural Radiance Fields

nerf{: .label .label-blue }

2023-09-21 | Floris Erich, Naoya Chiba, Yusuke Yoshiyasu, Noriaki Ando, Ryo Hanai, Yukiyasu Domae | cs.CV | [PDF](http://arxiv.org/pdf/2309.11966v1){: .btn .btn-green }

**Abstract**: We present NeuralLabeling, a labeling approach and toolset for annotating a
scene using either bounding boxes or meshes and generating segmentation masks,
affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth
maps and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as
renderer, allowing labeling to be performed using 3D spatial tools while
incorporating geometric clues such as occlusions, relying only on images
captured from multiple viewpoints as input. To demonstrate the applicability of
NeuralLabeling to a practical problem in robotics, we added ground truth depth
maps to 30000 frames of transparent object RGB and noisy depth maps of glasses
placed in a dishwasher captured using an RGBD sensor, yielding the
Dishwasher30k dataset. We show that training a simple deep neural network with
supervision using the annotated depth maps yields a higher reconstruction
performance than training with the previously applied weakly supervised
approach.

Comments:
- 8 pages, project website:
  https://florise.github.io/neural_labeling_web/

---

## Fast Satellite Tensorial Radiance Field for Multi-date Satellite Imagery  of Large Size

nerf{: .label .label-blue }

2023-09-21 | Tongtong Zhang, Yuanxiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2309.11767v1){: .btn .btn-green }

**Abstract**: Existing NeRF models for satellite images suffer from slow speeds, mandatory
solar information as input, and limitations in handling large satellite images.
In response, we present SatensoRF, which significantly accelerates the entire
process while employing fewer parameters for satellite imagery of large size.
Besides, we observed that the prevalent assumption of Lambertian surfaces in
neural radiance fields falls short for vegetative and aquatic elements. In
contrast to the traditional hierarchical MLP-based scene representation, we
have chosen a multiscale tensor decomposition approach for color, volume
density, and auxiliary variables to model the lightfield with specular color.
Additionally, to rectify inconsistencies in multi-date imagery, we incorporate
total variation loss to restore the density tensor field and treat the problem
as a denosing task.To validate our approach, we conducted assessments of
SatensoRF using subsets from the spacenet multi-view dataset, which includes
both multi-date and single-date multi-view RGB images. Our results clearly
demonstrate that SatensoRF surpasses the state-of-the-art Sat-NeRF series in
terms of novel view synthesis performance. Significantly, SatensoRF requires
fewer parameters for training, resulting in faster training and inference
speeds and reduced computational demands.

---

## ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average  Texture and Mesh Encoding

nerf{: .label .label-blue }

2023-09-21 | Yu Cheng, Bo Wang, Robby T. Tan | cs.CV | [PDF](http://arxiv.org/pdf/2309.12183v1){: .btn .btn-green }

**Abstract**: In 3D human shape and pose estimation from a monocular video, models trained
with limited labeled data cannot generalize well to videos with occlusion,
which is common in the wild videos. The recent human neural rendering
approaches focusing on novel view synthesis initialized by the off-the-shelf
human shape and pose methods have the potential to correct the initial human
shape. However, the existing methods have some drawbacks such as, erroneous in
handling occlusion, sensitive to inaccurate human segmentation, and ineffective
loss computation due to the non-regularized opacity field. To address these
problems, we introduce ORTexME, an occlusion-robust temporal method that
utilizes temporal information from the input video to better regularize the
occluded body parts. While our ORTexME is based on NeRF, to determine the
reliable regions for the NeRF ray sampling, we utilize our novel average
texture learning approach to learn the average appearance of a person, and to
infer a mask based on the average texture. In addition, to guide the
opacity-field updates in NeRF to suppress blur and noise, we propose the use of
human body mesh. The quantitative evaluation demonstrates that our method
achieves significant improvement on the challenging multi-person 3DPW dataset,
where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-based
methods fail and enlarge the error up to 5.6 on the same dataset.

Comments:
- 8 pages, 8 figures

---

## GenLayNeRF: Generalizable Layered Representations with 3D Model  Alignment for Multi-Human View Synthesis

nerf{: .label .label-blue }

2023-09-20 | Youssef Abdelkareem, Shady Shehata, Fakhri Karray | cs.CV | [PDF](http://arxiv.org/pdf/2309.11627v1){: .btn .btn-green }

**Abstract**: Novel view synthesis (NVS) of multi-human scenes imposes challenges due to
the complex inter-human occlusions. Layered representations handle the
complexities by dividing the scene into multi-layered radiance fields, however,
they are mainly constrained to per-scene optimization making them inefficient.
Generalizable human view synthesis methods combine the pre-fitted 3D human
meshes with image features to reach generalization, yet they are mainly
designed to operate on single-human scenes. Another drawback is the reliance on
multi-step optimization techniques for parametric pre-fitting of the 3D body
models that suffer from misalignment with the images in sparse view settings
causing hallucinations in synthesized views. In this work, we propose,
GenLayNeRF, a generalizable layered scene representation for free-viewpoint
rendering of multiple human subjects which requires no per-scene optimization
and very sparse views as input. We divide the scene into multi-human layers
anchored by the 3D body meshes. We then ensure pixel-level alignment of the
body models with the input views through a novel end-to-end trainable module
that carries out iterative parametric correction coupled with multi-view
feature fusion to produce aligned 3D models. For NVS, we extract point-wise
image-aligned and human-anchored features which are correlated and fused using
self-attention and cross-attention modules. We augment low-level RGB values
into the features with an attention-based RGB fusion module. To evaluate our
approach, we construct two multi-human view synthesis datasets; DeepMultiSyn
and ZJU-MultiHuman. The results indicate that our proposed approach outperforms
generalizable and non-human per-scene NeRF methods while performing at par with
layered per-scene methods without test time optimization.

Comments:
- Accepted to GCPR 2023

---

## Language-driven Object Fusion into Neural Radiance Fields with  Pose-Conditioned Dataset Updates



2023-09-20 | Ka Chun Shum, Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2309.11281v2){: .btn .btn-green }

**Abstract**: Neural radiance field is an emerging rendering method that generates
high-quality multi-view consistent images from a neural scene representation
and volume rendering. Although neural radiance field-based techniques are
robust for scene reconstruction, their ability to add or remove objects remains
limited. This paper proposes a new language-driven approach for object
manipulation with neural radiance fields through dataset updates. Specifically,
to insert a new foreground object represented by a set of multi-view images
into a background radiance field, we use a text-to-image diffusion model to
learn and generate combined images that fuse the object of interest into the
given background across views. These combined images are then used for refining
the background radiance field so that we can render view-consistent images
containing both the object and the background. To ensure view consistency, we
propose a dataset updates strategy that prioritizes radiance field training
with camera views close to the already-trained views prior to propagating the
training to remaining views. We show that under the same dataset updates
strategy, we can easily adapt our method for object insertion using data from
text-to-3D models as well as object removal. Experimental results show that our
method generates photorealistic images of the edited scenes, and outperforms
state-of-the-art methods in 3D reconstruction and neural radiance field
blending.

---

## Light Field Diffusion for Single-View Novel View Synthesis

nerf{: .label .label-blue }

2023-09-20 | Yifeng Xiong, Haoyu Ma, Shanlin Sun, Kun Han, Xiaohui Xie | cs.CV | [PDF](http://arxiv.org/pdf/2309.11525v2){: .btn .btn-green }

**Abstract**: Single-view novel view synthesis, the task of generating images from new
viewpoints based on a single reference image, is an important but challenging
task in computer vision. Recently, Denoising Diffusion Probabilistic Model
(DDPM) has become popular in this area due to its strong ability to generate
high-fidelity images. However, current diffusion-based methods directly rely on
camera pose matrices as viewing conditions, globally and implicitly introducing
3D constraints. These methods may suffer from inconsistency among generated
images from different perspectives, especially in regions with intricate
textures and structures. In this work, we present Light Field Diffusion (LFD),
a conditional diffusion-based model for single-view novel view synthesis.
Unlike previous methods that employ camera pose matrices, LFD transforms the
camera view information into light field encoding and combines it with the
reference image. This design introduces local pixel-wise constraints within the
diffusion models, thereby encouraging better multi-view consistency.
Experiments on several datasets show that our LFD can efficiently generate
high-fidelity images and maintain better 3D consistency even in intricate
regions. Our method can generate images with higher quality than NeRF-based
models, and we obtain sample quality similar to other diffusion-based models
but with only one-third of the model size.

---

## Controllable Dynamic Appearance for Neural 3D Portraits

nerf{: .label .label-blue }

2023-09-20 | ShahRukh Athar, Zhixin Shu, Zexiang Xu, Fujun Luan, Sai Bi, Kalyan Sunkavalli, Dimitris Samaras | cs.CV | [PDF](http://arxiv.org/pdf/2309.11009v2){: .btn .btn-green }

**Abstract**: Recent advances in Neural Radiance Fields (NeRFs) have made it possible to
reconstruct and reanimate dynamic portrait scenes with control over head-pose,
facial expressions and viewing direction. However, training such models assumes
photometric consistency over the deformed region e.g. the face must be evenly
lit as it deforms with changing head-pose and facial expression. Such
photometric consistency across frames of a video is hard to maintain, even in
studio environments, thus making the created reanimatable neural portraits
prone to artifacts during reanimation. In this work, we propose CoDyNeRF, a
system that enables the creation of fully controllable 3D portraits in
real-world capture conditions. CoDyNeRF learns to approximate illumination
dependent effects via a dynamic appearance model in the canonical space that is
conditioned on predicted surface normals and the facial expressions and
head-pose deformations. The surface normals prediction is guided using 3DMM
normals that act as a coarse prior for the normals of the human head, where
direct prediction of normals is hard due to rigid and non-rigid deformations
induced by head-pose and facial expression changes. Using only a
smartphone-captured short video of a subject for training, we demonstrate the
effectiveness of our method on free view synthesis of a portrait scene with
explicit head pose and expression controls, and realistic lighting effects. The
project page can be found here:
http://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html

---

## SpikingNeRF: Making Bio-inspired Neural Networks See through the Real  World

nerf{: .label .label-blue }

2023-09-20 | Xingting Yao, Qinghao Hu, Tielong Liu, Zitao Mo, Zeyu Zhu, Zhengyang Zhuge, Jian Cheng | cs.NE | [PDF](http://arxiv.org/pdf/2309.10987v3){: .btn .btn-green }

**Abstract**: Spiking neural networks (SNNs) have been thriving on numerous tasks to
leverage their promising energy efficiency and exploit their potentialities as
biologically plausible intelligence. Meanwhile, the Neural Radiance Fields
(NeRF) render high-quality 3D scenes with massive energy consumption, but few
works delve into the energy-saving solution with a bio-inspired approach. In
this paper, we propose SpikingNeRF, which aligns the radiance ray with the
temporal dimension of SNN, to naturally accommodate the SNN to the
reconstruction of Radiance Fields. Thus, the computation turns into a
spike-based, multiplication-free manner, reducing the energy consumption. In
SpikingNeRF, each sampled point on the ray is matched onto a particular time
step, and represented in a hybrid manner where the voxel grids are maintained
as well. Based on the voxel grids, sampled points are determined whether to be
masked for better training and inference. However, this operation also incurs
irregular temporal length. We propose the temporal padding strategy to tackle
the masked samples to maintain regular temporal length, i.e., regular tensors,
and the temporal condensing strategy to form a denser data structure for
hardware-friendly computation. Extensive experiments on various datasets
demonstrate that our method reduces the 70.79% energy consumption on average
and obtains comparable synthesis quality with the ANN baseline.

---

## Steganography for Neural Radiance Fields by Backdooring

nerf{: .label .label-blue }

2023-09-19 | Weina Dong, Jia Liu, Yan Ke, Lifeng Chen, Wenquan Sun, Xiaozhong Pan | cs.CR | [PDF](http://arxiv.org/pdf/2309.10503v1){: .btn .btn-green }

**Abstract**: The utilization of implicit representation for visual data (such as images,
videos, and 3D models) has recently gained significant attention in computer
vision research. In this letter, we propose a novel model steganography scheme
with implicit neural representation. The message sender leverages Neural
Radiance Fields (NeRF) and its viewpoint synthesis capabilities by introducing
a viewpoint as a key. The NeRF model generates a secret viewpoint image, which
serves as a backdoor. Subsequently, we train a message extractor using
overfitting to establish a one-to-one mapping between the secret message and
the secret viewpoint image. The sender delivers the trained NeRF model and the
message extractor to the receiver over the open channel, and the receiver
utilizes the key shared by both parties to obtain the rendered image in the
secret view from the NeRF model, and then obtains the secret message through
the message extractor. The inherent complexity of the viewpoint information
prevents attackers from stealing the secret message accurately. Experimental
results demonstrate that the message extractor trained in this letter achieves
high-capacity steganography with fast performance, achieving a 100\% accuracy
in message extraction. Furthermore, the extensive viewpoint key space of NeRF
ensures the security of the steganography scheme.

Comments:
- 6 pages, 7 figures

---

## Locally Stylized Neural Radiance Fields

nerf{: .label .label-blue }

2023-09-19 | Hong-Wing Pang, Binh-Son Hua, Sai-Kit Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2309.10684v1){: .btn .btn-green }

**Abstract**: In recent years, there has been increasing interest in applying stylization
on 3D scenes from a reference style image, in particular onto neural radiance
fields (NeRF). While performing stylization directly on NeRF guarantees
appearance consistency over arbitrary novel views, it is a challenging problem
to guide the transfer of patterns from the style image onto different parts of
the NeRF scene. In this work, we propose a stylization framework for NeRF based
on local style transfer. In particular, we use a hash-grid encoding to learn
the embedding of the appearance and geometry components, and show that the
mapping defined by the hash table allows us to control the stylization to a
certain extent. Stylization is then achieved by optimizing the appearance
branch while keeping the geometry branch fixed. To support local style
transfer, we propose a new loss function that utilizes a segmentation network
and bipartite matching to establish region correspondences between the style
image and the content images obtained from volume rendering. Our experiments
show that our method yields plausible stylization results with novel view
synthesis while having flexible controllability via manipulating and
customizing the region correspondences.

Comments:
- ICCV 2023

---

## RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering  Supervision

nerf{: .label .label-blue }

2023-09-18 | Mingjie Pan, Jiaming Liu, Renrui Zhang, Peixiang Huang, Xiaoqi Li, Li Liu, Shanghang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2309.09502v1){: .btn .btn-green }

**Abstract**: 3D occupancy prediction holds significant promise in the fields of robot
perception and autonomous driving, which quantifies 3D scenes into grid cells
with semantic labels. Recent works mainly utilize complete occupancy labels in
3D voxel space for supervision. However, the expensive annotation process and
sometimes ambiguous labels have severely constrained the usability and
scalability of 3D occupancy models. To address this, we present RenderOcc, a
novel paradigm for training 3D occupancy models only using 2D labels.
Specifically, we extract a NeRF-style 3D volume representation from multi-view
images, and employ volume rendering techniques to establish 2D renderings, thus
enabling direct 3D supervision from 2D semantics and depth labels.
Additionally, we introduce an Auxiliary Ray method to tackle the issue of
sparse viewpoints in autonomous driving scenarios, which leverages sequential
frames to construct comprehensive 2D rendering for each object. To our best
knowledge, RenderOcc is the first attempt to train multi-view 3D occupancy
models only using 2D labels, reducing the dependence on costly 3D occupancy
annotations. Extensive experiments demonstrate that RenderOcc achieves
comparable performance to models fully supervised with 3D labels, underscoring
the significance of this approach in real-world applications.

---

## Instant Photorealistic Style Transfer: A Lightweight and Adaptive  Approach

nerf{: .label .label-blue }

2023-09-18 | Rong Liu, Enyu Zhao, Zhiyuan Liu, Andrew Feng, Scott John Easley | cs.CV | [PDF](http://arxiv.org/pdf/2309.10011v2){: .btn .btn-green }

**Abstract**: In this paper, we propose an Instant Photorealistic Style Transfer (IPST)
approach, designed to achieve instant photorealistic style transfer on
super-resolution inputs without the need for pre-training on pair-wise datasets
or imposing extra constraints. Our method utilizes a lightweight StyleNet to
enable style transfer from a style image to a content image while preserving
non-color information. To further enhance the style transfer process, we
introduce an instance-adaptive optimization to prioritize the photorealism of
outputs and accelerate the convergence of the style network, leading to a rapid
training completion within seconds. Moreover, IPST is well-suited for
multi-frame style transfer tasks, as it retains temporal and multi-view
consistency of the multi-frame inputs such as video and Neural Radiance Field
(NeRF). Experimental results demonstrate that IPST requires less GPU memory
usage, offers faster multi-frame transfer speed, and generates photorealistic
outputs, making it a promising solution for various photorealistic transfer
applications.

Comments:
- 8 pages (reference excluded), 6 figures, 4 tables

---

## NeRF-VINS: A Real-time Neural Radiance Field Map-based Visual-Inertial  Navigation System

nerf{: .label .label-blue }

2023-09-17 | Saimouli Katragadda, Woosik Lee, Yuxiang Peng, Patrick Geneva, Chuchu Chen, Chao Guo, Mingyang Li, Guoquan Huang | cs.RO | [PDF](http://arxiv.org/pdf/2309.09295v1){: .btn .btn-green }

**Abstract**: Achieving accurate, efficient, and consistent localization within an a priori
environment map remains a fundamental challenge in robotics and computer
vision. Conventional map-based keyframe localization often suffers from
sub-optimal viewpoints due to limited field of view (FOV), thus degrading its
performance. To address this issue, in this paper, we design a real-time
tightly-coupled Neural Radiance Fields (NeRF)-aided visual-inertial navigation
system (VINS), termed NeRF-VINS. By effectively leveraging NeRF's potential to
synthesize novel views, essential for addressing limited viewpoints, the
proposed NeRF-VINS optimally fuses IMU and monocular image measurements along
with synthetically rendered images within an efficient filter-based framework.
This tightly coupled integration enables 3D motion tracking with bounded error.
We extensively compare the proposed NeRF-VINS against the state-of-the-art
methods that use prior map information, which is shown to achieve superior
performance. We also demonstrate the proposed method is able to perform
real-time estimation at 15 Hz, on a resource-constrained Jetson AGX Orin
embedded platform with impressive accuracy.

Comments:
- 6 pages, 7 figures

---

## DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic  NeRF

nerf{: .label .label-blue }

2023-09-16 | Mert Asim Karaoglu, Hannah Schieber, Nicolas Schischka, Melih Görgülü, Florian Grötzner, Alexander Ladikos, Daniel Roth, Nassir Navab, Benjamin Busam | cs.CV | [PDF](http://arxiv.org/pdf/2309.08927v1){: .btn .btn-green }

**Abstract**: Dynamic reconstruction with neural radiance fields (NeRF) requires accurate
camera poses. These are often hard to retrieve with existing
structure-from-motion (SfM) pipelines as both camera and scene content can
change. We propose DynaMoN that leverages simultaneous localization and mapping
(SLAM) jointly with motion masking to handle dynamic scene content. Our robust
SLAM-based tracking module significantly accelerates the training process of
the dynamic NeRF while improving the quality of synthesized views at the same
time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and
the DyCheck's iPhone dataset, three real-world datasets, shows the advantages
of DynaMoN both for camera pose estimation and novel view synthesis.

Comments:
- 6 pages, 4 figures

---

## Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion

nerf{: .label .label-blue }

2023-09-15 | Weng Fei Low, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2309.08596v1){: .btn .btn-green }

**Abstract**: Event cameras offer many advantages over standard cameras due to their
distinctive principle of operation: low power, low latency, high temporal
resolution and high dynamic range. Nonetheless, the success of many downstream
visual applications also hinges on an efficient and effective scene
representation, where Neural Radiance Field (NeRF) is seen as the leading
candidate. Such promise and potential of event cameras and NeRF inspired recent
works to investigate on the reconstruction of NeRF from moving event cameras.
However, these works are mainly limited in terms of the dependence on dense and
low-noise event streams, as well as generalization to arbitrary contrast
threshold values and camera speed profiles. In this work, we propose Robust
e-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving
event cameras under various real-world conditions, especially from sparse and
noisy events generated under non-uniform motion. It consists of two key
components: a realistic event generation model that accounts for various
intrinsic parameters (e.g. time-independent, asymmetric threshold and
refractory period) and non-idealities (e.g. pixel-to-pixel threshold
variation), as well as a complementary pair of normalized reconstruction losses
that can effectively generalize to arbitrary speed profiles and intrinsic
parameter values without such prior knowledge. Experiments on real and novel
realistically simulated sequences verify our effectiveness. Our code, synthetic
dataset and improved event simulator are public.

Comments:
- Accepted to ICCV 2023. Project website is accessible at
  https://wengflow.github.io/robust-e-nerf

---

## Breathing New Life into 3D Assets with Generative Repainting



2023-09-15 | Tianfu Wang, Menelaos Kanakis, Konrad Schindler, Luc Van Gool, Anton Obukhov | cs.CV | [PDF](http://arxiv.org/pdf/2309.08523v2){: .btn .btn-green }

**Abstract**: Diffusion-based text-to-image models ignited immense attention from the
vision community, artists, and content creators. Broad adoption of these models
is due to significant improvement in the quality of generations and efficient
conditioning on various modalities, not just text. However, lifting the rich
generative priors of these 2D models into 3D is challenging. Recent works have
proposed various pipelines powered by the entanglement of diffusion models and
neural fields. We explore the power of pretrained 2D diffusion models and
standard 3D neural radiance fields as independent, standalone tools and
demonstrate their ability to work together in a non-learned fashion. Such
modularity has the intrinsic advantage of eased partial upgrades, which became
an important property in such a fast-paced domain. Our pipeline accepts any
legacy renderable geometry, such as textured or untextured meshes, orchestrates
the interaction between 2D generative refinement and 3D consistency enforcement
tools, and outputs a painted input geometry in several formats. We conduct a
large-scale study on a wide range of objects and categories from the
ShapeNetSem dataset and demonstrate the advantages of our approach, both
qualitatively and quantitatively. Project page:
https://www.obukhov.ai/repainting_3d_assets

---

## Deformable Neural Radiance Fields using RGB and Event Cameras



2023-09-15 | Qi Ma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2309.08416v2){: .btn .btn-green }

**Abstract**: Modeling Neural Radiance Fields for fast-moving deformable objects from
visual data alone is a challenging problem. A major issue arises due to the
high deformation and low acquisition rates. To address this problem, we propose
to use event cameras that offer very fast acquisition of visual change in an
asynchronous manner. In this work, we develop a novel method to model the
deformable neural radiance fields using RGB and event cameras. The proposed
method uses the asynchronous stream of events and calibrated sparse RGB frames.
In our setup, the camera pose at the individual events required to integrate
them into the radiance fields remains unknown. Our method jointly optimizes
these poses and the radiance field. This happens efficiently by leveraging the
collection of events at once and actively sampling the events during learning.
Experiments conducted on both realistically rendered graphics and real-world
datasets demonstrate a significant benefit of the proposed method over the
state-of-the-art and the compared baseline.
  This shows a promising direction for modeling deformable neural radiance
fields in real-world dynamic scenes.

---

## Indoor Scene Reconstruction with Fine-Grained Details Using Hybrid  Representation and Normal Prior Enhancement



2023-09-14 | Sheng Ye, Yubin Hu, Matthieu Lin, Yu-Hui Wen, Wang Zhao, Yong-Jin Liu, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2309.07640v2){: .btn .btn-green }

**Abstract**: The reconstruction of indoor scenes from multi-view RGB images is challenging
due to the coexistence of flat and texture-less regions alongside delicate and
fine-grained regions. Recent methods leverage neural radiance fields aided by
predicted surface normal priors to recover the scene geometry. These methods
excel in producing complete and smooth results for floor and wall areas.
However, they struggle to capture complex surfaces with high-frequency
structures due to the inadequate neural representation and the inaccurately
predicted normal priors. This work aims to reconstruct high-fidelity surfaces
with fine-grained details by addressing the above limitations. To improve the
capacity of the implicit representation, we propose a hybrid architecture to
represent low-frequency and high-frequency regions separately. To enhance the
normal priors, we introduce a simple yet effective image sharpening and
denoising technique, coupled with a network that estimates the pixel-wise
uncertainty of the predicted surface normal vectors. Identifying such
uncertainty can prevent our model from being misled by unreliable surface
normal supervisions that hinder the accurate reconstruction of intricate
geometries. Experiments on the benchmark datasets show that our method
outperforms existing methods in terms of reconstruction quality. Furthermore,
the proposed method also generalizes well to real-world indoor scenarios
captured by our hand-held mobile phones. Our code is publicly available at:
https://github.com/yec22/Fine-Grained-Indoor-Recon.

---

## CoRF : Colorizing Radiance Fields using Knowledge Distillation

nerf{: .label .label-blue }

2023-09-14 | Ankit Dhiman, R Srinath, Srinjay Sarkar, Lokesh R Boregowda, R Venkatesh Babu | cs.CV | [PDF](http://arxiv.org/pdf/2309.07668v1){: .btn .btn-green }

**Abstract**: Neural radiance field (NeRF) based methods enable high-quality novel-view
synthesis for multi-view images. This work presents a method for synthesizing
colorized novel views from input grey-scale multi-view images. When we apply
image or video-based colorization methods on the generated grey-scale novel
views, we observe artifacts due to inconsistency across views. Training a
radiance field network on the colorized grey-scale image sequence also does not
solve the 3D consistency issue. We propose a distillation based method to
transfer color knowledge from the colorization networks trained on natural
images to the radiance field network. Specifically, our method uses the
radiance field network as a 3D representation and transfers knowledge from
existing 2D colorization methods. The experimental results demonstrate that the
proposed method produces superior colorized novel views for indoor and outdoor
scenes while maintaining cross-view consistency than baselines. Further, we
show the efficacy of our method on applications like colorization of radiance
field network trained from 1.) Infra-Red (IR) multi-view images and 2.) Old
grey-scale multi-view image sequences.

Comments:
- AI3DCC @ ICCV 2023

---

## DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for  High-Fidelity Talking Portrait Synthesis

nerf{: .label .label-blue }

2023-09-14 | Yaoyu Su, Shaohui Wang, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2309.07752v1){: .btn .btn-green }

**Abstract**: In this paper, we present the decomposed triplane-hash neural radiance fields
(DT-NeRF), a framework that significantly improves the photorealistic rendering
of talking faces and achieves state-of-the-art results on key evaluation
datasets. Our architecture decomposes the facial region into two specialized
triplanes: one specialized for representing the mouth, and the other for the
broader facial features. We introduce audio features as residual terms and
integrate them as query vectors into our model through an audio-mouth-face
transformer. Additionally, our method leverages the capabilities of Neural
Radiance Fields (NeRF) to enrich the volumetric representation of the entire
face through additive volumetric rendering techniques. Comprehensive
experimental evaluations corroborate the effectiveness and superiority of our
proposed approach.

Comments:
- 5 pages, 5 figures. Submitted to ICASSP 2024

---

## MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image  Acquisition Systems

nerf{: .label .label-blue }

2023-09-14 | Yu Gao, Lutong Su, Hao Liang, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2309.07846v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) employ multi-view images for 3D scene
representation and have shown remarkable performance. As one of the primary
sources of multi-view images, multi-camera systems encounter challenges such as
varying intrinsic parameters and frequent pose changes. Most previous
NeRF-based methods often assume a global unique camera and seldom consider
scenarios with multiple cameras. Besides, some pose-robust methods still remain
susceptible to suboptimal solutions when poses are poor initialized. In this
paper, we propose MC-NeRF, a method can jointly optimize both intrinsic and
extrinsic parameters for bundle-adjusting Neural Radiance Fields. Firstly, we
conduct a theoretical analysis to tackle the degenerate case and coupling issue
that arise from the joint optimization between intrinsic and extrinsic
parameters. Secondly, based on the proposed solutions, we introduce an
efficient calibration image acquisition scheme for multi-camera systems,
including the design of calibration object. Lastly, we present a global
end-to-end network with training sequence that enables the regression of
intrinsic and extrinsic parameters, along with the rendering network. Moreover,
most existing datasets are designed for unique camera, we create a new dataset
that includes four different styles of multi-camera acquisition systems,
allowing readers to generate custom datasets. Experiments confirm the
effectiveness of our method when each image corresponds to different camera
parameters. Specifically, we adopt up to 110 images with 110 different
intrinsic and extrinsic parameters, to achieve 3D scene representation without
providing initial poses. The Code and supplementary materials are available at
https://in2-viaun.github.io/MC-NeRF.

Comments:
- This manuscript is currently under review

---

## Gradient based Grasp Pose Optimization on a NeRF that Approximates Grasp  Success

nerf{: .label .label-blue }

2023-09-14 | Gergely Sóti, Björn Hein, Christian Wurll | cs.RO | [PDF](http://arxiv.org/pdf/2309.08040v1){: .btn .btn-green }

**Abstract**: Current robotic grasping methods often rely on estimating the pose of the
target object, explicitly predicting grasp poses, or implicitly estimating
grasp success probabilities. In this work, we propose a novel approach that
directly maps gripper poses to their corresponding grasp success values,
without considering objectness. Specifically, we leverage a Neural Radiance
Field (NeRF) architecture to learn a scene representation and use it to train a
grasp success estimator that maps each pose in the robot's task space to a
grasp success value. We employ this learned estimator to tune its inputs, i.e.,
grasp poses, by gradient-based optimization to obtain successful grasp poses.
Contrary to other NeRF-based methods which enhance existing grasp pose
estimation approaches by relying on NeRF's rendering capabilities or directly
estimate grasp poses in a discretized space using NeRF's scene representation
capabilities, our approach uniquely sidesteps both the need for rendering and
the limitation of discretization. We demonstrate the effectiveness of our
approach on four simulated 3DoF (Degree of Freedom) robotic grasping tasks and
show that it can generalize to novel objects. Our best model achieves an
average translation error of 3mm from valid grasp poses. This work opens the
door for future research to apply our approach to higher DoF grasps and
real-world scenarios.

---

## Spec-NeRF: Multi-spectral Neural Radiance Fields

nerf{: .label .label-blue }

2023-09-14 | Jiabao Li, Yuqi Li, Ciliang Sun, Chong Wang, Jinhui Xiang | eess.IV | [PDF](http://arxiv.org/pdf/2310.12987v1){: .btn .btn-green }

**Abstract**: We propose Multi-spectral Neural Radiance Fields(Spec-NeRF) for jointly
reconstructing a multispectral radiance field and spectral sensitivity
functions(SSFs) of the camera from a set of color images filtered by different
filters. The proposed method focuses on modeling the physical imaging process,
and applies the estimated SSFs and radiance field to synthesize novel views of
multispectral scenes. In this method, the data acquisition requires only a
low-cost trichromatic camera and several off-the-shelf color filters, making it
more practical than using specialized 3D scanning and spectral imaging
equipment. Our experiments on both synthetic and real scenario datasets
demonstrate that utilizing filtered RGB images with learnable NeRF and SSFs can
achieve high fidelity and promising spectral reconstruction while retaining the
inherent capability of NeRF to comprehend geometric structures. Code is
available at https://github.com/CPREgroup/SpecNeRF-v2.

---

## Text-Guided Generation and Editing of Compositional 3D Avatars

nerf{: .label .label-blue }

2023-09-13 | Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus Thies, Michael J. Black | cs.CV | [PDF](http://arxiv.org/pdf/2309.07125v1){: .btn .btn-green }

**Abstract**: Our goal is to create a realistic 3D facial avatar with hair and accessories
using only a text description. While this challenge has attracted significant
recent interest, existing methods either lack realism, produce unrealistic
shapes, or do not support editing, such as modifications to the hairstyle. We
argue that existing methods are limited because they employ a monolithic
modeling approach, using a single representation for the head, face, hair, and
accessories. Our observation is that the hair and face, for example, have very
different structural qualities that benefit from different representations.
Building on this insight, we generate avatars with a compositional model, in
which the head, face, and upper body are represented with traditional 3D
meshes, and the hair, clothing, and accessories with neural radiance fields
(NeRF). The model-based mesh representation provides a strong geometric prior
for the face region, improving realism while enabling editing of the person's
appearance. By using NeRFs to represent the remaining components, our method is
able to model and synthesize parts with complex geometry and appearance, such
as curly hair and fluffy scarves. Our novel system synthesizes these
high-quality compositional avatars from text descriptions. The experimental
results demonstrate that our method, Text-guided generation and Editing of
Compositional Avatars (TECA), produces avatars that are more realistic than
those of recent methods while being editable because of their compositional
nature. For example, our TECA enables the seamless transfer of compositional
features like hairstyles, scarves, and other accessories between avatars. This
capability supports applications such as virtual try-on.

Comments:
- Home page: https://yfeng95.github.io/teca

---

## Dynamic NeRFs for Soccer Scenes

nerf{: .label .label-blue }

2023-09-13 | Sacha Lewin, Maxime Vandegar, Thomas Hoyoux, Olivier Barnich, Gilles Louppe | cs.CV | [PDF](http://arxiv.org/pdf/2309.06802v1){: .btn .btn-green }

**Abstract**: The long-standing problem of novel view synthesis has many applications,
notably in sports broadcasting. Photorealistic novel view synthesis of soccer
actions, in particular, is of enormous interest to the broadcast industry. Yet
only a few industrial solutions have been proposed, and even fewer that achieve
near-broadcast quality of the synthetic replays. Except for their setup of
multiple static cameras around the playfield, the best proprietary systems
disclose close to no information about their inner workings. Leveraging
multiple static cameras for such a task indeed presents a challenge rarely
tackled in the literature, for a lack of public datasets: the reconstruction of
a large-scale, mostly static environment, with small, fast-moving elements.
Recently, the emergence of neural radiance fields has induced stunning progress
in many novel view synthesis applications, leveraging deep learning principles
to produce photorealistic results in the most challenging settings. In this
work, we investigate the feasibility of basing a solution to the task on
dynamic NeRFs, i.e., neural models purposed to reconstruct general dynamic
content. We compose synthetic soccer environments and conduct multiple
experiments using them, identifying key components that help reconstruct soccer
scenes with dynamic NeRFs. We show that, although this approach cannot fully
meet the quality requirements for the target application, it suggests promising
avenues toward a cost-efficient, automatic solution. We also make our work
dataset and code publicly available, with the goal to encourage further efforts
from the research community on the task of novel view synthesis for dynamic
soccer scenes. For code, data, and video results, please see
https://soccernerfs.isach.be.

Comments:
- Accepted at the 6th International ACM Workshop on Multimedia Content
  Analysis in Sports. 8 pages, 9 figures. Project page:
  https://soccernerfs.isach.be

---

## Learning Disentangled Avatars with Hybrid 3D Representations



2023-09-12 | Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, Michael J. Black | cs.CV | [PDF](http://arxiv.org/pdf/2309.06441v1){: .btn .btn-green }

**Abstract**: Tremendous efforts have been made to learn animatable and photorealistic
human avatars. Towards this end, both explicit and implicit 3D representations
are heavily studied for a holistic modeling and capture of the whole human
(e.g., body, clothing, face and hair), but neither representation is an optimal
choice in terms of representation efficacy since different parts of the human
avatar have different modeling desiderata. For example, meshes are generally
not suitable for modeling clothing and hair. Motivated by this, we present
Disentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit
3D representations. DELTA takes a monocular RGB video as input, and produces a
human avatar with separate body and clothing/hair layers. Specifically, we
demonstrate two important applications for DELTA. For the first one, we
consider the disentanglement of the human body and clothing and in the second,
we disentangle the face and hair. To do so, DELTA represents the body or face
with an explicit mesh-based parametric 3D model and the clothing or hair with
an implicit neural radiance field. To make this possible, we design an
end-to-end differentiable renderer that integrates meshes into volumetric
rendering, enabling DELTA to learn directly from monocular videos without any
3D supervision. Finally, we show that how these two applications can be easily
combined to model full-body avatars, such that the hair, face, body and
clothing can be fully disentangled yet jointly rendered. Such a disentanglement
enables hair and clothing transfer to arbitrary body shapes. We empirically
validate the effectiveness of DELTA's disentanglement by demonstrating its
promising performance on disentangled reconstruction, virtual clothing try-on
and hairstyle transfer. To facilitate future research, we also release an
open-sourced pipeline for the study of hybrid human avatar modeling.

Comments:
- home page: https://yfeng95.github.io/delta. arXiv admin note: text
  overlap with arXiv:2210.01868

---

## Federated Learning for Large-Scale Scene Modeling with Neural Radiance  Fields

nerf{: .label .label-blue }

2023-09-12 | Teppei Suzuki | cs.CV | [PDF](http://arxiv.org/pdf/2309.06030v1){: .btn .btn-green }

**Abstract**: We envision a system to continuously build and maintain a map based on
earth-scale neural radiance fields (NeRF) using data collected from vehicles
and drones in a lifelong learning manner. However, existing large-scale
modeling by NeRF has problems in terms of scalability and maintainability when
modeling earth-scale environments. Therefore, to address these problems, we
propose a federated learning pipeline for large-scale modeling with NeRF. We
tailor the model aggregation pipeline in federated learning for NeRF, thereby
allowing local updates of NeRF. In the aggregation step, the accuracy of the
clients' global pose is critical. Thus, we also propose global pose alignment
to align the noisy global pose of clients before the aggregation step. In
experiments, we show the effectiveness of the proposed pose alignment and the
federated learning pipeline on the large-scale scene dataset, Mill19.

---

## PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D  representations for agricultural robotics

nerf{: .label .label-blue }

2023-09-11 | Claus Smitt, Michael Halstead, Patrick Zimmer, Thomas Läbe, Esra Guclu, Cyrill Stachniss, Chris McCool | cs.RO | [PDF](http://arxiv.org/pdf/2309.05339v1){: .btn .btn-green }

**Abstract**: Precise scene understanding is key for most robot monitoring and intervention
tasks in agriculture. In this work we present PAg-NeRF which is a novel
NeRF-based system that enables 3D panoptic scene understanding. Our
representation is trained using an image sequence with noisy robot odometry
poses and automatic panoptic predictions with inconsistent IDs between frames.
Despite this noisy input, our system is able to output scene geometry,
photo-realistic renders and 3D consistent panoptic representations with
consistent instance IDs. We evaluate this novel system in a very challenging
horticultural scenario and in doing so demonstrate an end-to-end trainable
system that can make use of noisy robot poses rather than precise poses that
have to be pre-calculated. Compared to a baseline approach the peak signal to
noise ratio is improved from 21.34dB to 23.37dB while the panoptic quality
improves from 56.65% to 70.08%. Furthermore, our approach is faster and can be
tuned to improve inference time by more than a factor of 2 while being memory
efficient with approximately 12 times fewer parameters.

---

## Editing 3D Scenes via Text Prompts without Retraining

nerf{: .label .label-blue }

2023-09-10 | Shuangkang Fang, Yufeng Wang, Yi Yang, Yi-Hsuan Tsai, Wenrui Ding, Shuchang Zhou, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2309.04917v3){: .btn .btn-green }

**Abstract**: Numerous diffusion models have recently been applied to image synthesis and
editing. However, editing 3D scenes is still in its early stages. It poses
various challenges, such as the requirement to design specific methods for
different editing types, retraining new models for various 3D scenes, and the
absence of convenient human interaction during editing. To tackle these issues,
we introduce a text-driven editing method, termed DN2N, which allows for the
direct acquisition of a NeRF model with universal editing capabilities,
eliminating the requirement for retraining. Our method employs off-the-shelf
text-based editing models of 2D images to modify the 3D scene images, followed
by a filtering process to discard poorly edited images that disrupt 3D
consistency. We then consider the remaining inconsistency as a problem of
removing noise perturbation, which can be solved by generating training data
with similar perturbation characteristics for training. We further propose
cross-view regularization terms to help the generalized NeRF model mitigate
these perturbations. Our text-driven method allows users to edit a 3D scene
with their desired description, which is more friendly, intuitive, and
practical than prior works. Empirical results show that our method achieves
multiple editing types, including but not limited to appearance editing,
weather transition, material changing, and style transfer. Most importantly,
our method generalizes well with editing abilities shared among a set of model
parameters without requiring a customized editing model for some specific
scenes, thus inferring novel views with editing effects directly from user
input. The project website is available at https://sk-fun.fun/DN2N

Comments:
- Project Website: https://sk-fun.fun/DN2N

---

## SC-NeRF: Self-Correcting Neural Radiance Field with Sparse Views

nerf{: .label .label-blue }

2023-09-10 | Liang Song, Guangming Wang, Jiuming Liu, Zhenyang Fu, Yanzi Miao,  Hesheng | cs.CV | [PDF](http://arxiv.org/pdf/2309.05028v1){: .btn .btn-green }

**Abstract**: In recent studies, the generalization of neural radiance fields for novel
view synthesis task has been widely explored. However, existing methods are
limited to objects and indoor scenes. In this work, we extend the
generalization task to outdoor scenes, trained only on object-level datasets.
This approach presents two challenges. Firstly, the significant distributional
shift between training and testing scenes leads to black artifacts in rendering
results. Secondly, viewpoint changes in outdoor scenes cause ghosting or
missing regions in rendered images. To address these challenges, we propose a
geometric correction module and an appearance correction module based on
multi-head attention mechanisms. We normalize rendered depth and combine it
with light direction as query in the attention mechanism. Our network
effectively corrects varying scene structures and geometric features in outdoor
scenes, generalizing well from object-level to unseen outdoor scenes.
Additionally, we use appearance correction module to correct appearance
features, preventing rendering artifacts like blank borders and ghosting due to
viewpoint changes. By combining these modules, our approach successfully
tackles the challenges of outdoor scene generalization, producing high-quality
rendering results. When evaluated on four datasets (Blender, DTU, LLFF,
Spaces), our network outperforms previous methods. Notably, compared to
MVSNeRF, our network improves average PSNR from 19.369 to 25.989, SSIM from
0.838 to 0.889, and reduces LPIPS from 0.265 to 0.224 on Spaces outdoor scenes.

---

## Mirror-Aware Neural Humans

nerf{: .label .label-blue }

2023-09-09 | Daniel Ajisafe, James Tang, Shih-Yang Su, Bastian Wandt, Helge Rhodin | cs.CV | [PDF](http://arxiv.org/pdf/2309.04750v1){: .btn .btn-green }

**Abstract**: Human motion capture either requires multi-camera systems or is unreliable
using single-view input due to depth ambiguities. Meanwhile, mirrors are
readily available in urban environments and form an affordable alternative by
recording two views with only a single camera. However, the mirror setting
poses the additional challenge of handling occlusions of real and mirror image.
Going beyond existing mirror approaches for 3D human pose estimation, we
utilize mirrors for learning a complete body model, including shape and dense
appearance. Our main contributions are extending articulated neural radiance
fields to include a notion of a mirror, making it sample-efficient over
potential occlusion regions. Together, our contributions realize a
consumer-level 3D motion capture system that starts from off-the-shelf 2D poses
by automatically calibrating the camera, estimating mirror orientation, and
subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to
condition the mirror-aware NeRF. We empirically demonstrate the benefit of
learning a body model and accounting for occlusion in challenging mirror
scenes.

Comments:
- Project website:
  https://danielajisafe.github.io/mirror-aware-neural-humans/

---

## DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields

nerf{: .label .label-blue }

2023-09-08 | Junzhe Zhang, Yushi Lan, Shuai Yang, Fangzhou Hong, Quan Wang, Chai Kiat Yeo, Ziwei Liu, Chen Change Loy | cs.CV | [PDF](http://arxiv.org/pdf/2309.04410v1){: .btn .btn-green }

**Abstract**: In this paper, we address the challenging problem of 3D toonification, which
involves transferring the style of an artistic domain onto a target 3D face
with stylized geometry and texture. Although fine-tuning a pre-trained 3D GAN
on the artistic domain can produce reasonable performance, this strategy has
limitations in the 3D domain. In particular, fine-tuning can deteriorate the
original GAN latent space, which affects subsequent semantic editing, and
requires independent optimization and storage for each new style, limiting
flexibility and efficient deployment. To overcome these challenges, we propose
DeformToon3D, an effective toonification framework tailored for hierarchical 3D
GAN. Our approach decomposes 3D toonification into subproblems of geometry and
texture stylization to better preserve the original latent space. Specifically,
we devise a novel StyleField that predicts conditional 3D deformation to align
a real-space NeRF to the style space for geometry stylization. Thanks to the
StyleField formulation, which already handles geometry stylization well,
texture stylization can be achieved conveniently via adaptive style mixing that
injects information of the artistic domain into the decoder of the pre-trained
3D GAN. Due to the unique design, our method enables flexible style degree
control and shape-texture-specific style swap. Furthermore, we achieve
efficient training without any real-world 2D-3D training pairs but proxy
samples synthesized from off-the-shelf 2D toonification models.

Comments:
- ICCV 2023. Code: https://github.com/junzhezhang/DeformToon3D Project
  page: https://www.mmlab-ntu.com/project/deformtoon3d/

---

## Dynamic Mesh-Aware Radiance Fields

nerf{: .label .label-blue }

2023-09-08 | Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin Huang, Ming C. Lin | cs.GR | [PDF](http://arxiv.org/pdf/2309.04581v1){: .btn .btn-green }

**Abstract**: Embedding polygonal mesh assets within photorealistic Neural Radience Fields
(NeRF) volumes, such that they can be rendered and their dynamics simulated in
a physically consistent manner with the NeRF, is under-explored from the system
perspective of integrating NeRF into the traditional graphics pipeline. This
paper designs a two-way coupling between mesh and NeRF during rendering and
simulation. We first review the light transport equations for both mesh and
NeRF, then distill them into an efficient algorithm for updating radiance and
throughput along a cast ray with an arbitrary number of bounces. To resolve the
discrepancy between the linear color space that the path tracer assumes and the
sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range
(HDR) images. We also present a strategy to estimate light sources and cast
shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric
formulation can be efficiently integrated with a high-performance physics
simulator that supports cloth, rigid and soft bodies. The full rendering and
simulation system can be run on a GPU at interactive rates. We show that a
hybrid system approach outperforms alternatives in visual realism for mesh
insertion, because it allows realistic light transport from volumetric NeRF
media onto surfaces, which affects the appearance of reflective/refractive
surfaces and illumination of diffuse surfaces informed by the dynamic scene.

Comments:
- ICCV 2023

---

## SimpleNeRF: Regularizing Sparse Input Neural Radiance Fields with  Simpler Solutions

nerf{: .label .label-blue }

2023-09-07 | Nagabhushan Somraj, Adithyan Karanayil, Rajiv Soundararajan | cs.CV | [PDF](http://arxiv.org/pdf/2309.03955v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) show impressive performance for the
photorealistic free-view rendering of scenes. However, NeRFs require dense
sampling of images in the given scene, and their performance degrades
significantly when only a sparse set of views are available. Researchers have
found that supervising the depth estimated by the NeRF helps train it
effectively with fewer views. The depth supervision is obtained either using
classical approaches or neural networks pre-trained on a large dataset. While
the former may provide only sparse supervision, the latter may suffer from
generalization issues. As opposed to the earlier approaches, we seek to learn
the depth supervision by designing augmented models and training them along
with the NeRF. We design augmented models that encourage simpler solutions by
exploring the role of positional encoding and view-dependent radiance in
training the few-shot NeRF. The depth estimated by these simpler models is used
to supervise the NeRF depth estimates. Since the augmented models can be
inaccurate in certain regions, we design a mechanism to choose only reliable
depth estimates for supervision. Finally, we add a consistency loss between the
coarse and fine multi-layer perceptrons of the NeRF to ensure better
utilization of hierarchical sampling. We achieve state-of-the-art
view-synthesis performance on two popular datasets by employing the above
regularizations. The source code for our model can be found on our project
page: https://nagabhushansn95.github.io/publications/2023/SimpleNeRF.html

Comments:
- SIGGRAPH Asia 2023

---

## BluNF: Blueprint Neural Field

nerf{: .label .label-blue }

2023-09-07 | Robin Courant, Xi Wang, Marc Christie, Vicky Kalogeiton | cs.CV | [PDF](http://arxiv.org/pdf/2309.03933v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have revolutionized scene novel view
synthesis, offering visually realistic, precise, and robust implicit
reconstructions. While recent approaches enable NeRF editing, such as object
removal, 3D shape modification, or material property manipulation, the manual
annotation prior to such edits makes the process tedious. Additionally,
traditional 2D interaction tools lack an accurate sense of 3D space, preventing
precise manipulation and editing of scenes. In this paper, we introduce a novel
approach, called Blueprint Neural Field (BluNF), to address these editing
issues. BluNF provides a robust and user-friendly 2D blueprint, enabling
intuitive scene editing. By leveraging implicit neural representation, BluNF
constructs a blueprint of a scene using prior semantic and depth information.
The generated blueprint allows effortless editing and manipulation of NeRF
representations. We demonstrate BluNF's editability through an intuitive
click-and-change mechanism, enabling 3D manipulations, such as masking,
appearance modification, and object removal. Our approach significantly
contributes to visual content creation, paving the way for further research in
this area.

Comments:
- ICCV-W (AI3DCC) 2023. Project page with videos and code:
  https://www.lix.polytechnique.fr/vista/projects/2023_iccvw_courant/

---

## Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance  Fields using Geometry-Guided Text-to-Image Diffusion Model

nerf{: .label .label-blue }

2023-09-07 | Sungwon Hwang, Junha Hyung, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2309.03550v1){: .btn .btn-green }

**Abstract**: Recent advances in diffusion models such as ControlNet have enabled
geometrically controllable, high-fidelity text-to-image generation. However,
none of them addresses the question of adding such controllability to
text-to-3D generation. In response, we propose Text2Control3D, a controllable
text-to-3D avatar generation method whose facial expression is controllable
given a monocular video casually captured with hand-held camera. Our main
strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF)
optimized with a set of controlled viewpoint-aware images that we generate from
ControlNet, whose condition input is the depth map extracted from the input
video. When generating the viewpoint-aware images, we utilize cross-reference
attention to inject well-controlled, referential facial expression and
appearance via cross attention. We also conduct low-pass filtering of Gaussian
latent of the diffusion model in order to ameliorate the viewpoint-agnostic
texture problem we observed from our empirical analysis, where the
viewpoint-aware images contain identical textures on identical pixel positions
that are incomprehensible in 3D. Finally, to train NeRF with the images that
are viewpoint-aware yet are not strictly consistent in geometry, our approach
considers per-image geometric variation as a view of deformation from a shared
3D canonical space. Consequently, we construct the 3D avatar in a canonical
space of deformable NeRF by learning a set of per-image deformation via
deformation field table. We demonstrate the empirical results and discuss the
effectiveness of our method.

Comments:
- Project page: https://text2control3d.github.io/

---

## Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields

nerf{: .label .label-blue }

2023-09-06 | Lily Goli, Cody Reading, Silvia Sellán, Alec Jacobson, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2309.03185v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have shown promise in applications like view
synthesis and depth estimation, but learning from multiview images faces
inherent uncertainties. Current methods to quantify them are either heuristic
or computationally demanding. We introduce BayesRays, a post-hoc framework to
evaluate uncertainty in any pre-trained NeRF without modifying the training
process. Our method establishes a volumetric uncertainty field using spatial
perturbations and a Bayesian Laplace approximation. We derive our algorithm
statistically and show its superior performance in key metrics and
applications. Additional results available at: https://bayesrays.github.io.

---

## ResFields: Residual Neural Fields for Spatiotemporal Signals

nerf{: .label .label-blue }

2023-09-06 | Marko Mihajlovic, Sergey Prokudin, Marc Pollefeys, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2309.03160v2){: .btn .btn-green }

**Abstract**: Neural fields, a category of neural networks trained to represent
high-frequency signals, have gained significant attention in recent years due
to their impressive performance in modeling complex 3D data, especially large
neural signed distance (SDFs) or radiance fields (NeRFs) via a single
multi-layer perceptron (MLP). However, despite the power and simplicity of
representing signals with an MLP, these methods still face challenges when
modeling large and complex temporal signals due to the limited capacity of
MLPs. In this paper, we propose an effective approach to address this
limitation by incorporating temporal residual layers into neural fields, dubbed
ResFields, a novel class of networks specifically designed to effectively
represent complex temporal signals. We conduct a comprehensive analysis of the
properties of ResFields and propose a matrix factorization technique to reduce
the number of trainable parameters and enhance generalization capabilities.
Importantly, our formulation seamlessly integrates with existing techniques and
consistently improves results across various challenging tasks: 2D video
approximation, dynamic shape modeling via temporal SDFs, and dynamic NeRF
reconstruction. Lastly, we demonstrate the practical utility of ResFields by
showcasing its effectiveness in capturing dynamic 3D scenes from sparse sensory
inputs of a lightweight capture system.

Comments:
- Project page and code at https://markomih.github.io/ResFields/

---

## Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF

nerf{: .label .label-blue }

2023-09-04 | Leheng Li, Qing Lian, Ying-Cong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2309.01351v1){: .btn .btn-green }

**Abstract**: Deep neural networks (DNNs) have been proven extremely susceptible to
adversarial examples, which raises special safety-critical concerns for
DNN-based autonomous driving stacks (i.e., 3D object detection). Although there
are extensive works on image-level attacks, most are restricted to 2D pixel
spaces, and such attacks are not always physically realistic in our 3D world.
Here we present Adv3D, the first exploration of modeling adversarial examples
as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic
appearances and 3D accurate generation, yielding a more realistic and
realizable adversarial example. We train our adversarial NeRF by minimizing the
surrounding objects' confidence predicted by 3D detectors on the training set.
Then we evaluate Adv3D on the unseen validation set and show that it can cause
a large performance reduction when rendering NeRF in any sampled pose. To
generate physically realizable adversarial examples, we propose primitive-aware
sampling and semantic-guided regularization that enable 3D patch attacks with
camouflage adversarial texture. Experimental results demonstrate that the
trained adversarial NeRF generalizes well to different poses, scenes, and 3D
detectors. Finally, we provide a defense method to our attacks that involves
adversarial training through data augmentation. Project page:
https://len-li.github.io/adv3d-web

---

## Instant Continual Learning of Neural Radiance Fields

nerf{: .label .label-blue }

2023-09-04 | Ryan Po, Zhengyang Dong, Alexander W. Bergman, Gordon Wetzstein | cs.CV | [PDF](http://arxiv.org/pdf/2309.01811v2){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) have emerged as an effective method for
novel-view synthesis and 3D scene reconstruction. However, conventional
training methods require access to all training views during scene
optimization. This assumption may be prohibitive in continual learning
scenarios, where new data is acquired in a sequential manner and a continuous
update of the NeRF is desired, as in automotive or remote sensing applications.
When naively trained in such a continual setting, traditional scene
representation frameworks suffer from catastrophic forgetting, where previously
learned knowledge is corrupted after training on new data. Prior works in
alleviating forgetting with NeRFs suffer from low reconstruction quality and
high latency, making them impractical for real-world application. We propose a
continual learning framework for training NeRFs that leverages replay-based
methods combined with a hybrid explicit--implicit scene representation. Our
method outperforms previous methods in reconstruction quality when trained in a
continual setting, while having the additional benefit of being an order of
magnitude faster.

Comments:
- For project page please visit https://ryanpo.com/icngp/

---

## SparseSat-NeRF: Dense Depth Supervised Neural Radiance Fields for Sparse  Satellite Images

nerf{: .label .label-blue }

2023-09-01 | Lulin Zhang, Ewelina Rupnik | cs.CV | [PDF](http://arxiv.org/pdf/2309.00277v1){: .btn .btn-green }

**Abstract**: Digital surface model generation using traditional multi-view stereo matching
(MVS) performs poorly over non-Lambertian surfaces, with asynchronous
acquisitions, or at discontinuities. Neural radiance fields (NeRF) offer a new
paradigm for reconstructing surface geometries using continuous volumetric
representation. NeRF is self-supervised, does not require ground truth geometry
for training, and provides an elegant way to include in its representation
physical parameters about the scene, thus potentially remedying the challenging
scenarios where MVS fails. However, NeRF and its variants require many views to
produce convincing scene's geometries which in earth observation satellite
imaging is rare. In this paper we present SparseSat-NeRF (SpS-NeRF) - an
extension of Sat-NeRF adapted to sparse satellite views. SpS-NeRF employs dense
depth supervision guided by crosscorrelation similarity metric provided by
traditional semi-global MVS matching. We demonstrate the effectiveness of our
approach on stereo and tri-stereo Pleiades 1B/WorldView-3 images, and compare
against NeRF and Sat-NeRF. The code is available at
https://github.com/LulinZhang/SpS-NeRF

Comments:
- ISPRS Annals 2023

---

## GHuNeRF: Generalizable Human NeRF from a Monocular Video

nerf{: .label .label-blue }

2023-08-31 | Chen Li, Jiahao Lin, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2308.16576v3){: .btn .btn-green }

**Abstract**: In this paper, we tackle the challenging task of learning a generalizable
human NeRF model from a monocular video. Although existing generalizable human
NeRFs have achieved impressive results, they require muti-view images or videos
which might not be always available. On the other hand, some works on
free-viewpoint rendering of human from monocular videos cannot be generalized
to unseen identities. In view of these limitations, we propose GHuNeRF to learn
a generalizable human NeRF model from a monocular video of the human performer.
We first introduce a visibility-aware aggregation scheme to compute vertex-wise
features, which is used to construct a 3D feature volume. The feature volume
can only represent the overall geometry of the human performer with
insufficient accuracy due to the limited resolution. To solve this, we further
enhance the volume feature with temporally aligned point-wise features using an
attention mechanism. Finally, the enhanced feature is used for predicting
density and color for each sampled point. A surface-guided sampling strategy is
also adopted to improve the efficiency for both training and inference. We
validate our approach on the widely-used ZJU-MoCap dataset, where we achieve
comparable performance with existing multi-view video based approaches. We also
test on the monocular People-Snapshot dataset and achieve better performance
than existing works when only monocular video is used. Our code is available at
the project website.

Comments:
- Add in more baseline for comparison

---

## From Pixels to Portraits: A Comprehensive Survey of Talking Head  Generation Techniques and Applications

nerf{: .label .label-blue }

2023-08-30 | Shreyank N Gowda, Dheeraj Pandey, Shashank Narayana Gowda | cs.CV | [PDF](http://arxiv.org/pdf/2308.16041v1){: .btn .btn-green }

**Abstract**: Recent advancements in deep learning and computer vision have led to a surge
of interest in generating realistic talking heads. This paper presents a
comprehensive survey of state-of-the-art methods for talking head generation.
We systematically categorises them into four main approaches: image-driven,
audio-driven, video-driven and others (including neural radiance fields (NeRF),
and 3D-based methods). We provide an in-depth analysis of each method,
highlighting their unique contributions, strengths, and limitations.
Furthermore, we thoroughly compare publicly available models, evaluating them
on key aspects such as inference time and human-rated quality of the generated
outputs. Our aim is to provide a clear and concise overview of the current
landscape in talking head generation, elucidating the relationships between
different approaches and identifying promising directions for future research.
This survey will serve as a valuable reference for researchers and
practitioners interested in this rapidly evolving field.

---

## Drone-NeRF: Efficient NeRF Based 3D Scene Reconstruction for Large-Scale  Drone Survey

nerf{: .label .label-blue }

2023-08-30 | Zhihao Jia, Bing Wang, Changhao Chen | cs.CV | [PDF](http://arxiv.org/pdf/2308.15733v1){: .btn .btn-green }

**Abstract**: Neural rendering has garnered substantial attention owing to its capacity for
creating realistic 3D scenes. However, its applicability to extensive scenes
remains challenging, with limitations in effectiveness. In this work, we
propose the Drone-NeRF framework to enhance the efficient reconstruction of
unbounded large-scale scenes suited for drone oblique photography using Neural
Radiance Fields (NeRF). Our approach involves dividing the scene into uniform
sub-blocks based on camera position and depth visibility. Sub-scenes are
trained in parallel using NeRF, then merged for a complete scene. We refine the
model by optimizing camera poses and guiding NeRF with a uniform sampler.
Integrating chosen samples enhances accuracy. A hash-coded fusion MLP
accelerates density representation, yielding RGB and Depth outputs. Our
framework accounts for sub-scene constraints, reduces parallel-training noise,
handles shadow occlusion, and merges sub-regions for a polished rendering
result. This Drone-NeRF framework demonstrates promising capabilities in
addressing challenges related to scene complexity, rendering efficiency, and
accuracy in drone-obtained imagery.

Comments:
- 15 pages, 7 figures, in submission

---

## Efficient Ray Sampling for Radiance Fields Reconstruction

nerf{: .label .label-blue }

2023-08-29 | Shilei Sun, Ming Liu, Zhongyi Fan, Yuxue Liu, Chengwei Lv, Liquan Dong, Lingqin Kong | cs.CV | [PDF](http://arxiv.org/pdf/2308.15547v1){: .btn .btn-green }

**Abstract**: Accelerating neural radiance fields training is of substantial practical
value, as the ray sampling strategy profoundly impacts network convergence.
More efficient ray sampling can thus directly enhance existing NeRF models'
training efficiency. We therefore propose a novel ray sampling approach for
neural radiance fields that improves training efficiency while retaining
photorealistic rendering results. First, we analyze the relationship between
the pixel loss distribution of sampled rays and rendering quality. This reveals
redundancy in the original NeRF's uniform ray sampling. Guided by this finding,
we develop a sampling method leveraging pixel regions and depth boundaries. Our
main idea is to sample fewer rays in training views, yet with each ray more
informative for scene fitting. Sampling probability increases in pixel areas
exhibiting significant color and depth variation, greatly reducing wasteful
rays from other regions without sacrificing precision. Through this method, not
only can the convergence of the network be accelerated, but the spatial
geometry of a scene can also be perceived more accurately. Rendering outputs
are enhanced, especially for texture-complex regions. Experiments demonstrate
that our method significantly outperforms state-of-the-art techniques on public
benchmark datasets.

Comments:
- 15 pages

---

## Pose-Free Neural Radiance Fields via Implicit Pose Regularization

nerf{: .label .label-blue }

2023-08-29 | Jiahui Zhang, Fangneng Zhan, Yingchen Yu, Kunhao Liu, Rongliang Wu, Xiaoqin Zhang, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2308.15049v1){: .btn .btn-green }

**Abstract**: Pose-free neural radiance fields (NeRF) aim to train NeRF with unposed
multi-view images and it has achieved very impressive success in recent years.
Most existing works share the pipeline of training a coarse pose estimator with
rendered images at first, followed by a joint optimization of estimated poses
and neural radiance field. However, as the pose estimator is trained with only
rendered images, the pose estimation is usually biased or inaccurate for real
images due to the domain gap between real images and rendered images, leading
to poor robustness for the pose estimation of real images and further local
minima in joint optimization. We design IR-NeRF, an innovative pose-free NeRF
that introduces implicit pose regularization to refine pose estimator with
unposed real images and improve the robustness of the pose estimation for real
images. With a collection of 2D images of a specific scene, IR-NeRF constructs
a scene codebook that stores scene features and captures the scene-specific
pose distribution implicitly as priors. Thus, the robustness of pose estimation
can be promoted with the scene priors according to the rationale that a 2D real
image can be well reconstructed from the scene codebook only when its estimated
pose lies within the pose distribution. Extensive experiments show that IR-NeRF
achieves superior novel view synthesis and outperforms the state-of-the-art
consistently across multiple synthetic and real datasets.

Comments:
- Accepted by ICCV2023

---

## Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a  Light-Weight ToF Sensor



2023-08-28 | Xinyang Liu, Yijin Li, Yanbin Teng, Hujun Bao, Guofeng Zhang, Yinda Zhang, Zhaopeng Cui | cs.CV | [PDF](http://arxiv.org/pdf/2308.14383v1){: .btn .btn-green }

**Abstract**: Light-weight time-of-flight (ToF) depth sensors are compact and
cost-efficient, and thus widely used on mobile devices for tasks such as
autofocus and obstacle detection. However, due to the sparse and noisy depth
measurements, these sensors have rarely been considered for dense geometry
reconstruction. In this work, we present the first dense SLAM system with a
monocular camera and a light-weight ToF sensor. Specifically, we propose a
multi-modal implicit scene representation that supports rendering both the
signals from the RGB camera and light-weight ToF sensor which drives the
optimization by comparing with the raw sensor inputs. Moreover, in order to
guarantee successful pose tracking and reconstruction, we exploit a predicted
depth as an intermediate supervision and develop a coarse-to-fine optimization
strategy for efficient learning of the implicit representation. At last, the
temporal information is explicitly exploited to deal with the noisy signals
from light-weight ToF sensors to improve the accuracy and robustness of the
system. Experiments demonstrate that our system well exploits the signals of
light-weight ToF sensors and achieves competitive results both on camera
tracking and dense scene reconstruction. Project page:
\url{https://zju3dv.github.io/tof_slam/}.

Comments:
- Accepted to ICCV 2023 (Oral). Project Page:
  https://zju3dv.github.io/tof_slam/

---

## Flexible Techniques for Differentiable Rendering with 3D Gaussians



2023-08-28 | Leonid Keselman, Martial Hebert | cs.CV | [PDF](http://arxiv.org/pdf/2308.14737v1){: .btn .btn-green }

**Abstract**: Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus

---

## CLNeRF: Continual Learning Meets NeRF

nerf{: .label .label-blue }

2023-08-28 | Zhipeng Cai, Matthias Mueller | cs.CV | [PDF](http://arxiv.org/pdf/2308.14816v1){: .btn .btn-green }

**Abstract**: Novel view synthesis aims to render unseen views given a set of calibrated
images. In practical applications, the coverage, appearance or geometry of the
scene may change over time, with new images continuously being captured.
Efficiently incorporating such continuous change is an open challenge. Standard
NeRF benchmarks only involve scene coverage expansion. To study other practical
scene changes, we propose a new dataset, World Across Time (WAT), consisting of
scenes that change in appearance and geometry over time. We also propose a
simple yet effective method, CLNeRF, which introduces continual learning (CL)
to Neural Radiance Fields (NeRFs). CLNeRF combines generative replay and the
Instant Neural Graphics Primitives (NGP) architecture to effectively prevent
catastrophic forgetting and efficiently update the model when new data arrives.
We also add trainable appearance and geometry embeddings to NGP, allowing a
single compact model to handle complex scene changes. Without the need to store
historical images, CLNeRF trained sequentially over multiple scans of a
changing scene performs on-par with the upper bound model trained on all scans
at once. Compared to other CL baselines CLNeRF performs much better across
standard benchmarks and WAT. The source code, and the WAT dataset are available
at https://github.com/IntelLabs/CLNeRF. Video presentation is available at:
https://youtu.be/nLRt6OoDGq0?si=8yD6k-8MMBJInQPs

Comments:
- Accepted to ICCV 2023

---

## Sparse3D: Distilling Multiview-Consistent Diffusion for Object  Reconstruction from Sparse Views



2023-08-27 | Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang, Ying Shan, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2308.14078v2){: .btn .btn-green }

**Abstract**: Reconstructing 3D objects from extremely sparse views is a long-standing and
challenging problem. While recent techniques employ image diffusion models for
generating plausible images at novel viewpoints or for distilling pre-trained
diffusion priors into 3D representations using score distillation sampling
(SDS), these methods often struggle to simultaneously achieve high-quality,
consistent, and detailed results for both novel-view synthesis (NVS) and
geometry. In this work, we present Sparse3D, a novel 3D reconstruction method
tailored for sparse view inputs. Our approach distills robust priors from a
multiview-consistent diffusion model to refine a neural radiance field.
Specifically, we employ a controller that harnesses epipolar features from
input views, guiding a pre-trained diffusion model, such as Stable Diffusion,
to produce novel-view images that maintain 3D consistency with the input. By
tapping into 2D priors from powerful image diffusion models, our integrated
model consistently delivers high-quality results, even when faced with
open-world objects. To address the blurriness introduced by conventional SDS,
we introduce the category-score distillation sampling (C-SDS) to enhance
detail. We conduct experiments on CO3DV2 which is a multi-view dataset of
real-world objects. Both quantitative and qualitative evaluations demonstrate
that our approach outperforms previous state-of-the-art works on the metrics
regarding NVS and geometry reconstruction.

---

## Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code  Diffusion using Transformers



2023-08-27 | Abril Corona-Figueroa, Sam Bond-Taylor, Neelanjan Bhowmik, Yona Falinie A. Gaus, Toby P. Breckon, Hubert P. H. Shum, Chris G. Willcocks | cs.CV | [PDF](http://arxiv.org/pdf/2308.14152v1){: .btn .btn-green }

**Abstract**: Generating 3D images of complex objects conditionally from a few 2D views is
a difficult synthesis problem, compounded by issues such as domain gap and
geometric misalignment. For instance, a unified framework such as Generative
Adversarial Networks cannot achieve this unless they explicitly define both a
domain-invariant and geometric-invariant joint latent distribution, whereas
Neural Radiance Fields are generally unable to handle both issues as they
optimize at the pixel level. By contrast, we propose a simple and novel 2D to
3D synthesis approach based on conditional diffusion with vector-quantized
codes. Operating in an information-rich code space enables high-resolution 3D
synthesis via full-coverage attention across the views. Specifically, we
generate the 3D codes (e.g. for CT images) conditional on previously generated
3D codes and the entire codebook of two 2D views (e.g. 2D X-rays). Qualitative
and quantitative results demonstrate state-of-the-art performance over
specialized methods across varied evaluation criteria, including fidelity
metrics such as density, coverage, and distortion metrics for two complex
volumetric imagery datasets from in real-world scenarios.

Comments:
- Camera-ready version for ICCV 2023

---

## InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules

nerf{: .label .label-blue }

2023-08-26 | Yanqi Bao, Tianyu Ding, Jing Huo, Wenbin Li, Yuxin Li, Yang Gao | cs.CV | [PDF](http://arxiv.org/pdf/2308.13897v1){: .btn .btn-green }

**Abstract**: Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant
challenge that existing approaches struggle to address without extensive
modifications to vanilla NeRF framework. We introduce InsertNeRF, a method for
INStilling gEneRalizabiliTy into NeRF. By utilizing multiple plug-and-play
HyperNet modules, InsertNeRF dynamically tailors NeRF's weights to specific
reference scenes, transforming multi-scale sampling-aware features into
scene-specific representations. This novel design allows for more accurate and
efficient representations of complex appearances and geometries. Experiments
show that this method not only achieves superior generalization performance but
also provides a flexible pathway for integration with other NeRF-like systems,
even in sparse input settings. Code will be available
https://github.com/bbbbby-99/InsertNeRF.

---

## Relighting Neural Radiance Fields with Shadow and Highlight Hints



2023-08-25 | Chong Zeng, Guojun Chen, Yue Dong, Pieter Peers, Hongzhi Wu, Xin Tong | cs.CV | [PDF](http://arxiv.org/pdf/2308.13404v1){: .btn .btn-green }

**Abstract**: This paper presents a novel neural implicit radiance representation for free
viewpoint relighting from a small set of unstructured photographs of an object
lit by a moving point light source different from the view position. We express
the shape as a signed distance function modeled by a multi layer perceptron. In
contrast to prior relightable implicit neural representations, we do not
disentangle the different reflectance components, but model both the local and
global reflectance at each point by a second multi layer perceptron that, in
addition, to density features, the current position, the normal (from the
signed distace function), view direction, and light position, also takes shadow
and highlight hints to aid the network in modeling the corresponding high
frequency light transport effects. These hints are provided as a suggestion,
and we leave it up to the network to decide how to incorporate these in the
final relit result. We demonstrate and validate our neural implicit
representation on synthetic and real scenes exhibiting a wide variety of
shapes, material properties, and global illumination light transport.

Comments:
- Accepted to SIGGRAPH 2023. Author's version. Project page:
  https://nrhints.github.io/

---

## Improving NeRF Quality by Progressive Camera Placement for Unrestricted  Navigation in Complex Environments

nerf{: .label .label-blue }

2023-08-24 | Georgios Kopanas, George Drettakis | cs.CV | [PDF](http://arxiv.org/pdf/2309.00014v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields, or NeRFs, have drastically improved novel view
synthesis and 3D reconstruction for rendering. NeRFs achieve impressive results
on object-centric reconstructions, but the quality of novel view synthesis with
free-viewpoint navigation in complex environments (rooms, houses, etc) is often
problematic. While algorithmic improvements play an important role in the
resulting quality of novel view synthesis, in this work, we show that because
optimizing a NeRF is inherently a data-driven process, good quality data play a
fundamental role in the final quality of the reconstruction. As a consequence,
it is critical to choose the data samples -- in this case the cameras -- in a
way that will eventually allow the optimization to converge to a solution that
allows free-viewpoint navigation with good quality. Our main contribution is an
algorithm that efficiently proposes new camera placements that improve visual
quality with minimal assumptions. Our solution can be used with any NeRF model
and outperforms baselines and similar work.

---

## NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects

nerf{: .label .label-blue }

2023-08-24 | Dakshit Agrawal, Jiajie Xu, Siva Karthik Mustikovela, Ioannis Gkioulekas, Ashish Shrivastava, Yuning Chai | cs.CV | [PDF](http://arxiv.org/pdf/2308.12560v1){: .btn .btn-green }

**Abstract**: We propose a novel-view augmentation (NOVA) strategy to train NeRFs for
photo-realistic 3D composition of dynamic objects in a static scene. Compared
to prior work, our framework significantly reduces blending artifacts when
inserting multiple dynamic objects into a 3D scene at novel views and times;
achieves comparable PSNR without the need for additional ground truth
modalities like optical flow; and overall provides ease, flexibility, and
scalability in neural composition. Our codebase is on GitHub.

Comments:
- Accepted for publication in ICCV Computer Vision for Metaverse
  Workshop 2023 (code is available at https://github.com/dakshitagrawal/NoVA)

---

## ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for  3D Scene Stylization



2023-08-23 | Wenzhao Li, Tianhao Wu, Fangcheng Zhong, Cengiz Oztireli | cs.CV | [PDF](http://arxiv.org/pdf/2308.12452v2){: .btn .btn-green }

**Abstract**: The radiance fields style transfer is an emerging field that has recently
gained popularity as a means of 3D scene stylization, thanks to the outstanding
performance of neural radiance fields in 3D reconstruction and view synthesis.
We highlight a research gap in radiance fields style transfer, the lack of
sufficient perceptual controllability, motivated by the existing concept in the
2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style
transfer framework offering manageable control over perceptual factors, to
systematically explore the perceptual controllability in 3D scene stylization.
Four distinct types of controls - color preservation control, (style pattern)
scale control, spatial (selective stylization area) control, and depth
enhancement control - are proposed and integrated into this framework. Results
from real-world datasets, both quantitative and qualitative, show that the four
types of controls in our ARF-Plus framework successfully accomplish their
corresponding perceptual controls when stylizing 3D scenes. These techniques
work well for individual style inputs as well as for the simultaneous
application of multiple styles within a scene. This unlocks a realm of
limitless possibilities, allowing customized modifications of stylization
effects and flexible merging of the strengths of different styles, ultimately
enabling the creation of novel and eye-catching stylistic effects on 3D scenes.

---

## Pose Modulated Avatars from Video

nerf{: .label .label-blue }

2023-08-23 | Chunjin Song, Bastian Wandt, Helge Rhodin | cs.CV | [PDF](http://arxiv.org/pdf/2308.11951v3){: .btn .btn-green }

**Abstract**: It is now possible to reconstruct dynamic human motion and shape from a
sparse set of cameras using Neural Radiance Fields (NeRF) driven by an
underlying skeleton. However, a challenge remains to model the deformation of
cloth and skin in relation to skeleton pose. Unlike existing avatar models that
are learned implicitly or rely on a proxy surface, our approach is motivated by
the observation that different poses necessitate unique frequency assignments.
Neglecting this distinction yields noisy artifacts in smooth areas or blurs
fine-grained texture and shape details in sharp regions. We develop a
two-branch neural network that is adaptive and explicit in the frequency
domain. The first branch is a graph neural network that models correlations
among body parts locally, taking skeleton pose as input. The second branch
combines these correlation features to a set of global frequencies and then
modulates the feature encoding. Our experiments demonstrate that our network
outperforms state-of-the-art methods in terms of preserving details and
generalization capabilities.

---

## Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields

nerf{: .label .label-blue }

2023-08-23 | Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, Taehyeong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2308.11974v2){: .btn .btn-green }

**Abstract**: Text-driven localized editing of 3D objects is particularly difficult as
locally mixing the original 3D object with the intended new object and style
effects without distorting the object's form is not a straightforward process.
To address this issue, we propose a novel NeRF-based model, Blending-NeRF,
which consists of two NeRF networks: pretrained NeRF and editable NeRF.
Additionally, we introduce new blending operations that allow Blending-NeRF to
properly edit target regions which are localized by text. By using a pretrained
vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects
with varying colors and densities, modify textures, and remove parts of the
original object. Our extensive experiments demonstrate that Blending-NeRF
produces naturally and locally edited 3D objects from various text prompts. Our
project page is available at https://seokhunchoi.github.io/Blending-NeRF/

Comments:
- Accepted to ICCV 2023. The first two authors contributed equally to
  this work

---

## Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer  with Mixture-of-View-Experts

nerf{: .label .label-blue }

2023-08-22 | Wenyan Cong, Hanxue Liang, Peihao Wang, Zhiwen Fan, Tianlong Chen, Mukund Varma, Yi Wang, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2308.11793v1){: .btn .btn-green }

**Abstract**: Cross-scene generalizable NeRF models, which can directly synthesize novel
views of unseen scenes, have become a new spotlight of the NeRF field. Several
existing attempts rely on increasingly end-to-end "neuralized" architectures,
i.e., replacing scene representation and/or rendering modules with performant
neural networks such as transformers, and turning novel view synthesis into a
feed-forward inference pipeline. While those feedforward "neuralized"
architectures still do not fit diverse scenes well out of the box, we propose
to bridge them with the powerful Mixture-of-Experts (MoE) idea from large
language models (LLMs), which has demonstrated superior generalization ability
by balancing between larger overall model capacity and flexible per-instance
specialization. Starting from a recent generalizable NeRF architecture called
GNT, we first demonstrate that MoE can be neatly plugged in to enhance the
model. We further customize a shared permanent expert and a geometry-aware
consistency loss to enforce cross-scene consistency and spatial smoothness
respectively, which are essential for generalizable view synthesis. Our
proposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has
experimentally shown state-of-the-art results when transferring to unseen
scenes, indicating remarkably better cross-scene generalization in both
zero-shot and few-shot settings. Our codes are available at
https://github.com/VITA-Group/GNT-MOVE.

Comments:
- Accepted by ICCV2023

---

## SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene  Reconstruction by Neural Radiance Field (NeRF)

nerf{: .label .label-blue }

2023-08-22 | Ange Lou, Yamin Li, Xing Yao, Yike Zhang, Jack Noble | cs.CV | [PDF](http://arxiv.org/pdf/2308.11774v1){: .btn .btn-green }

**Abstract**: The accurate reconstruction of surgical scenes from surgical videos is
critical for various applications, including intraoperative navigation and
image-guided robotic surgery automation. However, previous approaches, mainly
relying on depth estimation, have limited effectiveness in reconstructing
surgical scenes with moving surgical tools. To address this limitation and
provide accurate 3D position prediction for surgical tools in all frames, we
propose a novel approach called SAMSNeRF that combines Segment Anything Model
(SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates
accurate segmentation masks of surgical tools using SAM, which guides the
refinement of the dynamic surgical scene reconstruction by NeRF. Our
experimental results on public endoscopy surgical videos demonstrate that our
approach successfully reconstructs high-fidelity dynamic surgical scenes and
accurately reflects the spatial information of surgical tools. Our proposed
approach can significantly enhance surgical navigation and automation by
providing surgeons with accurate 3D position information of surgical tools
during surgery.The source code will be released soon.

---

## Novel-view Synthesis and Pose Estimation for Hand-Object Interaction  from Sparse Views

nerf{: .label .label-blue }

2023-08-22 | Wentian Qu, Zhaopeng Cui, Yinda Zhang, Chenyu Meng, Cuixia Ma, Xiaoming Deng, Hongan Wang | cs.CV | [PDF](http://arxiv.org/pdf/2308.11198v1){: .btn .btn-green }

**Abstract**: Hand-object interaction understanding and the barely addressed novel view
synthesis are highly desired in the immersive communication, whereas it is
challenging due to the high deformation of hand and heavy occlusions between
hand and object. In this paper, we propose a neural rendering and pose
estimation system for hand-object interaction from sparse views, which can also
enable 3D hand-object interaction editing. We share the inspiration from recent
scene understanding work that shows a scene specific model built beforehand can
significantly improve and unblock vision tasks especially when inputs are
sparse, and extend it to the dynamic hand-object interaction scenario and
propose to solve the problem in two stages. We first learn the shape and
appearance prior knowledge of hands and objects separately with the neural
representation at the offline stage. During the online stage, we design a
rendering-based joint model fitting framework to understand the dynamic
hand-object interaction with the pre-built hand and object models as well as
interaction priors, which thereby overcomes penetration and separation issues
between hand and object and also enables novel view synthesis. In order to get
stable contact during the hand-object interaction process in a sequence, we
propose a stable contact loss to make the contact region to be consistent.
Experiments demonstrate that our method outperforms the state-of-the-art
methods. Code and dataset are available in project webpage
https://iscas3dv.github.io/HO-NeRF.

---

## Efficient View Synthesis with Neural Radiance Distribution Field

nerf{: .label .label-blue }

2023-08-22 | Yushuang Wu, Xiao Li, Jinglu Wang, Xiaoguang Han, Shuguang Cui, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2308.11130v1){: .btn .btn-green }

**Abstract**: Recent work on Neural Radiance Fields (NeRF) has demonstrated significant
advances in high-quality view synthesis. A major limitation of NeRF is its low
rendering efficiency due to the need for multiple network forwardings to render
a single pixel. Existing methods to improve NeRF either reduce the number of
required samples or optimize the implementation to accelerate the network
forwarding. Despite these efforts, the problem of multiple sampling persists
due to the intrinsic representation of radiance fields. In contrast, Neural
Light Fields (NeLF) reduce the computation cost of NeRF by querying only one
single network forwarding per pixel. To achieve a close visual quality to NeRF,
existing NeLF methods require significantly larger network capacities which
limits their rendering efficiency in practice. In this work, we propose a new
representation called Neural Radiance Distribution Field (NeRDF) that targets
efficient view synthesis in real-time. Specifically, we use a small network
similar to NeRF while preserving the rendering speed with a single network
forwarding per pixel as in NeLF. The key is to model the radiance distribution
along each ray with frequency basis and predict frequency weights using the
network. Pixel values are then computed via volume rendering on radiance
distributions. Experiments show that our proposed method offers a better
trade-off among speed, quality, and network size than existing methods: we
achieve a ~254x speed-up over NeRF with similar network size, with only a
marginal performance decline. Our project page is at
yushuang-wu.github.io/NeRDF.

Comments:
- Accepted by ICCV2023

---

## CamP: Camera Preconditioning for Neural Radiance Fields

nerf{: .label .label-blue }

2023-08-21 | Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T. Barron, Ricardo Martin-Brualla | cs.CV | [PDF](http://arxiv.org/pdf/2308.10902v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D
scene reconstructions of objects and large-scale scenes. However, NeRFs require
accurate camera parameters as input -- inaccurate camera parameters result in
blurry renderings. Extrinsic and intrinsic camera parameters are usually
estimated using Structure-from-Motion (SfM) methods as a pre-processing step to
NeRF, but these techniques rarely yield perfect estimates. Thus, prior works
have proposed jointly optimizing camera parameters alongside a NeRF, but these
methods are prone to local minima in challenging settings. In this work, we
analyze how different camera parameterizations affect this joint optimization
problem, and observe that standard parameterizations exhibit large differences
in magnitude with respect to small perturbations, which can lead to an
ill-conditioned optimization problem. We propose using a proxy problem to
compute a whitening transform that eliminates the correlation between camera
parameters and normalizes their effects, and we propose to use this transform
as a preconditioner for the camera parameters during joint optimization. Our
preconditioned camera optimization significantly improves reconstruction
quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE)
by 67% compared to state-of-the-art NeRF approaches that do not optimize for
cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint
optimization approaches using the camera parameterization of SCNeRF. Our
approach is easy to implement, does not significantly increase runtime, can be
applied to a wide variety of camera parameterizations, and can
straightforwardly be incorporated into other NeRF-like models.

Comments:
- SIGGRAPH Asia 2023, Project page: https://camp-nerf.github.io

---

## Strata-NeRF : Neural Radiance Fields for Stratified Scenes

nerf{: .label .label-blue }

2023-08-20 | Ankit Dhiman, Srinath R, Harsh Rangwani, Rishubh Parihar, Lokesh R Boregowda, Srinath Sridhar, R Venkatesh Babu | cs.CV | [PDF](http://arxiv.org/pdf/2308.10337v1){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) approaches learn the underlying 3D
representation of a scene and generate photo-realistic novel views with high
fidelity. However, most proposed settings concentrate on modelling a single
object or a single level of a scene. However, in the real world, we may capture
a scene at multiple levels, resulting in a layered capture. For example,
tourists usually capture a monument's exterior structure before capturing the
inner structure. Modelling such scenes in 3D with seamless switching between
levels can drastically improve immersive experiences. However, most existing
techniques struggle in modelling such scenes. We propose Strata-NeRF, a single
neural radiance field that implicitly captures a scene with multiple levels.
Strata-NeRF achieves this by conditioning the NeRFs on Vector Quantized (VQ)
latent representations which allow sudden changes in scene structure. We
evaluate the effectiveness of our approach in multi-layered synthetic dataset
comprising diverse scenes and then further validate its generalization on the
real-world RealEstate10K dataset. We find that Strata-NeRF effectively captures
stratified scenes, minimizes artifacts, and synthesizes high-fidelity views
compared to existing approaches.

Comments:
- ICCV 2023, Project Page: https://ankitatiisc.github.io/Strata-NeRF/

---

## AltNeRF: Learning Robust Neural Radiance Field via Alternating  Depth-Pose Optimization

nerf{: .label .label-blue }

2023-08-19 | Kun Wang, Zhiqiang Yan, Huang Tian, Zhenyu Zhang, Xiang Li, Jun Li, Jian Yang | cs.CV | [PDF](http://arxiv.org/pdf/2308.10001v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have shown promise in generating realistic
novel views from sparse scene images. However, existing NeRF approaches often
encounter challenges due to the lack of explicit 3D supervision and imprecise
camera poses, resulting in suboptimal outcomes. To tackle these issues, we
propose AltNeRF -- a novel framework designed to create resilient NeRF
representations using self-supervised monocular depth estimation (SMDE) from
monocular videos, without relying on known camera poses. SMDE in AltNeRF
masterfully learns depth and pose priors to regulate NeRF training. The depth
prior enriches NeRF's capacity for precise scene geometry depiction, while the
pose prior provides a robust starting point for subsequent pose refinement.
Moreover, we introduce an alternating algorithm that harmoniously melds NeRF
outputs into SMDE through a consistence-driven mechanism, thus enhancing the
integrity of depth priors. This alternation empowers AltNeRF to progressively
refine NeRF representations, yielding the synthesis of realistic novel views.
Additionally, we curate a distinctive dataset comprising indoor videos captured
via mobile devices. Extensive experiments showcase the compelling capabilities
of AltNeRF in generating high-fidelity and robust novel views that closely
resemble reality.

---

## Semantic-Human: Neural Rendering of Humans from Monocular Video with  Human Parsing

nerf{: .label .label-blue }

2023-08-19 | Jie Zhang, Pengcheng Shi, Zaiwang Gu, Yiyang Zhou, Zhi Wang | cs.CV | [PDF](http://arxiv.org/pdf/2308.09894v1){: .btn .btn-green }

**Abstract**: The neural rendering of humans is a topic of great research significance.
However, previous works mostly focus on achieving photorealistic details,
neglecting the exploration of human parsing. Additionally, classical semantic
work are all limited in their ability to efficiently represent fine results in
complex motions. Human parsing is inherently related to radiance
reconstruction, as similar appearance and geometry often correspond to similar
semantic part. Furthermore, previous works often design a motion field that
maps from the observation space to the canonical space, while it tends to
exhibit either underfitting or overfitting, resulting in limited
generalization. In this paper, we present Semantic-Human, a novel method that
achieves both photorealistic details and viewpoint-consistent human parsing for
the neural rendering of humans. Specifically, we extend neural radiance fields
(NeRF) to jointly encode semantics, appearance and geometry to achieve accurate
2D semantic labels using noisy pseudo-label supervision. Leveraging the
inherent consistency and smoothness properties of NeRF, Semantic-Human achieves
consistent human parsing in both continuous and novel views. We also introduce
constraints derived from the SMPL surface for the motion field and
regularization for the recovered volumetric geometry. We have evaluated the
model using the ZJU-MoCap dataset, and the obtained highly competitive results
demonstrate the effectiveness of our proposed Semantic-Human. We also showcase
various compelling applications, including label denoising, label synthesis and
image editing, and empirically validate its advantageous properties.

---

## HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision  Mitigation

nerf{: .label .label-blue }

2023-08-19 | Xiufeng Xie, Riccardo Gherardi, Zhihong Pan, Stephen Huang | cs.CV | [PDF](http://arxiv.org/pdf/2308.10122v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) have garnered significant attention, with
recent works such as Instant-NGP accelerating NeRF training and evaluation
through a combination of hashgrid-based positional encoding and neural
networks. However, effectively leveraging the spatial sparsity of 3D scenes
remains a challenge. To cull away unnecessary regions of the feature grid,
existing solutions rely on prior knowledge of object shape or periodically
estimate object shape during training by repeated model evaluations, which are
costly and wasteful.
  To address this issue, we propose HollowNeRF, a novel compression solution
for hashgrid-based NeRF which automatically sparsifies the feature grid during
the training phase. Instead of directly compressing dense features, HollowNeRF
trains a coarse 3D saliency mask that guides efficient feature pruning, and
employs an alternating direction method of multipliers (ADMM) pruner to
sparsify the 3D saliency mask during training. By exploiting the sparsity in
the 3D scene to redistribute hash collisions, HollowNeRF improves rendering
quality while using a fraction of the parameters of comparable state-of-the-art
solutions, leading to a better cost-accuracy trade-off. Our method delivers
comparable rendering quality to Instant-NGP, while utilizing just 31% of the
parameters. In addition, our solution can achieve a PSNR accuracy gain of up to
1dB using only 56% of the parameters.

Comments:
- Accepted to ICCV 2023

---

## DReg-NeRF: Deep Registration for Neural Radiance Fields

nerf{: .label .label-blue }

2023-08-18 | Yu Chen, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2308.09386v1){: .btn .btn-green }

**Abstract**: Although Neural Radiance Fields (NeRF) is popular in the computer vision
community recently, registering multiple NeRFs has yet to gain much attention.
Unlike the existing work, NeRF2NeRF, which is based on traditional optimization
methods and needs human annotated keypoints, we propose DReg-NeRF to solve the
NeRF registration problem on object-centric scenes without human intervention.
After training NeRF models, our DReg-NeRF first extracts features from the
occupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformer
architecture with self-attention and cross-attention layers to learn the
relations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA)
point cloud registration methods, the decoupled correspondences are supervised
by surface fields without any ground truth overlapping labels. We construct a
novel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse to
train our network. When evaluated on the test set, our proposed method beats
the SOTA point cloud registration methods by a large margin, with a mean
$\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$.
  Our code is available at https://github.com/AIBluefisher/DReg-NeRF.

Comments:
- Accepted at ICCV 2023

---

## MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection

nerf{: .label .label-blue }

2023-08-18 | Junkai Xu, Liang Peng, Haoran Cheng, Hao Li, Wei Qian, Ke Li, Wenxiao Wang, Deng Cai | cs.CV | [PDF](http://arxiv.org/pdf/2308.09421v2){: .btn .btn-green }

**Abstract**: In the field of monocular 3D detection, it is common practice to utilize
scene geometric clues to enhance the detector's performance. However, many
existing works adopt these clues explicitly such as estimating a depth map and
back-projecting it into 3D space. This explicit methodology induces sparsity in
3D representations due to the increased dimensionality from 2D to 3D, and leads
to substantial information loss, especially for distant and occluded objects.
To alleviate this issue, we propose MonoNeRD, a novel detection framework that
can infer dense 3D geometry and occupancy. Specifically, we model scenes with
Signed Distance Functions (SDF), facilitating the production of dense 3D
representations. We treat these representations as Neural Radiance Fields
(NeRF) and then employ volume rendering to recover RGB images and depth maps.
To the best of our knowledge, this work is the first to introduce volume
rendering for M3D, and demonstrates the potential of implicit reconstruction
for image-based 3D perception. Extensive experiments conducted on the KITTI-3D
benchmark and Waymo Open Dataset demonstrate the effectiveness of MonoNeRD.
Codes are available at https://github.com/cskkxjk/MonoNeRD.

Comments:
- Accepted by ICCV 2023

---

## Watch Your Steps: Local Image and Scene Editing by Text Instructions

nerf{: .label .label-blue }

2023-08-17 | Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2308.08947v1){: .btn .btn-green }

**Abstract**: Denoising diffusion models have enabled high-quality image generation and
editing. We present a method to localize the desired edit region implicit in a
text instruction. We leverage InstructPix2Pix (IP2P) and identify the
discrepancy between IP2P predictions with and without the instruction. This
discrepancy is referred to as the relevance map. The relevance map conveys the
importance of changing each pixel to achieve the edits, and is used to to guide
the modifications. This guidance ensures that the irrelevant pixels remain
unchanged. Relevance maps are further used to enhance the quality of
text-guided editing of 3D scenes in the form of neural radiance fields. A field
is trained on relevance maps of training views, denoted as the relevance field,
defining the 3D region within which modifications should be made. We perform
iterative updates on the training views guided by rendered relevance maps from
the relevance field. Our method achieves state-of-the-art performance on both
image and NeRF editing tasks. Project page:
https://ashmrz.github.io/WatchYourSteps/

Comments:
- Project page: https://ashmrz.github.io/WatchYourSteps/

---

## Language-enhanced RNR-Map: Querying Renderable Neural Radiance Field  maps with natural language



2023-08-17 | Francesco Taioli, Federico Cunico, Federico Girella, Riccardo Bologna, Alessandro Farinelli, Marco Cristani | cs.CV | [PDF](http://arxiv.org/pdf/2308.08854v1){: .btn .btn-green }

**Abstract**: We present Le-RNR-Map, a Language-enhanced Renderable Neural Radiance map for
Visual Navigation with natural language query prompts. The recently proposed
RNR-Map employs a grid structure comprising latent codes positioned at each
pixel. These latent codes, which are derived from image observation, enable: i)
image rendering given a camera pose, since they are converted to Neural
Radiance Field; ii) image navigation and localization with astonishing
accuracy. On top of this, we enhance RNR-Map with CLIP-based embedding latent
codes, allowing natural language search without additional label data. We
evaluate the effectiveness of this map in single and multi-object searches. We
also investigate its compatibility with a Large Language Model as an
"affordance query resolver". Code and videos are available at
https://intelligolabs.github.io/Le-RNR-Map/

Comments:
- Accepted at ICCVW23 VLAR

---

## Ref-DVGO: Reflection-Aware Direct Voxel Grid Optimization for an  Improved Quality-Efficiency Trade-Off in Reflective Scene Reconstruction

nerf{: .label .label-blue }

2023-08-16 | Georgios Kouros, Minye Wu, Shubham Shrivastava, Sushruth Nagesh, Punarjay Chakravarty, Tinne Tuytelaars | cs.CV | [PDF](http://arxiv.org/pdf/2308.08530v3){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have revolutionized the field of novel view
synthesis, demonstrating remarkable performance. However, the modeling and
rendering of reflective objects remain challenging problems. Recent methods
have shown significant improvements over the baselines in handling reflective
scenes, albeit at the expense of efficiency. In this work, we aim to strike a
balance between efficiency and quality. To this end, we investigate an
implicit-explicit approach based on conventional volume rendering to enhance
the reconstruction quality and accelerate the training and rendering processes.
We adopt an efficient density-based grid representation and reparameterize the
reflected radiance in our pipeline. Our proposed reflection-aware approach
achieves a competitive quality efficiency trade-off compared to competing
methods. Based on our experimental results, we propose and discuss hypotheses
regarding the factors influencing the results of density-based methods for
reconstructing reflective objects. The source code is available at
https://github.com/gkouros/ref-dvgo.

Comments:
- 5 pages, 4 figures, 3 tables, ICCV TRICKY 2023 Workshop

---

## SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes

nerf{: .label .label-blue }

2023-08-16 | Edith Tretschk, Vladislav Golyanik, Michael Zollhoefer, Aljaz Bozic, Christoph Lassner, Christian Theobalt | cs.CV | [PDF](http://arxiv.org/pdf/2308.08258v1){: .btn .btn-green }

**Abstract**: Existing methods for the 4D reconstruction of general, non-rigidly deforming
objects focus on novel-view synthesis and neglect correspondences. However,
time consistency enables advanced downstream tasks like 3D editing, motion
analysis, or virtual-asset creation. We propose SceNeRFlow to reconstruct a
general, non-rigid scene in a time-consistent manner. Our dynamic-NeRF method
takes multi-view RGB videos and background images from static cameras with
known camera parameters as input. It then reconstructs the deformations of an
estimated canonical model of the geometry and appearance in an online fashion.
Since this canonical model is time-invariant, we obtain correspondences even
for long-term, long-range motions. We employ neural scene representations to
parametrize the components of our method. Like prior dynamic-NeRF methods, we
use a backwards deformation model. We find non-trivial adaptations of this
model necessary to handle larger motions: We decompose the deformations into a
strongly regularized coarse component and a weakly regularized fine component,
where the coarse component also extends the deformation field into the space
surrounding the object, which enables tracking over time. We show
experimentally that, unlike prior work that only handles small motion, our
method enables the reconstruction of studio-scale motions.

Comments:
- Project page: https://vcai.mpi-inf.mpg.de/projects/scenerflow/

---

## Neural radiance fields in the industrial and robotics domain:  applications, research opportunities and use cases

nerf{: .label .label-blue }

2023-08-14 | Eugen Šlapak, Enric Pardo, Matúš Dopiriak, Taras Maksymyuk, Juraj Gazda | cs.RO | [PDF](http://arxiv.org/pdf/2308.07118v2){: .btn .btn-green }

**Abstract**: The proliferation of technologies, such as extended reality (XR), has
increased the demand for high-quality three-dimensional (3D) graphical
representations. Industrial 3D applications encompass computer-aided design
(CAD), finite element analysis (FEA), scanning, and robotics. However, current
methods employed for industrial 3D representations suffer from high
implementation costs and reliance on manual human input for accurate 3D
modeling. To address these challenges, neural radiance fields (NeRFs) have
emerged as a promising approach for learning 3D scene representations based on
provided training 2D images. Despite a growing interest in NeRFs, their
potential applications in various industrial subdomains are still unexplored.
In this paper, we deliver a comprehensive examination of NeRF industrial
applications while also providing direction for future research endeavors. We
also present a series of proof-of-concept experiments that demonstrate the
potential of NeRFs in the industrial domain. These experiments include
NeRF-based video compression techniques and using NeRFs for 3D motion
estimation in the context of collision avoidance. In the video compression
experiment, our results show compression savings up to 48\% and 74\% for
resolutions of 1920x1080 and 300x168, respectively. The motion estimation
experiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF)
and achieved an average peak signal-to-noise ratio (PSNR) of disparity map with
the value of 23 dB and an structural similarity index measure (SSIM) 0.97.

---

## S3IM: Stochastic Structural SIMilarity and Its Unreasonable  Effectiveness for Neural Fields

nerf{: .label .label-blue }

2023-08-14 | Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang, Haoran Wang, Yunfeng Cai, Mingming Sun | cs.CV | [PDF](http://arxiv.org/pdf/2308.07032v1){: .btn .btn-green }

**Abstract**: Recently, Neural Radiance Field (NeRF) has shown great success in rendering
novel-view images of a given scene by learning an implicit representation with
only posed RGB images. NeRF and relevant neural field methods (e.g., neural
surface representation) typically optimize a point-wise loss and make
point-wise predictions, where one data point corresponds to one pixel.
Unfortunately, this line of research failed to use the collective supervision
of distant pixels, although it is known that pixels in an image or scene can
provide rich structural information. To the best of our knowledge, we are the
first to design a nonlocal multiplex training paradigm for NeRF and relevant
neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss
that processes multiple data points as a whole set instead of process multiple
inputs independently. Our extensive experiments demonstrate the unreasonable
effectiveness of S3IM in improving NeRF and neural surface representation for
nearly free. The improvements of quality metrics can be particularly
significant for those relatively difficult tasks: e.g., the test MSE loss
unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view
synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance
reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is
consistently robust even with sparse inputs, corrupted images, and dynamic
scenes.

Comments:
- ICCV 2023 main conference. Code: https://github.com/Madaoer/S3IM. 14
  pages, 5 figures, 17 tables

---

## VERF: Runtime Monitoring of Pose Estimation with Neural Radiance Fields

nerf{: .label .label-blue }

2023-08-11 | Dominic Maggio, Courtney Mario, Luca Carlone | cs.RO | [PDF](http://arxiv.org/pdf/2308.05939v1){: .btn .btn-green }

**Abstract**: We present VERF, a collection of two methods (VERF-PnP and VERF-Light) for
providing runtime assurance on the correctness of a camera pose estimate of a
monocular camera without relying on direct depth measurements. We leverage the
ability of NeRF (Neural Radiance Fields) to render novel RGB perspectives of a
scene. We only require as input the camera image whose pose is being estimated,
an estimate of the camera pose we want to monitor, and a NeRF model containing
the scene pictured by the camera. We can then predict if the pose estimate is
within a desired distance from the ground truth and justify our prediction with
a level of confidence. VERF-Light does this by rendering a viewpoint with NeRF
at the estimated pose and estimating its relative offset to the sensor image up
to scale. Since scene scale is unknown, the approach renders another auxiliary
image and reasons over the consistency of the optical flows across the three
images. VERF-PnP takes a different approach by rendering a stereo pair of
images with NeRF and utilizing the Perspective-n-Point (PnP) algorithm. We
evaluate both methods on the LLFF dataset, on data from a Unitree A1 quadruped
robot, and on data collected from Blue Origin's sub-orbital New Shepard rocket
to demonstrate the effectiveness of the proposed pose monitoring method across
a range of scene scales. We also show monitoring can be completed in under half
a second on a 3090 GPU.

---

## Focused Specific Objects NeRF

nerf{: .label .label-blue }

2023-08-11 | Yuesong Li, Feng Pan, Helong Yan, Xiuli Xin, Xiaoxue Feng | cs.CV | [PDF](http://arxiv.org/pdf/2308.05970v1){: .btn .btn-green }

**Abstract**: Most NeRF-based models are designed for learning the entire scene, and
complex scenes can lead to longer learning times and poorer rendering effects.
This paper utilizes scene semantic priors to make improvements in fast
training, allowing the network to focus on the specific targets and not be
affected by complex backgrounds. The training speed can be increased by 7.78
times with better rendering effect, and small to medium sized targets can be
rendered faster. In addition, this improvement applies to all NeRF-based
models. Considering the inherent multi-view consistency and smoothness of NeRF,
this paper also studies weak supervision by sparsely sampling negative ray
samples. With this method, training can be further accelerated and rendering
quality can be maintained. Finally, this paper extends pixel semantic and color
rendering formulas and proposes a new scene editing technique that can achieve
unique displays of the specific semantic targets or masking them in rendering.
To address the problem of unsupervised regions incorrect inferences in the
scene, we also designed a self-supervised loop that combines morphological
operations and clustering.

Comments:
- 17 pages,32 figures

---

## WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields

nerf{: .label .label-blue }

2023-08-09 | Muyu Xu, Fangneng Zhan, Jiahui Zhang, Yingchen Yu, Xiaoqin Zhang, Christian Theobalt, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2308.04826v2){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) has shown impressive performance in novel view
synthesis via implicit scene representation. However, it usually suffers from
poor scalability as requiring densely sampled images for each new scene.
Several studies have attempted to mitigate this problem by integrating
Multi-View Stereo (MVS) technique into NeRF while they still entail a
cumbersome fine-tuning process for new scenes. Notably, the rendering quality
will drop severely without this fine-tuning process and the errors mainly
appear around the high-frequency features. In the light of this observation, we
design WaveNeRF, which integrates wavelet frequency decomposition into MVS and
NeRF to achieve generalizable yet high-quality synthesis without any per-scene
optimization. To preserve high-frequency information when generating 3D feature
volumes, WaveNeRF builds Multi-View Stereo in the Wavelet domain by integrating
the discrete wavelet transform into the classical cascade MVS, which
disentangles high-frequency information explicitly. With that, disentangled
frequency features can be injected into classic NeRF via a novel hybrid neural
renderer to yield faithful high-frequency details, and an intuitive
frequency-guided sampling strategy can be designed to suppress artifacts around
high-frequency regions. Extensive experiments over three widely studied
benchmarks show that WaveNeRF achieves superior generalizable radiance field
modeling when only given three images as input.

Comments:
- Accepted to ICCV 2023. Project website:
  https://mxuai.github.io/WaveNeRF/

---

## A General Implicit Framework for Fast NeRF Composition and Rendering

nerf{: .label .label-blue }

2023-08-09 | Xinyu Gao, Ziyi Yang, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin, Changqing Zou | cs.CV | [PDF](http://arxiv.org/pdf/2308.04669v4){: .btn .btn-green }

**Abstract**: A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.

Comments:
- AAAI 2024

---

## Digging into Depth Priors for Outdoor Neural Radiance Fields

nerf{: .label .label-blue }

2023-08-08 | Chen Wang, Jiadai Sun, Lina Liu, Chenming Wu, Zhelun Shen, Dayan Wu, Yuchao Dai, Liangjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2308.04413v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated impressive performance in
vision and graphics tasks, such as novel view synthesis and immersive reality.
However, the shape-radiance ambiguity of radiance fields remains a challenge,
especially in the sparse viewpoints setting. Recent work resorts to integrating
depth priors into outdoor NeRF training to alleviate the issue. However, the
criteria for selecting depth priors and the relative merits of different priors
have not been thoroughly investigated. Moreover, the relative merits of
selecting different approaches to use the depth priors is also an unexplored
problem. In this paper, we provide a comprehensive study and evaluation of
employing depth priors to outdoor neural radiance fields, covering common depth
sensing technologies and most application ways. Specifically, we conduct
extensive experiments with two representative NeRF methods equipped with four
commonly-used depth priors and different depth usages on two widely used
outdoor datasets. Our experimental results reveal several interesting findings
that can potentially benefit practitioners and researchers in training their
NeRF models with depth priors. Project Page:
https://cwchenwang.github.io/outdoor-nerf-depth

Comments:
- Accepted to ACM MM 2023. Project Page:
  https://cwchenwang.github.io/outdoor-nerf-depth

---

## 3D Gaussian Splatting for Real-Time Radiance Field Rendering

gaussian splatting{: .label .label-blue }

2023-08-08 | Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis | cs.GR | [PDF](http://arxiv.org/pdf/2308.04079v1){: .btn .btn-green }

**Abstract**: Radiance Field methods have recently revolutionized novel-view synthesis of
scenes captured with multiple photos or videos. However, achieving high visual
quality still requires neural networks that are costly to train and render,
while recent faster methods inevitably trade off speed for quality. For
unbounded and complete scenes (rather than isolated objects) and 1080p
resolution rendering, no current method can achieve real-time display rates. We
introduce three key elements that allow us to achieve state-of-the-art visual
quality while maintaining competitive training times and importantly allow
high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution.
First, starting from sparse points produced during camera calibration, we
represent the scene with 3D Gaussians that preserve desirable properties of
continuous volumetric radiance fields for scene optimization while avoiding
unnecessary computation in empty space; Second, we perform interleaved
optimization/density control of the 3D Gaussians, notably optimizing
anisotropic covariance to achieve an accurate representation of the scene;
Third, we develop a fast visibility-aware rendering algorithm that supports
anisotropic splatting and both accelerates training and allows realtime
rendering. We demonstrate state-of-the-art visual quality and real-time
rendering on several established datasets.

Comments:
- https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/

---

## Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with  Whitted-Style Ray Tracing

nerf{: .label .label-blue }

2023-08-07 | Junyi Zeng, Chong Bao, Rui Chen, Zilong Dong, Guofeng Zhang, Hujun Bao, Zhaopeng Cui | cs.CV | [PDF](http://arxiv.org/pdf/2308.03280v1){: .btn .btn-green }

**Abstract**: Recently, Neural Radiance Fields (NeRF) has exhibited significant success in
novel view synthesis, surface reconstruction, etc. However, since no physical
reflection is considered in its rendering pipeline, NeRF mistakes the
reflection in the mirror as a separate virtual scene, leading to the inaccurate
reconstruction of the mirror and multi-view inconsistent reflections in the
mirror. In this paper, we present a novel neural rendering framework, named
Mirror-NeRF, which is able to learn accurate geometry and reflection of the
mirror and support various scene manipulation applications with mirrors, such
as adding new objects or mirrors into the scene and synthesizing the
reflections of these new objects in mirrors, controlling mirror roughness, etc.
To achieve this goal, we propose a unified radiance field by introducing the
reflection probability and tracing rays following the light transport model of
Whitted Ray Tracing, and also develop several techniques to facilitate the
learning process. Experiments and comparisons on both synthetic and real
datasets demonstrate the superiority of our method. The code and supplementary
material are available on the project webpage:
https://zju3dv.github.io/Mirror-NeRF/.

Comments:
- Accepted to ACM Multimedia 2023. Project Page:
  https://zju3dv.github.io/Mirror-NeRF/

---

## Where and How: Mitigating Confusion in Neural Radiance Fields from  Sparse Inputs

nerf{: .label .label-blue }

2023-08-05 | Yanqi Bao, Yuxin Li, Jing Huo, Tianyu Ding, Xinyue Liang, Wenbin Li, Yang Gao | cs.CV | [PDF](http://arxiv.org/pdf/2308.02908v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields from Sparse input} (NeRF-S) have shown great potential
in synthesizing novel views with a limited number of observed viewpoints.
However, due to the inherent limitations of sparse inputs and the gap between
non-adjacent views, rendering results often suffer from over-fitting and foggy
surfaces, a phenomenon we refer to as "CONFUSION" during volume rendering. In
this paper, we analyze the root cause of this confusion and attribute it to two
fundamental questions: "WHERE" and "HOW". To this end, we present a novel
learning framework, WaH-NeRF, which effectively mitigates confusion by tackling
the following challenges: (i)"WHERE" to Sample? in NeRF-S -- we introduce a
Deformable Sampling strategy and a Weight-based Mutual Information Loss to
address sample-position confusion arising from the limited number of
viewpoints; and (ii) "HOW" to Predict? in NeRF-S -- we propose a
Semi-Supervised NeRF learning Paradigm based on pose perturbation and a
Pixel-Patch Correspondence Loss to alleviate prediction confusion caused by the
disparity between training and testing viewpoints. By integrating our proposed
modules and loss functions, WaH-NeRF outperforms previous methods under the
NeRF-S setting. Code is available https://github.com/bbbbby-99/WaH-NeRF.

Comments:
- Accepted In Proceedings of the 31st ACM International Conference on
  Multimedia (MM' 23)

---

## Learning Unified Decompositional and Compositional NeRF for Editable  Novel View Synthesis

nerf{: .label .label-blue }

2023-08-05 | Yuxin Wang, Wayne Wu, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2308.02840v1){: .btn .btn-green }

**Abstract**: Implicit neural representations have shown powerful capacity in modeling
real-world 3D scenes, offering superior performance in novel view synthesis. In
this paper, we target a more challenging scenario, i.e., joint scene novel view
synthesis and editing based on implicit neural scene representations.
State-of-the-art methods in this direction typically consider building separate
networks for these two tasks (i.e., view synthesis and editing). Thus, the
modeling of interactions and correlations between these two tasks is very
limited, which, however, is critical for learning high-quality scene
representations. To tackle this problem, in this paper, we propose a unified
Neural Radiance Field (NeRF) framework to effectively perform joint scene
decomposition and composition for modeling real-world scenes. The decomposition
aims at learning disentangled 3D representations of different objects and the
background, allowing for scene editing, while scene composition models an
entire scene representation for novel view synthesis. Specifically, with a
two-stage NeRF framework, we learn a coarse stage for predicting a global
radiance field as guidance for point sampling, and in the second fine-grained
stage, we perform scene decomposition by a novel one-hot object radiance field
regularization module and a pseudo supervision via inpainting to handle
ambiguous background regions occluded by objects. The decomposed object-level
radiance fields are further composed by using activations from the
decomposition module. Extensive quantitative and qualitative results show the
effectiveness of our method for scene decomposition and composition,
outperforming state-of-the-art methods for both novel-view synthesis and
editing tasks.

Comments:
- ICCV2023, Project Page: https://w-ted.github.io/publications/udc-nerf

---

## NeRFs: The Search for the Best 3D Representation

nerf{: .label .label-blue }

2023-08-05 | Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2308.02751v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields or NeRFs have become the representation of choice for
problems in view synthesis or image-based rendering, as well as in many other
applications across computer graphics and vision, and beyond. At their core,
NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of
meshes, disparity maps, multiplane images or even voxel grids, they represent
the scene as a continuous volume, with volumetric parameters like
view-dependent radiance and volume density obtained by querying a neural
network. The NeRF representation has now been widely used, with thousands of
papers extending or building on it every year, multiple authors and websites
providing overviews and surveys, and numerous industrial applications and
startup companies. In this article, we briefly review the NeRF representation,
and describe the three decades-long quest to find the best 3D representation
for view synthesis and related problems, culminating in the NeRF papers. We
then describe new developments in terms of NeRF representations and make some
observations and insights regarding the future of 3D representations.

Comments:
- Updated based on feedback in-person and via e-mail at SIGGRAPH 2023.
  In particular, I have added references and discussion of seminal SIGGRAPH
  image-based rendering papers, and better put the recent Kerbl et al. work in
  context, with more references

---

## ES-MVSNet: Efficient Framework for End-to-end Self-supervised Multi-View  Stereo

nerf{: .label .label-blue }

2023-08-04 | Qiang Zhou, Chaohui Yu, Jingliang Li, Yuang Liu, Jing Wang, Zhibin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2308.02191v1){: .btn .btn-green }

**Abstract**: Compared to the multi-stage self-supervised multi-view stereo (MVS) method,
the end-to-end (E2E) approach has received more attention due to its concise
and efficient training pipeline. Recent E2E self-supervised MVS approaches have
integrated third-party models (such as optical flow models, semantic
segmentation models, NeRF models, etc.) to provide additional consistency
constraints, which grows GPU memory consumption and complicates the model's
structure and training pipeline. In this work, we propose an efficient
framework for end-to-end self-supervised MVS, dubbed ES-MVSNet. To alleviate
the high memory consumption of current E2E self-supervised MVS frameworks, we
present a memory-efficient architecture that reduces memory usage by 43%
without compromising model performance. Furthermore, with the novel design of
asymmetric view selection policy and region-aware depth consistency, we achieve
state-of-the-art performance among E2E self-supervised MVS methods, without
relying on third-party models for additional consistency signals. Extensive
experiments on DTU and Tanks&Temples benchmarks demonstrate that the proposed
ES-MVSNet approach achieves state-of-the-art performance among E2E
self-supervised MVS methods and competitive performance to many supervised and
multi-stage self-supervised methods.

Comments:
- arXiv admin note: text overlap with arXiv:2203.03949 by other authors

---

## Incorporating Season and Solar Specificity into Renderings made by a  NeRF Architecture using Satellite Images

nerf{: .label .label-blue }

2023-08-02 | Michael Gableman, Avinash Kak | cs.CV | [PDF](http://arxiv.org/pdf/2308.01262v2){: .btn .btn-green }

**Abstract**: As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar
angle into account in a NeRF-based framework for rendering a scene from a novel
viewpoint using satellite images for training. Our work extends those
contributions and shows how one can make the renderings season-specific. Our
main challenge was creating a Neural Radiance Field (NeRF) that could render
seasonal features independently of viewing angle and solar angle while still
being able to render shadows. We teach our network to render seasonal features
by introducing one more input variable -- time of the year. However, the small
training datasets typical of satellite imagery can introduce ambiguities in
cases where shadows are present in the same location for every image of a
particular season. We add additional terms to the loss function to discourage
the network from using seasonal features for accounting for shadows. We show
the performance of our network on eight Areas of Interest containing images
captured by the Maxar WorldView-3 satellite. This evaluation includes tests
measuring the ability of our framework to accurately render novel views,
generate height maps, predict shadows, and specify seasonal features
independently from shadows. Our ablation studies justify the choices made for
network design parameters.

Comments:
- 18 pages, 17 figures, 10 tables

---

## Context-Aware Talking-Head Video Editing

nerf{: .label .label-blue }

2023-08-01 | Songlin Yang, Wei Wang, Jun Ling, Bo Peng, Xu Tan, Jing Dong | cs.MM | [PDF](http://arxiv.org/pdf/2308.00462v3){: .btn .btn-green }

**Abstract**: Talking-head video editing aims to efficiently insert, delete, and substitute
the word of a pre-recorded video through a text transcript editor. The key
challenge for this task is obtaining an editing model that generates new
talking-head video clips which simultaneously have accurate lip synchronization
and motion smoothness. Previous approaches, including 3DMM-based (3D Morphable
Model) methods and NeRF-based (Neural Radiance Field) methods, are sub-optimal
in that they either require minutes of source videos and days of training time
or lack the disentangled control of verbal (e.g., lip motion) and non-verbal
(e.g., head pose and expression) representations for video clip insertion. In
this work, we fully utilize the video context to design a novel framework for
talking-head video editing, which achieves efficiency, disentangled motion
control, and sequential smoothness. Specifically, we decompose this framework
to motion prediction and motion-conditioned rendering: (1) We first design an
animation prediction module that efficiently obtains smooth and lip-sync motion
sequences conditioned on the driven speech. This module adopts a
non-autoregressive network to obtain context prior and improve the prediction
efficiency, and it learns a speech-animation mapping prior with better
generalization to novel speech from a multi-identity video dataset. (2) We then
introduce a neural rendering module to synthesize the photo-realistic and
full-head video frames given the predicted motion sequence. This module adopts
a pre-trained head topology and uses only few frames for efficient fine-tuning
to obtain a person-specific rendering model. Extensive experiments demonstrate
that our method efficiently achieves smoother editing results with higher image
quality and lip accuracy using less data than previous methods.

Comments:
- The version of this paper needs to be further improved

---

## Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned  Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF)

nerf{: .label .label-blue }

2023-08-01 | Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N. Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi, Leon Cho, Sameer A. Ansari, Donald R. Cantrell | cs.CV | [PDF](http://arxiv.org/pdf/2308.00214v2){: .btn .btn-green }

**Abstract**: Many tasks performed in image-guided, mini-invasive, medical procedures can
be cast as pose estimation problems, where an X-ray projection is utilized to
reach a target in 3D space. Expanding on recent advances in the differentiable
rendering of optically reflective materials, we introduce new methods for pose
estimation of radiolucent objects using X-ray projections, and we demonstrate
the critical role of optimal view synthesis in performing this task. We first
develop an algorithm (DiffDRR) that efficiently computes Digitally
Reconstructed Radiographs (DRRs) and leverages automatic differentiation within
TensorFlow. Pose estimation is performed by iterative gradient descent using a
loss function that quantifies the similarity of the DRR synthesized from a
randomly initialized pose and the true fluoroscopic image at the target pose.
We propose two novel methods for high-fidelity view synthesis, Neural Tuned
Tomography (NeTT) and masked Neural Radiance Fields (mNeRF). Both methods rely
on classic Cone-Beam Computerized Tomography (CBCT); NeTT directly optimizes
the CBCT densities, while the non-zero values of mNeRF are constrained by a 3D
mask of the anatomic region segmented from CBCT. We demonstrate that both NeTT
and mNeRF distinctly improve pose estimation within our framework. By defining
a successful pose estimate to be a 3D angle error of less than 3 deg, we find
that NeTT and mNeRF can achieve similar results, both with overall success
rates more than 93%. However, the computational cost of NeTT is significantly
lower than mNeRF in both training and pose estimation. Furthermore, we show
that a NeTT trained for a single subject can generalize to synthesize
high-fidelity DRRs and ensure robust pose estimations for all other subjects.
Therefore, we suggest that NeTT is an attractive option for robust pose
estimation using fluoroscopic projections.

---

## High-Fidelity Eye Animatable Neural Radiance Fields for Human Face

nerf{: .label .label-blue }

2023-08-01 | Hengfei Wang, Zhongqun Zhang, Yihua Cheng, Hyung Jin Chang | cs.CV | [PDF](http://arxiv.org/pdf/2308.00773v3){: .btn .btn-green }

**Abstract**: Face rendering using neural radiance fields (NeRF) is a rapidly developing
research area in computer vision. While recent methods primarily focus on
controlling facial attributes such as identity and expression, they often
overlook the crucial aspect of modeling eyeball rotation, which holds
importance for various downstream tasks. In this paper, we aim to learn a face
NeRF model that is sensitive to eye movements from multi-view images. We
address two key challenges in eye-aware face NeRF learning: how to effectively
capture eyeball rotation for training and how to construct a manifold for
representing eyeball rotation. To accomplish this, we first fit FLAME, a
well-established parametric face model, to the multi-view images considering
multi-view consistency. Subsequently, we introduce a new Dynamic Eye-aware NeRF
(DeNeRF). DeNeRF transforms 3D points from different views into a canonical
space to learn a unified face NeRF model. We design an eye deformation field
for the transformation, including rigid transformation, e.g., eyeball rotation,
and non-rigid transformation. Through experiments conducted on the ETH-XGaze
dataset, we demonstrate that our model is capable of generating high-fidelity
images with accurate eyeball rotation and non-rigid periocular deformation,
even under novel viewing angles. Furthermore, we show that utilizing the
rendered images can effectively enhance gaze estimation performance.

Comments:
- BMVC2023 Oral

---

## Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF

nerf{: .label .label-blue }

2023-07-28 | Haotian Bai, Yiqi Lin, Yize Chen, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2307.15333v1){: .btn .btn-green }

**Abstract**: The explicit neural radiance field (NeRF) has gained considerable interest
for its efficient training and fast inference capabilities, making it a
promising direction such as virtual reality and gaming. In particular,
PlenOctree (POT)[1], an explicit hierarchical multi-scale octree
representation, has emerged as a structural and influential framework. However,
POT's fixed structure for direct optimization is sub-optimal as the scene
complexity evolves continuously with updates to cached color and density,
necessitating refining the sampling distribution to capture signal complexity
accordingly. To address this issue, we propose the dynamic PlenOctree DOT,
which adaptively refines the sample distribution to adjust to changing scene
complexity. Specifically, DOT proposes a concise yet novel hierarchical feature
fusion strategy during the iterative rendering process. Firstly, it identifies
the regions of interest through training signals to ensure adaptive and
efficient refinement. Next, rather than directly filtering out valueless nodes,
DOT introduces the sampling and pruning operations for octrees to aggregate
features, enabling rapid parameter learning. Compared with POT, our DOT
outperforms it by enhancing visual quality, reducing over $55.15$/$68.84\%$
parameters, and providing 1.7/1.9 times FPS for NeRF-synthetic and Tanks $\&$
Temples, respectively. Project homepage:https://vlislab22.github.io/DOT.
  [1] Yu, Alex, et al. "Plenoctrees for real-time rendering of neural radiance
fields." Proceedings of the IEEE/CVF International Conference on Computer
Vision. 2021.

Comments:
- Accepted by ICCV2023

---

## Improved Neural Radiance Fields Using Pseudo-depth and Fusion

nerf{: .label .label-blue }

2023-07-27 | Jingliang Li, Qiang Zhou, Chaohui Yu, Zhengda Lu, Jun Xiao, Zhibin Wang, Fan Wang | cs.CV | [PDF](http://arxiv.org/pdf/2308.03772v1){: .btn .btn-green }

**Abstract**: Since the advent of Neural Radiance Fields, novel view synthesis has received
tremendous attention. The existing approach for the generalization of radiance
field reconstruction primarily constructs an encoding volume from nearby source
images as additional inputs. However, these approaches cannot efficiently
encode the geometric information of real scenes with various scale
objects/structures. In this work, we propose constructing multi-scale encoding
volumes and providing multi-scale geometry information to NeRF models. To make
the constructed volumes as close as possible to the surfaces of objects in the
scene and the rendered depth more accurate, we propose to perform depth
prediction and radiance field reconstruction simultaneously. The predicted
depth map will be used to supervise the rendered depth, narrow the depth range,
and guide points sampling. Finally, the geometric information contained in
point volume features may be inaccurate due to occlusion, lighting, etc. To
this end, we propose enhancing the point volume feature from depth-guided
neighbor feature fusion. Experiments demonstrate the superior performance of
our method in both novel view synthesis and dense geometry modeling without
per-scene optimization.

---

## MapNeRF: Incorporating Map Priors into Neural Radiance Fields for  Driving View Simulation

nerf{: .label .label-blue }

2023-07-27 | Chenming Wu, Jiadai Sun, Zhelun Shen, Liangjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2307.14981v2){: .btn .btn-green }

**Abstract**: Simulating camera sensors is a crucial task in autonomous driving. Although
neural radiance fields are exceptional at synthesizing photorealistic views in
driving simulations, they still fail to generate extrapolated views. This paper
proposes to incorporate map priors into neural radiance fields to synthesize
out-of-trajectory driving views with semantic road consistency. The key insight
is that map information can be utilized as a prior to guiding the training of
the radiance fields with uncertainty. Specifically, we utilize the coarse
ground surface as uncertain information to supervise the density field and warp
depth with uncertainty from unknown camera poses to ensure multi-view
consistency. Experimental results demonstrate that our approach can produce
semantic consistency in deviated views for vehicle camera simulation. The
supplementary video can be viewed at https://youtu.be/jEQWr-Rfh3A.

Comments:
- Accepted by IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2023

---

## NeRF-Det: Learning Geometry-Aware Volumetric Representation for  Multi-View 3D Object Detection

nerf{: .label .label-blue }

2023-07-27 | Chenfeng Xu, Bichen Wu, Ji Hou, Sam Tsai, Ruilong Li, Jialiang Wang, Wei Zhan, Zijian He, Peter Vajda, Kurt Keutzer, Masayoshi Tomizuka | cs.CV | [PDF](http://arxiv.org/pdf/2307.14620v1){: .btn .btn-green }

**Abstract**: We present NeRF-Det, a novel method for indoor 3D detection with posed RGB
images as input. Unlike existing indoor 3D detection methods that struggle to
model scene geometry, our method makes novel use of NeRF in an end-to-end
manner to explicitly estimate 3D geometry, thereby improving 3D detection
performance. Specifically, to avoid the significant extra latency associated
with per-scene optimization of NeRF, we introduce sufficient geometry priors to
enhance the generalizability of NeRF-MLP. Furthermore, we subtly connect the
detection and NeRF branches through a shared MLP, enabling an efficient
adaptation of NeRF to detection and yielding geometry-aware volumetric
representations for 3D detection. Our method outperforms state-of-the-arts by
3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. We
provide extensive analysis to shed light on how NeRF-Det works. As a result of
our joint-training design, NeRF-Det is able to generalize well to unseen scenes
for object detection, view synthesis, and depth estimation tasks without
requiring per-scene optimization. Code is available at
\url{https://github.com/facebookresearch/NeRF-Det}.

Comments:
- Accepted by ICCV 2023

---

## MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous  Driving

nerf{: .label .label-blue }

2023-07-27 | Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang Shi, Yiyi Liao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2307.15058v1){: .btn .btn-green }

**Abstract**: Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is
widely recognized that realistic sensor simulation will play a critical role in
solving remaining corner cases by simulating them. To this end, we propose an
autonomous driving simulator based upon neural radiance fields (NeRFs).
Compared with existing works, ours has three notable features: (1)
Instance-aware. Our simulator models the foreground instances and background
environments separately with independent networks so that the static (e.g.,
size and appearance) and dynamic (e.g., trajectory) properties of instances can
be controlled separately. (2) Modular. Our simulator allows flexible switching
between different modern NeRF-related backbones, sampling strategies, input
modalities, etc. We expect this modular design to boost academic progress and
industrial deployment of NeRF-based autonomous driving simulation. (3)
Realistic. Our simulator set new state-of-the-art photo-realism results given
the best module selection. Our simulator will be open-sourced while most of our
counterparts are not. Project page: https://open-air-sun.github.io/mars/.

Comments:
- CICAI 2023, project page with code:
  https://open-air-sun.github.io/mars/

---

## Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields

nerf{: .label .label-blue }

2023-07-27 | Xiangyu Wang, Jingsen Zhu, Qi Ye, Yuchi Huo, Yunlong Ran, Zhihua Zhong, Jiming Chen | cs.CV | [PDF](http://arxiv.org/pdf/2307.15131v2){: .btn .btn-green }

**Abstract**: With the popularity of implicit neural representations, or neural radiance
fields (NeRF), there is a pressing need for editing methods to interact with
the implicit 3D models for tasks like post-processing reconstructed scenes and
3D content creation. While previous works have explored NeRF editing from
various perspectives, they are restricted in editing flexibility, quality, and
speed, failing to offer direct editing response and instant preview. The key
challenge is to conceive a locally editable neural representation that can
directly reflect the editing instructions and update instantly. To bridge the
gap, we propose a new interactive editing method and system for implicit
representations, called Seal-3D, which allows users to edit NeRF models in a
pixel-level and free manner with a wide range of NeRF-like backbone and preview
the editing effects instantly. To achieve the effects, the challenges are
addressed by our proposed proxy function mapping the editing instructions to
the original space of NeRF models in the teacher model and a two-stage training
strategy for the student model with local pretraining and global finetuning. A
NeRF editing system is built to showcase various editing types. Our system can
achieve compelling editing effects with an interactive speed of about 1 second.

Comments:
- Accepted by ICCV2023. Project Page:
  https://windingwind.github.io/seal-3d/ Code:
  https://github.com/windingwind/seal-3d/

---

## Points-to-3D: Bridging the Gap between Sparse Points and  Shape-Controllable Text-to-3D Generation

nerf{: .label .label-blue }

2023-07-26 | Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin Wang, Fan Wang | cs.CV | [PDF](http://arxiv.org/pdf/2307.13908v1){: .btn .btn-green }

**Abstract**: Text-to-3D generation has recently garnered significant attention, fueled by
2D diffusion models trained on billions of image-text pairs. Existing methods
primarily rely on score distillation to leverage the 2D diffusion priors to
supervise the generation of 3D models, e.g., NeRF. However, score distillation
is prone to suffer the view inconsistency problem, and implicit NeRF modeling
can also lead to an arbitrary shape, thus leading to less realistic and
uncontrollable 3D generation. In this work, we propose a flexible framework of
Points-to-3D to bridge the gap between sparse yet freely available 3D points
and realistic shape-controllable 3D generation by distilling the knowledge from
both 2D and 3D diffusion models. The core idea of Points-to-3D is to introduce
controllable sparse 3D points to guide the text-to-3D generation. Specifically,
we use the sparse point cloud generated from the 3D diffusion model, Point-E,
as the geometric prior, conditioned on a single reference image. To better
utilize the sparse 3D points, we propose an efficient point cloud guidance loss
to adaptively drive the NeRF's geometry to align with the shape of the sparse
3D points. In addition to controlling the geometry, we propose to optimize the
NeRF for a more view-consistent appearance. To be specific, we perform score
distillation to the publicly available 2D image diffusion model ControlNet,
conditioned on text as well as depth map of the learned compact geometry.
Qualitative and quantitative comparisons demonstrate that Points-to-3D improves
view consistency and achieves good shape controllability for text-to-3D
generation. Points-to-3D provides users with a new way to improve and control
text-to-3D generation.

Comments:
- Accepted by ACMMM 2023

---

## Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields

nerf{: .label .label-blue }

2023-07-24 | Shangzhan Zhang, Sida Peng, Yinji ShenTu, Qing Shuai, Tianrun Chen, Kaicheng Yu, Hujun Bao, Xiaowei Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2307.12909v1){: .btn .btn-green }

**Abstract**: Recently, the editing of neural radiance fields (NeRFs) has gained
considerable attention, but most prior works focus on static scenes while
research on the appearance editing of dynamic scenes is relatively lacking. In
this paper, we propose a novel framework to edit the local appearance of
dynamic NeRFs by manipulating pixels in a single frame of training video.
Specifically, to locally edit the appearance of dynamic NeRFs while preserving
unedited regions, we introduce a local surface representation of the edited
region, which can be inserted into and rendered along with the original NeRF
and warped to arbitrary other frames through a learned invertible motion
representation network. By employing our method, users without professional
expertise can easily add desired content to the appearance of a dynamic scene.
We extensively evaluate our approach on various scenes and show that our
approach achieves spatially and temporally consistent editing results. Notably,
our approach is versatile and applicable to different variants of dynamic NeRF
representations.

Comments:
- project page: https://dyn-e.github.io/

---

## CarPatch: A Synthetic Benchmark for Radiance Field Evaluation on Vehicle  Components

nerf{: .label .label-blue }

2023-07-24 | Davide Di Nucci, Alessandro Simoni, Matteo Tomei, Luca Ciuffreda, Roberto Vezzani, Rita Cucchiara | cs.CV | [PDF](http://arxiv.org/pdf/2307.12718v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have gained widespread recognition as a highly
effective technique for representing 3D reconstructions of objects and scenes
derived from sets of images. Despite their efficiency, NeRF models can pose
challenges in certain scenarios such as vehicle inspection, where the lack of
sufficient data or the presence of challenging elements (e.g. reflections)
strongly impact the accuracy of the reconstruction. To this aim, we introduce
CarPatch, a novel synthetic benchmark of vehicles. In addition to a set of
images annotated with their intrinsic and extrinsic camera parameters, the
corresponding depth maps and semantic segmentation masks have been generated
for each view. Global and part-based metrics have been defined and used to
evaluate, compare, and better characterize some state-of-the-art techniques.
The dataset is publicly released at
https://aimagelab.ing.unimore.it/go/carpatch and can be used as an evaluation
guide and as a baseline for future work on this challenging topic.

Comments:
- Accepted at ICIAP2023

---

## TransHuman: A Transformer-based Human Representation for Generalizable  Neural Human Rendering

nerf{: .label .label-blue }

2023-07-23 | Xiao Pan, Zongxin Yang, Jianxin Ma, Chang Zhou, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2307.12291v1){: .btn .btn-green }

**Abstract**: In this paper, we focus on the task of generalizable neural human rendering
which trains conditional Neural Radiance Fields (NeRF) from multi-view videos
of different characters. To handle the dynamic human motion, previous methods
have primarily used a SparseConvNet (SPC)-based human representation to process
the painted SMPL. However, such SPC-based representation i) optimizes under the
volatile observation space which leads to the pose-misalignment between
training and inference stages, and ii) lacks the global relationships among
human parts that is critical for handling the incomplete painted SMPL. Tackling
these issues, we present a brand-new framework named TransHuman, which learns
the painted SMPL under the canonical space and captures the global
relationships between human parts with transformers. Specifically, TransHuman
is mainly composed of Transformer-based Human Encoding (TransHE), Deformable
Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI).
TransHE first processes the painted SMPL under the canonical space via
transformers for capturing the global relationships between human parts. Then,
DPaRF binds each output token with a deformable radiance field for encoding the
query point under the observation space. Finally, the FDI is employed to
further integrate fine-grained information from reference images. Extensive
experiments on ZJU-MoCap and H36M show that our TransHuman achieves a
significantly new state-of-the-art performance with high efficiency. Project
page: https://pansanity666.github.io/TransHuman/

Comments:
- Accepted by ICCV 2023

---

## CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields

nerf{: .label .label-blue }

2023-07-21 | Ziyuan Luo, Qing Guo, Ka Chun Cheung, Simon See, Renjie Wan | cs.CV | [PDF](http://arxiv.org/pdf/2307.11526v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have the potential to be a major representation
of media. Since training a NeRF has never been an easy task, the protection of
its model copyright should be a priority. In this paper, by analyzing the pros
and cons of possible copyright protection solutions, we propose to protect the
copyright of NeRF models by replacing the original color representation in NeRF
with a watermarked color representation. Then, a distortion-resistant rendering
scheme is designed to guarantee robust message extraction in 2D renderings of
NeRF. Our proposed method can directly protect the copyright of NeRF models
while maintaining high rendering quality and bit accuracy when compared among
optional solutions.

Comments:
- 11 pages, 6 figures, accepted by ICCV 2023 non-camera-ready version

---

## FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural  Radiance Fields

nerf{: .label .label-blue }

2023-07-21 | Sungwon Hwang, Junha Hyung, Daejin Kim, Min-Jung Kim, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2307.11418v3){: .btn .btn-green }

**Abstract**: As recent advances in Neural Radiance Fields (NeRF) have enabled
high-fidelity 3D face reconstruction and novel view synthesis, its manipulation
also became an essential task in 3D vision. However, existing manipulation
methods require extensive human labor, such as a user-provided semantic mask
and manual attribute search unsuitable for non-expert users. Instead, our
approach is designed to require a single text to manipulate a face
reconstructed with NeRF. To do so, we first train a scene manipulator, a latent
code-conditional deformable NeRF, over a dynamic scene to control a face
deformation using the latent code. However, representing a scene deformation
with a single latent code is unfavorable for compositing local deformations
observed in different instances. As so, our proposed Position-conditional
Anchor Compositor (PAC) learns to represent a manipulated scene with spatially
varying latent codes. Their renderings with the scene manipulator are then
optimized to yield high cosine similarity to a target text in CLIP embedding
space for text-driven manipulation. To the best of our knowledge, our approach
is the first to address the text-driven manipulation of a face reconstructed
with NeRF. Extensive results, comparisons, and ablation studies demonstrate the
effectiveness of our approach.

Comments:
- ICCV 2023 project page at https://faceclipnerf.github.io

---

## Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural  Radiance Fields

nerf{: .label .label-blue }

2023-07-21 | Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao, Xiao Liu, Yuewen Ma | cs.CV | [PDF](http://arxiv.org/pdf/2307.11335v1){: .btn .btn-green }

**Abstract**: Despite the tremendous progress in neural radiance fields (NeRF), we still
face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF
presents fine-detailed and anti-aliased renderings but takes days for training,
while Instant-ngp can accomplish the reconstruction in a few minutes but
suffers from blurring or aliasing when rendering at various distances or
resolutions due to ignoring the sampling area. To this end, we propose a novel
Tri-Mip encoding that enables both instant reconstruction and anti-aliased
high-fidelity rendering for neural radiance fields. The key is to factorize the
pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can
efficiently perform 3D area sampling by taking advantage of 2D pre-filtered
feature maps, which significantly elevates the rendering quality without
sacrificing efficiency. To cope with the novel Tri-Mip representation, we
propose a cone-casting rendering technique to efficiently sample anti-aliased
3D features with the Tri-Mip encoding considering both pixel imaging and
observing distance. Extensive experiments on both synthetic and real-world
datasets demonstrate our method achieves state-of-the-art rendering quality and
reconstruction speed while maintaining a compact representation that reduces
25% model size compared against Instant-ngp.

Comments:
- Accepted to ICCV 2023 Project page:
  https://wbhu.github.io/projects/Tri-MipRF

---

## Urban Radiance Field Representation with Deformable Neural Mesh  Primitives

nerf{: .label .label-blue }

2023-07-20 | Fan Lu, Yan Xu, Guang Chen, Hongsheng Li, Kwan-Yee Lin, Changjun Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2307.10776v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have achieved great success in the past few
years. However, most current methods still require intensive resources due to
ray marching-based rendering. To construct urban-level radiance fields
efficiently, we design Deformable Neural Mesh Primitive~(DNMP), and propose to
parameterize the entire scene with such primitives. The DNMP is a flexible and
compact neural variant of classic mesh representation, which enjoys both the
efficiency of rasterization-based rendering and the powerful neural
representation capability for photo-realistic image synthesis. Specifically, a
DNMP consists of a set of connected deformable mesh vertices with paired vertex
features to parameterize the geometry and radiance information of a local area.
To constrain the degree of freedom for optimization and lower the storage
budgets, we enforce the shape of each primitive to be decoded from a relatively
low-dimensional latent space. The rendering colors are decoded from the vertex
features (interpolated with rasterization) by a view-dependent MLP. The DNMP
provides a new paradigm for urban-level scene representation with appealing
properties: $(1)$ High-quality rendering. Our method achieves leading
performance for novel view synthesis in urban scenarios. $(2)$ Low
computational costs. Our representation enables fast rendering (2.07ms/1k
pixels) and low peak memory usage (110MB/1k pixels). We also present a
lightweight version that can run 33$\times$ faster than vanilla NeRFs, and
comparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).
Project page: \href{https://dnmp.github.io/}{https://dnmp.github.io/}.

Comments:
- Accepted to ICCV2023

---

## Lighting up NeRF via Unsupervised Decomposition and Enhancement

nerf{: .label .label-blue }

2023-07-20 | Haoyuan Wang, Xiaogang Xu, Ke Xu, Rynson WH. Lau | cs.CV | [PDF](http://arxiv.org/pdf/2307.10664v1){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) is a promising approach for synthesizing novel
views, given a set of images and the corresponding camera poses of a scene.
However, images photographed from a low-light scene can hardly be used to train
a NeRF model to produce high-quality results, due to their low pixel
intensities, heavy noise, and color distortion. Combining existing low-light
image enhancement methods with NeRF methods also does not work well due to the
view inconsistency caused by the individual 2D enhancement process. In this
paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to
enhance the scene representation and synthesize normal-light novel views
directly from sRGB low-light images in an unsupervised manner. The core of our
approach is a decomposition of radiance field learning, which allows us to
enhance the illumination, reduce noise and correct the distorted colors jointly
with the NeRF optimization process. Our method is able to produce novel view
images with proper lighting and vivid colors and details, given a collection of
camera-finished low dynamic range (8-bits/channel) images from a low-light
scene. Experiments demonstrate that our method outperforms existing low-light
enhancement methods and NeRF methods.

Comments:
- ICCV 2023. Project website: https://whyy.site/paper/llnerf

---

## Magic NeRF Lens: Interactive Fusion of Neural Radiance Fields for  Virtual Facility Inspection

nerf{: .label .label-blue }

2023-07-19 | Ke Li, Susanne Schmidt, Tim Rolff, Reinhard Bacher, Wim Leemans, Frank Steinicke | cs.GR | [PDF](http://arxiv.org/pdf/2307.09860v1){: .btn .btn-green }

**Abstract**: Large industrial facilities such as particle accelerators and nuclear power
plants are critical infrastructures for scientific research and industrial
processes. These facilities are complex systems that not only require regular
maintenance and upgrades but are often inaccessible to humans due to various
safety hazards. Therefore, a virtual reality (VR) system that can quickly
replicate real-world remote environments to provide users with a high level of
spatial and situational awareness is crucial for facility maintenance planning.
However, the exact 3D shapes of these facilities are often too complex to be
accurately modeled with geometric primitives through the traditional
rasterization pipeline.
  In this work, we develop Magic NeRF Lens, an interactive framework to support
facility inspection in immersive VR using neural radiance fields (NeRF) and
volumetric rendering. We introduce a novel data fusion approach that combines
the complementary strengths of volumetric rendering and geometric
rasterization, allowing a NeRF model to be merged with other conventional 3D
data, such as a computer-aided design model. We develop two novel 3D magic lens
effects to optimize NeRF rendering by exploiting the properties of human vision
and context-aware visualization. We demonstrate the high usability of our
framework and methods through a technical benchmark, a visual search user
study, and expert reviews. In addition, the source code of our VR NeRF
framework is made publicly available for future research and development.

Comments:
- This work has been submitted to the IEEE TVCG for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible

---

## Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking  Portrait Synthesis

nerf{: .label .label-blue }

2023-07-18 | Jiahe Li, Jiawei Zhang, Xiao Bai, Jun Zhou, Lin Gu | cs.CV | [PDF](http://arxiv.org/pdf/2307.09323v2){: .btn .btn-green }

**Abstract**: This paper presents ER-NeRF, a novel conditional Neural Radiance Fields
(NeRF) based architecture for talking portrait synthesis that can concurrently
achieve fast convergence, real-time rendering, and state-of-the-art performance
with small model size. Our idea is to explicitly exploit the unequal
contribution of spatial regions to guide talking portrait modeling.
Specifically, to improve the accuracy of dynamic head reconstruction, a compact
and expressive NeRF-based Tri-Plane Hash Representation is introduced by
pruning empty spatial regions with three planar hash encoders. For speech
audio, we propose a Region Attention Module to generate region-aware condition
feature via an attention mechanism. Different from existing methods that
utilize an MLP-based encoder to learn the cross-modal relation implicitly, the
attention mechanism builds an explicit connection between audio features and
spatial regions to capture the priors of local motions. Moreover, a direct and
fast Adaptive Pose Encoding is introduced to optimize the head-torso separation
problem by mapping the complex transformation of the head pose into spatial
coordinates. Extensive experiments demonstrate that our method renders better
high-fidelity and audio-lips synchronized talking portrait videos, with
realistic details and high efficiency compared to previous methods.

Comments:
- Accepted by ICCV 2023. Project page:
  https://fictionarry.github.io/ER-NeRF/

---

## OPHAvatars: One-shot Photo-realistic Head Avatars



2023-07-18 | Shaoxu Li | cs.CV | [PDF](http://arxiv.org/pdf/2307.09153v2){: .btn .btn-green }

**Abstract**: We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.

Comments:
- code: https://github.com/lsx0101/OPHAvatars

---

## PixelHuman: Animatable Neural Radiance Fields from Few Images



2023-07-18 | Gyumin Shim, Jaeseong Lee, Junha Hyung, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2307.09070v1){: .btn .btn-green }

**Abstract**: In this paper, we propose PixelHuman, a novel human rendering model that
generates animatable human scenes from a few images of a person with unseen
identity, views, and poses. Previous work have demonstrated reasonable
performance in novel view and pose synthesis, but they rely on a large number
of images to train and are trained per scene from videos, which requires
significant amount of time to produce animatable scenes from unseen human
images. Our method differs from existing methods in that it can generalize to
any input image for animatable human synthesis. Given a random pose sequence,
our method synthesizes each target scene using a neural radiance field that is
conditioned on a canonical representation and pose-aware pixel-aligned
features, both of which can be obtained through deformation fields learned in a
data-driven manner. Our experiments show that our method achieves
state-of-the-art performance in multiview and novel pose synthesis from
few-shot images.

Comments:
- 8 pages

---

## Cross-Ray Neural Radiance Fields for Novel-view Synthesis from  Unconstrained Image Collections

nerf{: .label .label-blue }

2023-07-16 | Yifan Yang, Shuhai Zhang, Zixiong Huang, Yubing Zhang, Mingkui Tan | cs.CV | [PDF](http://arxiv.org/pdf/2307.08093v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) is a revolutionary approach for rendering
scenes by sampling a single ray per pixel and it has demonstrated impressive
capabilities in novel-view synthesis from static scene images. However, in
practice, we usually need to recover NeRF from unconstrained image collections,
which poses two challenges: 1) the images often have dynamic changes in
appearance because of different capturing time and camera settings; 2) the
images may contain transient objects such as humans and cars, leading to
occlusion and ghosting artifacts. Conventional approaches seek to address these
challenges by locally utilizing a single ray to synthesize a color of a pixel.
In contrast, humans typically perceive appearance and objects by globally
utilizing information across multiple pixels. To mimic the perception process
of humans, in this paper, we propose Cross-Ray NeRF (CR-NeRF) that leverages
interactive information across multiple rays to synthesize occlusion-free novel
views with the same appearances as the images. Specifically, to model varying
appearances, we first propose to represent multiple rays with a novel cross-ray
feature and then recover the appearance by fusing global statistics, i.e.,
feature covariance of the rays and the image appearance. Moreover, to avoid
occlusion introduced by transient objects, we propose a transient objects
handler and introduce a grid sampling strategy for masking out the transient
objects. We theoretically find that leveraging correlation across multiple rays
promotes capturing more global information. Moreover, extensive experimental
results on large real-world datasets verify the effectiveness of CR-NeRF.

Comments:
- ICCV 2023 Oral

---

## Improving NeRF with Height Data for Utilization of GIS Data

nerf{: .label .label-blue }

2023-07-15 | Hinata Aoki, Takao Yamanaka | cs.CV | [PDF](http://arxiv.org/pdf/2307.07729v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) has been applied to various tasks related to
representations of 3D scenes. Most studies based on NeRF have focused on a
small object, while a few studies have tried to reconstruct large-scale scenes
although these methods tend to require large computational cost. For the
application of NeRF to large-scale scenes, a method based on NeRF is proposed
in this paper to effectively use height data which can be obtained from GIS
(Geographic Information System). For this purpose, the scene space was divided
into multiple objects and a background using the height data to represent them
with separate neural networks. In addition, an adaptive sampling method is also
proposed by using the height data. As a result, the accuracy of image rendering
was improved with faster training speed.

Comments:
- ICIP2023

---

## Transient Neural Radiance Fields for Lidar View Synthesis and 3D  Reconstruction

nerf{: .label .label-blue }

2023-07-14 | Anagh Malik, Parsa Mirdehghan, Sotiris Nousias, Kiriakos N. Kutulakos, David B. Lindell | cs.CV | [PDF](http://arxiv.org/pdf/2307.09555v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling
scene appearance and geometry from multiview imagery. Recent work has also
begun to explore how to use additional supervision from lidar or depth sensor
measurements in the NeRF framework. However, previous lidar-supervised NeRFs
focus on rendering conventional camera imagery and use lidar-derived point
cloud data as auxiliary supervision; thus, they fail to incorporate the
underlying image formation model of the lidar. Here, we propose a novel method
for rendering transient NeRFs that take as input the raw, time-resolved photon
count histograms measured by a single-photon lidar system, and we seek to
render such histograms from novel views. Different from conventional NeRFs, the
approach relies on a time-resolved version of the volume rendering equation to
render the lidar measurements and capture transient light transport phenomena
at picosecond timescales. We evaluate our method on a first-of-its-kind dataset
of simulated and captured transient multiview scans from a prototype
single-photon lidar. Overall, our work brings NeRFs to a new dimension of
imaging at transient timescales, newly enabling rendering of transient imagery
from novel views. Additionally, we show that our approach recovers improved
geometry and conventional appearance compared to point cloud-based supervision
when training on few input viewpoints. Transient NeRFs may be especially useful
for applications which seek to simulate raw lidar measurements for downstream
tasks in autonomous driving, robotics, and remote sensing.

---

## CeRF: Convolutional Neural Radiance Fields for New View Synthesis with  Derivatives of Ray Modeling



2023-07-14 | Xiaoyan Yang, Dingbo Lu, Yang Li, Chenhui Li, Changbo Wang | cs.CV | [PDF](http://arxiv.org/pdf/2307.07125v2){: .btn .btn-green }

**Abstract**: In recent years, novel view synthesis has gained popularity in generating
high-fidelity images. While demonstrating superior performance in the task of
synthesizing novel views, the majority of these methods are still based on the
conventional multi-layer perceptron for scene embedding. Furthermore, light
field models suffer from geometric blurring during pixel rendering, while
radiance field-based volume rendering methods have multiple solutions for a
certain target of density distribution integration. To address these issues, we
introduce the Convolutional Neural Radiance Fields to model the derivatives of
radiance along rays. Based on 1D convolutional operations, our proposed method
effectively extracts potential ray representations through a structured neural
network architecture. Besides, with the proposed ray modeling, a proposed
recurrent module is employed to solve geometric ambiguity in the fully neural
rendering process. Extensive experiments demonstrate the promising results of
our proposed model compared with existing state-of-the-art methods.

Comments:
- 16 pages, 11 figures

---

## SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View  Representation

nerf{: .label .label-blue }

2023-07-11 | Zhengxin Lei, Feng Xu, Jiangtao Wei, Feng Cai, Feng Wang, Ya-Qiu Jin | cs.CV | [PDF](http://arxiv.org/pdf/2307.05087v1){: .btn .btn-green }

**Abstract**: SAR images are highly sensitive to observation configurations, and they
exhibit significant variations across different viewing angles, making it
challenging to represent and learn their anisotropic features. As a result,
deep learning methods often generalize poorly across different view angles.
Inspired by the concept of neural radiance fields (NeRF), this study combines
SAR imaging mechanisms with neural networks to propose a novel NeRF model for
SAR image generation. Following the mapping and projection pinciples, a set of
SAR images is modeled implicitly as a function of attenuation coefficients and
scattering intensities in the 3D imaging space through a differentiable
rendering equation. SAR-NeRF is then constructed to learn the distribution of
attenuation coefficients and scattering intensities of voxels, where the
vectorized form of 3D voxel SAR rendering equation and the sampling
relationship between the 3D space voxels and the 2D view ray grids are
analytically derived. Through quantitative experiments on various datasets, we
thoroughly assess the multi-view representation and generalization capabilities
of SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can
significantly improve SAR target classification performance under few-shot
learning setup, where a 10-type classification accuracy of 91.6\% can be
achieved by using only 12 images per class.

---

## RGB-D Mapping and Tracking in a Plenoxel Radiance Field

nerf{: .label .label-blue }

2023-07-07 | Andreas L. Teigen, Yeonsoo Park, Annette Stahl, Rudolf Mester | cs.CV | [PDF](http://arxiv.org/pdf/2307.03404v2){: .btn .btn-green }

**Abstract**: The widespread adoption of Neural Radiance Fields (NeRFs) have ensured
significant advances in the domain of novel view synthesis in recent years.
These models capture a volumetric radiance field of a scene, creating highly
convincing, dense, photorealistic models through the use of simple,
differentiable rendering equations. Despite their popularity, these algorithms
suffer from severe ambiguities in visual data inherent to the RGB sensor, which
means that although images generated with view synthesis can visually appear
very believable, the underlying 3D model will often be wrong. This considerably
limits the usefulness of these models in practical applications like Robotics
and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise
would be of significant value. In this paper, we present the vital differences
between view synthesis models and 3D reconstruction models. We also comment on
why a depth sensor is essential for modeling accurate geometry in general
outward-facing scenes using the current paradigm of novel view synthesis
methods. Focusing on the structure-from-motion task, we practically demonstrate
this need by extending the Plenoxel radiance field model: Presenting an
analytical differential approach for dense mapping and tracking with radiance
fields based on RGB-D data without a neural network. Our method achieves
state-of-the-art results in both mapping and tracking tasks, while also being
faster than competing neural network-based approaches. The code is available
at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git

Comments:
- 2024 IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV) *The first two authors contributed equally to this paper

---

## NOFA: NeRF-based One-shot Facial Avatar Reconstruction

nerf{: .label .label-blue }

2023-07-07 | Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, Baoyuan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2307.03441v1){: .btn .btn-green }

**Abstract**: 3D facial avatar reconstruction has been a significant research topic in
computer graphics and computer vision, where photo-realistic rendering and
flexible controls over poses and expressions are necessary for many related
applications. Recently, its performance has been greatly improved with the
development of neural radiance fields (NeRF). However, most existing NeRF-based
facial avatars focus on subject-specific reconstruction and reenactment,
requiring multi-shot images containing different views of the specific subject
for training, and the learned model cannot generalize to new identities,
limiting its further applications. In this work, we propose a one-shot 3D
facial avatar reconstruction framework that only requires a single source image
to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking
generalization ability and missing multi-view information, we leverage the
generative prior of 3D GAN and develop an efficient encoder-decoder network to
reconstruct the canonical neural volume of the source image, and further
propose a compensation network to complement facial details. To enable
fine-grained control over facial dynamics, we propose a deformation field to
warp the canonical volume into driven expressions. Through extensive
experimental comparisons, we achieve superior synthesis results compared to
several state-of-the-art methods.

---

## Magic123: One Image to High-Quality 3D Object Generation Using Both 2D  and 3D Diffusion Priors



2023-06-30 | Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, Bernard Ghanem | cs.CV | [PDF](http://arxiv.org/pdf/2306.17843v2){: .btn .btn-green }

**Abstract**: We present Magic123, a two-stage coarse-to-fine approach for high-quality,
textured 3D meshes generation from a single unposed image in the wild using
both2D and 3D priors. In the first stage, we optimize a neural radiance field
to produce a coarse geometry. In the second stage, we adopt a memory-efficient
differentiable mesh representation to yield a high-resolution mesh with a
visually appealing texture. In both stages, the 3D content is learned through
reference view supervision and novel views guided by a combination of 2D and 3D
diffusion priors. We introduce a single trade-off parameter between the 2D and
3D priors to control exploration (more imaginative) and exploitation (more
precise) of the generated geometry. Additionally, we employ textual inversion
and monocular depth regularization to encourage consistent appearances across
views and to prevent degenerate solutions, respectively. Magic123 demonstrates
a significant improvement over previous image-to-3D techniques, as validated
through extensive experiments on synthetic benchmarks and diverse real-world
images. Our code, models, and generated 3D assets are available at
https://github.com/guochengqian/Magic123.

Comments:
- webpage: https://guochengqian.github.io/project/magic123/

---

## FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis

nerf{: .label .label-blue }

2023-06-30 | Seunghyeon Seo, Yeonjin Chang, Nojun Kwak | cs.CV | [PDF](http://arxiv.org/pdf/2306.17723v4){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis
with its remarkable quality of rendered images and simple architecture.
Although NeRF has been developed in various directions improving continuously
its performance, the necessity of a dense set of multi-view images still exists
as a stumbling block to progress for practical application. In this work, we
propose FlipNeRF, a novel regularization method for few-shot novel view
synthesis by utilizing our proposed flipped reflection rays. The flipped
reflection rays are explicitly derived from the input ray directions and
estimated normal vectors, and play a role of effective additional training rays
while enabling to estimate more accurate surface normals and learn the 3D
geometry effectively. Since the surface normal and the scene depth are both
derived from the estimated densities along a ray, the accurate surface normal
leads to more exact depth estimation, which is a key factor for few-shot novel
view synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss
and Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more
reliable outputs with reducing floating artifacts effectively across the
different scene structures, and enhance the feature-level consistency between
the pair of the rays cast toward the photo-consistent pixels without any
additional feature extractor, respectively. Our FlipNeRF achieves the SOTA
performance on the multiple benchmarks across all the scenarios.

Comments:
- ICCV 2023. Project Page: https://shawn615.github.io/flipnerf/

---

## Sphere2Vec: A General-Purpose Location Representation Learning over a  Spherical Surface for Large-Scale Geospatial Predictions

nerf{: .label .label-blue }

2023-06-30 | Gengchen Mai, Yao Xuan, Wenyun Zuo, Yutong He, Jiaming Song, Stefano Ermon, Krzysztof Janowicz, Ni Lao | cs.CV | [PDF](http://arxiv.org/pdf/2306.17624v2){: .btn .btn-green }

**Abstract**: Generating learning-friendly representations for points in space is a
fundamental and long-standing problem in ML. Recently, multi-scale encoding
schemes (such as Space2Vec and NeRF) were proposed to directly encode any point
in 2D/3D Euclidean space as a high-dimensional vector, and has been
successfully applied to various geospatial prediction and generative tasks.
However, all current 2D and 3D location encoders are designed to model point
distances in Euclidean space. So when applied to large-scale real-world GPS
coordinate datasets, which require distance metric learning on the spherical
surface, both types of models can fail due to the map projection distortion
problem (2D) and the spherical-to-Euclidean distance approximation error (3D).
To solve these problems, we propose a multi-scale location encoder called
Sphere2Vec which can preserve spherical distances when encoding point
coordinates on a spherical surface. We developed a unified view of
distance-reserving encoding on spheres based on the DFS. We also provide
theoretical proof that the Sphere2Vec preserves the spherical surface distance
between any two points, while existing encoding schemes do not. Experiments on
20 synthetic datasets show that Sphere2Vec can outperform all baseline models
on all these datasets with up to 30.8% error rate reduction. We then apply
Sphere2Vec to three geo-aware image classification tasks - fine-grained species
recognition, Flickr image recognition, and remote sensing image classification.
Results on 7 real-world datasets show the superiority of Sphere2Vec over
multiple location encoders on all three tasks. Further analysis shows that
Sphere2Vec outperforms other location encoder models, especially in the polar
regions and data-sparse areas because of its nature for spherical surface
distance preservation. Code and data are available at
https://gengchenmai.github.io/sphere2vec-website/.

Comments:
- 30 Pages, 16 figures. Accepted to ISPRS Journal of Photogrammetry and
  Remote Sensing

---

## One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape  Optimization



2023-06-29 | Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, Hao Su | cs.CV | [PDF](http://arxiv.org/pdf/2306.16928v1){: .btn .btn-green }

**Abstract**: Single image 3D reconstruction is an important but challenging task that
requires extensive knowledge of our natural world. Many existing methods solve
this problem by optimizing a neural radiance field under the guidance of 2D
diffusion models but suffer from lengthy optimization time, 3D inconsistency
results, and poor geometry. In this work, we propose a novel method that takes
a single image of any object as input and generates a full 360-degree 3D
textured mesh in a single feed-forward pass. Given a single image, we first use
a view-conditioned 2D diffusion model, Zero123, to generate multi-view images
for the input view, and then aim to lift them up to 3D space. Since traditional
reconstruction methods struggle with inconsistent multi-view predictions, we
build our 3D reconstruction module upon an SDF-based generalizable neural
surface reconstruction method and propose several critical training strategies
to enable the reconstruction of 360-degree meshes. Without costly
optimizations, our method reconstructs 3D shapes in significantly less time
than existing methods. Moreover, our method favors better geometry, generates
more 3D consistent results, and adheres more closely to the input image. We
evaluate our approach on both synthetic data and in-the-wild images and
demonstrate its superiority in terms of both mesh quality and runtime. In
addition, our approach can seamlessly support the text-to-3D task by
integrating with off-the-shelf text-to-image diffusion models.

Comments:
- project website: one-2-3-45.com

---

## Envisioning a Next Generation Extended Reality Conferencing System with  Efficient Photorealistic Human Rendering

nerf{: .label .label-blue }

2023-06-28 | Chuanyue Shen, Letian Zhang, Zhangsihao Yang, Masood Mortazavi, Xiyun Song, Liang Peng, Heather Yu | cs.CV | [PDF](http://arxiv.org/pdf/2306.16541v1){: .btn .btn-green }

**Abstract**: Meeting online is becoming the new normal. Creating an immersive experience
for online meetings is a necessity towards more diverse and seamless
environments. Efficient photorealistic rendering of human 3D dynamics is the
core of immersive meetings. Current popular applications achieve real-time
conferencing but fall short in delivering photorealistic human dynamics, either
due to limited 2D space or the use of avatars that lack realistic interactions
between participants. Recent advances in neural rendering, such as the Neural
Radiance Field (NeRF), offer the potential for greater realism in metaverse
meetings. However, the slow rendering speed of NeRF poses challenges for
real-time conferencing. We envision a pipeline for a future extended reality
metaverse conferencing system that leverages monocular video acquisition and
free-viewpoint synthesis to enhance data and hardware efficiency. Towards an
immersive conferencing experience, we explore an accelerated NeRF-based
free-viewpoint synthesis algorithm for rendering photorealistic human dynamics
more efficiently. We show that our algorithm achieves comparable rendering
quality while performing training and inference 44.5% and 213% faster than
state-of-the-art methods, respectively. Our exploration provides a design basis
for constructing metaverse conferencing systems that can handle complex
application scenarios, including dynamic scene relighting with customized
themes and multi-user conferencing that harmonizes real-world people into an
extended world.

Comments:
- Accepted to CVPR 2023 ECV Workshop

---

## Unsupervised Polychromatic Neural Representation for CT Metal Artifact  Reduction

nerf{: .label .label-blue }

2023-06-27 | Qing Wu, Lixuan Chen, Ce Wang, Hongjiang Wei, S. Kevin Zhou, Jingyi Yu, Yuyao Zhang | eess.IV | [PDF](http://arxiv.org/pdf/2306.15203v2){: .btn .btn-green }

**Abstract**: Emerging neural reconstruction techniques based on tomography (e.g., NeRF,
NeAT, and NeRP) have started showing unique capabilities in medical imaging. In
this work, we present a novel Polychromatic neural representation (Polyner) to
tackle the challenging problem of CT imaging when metallic implants exist
within the human body. CT metal artifacts arise from the drastic variation of
metal's attenuation coefficients at various energy levels of the X-ray
spectrum, leading to a nonlinear metal effect in CT measurements. Recovering CT
images from metal-affected measurements hence poses a complicated nonlinear
inverse problem where empirical models adopted in previous metal artifact
reduction (MAR) approaches lead to signal loss and strongly aliased
reconstructions. Polyner instead models the MAR problem from a nonlinear
inverse problem perspective. Specifically, we first derive a polychromatic
forward model to accurately simulate the nonlinear CT acquisition process.
Then, we incorporate our forward model into the implicit neural representation
to accomplish reconstruction. Lastly, we adopt a regularizer to preserve the
physical properties of the CT images across different energy levels while
effectively constraining the solution space. Our Polyner is an unsupervised
method and does not require any external training data. Experimenting with
multiple datasets shows that our Polyner achieves comparable or better
performance than supervised methods on in-domain datasets while demonstrating
significant performance improvements on out-of-domain datasets. To the best of
our knowledge, our Polyner is the first unsupervised MAR method that
outperforms its supervised counterparts. The code for this work is available
at: https://github.com/iwuqing/Polyner.

Comments:
- Accepted by NeurIPS 2023

---

## Toward a Spectral Foundation Model: An Attention-Based Approach with  Domain-Inspired Fine-Tuning and Wavelength Parameterization



2023-06-27 | Tomasz Różański, Yuan-Sen Ting, Maja Jabłońska | astro-ph.IM | [PDF](http://arxiv.org/pdf/2306.15703v1){: .btn .btn-green }

**Abstract**: Astrophysical explorations are underpinned by large-scale stellar
spectroscopy surveys, necessitating a paradigm shift in spectral fitting
techniques. Our study proposes three enhancements to transcend the limitations
of the current spectral emulation models. We implement an attention-based
emulator, adept at unveiling long-range information between wavelength pixels.
We leverage a domain-specific fine-tuning strategy where the model is
pre-trained on spectra with fixed stellar parameters and variable elemental
abundances, followed by fine-tuning on the entire domain. Moreover, by treating
wavelength as an autonomous model parameter, akin to neural radiance fields,
the model can generate spectra on any wavelength grid. In the case with a
training set of O(1000), our approach exceeds current leading methods by a
factor of 5-10 across all metrics.

Comments:
- 7 pages, 3 figures, accepted to ICML 2023 Workshop on Machine
  Learning for Astrophysics

---

## TaiChi Action Capture and Performance Analysis with Multi-view RGB  Cameras



2023-06-26 | Jianwei Li, Siyu Mo, Yanfei Shen | cs.CV | [PDF](http://arxiv.org/pdf/2306.14490v1){: .btn .btn-green }

**Abstract**: Recent advances in computer vision and deep learning have influenced the
field of sports performance analysis for researchers to track and reconstruct
freely moving humans without any marker attachment. However, there are few
works for vision-based motion capture and intelligent analysis for professional
TaiChi movement. In this paper, we propose a framework for TaiChi performance
capture and analysis with multi-view geometry and artificial intelligence
technology. The main innovative work is as follows: 1) A multi-camera system
suitable for TaiChi motion capture is built and the multi-view TaiChi data is
collected and processed; 2) A combination of traditional visual method and
implicit neural radiance field is proposed to achieve sparse 3D skeleton fusion
and dense 3D surface reconstruction. 3) The normalization modeling of movement
sequences is carried out based on motion transfer, so as to realize TaiChi
performance analysis for different groups. We have carried out evaluation
experiments, and the experimental results have shown the efficiency of our
method.

---

## Blended-NeRF: Zero-Shot Object Generation and Blending in Existing  Neural Radiance Fields

nerf{: .label .label-blue }

2023-06-22 | Ori Gordon, Omri Avrahami, Dani Lischinski | cs.CV | [PDF](http://arxiv.org/pdf/2306.12760v2){: .btn .btn-green }

**Abstract**: Editing a local region or a specific object in a 3D scene represented by a
NeRF or consistently blending a new realistic object into the scene is
challenging, mainly due to the implicit nature of the scene representation. We
present Blended-NeRF, a robust and flexible framework for editing a specific
region of interest in an existing NeRF scene, based on text prompts, along with
a 3D ROI box. Our method leverages a pretrained language-image model to steer
the synthesis towards a user-provided text prompt, along with a 3D MLP model
initialized on an existing NeRF scene to generate the object and blend it into
a specified region in the original scene. We allow local editing by localizing
a 3D ROI box in the input scene, and blend the content synthesized inside the
ROI with the existing scene using a novel volumetric blending technique. To
obtain natural looking and view-consistent results, we leverage existing and
new geometric priors and 3D augmentations for improving the visual fidelity of
the final result. We test our framework both qualitatively and quantitatively
on a variety of real 3D scenes and text prompts, demonstrating realistic
multi-view consistent results with much flexibility and diversity compared to
the baselines. Finally, we show the applicability of our framework for several
3D editing applications, including adding new objects to a scene,
removing/replacing/altering existing objects, and texture conversion.

Comments:
- 16 pages, 14 figures. Project page:
  https://www.vision.huji.ac.il/blended-nerf/

---

## Local 3D Editing via 3D Distillation of CLIP Knowledge

nerf{: .label .label-blue }

2023-06-21 | Junha Hyung, Sungwon Hwang, Daejin Kim, Hyunji Lee, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2306.12570v1){: .btn .btn-green }

**Abstract**: 3D content manipulation is an important computer vision task with many
real-world applications (e.g., product design, cartoon generation, and 3D
Avatar editing). Recently proposed 3D GANs can generate diverse photorealistic
3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of
NeRF still remains a challenging problem since the visual quality tends to
degrade after manipulation and suboptimal control handles such as 2D semantic
maps are used for manipulations. While text-guided manipulations have shown
potential in 3D editing, such approaches often lack locality. To overcome these
problems, we propose Local Editing NeRF (LENeRF), which only requires text
inputs for fine-grained and localized manipulation. Specifically, we present
three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field
Network, and the Deformation Network, which are jointly used for local
manipulations of 3D features by estimating a 3D attention field. The 3D
attention field is learned in an unsupervised way, by distilling the zero-shot
mask generation capability of CLIP to the 3D space with multi-view guidance. We
conduct diverse experiments and thorough evaluations both quantitatively and
qualitatively.

Comments:
- conference: CVPR 2023

---

## Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized  Codebase

nerf{: .label .label-blue }

2023-06-21 | Qiuyu Wang, Zifan Shi, Kecheng Zheng, Yinghao Xu, Sida Peng, Yujun Shen | cs.CV | [PDF](http://arxiv.org/pdf/2306.12423v1){: .btn .btn-green }

**Abstract**: Despite the rapid advance of 3D-aware image synthesis, existing studies
usually adopt a mixture of techniques and tricks, leaving it unclear how each
part contributes to the final performance in terms of generality. Following the
most popular and effective paradigm in this field, which incorporates a neural
radiance field (NeRF) into the generator of a generative adversarial network
(GAN), we build a well-structured codebase, dubbed Carver, through modularizing
the generation process. Such a design allows researchers to develop and replace
each module independently, and hence offers an opportunity to fairly compare
various approaches and recognize their contributions from the module
perspective. The reproduction of a range of cutting-edge algorithms
demonstrates the availability of our modularized codebase. We also perform a
variety of in-depth analyses, such as the comparison across different types of
point feature, the necessity of the tailing upsampler in the generator, the
reliance on the camera pose prior, etc., which deepen our understanding of
existing methods and point out some further directions of the research work. We
release code and models at https://github.com/qiuyu96/Carver to facilitate the
development and evaluation of this field.

Comments:
- Code: https://github.com/qiuyu96/Carver

---

## DreamTime: An Improved Optimization Strategy for Text-to-3D Content  Creation

nerf{: .label .label-blue }

2023-06-21 | Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2306.12422v1){: .btn .btn-green }

**Abstract**: Text-to-image diffusion models pre-trained on billions of image-text pairs
have recently enabled text-to-3D content creation by optimizing a randomly
initialized Neural Radiance Fields (NeRF) with score distillation. However, the
resultant 3D models exhibit two limitations: (a) quality concerns such as
saturated color and the Janus problem; (b) extremely low diversity comparing to
text-guided image synthesis. In this paper, we show that the conflict between
NeRF optimization process and uniform timestep sampling in score distillation
is the main reason for these limitations. To resolve this conflict, we propose
to prioritize timestep sampling with monotonically non-increasing functions,
which aligns NeRF optimization with the sampling process of diffusion model.
Extensive experiments show that our simple redesign significantly improves
text-to-3D content creation with higher quality and diversity.

---

## NeRF synthesis with shading guidance

nerf{: .label .label-blue }

2023-06-20 | Chenbin Li, Yu Xin, Gaoyi Liu, Xiang Zeng, Ligang Liu | cs.CV | [PDF](http://arxiv.org/pdf/2306.11556v1){: .btn .btn-green }

**Abstract**: The emerging Neural Radiance Field (NeRF) shows great potential in
representing 3D scenes, which can render photo-realistic images from novel view
with only sparse views given. However, utilizing NeRF to reconstruct real-world
scenes requires images from different viewpoints, which limits its practical
application. This problem can be even more pronounced for large scenes. In this
paper, we introduce a new task called NeRF synthesis that utilizes the
structural content of a NeRF patch exemplar to construct a new radiance field
of large size. We propose a two-phase method for synthesizing new scenes that
are continuous in geometry and appearance. We also propose a boundary
constraint method to synthesize scenes of arbitrary size without artifacts.
Specifically, we control the lighting effects of synthesized scenes using
shading guidance instead of decoupling the scene. We have demonstrated that our
method can generate high-quality results with consistent geometry and
appearance, even for scenes with complex lighting. We can also synthesize new
scenes on curved surface with arbitrary lighting effects, which enhances the
practicality of our proposed NeRF synthesis approach.

Comments:
- 16 pages, 16 figures, accepted by CAD/Graphics 2023(poster)

---

## MA-NeRF: Motion-Assisted Neural Radiance Fields for Face Synthesis from  Sparse Images

nerf{: .label .label-blue }

2023-06-17 | Weichen Zhang, Xiang Zhou, Yukang Cao, Wensen Feng, Chun Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2306.10350v2){: .btn .btn-green }

**Abstract**: We address the problem of photorealistic 3D face avatar synthesis from sparse
images. Existing Parametric models for face avatar reconstruction struggle to
generate details that originate from inputs. Meanwhile, although current
NeRF-based avatar methods provide promising results for novel view synthesis,
they fail to generalize well for unseen expressions. We improve from NeRF and
propose a novel framework that, by leveraging the parametric 3DMM models, can
reconstruct a high-fidelity drivable face avatar and successfully handle the
unseen expressions. At the core of our implementation are structured
displacement feature and semantic-aware learning module. Our structured
displacement feature will introduce the motion prior as an additional
constraints and help perform better for unseen expressions, by constructing
displacement volume. Besides, the semantic-aware learning incorporates
multi-level prior, e.g., semantic embedding, learnable latent code, to lift the
performance to a higher level. Thorough experiments have been doen both
quantitatively and qualitatively to demonstrate the design of our framework,
and our method achieves much better results than the current state-of-the-arts.

---

## Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion  Model

nerf{: .label .label-blue }

2023-06-15 | Lu Yu, Wei Xiang, Kang Han | cs.CV | [PDF](http://arxiv.org/pdf/2306.09551v1){: .btn .btn-green }

**Abstract**: Recent research has demonstrated that the combination of pretrained diffusion
models with neural radiance fields (NeRFs) has emerged as a promising approach
for text-to-3D generation. Simply coupling NeRF with diffusion models will
result in cross-view inconsistency and degradation of stylized view syntheses.
To address this challenge, we propose the Edit-DiffNeRF framework, which is
composed of a frozen diffusion model, a proposed delta module to edit the
latent semantic space of the diffusion model, and a NeRF. Instead of training
the entire diffusion for each scene, our method focuses on editing the latent
semantic space in frozen pretrained diffusion models by the delta module. This
fundamental change to the standard diffusion framework enables us to make
fine-grained modifications to the rendered views and effectively consolidate
these instructions in a 3D scene via NeRF training. As a result, we are able to
produce an edited 3D scene that faithfully aligns to input text instructions.
Furthermore, to ensure semantic consistency across different viewpoints, we
propose a novel multi-view semantic consistency loss that extracts a latent
semantic embedding from the input view as a prior, and aim to reconstruct it in
different views. Our proposed method has been shown to effectively edit
real-world 3D scenes, resulting in 25% improvement in the alignment of the
performed 3D edits with text instructions compared to prior work.

---

## UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video

nerf{: .label .label-blue }

2023-06-15 | Zhi-Hao Lin, Bohan Liu, Yi-Ting Chen, David Forsyth, Jia-Bin Huang, Anand Bhattad, Shenlong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2306.09349v2){: .btn .btn-green }

**Abstract**: We show how to build a model that allows realistic, free-viewpoint renderings
of a scene under novel lighting conditions from video. Our method -- UrbanIR:
Urban Scene Inverse Rendering -- computes an inverse graphics representation
from the video. UrbanIR jointly infers shape, albedo, visibility, and sun and
sky illumination from a single video of unbounded outdoor scenes with unknown
lighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many
views of the same points in typical NeRF-style estimation). As a result,
standard methods produce poor geometry estimates (for example, roofs), and
there are numerous ''floaters''. Errors in inverse graphics inference can
result in strong rendering artifacts. UrbanIR uses novel losses to control
these and other sources of error. UrbanIR uses a novel loss to make very good
estimates of shadow volumes in the original scene. The resulting
representations facilitate controllable editing, delivering photorealistic
free-viewpoint renderings of relit scenes and inserted objects. Qualitative
evaluation demonstrates strong improvements over the state-of-the-art.

Comments:
- https://urbaninverserendering.github.io/

---

## DreamHuman: Animatable 3D Avatars from Text



2023-06-15 | Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Fieraru, Cristian Sminchisescu | cs.CV | [PDF](http://arxiv.org/pdf/2306.09329v1){: .btn .btn-green }

**Abstract**: We present DreamHuman, a method to generate realistic animatable 3D human
avatar models solely from textual descriptions. Recent text-to-3D methods have
made considerable strides in generation, but are still lacking in important
aspects. Control and often spatial resolution remain limited, existing methods
produce fixed rather than animated 3D human models, and anthropometric
consistency for complex structures like people remains a challenge. DreamHuman
connects large text-to-image synthesis models, neural radiance fields, and
statistical human body models in a novel modeling and optimization framework.
This makes it possible to generate dynamic 3D human avatars with high-quality
textures and learned, instance-specific, surface deformations. We demonstrate
that our method is capable to generate a wide variety of animatable, realistic
3D human models from text. Our 3D models have diverse appearance, clothing,
skin tones and body shapes, and significantly outperform both generic
text-to-3D approaches and previous text-based 3D avatar generators in visual
fidelity. For more results and animations please check our website at
https://dream-human.github.io.

Comments:
- Project website at https://dream-human.github.io/

---

## Parametric Implicit Face Representation for Audio-Driven Facial  Reenactment



2023-06-13 | Ricong Huang, Peiwen Lai, Yipeng Qin, Guanbin Li | cs.CV | [PDF](http://arxiv.org/pdf/2306.07579v1){: .btn .btn-green }

**Abstract**: Audio-driven facial reenactment is a crucial technique that has a range of
applications in film-making, virtual avatars and video conferences. Existing
works either employ explicit intermediate face representations (e.g., 2D facial
landmarks or 3D face models) or implicit ones (e.g., Neural Radiance Fields),
thus suffering from the trade-offs between interpretability and expressive
power, hence between controllability and quality of the results. In this work,
we break these trade-offs with our novel parametric implicit face
representation and propose a novel audio-driven facial reenactment framework
that is both controllable and can generate high-quality talking heads.
Specifically, our parametric implicit representation parameterizes the implicit
representation with interpretable parameters of 3D face models, thereby taking
the best of both explicit and implicit methods. In addition, we propose several
new techniques to improve the three components of our framework, including i)
incorporating contextual information into the audio-to-expression parameters
encoding; ii) using conditional image synthesis to parameterize the implicit
representation and implementing it with an innovative tri-plane structure for
efficient learning; iii) formulating facial reenactment as a conditional image
inpainting problem and proposing a novel data augmentation technique to improve
model generalizability. Extensive experiments demonstrate that our method can
generate more realistic results than previous methods with greater fidelity to
the identities and talking styles of speakers.

Comments:
- CVPR 2023

---

## Binary Radiance Fields

nerf{: .label .label-blue }

2023-06-13 | Seungjoo Shin, Jaesik Park | cs.CV | [PDF](http://arxiv.org/pdf/2306.07581v2){: .btn .btn-green }

**Abstract**: In this paper, we propose \textit{binary radiance fields} (BiRF), a
storage-efficient radiance field representation employing binary feature
encoding that encodes local features using binary encoding parameters in a
format of either $+1$ or $-1$. This binarization strategy lets us represent the
feature grid with highly compact feature encoding and a dramatic reduction in
storage size. Furthermore, our 2D-3D hybrid feature grid design enhances the
compactness of feature encoding as the 3D grid includes main components while
2D grids capture details. In our experiments, binary radiance field
representation successfully outperforms the reconstruction performance of
state-of-the-art (SOTA) efficient radiance field models with lower storage
allocation. In particular, our model achieves impressive results in static
scene reconstruction, with a PSNR of 32.03 dB for Synthetic-NeRF scenes, 34.48
dB for Synthetic-NSVF scenes, 28.20 dB for Tanks and Temples scenes while only
utilizing 0.5 MB of storage space, respectively. We hope the proposed binary
radiance field representation will make radiance fields more accessible without
a storage bottleneck.

Comments:
- Accepted to NeurIPS 2023. Project page:
  https://seungjooshin.github.io/BiRF

---

## DORSal: Diffusion for Object-centric Representations of Scenes et al

nerf{: .label .label-blue }

2023-06-13 | Allan Jabri, Sjoerd van Steenkiste, Emiel Hoogeboom, Mehdi S. M. Sajjadi, Thomas Kipf | cs.CV | [PDF](http://arxiv.org/pdf/2306.08068v2){: .btn .btn-green }

**Abstract**: Recent progress in 3D scene understanding enables scalable learning of
representations across large datasets of diverse scenes. As a consequence,
generalization to unseen scenes and objects, rendering novel views from just a
single or a handful of input images, and controllable scene generation that
supports editing, is now possible. However, training jointly on a large number
of scenes typically compromises rendering quality when compared to single-scene
optimized models such as NeRFs. In this paper, we leverage recent progress in
diffusion models to equip 3D scene representation learning models with the
ability to render high-fidelity novel views, while retaining benefits such as
object-level scene editing to a large degree. In particular, we propose DORSal,
which adapts a video diffusion architecture for 3D scene generation conditioned
on frozen object-centric slot-based representations of scenes. On both complex
synthetic multi-object scenes and on the real-world large-scale Street View
dataset, we show that DORSal enables scalable neural rendering of 3D scenes
with object-level editing and improves upon existing approaches.

Comments:
- Project page: https://www.sjoerdvansteenkiste.com/dorsal

---

## From NeRFLiX to NeRFLiX++: A General NeRF-Agnostic Restorer Paradigm

nerf{: .label .label-blue }

2023-06-10 | Kun Zhou, Wenbo Li, Nianjuan Jiang, Xiaoguang Han, Jiangbo Lu | cs.CV | [PDF](http://arxiv.org/pdf/2306.06388v3){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) have shown great success in novel view
synthesis. However, recovering high-quality details from real-world scenes is
still challenging for the existing NeRF-based approaches, due to the potential
imperfect calibration information and scene representation inaccuracy. Even
with high-quality training frames, the synthetic novel views produced by NeRF
models still suffer from notable rendering artifacts, such as noise and blur.
To address this, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm
that learns a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for deep neural networks. Moreover, beyond the degradation removal,
we propose an inter-viewpoint aggregation framework that fuses highly related
high-quality training images, pushing the performance of cutting-edge NeRF
models to entirely new levels and producing highly photo-realistic synthetic
views. Based on this paradigm, we further present NeRFLiX++ with a stronger
two-stage NeRF degradation simulator and a faster inter-viewpoint mixer,
achieving superior performance with significantly improved computational
efficiency. Notably, NeRFLiX++ is capable of restoring photo-realistic
ultra-high-resolution outputs from noisy low-resolution NeRF-rendered views.
Extensive experiments demonstrate the excellent restoration ability of
NeRFLiX++ on various novel view synthesis benchmarks.

Comments:
- 17 pages, 17 figures. To appear in TPAMI2023. Project Page:
  https://redrock303.github.io/nerflix_plus/. arXiv admin note: text overlap
  with arXiv:2303.06919

---

## NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance  Fields against Adversarial Perturbations

nerf{: .label .label-blue }

2023-06-10 | Yonggan Fu, Ye Yuan, Souvik Kundu, Shang Wu, Shunyao Zhang, Yingyan Lin | cs.CV | [PDF](http://arxiv.org/pdf/2306.06359v1){: .btn .btn-green }

**Abstract**: Generalizable Neural Radiance Fields (GNeRF) are one of the most promising
real-world solutions for novel view synthesis, thanks to their cross-scene
generalization capability and thus the possibility of instant rendering on new
scenes. While adversarial robustness is essential for real-world applications,
little study has been devoted to understanding its implication on GNeRF. We
hypothesize that because GNeRF is implemented by conditioning on the source
views from new scenes, which are often acquired from the Internet or
third-party providers, there are potential new security concerns regarding its
real-world applications. Meanwhile, existing understanding and solutions for
neural networks' adversarial robustness may not be applicable to GNeRF, due to
its 3D nature and uniquely diverse operations. To this end, we present NeRFool,
which to the best of our knowledge is the first work that sets out to
understand the adversarial robustness of GNeRF. Specifically, NeRFool unveils
the vulnerability patterns and important insights regarding GNeRF's adversarial
robustness. Built upon the above insights gained from NeRFool, we further
develop NeRFool+, which integrates two techniques capable of effectively
attacking GNeRF across a wide range of target views, and provide guidelines for
defending against our proposed attacks. We believe that our NeRFool/NeRFool+
lays the initial foundation for future innovations in developing robust
real-world GNeRF solutions. Our codes are available at:
https://github.com/GATECH-EIC/NeRFool.

Comments:
- Accepted by ICML 2023

---

## NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction

nerf{: .label .label-blue }

2023-06-09 | Ali Karami, Simone Rigon, Gabriele Mazzacca, Ziyang Yan, Fabio Remondino | cs.CV | [PDF](http://arxiv.org/pdf/2306.06300v2){: .btn .btn-green }

**Abstract**: This paper introduces a new real and synthetic dataset called NeRFBK
specifically designed for testing and comparing NeRF-based 3D reconstruction
algorithms. High-quality 3D reconstruction has significant potential in various
fields, and advancements in image-based algorithms make it essential to
evaluate new advanced techniques. However, gathering diverse data with precise
ground truth is challenging and may not encompass all relevant applications.
The NeRFBK dataset addresses this issue by providing multi-scale, indoor and
outdoor datasets with high-resolution images and videos and camera parameters
for testing and comparing NeRF-based algorithms. This paper presents the design
and creation of the NeRFBK benchmark, various examples and application
scenarios, and highlights its potential for advancing the field of 3D
reconstruction.

Comments:
- paper result has problem

---

## HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork

nerf{: .label .label-blue }

2023-06-09 | Bipasha Sen, Gaurav Singh, Aditya Agarwal, Rohith Agaram, K Madhava Krishna, Srinath Sridhar | cs.CV | [PDF](http://arxiv.org/pdf/2306.06093v3){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have become an increasingly popular
representation to capture high-quality appearance and shape of scenes and
objects. However, learning generalizable NeRF priors over categories of scenes
or objects has been challenging due to the high dimensionality of network
weight space. To address the limitations of existing work on generalization,
multi-view consistency and to improve quality, we propose HyP-NeRF, a latent
conditioning method for learning generalizable category-level NeRF priors using
hypernetworks. Rather than using hypernetworks to estimate only the weights of
a NeRF, we estimate both the weights and the multi-resolution hash encodings
resulting in significant quality gains. To improve quality even further, we
incorporate a denoise and finetune strategy that denoises images rendered from
NeRFs estimated by the hypernetwork and finetunes it while retaining multiview
consistency. These improvements enable us to use HyP-NeRF as a generalizable
prior for multiple downstream tasks including NeRF reconstruction from
single-view or cluttered scenes and text-to-NeRF. We provide qualitative
comparisons and evaluate HyP-NeRF on three tasks: generalization, compression,
and retrieval, demonstrating our state-of-the-art results.

Comments:
- Project Page: https://hyp-nerf.github.io

---

## GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields

nerf{: .label .label-blue }

2023-06-09 | Barbara Roessle, Norman Müller, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2306.06044v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive novel view synthesis
results; nonetheless, even thorough recordings yield imperfections in
reconstructions, for instance due to poorly observed areas or minor lighting
changes. Our goal is to mitigate these imperfections from various sources with
a joint solution: we take advantage of the ability of generative adversarial
networks (GANs) to produce realistic images and use them to enhance realism in
3D scene reconstruction with NeRFs. To this end, we learn the patch
distribution of a scene using an adversarial discriminator, which provides
feedback to the radiance field reconstruction, thus improving realism in a
3D-consistent fashion. Thereby, rendering artifacts are repaired directly in
the underlying 3D representation by imposing multi-view path rendering
constraints. In addition, we condition a generator with multi-resolution NeRF
renderings which is adversarially trained to further improve rendering quality.
We demonstrate that our approach significantly improves rendering quality,
e.g., nearly halving LPIPS scores compared to Nerfacto while at the same time
improving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.

Comments:
- SIGGRAPH Asia 2023, project page:
  https://barbararoessle.github.io/ganerf , video: https://youtu.be/352ccXWxQVE

---

## RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models

nerf{: .label .label-blue }

2023-06-09 | Xingchen Zhou, Ying He, F. Richard Yu, Jianqiang Li, You Li | cs.CV | [PDF](http://arxiv.org/pdf/2306.05668v2){: .btn .btn-green }

**Abstract**: The emergence of Neural Radiance Fields (NeRF) has promoted the development
of synthesized high-fidelity views of the intricate real world. However, it is
still a very demanding task to repaint the content in NeRF. In this paper, we
propose a novel framework that can take RGB images as input and alter the 3D
content in neural scenes. Our work leverages existing diffusion models to guide
changes in the designated 3D content. Specifically, we semantically select the
target object and a pre-trained diffusion model will guide the NeRF model to
generate new 3D objects, which can improve the editability, diversity, and
application range of NeRF. Experiment results show that our algorithm is
effective for editing 3D objects in NeRF under different text prompts,
including editing appearance, shape, and more. We validate our method on both
real-world datasets and synthetic-world datasets for these editing tasks.
Please visit https://starstesla.github.io/repaintnerf for a better view of our
results.

Comments:
- IJCAI 2023 Accepted (Main Track)

---

## Variable Radiance Field for Real-Life Category-Specifc Reconstruction  from Single Image



2023-06-08 | Kun Wang, Zhiqiang Yan, Zhenyu Zhang, Xiang Li, Jun Li, Jian Yang | cs.CV | [PDF](http://arxiv.org/pdf/2306.05145v1){: .btn .btn-green }

**Abstract**: Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.

---

## Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields

nerf{: .label .label-blue }

2023-06-08 | Qianqiu Tan, Tao Liu, Yinling Xie, Shuwan Yu, Baohua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2306.05303v1){: .btn .btn-green }

**Abstract**: The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored "fog" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF

---

## LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs

nerf{: .label .label-blue }

2023-06-08 | Zezhou Cheng, Carlos Esteves, Varun Jampani, Abhishek Kar, Subhransu Maji, Ameesh Makadia | cs.CV | [PDF](http://arxiv.org/pdf/2306.05410v1){: .btn .btn-green }

**Abstract**: A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.

Comments:
- Project website: https://people.cs.umass.edu/~zezhoucheng/lu-nerf/

---

## BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives



2023-06-07 | Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip | cs.CV | [PDF](http://arxiv.org/pdf/2306.04166v3){: .btn .btn-green }

**Abstract**: Implicit neural representation has emerged as a powerful method for
reconstructing 3D scenes from 2D images. Given a set of camera poses and
associated images, the models can be trained to synthesize novel, unseen views.
In order to expand the use cases for implicit neural representations, we need
to incorporate camera pose estimation capabilities as part of the
representation learning, as this is necessary for reconstructing scenes from
real-world video sequences where cameras are generally not being tracked.
Existing approaches like COLMAP and, most recently, bundle-adjusting neural
radiance field methods often suffer from lengthy processing times. These delays
ranging from hours to days, arise from laborious feature matching, hardware
limitations, dense point sampling, and long training times required by a
multi-layer perceptron structure with a large number of parameters. To address
these challenges, we propose a framework called bundle-adjusting accelerated
neural graphics primitives (BAA-NGP). Our approach leverages accelerated
sampling and hash encoding to expedite both pose refinement/estimation and 3D
scene reconstruction. Experimental results demonstrate that our method achieves
a more than 10 to 20 $\times$ speed improvement in novel view synthesis
compared to other bundle-adjusting neural radiance field methods without
sacrificing the quality of pose estimation. The github repository can be found
here https://github.com/IntelLabs/baa-ngp.

---

## ATT3D: Amortized Text-to-3D Object Synthesis



2023-06-06 | Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, James Lucas | cs.LG | [PDF](http://arxiv.org/pdf/2306.07349v1){: .btn .btn-green }

**Abstract**: Text-to-3D modelling has seen exciting progress by combining generative
text-to-image models with image-to-3D methods like Neural Radiance Fields.
DreamFusion recently achieved high-quality results but requires a lengthy,
per-prompt optimization to create 3D objects. To address this, we amortize
optimization over text prompts by training on many prompts simultaneously with
a unified model, instead of separately. With this, we share computation across
a prompt set, training in less time than per-prompt optimization. Our framework
- Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to
generalize to unseen setups and smooth interpolations between text for novel
assets and simple animations.

Comments:
- 22 pages, 20 figures

---

## Towards Visual Foundational Models of Physical Scenes

nerf{: .label .label-blue }

2023-06-06 | Chethan Parameshwara, Alessandro Achille, Matthew Trager, Xiaolong Li, Jiawei Mo, Matthew Trager, Ashwin Swaminathan, CJ Taylor, Dheera Venkatraman, Xiaohan Fei, Stefano Soatto | cs.CV | [PDF](http://arxiv.org/pdf/2306.03727v1){: .btn .btn-green }

**Abstract**: We describe a first step towards learning general-purpose visual
representations of physical scenes using only image prediction as a training
criterion. To do so, we first define "physical scene" and show that, even
though different agents may maintain different representations of the same
scene, the underlying physical scene that can be inferred is unique. Then, we
show that NeRFs cannot represent the physical scene, as they lack extrapolation
mechanisms. Those, however, could be provided by Diffusion Models, at least in
theory. To test this hypothesis empirically, NeRFs can be combined with
Diffusion Models, a process we refer to as NeRF Diffusion, used as unsupervised
representations of the physical scene. Our analysis is limited to visual data,
without external grounding mechanisms that can be provided by independent
sensory modalities.

Comments:
- TLDR: Physical scenes are equivalence classes of sufficient
  statistics, and can be inferred uniquely by any agent measuring the same
  finite data; We formalize and implement an approach to representation
  learning that overturns "naive realism" in favor of an analytical approach of
  Russell and Koenderink. NeRFs cannot capture the physical scenes, but
  combined with Diffusion Models they can

---

## Human 3D Avatar Modeling with Implicit Neural Representation: A Brief  Survey

nerf{: .label .label-blue }

2023-06-06 | Mingyang Sun, Dingkang Yang, Dongliang Kou, Yang Jiang, Weihua Shan, Zhe Yan, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2306.03576v1){: .btn .btn-green }

**Abstract**: A human 3D avatar is one of the important elements in the metaverse, and the
modeling effect directly affects people's visual experience. However, the human
body has a complex topology and diverse details, so it is often expensive,
time-consuming, and laborious to build a satisfactory model. Recent studies
have proposed a novel method, implicit neural representation, which is a
continuous representation method and can describe objects with arbitrary
topology at arbitrary resolution. Researchers have applied implicit neural
representation to human 3D avatar modeling and obtained more excellent results
than traditional methods. This paper comprehensively reviews the application of
implicit neural representation in human body modeling. First, we introduce
three implicit representations of occupancy field, SDF, and NeRF, and make a
classification of the literature investigated in this paper. Then the
application of implicit modeling methods in the body, hand, and head are
compared and analyzed respectively. Finally, we point out the shortcomings of
current work and provide available suggestions for researchers.

Comments:
- A Brief Survey

---

## H2-Mapping: Real-time Dense Mapping Using Hierarchical Hybrid  Representation

nerf{: .label .label-blue }

2023-06-05 | Chenxing Jiang, Hanwen Zhang, Peize Liu, Zehuan Yu, Hui Cheng, Boyu Zhou, Shaojie Shen | cs.RO | [PDF](http://arxiv.org/pdf/2306.03207v2){: .btn .btn-green }

**Abstract**: Constructing a high-quality dense map in real-time is essential for robotics,
AR/VR, and digital twins applications. As Neural Radiance Field (NeRF) greatly
improves the mapping performance, in this paper, we propose a NeRF-based
mapping method that enables higher-quality reconstruction and real-time
capability even on edge computers. Specifically, we propose a novel
hierarchical hybrid representation that leverages implicit multiresolution hash
encoding aided by explicit octree SDF priors, describing the scene at different
levels of detail. This representation allows for fast scene geometry
initialization and makes scene geometry easier to learn. Besides, we present a
coverage-maximizing keyframe selection strategy to address the forgetting issue
and enhance mapping quality, particularly in marginal areas. To the best of our
knowledge, our method is the first to achieve high-quality NeRF-based mapping
on edge computers of handheld devices and quadrotors in real-time. Experiments
demonstrate that our method outperforms existing NeRF-based mapping methods in
geometry accuracy, texture realism, and time consumption. The code will be
released at: https://github.com/SYSU-STAR/H2-Mapping

Comments:
- Accepted by IEEE Robotics and Automation Letters

---

## BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance  Fields

nerf{: .label .label-blue }

2023-06-05 | AKM Shahariar Azad Rabby, Chengcui Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2306.03000v2){: .btn .btn-green }

**Abstract**: Neural rendering combines ideas from classical computer graphics and machine
learning to synthesize images from real-world observations. NeRF, short for
Neural Radiance Fields, is a recent innovation that uses AI algorithms to
create 3D objects from 2D images. By leveraging an interpolation approach, NeRF
can produce new 3D reconstructed views of complicated scenes. Rather than
directly restoring the whole 3D scene geometry, NeRF generates a volumetric
representation called a ``radiance field,'' which is capable of creating color
and density for every point within the relevant 3D space. The broad appeal and
notoriety of NeRF make it imperative to examine the existing research on the
topic comprehensively. While previous surveys on 3D rendering have primarily
focused on traditional computer vision-based or deep learning-based approaches,
only a handful of them discuss the potential of NeRF. However, such surveys
have predominantly focused on NeRF's early contributions and have not explored
its full potential. NeRF is a relatively new technique continuously being
investigated for its capabilities and limitations. This survey reviews recent
advances in NeRF and categorizes them according to their architectural designs,
especially in the field of novel view synthesis.

Comments:
- 22 page, 1 figure, 5 table

---

## Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions



2023-06-05 | Shaoxu Li | cs.CV | [PDF](http://arxiv.org/pdf/2306.02903v1){: .btn .btn-green }

**Abstract**: We propose a method for synthesizing edited photo-realistic digital avatars
with text instructions. Given a short monocular RGB video and text
instructions, our method uses an image-conditioned diffusion model to edit one
head image and uses the video stylization method to accomplish the editing of
other head images. Through iterative training and update (three times or more),
our method synthesizes edited photo-realistic animatable 3D neural head avatars
with a deformable neural radiance field head synthesis method. In quantitative
and qualitative studies on various subjects, our method outperforms
state-of-the-art methods.

Comments:
- https://github.com/lsx0101/Instruct-Video2Avatar

---

## ZIGNeRF: Zero-shot 3D Scene Representation with Invertible Generative  Neural Radiance Fields

nerf{: .label .label-blue }

2023-06-05 | Kanghyeok Ko, Minhyeok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2306.02741v1){: .btn .btn-green }

**Abstract**: Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable
proficiency in synthesizing multi-view images by learning the distribution of a
set of unposed images. Despite the aptitude of existing generative NeRFs in
generating 3D-consistent high-quality random samples within data distribution,
the creation of a 3D representation of a singular input image remains a
formidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative
model that executes zero-shot Generative Adversarial Network (GAN) inversion
for the generation of multi-view images from a single out-of-domain image. The
model is underpinned by a novel inverter that maps out-of-domain images into
the latent code of the generator manifold. Notably, ZIGNeRF is capable of
disentangling the object from the background and executing 3D operations such
as 360-degree rotation or depth and horizontal translation. The efficacy of our
model is validated using multiple real-image datasets: Cats, AFHQ, CelebA,
CelebA-HQ, and CompCars.

---

## PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline  Panoramas



2023-06-02 | Zheng Chen, Yan-Pei Cao, Yuan-Chen Guo, Chen Wang, Ying Shan, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2306.01531v2){: .btn .btn-green }

**Abstract**: Achieving an immersive experience enabling users to explore virtual
environments with six degrees of freedom (6DoF) is essential for various
applications such as virtual reality (VR). Wide-baseline panoramas are commonly
used in these applications to reduce network bandwidth and storage
requirements. However, synthesizing novel views from these panoramas remains a
key challenge. Although existing neural radiance field methods can produce
photorealistic views under narrow-baseline and dense image captures, they tend
to overfit the training views when dealing with \emph{wide-baseline} panoramas
due to the difficulty in learning accurate geometry from sparse $360^{\circ}$
views. To address this problem, we propose PanoGRF, Generalizable Spherical
Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance
fields incorporating $360^{\circ}$ scene priors. Unlike generalizable radiance
fields trained on perspective images, PanoGRF avoids the information loss from
panorama-to-perspective conversion and directly aggregates geometry and
appearance features of 3D sample points from each panoramic view based on
spherical projection. Moreover, as some regions of the panorama are only
visible from one view while invisible from others under wide baseline settings,
PanoGRF incorporates $360^{\circ}$ monocular depth priors into spherical depth
estimation to improve the geometry features. Experimental results on multiple
panoramic datasets demonstrate that PanoGRF significantly outperforms
state-of-the-art generalizable view synthesis methods for wide-baseline
panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay).

Comments:
- accepted to NeurIPS2023; Project Page:
  https://thucz.github.io/PanoGRF/

---

## FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and  Relighting with Diffusion Models

nerf{: .label .label-blue }

2023-06-01 | Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2306.00783v2){: .btn .btn-green }

**Abstract**: The ability to create high-quality 3D faces from a single image has become
increasingly important with wide applications in video conferencing, AR/VR, and
advanced video editing in movie industries. In this paper, we propose Face
Diffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality
Face NeRFs from single images, complete with semantic editing and relighting
capabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly
trained 2D latent-diffusion model, allowing users to manipulate and construct
Face NeRFs in zero-shot learning without the need for explicit 3D data. With
carefully designed illumination and identity preserving loss, as well as
multi-modal pre-training, FaceDNeRF offers users unparalleled control over the
editing process enabling them to create and edit face NeRFs using just
single-view images, text prompts, and explicit target lighting. The advanced
features of FaceDNeRF have been designed to produce more impressive results
than existing 2D editing approaches that rely on 2D segmentation maps for
editable attributes. Experiments show that our FaceDNeRF achieves exceptionally
realistic results and unprecedented flexibility in editing compared with
state-of-the-art 3D face reconstruction and editing methods. Our code will be
available at https://github.com/BillyXYB/FaceDNeRF.

---

## Analyzing the Internals of Neural Radiance Fields

nerf{: .label .label-blue }

2023-06-01 | Lukas Radl, Andreas Kurz, Markus Steinberger | cs.CV | [PDF](http://arxiv.org/pdf/2306.00696v1){: .btn .btn-green }

**Abstract**: Modern Neural Radiance Fields (NeRFs) learn a mapping from position to
volumetric density via proposal network samplers. In contrast to the
coarse-to-fine sampling approach with two NeRFs, this offers significant
potential for speedups using lower network capacity as the task of mapping
spatial coordinates to volumetric density involves no view-dependent effects
and is thus much easier to learn. Given that most of the network capacity is
utilized to estimate radiance, NeRFs could store valuable density information
in their parameters or their deep features. To this end, we take one step back
and analyze large, trained ReLU-MLPs used in coarse-to-fine sampling. We find
that trained NeRFs, Mip-NeRFs and proposal network samplers map samples with
high density to local minima along a ray in activation feature space. We show
how these large MLPs can be accelerated by transforming the intermediate
activations to a weight estimate, without any modifications to the parameters
post-optimization. With our approach, we can reduce the computational
requirements of trained NeRFs by up to 50% with only a slight hit in rendering
quality and no changes to the training protocol or architecture. We evaluate
our approach on a variety of architectures and datasets, showing that our
proposition holds in various settings.

Comments:
- project page: nerfinternals.github.io

---

## AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars

nerf{: .label .label-blue }

2023-06-01 | Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt | cs.CV | [PDF](http://arxiv.org/pdf/2306.00547v2){: .btn .btn-green }

**Abstract**: Capturing and editing full head performances enables the creation of virtual
characters with various applications such as extended reality and media
production. The past few years witnessed a steep rise in the photorealism of
human head avatars. Such avatars can be controlled through different input data
modalities, including RGB, audio, depth, IMUs and others. While these data
modalities provide effective means of control, they mostly focus on editing the
head movements such as the facial expressions, head pose and/or camera
viewpoint. In this paper, we propose AvatarStudio, a text-based method for
editing the appearance of a dynamic full head avatar. Our approach builds on
existing work to capture dynamic performances of human heads using neural
radiance field (NeRF) and edits this representation with a text-to-image
diffusion model. Specifically, we introduce an optimization strategy for
incorporating multiple keyframes representing different camera viewpoints and
time stamps of a video performance into a single diffusion model. Using this
personalized diffusion model, we edit the dynamic NeRF by introducing
view-and-time-aware Score Distillation Sampling (VT-SDS) following a
model-based guidance approach. Our method edits the full head in a canonical
space, and then propagates these edits to remaining time steps via a pretrained
deformation network. We evaluate our method visually and numerically via a user
study, and results show that our method outperforms existing approaches. Our
experiments validate the design choices of our method and highlight that our
edits are genuine, personalized, as well as 3D- and time-consistent.

Comments:
- 17 pages, 17 figures. Project page:
  https://vcai.mpi-inf.mpg.de/projects/AvatarStudio/

---

## Control4D: Efficient 4D Portrait Editing with Text

gaussian splatting{: .label .label-blue }

2023-05-31 | Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2305.20082v2){: .btn .btn-green }

**Abstract**: We introduce Control4D, an innovative framework for editing dynamic 4D
portraits using text instructions. Our method addresses the prevalent
challenges in 4D editing, notably the inefficiencies of existing 4D
representations and the inconsistent editing effect caused by diffusion-based
editors. We first propose GaussianPlanes, a novel 4D representation that makes
Gaussian Splatting more structured by applying plane-based decomposition in 3D
space and time. This enhances both efficiency and robustness in 4D editing.
Furthermore, we propose to leverage a 4D generator to learn a more continuous
generation space from inconsistent edited images produced by the
diffusion-based editor, which effectively improves the consistency and quality
of 4D editing. Comprehensive evaluation demonstrates the superiority of
Control4D, including significantly reduced training time, high-quality
rendering, and spatial-temporal consistency in 4D portrait editing. The link to
our project website is https://control4darxiv.github.io.

Comments:
- The link to our project website is https://control4darxiv.github.io

---

## DaRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth  Adaptation

nerf{: .label .label-blue }

2023-05-30 | Jiuhn Song, Seonghoon Park, Honggyu An, Seokju Cho, Min-Seop Kwak, Sungjin Cho, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2305.19201v2){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) shows powerful performance in novel view
synthesis and 3D geometry reconstruction, but it suffers from critical
performance degradation when the number of known viewpoints is drastically
reduced. Existing works attempt to overcome this problem by employing external
priors, but their success is limited to certain types of scenes or datasets.
Employing monocular depth estimation (MDE) networks, pretrained on large-scale
RGB-D datasets, with powerful generalization capability would be a key to
solving this problem: however, using MDE in conjunction with NeRF comes with a
new set of challenges due to various ambiguity problems exhibited by monocular
depths. In this light, we propose a novel framework, dubbed D\"aRF, that
achieves robust NeRF reconstruction with a handful of real-world images by
combining the strengths of NeRF and monocular depth estimation through online
complementary training. Our framework imposes the MDE network's powerful
geometry prior to NeRF representation at both seen and unseen viewpoints to
enhance its robustness and coherence. In addition, we overcome the ambiguity
problems of monocular depths through patch-wise scale-shift fitting and
geometry distillation, which adapts the MDE network to produce depths aligned
accurately with NeRF geometry. Experiments show our framework achieves
state-of-the-art results both quantitatively and qualitatively, demonstrating
consistent and reliable performance in both indoor and outdoor real-world
datasets. Project page is available at https://ku-cvlab.github.io/DaRF/.

Comments:
- To appear at NeurIPS 2023. Project Page:
  https://ku-cvlab.github.io/DaRF/

---

## Template-free Articulated Neural Point Clouds for Reposable View  Synthesis

nerf{: .label .label-blue }

2023-05-30 | Lukas Uzolas, Elmar Eisemann, Petr Kellnhofer | cs.CV | [PDF](http://arxiv.org/pdf/2305.19065v2){: .btn .btn-green }

**Abstract**: Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when
synthesizing novel views of time-evolving 3D scenes. However, the common
reliance on backward deformation fields makes reanimation of the captured
object poses challenging. Moreover, the state of the art dynamic models are
often limited by low visual fidelity, long reconstruction time or specificity
to narrow application domains. In this paper, we present a novel method
utilizing a point-based representation and Linear Blend Skinning (LBS) to
jointly learn a Dynamic NeRF and an associated skeletal model from even sparse
multi-view video. Our forward-warping approach achieves state-of-the-art visual
fidelity when synthesizing novel views and poses while significantly reducing
the necessary learning time when compared to existing work. We demonstrate the
versatility of our representation on a variety of articulated objects from
common datasets and obtain reposable 3D reconstructions without the need of
object-specific skeletal templates. Code will be made available at
https://github.com/lukasuz/Articulated-Point-NeRF.

---

## HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion  Guidance

nerf{: .label .label-blue }

2023-05-30 | Junzhe Zhu, Peiye Zhuang | cs.CV | [PDF](http://arxiv.org/pdf/2305.18766v3){: .btn .btn-green }

**Abstract**: The advancements in automatic text-to-3D generation have been remarkable.
Most existing methods use pre-trained text-to-image diffusion models to
optimize 3D representations like Neural Radiance Fields (NeRFs) via
latent-space denoising score matching. Yet, these methods often result in
artifacts and inconsistencies across different views due to their suboptimal
optimization approaches and limited understanding of 3D geometry. Moreover, the
inherent constraints of NeRFs in rendering crisp geometry and stable textures
usually lead to a two-stage optimization to attain high-resolution details.
This work proposes holistic sampling and smoothing approaches to achieve
high-quality text-to-3D generation, all in a single-stage optimization. We
compute denoising scores in the text-to-image diffusion model's latent and
image spaces. Instead of randomly sampling timesteps (also referred to as noise
levels in denoising score matching), we introduce a novel timestep annealing
approach that progressively reduces the sampled timestep throughout
optimization. To generate high-quality renderings in a single-stage
optimization, we propose regularization for the variance of z-coordinates along
NeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel
smoothing technique that refines importance sampling weights coarse-to-fine,
ensuring accurate and thorough sampling in high-density regions. Extensive
experiments demonstrate the superiority of our method over previous approaches,
enabling the generation of highly detailed and view-consistent 3D assets
through a single-stage training process.

Comments:
- Project page: https://hifa-team.github.io/HiFA-site/

---

## Volume Feature Rendering for Fast Neural Radiance Field Reconstruction

nerf{: .label .label-blue }

2023-05-29 | Kang Han, Wei Xiang, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2305.17916v2){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) are able to synthesize realistic novel views
from multi-view images captured from distinct positions and perspectives. In
NeRF's rendering pipeline, neural networks are used to represent a scene
independently or transform queried learnable feature vector of a point to the
expected color or density. With the aid of geometry guides either in occupancy
grids or proposal networks, the number of neural network evaluations can be
reduced from hundreds to dozens in the standard volume rendering framework.
Instead of rendering yielded color after neural network evaluation, we propose
to render the queried feature vectors of a ray first and then transform the
rendered feature vector to the final pixel color by a neural network. This
fundamental change to the standard volume rendering framework requires only one
single neural network evaluation to render a pixel, which substantially lowers
the high computational complexity of the rendering framework attributed to a
large number of neural network evaluations. Consequently, we can use a
comparably larger neural network to achieve a better rendering quality while
maintaining the same training and rendering time costs. Our model achieves the
state-of-the-art rendering quality on both synthetic and real-world datasets
while requiring a training time of several minutes.

---

## Towards a Robust Framework for NeRF Evaluation

nerf{: .label .label-blue }

2023-05-29 | Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull | cs.CV | [PDF](http://arxiv.org/pdf/2305.18079v3){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) research has attracted significant attention
recently, with 3D modelling, virtual/augmented reality, and visual effects
driving its application. While current NeRF implementations can produce high
quality visual results, there is a conspicuous lack of reliable methods for
evaluating them. Conventional image quality assessment methods and analytical
metrics (e.g. PSNR, SSIM, LPIPS etc.) only provide approximate indicators of
performance since they generalise the ability of the entire NeRF pipeline.
Hence, in this paper, we propose a new test framework which isolates the neural
rendering network from the NeRF pipeline and then performs a parametric
evaluation by training and evaluating the NeRF on an explicit radiance field
representation. We also introduce a configurable approach for generating
representations specifically for evaluation purposes. This employs ray-casting
to transform mesh models into explicit NeRF samples, as well as to "shade"
these representations. Combining these two approaches, we demonstrate how
different "tasks" (scenes with different visual effects or learning strategies)
and types of networks (NeRFs and depth-wise implicit neural representations
(INRs)) can be evaluated within this framework. Additionally, we propose a
novel metric to measure task complexity of the framework which accounts for the
visual parameters and the distribution of the spatial data. Our approach offers
the potential to create a comparative objective evaluation framework for NeRF
methods.

Comments:
- 9 pages, 2 main experiments, 2 additional experiments

---

## Compact Real-time Radiance Fields with Neural Codebook



2023-05-29 | Lingzhi Li, Zhongshu Wang, Zhen Shen, Li Shen, Ping Tan | cs.CV | [PDF](http://arxiv.org/pdf/2305.18163v1){: .btn .btn-green }

**Abstract**: Reconstructing neural radiance fields with explicit volumetric
representations, demonstrated by Plenoxels, has shown remarkable advantages on
training and rendering efficiency, while grid-based representations typically
induce considerable overhead for storage and transmission. In this work, we
present a simple and effective framework for pursuing compact radiance fields
from the perspective of compression methodology. By exploiting intrinsic
properties exhibiting in grid models, a non-uniform compression stem is
developed to significantly reduce model complexity and a novel parameterized
module, named Neural Codebook, is introduced for better encoding high-frequency
details specific to per-scene models via a fast optimization. Our approach can
achieve over 40 $\times$ reduction on grid model storage with competitive
rendering quality. In addition, the method can achieve real-time rendering
speed with 180 fps, realizing significant advantage on storage cost compared to
real-time rendering methods.

Comments:
- Accepted by ICME 2023

---

## PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale  Scene Reconstruction

nerf{: .label .label-blue }

2023-05-26 | Fusang Wang, Arnaud Louys, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou | cs.CV | [PDF](http://arxiv.org/pdf/2305.16914v4){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images
and camera poses for Novel View Synthesis (NVS). Although NeRF can produce
photorealistic results, it often suffers from overfitting to training views,
leading to poor geometry reconstruction, especially in low-texture areas. This
limitation restricts many important applications which require accurate
geometry, such as extrapolated NVS, HD mapping and scene editing. To address
this limitation, we propose a new method to improve NeRF's 3D structure using
only RGB images and semantic maps. Our approach introduces a novel plane
regularization based on Singular Value Decomposition (SVD), that does not rely
on any geometric prior. In addition, we leverage the Structural Similarity
Index Measure (SSIM) in our loss design to properly initialize the volumetric
representation of NeRF. Quantitative and qualitative results show that our
method outperforms popular regularization approaches in accurate geometry
reconstruction for large-scale outdoor scenes and achieves SoTA rendering
quality on the KITTI-360 NVS benchmark.

Comments:
- Accepted to 3DV 2023

---

## ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image

nerf{: .label .label-blue }

2023-05-25 | Zhenzhen Weng, Zeyu Wang, Serena Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2305.16411v1){: .btn .btn-green }

**Abstract**: Recent advancements in text-to-image generation have enabled significant
progress in zero-shot 3D shape generation. This is achieved by score
distillation, a methodology that uses pre-trained text-to-image diffusion
models to optimize the parameters of a 3D neural presentation, e.g. Neural
Radiance Field (NeRF). While showing promising results, existing methods are
often not able to preserve the geometry of complex shapes, such as human
bodies. To address this challenge, we present ZeroAvatar, a method that
introduces the explicit 3D human body prior to the optimization process.
Specifically, we first estimate and refine the parameters of a parametric human
body from a single image. Then during optimization, we use the posed parametric
body as additional geometry constraint to regularize the diffusion model as
well as the underlying density field. Lastly, we propose a UV-guided texture
regularization term to further guide the completion of texture on invisible
body parts. We show that ZeroAvatar significantly enhances the robustness and
3D consistency of optimization-based image-to-3D avatar generation,
outperforming existing zero-shot image-to-3D methods.

---

## Interactive Segment Anything NeRF with Feature Imitation

nerf{: .label .label-blue }

2023-05-25 | Xiaokang Chen, Jiaxiang Tang, Diwen Wan, Jingbo Wang, Gang Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2305.16233v1){: .btn .btn-green }

**Abstract**: This paper investigates the potential of enhancing Neural Radiance Fields
(NeRF) with semantics to expand their applications. Although NeRF has been
proven useful in real-world applications like VR and digital creation, the lack
of semantics hinders interaction with objects in complex scenes. We propose to
imitate the backbone feature of off-the-shelf perception models to achieve
zero-shot semantic segmentation with NeRF. Our framework reformulates the
segmentation process by directly rendering semantic features and only applying
the decoder from perception models. This eliminates the need for expensive
backbones and benefits 3D consistency. Furthermore, we can project the learned
semantics onto extracted mesh surfaces for real-time interaction. With the
state-of-the-art Segment Anything Model (SAM), our framework accelerates
segmentation by 16 times with comparable mask quality. The experimental results
demonstrate the efficacy and computational advantages of our approach. Project
page: \url{https://me.kiui.moe/san/}.

Comments:
- Technical Report

---

## ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with  Variational Score Distillation

nerf{: .label .label-blue }

2023-05-25 | Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu | cs.LG | [PDF](http://arxiv.org/pdf/2305.16213v2){: .btn .btn-green }

**Abstract**: Score distillation sampling (SDS) has shown great promise in text-to-3D
generation by distilling pretrained large-scale text-to-image diffusion models,
but suffers from over-saturation, over-smoothing, and low-diversity problems.
In this work, we propose to model the 3D parameter as a random variable instead
of a constant as in SDS and present variational score distillation (VSD), a
principled particle-based variational framework to explain and address the
aforementioned issues in text-to-3D generation. We show that SDS is a special
case of VSD and leads to poor samples with both small and large CFG weights. In
comparison, VSD works well with various CFG weights as ancestral sampling from
diffusion models and simultaneously improves the diversity and sample quality
with a common CFG weight (i.e., $7.5$). We further present various improvements
in the design space for text-to-3D such as distillation time schedule and
density initialization, which are orthogonal to the distillation algorithm yet
not well explored. Our overall approach, dubbed ProlificDreamer, can generate
high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with
rich structure and complex effects (e.g., smoke and drops). Further,
initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and
photo-realistic. Project page and codes:
https://ml.cs.tsinghua.edu.cn/prolificdreamer/

Comments:
- NeurIPS 2023 (Spotlight)

---

## Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations  from Diffusion Models

nerf{: .label .label-blue }

2023-05-24 | Xinhang Liu, Jiaben Chen, Shiu-hong Kao, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2305.15171v3){: .btn .btn-green }

**Abstract**: We introduce Deceptive-NeRF, a novel methodology for few-shot NeRF
reconstruction, which leverages diffusion models to synthesize plausible
pseudo-observations to improve the reconstruction. This approach unfolds
through three key steps: 1) reconstructing a coarse NeRF from sparse input
data; 2) utilizing the coarse NeRF to render images and subsequently generating
pseudo-observations based on them; 3) training a refined NeRF model utilizing
input images augmented with pseudo-observations. We develop a deceptive
diffusion model that adeptly transitions RGB images and depth maps from coarse
NeRFs into photo-realistic pseudo-observations, all while preserving scene
semantics for reconstruction. Furthermore, we propose a progressive strategy
for training the Deceptive-NeRF, using the current NeRF renderings to create
pseudo-observations that enhance the next iteration's NeRF. Extensive
experiments demonstrate that our approach is capable of synthesizing
photo-realistic novel views, even for highly complex scenes with very sparse
inputs. Codes will be released.

---

## InpaintNeRF360: Text-Guided 3D Inpainting on Unbounded Neural Radiance  Fields

nerf{: .label .label-blue }

2023-05-24 | Dongqing Wang, Tong Zhang, Alaa Abboud, Sabine Süsstrunk | cs.CV | [PDF](http://arxiv.org/pdf/2305.15094v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) can generate highly realistic novel views.
However, editing 3D scenes represented by NeRF across 360-degree views,
particularly removing objects while preserving geometric and photometric
consistency, remains a challenging problem due to NeRF's implicit scene
representation. In this paper, we propose InpaintNeRF360, a unified framework
that utilizes natural language instructions as guidance for inpainting
NeRF-based 3D scenes.Our approach employs a promptable segmentation model by
generating multi-modal prompts from the encoded text for multiview
segmentation. We apply depth-space warping to enforce viewing consistency in
the segmentations, and further refine the inpainted NeRF model using perceptual
priors to ensure visual plausibility. InpaintNeRF360 is capable of
simultaneously removing multiple objects or modifying object appearance based
on text instructions while synthesizing 3D viewing-consistent and
photo-realistic inpainting. Through extensive experiments on both unbounded and
frontal-facing scenes trained through NeRF, we demonstrate the effectiveness of
our approach and showcase its potential to enhance the editability of implicit
radiance fields.

---

## OD-NeRF: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields

nerf{: .label .label-blue }

2023-05-24 | Zhiwen Yan, Chen Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2305.14831v1){: .btn .btn-green }

**Abstract**: Dynamic neural radiance fields (dynamic NeRFs) have demonstrated impressive
results in novel view synthesis on 3D dynamic scenes. However, they often
require complete video sequences for training followed by novel view synthesis,
which is similar to playing back the recording of a dynamic 3D scene. In
contrast, we propose OD-NeRF to efficiently train and render dynamic NeRFs
on-the-fly which instead is capable of streaming the dynamic scene. When
training on-the-fly, the training frames become available sequentially and the
model is trained and rendered frame-by-frame. The key challenge of efficient
on-the-fly training is how to utilize the radiance field estimated from the
previous frames effectively. To tackle this challenge, we propose: 1) a NeRF
model conditioned on the multi-view projected colors to implicitly track
correspondence between the current and previous frames, and 2) a transition and
update algorithm that leverages the occupancy grid from the last frame to
sample efficiently at the current frame. Our algorithm can achieve an
interactive speed of 6FPS training and rendering on synthetic dynamic scenes
on-the-fly, and a significant speed-up compared to the state-of-the-art on
real-world dynamic scenes.

---

## Weakly Supervised 3D Open-vocabulary Segmentation

nerf{: .label .label-blue }

2023-05-23 | Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt, Eric Xing, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2305.14093v4){: .btn .btn-green }

**Abstract**: Open-vocabulary segmentation of 3D scenes is a fundamental function of human
perception and thus a crucial objective in computer vision research. However,
this task is heavily impeded by the lack of large-scale and diverse 3D
open-vocabulary segmentation datasets for training robust and generalizable
models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation
models helps but it compromises the open-vocabulary feature as the 2D models
are mostly finetuned with close-vocabulary datasets. We tackle the challenges
in 3D open-vocabulary segmentation by exploiting pre-trained foundation models
CLIP and DINO in a weakly supervised manner. Specifically, given only the
open-vocabulary text descriptions of the objects in a scene, we distill the
open-vocabulary multimodal knowledge and object reasoning capability of CLIP
and DINO into a neural radiance field (NeRF), which effectively lifts 2D
features into view-consistent 3D segmentation. A notable aspect of our approach
is that it does not require any manual segmentation annotations for either the
foundation models or the distillation process. Extensive experiments show that
our method even outperforms fully supervised models trained with segmentation
annotations in certain scenes, suggesting that 3D open-vocabulary segmentation
can be effectively learned from 2D images and text-image pairs. Code is
available at \url{https://github.com/Kunhao-Liu/3D-OVS}.

Comments:
- Accepted to NeurIPS 2023

---

## NeRFuser: Large-Scale Scene Representation by NeRF Fusion

nerf{: .label .label-blue }

2023-05-22 | Jiading Fang, Shengjie Lin, Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Adrien Gaidon, Gregory Shakhnarovich, Matthew R. Walter | cs.CV | [PDF](http://arxiv.org/pdf/2305.13307v1){: .btn .btn-green }

**Abstract**: A practical benefit of implicit visual representations like Neural Radiance
Fields (NeRFs) is their memory efficiency: large scenes can be efficiently
stored and shared as small neural nets instead of collections of images.
However, operating on these implicit visual data structures requires extending
classical image-based vision techniques (e.g., registration, blending) from
image sets to neural fields. Towards this goal, we propose NeRFuser, a novel
architecture for NeRF registration and blending that assumes only access to
pre-generated NeRFs, and not the potentially large sets of images used to
generate them. We propose registration from re-rendering, a technique to infer
the transformation between NeRFs based on images synthesized from individual
NeRFs. For blending, we propose sample-based inverse distance weighting to
blend visual information at the ray-sample level. We evaluate NeRFuser on
public benchmarks and a self-collected object-centric indoor dataset, showing
the robustness of our method, including to views that are challenging to render
from the individual source NeRFs.

Comments:
- Code available at https://github.com/ripl/nerfuser

---

## Registering Neural Radiance Fields as 3D Density Images

nerf{: .label .label-blue }

2023-05-22 | Han Jiang, Ruoxuan Li, Haosen Sun, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2305.12843v1){: .btn .btn-green }

**Abstract**: No significant work has been done to directly merge two partially overlapping
scenes using NeRF representations. Given pre-trained NeRF models of a 3D scene
with partial overlapping, this paper aligns them with a rigid transform, by
generalizing the traditional registration pipeline, that is, key point
detection and point set registration, to operate on 3D density fields. To
describe corner points as key points in 3D, we propose to use universal
pre-trained descriptor-generating neural networks that can be trained and
tested on different scenes. We perform experiments to demonstrate that the
descriptor networks can be conveniently trained using a contrastive learning
strategy. We demonstrate that our method, as a global approach, can effectively
register NeRF models, thus making possible future large-scale NeRF construction
by registering its smaller and overlapping NeRFs captured individually.

---

## Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields

nerf{: .label .label-blue }

2023-05-19 | Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao | cs.CV | [PDF](http://arxiv.org/pdf/2305.11588v1){: .btn .btn-green }

**Abstract**: Text-driven 3D scene generation is widely applicable to video gaming, film
industry, and metaverse applications that have a large demand for 3D scenes.
However, existing text-to-3D generation methods are limited to producing 3D
objects with simple geometries and dreamlike styles that lack realism. In this
work, we present Text2NeRF, which is able to generate a wide range of 3D scenes
with complicated geometric structures and high-fidelity textures purely from a
text prompt. To this end, we adopt NeRF as the 3D representation and leverage a
pre-trained text-to-image diffusion model to constrain the 3D reconstruction of
the NeRF to reflect the scene description. Specifically, we employ the
diffusion model to infer the text-related image as the content prior and use a
monocular depth estimation method to offer the geometric prior. Both content
and geometric priors are utilized to update the NeRF model. To guarantee
textured and geometric consistency between different views, we introduce a
progressive scene inpainting and updating strategy for novel view synthesis of
the scene. Our method requires no additional training data but only a natural
language description of the scene as the input. Extensive experiments
demonstrate that our Text2NeRF outperforms existing methods in producing
photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of
natural language prompts.

Comments:
- Homepage: https://eckertzhang.github.io/Text2NeRF.github.io/

---

## MVPSNet: Fast Generalizable Multi-view Photometric Stereo

nerf{: .label .label-blue }

2023-05-18 | Dongxu Zhao, Daniel Lichy, Pierre-Nicolas Perrin, Jan-Michael Frahm, Soumyadip Sengupta | cs.CV | [PDF](http://arxiv.org/pdf/2305.11167v1){: .btn .btn-green }

**Abstract**: We propose a fast and generalizable solution to Multi-view Photometric Stereo
(MVPS), called MVPSNet. The key to our approach is a feature extraction network
that effectively combines images from the same view captured under multiple
lighting conditions to extract geometric features from shading cues for stereo
matching. We demonstrate these features, termed `Light Aggregated Feature Maps'
(LAFM), are effective for feature matching even in textureless regions, where
traditional multi-view stereo methods fail. Our method produces similar
reconstruction results to PS-NeRF, a state-of-the-art MVPS method that
optimizes a neural network per-scene, while being 411$\times$ faster (105
seconds vs. 12 hours) in inference. Additionally, we introduce a new synthetic
dataset for MVPS, sMVPS, which is shown to be effective to train a
generalizable MVPS method.

---

## ConsistentNeRF: Enhancing Neural Radiance Fields with 3D Consistency for  Sparse View Synthesis

nerf{: .label .label-blue }

2023-05-18 | Shoukang Hu, Kaichen Zhou, Kaiyu Li, Longhui Yu, Lanqing Hong, Tianyang Hu, Zhenguo Li, Gim Hee Lee, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2305.11031v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) has demonstrated remarkable 3D reconstruction
capabilities with dense view images. However, its performance significantly
deteriorates under sparse view settings. We observe that learning the 3D
consistency of pixels among different views is crucial for improving
reconstruction quality in such cases. In this paper, we propose ConsistentNeRF,
a method that leverages depth information to regularize both multi-view and
single-view 3D consistency among pixels. Specifically, ConsistentNeRF employs
depth-derived geometry information and a depth-invariant loss to concentrate on
pixels that exhibit 3D correspondence and maintain consistent depth
relationships. Extensive experiments on recent representative works reveal that
our approach can considerably enhance model performance in sparse view
conditions, achieving improvements of up to 94% in PSNR, 76% in SSIM, and 31%
in LPIPS compared to the vanilla baselines across various benchmarks, including
DTU, NeRF Synthetic, and LLFF.

Comments:
- https://github.com/skhu101/ConsistentNeRF

---

## MultiPlaneNeRF: Neural Radiance Field with Non-Trainable Representation

nerf{: .label .label-blue }

2023-05-17 | Dominik Zimny, Artur Kasymov, Adam Kania, Jacek Tabor, Maciej Zięba, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2305.10579v2){: .btn .btn-green }

**Abstract**: NeRF is a popular model that efficiently represents 3D objects from 2D
images. However, vanilla NeRF has some important limitations. NeRF must be
trained on each object separately. The training time is long since we encode
the object's shape and color in neural network weights. Moreover, NeRF does not
generalize well to unseen data. In this paper, we present MultiPlaneNeRF -- a
model that simultaneously solves the above problems. Our model works directly
on 2D images. We project 3D points on 2D images to produce non-trainable
representations. The projection step is not parametrized and a very shallow
decoder can efficiently process the representation. Furthermore, we can train
MultiPlaneNeRF on a large data set and force our implicit decoder to generalize
across many objects. Consequently, we can only replace the 2D images (without
additional training) to produce a NeRF representation of the new object. In the
experimental section, we demonstrate that MultiPlaneNeRF achieves results
comparable to state-of-the-art models for synthesizing new views and has
generalization properties. Additionally, MultiPlane decoder can be used as a
component in large generative models like GANs.

---

## OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation  with Neural Radiance Fields

nerf{: .label .label-blue }

2023-05-17 | Youtan Yin, Zhoujie Fu, Fan Yang, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2305.10503v3){: .btn .btn-green }

**Abstract**: The emergence of Neural Radiance Fields (NeRF) for novel view synthesis has
increased interest in 3D scene editing. An essential task in editing is
removing objects from a scene while ensuring visual reasonability and multiview
consistency. However, current methods face challenges such as time-consuming
object labeling, limited capability to remove specific targets, and compromised
rendering quality after removal. This paper proposes a novel object-removing
pipeline, named OR-NeRF, that can remove objects from 3D scenes with user-given
points or text prompts on a single view, achieving better performance in less
time than previous works. Our method spreads user annotations to all views
through 3D geometry and sparse correspondence, ensuring 3D consistency with
less processing burden. Then recent 2D segmentation model Segment-Anything
(SAM) is applied to predict masks, and a 2D inpainting model is used to
generate color supervision. Finally, our algorithm applies depth supervision
and perceptual loss to maintain consistency in geometry and appearance after
object removal. Experimental results demonstrate that our method achieves
better editing quality with less time than previous works, considering both
quality and quantity.

Comments:
- project site: https://ornerf.github.io/ (codes available)

---

## NerfBridge: Bringing Real-time, Online Neural Radiance Field Training to  Robotics

nerf{: .label .label-blue }

2023-05-16 | Javier Yu, Jun En Low, Keiko Nagami, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2305.09761v1){: .btn .btn-green }

**Abstract**: This work was presented at the IEEE International Conference on Robotics and
Automation 2023 Workshop on Unconventional Spatial Representations.
  Neural radiance fields (NeRFs) are a class of implicit scene representations
that model 3D environments from color images. NeRFs are expressive, and can
model the complex and multi-scale geometry of real world environments, which
potentially makes them a powerful tool for robotics applications. Modern NeRF
training libraries can generate a photo-realistic NeRF from a static data set
in just a few seconds, but are designed for offline use and require a slow pose
optimization pre-computation step.
  In this work we propose NerfBridge, an open-source bridge between the Robot
Operating System (ROS) and the popular Nerfstudio library for real-time, online
training of NeRFs from a stream of images. NerfBridge enables rapid development
of research on applications of NeRFs in robotics by providing an extensible
interface to the efficient training pipelines and model libraries provided by
Nerfstudio. As an example use case we outline a hardware setup that can be used
NerfBridge to train a NeRF from images captured by a camera mounted to a
quadrotor in both indoor and outdoor environments.
  For accompanying video https://youtu.be/EH0SLn-RcDg and code
https://github.com/javieryu/nerf_bridge.

---

## Curvature-Aware Training for Coordinate Networks



2023-05-15 | Hemanth Saratchandran, Shin-Fang Chng, Sameera Ramasinghe, Lachlan MacDonald, Simon Lucey | cs.CV | [PDF](http://arxiv.org/pdf/2305.08552v1){: .btn .btn-green }

**Abstract**: Coordinate networks are widely used in computer vision due to their ability
to represent signals as compressed, continuous entities. However, training
these networks with first-order optimizers can be slow, hindering their use in
real-time applications. Recent works have opted for shallow voxel-based
representations to achieve faster training, but this sacrifices memory
efficiency. This work proposes a solution that leverages second-order
optimization methods to significantly reduce training times for coordinate
networks while maintaining their compressibility. Experiments demonstrate the
effectiveness of this approach on various signal modalities, such as audio,
images, videos, shape reconstruction, and neural radiance fields.

---

## MV-Map: Offboard HD-Map Generation with Multi-view Consistency

nerf{: .label .label-blue }

2023-05-15 | Ziyang Xie, Ziqi Pang, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2305.08851v3){: .btn .btn-green }

**Abstract**: While bird's-eye-view (BEV) perception models can be useful for building
high-definition maps (HD-Maps) with less human labor, their results are often
unreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps
from different viewpoints. This is because BEV perception is typically set up
in an 'onboard' manner, which restricts the computation and consequently
prevents algorithms from reasoning multiple views simultaneously. This paper
overcomes these limitations and advocates a more practical 'offboard' HD-Map
generation setup that removes the computation constraints, based on the fact
that HD-Maps are commonly reusable infrastructures built offline in data
centers. To this end, we propose a novel offboard pipeline called MV-Map that
capitalizes multi-view consistency and can handle an arbitrary number of frames
with the key design of a 'region-centric' framework. In MV-Map, the target
HD-Maps are created by aggregating all the frames of onboard predictions,
weighted by the confidence scores assigned by an 'uncertainty network'. To
further enhance multi-view consistency, we augment the uncertainty network with
the global 3D structure optimized by a voxelized neural radiance field
(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map
significantly improves the quality of HD-Maps, further highlighting the
importance of offboard methods for HD-Map generation.

Comments:
- ICCV 2023

---

## BundleRecon: Ray Bundle-Based 3D Neural Reconstruction



2023-05-12 | Weikun Zhang, Jianke Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2305.07342v1){: .btn .btn-green }

**Abstract**: With the growing popularity of neural rendering, there has been an increasing
number of neural implicit multi-view reconstruction methods. While many models
have been enhanced in terms of positional encoding, sampling, rendering, and
other aspects to improve the reconstruction quality, current methods do not
fully leverage the information among neighboring pixels during the
reconstruction process. To address this issue, we propose an enhanced model
called BundleRecon. In the existing approaches, sampling is performed by a
single ray that corresponds to a single pixel. In contrast, our model samples a
patch of pixels using a bundle of rays, which incorporates information from
neighboring pixels. Furthermore, we design bundle-based constraints to further
improve the reconstruction quality. Experimental results demonstrate that
BundleRecon is compatible with the existing neural implicit multi-view
reconstruction methods and can improve their reconstruction quality.

Comments:
- CVPR 2023 workshop XRNeRF: Advances in NeRF for the Metaverse

---

## SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input  Views



2023-05-11 | Weihao Cheng, Yan-Pei Cao, Ying Shan | cs.CV | [PDF](http://arxiv.org/pdf/2305.07024v1){: .btn .btn-green }

**Abstract**: We study to generate novel views of indoor scenes given sparse input views.
The challenge is to achieve both photorealism and view consistency. We present
SparseGNV: a learning framework that incorporates 3D structures and image
generative models to generate novel views with three modules. The first module
builds a neural point cloud as underlying geometry, providing contextual
information and guidance for the target novel view. The second module utilizes
a transformer-based network to map the scene context and the guidance into a
shared latent space and autoregressively decodes the target view in the form of
discrete image tokens. The third module reconstructs the tokens into the image
of the target view. SparseGNV is trained across a large indoor scene dataset to
learn generalizable priors. Once trained, it can efficiently generate novel
views of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNV
on both real-world and synthetic indoor scenes and demonstrate that it
outperforms state-of-the-art methods based on either neural radiance fields or
conditional image generation.

Comments:
- 10 pages, 6 figures

---

## HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion



2023-05-10 | Mustafa Işık, Martin Rünz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2305.06356v2){: .btn .btn-green }

**Abstract**: Representing human performance at high-fidelity is an essential building
block in diverse applications, such as film production, computer games or
videoconferencing. To close the gap to production-level quality, we introduce
HumanRF, a 4D dynamic neural scene representation that captures full-body
appearance in motion from multi-view video input, and enables playback from
novel, unseen viewpoints. Our novel representation acts as a dynamic video
encoding that captures fine details at high compression rates by factorizing
space-time into a temporal matrix-vector decomposition. This allows us to
obtain temporally coherent reconstructions of human actors for long sequences,
while representing high-resolution details even in the context of challenging
motion. While most research focuses on synthesizing at resolutions of 4MP or
lower, we address the challenge of operating at 12MP. To this end, we introduce
ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160
cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. We
demonstrate challenges that emerge from using such high-resolution data and
show that our newly introduced HumanRF effectively leverages this data, making
a significant step towards production-level quality novel view synthesis.

Comments:
- Project webpage: https://synthesiaresearch.github.io/humanrf Dataset
  webpage: https://www.actors-hq.com/ Video:
  https://www.youtube.com/watch?v=OTnhiLLE7io Code:
  https://github.com/synthesiaresearch/humanrf

---

## Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era

nerf{: .label .label-blue }

2023-05-10 | Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, Choong Seon Hong | cs.CV | [PDF](http://arxiv.org/pdf/2305.06131v2){: .btn .btn-green }

**Abstract**: Generative AI (AIGC, a.k.a. AI generated content) has made remarkable
progress in the past few years, among which text-guided content generation is
the most practical one since it enables the interaction between human
instruction and AIGC. Due to the development in text-to-image as well 3D
modeling technologies (like NeRF), text-to-3D has become a newly emerging yet
highly active research field. Our work conducts the first yet comprehensive
survey on text-to-3D to help readers interested in this direction quickly catch
up with its fast development. First, we introduce 3D data representations,
including both Euclidean data and non-Euclidean data. On top of that, we
introduce various foundation technologies as well as summarize how recent works
combine those foundation technologies to realize satisfactory text-to-3D.
Moreover, we summarize how text-to-3D technology is used in various
applications, including avatar generation, texture generation, shape
transformation, and scene generation.

---

## NeRF2: Neural Radio-Frequency Radiance Fields

nerf{: .label .label-blue }

2023-05-10 | Xiaopeng Zhao, Zhenlin An, Qingrui Pan, Lei Yang | cs.NI | [PDF](http://arxiv.org/pdf/2305.06118v2){: .btn .btn-green }

**Abstract**: Although Maxwell discovered the physical laws of electromagnetic waves 160
years ago, how to precisely model the propagation of an RF signal in an
electrically large and complex environment remains a long-standing problem. The
difficulty is in the complex interactions between the RF signal and the
obstacles (e.g., reflection, diffraction, etc.). Inspired by the great success
of using a neural network to describe the optical field in computer vision, we
propose a neural radio-frequency radiance field, NeRF$^\textbf{2}$, which
represents a continuous volumetric scene function that makes sense of an RF
signal's propagation. Particularly, after training with a few signal
measurements, NeRF$^\textbf{2}$ can tell how/what signal is received at any
position when it knows the position of a transmitter. As a physical-layer
neural network, NeRF$^\textbf{2}$ can take advantage of the learned statistic
model plus the physical model of ray tracing to generate a synthetic dataset
that meets the training demands of application-layer artificial neural networks
(ANNs). Thus, we can boost the performance of ANNs by the proposed
turbo-learning, which mixes the true and synthetic datasets to intensify the
training. Our experiment results show that turbo-learning can enhance
performance with an approximate 50% increase. We also demonstrate the power of
NeRF$^\textbf{2}$ in the field of indoor localization and 5G MIMO.

---

## Instant-NeRF: Instant On-Device Neural Radiance Field Training via  Algorithm-Accelerator Co-Designed Near-Memory Processing

nerf{: .label .label-blue }

2023-05-09 | Yang Zhao, Shang Wu, Jingqun Zhang, Sixu Li, Chaojian Li, Yingyan Lin | cs.CV | [PDF](http://arxiv.org/pdf/2305.05766v1){: .btn .btn-green }

**Abstract**: Instant on-device Neural Radiance Fields (NeRFs) are in growing demand for
unleashing the promise of immersive AR/VR experiences, but are still limited by
their prohibitive training time. Our profiling analysis reveals a memory-bound
inefficiency in NeRF training. To tackle this inefficiency, near-memory
processing (NMP) promises to be an effective solution, but also faces
challenges due to the unique workloads of NeRFs, including the random hash
table lookup, random point processing sequence, and heterogeneous bottleneck
steps. Therefore, we propose the first NMP framework, Instant-NeRF, dedicated
to enabling instant on-device NeRF training. Experiments on eight datasets
consistently validate the effectiveness of Instant-NeRF.

Comments:
- Accepted by DAC 2023

---

## PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces

nerf{: .label .label-blue }

2023-05-09 | Yiqun Wang, Ivan Skorokhodov, Peter Wonka | cs.CV | [PDF](http://arxiv.org/pdf/2305.05594v1){: .btn .btn-green }

**Abstract**: A signed distance function (SDF) parametrized by an MLP is a common
ingredient of neural surface reconstruction. We build on the successful recent
method NeuS to extend it by three new components. The first component is to
borrow the tri-plane representation from EG3D and represent signed distance
fields as a mixture of tri-planes and MLPs instead of representing it with MLPs
only. Using tri-planes leads to a more expressive data structure but will also
introduce noise in the reconstructed surface. The second component is to use a
new type of positional encoding with learnable weights to combat noise in the
reconstruction process. We divide the features in the tri-plane into multiple
frequency scales and modulate them with sin and cos functions of different
frequencies. The third component is to use learnable convolution operations on
the tri-plane features using self-attention convolution to produce features
with different frequency bands. The experiments show that PET-NeuS achieves
high-fidelity surface reconstruction on standard datasets. Following previous
work and using the Chamfer metric as the most important way to measure surface
reconstruction quality, we are able to improve upon the NeuS baseline by 57% on
Nerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to
0.84). The qualitative evaluation reveals how our method can better control the
interference of high-frequency noise. Code available at
\url{https://github.com/yiqun-wang/PET-NeuS}.

Comments:
- CVPR 2023; 20 Pages; Project page:
  \url{https://github.com/yiqun-wang/PET-NeuS}

---

## AvatarReX: Real-time Expressive Full-body Avatars

nerf{: .label .label-blue }

2023-05-08 | Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2305.04789v1){: .btn .btn-green }

**Abstract**: We present AvatarReX, a new method for learning NeRF-based full-body avatars
from video data. The learnt avatar not only provides expressive control of the
body, hands and the face together, but also supports real-time animation and
rendering. To this end, we propose a compositional avatar representation, where
the body, hands and the face are separately modeled in a way that the
structural prior from parametric mesh templates is properly utilized without
compromising representation flexibility. Furthermore, we disentangle the
geometry and appearance for each part. With these technical designs, we propose
a dedicated deferred rendering pipeline, which can be executed in real-time
framerate to synthesize high-quality free-view images. The disentanglement of
geometry and appearance also allows us to design a two-pass training strategy
that combines volume rendering and surface rendering for network training. In
this way, patch-level supervision can be applied to force the network to learn
sharp appearance details on the basis of geometry estimation. Overall, our
method enables automatic construction of expressive full-body avatars with
real-time rendering capability, and can generate photo-realistic images with
dynamic details for novel body motions and facial expressions.

Comments:
- To appear in SIGGRAPH 2023 Journal Track. Project page at
  https://liuyebin.com/AvatarRex/

---

## NerfAcc: Efficient Sampling Accelerates NeRFs

nerf{: .label .label-blue }

2023-05-08 | Ruilong Li, Hang Gao, Matthew Tancik, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2305.04966v2){: .btn .btn-green }

**Abstract**: Optimizing and rendering Neural Radiance Fields is computationally expensive
due to the vast number of samples required by volume rendering. Recent works
have included alternative sampling approaches to help accelerate their methods,
however, they are often not the focus of the work. In this paper, we
investigate and compare multiple sampling approaches and demonstrate that
improved sampling is generally applicable across NeRF variants under an unified
concept of transmittance estimator. To facilitate future experiments, we
develop NerfAcc, a Python toolbox that provides flexible APIs for incorporating
advanced sampling methods into NeRF related methods. We demonstrate its
flexibility by showing that it can reduce the training time of several recent
NeRF methods by 1.5x to 20x with minimal modifications to the existing
codebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be
implemented in native PyTorch using NerfAcc.

Comments:
- Website: https://www.nerfacc.com

---

## HashCC: Lightweight Method to Improve the Quality of the Camera-less  NeRF Scene Generation

nerf{: .label .label-blue }

2023-05-07 | Jan Olszewski | cs.CV | [PDF](http://arxiv.org/pdf/2305.04296v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields has become a prominent method of scene generation via
view synthesis. A critical requirement for the original algorithm to learn
meaningful scene representation is camera pose information for each image in a
data set. Current approaches try to circumnavigate this assumption with
moderate success, by learning approximate camera positions alongside learning
neural representations of a scene. This requires complicated camera models,
causing a long and complicated training process, or results in a lack of
texture and sharp details in rendered scenes. In this work we introduce Hash
Color Correction (HashCC) -- a lightweight method for improving Neural Radiance
Fields rendered image quality, applicable also in situations where camera
positions for a given set of images are unknown.

---

## Multi-Space Neural Radiance Fields

nerf{: .label .label-blue }

2023-05-07 | Ze-Xin Yin, Jiaxiong Qiu, Ming-Ming Cheng, Bo Ren | cs.CV | [PDF](http://arxiv.org/pdf/2305.04268v1){: .btn .btn-green }

**Abstract**: Existing Neural Radiance Fields (NeRF) methods suffer from the existence of
reflective objects, often resulting in blurry or distorted rendering. Instead
of calculating a single radiance field, we propose a multi-space neural
radiance field (MS-NeRF) that represents the scene using a group of feature
fields in parallel sub-spaces, which leads to a better understanding of the
neural network toward the existence of reflective and refractive objects. Our
multi-space scheme works as an enhancement to existing NeRF methods, with only
small computational overheads needed for training and inferring the extra-space
outputs. We demonstrate the superiority and compatibility of our approach using
three representative NeRF-based models, i.e., NeRF, Mip-NeRF, and Mip-NeRF 360.
Comparisons are performed on a novelly constructed dataset consisting of 25
synthetic scenes and 7 real captured scenes with complex reflection and
refraction, all having 360-degree viewpoints. Extensive experiments show that
our approach significantly outperforms the existing single-space NeRF methods
for rendering high-quality scenes concerned with complex light paths through
mirror-like objects. Our code and dataset will be publicly available at
https://zx-yin.github.io/msnerf.

Comments:
- CVPR 2023, 10 pages, 12 figures

---

## General Neural Gauge Fields



2023-05-05 | Fangneng Zhan, Lingjie Liu, Adam Kortylewski, Christian Theobalt | cs.CV | [PDF](http://arxiv.org/pdf/2305.03462v2){: .btn .btn-green }

**Abstract**: The recent advance of neural fields, such as neural radiance fields, has
significantly pushed the boundary of scene representation learning. Aiming to
boost the computation efficiency and rendering quality of 3D scenes, a popular
line of research maps the 3D coordinate system to another measuring system,
e.g., 2D manifolds and hash tables, for modeling neural fields. The conversion
of coordinate systems can be typically dubbed as \emph{gauge transformation},
which is usually a pre-defined mapping function, e.g., orthogonal projection or
spatial hash function. This begs a question: can we directly learn a desired
gauge transformation along with the neural field in an end-to-end manner? In
this work, we extend this problem to a general paradigm with a taxonomy of
discrete \& continuous cases, and develop a learning framework to jointly
optimize gauge transformations and neural fields. To counter the problem that
the learning of gauge transformations can collapse easily, we derive a general
regularization mechanism from the principle of information conservation during
the gauge transformation. To circumvent the high computation cost in gauge
learning with regularization, we directly derive an information-invariant gauge
transformation which allows to preserve scene information inherently and yield
superior performance. Project: https://fnzhan.com/Neural-Gauge-Fields

Comments:
- ICLR 2023

---

## NeRF-QA: Neural Radiance Fields Quality Assessment Database

nerf{: .label .label-blue }

2023-05-04 | Pedro Martin, António Rodrigues, João Ascenso, Maria Paula Queluz | cs.MM | [PDF](http://arxiv.org/pdf/2305.03176v1){: .btn .btn-green }

**Abstract**: This short paper proposes a new database - NeRF-QA - containing 48 videos
synthesized with seven NeRF based methods, along with their perceived quality
scores, resulting from subjective assessment tests; for the videos selection,
both real and synthetic, 360 degrees scenes were considered. This database will
allow to evaluate the suitability, to NeRF based synthesized views, of existing
objective quality metrics and also the development of new quality metrics,
specific for this case.

---

## NeuralEditor: Editing Neural Radiance Fields via Manipulating Point  Clouds

nerf{: .label .label-blue }

2023-05-04 | Jun-Kun Chen, Jipeng Lyu, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2305.03049v1){: .btn .btn-green }

**Abstract**: This paper proposes NeuralEditor that enables neural radiance fields (NeRFs)
natively editable for general shape editing tasks. Despite their impressive
results on novel-view synthesis, it remains a fundamental challenge for NeRFs
to edit the shape of the scene. Our key insight is to exploit the explicit
point cloud representation as the underlying structure to construct NeRFs,
inspired by the intuitive interpretation of NeRF rendering as a process that
projects or "plots" the associated 3D point cloud to a 2D image plane. To this
end, NeuralEditor introduces a novel rendering scheme based on deterministic
integration within K-D tree-guided density-adaptive voxels, which produces both
high-quality rendering results and precise point clouds through optimization.
NeuralEditor then performs shape editing via mapping associated points between
point clouds. Extensive evaluation shows that NeuralEditor achieves
state-of-the-art performance in both shape deformation and scene morphing
tasks. Notably, NeuralEditor supports both zero-shot inference and further
fine-tuning over the edited scene. Our code, benchmark, and demo video are
available at https://immortalco.github.io/NeuralEditor.

Comments:
- CVPR 2023

---

## Single-Shot Implicit Morphable Faces with Consistent Texture  Parameterization



2023-05-04 | Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis | cs.CV | [PDF](http://arxiv.org/pdf/2305.03043v1){: .btn .btn-green }

**Abstract**: There is a growing demand for the accessible creation of high-quality 3D
avatars that are animatable and customizable. Although 3D morphable models
provide intuitive control for editing and animation, and robustness for
single-view face reconstruction, they cannot easily capture geometric and
appearance details. Methods based on neural implicit representations, such as
signed distance functions (SDF) or neural radiance fields, approach
photo-realism, but are difficult to animate and do not generalize well to
unseen data. To tackle this problem, we propose a novel method for constructing
implicit 3D morphable face models that are both generalizable and intuitive for
editing. Trained from a collection of high-quality 3D scans, our face model is
parameterized by geometry, expression, and texture latent codes with a learned
SDF and explicit UV texture parameterization. Once trained, we can reconstruct
an avatar from a single in-the-wild image by leveraging the learned prior to
project the image into the latent space of our model. Our implicit morphable
face models can be used to render an avatar from novel views, animate facial
expressions by modifying expression codes, and edit textures by directly
painting on the learned UV-texture maps. We demonstrate quantitatively and
qualitatively that our method improves upon photo-realism, geometry, and
expression accuracy compared to state-of-the-art methods.

Comments:
- SIGGRAPH 2023, Project Page:
  https://research.nvidia.com/labs/toronto-ai/ssif

---

## NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads



2023-05-04 | Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2305.03027v1){: .btn .btn-green }

**Abstract**: We focus on reconstructing high-fidelity radiance fields of human heads,
capturing their animations over time, and synthesizing re-renderings from novel
viewpoints at arbitrary time steps. To this end, we propose a new multi-view
capture setup composed of 16 calibrated machine vision cameras that record
time-synchronized images at 7.1 MP resolution and 73 frames per second. With
our setup, we collect a new dataset of over 4700 high-resolution,
high-framerate sequences of more than 220 human heads, from which we introduce
a new human head reconstruction benchmark. The recorded sequences cover a wide
range of facial dynamics, including head motions, natural expressions,
emotions, and spoken language. In order to reconstruct high-fidelity human
heads, we propose Dynamic Neural Radiance Fields using Hash Ensembles
(NeRSemble). We represent scene dynamics by combining a deformation field and
an ensemble of 3D multi-resolution hash encodings. The deformation field allows
for precise modeling of simple scene movements, while the ensemble of hash
encodings helps to represent complex dynamics. As a result, we obtain radiance
field representations of human heads that capture motion over time and
facilitate re-rendering of arbitrary novel viewpoints. In a series of
experiments, we explore the design choices of our method and demonstrate that
our approach outperforms state-of-the-art dynamic radiance field approaches by
a significant margin.

Comments:
- Siggraph 2023, Project Page:
  https://tobias-kirschstein.github.io/nersemble/ , Video:
  https://youtu.be/a-OAWqBzldU

---

## Floaters No More: Radiance Field Gradient Scaling for Improved  Near-Camera Training

nerf{: .label .label-blue }

2023-05-04 | Julien Philip, Valentin Deschaintre | cs.CV | [PDF](http://arxiv.org/pdf/2305.02756v2){: .btn .btn-green }

**Abstract**: NeRF acquisition typically requires careful choice of near planes for the
different cameras or suffers from background collapse, creating floating
artifacts on the edges of the captured scene. The key insight of this work is
that background collapse is caused by a higher density of samples in regions
near cameras. As a result of this sampling imbalance, near-camera volumes
receive significantly more gradients, leading to incorrect density buildup. We
propose a gradient scaling approach to counter-balance this sampling imbalance,
removing the need for near planes, while preventing background collapse. Our
method can be implemented in a few lines, does not induce any significant
overhead, and is compatible with most NeRF implementations.

Comments:
- EGSR 2023

---

## Semantic-aware Generation of Multi-view Portrait Drawings

nerf{: .label .label-blue }

2023-05-04 | Biao Ma, Fei Gao, Chang Jiang, Nannan Wang, Gang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2305.02618v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) based methods have shown amazing performance in
synthesizing 3D-consistent photographic images, but fail to generate multi-view
portrait drawings. The key is that the basic assumption of these methods -- a
surface point is consistent when rendered from different views -- doesn't hold
for drawings. In a portrait drawing, the appearance of a facial point may
changes when viewed from different angles. Besides, portrait drawings usually
present little 3D information and suffer from insufficient training data. To
combat this challenge, in this paper, we propose a Semantic-Aware GEnerator
(SAGE) for synthesizing multi-view portrait drawings. Our motivation is that
facial semantic labels are view-consistent and correlate with drawing
techniques. We therefore propose to collaboratively synthesize multi-view
semantic maps and the corresponding portrait drawings. To facilitate training,
we design a semantic-aware domain translator, which generates portrait drawings
based on features of photographic faces. In addition, use data augmentation via
synthesis to mitigate collapsed results. We apply SAGE to synthesize multi-view
portrait drawings in diverse artistic styles. Experimental results show that
SAGE achieves significantly superior or highly competitive performance,
compared to existing 3D-aware image synthesis methods. The codes are available
at https://github.com/AiArt-HDU/SAGE.

Comments:
- Accepted by IJCAI 2023

---

## Real-Time Radiance Fields for Single-Image Portrait View Synthesis



2023-05-03 | Alex Trevithick, Matthew Chan, Michael Stengel, Eric R. Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano | cs.CV | [PDF](http://arxiv.org/pdf/2305.02310v1){: .btn .btn-green }

**Abstract**: We present a one-shot method to infer and render a photorealistic 3D
representation from a single unposed image (e.g., face portrait) in real-time.
Given a single RGB input, our image encoder directly predicts a canonical
triplane representation of a neural radiance field for 3D-aware novel view
synthesis via volume rendering. Our method is fast (24 fps) on consumer
hardware, and produces higher quality results than strong GAN-inversion
baselines that require test-time optimization. To train our triplane encoder
pipeline, we use only synthetic data, showing how to distill the knowledge from
a pretrained 3D GAN into a feedforward encoder. Technical contributions include
a Vision Transformer-based triplane encoder, a camera data augmentation
strategy, and a well-designed loss function for synthetic data training. We
benchmark against the state-of-the-art methods, demonstrating significant
improvements in robustness and image quality in challenging real-world
settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ),
but our algorithm can also be applied in the future to other categories with a
3D-aware image generator.

Comments:
- Project page: https://research.nvidia.com/labs/nxp/lp3d/

---

## Shap-E: Generating Conditional 3D Implicit Functions



2023-05-03 | Heewoo Jun, Alex Nichol | cs.CV | [PDF](http://arxiv.org/pdf/2305.02463v1){: .btn .btn-green }

**Abstract**: We present Shap-E, a conditional generative model for 3D assets. Unlike
recent work on 3D generative models which produce a single output
representation, Shap-E directly generates the parameters of implicit functions
that can be rendered as both textured meshes and neural radiance fields. We
train Shap-E in two stages: first, we train an encoder that deterministically
maps 3D assets into the parameters of an implicit function; second, we train a
conditional diffusion model on outputs of the encoder. When trained on a large
dataset of paired 3D and text data, our resulting models are capable of
generating complex and diverse 3D assets in a matter of seconds. When compared
to Point-E, an explicit generative model over point clouds, Shap-E converges
faster and reaches comparable or better sample quality despite modeling a
higher-dimensional, multi-representation output space. We release model
weights, inference code, and samples at https://github.com/openai/shap-e.

Comments:
- 23 pages, 13 figures

---

## LatentAvatar: Learning Latent Expression Code for Expressive Neural Head  Avatar

nerf{: .label .label-blue }

2023-05-02 | Yuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen Zhao, Han Huang, Guojun Qi, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2305.01190v2){: .btn .btn-green }

**Abstract**: Existing approaches to animatable NeRF-based head avatars are either built
upon face templates or use the expression coefficients of templates as the
driving signal. Despite the promising progress, their performances are heavily
bound by the expression power and the tracking accuracy of the templates. In
this work, we present LatentAvatar, an expressive neural head avatar driven by
latent expression codes. Such latent expression codes are learned in an
end-to-end and self-supervised manner without templates, enabling our method to
get rid of expression and tracking issues. To achieve this, we leverage a
latent head NeRF to learn the person-specific latent expression codes from a
monocular portrait video, and further design a Y-shaped network to learn the
shared latent expression codes of different subjects for cross-identity
reenactment. By optimizing the photometric reconstruction objectives in NeRF,
the latent expression codes are learned to be 3D-aware while faithfully
capturing the high-frequency detailed expressions. Moreover, by learning a
mapping between the latent expression code learned in shared and
person-specific settings, LatentAvatar is able to perform expressive
reenactment between different subjects. Experimental results show that our
LatentAvatar is able to capture challenging expressions and the subtle movement
of teeth and even eyeballs, which outperforms previous state-of-the-art
solutions in both quantitative and qualitative comparisons. Project page:
https://www.liuyebin.com/latentavatar.

Comments:
- Accepted by SIGGRAPH 2023

---

## Federated Neural Radiance Fields

nerf{: .label .label-blue }

2023-05-02 | Lachlan Holden, Feras Dayoub, David Harvey, Tat-Jun Chin | cs.CV | [PDF](http://arxiv.org/pdf/2305.01163v1){: .btn .btn-green }

**Abstract**: The ability of neural radiance fields or NeRFs to conduct accurate 3D
modelling has motivated application of the technique to scene representation.
Previous approaches have mainly followed a centralised learning paradigm, which
assumes that all training images are available on one compute node for
training. In this paper, we consider training NeRFs in a federated manner,
whereby multiple compute nodes, each having acquired a distinct set of
observations of the overall scene, learn a common NeRF in parallel. This
supports the scenario of cooperatively modelling a scene using multiple agents.
Our contribution is the first federated learning algorithm for NeRF, which
splits the training effort across multiple compute nodes and obviates the need
to pool the images at a central node. A technique based on low-rank
decomposition of NeRF layers is introduced to reduce bandwidth consumption to
transmit the model parameters for aggregation. Transferring compressed models
instead of the raw data also contributes to the privacy of the data collecting
agents.

Comments:
- 10 pages, 7 figures

---

## Neural LiDAR Fields for Novel View Synthesis

nerf{: .label .label-blue }

2023-05-02 | Shengyu Huang, Zan Gojcic, Zian Wang, Francis Williams, Yoni Kasten, Sanja Fidler, Konrad Schindler, Or Litany | cs.CV | [PDF](http://arxiv.org/pdf/2305.01643v2){: .btn .btn-green }

**Abstract**: We present Neural Fields for LiDAR (NFL), a method to optimise a neural field
scene representation from LiDAR measurements, with the goal of synthesizing
realistic LiDAR scans from novel viewpoints. NFL combines the rendering power
of neural fields with a detailed, physically motivated model of the LiDAR
sensing process, thus enabling it to accurately reproduce key sensor behaviors
like beam divergence, secondary returns, and ray dropping. We evaluate NFL on
synthetic and real LiDAR scans and show that it outperforms explicit
reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR
novel view synthesis task. Moreover, we show that the improved realism of the
synthesized views narrows the domain gap to real scans and translates to better
registration and semantic segmentation performance.

Comments:
- ICCV 2023 - camera ready. Project page:
  https://research.nvidia.com/labs/toronto-ai/nfl/

---

## GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking  Face Generation

nerf{: .label .label-blue }

2023-05-01 | Zhenhui Ye, Jinzheng He, Ziyue Jiang, Rongjie Huang, Jiawei Huang, Jinglin Liu, Yi Ren, Xiang Yin, Zejun Ma, Zhou Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2305.00787v1){: .btn .btn-green }

**Abstract**: Generating talking person portraits with arbitrary speech audio is a crucial
problem in the field of digital human and metaverse. A modern talking face
generation method is expected to achieve the goals of generalized audio-lip
synchronization, good video quality, and high system efficiency. Recently,
neural radiance field (NeRF) has become a popular rendering technique in this
field since it could achieve high-fidelity and 3D-consistent talking face
generation with a few-minute-long training video. However, there still exist
several challenges for NeRF-based methods: 1) as for the lip synchronization,
it is hard to generate a long facial motion sequence of high temporal
consistency and audio-lip accuracy; 2) as for the video quality, due to the
limited data used to train the renderer, it is vulnerable to out-of-domain
input condition and produce bad rendering results occasionally; 3) as for the
system efficiency, the slow training and inference speed of the vanilla NeRF
severely obstruct its usage in real-world applications. In this paper, we
propose GeneFace++ to handle these challenges by 1) utilizing the pitch contour
as an auxiliary feature and introducing a temporal loss in the facial motion
prediction process; 2) proposing a landmark locally linear embedding method to
regulate the outliers in the predicted motion sequence to avoid robustness
issues; 3) designing a computationally efficient NeRF-based motion-to-video
renderer to achieves fast training and real-time inference. With these
settings, GeneFace++ becomes the first NeRF-based method that achieves stable
and real-time talking face generation with generalized audio-lip
synchronization. Extensive experiments show that our method outperforms
state-of-the-art baselines in terms of subjective and objective evaluation.
Video samples are available at https://genefaceplusplus.github.io .

Comments:
- 18 Pages, 7 figures

---

## Unsupervised Object-Centric Voxelization for Dynamic Scene Understanding

nerf{: .label .label-blue }

2023-04-30 | Siyu Gao, Yanpeng Zhao, Yunbo Wang, Xiaokang Yang | cs.CV | [PDF](http://arxiv.org/pdf/2305.00393v3){: .btn .btn-green }

**Abstract**: Understanding the compositional dynamics of multiple objects in unsupervised
visual environments is challenging, and existing object-centric representation
learning methods often ignore 3D consistency in scene decomposition. We propose
DynaVol, an inverse graphics approach that learns object-centric volumetric
representations in a neural rendering framework. DynaVol maintains time-varying
3D voxel grids that explicitly represent the probability of each spatial
location belonging to different objects, and decouple temporal dynamics and
spatial information by learning a canonical-space deformation field. To
optimize the volumetric features, we embed them into a fully differentiable
neural network, binding them to object-centric global features and then driving
a compositional NeRF for scene reconstruction. DynaVol outperforms existing
methods in novel view synthesis and unsupervised scene decomposition and allows
for the editing of dynamic scenes, such as adding, deleting, replacing objects,
and modifying their trajectories.

---

## Neural Radiance Fields (NeRFs): A Review and Some Recent Developments

nerf{: .label .label-blue }

2023-04-30 | Mohamed Debbagh | cs.CV | [PDF](http://arxiv.org/pdf/2305.00375v1){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) is a framework that represents a 3D scene in the
weights of a fully connected neural network, known as the Multi-Layer
Perception(MLP). The method was introduced for the task of novel view synthesis
and is able to achieve state-of-the-art photorealistic image renderings from a
given continuous viewpoint. NeRFs have become a popular field of research as
recent developments have been made that expand the performance and capabilities
of the base framework. Recent developments include methods that require less
images to train the model for view synthesis as well as methods that are able
to generate views from unconstrained and dynamic scene representations.

Comments:
- volume rendering, view synthesis, scene representation, deep learning

---

## ViP-NeRF: Visibility Prior for Sparse Input Neural Radiance Fields

nerf{: .label .label-blue }

2023-04-28 | Nagabhushan Somraj, Rajiv Soundararajan | cs.CV | [PDF](http://arxiv.org/pdf/2305.00041v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) have achieved impressive performances in view
synthesis by encoding neural representations of a scene. However, NeRFs require
hundreds of images per scene to synthesize photo-realistic novel views.
Training them on sparse input views leads to overfitting and incorrect scene
depth estimation resulting in artifacts in the rendered novel views. Sparse
input NeRFs were recently regularized by providing dense depth estimated from
pre-trained networks as supervision, to achieve improved performance over
sparse depth constraints. However, we find that such depth priors may be
inaccurate due to generalization issues. Instead, we hypothesize that the
visibility of pixels in different input views can be more reliably estimated to
provide dense supervision. In this regard, we compute a visibility prior
through the use of plane sweep volumes, which does not require any
pre-training. By regularizing the NeRF training with the visibility prior, we
successfully train the NeRF with few input views. We reformulate the NeRF to
also directly output the visibility of a 3D point from a given viewpoint to
reduce the training time with the visibility constraint. On multiple datasets,
our model outperforms the competing sparse input NeRF models including those
that use learned priors. The source code for our model can be found on our
project page:
https://nagabhushansn95.github.io/publications/2023/ViP-NeRF.html.

Comments:
- SIGGRAPH 2023

---

## NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance  Fields

nerf{: .label .label-blue }

2023-04-28 | Junge Zhang, Feihu Zhang, Shaochen Kuang, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2304.14811v2){: .btn .btn-green }

**Abstract**: Labeling LiDAR point clouds for training autonomous driving is extremely
expensive and difficult. LiDAR simulation aims at generating realistic LiDAR
data with labels for training and verifying self-driving algorithms more
efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for
novel view synthesis using implicit reconstruction of 3D scenes. Inspired by
this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages
real-world information to generate realistic LIDAR point clouds. Different from
existing LiDAR simulators, we use real images and point cloud data collected by
self-driving cars to learn the 3D scene representation, point cloud generation
and label rendering. We verify the effectiveness of our NeRF-LiDAR by training
different 3D segmentation models on the generated LiDAR point clouds. It
reveals that the trained models are able to achieve similar accuracy when
compared with the same model trained on the real LiDAR data. Besides, the
generated data is capable of boosting the accuracy through pre-training which
helps reduce the requirements of the real labeled data.

---

## Compositional 3D Human-Object Neural Animation

nerf{: .label .label-blue }

2023-04-27 | Zhi Hou, Baosheng Yu, Dacheng Tao | cs.CV | [PDF](http://arxiv.org/pdf/2304.14070v1){: .btn .btn-green }

**Abstract**: Human-object interactions (HOIs) are crucial for human-centric scene
understanding applications such as human-centric visual generation, AR/VR, and
robotics. Since existing methods mainly explore capturing HOIs, rendering HOI
remains less investigated. In this paper, we address this challenge in HOI
animation from a compositional perspective, i.e., animating novel HOIs
including novel interaction, novel human and/or novel object driven by a novel
pose sequence. Specifically, we adopt neural human-object deformation to model
and render HOI dynamics based on implicit neural representations. To enable the
interaction pose transferring among different persons and objects, we then
devise a new compositional conditional neural radiance field (or CC-NeRF),
which decomposes the interdependence between human and object using latent
codes to enable compositionally animation control of novel HOIs. Experiments
show that the proposed method can generalize well to various novel HOI
animation settings. Our project page is https://zhihou7.github.io/CHONA/

Comments:
- 14 pages, 6 figures

---

## ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with  Unsupervised Implicit Pose Embedding

nerf{: .label .label-blue }

2023-04-27 | Mijeong Kim, Hyunjoon Lee, Bohyung Han | cs.CV | [PDF](http://arxiv.org/pdf/2304.14005v2){: .btn .btn-green }

**Abstract**: Although 3D-aware GANs based on neural radiance fields have achieved
competitive performance, their applicability is still limited to objects or
scenes with the ground-truths or prediction models for clearly defined
canonical camera poses. To extend the scope of applicable datasets, we propose
a novel 3D-aware GAN optimization technique through contrastive learning with
implicit pose embeddings. To this end, we first revise the discriminator design
and remove dependency on ground-truth camera poses. Then, to capture complex
and challenging 3D scene structures more effectively, we make the discriminator
estimate a high-dimensional implicit pose embedding from a given image and
perform contrastive learning on the pose embedding. The proposed approach can
be employed for the dataset, where the canonical camera pose is ill-defined
because it does not look up or estimate camera poses. Experimental results show
that our algorithm outperforms existing methods by large margins on the
datasets with multiple object categories and inconsistent canonical camera
poses.

Comments:
- 20 pages. For the project page, see
  https://cv.snu.ac.kr/research/ContraNeRF/

---

## ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs

nerf{: .label .label-blue }

2023-04-27 | Jiteng Mu, Shen Sang, Nuno Vasconcelos, Xiaolong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2304.14401v1){: .btn .btn-green }

**Abstract**: While NeRF-based human representations have shown impressive novel view
synthesis results, most methods still rely on a large number of images / views
for training. In this work, we propose a novel animatable NeRF called
ActorsNeRF. It is first pre-trained on diverse human subjects, and then adapted
with few-shot monocular video frames for a new actor with unseen poses.
Building on previous generalizable NeRFs with parameter sharing using a ConvNet
encoder, ActorsNeRF further adopts two human priors to capture the large human
appearance, shape, and pose variations. Specifically, in the encoded feature
space, we will first align different human subjects in a category-level
canonical space, and then align the same human from different frames in an
instance-level canonical space for rendering. We quantitatively and
qualitatively demonstrate that ActorsNeRF significantly outperforms the
existing state-of-the-art on few-shot generalization to new people and poses on
multiple datasets. Project Page: https://jitengmu.github.io/ActorsNeRF/

Comments:
- Project Page : https://jitengmu.github.io/ActorsNeRF/

---

## Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile  Mapping

nerf{: .label .label-blue }

2023-04-27 | Dennis Haitz, Boris Jutzi, Markus Ulrich, Miriam Jaeger, Patrick Huebner | cs.CV | [PDF](http://arxiv.org/pdf/2304.14301v2){: .btn .btn-green }

**Abstract**: This work represents a large step into modern ways of fast 3D reconstruction
based on RGB camera images. Utilizing a Microsoft HoloLens 2 as a multisensor
platform that includes an RGB camera and an inertial measurement unit for
SLAM-based camera-pose determination, we train a Neural Radiance Field (NeRF)
as a neural scene representation in real-time with the acquired data from the
HoloLens. The HoloLens is connected via Wifi to a high-performance PC that is
responsible for the training and 3D reconstruction. After the data stream ends,
the training is stopped and the 3D reconstruction is initiated, which extracts
a point cloud of the scene. With our specialized inference algorithm, five
million scene points can be extracted within 1 second. In addition, the point
cloud also includes radiometry per point. Our method of 3D reconstruction
outperforms grid point sampling with NeRFs by multiple orders of magnitude and
can be regarded as a complete real-time 3D reconstruction method in a mobile
mapping setup.

Comments:
- 8 pages, 6 figures

---

## Learning a Diffusion Prior for NeRFs

nerf{: .label .label-blue }

2023-04-27 | Guandao Yang, Abhijit Kundu, Leonidas J. Guibas, Jonathan T. Barron, Ben Poole | cs.CV | [PDF](http://arxiv.org/pdf/2304.14473v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D
representation for objects and scenes derived from 2D data. Generating NeRFs,
however, remains difficult in many scenarios. For instance, training a NeRF
with only a small number of views as supervision remains challenging since it
is an under-constrained problem. In such settings, it calls for some inductive
prior to filter out bad local minima. One way to introduce such inductive
priors is to learn a generative model for NeRFs modeling a certain class of
scenes. In this paper, we propose to use a diffusion model to generate NeRFs
encoded on a regularized grid. We show that our model can sample realistic
NeRFs, while at the same time allowing conditional generations, given a certain
observation as guidance.

---

## Super-NeRF: View-consistent Detail Generation for NeRF super-resolution

nerf{: .label .label-blue }

2023-04-26 | Yuqi Han, Tao Yu, Xiaohang Yu, Yuwang Wang, Qionghai Dai | cs.CV | [PDF](http://arxiv.org/pdf/2304.13518v1){: .btn .btn-green }

**Abstract**: The neural radiance field (NeRF) achieved remarkable success in modeling 3D
scenes and synthesizing high-fidelity novel views. However, existing NeRF-based
methods focus more on the make full use of the image resolution to generate
novel views, but less considering the generation of details under the limited
input resolution. In analogy to the extensive usage of image super-resolution,
NeRF super-resolution is an effective way to generate the high-resolution
implicit representation of 3D scenes and holds great potential applications. Up
to now, such an important topic is still under-explored. In this paper, we
propose a NeRF super-resolution method, named Super-NeRF, to generate
high-resolution NeRF from only low-resolution inputs. Given multi-view
low-resolution images, Super-NeRF constructs a consistency-controlling
super-resolution module to generate view-consistent high-resolution details for
NeRF. Specifically, an optimizable latent code is introduced for each
low-resolution input image to control the 2D super-resolution images to
converge to the view-consistent output. The latent codes of each low-resolution
image are optimized synergistically with the target Super-NeRF representation
to fully utilize the view consistency constraint inherent in NeRF construction.
We verify the effectiveness of Super-NeRF on synthetic, real-world, and
AI-generated NeRF datasets. Super-NeRF achieves state-of-the-art NeRF
super-resolution performance on high-resolution detail generation and
cross-view consistency.

---

## VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs

nerf{: .label .label-blue }

2023-04-26 | Jiakai Sun, Zhanjie Zhang, Jiafu Chen, Guangyuan Li, Boyan Ji, Lei Zhao, Wei Xing, Huaizhong Lin | cs.CV | [PDF](http://arxiv.org/pdf/2304.13386v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) has shown great success in novel view synthesis
due to its state-of-the-art quality and flexibility. However, NeRF requires
dense input views (tens to hundreds) and a long training time (hours to days)
for a single scene to generate high-fidelity images. Although using the voxel
grids to represent the radiance field can significantly accelerate the
optimization process, we observe that for sparse inputs, the voxel grids are
more prone to overfitting to the training views and will have holes and
floaters, which leads to artifacts. In this paper, we propose VGOS, an approach
for fast (3-5 minutes) radiance field reconstruction from sparse inputs (3-10
views) to address these issues. To improve the performance of voxel-based
radiance field in sparse input scenarios, we propose two methods: (a) We
introduce an incremental voxel training strategy, which prevents overfitting by
suppressing the optimization of peripheral voxels in the early stage of
reconstruction. (b) We use several regularization techniques to smooth the
voxels, which avoids degenerate solutions. Experiments demonstrate that VGOS
achieves state-of-the-art performance for sparse inputs with super-fast
convergence. Code will be available at https://github.com/SJoJoK/VGOS.

Comments:
- IJCAI 2023 Accepted (Main Track)

---

## Local Implicit Ray Function for Generalizable Radiance Field  Representation

nerf{: .label .label-blue }

2023-04-25 | Xin Huang, Qi Zhang, Ying Feng, Xiaoyu Li, Xuan Wang, Qing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2304.12746v1){: .btn .btn-green }

**Abstract**: We propose LIRF (Local Implicit Ray Function), a generalizable neural
rendering approach for novel view rendering. Current generalizable neural
radiance fields (NeRF) methods sample a scene with a single ray per pixel and
may therefore render blurred or aliased views when the input views and rendered
views capture scene content with different resolutions. To solve this problem,
we propose LIRF to aggregate the information from conical frustums to construct
a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates
and the features of conical frustums as inputs and predicts a local volumetric
radiance field. Since the coordinates are continuous, LIRF renders high-quality
novel views at a continuously-valued scale via volume rendering. Besides, we
predict the visible weights for each input view via transformer-based feature
matching to improve the performance in occluded areas. Experimental results on
real-world scenes validate that our method outperforms state-of-the-art methods
on novel view rendering of unseen scenes at arbitrary scales.

Comments:
- Accepted to CVPR 2023. Project page: https://xhuangcv.github.io/lirf/

---

## MF-NeRF: Memory Efficient NeRF with Mixed-Feature Hash Table

nerf{: .label .label-blue }

2023-04-25 | Yongjae Lee, Li Yang, Deliang Fan | cs.CV | [PDF](http://arxiv.org/pdf/2304.12587v4){: .btn .btn-green }

**Abstract**: Neural radiance field (NeRF) has shown remarkable performance in generating
photo-realistic novel views. Among recent NeRF related research, the approaches
that involve the utilization of explicit structures like grids to manage
features achieve exceptionally fast training by reducing the complexity of
multilayer perceptron (MLP) networks. However, storing features in dense grids
demands a substantial amount of memory space, resulting in a notable memory
bottleneck within computer system. Consequently, it leads to a significant
increase in training times without prior hyper-parameter tuning. To address
this issue, in this work, we are the first to propose MF-NeRF, a
memory-efficient NeRF framework that employs a Mixed-Feature hash table to
improve memory efficiency and reduce training time while maintaining
reconstruction quality. Specifically, we first design a mixed-feature hash
encoding to adaptively mix part of multi-level feature grids and map it to a
single hash table. Following that, in order to obtain the correct index of a
grid point, we further develop an index transformation method that transforms
indices of an arbitrary level grid to those of a canonical grid. Extensive
experiments benchmarking with state-of-the-art Instant-NGP, TensoRF, and DVGO,
indicate our MF-NeRF could achieve the fastest training time on the same GPU
hardware with similar or even higher reconstruction quality.

---

## Explicit Correspondence Matching for Generalizable Neural Radiance  Fields

nerf{: .label .label-blue }

2023-04-24 | Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2304.12294v1){: .btn .btn-green }

**Abstract**: We present a new generalizable NeRF method that is able to directly
generalize to new unseen scenarios and perform novel view synthesis with as few
as two source views. The key to our approach lies in the explicitly modeled
correspondence matching information, so as to provide the geometry prior to the
prediction of NeRF color and density for volume rendering. The explicit
correspondence matching is quantified with the cosine similarity between image
features sampled at the 2D projections of a 3D point on different views, which
is able to provide reliable cues about the surface geometry. Unlike previous
methods where image features are extracted independently for each view, we
consider modeling the cross-view interactions via Transformer cross-attention,
which greatly improves the feature matching quality. Our method achieves
state-of-the-art results on different evaluation settings, with the experiments
showing a strong correlation between our learned cosine feature similarity and
volume density, demonstrating the effectiveness and superiority of our proposed
method. Code is at https://github.com/donydchen/matchnerf

Comments:
- Code and pre-trained models: https://github.com/donydchen/matchnerf
  Project Page: https://donydchen.github.io/matchnerf/

---

## Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via  Algorithm-Hardware Co-Design

nerf{: .label .label-blue }

2023-04-24 | Yonggan Fu, Zhifan Ye, Jiayi Yuan, Shunyao Zhang, Sixu Li, Haoran You, Yingyan Lin | cs.CV | [PDF](http://arxiv.org/pdf/2304.11842v3){: .btn .btn-green }

**Abstract**: Novel view synthesis is an essential functionality for enabling immersive
experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for
which generalizable Neural Radiance Fields (NeRFs) have gained increasing
popularity thanks to their cross-scene generalization capability. Despite their
promise, the real-device deployment of generalizable NeRFs is bottlenecked by
their prohibitive complexity due to the required massive memory accesses to
acquire scene features, causing their ray marching process to be
memory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware
co-design framework dedicated to generalizable NeRF acceleration, which for the
first time enables real-time generalizable NeRFs. On the algorithm side,
Gen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact
that different regions of a 3D scene contribute differently to the rendered
pixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF
highlights an accelerator micro-architecture to maximize the data reuse
opportunities among different rays by making use of their epipolar geometric
relationship. Furthermore, our Gen-NeRF accelerator features a customized
dataflow to enhance data locality during point-to-hardware mapping and an
optimized scene feature storage strategy to minimize memory bank conflicts.
Extensive experiments validate the effectiveness of our proposed Gen-NeRF
framework in enabling real-time and generalizable novel view synthesis.

Comments:
- Accepted by ISCA 2023

---

## HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single  Video

nerf{: .label .label-blue }

2023-04-24 | Jia-Wei Liu, Yan-Pei Cao, Tianyuan Yang, Eric Zhongcong Xu, Jussi Keppo, Ying Shan, Xiaohu Qie, Mike Zheng Shou | cs.CV | [PDF](http://arxiv.org/pdf/2304.12281v1){: .btn .btn-green }

**Abstract**: We introduce HOSNeRF, a novel 360{\deg} free-viewpoint rendering method that
reconstructs neural radiance fields for dynamic human-object-scene from a
single monocular in-the-wild video. Our method enables pausing the video at any
frame and rendering all scene details (dynamic humans, objects, and
backgrounds) from arbitrary viewpoints. The first challenge in this task is the
complex object motions in human-object interactions, which we tackle by
introducing the new object bones into the conventional human skeleton hierarchy
to effectively estimate large object deformations in our dynamic human-object
model. The second challenge is that humans interact with different objects at
different times, for which we introduce two new learnable object state
embeddings that can be used as conditions for learning our human-object
representation and scene representation, respectively. Extensive experiments
show that HOSNeRF significantly outperforms SOTA approaches on two challenging
datasets by a large margin of 40% ~ 50% in terms of LPIPS. The code, data, and
compelling examples of 360{\deg} free-viewpoint renderings from single videos
will be released in https://showlab.github.io/HOSNeRF.

Comments:
- Project page: https://showlab.github.io/HOSNeRF

---

## TextMesh: Generation of Realistic 3D Meshes From Text Prompts

nerf{: .label .label-blue }

2023-04-24 | Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2304.12439v1){: .btn .btn-green }

**Abstract**: The ability to generate highly realistic 2D images from mere text prompts has
recently made huge progress in terms of speed and quality, thanks to the advent
of image diffusion models. Naturally, the question arises if this can be also
achieved in the generation of 3D content from such text prompts. To this end, a
new line of methods recently emerged trying to harness diffusion models,
trained on 2D images, for supervision of 3D model generation using view
dependent prompts. While achieving impressive results, these methods, however,
have two major drawbacks. First, rather than commonly used 3D meshes, they
instead generate neural radiance fields (NeRFs), making them impractical for
most real applications. Second, these approaches tend to produce over-saturated
models, giving the output a cartoonish looking effect. Therefore, in this work
we propose a novel method for generation of highly realistic-looking 3D meshes.
To this end, we extend NeRF to employ an SDF backbone, leading to improved 3D
mesh extraction. In addition, we propose a novel way to finetune the mesh
texture, removing the effect of high saturation and improving the details of
the output 3D mesh.

Comments:
- Project Website: https://fabi92.github.io/textmesh/

---

## Segment Anything in 3D with NeRFs

nerf{: .label .label-blue }

2023-04-24 | Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian | cs.CV | [PDF](http://arxiv.org/pdf/2304.12308v4){: .btn .btn-green }

**Abstract**: Recently, the Segment Anything Model (SAM) emerged as a powerful vision
foundation model which is capable to segment anything in 2D images. This paper
aims to generalize SAM to segment 3D objects. Rather than replicating the data
acquisition and annotation procedure which is costly in 3D, we design an
efficient solution, leveraging the Neural Radiance Field (NeRF) as a cheap and
off-the-shelf prior that connects multi-view 2D images to the 3D space. We
refer to the proposed solution as SA3D, for Segment Anything in 3D. It is only
required to provide a manual segmentation prompt (e.g., rough points) for the
target object in a single view, which is used to generate its 2D mask in this
view with SAM. Next, SA3D alternately performs mask inverse rendering and
cross-view self-prompting across various views to iteratively complete the 3D
mask of the target object constructed with voxel grids. The former projects the
2D mask obtained by SAM in the current view onto 3D mask with guidance of the
density distribution learned by the NeRF; The latter extracts reliable prompts
automatically as the input to SAM from the NeRF-rendered 2D mask in another
view. We show in experiments that SA3D adapts to various scenes and achieves 3D
segmentation within minutes. Our research reveals a potential methodology to
lift the ability of a 2D vision foundation model to 3D, as long as the 2D model
can steadily address promptable segmentation across multiple views. Our code is
available at https://github.com/Jumpat/SegmentAnythingin3D.

Comments:
- NeurIPS 2023. Project page: https://jumpat.github.io/SA3D/

---

## Instant-3D: Instant Neural Radiance Field Training Towards On-Device  AR/VR 3D Reconstruction

nerf{: .label .label-blue }

2023-04-24 | Sixu Li, Chaojian Li, Wenbo Zhu,  Boyang,  Yu,  Yang,  Zhao, Cheng Wan, Haoran You, Huihong Shi,  Yingyan,  Lin | cs.AR | [PDF](http://arxiv.org/pdf/2304.12467v2){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) based 3D reconstruction is highly desirable for
immersive Augmented and Virtual Reality (AR/VR) applications, but achieving
instant (i.e., < 5 seconds) on-device NeRF training remains a challenge. In
this work, we first identify the inefficiency bottleneck: the need to
interpolate NeRF embeddings up to 200,000 times from a 3D embedding grid during
each training iteration. To alleviate this, we propose Instant-3D, an
algorithm-hardware co-design acceleration framework that achieves instant
on-device NeRF training. Our algorithm decomposes the embedding grid
representation in terms of color and density, enabling computational redundancy
to be squeezed out by adopting different (1) grid sizes and (2) update
frequencies for the color and density branches. Our hardware accelerator
further reduces the dominant memory accesses for embedding grid interpolation
by (1) mapping multiple nearby points' memory read requests into one during the
feed-forward process, (2) merging embedding grid updates from the same sliding
time window during back-propagation, and (3) fusing different computation cores
to support the different grid sizes needed by the color and density branches of
Instant-3D algorithm. Extensive experiments validate the effectiveness of
Instant-3D, achieving a large training time reduction of 41x - 248x while
maintaining the same reconstruction quality. Excitingly, Instant-3D has enabled
instant 3D reconstruction for AR/VR, requiring a reconstruction time of only
1.6 seconds per scene and meeting the AR/VR power consumption constraint of 1.9
W.

Comments:
- Accepted by ISCA'23

---

## 3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive  Physics under Challenging Scenes

nerf{: .label .label-blue }

2023-04-22 | Haotian Xue, Antonio Torralba, Joshua B. Tenenbaum, Daniel LK Yamins, Yunzhu Li, Hsiao-Yu Tung | cs.CV | [PDF](http://arxiv.org/pdf/2304.11470v1){: .btn .btn-green }

**Abstract**: Given a visual scene, humans have strong intuitions about how a scene can
evolve over time under given actions. The intuition, often termed visual
intuitive physics, is a critical ability that allows us to make effective plans
to manipulate the scene to achieve desired outcomes without relying on
extensive trial and error. In this paper, we present a framework capable of
learning 3D-grounded visual intuitive physics models from videos of complex
scenes with fluids. Our method is composed of a conditional Neural Radiance
Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction
backend, using which we can impose strong relational and structural inductive
bias to capture the structure of the underlying environment. Unlike existing
intuitive point-based dynamics works that rely on the supervision of dense
point trajectory from simulators, we relax the requirements and only assume
access to multi-view RGB images and (imperfect) instance masks acquired using
color prior. This enables the proposed model to handle scenarios where accurate
point estimation and tracking are hard or impossible. We generate datasets
including three challenging scenarios involving fluid, granular materials, and
rigid objects in the simulation. The datasets do not include any dense particle
information so most previous 3D-based intuitive physics pipelines can barely
deal with that. We show our model can make long-horizon future predictions by
learning from raw images and significantly outperforms models that do not
employ an explicit 3D representation space. We also show that once trained, our
model can achieve strong generalization in complex scenarios under extrapolate
settings.

---

## Dehazing-NeRF: Neural Radiance Fields from Hazy Images

nerf{: .label .label-blue }

2023-04-22 | Tian Li, LU Li, Wei Wang, Zhangchi Feng | cs.CV | [PDF](http://arxiv.org/pdf/2304.11448v1){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) has received much attention in recent years due
to the impressively high quality in 3D scene reconstruction and novel view
synthesis. However, image degradation caused by the scattering of atmospheric
light and object light by particles in the atmosphere can significantly
decrease the reconstruction quality when shooting scenes in hazy conditions. To
address this issue, we propose Dehazing-NeRF, a method that can recover clear
NeRF from hazy image inputs. Our method simulates the physical imaging process
of hazy images using an atmospheric scattering model, and jointly learns the
atmospheric scattering model and a clean NeRF model for both image dehazing and
novel view synthesis. Different from previous approaches, Dehazing-NeRF is an
unsupervised method with only hazy images as the input, and also does not rely
on hand-designed dehazing priors. By jointly combining the depth estimated from
the NeRF 3D scene with the atmospheric scattering model, our proposed model
breaks through the ill-posed problem of single-image dehazing while maintaining
geometric consistency. Besides, to alleviate the degradation of image quality
caused by information loss, soft margin consistency regularization, as well as
atmospheric consistency and contrast discriminative loss, are addressed during
the model training process. Extensive experiments demonstrate that our method
outperforms the simple combination of single-image dehazing and NeRF on both
image dehazing and novel view image synthesis.

---

## NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent  Semantic Navigation

nerf{: .label .label-blue }

2023-04-22 | Baao Xie, Bohan Li, Zequn Zhang, Junting Dong, Xin Jin, Jingyu Yang, Wenjun Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2304.11342v1){: .btn .btn-green }

**Abstract**: 3D representation disentanglement aims to identify, decompose, and manipulate
the underlying explanatory factors of 3D data, which helps AI fundamentally
understand our 3D world. This task is currently under-explored and poses great
challenges: (i) the 3D representations are complex and in general contains much
more information than 2D image; (ii) many 3D representations are not well
suited for gradient-based optimization, let alone disentanglement. To address
these challenges, we use NeRF as a differentiable 3D representation, and
introduce a self-supervised Navigation to identify interpretable semantic
directions in the latent space. To our best knowledge, this novel method,
dubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglement
without any priors or supervisions. Specifically, NaviNeRF is built upon the
generative NeRF pipeline, and equipped with an Outer Navigation Branch and an
Inner Refinement Branch. They are complementary -- the outer navigation is to
identify global-view semantic directions, and the inner refinement dedicates to
fine-grained attributes. A synergistic loss is further devised to coordinate
two branches. Extensive experiments demonstrate that NaviNeRF has a superior
fine-grained 3D disentanglement ability than the previous 3D-aware models. Its
performance is also comparable to editing-oriented models relying on semantic
or geometry priors.

---

## AutoNeRF: Training Implicit Scene Representations with Autonomous Agents

nerf{: .label .label-blue }

2023-04-21 | Pierre Marza, Laetitia Matignon, Olivier Simonin, Dhruv Batra, Christian Wolf, Devendra Singh Chaplot | cs.CV | [PDF](http://arxiv.org/pdf/2304.11241v2){: .btn .btn-green }

**Abstract**: Implicit representations such as Neural Radiance Fields (NeRF) have been
shown to be very effective at novel view synthesis. However, these models
typically require manual and careful human data collection for training. In
this paper, we present AutoNeRF, a method to collect data required to train
NeRFs using autonomous embodied agents. Our method allows an agent to explore
an unseen environment efficiently and use the experience to build an implicit
map representation autonomously. We compare the impact of different exploration
strategies including handcrafted frontier-based exploration, end-to-end and
modular approaches composed of trained high-level planners and classical
low-level path followers. We train these models with different reward functions
tailored to this problem and evaluate the quality of the learned
representations on four different downstream tasks: classical viewpoint
rendering, map reconstruction, planning, and pose refinement. Empirical results
show that NeRFs can be trained on actively collected data using just a single
episode of experience in an unseen environment, and can be used for several
downstream robotic tasks, and that modular trained exploration models
outperform other classical and end-to-end baselines. Finally, we show that
AutoNeRF can reconstruct large-scale scenes, and is thus a useful tool to
perform scene-specific adaptation as the produced 3D environment models can be
loaded into a simulator to fine-tune a policy of interest.

---

## Omni-Line-of-Sight Imaging for Holistic Shape Reconstruction

nerf{: .label .label-blue }

2023-04-21 | Binbin Huang, Xingyue Peng, Siyuan Shen, Suan Xia, Ruiqian Li, Yanhua Yu, Yuehan Wang, Shenghua Gao, Wenzheng Chen, Shiying Li, Jingyi Yu | cs.CV | [PDF](http://arxiv.org/pdf/2304.10780v1){: .btn .btn-green }

**Abstract**: We introduce Omni-LOS, a neural computational imaging method for conducting
holistic shape reconstruction (HSR) of complex objects utilizing a
Single-Photon Avalanche Diode (SPAD)-based time-of-flight sensor. As
illustrated in Fig. 1, our method enables new capabilities to reconstruct
near-$360^\circ$ surrounding geometry of an object from a single scan spot. In
such a scenario, traditional line-of-sight (LOS) imaging methods only see the
front part of the object and typically fail to recover the occluded back
regions. Inspired by recent advances of non-line-of-sight (NLOS) imaging
techniques which have demonstrated great power to reconstruct occluded objects,
Omni-LOS marries LOS and NLOS together, leveraging their complementary
advantages to jointly recover the holistic shape of the object from a single
scan position. The core of our method is to put the object nearby diffuse walls
and augment the LOS scan in the front view with the NLOS scans from the
surrounding walls, which serve as virtual ``mirrors'' to trap lights toward the
object. Instead of separately recovering the LOS and NLOS signals, we adopt an
implicit neural network to represent the object, analogous to NeRF and NeTF.
While transients are measured along straight rays in LOS but over the spherical
wavefronts in NLOS, we derive differentiable ray propagation models to
simultaneously model both types of transient measurements so that the NLOS
reconstruction also takes into account the direct LOS measurements and vice
versa. We further develop a proof-of-concept Omni-LOS hardware prototype for
real-world validation. Comprehensive experiments on various wall settings
demonstrate that Omni-LOS successfully resolves shape ambiguities caused by
occlusions, achieves high-fidelity 3D scan quality, and manages to recover
objects of various scales and complexity.

---

## A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses  from HoloLens Trajectories and Structure from Motion

nerf{: .label .label-blue }

2023-04-20 | Miriam Jäger, Patrick Hübner, Dennis Haitz, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2304.10664v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) are trained using a set of camera poses and
associated images as input to estimate density and color values for each
position. The position-dependent density learning is of particular interest for
photogrammetry, enabling 3D reconstruction by querying and filtering the NeRF
coordinate system based on the object density. While traditional methods like
Structure from Motion are commonly used for camera pose calculation in
pre-processing for NeRFs, the HoloLens offers an interesting interface for
extracting the required input data directly. We present a workflow for
high-resolution 3D reconstructions almost directly from HoloLens data using
NeRFs. Thereby, different investigations are considered: Internal camera poses
from the HoloLens trajectory via a server application, and external camera
poses from Structure from Motion, both with an enhanced variant applied through
pose refinement. Results show that the internal camera poses lead to NeRF
convergence with a PSNR of 25\,dB with a simple rotation around the x-axis and
enable a 3D reconstruction. Pose refinement enables comparable quality compared
to external camera poses, resulting in improved training process with a PSNR of
27\,dB and a better 3D reconstruction. Overall, NeRF reconstructions outperform
the conventional photogrammetric dense reconstruction using Multi-View Stereo
in terms of completeness and level of detail.

Comments:
- 7 pages, 5 figures. Will be published in the ISPRS The International
  Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences

---

## Learning Neural Duplex Radiance Fields for Real-Time View Synthesis

nerf{: .label .label-blue }

2023-04-20 | Ziyu Wan, Christian Richardt, Aljaž Božič, Chao Li, Vijay Rengarajan, Seonghyeon Nam, Xiaoyu Xiang, Tuotuo Li, Bo Zhu, Rakesh Ranjan, Jing Liao | cs.CV | [PDF](http://arxiv.org/pdf/2304.10537v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) enable novel view synthesis with unprecedented
visual quality. However, to render photorealistic images, NeRFs require
hundreds of deep multilayer perceptron (MLP) evaluations - for each pixel. This
is prohibitively expensive and makes real-time rendering infeasible, even on
powerful modern GPUs. In this paper, we propose a novel approach to distill and
bake NeRFs into highly efficient mesh-based neural representations that are
fully compatible with the massively parallel graphics rendering pipeline. We
represent scenes as neural radiance features encoded on a two-layer duplex
mesh, which effectively overcomes the inherent inaccuracies in 3D surface
reconstruction by learning the aggregated radiance information from a reliable
interval of ray-surface intersections. To exploit local geometric relationships
of nearby pixels, we leverage screen-space convolutions instead of the MLPs
used in NeRFs to achieve high-quality appearance. Finally, the performance of
the whole framework is further boosted by a novel multi-view distillation
optimization strategy. We demonstrate the effectiveness and superiority of our
approach via extensive experiments on a range of standard datasets.

Comments:
- CVPR 2023. Project page: http://raywzy.com/NDRF

---

## Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs

nerf{: .label .label-blue }

2023-04-20 | Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Holynski, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2304.10532v3){: .btn .btn-green }

**Abstract**: Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such
as floaters or flawed geometry when rendered outside the camera trajectory.
Existing evaluation protocols often do not capture these effects, since they
usually only assess image quality at every 8th frame of the training capture.
To push forward progress in novel-view synthesis, we propose a new dataset and
evaluation procedure, where two camera trajectories are recorded of the scene:
one used for training, and the other for evaluation. In this more challenging
in-the-wild setting, we find that existing hand-crafted regularizers do not
remove floaters nor improve scene geometry. Thus, we propose a 3D
diffusion-based method that leverages local 3D priors and a novel density-based
score distillation sampling loss to discourage artifacts during NeRF
optimization. We show that this data-driven prior removes floaters and improves
scene geometry for casual captures.

Comments:
- ICCV 2023, project page: https://ethanweber.me/nerfbusters

---

## ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of  Real World Objects

nerf{: .label .label-blue }

2023-04-20 | Marco Toschi, Riccardo De Matteo, Riccardo Spezialetti, Daniele De Gregorio, Luigi Di Stefano, Samuele Salti | cs.CV | [PDF](http://arxiv.org/pdf/2304.10448v1){: .btn .btn-green }

**Abstract**: In this paper, we focus on the problem of rendering novel views from a Neural
Radiance Field (NeRF) under unobserved light conditions. To this end, we
introduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world
objects under one-light-at-time (OLAT) conditions, annotated with accurate
ground-truth camera and light poses. Our acquisition pipeline leverages two
robotic arms holding, respectively, a camera and an omni-directional point-wise
light source. We release a total of 20 scenes depicting a variety of objects
with complex geometry and challenging materials. Each scene includes 2000
images, acquired from 50 different points of views under 40 different OLAT
conditions. By leveraging the dataset, we perform an ablation study on the
relighting capability of variants of the vanilla NeRF architecture and identify
a lightweight architecture that can render novel views of an object under novel
light conditions, which we use to establish a non-trivial baseline for the
dataset. Dataset and benchmark are available at
https://eyecan-ai.github.io/rene.

Comments:
- Accepted at CVPR 2023 as a highlight

---

## LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields

nerf{: .label .label-blue }

2023-04-20 | Tang Tao, Longfei Gao, Guangrun Wang, Yixing Lao, Peng Chen, Hengshuang Zhao, Dayang Hao, Xiaodan Liang, Mathieu Salzmann, Kaicheng Yu | cs.CV | [PDF](http://arxiv.org/pdf/2304.10406v2){: .btn .btn-green }

**Abstract**: We introduce a new task, novel view synthesis for LiDAR sensors. While
traditional model-based LiDAR simulators with style-transfer neural networks
can be applied to render novel views, they fall short of producing accurate and
realistic LiDAR patterns because the renderers rely on explicit 3D
reconstruction and exploit game engines, that ignore important attributes of
LiDAR points. We address this challenge by formulating, to the best of our
knowledge, the first differentiable end-to-end LiDAR rendering framework,
LiDAR-NeRF, leveraging a neural radiance field (NeRF) to facilitate the joint
learning of geometry and the attributes of 3D points. However, simply employing
NeRF cannot achieve satisfactory results, as it only focuses on learning
individual pixels while ignoring local information, especially at low texture
areas, resulting in poor geometry. To this end, we have taken steps to address
this issue by introducing a structural regularization method to preserve local
structural details. To evaluate the effectiveness of our approach, we establish
an object-centric multi-view LiDAR dataset, dubbed NeRF-MVL. It contains
observations of objects from 9 categories seen from 360-degree viewpoints
captured with multiple LiDAR sensors. Our extensive experiments on the
scene-level KITTI-360 dataset, and on our object-level NeRF-MVL show that our
LiDAR-NeRF surpasses the model-based algorithms significantly.

Comments:
- This paper introduces a new task of novel LiDAR view synthesis, and
  proposes a differentiable framework called LiDAR-NeRF with a structural
  regularization, as well as an object-centric multi-view LiDAR dataset called
  NeRF-MVL

---

## Revisiting Implicit Neural Representations in Low-Level Vision

nerf{: .label .label-blue }

2023-04-20 | Wentian Xu, Jianbo Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2304.10250v1){: .btn .btn-green }

**Abstract**: Implicit Neural Representation (INR) has been emerging in computer vision in
recent years. It has been shown to be effective in parameterising continuous
signals such as dense 3D models from discrete image data, e.g. the neural
radius field (NeRF). However, INR is under-explored in 2D image processing
tasks. Considering the basic definition and the structure of INR, we are
interested in its effectiveness in low-level vision problems such as image
restoration. In this work, we revisit INR and investigate its application in
low-level image restoration tasks including image denoising, super-resolution,
inpainting, and deblurring. Extensive experimental evaluations suggest the
superior performance of INR in several low-level vision tasks with limited
resources, outperforming its counterparts by over 2dB. Code and models are
available at https://github.com/WenTXuL/LINR

Comments:
- Published at the ICLR 2023 Neural Fields workshop. Project Webpage:
  https://wentxul.github.io/LINR-projectpage

---

## Multiscale Representation for Real-Time Anti-Aliasing Neural Rendering

nerf{: .label .label-blue }

2023-04-20 | Dongting Hu, Zhenkai Zhang, Tingbo Hou, Tongliang Liu, Huan Fu, Mingming Gong | cs.CV | [PDF](http://arxiv.org/pdf/2304.10075v2){: .btn .btn-green }

**Abstract**: The rendering scheme in neural radiance field (NeRF) is effective in
rendering a pixel by casting a ray into the scene. However, NeRF yields blurred
rendering results when the training images are captured at non-uniform scales,
and produces aliasing artifacts if the test images are taken in distant views.
To address this issue, Mip-NeRF proposes a multiscale representation as a
conical frustum to encode scale information. Nevertheless, this approach is
only suitable for offline rendering since it relies on integrated positional
encoding (IPE) to query a multilayer perceptron (MLP). To overcome this
limitation, we propose mip voxel grids (Mip-VoG), an explicit multiscale
representation with a deferred architecture for real-time anti-aliasing
rendering. Our approach includes a density Mip-VoG for scene geometry and a
feature Mip-VoG with a small MLP for view-dependent color. Mip-VoG encodes
scene scale using the level of detail (LOD) derived from ray differentials and
uses quadrilinear interpolation to map a queried 3D location to its features
and density from two neighboring downsampled voxel grids. To our knowledge, our
approach is the first to offer multiscale training and real-time anti-aliasing
rendering simultaneously. We conducted experiments on multiscale datasets, and
the results show that our approach outperforms state-of-the-art real-time
rendering baselines.

---

## Neural Radiance Fields: Past, Present, and Future

nerf{: .label .label-blue }

2023-04-20 | Ansh Mittal | cs.CV | [PDF](http://arxiv.org/pdf/2304.10050v2){: .btn .btn-green }

**Abstract**: The various aspects like modeling and interpreting 3D environments and
surroundings have enticed humans to progress their research in 3D Computer
Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall
et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in
Computer Graphics, Robotics, Computer Vision, and the possible scope of
High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D
models have gained traction from res with more than 1000 preprints related to
NeRFs published. This paper serves as a bridge for people starting to study
these fields by building on the basics of Mathematics, Geometry, Computer
Vision, and Computer Graphics to the difficulties encountered in Implicit
Representations at the intersection of all these disciplines. This survey
provides the history of rendering, Implicit Learning, and NeRFs, the
progression of research on NeRFs, and the potential applications and
implications of NeRFs in today's world. In doing so, this survey categorizes
all the NeRF-related research in terms of the datasets used, objective
functions, applications solved, and evaluation criteria for these applications.

Comments:
- 413 pages, 9 figures, 277 citations

---

## Reference-guided Controllable Inpainting of Neural Radiance Fields

nerf{: .label .label-blue }

2023-04-19 | Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2304.09677v2){: .btn .btn-green }

**Abstract**: The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led
to a desire for NeRF editing tools. Here, we focus on inpainting regions in a
view-consistent and controllable manner. In addition to the typical NeRF inputs
and masks delineating the unwanted region in each view, we require only a
single inpainted view of the scene, i.e., a reference view. We use monocular
depth estimators to back-project the inpainted view to the correct 3D
positions. Then, via a novel rendering technique, a bilateral solver can
construct view-dependent effects in non-reference views, making the inpainted
region appear consistent from any view. For non-reference disoccluded regions,
which cannot be supervised by the single reference view, we devise a method
based on image inpainters to guide both the geometry and appearance. Our
approach shows superior performance to NeRF inpainting baselines, with the
additional advantage that a user can control the generated scene via a single
inpainted image. Project page: https://ashmrz.github.io/reference-guided-3d

Comments:
- Project Page: https://ashmrz.github.io/reference-guided-3d

---

## Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra

nerf{: .label .label-blue }

2023-04-19 | Jonas Kulhanek, Torsten Sattler | cs.CV | [PDF](http://arxiv.org/pdf/2304.09987v3){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) are a very recent and very popular approach
for the problems of novel view synthesis and 3D reconstruction. A popular scene
representation used by NeRFs is to combine a uniform, voxel-based subdivision
of the scene with an MLP. Based on the observation that a (sparse) point cloud
of the scene is often available, this paper proposes to use an adaptive
representation based on tetrahedra obtained by Delaunay triangulation instead
of uniform subdivision or point-based representations. We show that such a
representation enables efficient training and leads to state-of-the-art
results. Our approach elegantly combines concepts from 3D geometry processing,
triangle-based rendering, and modern neural radiance fields. Compared to
voxel-based representations, ours provides more detail around parts of the
scene likely to be close to the surface. Compared to point-based
representations, our approach achieves better performance. The source code is
publicly available at: https://jkulhanek.com/tetra-nerf.

Comments:
- ICCV 2023, Web: https://jkulhanek.com/tetra-nerf

---

## Anything-3D: Towards Single-view Anything Reconstruction in the Wild



2023-04-19 | Qiuhong Shen, Xingyi Yang, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2304.10261v1){: .btn .btn-green }

**Abstract**: 3D reconstruction from a single-RGB image in unconstrained real-world
scenarios presents numerous challenges due to the inherent diversity and
complexity of objects and environments. In this paper, we introduce
Anything-3D, a methodical framework that ingeniously combines a series of
visual-language models and the Segment-Anything object segmentation model to
elevate objects to 3D, yielding a reliable and versatile system for single-view
conditioned 3D reconstruction task. Our approach employs a BLIP model to
generate textural descriptions, utilizes the Segment-Anything model for the
effective extraction of objects of interest, and leverages a text-to-image
diffusion model to lift object into a neural radiance field. Demonstrating its
ability to produce accurate and detailed 3D reconstructions for a wide array of
objects, \emph{Anything-3D\footnotemark[2]} shows promise in addressing the
limitations of existing methodologies. Through comprehensive experiments and
evaluations on various datasets, we showcase the merits of our approach,
underscoring its potential to contribute meaningfully to the field of 3D
reconstruction. Demos and code will be available at
\href{https://github.com/Anything-of-anything/Anything-3D}{https://github.com/Anything-of-anything/Anything-3D}.

---

## NeAI: A Pre-convoluted Representation for Plug-and-Play Neural Ambient  Illumination

nerf{: .label .label-blue }

2023-04-18 | Yiyu Zhuang, Qi Zhang, Xuan Wang, Hao Zhu, Ying Feng, Xiaoyu Li, Ying Shan, Xun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2304.08757v1){: .btn .btn-green }

**Abstract**: Recent advances in implicit neural representation have demonstrated the
ability to recover detailed geometry and material from multi-view images.
However, the use of simplified lighting models such as environment maps to
represent non-distant illumination, or using a network to fit indirect light
modeling without a solid basis, can lead to an undesirable decomposition
between lighting and material. To address this, we propose a fully
differentiable framework named neural ambient illumination (NeAI) that uses
Neural Radiance Fields (NeRF) as a lighting model to handle complex lighting in
a physically based way. Together with integral lobe encoding for
roughness-adaptive specular lobe and leveraging the pre-convoluted background
for accurate decomposition, the proposed method represents a significant step
towards integrating physically based rendering into the NeRF representation.
The experiments demonstrate the superior performance of novel-view rendering
compared to previous works, and the capability to re-render objects under
arbitrary NeRF-style environments opens up exciting possibilities for bridging
the gap between virtual and real-world scenes. The project and supplementary
materials are available at https://yiyuzhuang.github.io/NeAI/.

Comments:
- Project page: <a class="link-external link-https"
  href="https://yiyuzhuang.github.io/NeAI/" rel="external noopener
  nofollow">https://yiyuzhuang.github.io/NeAI/</a>

---

## SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic  Reconstruction of Indoor Scenes

nerf{: .label .label-blue }

2023-04-18 | Yiming Gao, Yan-Pei Cao, Ying Shan | cs.CV | [PDF](http://arxiv.org/pdf/2304.08971v1){: .btn .btn-green }

**Abstract**: Online reconstructing and rendering of large-scale indoor scenes is a
long-standing challenge. SLAM-based methods can reconstruct 3D scene geometry
progressively in real time but can not render photorealistic results. While
NeRF-based methods produce promising novel view synthesis results, their long
offline optimization time and lack of geometric constraints pose challenges to
efficiently handling online input. Inspired by the complementary advantages of
classical 3D reconstruction and NeRF, we thus investigate marrying explicit
geometric representation with NeRF rendering to achieve efficient online
reconstruction and high-quality rendering. We introduce SurfelNeRF, a variant
of neural radiance field which employs a flexible and scalable neural surfel
representation to store geometric attributes and extracted appearance features
from input images. We further extend the conventional surfel-based fusion
scheme to progressively integrate incoming input frames into the reconstructed
global neural scene representation. In addition, we propose a highly-efficient
differentiable rasterization scheme for rendering neural surfel radiance
fields, which helps SurfelNeRF achieve $10\times$ speedups in training and
inference time, respectively. Experimental results show that our method
achieves the state-of-the-art 23.82 PSNR and 29.58 PSNR on ScanNet in
feedforward inference and per-scene optimization settings, respectively.

Comments:
- To appear in CVPR 2023

---

## MoDA: Modeling Deformable 3D Objects from Casual Videos

nerf{: .label .label-blue }

2023-04-17 | Chaoyue Song, Tianyi Chen, Yiwen Chen, Jiacheng Wei, Chuan Sheng Foo, Fayao Liu, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2304.08279v2){: .btn .btn-green }

**Abstract**: In this paper, we focus on the challenges of modeling deformable 3D objects
from casual videos. With the popularity of neural radiance fields (NeRF), many
works extend it to dynamic scenes with a canonical NeRF and a deformation model
that achieves 3D point transformation between the observation space and the
canonical space. Recent works rely on linear blend skinning (LBS) to achieve
the canonical-observation transformation. However, the linearly weighted
combination of rigid transformation matrices is not guaranteed to be rigid. As
a matter of fact, unexpected scale and shear factors often appear. In practice,
using LBS as the deformation model can always lead to skin-collapsing artifacts
for bending or twisting motions. To solve this problem, we propose neural dual
quaternion blend skinning (NeuDBS) to achieve 3D point deformation, which can
perform rigid transformation without skin-collapsing artifacts. In the endeavor
to register 2D pixels across different frames, we establish a correspondence
between canonical feature embeddings that encodes 3D points within the
canonical space, and 2D image features by solving an optimal transport problem.
Besides, we introduce a texture filtering approach for texture rendering that
effectively minimizes the impact of noisy colors outside target deformable
objects. Extensive experiments on real and synthetic datasets show that our
approach can reconstruct 3D models for humans and animals with better
qualitative and quantitative performance than state-of-the-art methods.

---

## NeRF-Loc: Visual Localization with Conditional Neural Radiance Field

nerf{: .label .label-blue }

2023-04-17 | Jianlin Liu, Qiang Nie, Yong Liu, Chengjie Wang | cs.CV | [PDF](http://arxiv.org/pdf/2304.07979v1){: .btn .btn-green }

**Abstract**: We propose a novel visual re-localization method based on direct matching
between the implicit 3D descriptors and the 2D image with transformer. A
conditional neural radiance field(NeRF) is chosen as the 3D scene
representation in our pipeline, which supports continuous 3D descriptors
generation and neural rendering. By unifying the feature matching and the scene
coordinate regression to the same framework, our model learns both
generalizable knowledge and scene prior respectively during two training
stages. Furthermore, to improve the localization robustness when domain gap
exists between training and testing phases, we propose an appearance adaptation
layer to explicitly align styles between the 3D model and the query image.
Experiments show that our method achieves higher localization accuracy than
other learning-based approaches on multiple benchmarks. Code is available at
\url{https://github.com/JenningsL/nerf-loc}.

Comments:
- accepted by ICRA 2023

---

## Likelihood-Based Generative Radiance Field with Latent Space  Energy-Based Model for 3D-Aware Disentangled Image Representation

nerf{: .label .label-blue }

2023-04-16 | Yaxuan Zhu, Jianwen Xie, Ping Li | cs.CV | [PDF](http://arxiv.org/pdf/2304.07918v1){: .btn .btn-green }

**Abstract**: We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image
generative model that incorporates 3D representation via Neural Radiance Fields
(NeRF) and 2D imaging process via differentiable volume rendering. The model
represents an image as a rendering process from 3D object to 2D image and is
conditioned on some latent variables that account for object characteristics
and are assumed to follow informative trainable energy-based prior models. We
propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i)
maximum likelihood estimation with Markov chain Monte Carlo-based inference and
(ii) variational inference with the reparameterization trick. We study our
models in the scenarios with both known and unknown camera poses. Experiments
on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D
object structures from 2D images, generate 2D images with novel views and
objects, learn from incomplete 2D images, and learn from 2D images with known
or unknown camera poses.

---

## SeaThru-NeRF: Neural Radiance Fields in Scattering Media

nerf{: .label .label-blue }

2023-04-16 | Deborah Levy, Amit Peleg, Naama Pearl, Dan Rosenbaum, Derya Akkaynak, Simon Korman, Tali Treibitz | cs.CV | [PDF](http://arxiv.org/pdf/2304.07743v1){: .btn .btn-green }

**Abstract**: Research on neural radiance fields (NeRFs) for novel view generation is
exploding with new models and extensions. However, a question that remains
unanswered is what happens in underwater or foggy scenes where the medium
strongly influences the appearance of objects. Thus far, NeRF and its variants
have ignored these cases. However, since the NeRF framework is based on
volumetric rendering, it has inherent capability to account for the medium's
effects, once modeled appropriately. We develop a new rendering model for NeRFs
in scattering media, which is based on the SeaThru image formation model, and
suggest a suitable architecture for learning both scene information and medium
parameters. We demonstrate the strength of our method using simulated and
real-world scenes, correctly rendering novel photorealistic views underwater.
Even more excitingly, we can render clear views of these scenes, removing the
medium between the camera and the scene and reconstructing the appearance and
depth of far objects, which are severely occluded by the medium. Our code and
unique datasets are available on the project's website.

---

## CAT-NeRF: Constancy-Aware Tx$^2$Former for Dynamic Body Modeling

nerf{: .label .label-blue }

2023-04-16 | Haidong Zhu, Zhaoheng Zheng, Wanrong Zheng, Ram Nevatia | cs.CV | [PDF](http://arxiv.org/pdf/2304.07915v1){: .btn .btn-green }

**Abstract**: This paper addresses the problem of human rendering in the video with
temporal appearance constancy. Reconstructing dynamic body shapes with
volumetric neural rendering methods, such as NeRF, requires finding the
correspondence of the points in the canonical and observation space, which
demands understanding human body shape and motion. Some methods use rigid
transformation, such as SE(3), which cannot precisely model each frame's unique
motion and muscle movements. Others generate the transformation for each frame
with a trainable network, such as neural blend weight field or translation
vector field, which does not consider the appearance constancy of general body
shape. In this paper, we propose CAT-NeRF for self-awareness of appearance
constancy with Tx$^2$Former, a novel way to combine two Transformer layers, to
separate appearance constancy and uniqueness. Appearance constancy models the
general shape across the video, and uniqueness models the unique patterns for
each frame. We further introduce a novel Covariance Loss to limit the
correlation between each pair of appearance uniquenesses to ensure the
frame-unique pattern is maximally captured in appearance uniqueness. We assess
our method on H36M and ZJU-MoCap and show state-of-the-art performance.

---

## UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose  rendering, Geometry and Texture Editing

nerf{: .label .label-blue }

2023-04-14 | Jinlong Fan, Jing Zhang, Dacheng Tao | cs.CV | [PDF](http://arxiv.org/pdf/2304.06969v1){: .btn .btn-green }

**Abstract**: Neural radiance field (NeRF) has become a popular 3D representation method
for human avatar reconstruction due to its high-quality rendering capabilities,
e.g., regarding novel views and poses. However, previous methods for editing
the geometry and appearance of the avatar only allow for global editing through
body shape parameters and 2D texture maps. In this paper, we propose a new
approach named \textbf{U}nified \textbf{V}olumetric \textbf{A}vatar
(\textbf{UVA}) that enables local and independent editing of both geometry and
texture, while retaining the ability to render novel views and poses. UVA
transforms each observation point to a canonical space using a skinning motion
field and represents geometry and texture in separate neural fields. Each field
is composed of a set of structured latent codes that are attached to anchor
nodes on a deformable mesh in canonical space and diffused into the entire
space via interpolation, allowing for local editing. To address spatial
ambiguity in code interpolation, we use a local signed height indicator. We
also replace the view-dependent radiance color with a pose-dependent shading
factor to better represent surface illumination in different poses. Experiments
on multiple human avatars demonstrate that our UVA achieves competitive results
in novel view synthesis and novel pose rendering while enabling local and
independent editing of geometry and appearance. The source code will be
released.

---

## Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and  Reconstruction

nerf{: .label .label-blue }

2023-04-13 | Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, Hao Su | cs.CV | [PDF](http://arxiv.org/pdf/2304.06714v4){: .btn .btn-green }

**Abstract**: 3D-aware image synthesis encompasses a variety of tasks, such as scene
generation and novel view synthesis from images. Despite numerous task-specific
methods, developing a comprehensive model remains challenging. In this paper,
we present SSDNeRF, a unified approach that employs an expressive diffusion
model to learn a generalizable prior of neural radiance fields (NeRF) from
multi-view images of diverse objects. Previous studies have used two-stage
approaches that rely on pretrained NeRFs as real data to train diffusion
models. In contrast, we propose a new single-stage training paradigm with an
end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent
diffusion model, enabling simultaneous 3D reconstruction and prior learning,
even from sparsely available views. At test time, we can directly sample the
diffusion prior for unconditional generation, or combine it with arbitrary
observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates
robust results comparable to or better than leading task-specific methods in
unconditional generation and single/sparse-view 3D reconstruction.

Comments:
- ICCV 2023 final version. Project page:
  https://lakonik.github.io/ssdnerf

---

## Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields

nerf{: .label .label-blue }

2023-04-13 | Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman | cs.CV | [PDF](http://arxiv.org/pdf/2304.06706v3){: .btn .btn-green }

**Abstract**: Neural Radiance Field training can be accelerated through the use of
grid-based representations in NeRF's learned mapping from spatial coordinates
to colors and volumetric density. However, these grid-based approaches lack an
explicit understanding of scale and therefore often introduce aliasing, usually
in the form of jaggies or missing scene content. Anti-aliasing has previously
been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone
rather than points along a ray, but this approach is not natively compatible
with current grid-based techniques. We show how ideas from rendering and signal
processing can be used to construct a technique that combines mip-NeRF 360 and
grid-based models such as Instant NGP to yield error rates that are 8% - 77%
lower than either prior technique, and that trains 24x faster than mip-NeRF
360.

Comments:
- Project page: https://jonbarron.info/zipnerf/

---

## NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry  Scaffolds

nerf{: .label .label-blue }

2023-04-13 | Chen Yang, Peihao Li, Zanwei Zhou, Shanxin Yuan, Bingbing Liu, Xiaokang Yang, Weichao Qiu, Wei Shen | cs.CV | [PDF](http://arxiv.org/pdf/2304.06287v2){: .btn .btn-green }

**Abstract**: We present NeRFVS, a novel neural radiance fields (NeRF) based method to
enable free navigation in a room. NeRF achieves impressive performance in
rendering images for novel views similar to the input views while suffering for
novel views that are significantly different from the training views. To
address this issue, we utilize the holistic priors, including pseudo depth maps
and view coverage information, from neural reconstruction to guide the learning
of implicit neural representations of 3D indoor scenes. Concretely, an
off-the-shelf neural reconstruction method is leveraged to generate a geometry
scaffold. Then, two loss functions based on the holistic priors are proposed to
improve the learning of NeRF: 1) A robust depth loss that can tolerate the
error of the pseudo depth map to guide the geometry learning of NeRF; 2) A
variance loss to regularize the variance of implicit neural representations to
reduce the geometry and color ambiguity in the learning procedure. These two
loss functions are modulated during NeRF optimization according to the view
coverage information to reduce the negative influence brought by the view
coverage imbalance. Extensive results demonstrate that our NeRFVS outperforms
state-of-the-art view synthesis methods quantitatively and qualitatively on
indoor scenes, achieving high-fidelity free navigation results.

Comments:
- 10 pages, 7 figures

---

## RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields



2023-04-12 | Xiao Han, Houxuan Liu, Yunchao Ding, Lu Yang | cs.RO | [PDF](http://arxiv.org/pdf/2304.05735v2){: .btn .btn-green }

**Abstract**: Accurate perception of objects in the environment is important for improving
the scene understanding capability of SLAM systems. In robotic and augmented
reality applications, object maps with semantic and metric information show
attractive advantages. In this paper, we present RO-MAP, a novel multi-object
mapping pipeline that does not rely on 3D priors. Given only monocular input,
we use neural radiance fields to represent objects and couple them with a
lightweight object SLAM based on multi-view geometry, to simultaneously
localize objects and implicitly learn their dense geometry. We create separate
implicit models for each detected object and train them dynamically and in
parallel as new observations are added. Experiments on synthetic and real-world
datasets demonstrate that our method can generate semantic object map with
shape reconstruction, and be competitive with offline methods while achieving
real-time performance (25Hz). The code and dataset will be available at:
https://github.com/XiaoHan-Git/RO-MAP

Comments:
- The code and dataset are available at:
  https://github.com/XiaoHan-Git/RO-MAP

---

## NutritionVerse-Thin: An Optimized Strategy for Enabling Improved  Rendering of 3D Thin Food Models

nerf{: .label .label-blue }

2023-04-12 | Chi-en Amy Tai, Jason Li, Sriram Kumar, Saeejith Nair, Yuhao Chen, Pengcheng Xi, Alexander Wong | cs.CV | [PDF](http://arxiv.org/pdf/2304.05620v1){: .btn .btn-green }

**Abstract**: With the growth in capabilities of generative models, there has been growing
interest in using photo-realistic renders of common 3D food items to improve
downstream tasks such as food printing, nutrition prediction, or management of
food wastage. Despite 3D modelling capabilities being more accessible than ever
due to the success of NeRF based view-synthesis, such rendering methods still
struggle to correctly capture thin food objects, often generating meshes with
significant holes. In this study, we present an optimized strategy for enabling
improved rendering of thin 3D food models, and demonstrate qualitative
improvements in rendering quality. Our method generates the 3D model mesh via a
proposed thin-object-optimized differentiable reconstruction method and tailors
the strategy at both the data collection and training stages to better handle
thin objects. While simple, we find that this technique can be employed for
quick and highly consistent capturing of thin 3D objects.

---

## Improving Neural Radiance Fields with Depth-aware Optimization for Novel  View Synthesis

nerf{: .label .label-blue }

2023-04-11 | Shu Chen, Junyao Li, Yang Zhang, Beiji Zou | cs.CV | [PDF](http://arxiv.org/pdf/2304.05218v1){: .btn .btn-green }

**Abstract**: With dense inputs, Neural Radiance Fields (NeRF) is able to render
photo-realistic novel views under static conditions. Although the synthesis
quality is excellent, existing NeRF-based methods fail to obtain moderate
three-dimensional (3D) structures. The novel view synthesis quality drops
dramatically given sparse input due to the implicitly reconstructed inaccurate
3D-scene structure. We propose SfMNeRF, a method to better synthesize novel
views as well as reconstruct the 3D-scene geometry. SfMNeRF leverages the
knowledge from the self-supervised depth estimation methods to constrain the
3D-scene geometry during view synthesis training. Specifically, SfMNeRF employs
the epipolar, photometric consistency, depth smoothness, and
position-of-matches constraints to explicitly reconstruct the 3D-scene
structure. Through these explicit constraints and the implicit constraint from
NeRF, our method improves the view synthesis as well as the 3D-scene geometry
performance of NeRF at the same time. In addition, SfMNeRF synthesizes novel
sub-pixels in which the ground truth is obtained by image interpolation. This
strategy enables SfMNeRF to include more samples to improve generalization
performance. Experiments on two public datasets demonstrate that SfMNeRF
surpasses state-of-the-art approaches. Code is available at
https://github.com/XTU-PR-LAB/SfMNeRF

---

## One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural  Radiance Field

nerf{: .label .label-blue }

2023-04-11 | Weichuang Li, Longhao Zhang, Dong Wang, Bin Zhao, Zhigang Wang, Mulin Chen, Bang Zhang, Zhongjian Wang, Liefeng Bo, Xuelong Li | cs.CV | [PDF](http://arxiv.org/pdf/2304.05097v1){: .btn .btn-green }

**Abstract**: Talking head generation aims to generate faces that maintain the identity
information of the source image and imitate the motion of the driving image.
Most pioneering methods rely primarily on 2D representations and thus will
inevitably suffer from face distortion when large head rotations are
encountered. Recent works instead employ explicit 3D structural representations
or implicit neural rendering to improve performance under large pose changes.
Nevertheless, the fidelity of identity and expression is not so desirable,
especially for novel-view synthesis. In this paper, we propose HiDe-NeRF, which
achieves high-fidelity and free-view talking-head synthesis. Drawing on the
recently proposed Deformable Neural Radiance Fields, HiDe-NeRF represents the
3D dynamic scene into a canonical appearance field and an implicit deformation
field, where the former comprises the canonical source face and the latter
models the driving pose and expression. In particular, we improve fidelity from
two aspects: (i) to enhance identity expressiveness, we design a generalized
appearance module that leverages multi-scale volume features to preserve face
shape and details; (ii) to improve expression preciseness, we propose a
lightweight deformation module that explicitly decouples the pose and
expression to enable precise expression modeling. Extensive experiments
demonstrate that our proposed approach can generate better results than
previous works. Project page: https://www.waytron.net/hidenerf/

Comments:
- Accepted by CVPR 2023

---

## MRVM-NeRF: Mask-Based Pretraining for Neural Radiance Fields

nerf{: .label .label-blue }

2023-04-11 | Ganlin Yang, Guoqiang Wei, Zhizheng Zhang, Yan Lu, Dong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2304.04962v1){: .btn .btn-green }

**Abstract**: Most Neural Radiance Fields (NeRFs) have poor generalization ability,
limiting their application when representing multiple scenes by a single model.
To ameliorate this problem, existing methods simply condition NeRF models on
image features, lacking the global understanding and modeling of the entire 3D
scene. Inspired by the significant success of mask-based modeling in other
research fields, we propose a masked ray and view modeling method for
generalizable NeRF (MRVM-NeRF), the first attempt to incorporate mask-based
pretraining into 3D implicit representations. Specifically, considering that
the core of NeRFs lies in modeling 3D representations along the rays and across
the views, we randomly mask a proportion of sampled points along the ray at
fine stage by discarding partial information obtained from multi-viewpoints,
targeting at predicting the corresponding features produced in the coarse
branch. In this way, the learned prior knowledge of 3D scenes during
pretraining helps the model generalize better to novel scenarios after
finetuning. Extensive experiments demonstrate the superiority of our proposed
MRVM-NeRF under various synthetic and real-world settings, both qualitatively
and quantitatively. Our empirical studies reveal the effectiveness of our
proposed innovative MRVM which is specifically designed for NeRF models.

---

## Neural Image-based Avatars: Generalizable Radiance Fields for Human  Avatar Modeling

nerf{: .label .label-blue }

2023-04-10 | Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs | cs.CV | [PDF](http://arxiv.org/pdf/2304.04897v1){: .btn .btn-green }

**Abstract**: We present a method that enables synthesizing novel views and novel poses of
arbitrary human performers from sparse multi-view images. A key ingredient of
our method is a hybrid appearance blending module that combines the advantages
of the implicit body NeRF representation and image-based rendering. Existing
generalizable human NeRF methods that are conditioned on the body model have
shown robustness against the geometric variation of arbitrary human performers.
Yet they often exhibit blurry results when generalized onto unseen identities.
Meanwhile, image-based rendering shows high-quality results when sufficient
observations are available, whereas it suffers artifacts in sparse-view
settings. We propose Neural Image-based Avatars (NIA) that exploits the best of
those two methods: to maintain robustness under new articulations and
self-occlusions while directly leveraging the available (sparse) source view
colors to preserve appearance details of new subject identities. Our hybrid
design outperforms recent methods on both in-domain identity generalization as
well as challenging cross-dataset generalization settings. Also, in terms of
the pose generalization, our method outperforms even the per-subject optimized
animatable NeRF methods. The video results are available at
https://youngjoongunc.github.io/nia

---

## Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos

nerf{: .label .label-blue }

2023-04-10 | Liao Wang, Qiang Hu, Qihan He, Ziyu Wang, Jingyi Yu, Tinne Tuytelaars, Lan Xu, Minye Wu | cs.CV | [PDF](http://arxiv.org/pdf/2304.04452v2){: .btn .btn-green }

**Abstract**: The success of the Neural Radiance Fields (NeRFs) for modeling and free-view
rendering static objects has inspired numerous attempts on dynamic scenes.
Current techniques that utilize neural rendering for facilitating free-view
videos (FVVs) are restricted to either offline rendering or are capable of
processing only brief sequences with minimal motion. In this paper, we present
a novel technique, Residual Radiance Field or ReRF, as a highly compact neural
representation to achieve real-time FVV rendering on long-duration dynamic
scenes. ReRF explicitly models the residual information between adjacent
timestamps in the spatial-temporal feature space, with a global
coordinate-based tiny MLP as the feature decoder. Specifically, ReRF employs a
compact motion grid along with a residual feature grid to exploit inter-frame
feature similarities. We show such a strategy can handle large motions without
sacrificing quality. We further present a sequential training scheme to
maintain the smoothness and the sparsity of the motion/residual grids. Based on
ReRF, we design a special FVV codec that achieves three orders of magnitudes
compression rate and provides a companion ReRF player to support online
streaming of long-duration FVVs of dynamic scenes. Extensive experiments
demonstrate the effectiveness of ReRF for compactly representing dynamic
radiance fields, enabling an unprecedented free-viewpoint viewing experience in
speed and quality.

Comments:
- Accepted by CVPR 2023. Project page, see
  https://aoliao12138.github.io/ReRF/

---

## Inferring Fluid Dynamics via Inverse Rendering

nerf{: .label .label-blue }

2023-04-10 | Jinxian Liu, Ye Chen, Bingbing Ni, Jiyao Mao, Zhenbo Yu | cs.CV | [PDF](http://arxiv.org/pdf/2304.04446v1){: .btn .btn-green }

**Abstract**: Humans have a strong intuitive understanding of physical processes such as
fluid falling by just a glimpse of such a scene picture, i.e., quickly derived
from our immersive visual experiences in memory. This work achieves such a
photo-to-fluid-dynamics reconstruction functionality learned from unannotated
videos, without any supervision of ground-truth fluid dynamics. In a nutshell,
a differentiable Euler simulator modeled with a ConvNet-based pressure
projection solver, is integrated with a volumetric renderer, supporting
end-to-end/coherent differentiable dynamic simulation and rendering. By
endowing each sampled point with a fluid volume value, we derive a NeRF-like
differentiable renderer dedicated from fluid data; and thanks to this
volume-augmented representation, fluid dynamics could be inversely inferred
from the error signal between the rendered result and ground-truth video frame
(i.e., inverse rendering). Experiments on our generated Fluid Fall datasets and
DPI Dam Break dataset are conducted to demonstrate both effectiveness and
generalization ability of our method.

---

## Instance Neural Radiance Field

nerf{: .label .label-blue }

2023-04-10 | Yichen Liu, Benran Hu, Junkai Huang, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2304.04395v3){: .btn .btn-green }

**Abstract**: This paper presents one of the first learning-based NeRF 3D instance
segmentation pipelines, dubbed as Instance Neural Radiance Field, or Instance
NeRF. Taking a NeRF pretrained from multi-view RGB images as input, Instance
NeRF can learn 3D instance segmentation of a given scene, represented as an
instance field component of the NeRF model. To this end, we adopt a 3D
proposal-based mask prediction network on the sampled volumetric features from
NeRF, which generates discrete 3D instance masks. The coarse 3D mask prediction
is then projected to image space to match 2D segmentation masks from different
views generated by existing panoptic segmentation models, which are used to
supervise the training of the instance field. Notably, beyond generating
consistent 2D segmentation maps from novel views, Instance NeRF can query
instance information at any 3D point, which greatly enhances NeRF object
segmentation and manipulation. Our method is also one of the first to achieve
such results in pure inference. Experimented on synthetic and real-world NeRF
datasets with complex indoor scenes, Instance NeRF surpasses previous NeRF
segmentation works and competitive 2D segmentation methods in segmentation
performance on unseen views. Watch the demo video at
https://youtu.be/wW9Bme73coI. Code and data are available at
https://github.com/lyclyc52/Instance_NeRF.

Comments:
- International Conference on Computer Vision (ICCV) 2023

---

## NeRF applied to satellite imagery for surface reconstruction

nerf{: .label .label-blue }

2023-04-09 | Federico Semeraro, Yi Zhang, Wenying Wu, Patrick Carroll | cs.CV | [PDF](http://arxiv.org/pdf/2304.04133v4){: .btn .btn-green }

**Abstract**: We present Surf-NeRF, a modified implementation of the recently introduced
Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize
novel views from a sparse set of satellite images of a scene, while accounting
for the variation in lighting present in the pictures. The trained model can
also be used to accurately estimate the surface elevation of the scene, which
is often a desirable quantity for satellite observation applications. S-NeRF
improves on the standard Neural Radiance Field (NeRF) method by considering the
radiance as a function of the albedo and the irradiance. Both these quantities
are output by fully connected neural network branches of the model, and the
latter is considered as a function of the direct light from the sun and the
diffuse color from the sky. The implementations were run on a dataset of
satellite images, augmented using a zoom-and-crop technique. A hyperparameter
study for NeRF was carried out, leading to intriguing observations on the
model's convergence. Finally, both NeRF and S-NeRF were run until 100k epochs
in order to fully fit the data and produce their best possible predictions. The
code related to this article can be found at
https://github.com/fsemerar/surfnerf.

---

## PVD-AL: Progressive Volume Distillation with Active Learning for  Efficient Conversion Between Different NeRF Architectures

nerf{: .label .label-blue }

2023-04-08 | Shuangkang Fang, Yufeng Wang, Yi Yang, Weixin Xu, Heng Wang, Wenrui Ding, Shuchang Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2304.04012v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have been widely adopted as practical and
versatile representations for 3D scenes, facilitating various downstream tasks.
However, different architectures, including plain Multi-Layer Perceptron (MLP),
Tensors, low-rank Tensors, Hashtables, and their compositions, have their
trade-offs. For instance, Hashtables-based representations allow for faster
rendering but lack clear geometric meaning, making spatial-relation-aware
editing challenging. To address this limitation and maximize the potential of
each architecture, we propose Progressive Volume Distillation with Active
Learning (PVD-AL), a systematic distillation method that enables any-to-any
conversions between different architectures. PVD-AL decomposes each structure
into two parts and progressively performs distillation from shallower to deeper
volume representation, leveraging effective information retrieved from the
rendering process. Additionally, a Three-Levels of active learning technique
provides continuous feedback during the distillation process, resulting in
high-performance results. Empirical evidence is presented to validate our
method on multiple benchmark datasets. For example, PVD-AL can distill an
MLP-based model from a Hashtables-based model at a 10~20X faster speed and
0.8dB~2dB higher PSNR than training the NeRF model from scratch. Moreover,
PVD-AL permits the fusion of diverse features among distinct structures,
enabling models with multiple editing properties and providing a more efficient
model to meet real-time requirements. Project website:http://sk-fun.fun/PVD-AL.

Comments:
- Project website: http://sk-fun.fun/PVD-AL. arXiv admin note:
  substantial text overlap with arXiv:2211.15977

---

## Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative  Radiance Field

nerf{: .label .label-blue }

2023-04-07 | Leheng Li, Qing Lian, Luozhou Wang, Ningning Ma, Ying-Cong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2304.03526v1){: .btn .btn-green }

**Abstract**: This work explores the use of 3D generative models to synthesize training
data for 3D vision tasks. The key requirements of the generative models are
that the generated data should be photorealistic to match the real-world
scenarios, and the corresponding 3D attributes should be aligned with given
sampling labels. However, we find that the recent NeRF-based 3D GANs hardly
meet the above requirements due to their designed generation pipeline and the
lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted
2D-to-3D generation framework to achieve the data generation objectives. Lift3D
has several merits compared to prior methods: (1) Unlike previous 3D GANs that
the output resolution is fixed after training, Lift3D can generalize to any
camera intrinsic with higher resolution and photorealistic output. (2) By
lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D
information of generated objects, thus offering accurate 3D annotations for
downstream tasks. We evaluate the effectiveness of our framework by augmenting
autonomous driving datasets. Experimental results demonstrate that our data
generation framework can effectively improve the performance of 3D object
detectors. Project page: https://len-li.github.io/lift3d-web.

Comments:
- CVPR 2023

---

## Event-based Camera Tracker by $\nabla$t NeRF

nerf{: .label .label-blue }

2023-04-07 | Mana Masuda, Yusuke Sekikawa, Hideo Saito | cs.CV | [PDF](http://arxiv.org/pdf/2304.04559v1){: .btn .btn-green }

**Abstract**: When a camera travels across a 3D world, only a fraction of pixel value
changes; an event-based camera observes the change as sparse events. How can we
utilize sparse events for efficient recovery of the camera pose? We show that
we can recover the camera pose by minimizing the error between sparse events
and the temporal gradient of the scene represented as a neural radiance field
(NeRF). To enable the computation of the temporal gradient of the scene, we
augment NeRF's camera pose as a time function. When the input pose to the NeRF
coincides with the actual pose, the output of the temporal gradient of NeRF
equals the observed intensity changes on the event's points. Using this
principle, we propose an event-based camera pose tracking framework called
TeGRA which realizes the pose update by using the sparse event's observation.
To the best of our knowledge, this is the first camera pose estimation
algorithm using the scene's implicit representation and the sparse intensity
change from events.

---

## Beyond NeRF Underwater: Learning Neural Reflectance Fields for True  Color Correction of Marine Imagery

nerf{: .label .label-blue }

2023-04-06 | Tianyi Zhang, Matthew Johnson-Roberson | cs.CV | [PDF](http://arxiv.org/pdf/2304.03384v2){: .btn .btn-green }

**Abstract**: Underwater imagery often exhibits distorted coloration as a result of
light-water interactions, which complicates the study of benthic environments
in marine biology and geography. In this research, we propose an algorithm to
restore the true color (albedo) in underwater imagery by jointly learning the
effects of the medium and neural scene representations. Our approach models
water effects as a combination of light attenuation with distance and
backscattered light. The proposed neural scene representation is based on a
neural reflectance field model, which learns albedos, normals, and volume
densities of the underwater environment. We introduce a logistic regression
model to separate water from the scene and apply distinct light physics during
training. Our method avoids the need to estimate complex backscatter effects in
water by employing several approximations, enhancing sampling efficiency and
numerical stability during training. The proposed technique integrates
underwater light effects into a volume rendering framework with end-to-end
differentiability. Experimental results on both synthetic and real-world data
demonstrate that our method effectively restores true color from underwater
imagery, outperforming existing approaches in terms of color consistency.

Comments:
- Robotics and Automation Letters (RA-L) VOL. 8, NO. 10, OCTOBER 2023

---

## LANe: Lighting-Aware Neural Fields for Compositional Scene Synthesis

nerf{: .label .label-blue }

2023-04-06 | Akshay Krishnan, Amit Raj, Xianling Zhang, Alexandra Carlson, Nathan Tseng, Sandhya Sridhar, Nikita Jaipuria, James Hays | cs.CV | [PDF](http://arxiv.org/pdf/2304.03280v1){: .btn .btn-green }

**Abstract**: Neural fields have recently enjoyed great success in representing and
rendering 3D scenes. However, most state-of-the-art implicit representations
model static or dynamic scenes as a whole, with minor variations. Existing work
on learning disentangled world and object neural fields do not consider the
problem of composing objects into different world neural fields in a
lighting-aware manner. We present Lighting-Aware Neural Field (LANe) for the
compositional synthesis of driving scenes in a physically consistent manner.
Specifically, we learn a scene representation that disentangles the static
background and transient elements into a world-NeRF and class-specific
object-NeRFs to allow compositional synthesis of multiple objects in the scene.
Furthermore, we explicitly designed both the world and object models to handle
lighting variation, which allows us to compose objects into scenes with
spatially varying lighting. This is achieved by constructing a light field of
the scene and using it in conjunction with a learned shader to modulate the
appearance of the object NeRFs. We demonstrate the performance of our model on
a synthetic dataset of diverse lighting conditions rendered with the CARLA
simulator, as well as a novel real-world dataset of cars collected at different
times of the day. Our approach shows that it outperforms state-of-the-art
compositional scene synthesis on the challenging dataset setup, via composing
object-NeRFs learned from one scene into an entirely different scene whilst
still respecting the lighting variations in the novel scene. For more results,
please visit our project website https://lane-composition.github.io/.

Comments:
- Project website: https://lane-composition.github.io

---

## Neural Fields meet Explicit Geometric Representation for Inverse  Rendering of Urban Scenes

nerf{: .label .label-blue }

2023-04-06 | Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, Sanja Fidler | cs.CV | [PDF](http://arxiv.org/pdf/2304.03266v1){: .btn .btn-green }

**Abstract**: Reconstruction and intrinsic decomposition of scenes from captured imagery
would enable many applications such as relighting and virtual object insertion.
Recent NeRF based methods achieve impressive fidelity of 3D reconstruction, but
bake the lighting and shadows into the radiance field, while mesh-based methods
that facilitate intrinsic decomposition through differentiable rendering have
not yet scaled to the complexity and scale of outdoor scenes. We present a
novel inverse rendering framework for large urban scenes capable of jointly
reconstructing the scene geometry, spatially-varying materials, and HDR
lighting from a set of posed RGB images with optional depth. Specifically, we
use a neural field to account for the primary rays, and use an explicit mesh
(reconstructed from the underlying neural field) for modeling secondary rays
that produce higher-order lighting effects such as cast shadows. By faithfully
disentangling complex geometry and materials from lighting effects, our method
enables photorealistic relighting with specular and shadow effects on several
outdoor datasets. Moreover, it supports physics-based scene manipulations such
as virtual object insertion with ray-traced shadow casting.

Comments:
- CVPR 2023. Project page: https://nv-tlabs.github.io/fegr/

---

## DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model

nerf{: .label .label-blue }

2023-04-06 | Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun | cs.CV | [PDF](http://arxiv.org/pdf/2304.02827v1){: .btn .btn-green }

**Abstract**: The increasing demand for high-quality 3D content creation has motivated the
development of automated methods for creating 3D object models from a single
image and/or from a text prompt. However, the reconstructed 3D objects using
state-of-the-art image-to-3D methods still exhibit low correspondence to the
given image and low multi-view consistency. Recent state-of-the-art text-to-3D
methods are also limited, yielding 3D samples with low diversity per prompt
with long synthesis time. To address these challenges, we propose DITTO-NeRF, a
novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a
single image. Our DITTO-NeRF consists of constructing high-quality partial 3D
object for limited in-boundary (IB) angles using the given or text-generated 2D
image from the frontal view and then iteratively reconstructing the remaining
3D NeRF using inpainting latent diffusion model. We propose progressive 3D
object reconstruction schemes in terms of scales (low to high resolution),
angles (IB angles initially to outer-boundary (OB) later), and masks (object to
background boundary) in our DITTO-NeRF so that high-quality information on IB
can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods
in terms of fidelity and diversity qualitatively and quantitatively with much
faster training times than prior arts on image/text-to-3D such as DreamFusion,
and NeuralLift-360.

Comments:
- Project page: https://janeyeon.github.io/ditto-nerf/

---

## Image Stabilization for Hololens Camera in Remote Collaboration

nerf{: .label .label-blue }

2023-04-05 | Gowtham Senthil, Siva Vignesh Krishnan, Annamalai Lakshmanan, Florence Kissling | cs.CV | [PDF](http://arxiv.org/pdf/2304.02736v1){: .btn .btn-green }

**Abstract**: With the advent of new technologies, Augmented Reality (AR) has become an
effective tool in remote collaboration. Narrow field-of-view (FoV) and motion
blur can offer an unpleasant experience with limited cognition for remote
viewers of AR headsets. In this article, we propose a two-stage pipeline to
tackle this issue and ensure a stable viewing experience with a larger FoV. The
solution involves an offline 3D reconstruction of the indoor environment,
followed by enhanced rendering using only the live poses of AR device. We
experiment with and evaluate the two different 3D reconstruction methods, RGB-D
geometric approach and Neural Radiance Fields (NeRF), based on their data
requirements, reconstruction quality, rendering, and training times. The
generated sequences from these methods had smoother transitions and provided a
better perspective of the environment. The geometry-based enhanced FoV method
had better renderings as it lacked blurry outputs making it better than the
other attempted approaches. Structural Similarity Index (SSIM) and Peak Signal
to Noise Ratio (PSNR) metrics were used to quantitatively show that the
rendering quality using the geometry-based enhanced FoV method is better. Link
to the code repository -
https://github.com/MixedRealityETHZ/ImageStabilization.

---

## Generating Continual Human Motion in Diverse 3D Scenes

nerf{: .label .label-blue }

2023-04-04 | Aymen Mir, Xavier Puig, Angjoo Kanazawa, Gerard Pons-Moll | cs.CV | [PDF](http://arxiv.org/pdf/2304.02061v3){: .btn .btn-green }

**Abstract**: We introduce a method to synthesize animator guided human motion across 3D
scenes. Given a set of sparse (3 or 4) joint locations (such as the location of
a person's hand and two feet) and a seed motion sequence in a 3D scene, our
method generates a plausible motion sequence starting from the seed motion
while satisfying the constraints imposed by the provided keypoints. We
decompose the continual motion synthesis problem into walking along paths and
transitioning in and out of the actions specified by the keypoints, which
enables long generation of motions that satisfy scene constraints without
explicitly incorporating scene information. Our method is trained only using
scene agnostic mocap data. As a result, our approach is deployable across 3D
scenes with various geometries. For achieving plausible continual motion
synthesis without drift, our key contribution is to generate motion in a
goal-centric canonical coordinate frame where the next immediate target is
situated at the origin. Our model can generate long sequences of diverse
actions such as grabbing, sitting and leaning chained together in arbitrary
order, demonstrated on scenes of varying geometry: HPS, Replica, Matterport,
ScanNet and scenes represented using NeRFs. Several experiments demonstrate
that our method outperforms existing methods that navigate paths in 3D scenes.

---

## MonoHuman: Animatable Human Neural Field from Monocular Video

nerf{: .label .label-blue }

2023-04-04 | Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, Kwan-Yee Lin | cs.CV | [PDF](http://arxiv.org/pdf/2304.02001v1){: .btn .btn-green }

**Abstract**: Animating virtual avatars with free-view control is crucial for various
applications like virtual reality and digital entertainment. Previous studies
have attempted to utilize the representation power of the neural radiance field
(NeRF) to reconstruct the human body from monocular videos. Recent works
propose to graft a deformation network into the NeRF to further model the
dynamics of the human neural field for animating vivid human motions. However,
such pipelines either rely on pose-dependent representations or fall short of
motion coherency due to frame-independent optimization, making it difficult to
generalize to unseen pose sequences realistically. In this paper, we propose a
novel framework MonoHuman, which robustly renders view-consistent and
high-fidelity avatars under arbitrary novel poses. Our key insight is to model
the deformation field with bi-directional constraints and explicitly leverage
the off-the-peg keyframe information to reason the feature correlations for
coherent results. Specifically, we first propose a Shared Bidirectional
Deformation module, which creates a pose-independent generalizable deformation
field by disentangling backward and forward deformation correspondences into
shared skeletal motion weight and separate non-rigid motions. Then, we devise a
Forward Correspondence Search module, which queries the correspondence feature
of keyframes to guide the rendering network. The rendered results are thus
multi-view consistent with high fidelity, even under challenging novel pose
settings. Extensive experiments demonstrate the superiority of our proposed
MonoHuman over state-of-the-art methods.

Comments:
- 15 pages, 14 figures. Accepted to CVPR 2023. Project page:
  https://yzmblog.github.io/projects/MonoHuman/

---

## Learning Personalized High Quality Volumetric Head Avatars from  Monocular RGB Videos



2023-04-04 | Ziqian Bai, Feitong Tan, Zeng Huang, Kripasindhu Sarkar, Danhang Tang, Di Qiu, Abhimitra Meka, Ruofei Du, Mingsong Dou, Sergio Orts-Escolano, Rohit Pandey, Ping Tan, Thabo Beeler, Sean Fanello, Yinda Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2304.01436v1){: .btn .btn-green }

**Abstract**: We propose a method to learn a high-quality implicit 3D head avatar from a
monocular RGB video captured in the wild. The learnt avatar is driven by a
parametric face model to achieve user-controlled facial expressions and head
poses. Our hybrid pipeline combines the geometry prior and dynamic tracking of
a 3DMM with a neural radiance field to achieve fine-grained control and
photorealism. To reduce over-smoothing and improve out-of-model expressions
synthesis, we propose to predict local features anchored on the 3DMM geometry.
These learnt features are driven by 3DMM deformation and interpolated in 3D
space to yield the volumetric radiance at a designated query point. We further
show that using a Convolutional Neural Network in the UV space is critical in
incorporating spatial context and producing representative local features.
Extensive experiments show that we are able to reconstruct high-quality
avatars, with more accurate expression-dependent details, good generalization
to out-of-training expressions, and quantitatively superior renderings compared
to other state-of-the-art approaches.

Comments:
- In CVPR2023. Project page:
  https://augmentedperception.github.io/monoavatar/

---

## DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via  Diffusion Models

nerf{: .label .label-blue }

2023-04-03 | Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong | cs.CV | [PDF](http://arxiv.org/pdf/2304.00916v3){: .btn .btn-green }

**Abstract**: We present DreamAvatar, a text-and-shape guided framework for generating
high-quality 3D human avatars with controllable poses. While encouraging
results have been reported by recent methods on text-guided 3D common object
generation, generating high-quality human avatars remains an open challenge due
to the complexity of the human body's shape, pose, and appearance. We propose
DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for
predicting density and color for 3D points and pretrained text-to-image
diffusion models for providing 2D self-supervision. Specifically, we leverage
the SMPL model to provide shape and pose guidance for the generation. We
introduce a dual-observation-space design that involves the joint optimization
of a canonical space and a posed space that are related by a learnable
deformation field. This facilitates the generation of more complete textures
and geometry faithful to the target pose. We also jointly optimize the losses
computed from the full body and from the zoomed-in 3D head to alleviate the
common multi-face ''Janus'' problem and improve facial details in the generated
avatars. Extensive evaluations demonstrate that DreamAvatar significantly
outperforms existing methods, establishing a new state-of-the-art for
text-and-shape guided 3D human avatar generation.

Comments:
- Project page: https://yukangcao.github.io/DreamAvatar/

---

## Disorder-invariant Implicit Neural Representation



2023-04-03 | Hao Zhu, Shaowen Xie, Zhen Liu, Fengyi Liu, Qi Zhang, You Zhou, Yi Lin, Zhan Ma, Xun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2304.00837v1){: .btn .btn-green }

**Abstract**: Implicit neural representation (INR) characterizes the attributes of a signal
as a function of corresponding coordinates which emerges as a sharp weapon for
solving inverse problems. However, the expressive power of INR is limited by
the spectral bias in the network training. In this paper, we find that such a
frequency-related problem could be greatly solved by re-arranging the
coordinates of the input signal, for which we propose the disorder-invariant
implicit neural representation (DINER) by augmenting a hash-table to a
traditional INR backbone. Given discrete signals sharing the same histogram of
attributes and different arrangement orders, the hash-table could project the
coordinates into the same distribution for which the mapped signal can be
better modeled using the subsequent INR network, leading to significantly
alleviated spectral bias. Furthermore, the expressive power of the DINER is
determined by the width of the hash-table. Different width corresponds to
different geometrical elements in the attribute space, \textit{e.g.}, 1D curve,
2D curved-plane and 3D curved-volume when the width is set as $1$, $2$ and $3$,
respectively. More covered areas of the geometrical elements result in stronger
expressive power. Experiments not only reveal the generalization of the DINER
for different INR backbones (MLP vs. SIREN) and various tasks (image/video
representation, phase retrieval, refractive index recovery, and neural radiance
field optimization) but also show the superiority over the state-of-the-art
algorithms both in quality and speed. \textit{Project page:}
\url{https://ezio77.github.io/DINER-website/}

Comments:
- Journal extension of the CVPR'23 highlight paper "DINER:
  Disorder-invariant Implicit Neural Representation". In the extension, we
  model the expressive power of the DINER using parametric functions in the
  attribute space. As a result, better results are achieved than the conference
  version. arXiv admin note: substantial text overlap with arXiv:2211.07871

---

## JacobiNeRF: NeRF Shaping with Mutual Information Gradients

nerf{: .label .label-blue }

2023-04-01 | Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, Li Yi, Leonidas Guibas | cs.CV | [PDF](http://arxiv.org/pdf/2304.00341v1){: .btn .btn-green }

**Abstract**: We propose a method that trains a neural radiance field (NeRF) to encode not
only the appearance of the scene but also semantic correlations between scene
points, regions, or entities -- aiming to capture their mutual co-variation
patterns. In contrast to the traditional first-order photometric reconstruction
objective, our method explicitly regularizes the learning dynamics to align the
Jacobians of highly-correlated entities, which proves to maximize the mutual
information between them under random scene perturbations. By paying attention
to this second-order information, we can shape a NeRF to express semantically
meaningful synergies when the network weights are changed by a delta along the
gradient of a single entity, region, or even a point. To demonstrate the merit
of this mutual information modeling, we leverage the coordinated behavior of
scene entities that emerges from our shaping to perform label propagation for
semantic and instance segmentation. Our experiments show that a JacobiNeRF is
more efficient in propagating annotations among 2D pixels and 3D points
compared to NeRFs without mutual information shaping, especially in extremely
sparse label regimes -- thus reducing annotation burden. The same machinery can
further be used for entity selection or scene modifications.

---

## VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence  Normalization

nerf{: .label .label-blue }

2023-03-31 | Bingfan Zhu, Yanchao Yang, Xulong Wang, Youyi Zheng, Leonidas Guibas | cs.CV | [PDF](http://arxiv.org/pdf/2303.17968v1){: .btn .btn-green }

**Abstract**: We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for
better geometry under non-Lambertian surface and dynamic lighting conditions
that cause significant variation in the radiance of a point when viewed from
different angles. Instead of explicitly modeling the underlying factors that
result in the view-dependent phenomenon, which could be complex yet not
inclusive, we develop a simple and effective technique that normalizes the
view-dependence by distilling invariant information already encoded in the
learned NeRFs. We then jointly train NeRFs for view synthesis with
view-dependence normalization to attain quality geometry. Our experiments show
that even though shape-radiance ambiguity is inevitable, the proposed
normalization can minimize its effect on geometry, which essentially aligns the
optimal capacity needed for explaining view-dependent variations. Our method
applies to various baselines and significantly improves geometry without
changing the volume rendering pipeline, even if the data is captured under a
moving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.

---

## NeILF++: Inter-Reflectable Light Fields for Geometry and Material  Estimation

nerf{: .label .label-blue }

2023-03-30 | Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan | cs.CV | [PDF](http://arxiv.org/pdf/2303.17147v1){: .btn .btn-green }

**Abstract**: We present a novel differentiable rendering framework for joint geometry,
material, and lighting estimation from multi-view images. In contrast to
previous methods which assume a simplified environment map or co-located
flashlights, in this work, we formulate the lighting of a static scene as one
neural incident light field (NeILF) and one outgoing neural radiance field
(NeRF). The key insight of the proposed method is the union of the incident and
outgoing light fields through physically-based rendering and inter-reflections
between surfaces, making it possible to disentangle the scene geometry,
material, and lighting from image observations in a physically-based manner.
The proposed incident light and inter-reflection framework can be easily
applied to other NeRF systems. We show that our method can not only decompose
the outgoing radiance into incident lights and surface materials, but also
serve as a surface refinement module that further improves the reconstruction
detail of the neural surface. We demonstrate on several datasets that the
proposed method is able to achieve state-of-the-art results in terms of
geometry reconstruction quality, material estimation accuracy, and the fidelity
of novel view rendering.

Comments:
- Project page: \url{https://yoyo000.github.io/NeILF_pp}

---

## Enhanced Stable View Synthesis

nerf{: .label .label-blue }

2023-03-30 | Nishant Jain, Suryansh Kumar, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2303.17094v1){: .btn .btn-green }

**Abstract**: We introduce an approach to enhance the novel view synthesis from images
taken from a freely moving camera. The introduced approach focuses on outdoor
scenes where recovering accurate geometric scaffold and camera pose is
challenging, leading to inferior results using the state-of-the-art stable view
synthesis (SVS) method. SVS and related methods fail for outdoor scenes
primarily due to (i) over-relying on the multiview stereo (MVS) for geometric
scaffold recovery and (ii) assuming COLMAP computed camera poses as the best
possible estimates, despite it being well-studied that MVS 3D reconstruction
accuracy is limited to scene disparity and camera-pose accuracy is sensitive to
key-point correspondence selection. This work proposes a principled way to
enhance novel view synthesis solutions drawing inspiration from the basics of
multiple view geometry. By leveraging the complementary behavior of MVS and
monocular depth, we arrive at a better scene depth per view for nearby and far
points, respectively. Moreover, our approach jointly refines camera poses with
image-based rendering via multiple rotation averaging graph optimization. The
recovered scene depth and the camera-pose help better view-dependent on-surface
feature aggregation of the entire scene. Extensive evaluation of our approach
on the popular benchmark dataset, such as Tanks and Temples, shows substantial
improvement in view synthesis results compared to the prior art. For instance,
our method shows 1.5 dB of PSNR improvement on the Tank and Temples. Similar
statistics are observed when tested on other benchmark datasets such as FVS,
Mip-NeRF 360, and DTU.

Comments:
- Accepted to IEEE/CVF CVPR 2023. Draft info: 13 pages, 6 Figures, 7
  Tables

---

## SynBody: Synthetic Dataset with Layered Human Models for 3D Human  Perception and Modeling

nerf{: .label .label-blue }

2023-03-30 | Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, Wayne Wu, Chen Qian, Dahua Lin, Ziwei Liu, Lei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2303.17368v2){: .btn .btn-green }

**Abstract**: Synthetic data has emerged as a promising source for 3D human research as it
offers low-cost access to large-scale human datasets. To advance the diversity
and annotation quality of human models, we introduce a new synthetic dataset,
SynBody, with three appealing features: 1) a clothed parametric human model
that can generate a diverse range of subjects; 2) the layered human
representation that naturally offers high-quality 3D annotations to support
multiple tasks; 3) a scalable system for producing realistic data to facilitate
real-world tasks. The dataset comprises 1.2M images with corresponding accurate
3D annotations, covering 10,000 human body models, 1,187 actions, and various
viewpoints. The dataset includes two subsets for human pose and shape
estimation as well as human neural rendering. Extensive experiments on SynBody
indicate that it substantially enhances both SMPL and SMPL-X estimation.
Furthermore, the incorporation of layered annotations offers a valuable
training resource for investigating the Human Neural Radiance Fields (NeRF).

Comments:
- Accepted by ICCV 2023. Project webpage: https://synbody.github.io/

---

## NeRF-Supervised Deep Stereo

nerf{: .label .label-blue }

2023-03-30 | Fabio Tosi, Alessio Tonioni, Daniele De Gregorio, Matteo Poggi | cs.CV | [PDF](http://arxiv.org/pdf/2303.17603v1){: .btn .btn-green }

**Abstract**: We introduce a novel framework for training deep stereo networks effortlessly
and without any ground-truth. By leveraging state-of-the-art neural rendering
solutions, we generate stereo training data from image sequences collected with
a single handheld camera. On top of them, a NeRF-supervised training procedure
is carried out, from which we exploit rendered stereo triplets to compensate
for occlusions and depth maps as proxy labels. This results in stereo networks
capable of predicting sharp and detailed disparity maps. Experimental results
show that models trained under this regime yield a 30-40% improvement over
existing self-supervised methods on the challenging Middlebury dataset, filling
the gap to supervised models and, most times, outperforming them at zero-shot
generalization.

Comments:
- CVPR 2023. Project page: https://nerfstereo.github.io/ Code:
  https://github.com/fabiotosi92/NeRF-Supervised-Deep-Stereo

---

## Instant Neural Radiance Fields Stylization



2023-03-29 | Shaoxu Li, Ye Pan | cs.CV | [PDF](http://arxiv.org/pdf/2303.16884v1){: .btn .btn-green }

**Abstract**: We present Instant Neural Radiance Fields Stylization, a novel approach for
multi-view image stylization for the 3D scene. Our approach models a neural
radiance field based on neural graphics primitives, which use a hash
table-based position encoder for position embedding. We split the position
encoder into two parts, the content and style sub-branches, and train the
network for normal novel view image synthesis with the content and style
targets. In the inference stage, we execute AdaIN to the output features of the
position encoder, with content and style voxel grid features as reference. With
the adjusted features, the stylization of novel view images could be obtained.
Our method extends the style target from style images to image sets of scenes
and does not require additional network training for stylization. Given a set
of images of 3D scenes and a style target(a style image or another set of 3D
scenes), our method can generate stylized novel views with a consistent
appearance at various view angles in less than 10 minutes on modern GPU
hardware. Extensive experimental results demonstrate the validity and
superiority of our method.

---

## TriVol: Point Cloud Rendering via Triple Volumes

nerf{: .label .label-blue }

2023-03-29 | Tao Hu, Xiaogang Xu, Ruihang Chu, Jiaya Jia | cs.CV | [PDF](http://arxiv.org/pdf/2303.16485v1){: .btn .btn-green }

**Abstract**: Existing learning-based methods for point cloud rendering adopt various 3D
representations and feature querying mechanisms to alleviate the sparsity
problem of point clouds. However, artifacts still appear in rendered images,
due to the challenges in extracting continuous and discriminative 3D features
from point clouds. In this paper, we present a dense while lightweight 3D
representation, named TriVol, that can be combined with NeRF to render
photo-realistic images from point clouds. Our TriVol consists of triple slim
volumes, each of which is encoded from the point cloud. TriVol has two
advantages. First, it fuses respective fields at different scales and thus
extracts local and non-local features for discriminative representation.
Second, since the volume size is greatly reduced, our 3D decoder can be
efficiently inferred, allowing us to increase the resolution of the 3D space to
render more point details. Extensive experiments on different benchmarks with
varying kinds of scenes/objects demonstrate our framework's effectiveness
compared with current approaches. Moreover, our framework has excellent
generalization ability to render a category of scenes/objects without
fine-tuning.

---

## Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance  Fields

nerf{: .label .label-blue }

2023-03-29 | Tao Hu, Xiaogang Xu, Shu Liu, Jiaya Jia | cs.CV | [PDF](http://arxiv.org/pdf/2303.16482v1){: .btn .btn-green }

**Abstract**: Synthesizing photo-realistic images from a point cloud is challenging because
of the sparsity of point cloud representation. Recent Neural Radiance Fields
and extensions are proposed to synthesize realistic images from 2D input. In
this paper, we present Point2Pix as a novel point renderer to link the 3D
sparse point clouds with 2D dense image pixels. Taking advantage of the point
cloud 3D prior and NeRF rendering pipeline, our method can synthesize
high-quality images from colored point clouds, generally for novel indoor
scenes. To improve the efficiency of ray sampling, we propose point-guided
sampling, which focuses on valid samples. Also, we present Point Encoding to
build Multi-scale Radiance Fields that provide discriminative 3D point
features. Finally, we propose Fusion Encoding to efficiently synthesize
high-quality images. Extensive experiments on the ScanNet and ArkitScenes
datasets demonstrate the effectiveness and generalization.

---

## Flow supervision for Deformable NeRF

nerf{: .label .label-blue }

2023-03-28 | Chaoyang Wang, Lachlan Ewen MacDonald, Laszlo A. Jeni, Simon Lucey | cs.CV | [PDF](http://arxiv.org/pdf/2303.16333v1){: .btn .btn-green }

**Abstract**: In this paper we present a new method for deformable NeRF that can directly
use optical flow as supervision. We overcome the major challenge with respect
to the computationally inefficiency of enforcing the flow constraints to the
backward deformation field, used by deformable NeRFs. Specifically, we show
that inverting the backward deformation function is actually not needed for
computing scene flows between frames. This insight dramatically simplifies the
problem, as one is no longer constrained to deformation functions that can be
analytically inverted. Instead, thanks to the weak assumptions required by our
derivation based on the inverse function theorem, our approach can be extended
to a broad class of commonly used backward deformation field. We present
results on monocular novel view synthesis with rapid object motion, and
demonstrate significant improvements over baselines without flow supervision.

---

## CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image  Arbitrary-Scale Super Resolution

nerf{: .label .label-blue }

2023-03-28 | Zixuan Chen, Jian-Huang Lai, Lingxiao Yang, Xiaohua Xie | eess.IV | [PDF](http://arxiv.org/pdf/2303.16242v3){: .btn .btn-green }

**Abstract**: Medical image arbitrary-scale super-resolution (MIASSR) has recently gained
widespread attention, aiming to super sample medical volumes at arbitrary
scales via a single model. However, existing MIASSR methods face two major
limitations: (i) reliance on high-resolution (HR) volumes and (ii) limited
generalization ability, which restricts their application in various scenarios.
To overcome these limitations, we propose Cube-based Neural Radiance Field
(CuNeRF), a zero-shot MIASSR framework that can yield medical images at
arbitrary scales and viewpoints in a continuous domain. Unlike existing MIASSR
methods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRF
focuses on building a coordinate-intensity continuous representation from LR
volumes without the need for HR references. This is achieved by the proposed
differentiable modules: including cube-based sampling, isotropic volume
rendering, and cube-based hierarchical rendering. Through extensive experiments
on magnetic resource imaging (MRI) and computed tomography (CT) modalities, we
demonstrate that CuNeRF outperforms state-of-the-art MIASSR methods. CuNeRF
yields better visual verisimilitude and reduces aliasing artifacts at various
upsampling factors. Moreover, our CuNeRF does not need any LR-HR training
pairs, which is more flexible and easier to be used than others. Our code will
be publicly available soon.

Comments:
- This paper is accepted by the International Conference on Computer
  Vision (ICCV) 2023

---

## SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis

nerf{: .label .label-blue }

2023-03-28 | Guangcong Wang, Zhaoxi Chen, Chen Change Loy, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2303.16196v2){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) significantly degrades when only a limited
number of views are available. To complement the lack of 3D information,
depth-based models, such as DSNeRF and MonoSDF, explicitly assume the
availability of accurate depth maps of multiple views. They linearly scale the
accurate depth maps as supervision to guide the predicted depth of few-shot
NeRFs. However, accurate depth maps are difficult and expensive to capture due
to wide-range depth distances in the wild.
  In this work, we present a new Sparse-view NeRF (SparseNeRF) framework that
exploits depth priors from real-world inaccurate observations. The inaccurate
depth observations are either from pre-trained depth models or coarse depth
maps of consumer-level depth sensors. Since coarse depth maps are not strictly
scaled to the ground-truth depth maps, we propose a simple yet effective
constraint, a local depth ranking method, on NeRFs such that the expected depth
ranking of the NeRF is consistent with that of the coarse depth maps in local
patches. To preserve the spatial continuity of the estimated depth of NeRF, we
further propose a spatial continuity constraint to encourage the consistency of
the expected depth continuity of NeRF with coarse depth maps. Surprisingly,
with simple depth ranking constraints, SparseNeRF outperforms all
state-of-the-art few-shot NeRF methods (including depth-based models) on
standard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBD
that contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13
Pro. Extensive experiments on NVS-RGBD dataset also validate the superiority
and generalizability of SparseNeRF. Code and dataset are available at
https://sparsenerf.github.io/.

Comments:
- Accepted by ICCV 2023, Project page: https://sparsenerf.github.io/

---

## Adaptive Voronoi NeRFs

nerf{: .label .label-blue }

2023-03-28 | Tim Elsner, Victor Czech, Julia Berger, Zain Selman, Isaak Lim, Leif Kobbelt | cs.CV | [PDF](http://arxiv.org/pdf/2303.16001v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a set
of registered images. Increasing sizes of a scene demands more complex
functions, typically represented by neural networks, to capture all details.
Training and inference then involves querying the neural network millions of
times per image, which becomes impractically slow. Since such complex functions
can be replaced by multiple simpler functions to improve speed, we show that a
hierarchy of Voronoi diagrams is a suitable choice to partition the scene. By
equipping each Voronoi cell with its own NeRF, our approach is able to quickly
learn a scene representation. We propose an intuitive partitioning of the space
that increases quality gains during training by distributing information evenly
among the networks and avoids artifacts through a top-down adaptive refinement.
Our framework is agnostic to the underlying NeRF method and easy to implement,
which allows it to be applied to various NeRF variants for improved learning
and rendering speeds.

---

## F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera  Trajectories

nerf{: .label .label-blue }

2023-03-28 | Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura, Christian Theobalt, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2303.15951v1){: .btn .btn-green }

**Abstract**: This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF)
for novel view synthesis, which enables arbitrary input camera trajectories and
only costs a few minutes for training. Existing fast grid-based NeRF training
frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed
for bounded scenes and rely on space warping to handle unbounded scenes.
Existing two widely-used space-warping methods are only designed for the
forward-facing trajectory or the 360-degree object-centric trajectory but
cannot process arbitrary trajectories. In this paper, we delve deep into the
mechanism of space warping to handle unbounded scenes. Based on our analysis,
we further propose a novel space-warping method called perspective warping,
which allows us to handle arbitrary trajectories in the grid-based NeRF
framework. Extensive experiments demonstrate that F2-NeRF is able to use the
same perspective warping to render high-quality images on two standard datasets
and a new free trajectory dataset collected by us. Project page:
https://totoro97.github.io/projects/f2-nerf.

Comments:
- CVPR 2023. Project page: https://totoro97.github.io/projects/f2-nerf

---

## VMesh: Hybrid Volume-Mesh Representation for Efficient View Synthesis

nerf{: .label .label-blue }

2023-03-28 | Yuan-Chen Guo, Yan-Pei Cao, Chen Wang, Yu He, Ying Shan, Xiaohu Qie, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2303.16184v1){: .btn .btn-green }

**Abstract**: With the emergence of neural radiance fields (NeRFs), view synthesis quality
has reached an unprecedented level. Compared to traditional mesh-based assets,
this volumetric representation is more powerful in expressing scene geometry
but inevitably suffers from high rendering costs and can hardly be involved in
further processes like editing, posing significant difficulties in combination
with the existing graphics pipeline. In this paper, we present a hybrid
volume-mesh representation, VMesh, which depicts an object with a textured mesh
along with an auxiliary sparse volume. VMesh retains the advantages of
mesh-based assets, such as efficient rendering, compact storage, and easy
editing, while also incorporating the ability to represent subtle geometric
structures provided by the volumetric counterpart. VMesh can be obtained from
multi-view images of an object and renders at 2K 60FPS on common consumer
devices with high fidelity, unleashing new opportunities for real-time
immersive applications.

Comments:
- Project page: https://bennyguo.github.io/vmesh/

---

## JAWS: Just A Wild Shot for Cinematic Transfer in Neural Radiance Fields

nerf{: .label .label-blue }

2023-03-27 | Xi Wang, Robin Courant, Jinglei Shi, Eric Marchand, Marc Christie | cs.CV | [PDF](http://arxiv.org/pdf/2303.15427v1){: .btn .btn-green }

**Abstract**: This paper presents JAWS, an optimization-driven approach that achieves the
robust transfer of visual cinematic features from a reference in-the-wild video
clip to a newly generated clip. To this end, we rely on an
implicit-neural-representation (INR) in a way to compute a clip that shares the
same cinematic features as the reference clip. We propose a general formulation
of a camera optimization problem in an INR that computes extrinsic and
intrinsic camera parameters as well as timing. By leveraging the
differentiability of neural representations, we can back-propagate our designed
cinematic losses measured on proxy estimators through a NeRF network to the
proposed cinematic parameters directly. We also introduce specific enhancements
such as guidance maps to improve the overall quality and efficiency. Results
display the capacity of our system to replicate well known camera sequences
from movies, adapting the framing, camera parameters and timing of the
generated video clip to maximize the similarity with the reference clip.

Comments:
- CVPR 2023. Project page with videos and code:
  http://www.lix.polytechnique.fr/vista/projects/2023_cvpr_wang

---

## Generalizable Neural Voxels for Fast Human Radiance Fields

nerf{: .label .label-blue }

2023-03-27 | Taoran Yi, Jiemin Fang, Xinggang Wang, Wenyu Liu | cs.CV | [PDF](http://arxiv.org/pdf/2303.15387v1){: .btn .btn-green }

**Abstract**: Rendering moving human bodies at free viewpoints only from a monocular video
is quite a challenging problem. The information is too sparse to model
complicated human body structures and motions from both view and pose
dimensions. Neural radiance fields (NeRF) have shown great power in novel view
synthesis and have been applied to human body rendering. However, most current
NeRF-based methods bear huge costs for both training and rendering, which
impedes the wide applications in real-life scenarios. In this paper, we propose
a rendering framework that can learn moving human body structures extremely
quickly from a monocular video. The framework is built by integrating both
neural fields and neural voxels. Especially, a set of generalizable neural
voxels are constructed. With pretrained on various human bodies, these general
voxels represent a basic skeleton and can provide strong geometric priors. For
the fine-tuning process, individual voxels are constructed for learning
differential textures, complementary to general voxels. Thus learning a novel
body can be further accelerated, taking only a few minutes. Our method shows
significantly higher training efficiency compared with previous methods, while
maintaining similar rendering quality. The project page is at
https://taoranyi.com/gneuvox .

Comments:
- Project page: http://taoranyi.com/gneuvox

---

## NeUDF: Learning Unsigned Distance Fields from Multi-view Images for  Reconstructing Non-watertight Models

nerf{: .label .label-blue }

2023-03-27 | Fei Hou, Jukai Deng, Xuhui Chen, Wencheng Wang, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2303.15368v1){: .btn .btn-green }

**Abstract**: Volume rendering-based 3D reconstruction from multi-view images has gained
popularity in recent years, largely due to the success of neural radiance
fields (NeRF). A number of methods have been developed that build upon NeRF and
use neural volume rendering to learn signed distance fields (SDFs) for
reconstructing 3D models. However, SDF-based methods cannot represent
non-watertight models and, therefore, cannot capture open boundaries. This
paper proposes a new algorithm for learning an accurate unsigned distance field
(UDF) from multi-view images, which is specifically designed for reconstructing
non-watertight, textureless models. The proposed method, called NeUDF,
addresses the limitations of existing UDF-based methods by introducing a simple
and approximately unbiased and occlusion-aware density function. In addition, a
smooth and differentiable UDF representation is presented to make the learning
process easier and more efficient. Experiments on both texture-rich and
textureless models demonstrate the robustness and effectiveness of the proposed
approach, making it a promising solution for reconstructing challenging 3D
models from multi-view images.

---

## 3D-Aware Multi-Class Image-to-Image Translation with NeRFs

nerf{: .label .label-blue }

2023-03-27 | Senmao Li, Joost van de Weijer, Yaxing Wang, Fahad Shahbaz Khan, Meiqin Liu, Jian Yang | cs.CV | [PDF](http://arxiv.org/pdf/2303.15012v1){: .btn .btn-green }

**Abstract**: Recent advances in 3D-aware generative models (3D-aware GANs) combined with
Neural Radiance Fields (NeRF) have achieved impressive results. However no
prior works investigate 3D-aware GANs for 3D consistent multi-class
image-to-image (3D-aware I2I) translation. Naively using 2D-I2I translation
methods suffers from unrealistic shape/identity change. To perform 3D-aware
multi-class I2I translation, we decouple this learning process into a
multi-class 3D-aware GAN step and a 3D-aware I2I translation step. In the first
step, we propose two novel techniques: a new conditional architecture and an
effective training strategy. In the second step, based on the well-trained
multi-class 3D-aware GAN architecture, that preserves view-consistency, we
construct a 3D-aware I2I translation system. To further reduce the
view-consistency problems, we propose several new techniques, including a
U-net-like adaptor network design, a hierarchical representation constrain and
a relative regularization loss. In extensive experiments on two datasets,
quantitative and qualitative results demonstrate that we successfully perform
3D-aware I2I translation with multi-view consistency.

Comments:
- Accepted by CVPR2023

---

## Clean-NeRF: Reformulating NeRF to account for View-Dependent  Observations

nerf{: .label .label-blue }

2023-03-26 | Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2303.14707v1){: .btn .btn-green }

**Abstract**: While Neural Radiance Fields (NeRFs) had achieved unprecedented novel view
synthesis results, they have been struggling in dealing with large-scale
cluttered scenes with sparse input views and highly view-dependent appearances.
Specifically, existing NeRF-based models tend to produce blurry rendering with
the volumetric reconstruction often inaccurate, where a lot of reconstruction
errors are observed in the form of foggy "floaters" hovering within the entire
volume of an opaque 3D scene. Such inaccuracies impede NeRF's potential for
accurate 3D NeRF registration, object detection, segmentation, etc., which
possibly accounts for only limited significant research effort so far to
directly address these important 3D fundamental computer vision problems to
date. This paper analyzes the NeRF's struggles in such settings and proposes
Clean-NeRF for accurate 3D reconstruction and novel view rendering in complex
scenes. Our key insights consist of enforcing effective appearance and geometry
constraints, which are absent in the conventional NeRF reconstruction, by 1)
automatically detecting and modeling view-dependent appearances in the training
views to prevent them from interfering with density estimation, which is
complete with 2) a geometric correction procedure performed on each traced ray
during inference. Clean-NeRF can be implemented as a plug-in that can
immediately benefit existing NeRF-based methods without additional input. Codes
will be released.

---

## DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields

nerf{: .label .label-blue }

2023-03-25 | Yu Chen, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2303.14478v1){: .btn .btn-green }

**Abstract**: Recent works such as BARF and GARF can bundle adjust camera poses with neural
radiance fields (NeRF) which is based on coordinate-MLPs. Despite the
impressive results, these methods cannot be applied to Generalizable NeRFs
(GeNeRFs) which require image feature extractions that are often based on more
complicated 3D CNN or transformer architectures. In this work, we first analyze
the difficulties of jointly optimizing camera poses with GeNeRFs, and then
further propose our DBARF to tackle these issues. Our DBARF which bundle
adjusts camera poses by taking a cost feature map as an implicit cost function
can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF
and its follow-up works, which can only be applied to per-scene optimized NeRFs
and need accurate initial camera poses with the exception of forward-facing
scenes, our method can generalize across scenes and does not require any good
initialization. Experiments show the effectiveness and generalization ability
of our DBARF when evaluated on real-world datasets. Our code is available at
\url{https://aibluefisher.github.io/dbarf}.

---

## NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects

nerf{: .label .label-blue }

2023-03-25 | Zhiwen Yan, Chen Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2303.14435v1){: .btn .btn-green }

**Abstract**: Dynamic Neural Radiance Field (NeRF) is a powerful algorithm capable of
rendering photo-realistic novel view images from a monocular RGB video of a
dynamic scene. Although it warps moving points across frames from the
observation spaces to a common canonical space for rendering, dynamic NeRF does
not model the change of the reflected color during the warping. As a result,
this approach often fails drastically on challenging specular objects in
motion. We address this limitation by reformulating the neural radiance field
function to be conditioned on surface position and orientation in the
observation space. This allows the specular surface at different poses to keep
the different reflected colors when mapped to the common canonical space.
Additionally, we add the mask of moving objects to guide the deformation field.
As the specular surface changes color during motion, the mask mitigates the
problem of failure to find temporal correspondences with only RGB supervision.
We evaluate our model based on the novel view synthesis quality with a
self-collected dataset of different moving specular objects in realistic
environments. The experimental results demonstrate that our method
significantly improves the reconstruction quality of moving specular objects
from monocular RGB videos compared to the existing NeRF models. Our code and
data are available at the project website https://github.com/JokerYan/NeRF-DS.

Comments:
- CVPR 2023

---

## SUDS: Scalable Urban Dynamic Scenes

nerf{: .label .label-blue }

2023-03-25 | Haithem Turki, Jason Y. Zhang, Francesco Ferroni, Deva Ramanan | cs.CV | [PDF](http://arxiv.org/pdf/2303.14536v1){: .btn .btn-green }

**Abstract**: We extend neural radiance fields (NeRFs) to dynamic large-scale urban scenes.
Prior work tends to reconstruct single video clips of short durations (up to 10
seconds). Two reasons are that such methods (a) tend to scale linearly with the
number of moving objects and input videos because a separate model is built for
each and (b) tend to require supervision via 3D bounding boxes and panoptic
labels, obtained manually or via category-specific models. As a step towards
truly open-world reconstructions of dynamic cities, we introduce two key
innovations: (a) we factorize the scene into three separate hash table data
structures to efficiently encode static, dynamic, and far-field radiance
fields, and (b) we make use of unlabeled target signals consisting of RGB
images, sparse LiDAR, off-the-shelf self-supervised 2D descriptors, and most
importantly, 2D optical flow.
  Operationalizing such inputs via photometric, geometric, and feature-metric
reconstruction losses enables SUDS to decompose dynamic scenes into the static
background, individual objects, and their motions. When combined with our
multi-branch table representation, such reconstructions can be scaled to tens
of thousands of objects across 1.2 million frames from 1700 videos spanning
geospatial footprints of hundreds of kilometers, (to our knowledge) the largest
dynamic NeRF built to date.
  We present qualitative initial results on a variety of tasks enabled by our
representations, including novel-view synthesis of dynamic urban scenes,
unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. To
compare to prior work, we also evaluate on KITTI and Virtual KITTI 2,
surpassing state-of-the-art methods that rely on ground truth 3D bounding box
annotations while being 10x quicker to train.

Comments:
- CVPR 2023 Project page: https://haithemturki.com/suds/

---

## TEGLO: High Fidelity Canonical Texture Mapping from Single-View Images

nerf{: .label .label-blue }

2023-03-24 | Vishal Vinod, Tanmay Shah, Dmitry Lagun | cs.CV | [PDF](http://arxiv.org/pdf/2303.13743v1){: .btn .btn-green }

**Abstract**: Recent work in Neural Fields (NFs) learn 3D representations from
class-specific single view image collections. However, they are unable to
reconstruct the input data preserving high-frequency details. Further, these
methods do not disentangle appearance from geometry and hence are not suitable
for tasks such as texture transfer and editing. In this work, we propose TEGLO
(Textured EG3D-GLO) for learning 3D representations from single view
in-the-wild image collections for a given class of objects. We accomplish this
by training a conditional Neural Radiance Field (NeRF) without any explicit 3D
supervision. We equip our method with editing capabilities by creating a dense
correspondence mapping to a 2D canonical space. We demonstrate that such
mapping enables texture transfer and texture editing without requiring meshes
with shared topology. Our key insight is that by mapping the input image pixels
onto the texture space we can achieve near perfect reconstruction (>= 74 dB
PSNR at 1024^2 resolution). Our formulation allows for high quality 3D
consistent novel view synthesis with high-frequency details at megapixel image
resolution.

---

## Grid-guided Neural Radiance Fields for Large Urban Scenes

nerf{: .label .label-blue }

2023-03-24 | Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, Dahua Lin | cs.CV | [PDF](http://arxiv.org/pdf/2303.14001v1){: .btn .btn-green }

**Abstract**: Purely MLP-based neural radiance fields (NeRF-based methods) often suffer
from underfitting with blurred renderings on large-scale scenes due to limited
model capacity. Recent approaches propose to geographically divide the scene
and adopt multiple sub-NeRFs to model each region individually, leading to
linear scale-up in training costs and the number of sub-NeRFs as the scene
expands. An alternative solution is to use a feature grid representation, which
is computationally efficient and can naturally scale to a large scene with
increased grid resolutions. However, the feature grid tends to be less
constrained and often reaches suboptimal solutions, producing noisy artifacts
in renderings, especially in regions with complex geometry and texture. In this
work, we present a new framework that realizes high-fidelity rendering on large
urban scenes while being computationally efficient. We propose to use a compact
multiresolution ground feature plane representation to coarsely capture the
scene, and complement it with positional encoding inputs through another NeRF
branch for rendering in a joint learning fashion. We show that such an
integration can utilize the advantages of two alternative solutions: a
light-weighted NeRF is sufficient, under the guidance of the feature grid
representation, to render photorealistic novel views with fine details; and the
jointly optimized ground feature planes, can meanwhile gain further
refinements, forming a more accurate and compact feature space and output much
more natural rendering results.

Comments:
- CVPR2023, Project page at https://city-super.github.io/gridnerf/

---

## Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods  for Front-Facing Views

nerf{: .label .label-blue }

2023-03-24 | Hanxue Liang, Tianhao Wu, Param Hanji, Francesco Banterle, Hongyun Gao, Rafal Mantiuk, Cengiz Oztireli | cs.CV | [PDF](http://arxiv.org/pdf/2303.15206v3){: .btn .btn-green }

**Abstract**: Neural view synthesis (NVS) is one of the most successful techniques for
synthesizing free viewpoint videos, capable of achieving high fidelity from
only a sparse set of captured images. This success has led to many variants of
the techniques, each evaluated on a set of test views typically using image
quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research
on how NVS methods perform with respect to perceived video quality. We present
the first study on perceptual evaluation of NVS and NeRF variants. For this
study, we collected two datasets of scenes captured in a controlled lab
environment as well as in-the-wild. In contrast to existing datasets, these
scenes come with reference video sequences, allowing us to test for temporal
artifacts and subtle distortions that are easily overlooked when viewing only
static images. We measured the quality of videos synthesized by several NVS
methods in a well-controlled perceptual quality assessment experiment as well
as with many existing state-of-the-art image/video quality metrics. We present
a detailed analysis of the results and recommendations for dataset and metric
selection for NVS evaluation.

---

## CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D  Scene Layout

nerf{: .label .label-blue }

2023-03-24 | Haotian Bai, Yuanhuiyi Lyu, Lutao Jiang, Sijia Li, Haonan Lu, Xiaodong Lin, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2303.13843v3){: .btn .btn-green }

**Abstract**: Recent advances have shown promise in merging neural radiance fields (NeRFs)
with pre-trained diffusion models for text-to-3D object generation. However,
one enduring challenge is their inadequate capability to accurately parse and
regenerate consistent multi-object environments. Specifically, these models
encounter difficulties in accurately representing quantity and style prompted
by multi-object texts, often resulting in a collapse of the rendering fidelity
that fails to match the semantic intricacies. Moreover, amalgamating these
elements into a coherent 3D scene is a substantial challenge, stemming from
generic distribution inherent in diffusion models. To tackle the issue of
'guidance collapse' and enhance consistency, we propose a novel framework,
dubbed CompoNeRF, by integrating an editable 3D scene layout with object
specific and scene-wide guidance mechanisms. It initiates by interpreting a
complex text into an editable 3D layout populated with multiple NeRFs, each
paired with a corresponding subtext prompt for precise object depiction. Next,
a tailored composition module seamlessly blends these NeRFs, promoting
consistency, while the dual-level text guidance reduces ambiguity and boosts
accuracy. Noticeably, the unique modularity of CompoNeRF permits NeRF
decomposition. This enables flexible scene editing and recomposition into new
scenes based on the edited layout or text prompts. Utilizing the open source
Stable Diffusion model, CompoNeRF not only generates scenes with high fidelity
but also paves the way for innovative multi-object composition using editable
3D layouts. Remarkably, our framework achieves up to a 54\% improvement in
performance, as measured by the multi-view CLIP score metric. Code is available
at https://github.com/hbai98/Componerf.

---

## HandNeRF: Neural Radiance Fields for Animatable Interacting Hands

nerf{: .label .label-blue }

2023-03-24 | Zhiyang Guo, Wengang Zhou, Min Wang, Li Li, Houqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2303.13825v1){: .btn .btn-green }

**Abstract**: We propose a novel framework to reconstruct accurate appearance and geometry
with neural radiance fields (NeRF) for interacting hands, enabling the
rendering of photo-realistic images and videos for gesture animation from
arbitrary views. Given multi-view images of a single hand or interacting hands,
an off-the-shelf skeleton estimator is first employed to parameterize the hand
poses. Then we design a pose-driven deformation field to establish
correspondence from those different poses to a shared canonical space, where a
pose-disentangled NeRF for one hand is optimized. Such unified modeling
efficiently complements the geometry and texture cues in rarely-observed areas
for both hands. Meanwhile, we further leverage the pose priors to generate
pseudo depth maps as guidance for occlusion-aware density learning. Moreover, a
neural feature distillation method is proposed to achieve cross-domain
alignment for color optimization. We conduct extensive experiments to verify
the merits of our proposed HandNeRF and report a series of state-of-the-art
results both qualitatively and quantitatively on the large-scale InterHand2.6M
dataset.

Comments:
- CVPR 2023

---

## ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for  Neural Radiance Field

nerf{: .label .label-blue }

2023-03-24 | Zhe Jun Tang, Tat-Jen Cham, Haiyu Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2303.13817v1){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) is a popular method in representing 3D scenes by
optimising a continuous volumetric scene function. Its large success which lies
in applying volumetric rendering (VR) is also its Achilles' heel in producing
view-dependent effects. As a consequence, glossy and transparent surfaces often
appear murky. A remedy to reduce these artefacts is to constrain this VR
equation by excluding volumes with back-facing normal. While this approach has
some success in rendering glossy surfaces, translucent objects are still poorly
represented. In this paper, we present an alternative to the physics-based VR
approach by introducing a self-attention-based framework on volumes along a
ray. In addition, inspired by modern game engines which utilise Light Probes to
store local lighting passing through the scene, we incorporate Learnable
Embeddings to capture view dependent effects within the scene. Our method,
which we call ABLE-NeRF, significantly reduces `blurry' glossy surfaces in
rendering and produces realistic translucent surfaces which lack in prior art.
In the Blender dataset, ABLE-NeRF achieves SOTA results and surpasses Ref-NeRF
in all 3 image quality metrics PSNR, SSIM, LPIPS.

Comments:
- IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)
  2023

---

## GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from  Multi-view Images

nerf{: .label .label-blue }

2023-03-24 | Jianchuan Chen, Wentao Yi, Liqian Ma, Xu Jia, Huchuan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2303.13777v1){: .btn .btn-green }

**Abstract**: In this work, we focus on synthesizing high-fidelity novel view images for
arbitrary human performers, given a set of sparse multi-view images. It is a
challenging task due to the large variation among articulated body poses and
heavy self-occlusions. To alleviate this, we introduce an effective
generalizable framework Generalizable Model-based Neural Radiance Fields
(GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a
geometry-guided attention mechanism to register the appearance code from
multi-view 2D images to a geometry proxy which can alleviate the misalignment
between inaccurate geometry prior and pixel space. On top of that, we further
conduct neural rendering and partial gradient backpropagation for efficient
perceptual supervision and improvement of the perceptual quality of synthesis.
To evaluate our method, we conduct experiments on synthesized datasets
THuman2.0 and Multi-garment, and real-world datasets Genebody and ZJUMocap. The
results demonstrate that our approach outperforms state-of-the-art methods in
terms of novel view synthesis and geometric reconstruction.

Comments:
- Accepted at CVPR 2023

---

## Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion  Prior



2023-03-24 | Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, Dong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2303.14184v2){: .btn .btn-green }

**Abstract**: In this work, we investigate the problem of creating high-fidelity 3D content
from only a single image. This is inherently challenging: it essentially
involves estimating the underlying 3D geometry while simultaneously
hallucinating unseen textures. To address this challenge, we leverage prior
knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision
for 3D creation. Our approach, Make-It-3D, employs a two-stage optimization
pipeline: the first stage optimizes a neural radiance field by incorporating
constraints from the reference image at the frontal view and diffusion prior at
novel views; the second stage transforms the coarse model into textured point
clouds and further elevates the realism with diffusion prior while leveraging
the high-quality textures from the reference image. Extensive experiments
demonstrate that our method outperforms prior works by a large margin,
resulting in faithful reconstructions and impressive visual quality. Our method
presents the first attempt to achieve high-quality 3D creation from a single
image for general objects and enables various applications such as text-to-3D
creation and texture editing.

Comments:
- 17 pages, 18 figures, Project page: https://make-it-3d.github.io/

---

## Set-the-Scene: Global-Local Training for Generating Controllable NeRF  Scenes

nerf{: .label .label-blue }

2023-03-23 | Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, Daniel Cohen-Or | cs.CV | [PDF](http://arxiv.org/pdf/2303.13450v1){: .btn .btn-green }

**Abstract**: Recent breakthroughs in text-guided image generation have led to remarkable
progress in the field of 3D synthesis from text. By optimizing neural radiance
fields (NeRF) directly from text, recent methods are able to produce remarkable
results. Yet, these methods are limited in their control of each object's
placement or appearance, as they represent the scene as a whole. This can be a
major issue in scenarios that require refining or manipulating objects in the
scene. To remedy this deficit, we propose a novel GlobalLocal training
framework for synthesizing a 3D scene using object proxies. A proxy represents
the object's placement in the generated scene and optionally defines its coarse
geometry. The key to our approach is to represent each object as an independent
NeRF. We alternate between optimizing each NeRF on its own and as part of the
full scene. Thus, a complete representation of each object can be learned,
while also creating a harmonious scene with style and lighting match. We show
that using proxies allows a wide variety of editing options, such as adjusting
the placement of each independent object, removing objects from a scene, or
refining an object. Our results show that Set-the-Scene offers a powerful
solution for scene synthesis and manipulation, filling a crucial gap in
controllable text-to-3D synthesis.

Comments:
- project page at https://danacohen95.github.io/Set-the-Scene/

---

## Transforming Radiance Field with Lipschitz Network for Photorealistic 3D  Scene Stylization

nerf{: .label .label-blue }

2023-03-23 | Zicheng Zhang, Yinglu Liu, Congying Han, Yingwei Pan, Tiande Guo, Ting Yao | cs.CV | [PDF](http://arxiv.org/pdf/2303.13232v1){: .btn .btn-green }

**Abstract**: Recent advances in 3D scene representation and novel view synthesis have
witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not
trivial to exploit NeRF for the photorealistic 3D scene stylization task, which
aims to generate visually consistent and photorealistic stylized scenes from
novel views. Simply coupling NeRF with photorealistic style transfer (PST) will
result in cross-view inconsistency and degradation of stylized view syntheses.
Through a thorough analysis, we demonstrate that this non-trivial task can be
simplified in a new light: When transforming the appearance representation of a
pre-trained NeRF with Lipschitz mapping, the consistency and photorealism
across source views will be seamlessly encoded into the syntheses. That
motivates us to build a concise and flexible learning framework namely LipRF,
which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the
3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct
the 3D scene, and then emulates the style on each view by 2D PST as the prior
to learn a Lipschitz network to stylize the pre-trained appearance. In view of
that Lipschitz condition highly impacts the expressivity of the neural network,
we devise an adaptive regularization to balance the reconstruction and
stylization. A gradual gradient aggregation strategy is further introduced to
optimize LipRF in a cost-efficient manner. We conduct extensive experiments to
show the high quality and robust performance of LipRF on both photorealistic 3D
stylization and object appearance editing.

Comments:
- CVPR 2023, Highlight

---

## SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing  Field

nerf{: .label .label-blue }

2023-03-23 | Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui | cs.CV | [PDF](http://arxiv.org/pdf/2303.13277v2){: .btn .btn-green }

**Abstract**: Despite the great success in 2D editing using user-friendly tools, such as
Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D
areas are still limited, either relying on 3D modeling skills or allowing
editing within only a few categories. In this paper, we present a novel
semantic-driven NeRF editing approach, which enables users to edit a neural
radiance field with a single image, and faithfully delivers edited novel views
with high fidelity and multi-view consistency. To achieve this goal, we propose
a prior-guided editing field to encode fine-grained geometric and texture
editing in 3D space, and develop a series of techniques to aid the editing
process, including cyclic constraints with a proxy mesh to facilitate geometric
supervision, a color compositing mechanism to stabilize semantic-driven texture
editing, and a feature-cluster-based regularization to preserve the irrelevant
content unchanged. Extensive experiments and editing examples on both
real-world and synthetic data demonstrate that our method achieves
photo-realistic 3D editing using only a single edited image, pushing the bound
of semantic-driven editing in 3D real-world scenes. Our project webpage:
https://zju3dv.github.io/sine/.

Comments:
- Accepted to CVPR 2023. Project Page: https://zju3dv.github.io/sine/

---

## SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates

nerf{: .label .label-blue }

2023-03-23 | Mikaela Angelina Uy, Ricardo Martin-Brualla, Leonidas Guibas, Ke Li | cs.CV | [PDF](http://arxiv.org/pdf/2303.13582v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) have enabled high fidelity 3D reconstruction
from multiple 2D input views. However, a well-known drawback of NeRFs is the
less-than-ideal performance under a small number of views, due to insufficient
constraints enforced by volumetric rendering. To address this issue, we
introduce SCADE, a novel technique that improves NeRF reconstruction quality on
sparse, unconstrained input views for in-the-wild indoor scenes. To constrain
NeRF reconstruction, we leverage geometric priors in the form of per-view depth
estimates produced with state-of-the-art monocular depth estimation models,
which can generalize across scenes. A key challenge is that monocular depth
estimation is an ill-posed problem, with inherent ambiguities. To handle this
issue, we propose a new method that learns to predict, for each view, a
continuous, multimodal distribution of depth estimates using conditional
Implicit Maximum Likelihood Estimation (cIMLE). In order to disambiguate
exploiting multiple views, we introduce an original space carving loss that
guides the NeRF representation to fuse multiple hypothesized depth maps from
each view and distill from them a common geometry that is consistent with all
views. Experiments show that our approach enables higher fidelity novel view
synthesis from sparse views. Our project page can be found at
https://scade-spacecarving-nerfs.github.io .

Comments:
- CVPR 2023

---

## Plotting Behind the Scenes: Towards Learnable Game Engines

nerf{: .label .label-blue }

2023-03-23 | Willi Menapace, Aliaksandr Siarohin, Stéphane Lathuilière, Panos Achlioptas, Vladislav Golyanik, Sergey Tulyakov, Elisa Ricci | cs.CV | [PDF](http://arxiv.org/pdf/2303.13472v2){: .btn .btn-green }

**Abstract**: Neural video game simulators emerged as powerful tools to generate and edit
videos. Their idea is to represent games as the evolution of an environment's
state driven by the actions of its agents. While such a paradigm enables users
to play a game action-by-action, its rigidity precludes more semantic forms of
control. To overcome this limitation, we augment game models with prompts
specified as a set of natural language actions and desired states. The result-a
Promptable Game Model (PGM)-makes it possible for a user to play the game by
prompting it with high- and low-level action sequences. Most captivatingly, our
PGM unlocks the director's mode, where the game is played by specifying goals
for the agents in the form of a prompt. This requires learning "game AI",
encapsulated by our animation model, to navigate the scene using high-level
constraints, play against an adversary, and devise a strategy to win a point.
To render the resulting state, we use a compositional NeRF representation
encapsulated in our synthesis model. To foster future research, we present
newly collected, annotated and calibrated Tennis and Minecraft datasets. Our
method significantly outperforms existing neural video game simulators in terms
of rendering quality and unlocks applications beyond the capabilities of the
current state of the art. Our framework, data, and models are available at
https://snap-research.github.io/promptable-game-models/.

Comments:
- ACM Transactions on Graphics \c{opyright} Copyright is held by the
  owner/author(s) 2023. This is the author's version of the work. It is posted
  here for your personal use. Not for redistribution. The definitive Version of
  Record was published in ACM Transactions on Graphics,
  http://dx.doi.org/10.1145/3635705

---

## TriPlaneNet: An Encoder for EG3D Inversion

nerf{: .label .label-blue }

2023-03-23 | Ananta R. Bhattarai, Matthias Nießner, Artem Sevastopolsky | cs.CV | [PDF](http://arxiv.org/pdf/2303.13497v2){: .btn .btn-green }

**Abstract**: Recent progress in NeRF-based GANs has introduced a number of approaches for
high-resolution and high-fidelity generative modeling of human heads with a
possibility for novel view rendering. At the same time, one must solve an
inverse problem to be able to re-render or modify an existing image or video.
Despite the success of universal optimization-based methods for 2D GAN
inversion, those applied to 3D GANs may fail to extrapolate the result onto the
novel view, whereas optimization-based 3D GAN inversion methods are
time-consuming and can require at least several minutes per image. Fast
encoder-based techniques, such as those developed for StyleGAN, may also be
less appealing due to the lack of identity preservation. Our work introduces a
fast technique that bridges the gap between the two approaches by directly
utilizing the tri-plane representation presented for the EG3D generative model.
In particular, we build upon a feed-forward convolutional encoder for the
latent code and extend it with a fully-convolutional predictor of tri-plane
numerical offsets. The renderings are similar in quality to the ones produced
by optimization-based techniques and outperform the ones by encoder-based
methods. As we empirically prove, this is a consequence of directly operating
in the tri-plane space, not in the GAN parameter space, while making use of an
encoder-based trainable approach. Finally, we demonstrate significantly more
correct embedding of a face image in 3D than for all the baselines, further
strengthened by a probably symmetric prior enabled during training.

Comments:
- Project page: https://anantarb.github.io/triplanenet

---

## DreamBooth3D: Subject-Driven Text-to-3D Generation



2023-03-23 | Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, Varun Jampani | cs.CV | [PDF](http://arxiv.org/pdf/2303.13508v2){: .btn .btn-green }

**Abstract**: We present DreamBooth3D, an approach to personalize text-to-3D generative
models from as few as 3-6 casually captured images of a subject. Our approach
combines recent advances in personalizing text-to-image models (DreamBooth)
with text-to-3D generation (DreamFusion). We find that naively combining these
methods fails to yield satisfactory subject-specific 3D assets due to
personalized text-to-image models overfitting to the input viewpoints of the
subject. We overcome this through a 3-stage optimization strategy where we
jointly leverage the 3D consistency of neural radiance fields together with the
personalization capability of text-to-image models. Our method can produce
high-quality, subject-specific 3D assets with text-driven modifications such as
novel poses, colors and attributes that are not seen in any of the input images
of the subject.

Comments:
- Project page at https://dreambooth3d.github.io/ Video Summary at
  https://youtu.be/kKVDrbfvOoA

---

## Semantic Ray: Learning a Generalizable Semantic Field with  Cross-Reprojection Attention

nerf{: .label .label-blue }

2023-03-23 | Fangfu Liu, Chubin Zhang, Yu Zheng, Yueqi Duan | cs.CV | [PDF](http://arxiv.org/pdf/2303.13014v1){: .btn .btn-green }

**Abstract**: In this paper, we aim to learn a semantic radiance field from multiple scenes
that is accurate, efficient and generalizable. While most existing NeRFs target
at the tasks of neural scene rendering, image synthesis and multi-view
reconstruction, there are a few attempts such as Semantic-NeRF that explore to
learn high-level semantic understanding with the NeRF structure. However,
Semantic-NeRF simultaneously learns color and semantic label from a single ray
with multiple heads, where the single ray fails to provide rich semantic
information. As a result, Semantic NeRF relies on positional encoding and needs
to train one specific model for each scene. To address this, we propose
Semantic Ray (S-Ray) to fully exploit semantic information along the ray
direction from its multi-view reprojections. As directly performing dense
attention over multi-view reprojected rays would suffer from heavy
computational cost, we design a Cross-Reprojection Attention module with
consecutive intra-view radial and cross-view sparse attentions, which
decomposes contextual information along reprojected rays and cross multiple
views and then collects dense connections by stacking the modules. Experiments
show that our S-Ray is able to learn from multiple scenes, and it presents
strong generalization ability to adapt to unseen scenes.

Comments:
- Accepted by CVPR 2023. Project page: https://liuff19.github.io/S-Ray/

---

## Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions

nerf{: .label .label-blue }

2023-03-22 | Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2303.12789v2){: .btn .btn-green }

**Abstract**: We propose a method for editing NeRF scenes with text-instructions. Given a
NeRF of a scene and the collection of images used to reconstruct it, our method
uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit
the input images while optimizing the underlying scene, resulting in an
optimized 3D scene that respects the edit instruction. We demonstrate that our
proposed method is able to edit large-scale, real-world scenes, and is able to
accomplish more realistic, targeted edits than prior work.

Comments:
- Project website: https://instruct-nerf2nerf.github.io; v1. Revisions
  to related work and discussion

---

## FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation  Models

nerf{: .label .label-blue }

2023-03-22 | Jianglong Ye, Naiyan Wang, Xiaolong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2303.12786v1){: .btn .btn-green }

**Abstract**: Recent works on generalizable NeRFs have shown promising results on novel
view synthesis from single or few images. However, such models have rarely been
applied on other downstream tasks beyond synthesis such as semantic
understanding and parsing. In this paper, we propose a novel framework named
FeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision
foundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D
pre-trained foundation models to 3D space via neural rendering, and then
extract deep features for 3D query points from NeRF MLPs. Consequently, it
allows to map 2D images to continuous 3D semantic feature volumes, which can be
used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D
semantic keypoint transfer and 2D/3D object part segmentation. Our extensive
experiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D
semantic feature extractor. Our project page is available at
https://jianglongye.com/featurenerf/ .

Comments:
- Project page: https://jianglongye.com/featurenerf/

---

## NLOS-NeuS: Non-line-of-sight Neural Implicit Surface



2023-03-22 | Yuki Fujimura, Takahiro Kushida, Takuya Funatomi, Yasuhiro Mukaigawa | cs.CV | [PDF](http://arxiv.org/pdf/2303.12280v2){: .btn .btn-green }

**Abstract**: Non-line-of-sight (NLOS) imaging is conducted to infer invisible scenes from
indirect light on visible objects. The neural transient field (NeTF) was
proposed for representing scenes as neural radiance fields in NLOS scenes. We
propose NLOS neural implicit surface (NLOS-NeuS), which extends the NeTF to
neural implicit surfaces with a signed distance function (SDF) for
reconstructing three-dimensional surfaces in NLOS scenes. We introduce two
constraints as loss functions for correctly learning an SDF to avoid non-zero
level-set surfaces. We also introduce a lower bound constraint of an SDF based
on the geometry of the first-returning photons. The experimental results
indicate that these constraints are essential for learning a correct SDF in
NLOS scenes. Compared with previous methods with discretized representation,
NLOS-NeuS with the neural continuous representation enables us to reconstruct
smooth surfaces while preserving fine details in NLOS scenes. To the best of
our knowledge, this is the first study on neural implicit surfaces with volume
rendering in NLOS scenes.

Comments:
- ICCV 2023

---

## SHERF: Generalizable Human NeRF from a Single Image

nerf{: .label .label-blue }

2023-03-22 | Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2303.12791v2){: .btn .btn-green }

**Abstract**: Existing Human NeRF methods for reconstructing 3D humans typically rely on
multiple 2D images from multi-view cameras or monocular videos captured from
fixed camera views. However, in real-world scenarios, human images are often
captured from random camera angles, presenting challenges for high-quality 3D
human reconstruction. In this paper, we propose SHERF, the first generalizable
Human NeRF model for recovering animatable 3D humans from a single input image.
SHERF extracts and encodes 3D human representations in canonical space,
enabling rendering and animation from free views and poses. To achieve
high-fidelity novel view and pose synthesis, the encoded 3D human
representations should capture both global appearance and local fine-grained
textures. To this end, we propose a bank of 3D-aware hierarchical features,
including global, point-level, and pixel-aligned features, to facilitate
informative encoding. Global features enhance the information extracted from
the single input image and complement the information missing from the partial
2D observation. Point-level features provide strong clues of 3D human
structure, while pixel-aligned features preserve more fine-grained details. To
effectively integrate the 3D-aware hierarchical feature bank, we design a
feature fusion transformer. Extensive experiments on THuman, RenderPeople,
ZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-art
performance, with better generalizability for novel view and pose synthesis.

Comments:
- Accepted by ICCV2023. Project webpage:
  https://skhu101.github.io/SHERF/

---

## NeRF-GAN Distillation for Efficient 3D-Aware Generation with  Convolutions

nerf{: .label .label-blue }

2023-03-22 | Mohamad Shahbazi, Evangelos Ntavelis, Alessio Tonioni, Edo Collins, Danda Pani Paudel, Martin Danelljan, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2303.12865v3){: .btn .btn-green }

**Abstract**: Pose-conditioned convolutional generative models struggle with high-quality
3D-consistent image generation from single-view datasets, due to their lack of
sufficient 3D priors. Recently, the integration of Neural Radiance Fields
(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),
has transformed 3D-aware generation from single-view images. NeRF-GANs exploit
the strong inductive bias of neural 3D representations and volumetric rendering
at the cost of higher computational complexity. This study aims at revisiting
pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by
distilling 3D knowledge from pretrained NeRF-GANs. We propose a simple and
effective method, based on re-using the well-disentangled latent space of a
pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly
generate 3D-consistent images corresponding to the underlying 3D
representations. Experiments on several datasets demonstrate that the proposed
method obtains results comparable with volumetric rendering in terms of quality
and 3D consistency while benefiting from the computational advantage of
convolutional networks. The code will be available at:
https://github.com/mshahbazi72/NeRF-GAN-Distillation

---

## Balanced Spherical Grid for Egocentric View Synthesis

nerf{: .label .label-blue }

2023-03-22 | Changwoon Choi, Sang Min Kim, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2303.12408v2){: .btn .btn-green }

**Abstract**: We present EgoNeRF, a practical solution to reconstruct large-scale
real-world environments for VR assets. Given a few seconds of casually captured
360 video, EgoNeRF can efficiently build neural radiance fields which enable
high-quality rendering from novel viewpoints. Motivated by the recent
acceleration of NeRF using feature grids, we adopt spherical coordinate instead
of conventional Cartesian coordinate. Cartesian feature grid is inefficient to
represent large-scale unbounded scenes because it has a spatially uniform
resolution, regardless of distance from viewers. The spherical parameterization
better aligns with the rays of egocentric images, and yet enables factorization
for performance enhancement. However, the na\"ive spherical grid suffers from
irregularities at two poles, and also cannot represent unbounded scenes. To
avoid singularities near poles, we combine two balanced grids, which results in
a quasi-uniform angular grid. We also partition the radial grid exponentially
and place an environment map at infinity to represent unbounded scenes.
Furthermore, with our resampling technique for grid-based methods, we can
increase the number of valid samples to train NeRF volume. We extensively
evaluate our method in our newly introduced synthetic and real-world egocentric
360 video datasets, and it consistently achieves state-of-the-art performance.

Comments:
- Accepted to CVPR 2023

---

## Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields

nerf{: .label .label-blue }

2023-03-21 | Ahmad AlMughrabi, Umair Haroon, Ricardo Marques, Petia Radeva | cs.CV | [PDF](http://arxiv.org/pdf/2303.12234v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) appeared recently as a powerful tool to
generate realistic views of objects and confined areas. Still, they face
serious challenges with open scenes, where the camera has unrestricted movement
and content can appear at any distance. In such scenarios, current
NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow
training times, and might display irregularities, because of the challenging
task of reconstructing an extensive scene from a limited number of images. We
propose a new framework to boost the performance of NeRF-based architectures
yielding significantly superior outcomes compared to the prior work. Our
solution overcomes several obstacles that plagued earlier versions of NeRF,
including handling multiple video inputs, selecting keyframes, and extracting
poses from real-world frames that are ambiguous and symmetrical. Furthermore,
we applied our framework, dubbed as "Pre-NeRF 360", to enable the use of the
Nutrition5k dataset in NeRF and introduce an updated version of this dataset,
known as the N5k360 dataset.

---

## Few-shot Neural Radiance Fields Under Unconstrained Illumination

nerf{: .label .label-blue }

2023-03-21 | SeokYeong Lee, JunYong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho | cs.CV | [PDF](http://arxiv.org/pdf/2303.11728v3){: .btn .btn-green }

**Abstract**: In this paper, we introduce a new challenge for synthesizing novel view
images in practical environments with limited input multi-view images and
varying lighting conditions. Neural radiance fields (NeRF), one of the
pioneering works for this task, demand an extensive set of multi-view images
taken under constrained illumination, which is often unattainable in real-world
settings. While some previous works have managed to synthesize novel views
given images with different illumination, their performance still relies on a
substantial number of input multi-view images. To address this problem, we
suggest ExtremeNeRF, which utilizes multi-view albedo consistency, supported by
geometric alignment. Specifically, we extract intrinsic image components that
should be illumination-invariant across different views, enabling direct
appearance comparison between the input and novel view under unconstrained
illumination. We offer thorough experimental results for task evaluation,
employing the newly created NeRF Extreme benchmark-the first in-the-wild
benchmark for novel view synthesis under multiple viewing directions and
varying illuminations.

Comments:
- Project Page: https://seokyeong94.github.io/ExtremeNeRF/

---

## Interactive Geometry Editing of Neural Radiance Fields



2023-03-21 | Shaoxu Li, Ye Pan | cs.CV | [PDF](http://arxiv.org/pdf/2303.11537v2){: .btn .btn-green }

**Abstract**: In this paper, we propose a method that enables interactive geometry editing
for neural radiance fields manipulation. We use two proxy cages(inner cage and
outer cage) to edit a scene. The inner cage defines the operation target, and
the outer cage defines the adjustment space. Various operations apply to the
two cages. After cage selection, operations on the inner cage lead to the
desired transformation of the inner cage and adjustment of the outer cage.
Users can edit the scene with translation, rotation, scaling, or combinations.
The operations on the corners and edges of the cage are also supported. Our
method does not need any explicit 3D geometry representations. The interactive
geometry editing applies directly to the implicit neural radiance fields.
Extensive experimental results demonstrate the effectiveness of our approach.

---

## 3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion

nerf{: .label .label-blue }

2023-03-21 | Yu-Jhe Li, Tao Xu, Ji Hou, Bichen Wu, Xiaoliang Dai, Albert Pumarola, Peizhao Zhang, Peter Vajda, Kris Kitani | cs.CV | [PDF](http://arxiv.org/pdf/2303.11938v2){: .btn .btn-green }

**Abstract**: We tackle the task of text-to-3D creation with pre-trained latent-based NeRFs
(NeRFs that generate 3D objects given input latent code). Recent works such as
DreamFusion and Magic3D have shown great success in generating 3D content using
NeRFs and text prompts, but the current approach of optimizing a NeRF for every
text prompt is 1) extremely time-consuming and 2) often leads to low-resolution
outputs. To address these challenges, we propose a novel method named
3D-CLFusion which leverages the pre-trained latent-based NeRFs and performs
fast 3D content creation in less than a minute. In particular, we introduce a
latent diffusion prior network for learning the w latent from the input CLIP
text/image embeddings. This pipeline allows us to produce the w latent without
further optimization during inference and the pre-trained NeRF is able to
perform multi-view high-resolution 3D synthesis based on the latent. We note
that the novelty of our model lies in that we introduce contrastive learning
during training the diffusion prior which enables the generation of the valid
view-invariant latent code. We demonstrate through experiments the
effectiveness of our proposed view-invariant diffusion process for fast
text-to-3D creation, e.g., 100 times faster than DreamFusion. We note that our
model is able to serve as the role of a plug-and-play tool for text-to-3D with
pre-trained NeRFs.

Comments:
- 15 pages

---

## DehazeNeRF: Multiple Image Haze Removal and 3D Shape Reconstruction  using Neural Radiance Fields

nerf{: .label .label-blue }

2023-03-20 | Wei-Ting Chen, Wang Yifan, Sy-Yen Kuo, Gordon Wetzstein | cs.CV | [PDF](http://arxiv.org/pdf/2303.11364v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance
for 3D computer vision tasks, including novel view synthesis and 3D shape
reconstruction. However, these methods fail in adverse weather conditions. To
address this challenge, we introduce DehazeNeRF as a framework that robustly
operates in hazy conditions. DehazeNeRF extends the volume rendering equation
by adding physically realistic terms that model atmospheric scattering. By
parameterizing these terms using suitable networks that match the physical
properties, we introduce effective inductive biases, which, together with the
proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view
haze removal, novel view synthesis, and 3D shape reconstruction where existing
approaches fail.

Comments:
- including supplemental material; project page:
  https://www.computationalimaging.org/publications/dehazenerf

---

## ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real  Novel View Synthesis via Contrastive Learning

nerf{: .label .label-blue }

2023-03-20 | Hao Yang, Lanqing Hong, Aoxue Li, Tianyang Hu, Zhenguo Li, Gim Hee Lee, Liwei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2303.11052v3){: .btn .btn-green }

**Abstract**: Although many recent works have investigated generalizable NeRF-based novel
view synthesis for unseen scenes, they seldom consider the synthetic-to-real
generalization, which is desired in many practical applications. In this work,
we first investigate the effects of synthetic data in synthetic-to-real novel
view synthesis and surprisingly observe that models trained with synthetic data
tend to produce sharper but less accurate volume densities. For pixels where
the volume densities are correct, fine-grained details will be obtained.
Otherwise, severe artifacts will be produced. To maintain the advantages of
using synthetic data while avoiding its negative effects, we propose to
introduce geometry-aware contrastive learning to learn multi-view consistent
features with geometric constraints. Meanwhile, we adopt cross-view attention
to further enhance the geometry perception of features by querying features
across input views. Experiments demonstrate that under the synthetic-to-real
setting, our method can render images with higher quality and better
fine-grained details, outperforming existing generalizable novel view synthesis
methods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our
method also achieves state-of-the-art results.

---

## SKED: Sketch-guided Text-based 3D Editing

nerf{: .label .label-blue }

2023-03-19 | Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri | cs.CV | [PDF](http://arxiv.org/pdf/2303.10735v4){: .btn .btn-green }

**Abstract**: Text-to-image diffusion models are gradually introduced into computer
graphics, recently enabling the development of Text-to-3D pipelines in an open
domain. However, for interactive editing purposes, local manipulations of
content through a simplistic textual interface can be arduous. Incorporating
user guided sketches with Text-to-image pipelines offers users more intuitive
control. Still, as state-of-the-art Text-to-3D pipelines rely on optimizing
Neural Radiance Fields (NeRF) through gradients from arbitrary rendering views,
conditioning on sketches is not straightforward. In this paper, we present
SKED, a technique for editing 3D shapes represented by NeRFs. Our technique
utilizes as few as two guiding sketches from different views to alter an
existing neural field. The edited region respects the prompt semantics through
a pre-trained diffusion model. To ensure the generated output adheres to the
provided sketches, we propose novel loss functions to generate the desired
edits while preserving the density and radiance of the base instance. We
demonstrate the effectiveness of our proposed method through several
qualitative and quantitative experiments. https://sked-paper.github.io/

---

## NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental  LiDAR Odometry and Mapping

nerf{: .label .label-blue }

2023-03-19 | Junyuan Deng, Xieyuanli Chen, Songpengcheng Xia, Zhen Sun, Guoqing Liu, Wenxian Yu, Ling Pei | cs.CV | [PDF](http://arxiv.org/pdf/2303.10709v1){: .btn .btn-green }

**Abstract**: Simultaneously odometry and mapping using LiDAR data is an important task for
mobile systems to achieve full autonomy in large-scale environments. However,
most existing LiDAR-based methods prioritize tracking quality over
reconstruction quality. Although the recently developed neural radiance fields
(NeRF) have shown promising advances in implicit reconstruction for indoor
environments, the problem of simultaneous odometry and mapping for large-scale
scenarios using incremental LiDAR data remains unexplored. To bridge this gap,
in this paper, we propose a novel NeRF-based LiDAR odometry and mapping
approach, NeRF-LOAM, consisting of three modules neural odometry, neural
mapping, and mesh reconstruction. All these modules utilize our proposed neural
signed distance function, which separates LiDAR points into ground and
non-ground points to reduce Z-axis drift, optimizes odometry and voxel
embeddings concurrently, and in the end generates dense smooth mesh maps of the
environment. Moreover, this joint optimization allows our NeRF-LOAM to be
pre-trained free and exhibit strong generalization abilities when applied to
different environments. Extensive evaluations on three publicly available
datasets demonstrate that our approach achieves state-of-the-art odometry and
mapping performance, as well as a strong generalization in large-scale
environments utilizing LiDAR data. Furthermore, we perform multiple ablation
studies to validate the effectiveness of our network design. The implementation
of our approach will be made available at
https://github.com/JunyuanDeng/NeRF-LOAM.

---

## StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields



2023-03-19 | Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, Eric Xing | cs.CV | [PDF](http://arxiv.org/pdf/2303.10598v3){: .btn .btn-green }

**Abstract**: 3D style transfer aims to render stylized novel views of a 3D scene with
multi-view consistency. However, most existing work suffers from a three-way
dilemma over accurate geometry reconstruction, high-quality stylization, and
being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance
Fields), an innovative 3D style transfer technique that resolves the three-way
dilemma by performing style transformation within the feature space of a
radiance field. StyleRF employs an explicit grid of high-level features to
represent 3D scenes, with which high-fidelity geometry can be reliably restored
via volume rendering. In addition, it transforms the grid features according to
the reference style which directly leads to high-quality zero-shot style
transfer. StyleRF consists of two innovative designs. The first is
sampling-invariant content transformation that makes the transformation
invariant to the holistic statistics of the sampled 3D points and accordingly
ensures multi-view consistency. The second is deferred style transformation of
2D feature maps which is equivalent to the transformation of 3D points but
greatly reduces memory footprint without degrading multi-view consistency.
Extensive experiments show that StyleRF achieves superior 3D stylization
quality with precise geometry reconstruction and it can generalize to various
new styles in a zero-shot manner.

Comments:
- Accepted to CVPR 2023. Project website:
  https://kunhao-liu.github.io/StyleRF/

---

## 3D Data Augmentation for Driving Scenes on Camera

nerf{: .label .label-blue }

2023-03-18 | Wenwen Tong, Jiangwei Xie, Tianyu Li, Hanming Deng, Xiangwei Geng, Ruoyi Zhou, Dingchen Yang, Bo Dai, Lewei Lu, Hongyang Li | cs.CV | [PDF](http://arxiv.org/pdf/2303.10340v1){: .btn .btn-green }

**Abstract**: Driving scenes are extremely diverse and complicated that it is impossible to
collect all cases with human effort alone. While data augmentation is an
effective technique to enrich the training data, existing methods for camera
data in autonomous driving applications are confined to the 2D image plane,
which may not optimally increase data diversity in 3D real-world scenarios. To
this end, we propose a 3D data augmentation approach termed Drive-3DAug, aiming
at augmenting the driving scenes on camera in the 3D space. We first utilize
Neural Radiance Field (NeRF) to reconstruct the 3D models of background and
foreground objects. Then, augmented driving scenes can be obtained by placing
the 3D objects with adapted location and orientation at the pre-defined valid
region of backgrounds. As such, the training database could be effectively
scaled up. However, the 3D object modeling is constrained to the image quality
and the limited viewpoints. To overcome these problems, we modify the original
NeRF by introducing a geometric rectified loss and a symmetric-aware training
strategy. We evaluate our method for the camera-only monocular 3D detection
task on the Waymo and nuScences datasets. The proposed data augmentation
approach contributes to a gain of 1.7% and 1.4% in terms of detection accuracy,
on Waymo and nuScences respectively. Furthermore, the constructed 3D models
serve as digital driving assets and could be recycled for different detectors
or other 3D perception tasks.

---

## $α$Surf: Implicit Surface Reconstruction for Semi-Transparent and  Thin Objects with Decoupled Geometry and Opacity

nerf{: .label .label-blue }

2023-03-17 | Tianhao Wu, Hanxue Liang, Fangcheng Zhong, Gernot Riegler, Shimon Vainer, Cengiz Oztireli | cs.CV | [PDF](http://arxiv.org/pdf/2303.10083v1){: .btn .btn-green }

**Abstract**: Implicit surface representations such as the signed distance function (SDF)
have emerged as a promising approach for image-based surface reconstruction.
However, existing optimization methods assume solid surfaces and are therefore
unable to properly reconstruct semi-transparent surfaces and thin structures,
which also exhibit low opacity due to the blending effect with the background.
While neural radiance field (NeRF) based methods can model semi-transparency
and achieve photo-realistic quality in synthesized novel views, their
volumetric geometry representation tightly couples geometry and opacity, and
therefore cannot be easily converted into surfaces without introducing
artifacts. We present $\alpha$Surf, a novel surface representation with
decoupled geometry and opacity for the reconstruction of semi-transparent and
thin surfaces where the colors mix. Ray-surface intersections on our
representation can be found in closed-form via analytical solutions of cubic
polynomials, avoiding Monte-Carlo sampling and is fully differentiable by
construction. Our qualitative and quantitative evaluations show that our
approach can accurately reconstruct surfaces with semi-transparent and thin
parts with fewer artifacts, achieving better reconstruction quality than
state-of-the-art SDF and NeRF methods. Website: https://alphasurf.netlify.app/

---

## Single-view Neural Radiance Fields with Depth Teacher

nerf{: .label .label-blue }

2023-03-17 | Yurui Chen, Chun Gu, Feihu Zhang, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2303.09952v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have been proposed for photorealistic novel
view rendering. However, it requires many different views of one scene for
training. Moreover, it has poor generalizations to new scenes and requires
retraining or fine-tuning on each scene. In this paper, we develop a new NeRF
model for novel view synthesis using only a single image as input. We propose
to combine the (coarse) planar rendering and the (fine) volume rendering to
achieve higher rendering quality and better generalizations. We also design a
depth teacher net that predicts dense pseudo depth maps to supervise the joint
rendering mechanism and boost the learning of consistent 3D geometry. We
evaluate our method on three challenging datasets. It outperforms
state-of-the-art single-view NeRFs by achieving 5$\sim$20\% improvements in
PSNR and reducing 20$\sim$50\% of the errors in the depth rendering. It also
shows excellent generalization abilities to unseen data without the need to
fine-tune on each new scene.

---

## NeRFMeshing: Distilling Neural Radiance Fields into  Geometrically-Accurate 3D Meshes

nerf{: .label .label-blue }

2023-03-16 | Marie-Julie Rakotosaona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer, Abhijit Kundu, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2303.09431v1){: .btn .btn-green }

**Abstract**: With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis
has recently made a big leap forward. At the core, NeRF proposes that each 3D
point can emit radiance, allowing to conduct view synthesis using
differentiable volumetric rendering. While neural radiance fields can
accurately represent 3D scenes for computing the image rendering, 3D meshes are
still the main scene representation supported by most computer graphics and
simulation pipelines, enabling tasks such as real time rendering and
physics-based simulations. Obtaining 3D meshes from neural radiance fields
still remains an open challenge since NeRFs are optimized for view synthesis,
not enforcing an accurate underlying geometry on the radiance field. We thus
propose a novel compact and flexible architecture that enables easy 3D surface
reconstruction from any NeRF-driven approach. Upon having trained the radiance
field, we distill the volumetric 3D representation into a Signed Surface
Approximation Network, allowing easy extraction of the 3D mesh and appearance.
Our final 3D mesh is physically accurate and can be rendered in real time on an
array of devices.

---

## NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing  Diverse Intrinsic and Extrinsic Camera Parameters

nerf{: .label .label-blue }

2023-03-16 | Hannah Schieber, Fabian Deuser, Bernhard Egger, Norbert Oswald, Daniel Roth | cs.CV | [PDF](http://arxiv.org/pdf/2303.09412v4){: .btn .btn-green }

**Abstract**: Novel view synthesis using neural radiance fields (NeRF) is the
state-of-the-art technique for generating high-quality images from novel
viewpoints. Existing methods require a priori knowledge about extrinsic and
intrinsic camera parameters. This limits their applicability to synthetic
scenes, or real-world scenarios with the necessity of a preprocessing step.
Current research on the joint optimization of camera parameters and NeRF
focuses on refining noisy extrinsic camera parameters and often relies on the
preprocessing of intrinsic camera parameters. Further approaches are limited to
cover only one single camera intrinsic. To address these limitations, we
propose a novel end-to-end trainable approach called NeRFtrinsic Four. We
utilize Gaussian Fourier features to estimate extrinsic camera parameters and
dynamically predict varying intrinsic camera parameters through the supervision
of the projection error. Our approach outperforms existing joint optimization
methods on LLFF and BLEFF. In addition to these existing datasets, we introduce
a new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic
Four is a step forward in joint optimization NeRF-based view synthesis and
enables more realistic and flexible rendering in real-world scenarios with
varying camera parameters.

---

## Reliable Image Dehazing by NeRF

nerf{: .label .label-blue }

2023-03-16 | Zheyan Jin, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, Yueting Chen | cs.CV | [PDF](http://arxiv.org/pdf/2303.09153v1){: .btn .btn-green }

**Abstract**: We present an image dehazing algorithm with high quality, wide application,
and no data training or prior needed. We analyze the defects of the original
dehazing model, and propose a new and reliable dehazing reconstruction and
dehazing model based on the combination of optical scattering model and
computer graphics lighting rendering model. Based on the new haze model and the
images obtained by the cameras, we can reconstruct the three-dimensional space,
accurately calculate the objects and haze in the space, and use the
transparency relationship of haze to perform accurate haze removal. To obtain a
3D simulation dataset we used the Unreal 5 computer graphics rendering engine.
In order to obtain real shot data in different scenes, we used fog generators,
array cameras, mobile phones, underwater cameras and drones to obtain haze
data. We use formula derivation, simulation data set and real shot data set
result experimental results to prove the feasibility of the new method.
Compared with various other methods, we are far ahead in terms of calculation
indicators (4 dB higher quality average scene), color remains more natural, and
the algorithm is more robust in different scenarios and best in the subjective
perception.

Comments:
- 12pages, 8figures

---

## LERF: Language Embedded Radiance Fields

nerf{: .label .label-blue }

2023-03-16 | Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, Matthew Tancik | cs.CV | [PDF](http://arxiv.org/pdf/2303.09553v1){: .btn .btn-green }

**Abstract**: Humans describe the physical world using natural language to refer to
specific 3D locations based on a vast range of properties: visual appearance,
semantics, abstract associations, or actionable affordances. In this work we
propose Language Embedded Radiance Fields (LERFs), a method for grounding
language embeddings from off-the-shelf models like CLIP into NeRF, which enable
these types of open-ended language queries in 3D. LERF learns a dense,
multi-scale language field inside NeRF by volume rendering CLIP embeddings
along training rays, supervising these embeddings across training views to
provide multi-view consistency and smooth the underlying language field. After
optimization, LERF can extract 3D relevancy maps for a broad range of language
prompts interactively in real-time, which has potential use cases in robotics,
understanding vision-language models, and interacting with 3D scenes. LERF
enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings
without relying on region proposals or masks, supporting long-tail
open-vocabulary queries hierarchically across the volume. The project website
can be found at https://lerf.io .

Comments:
- Project website can be found at https://lerf.io

---

## PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D  Supervision

nerf{: .label .label-blue }

2023-03-16 | Konstantinos Tertikas, Despoina Paschalidou, Boxiao Pan, Jeong Joon Park, Mikaela Angelina Uy, Ioannis Emiris, Yannis Avrithis, Leonidas Guibas | cs.CV | [PDF](http://arxiv.org/pdf/2303.09554v3){: .btn .btn-green }

**Abstract**: Impressive progress in generative models and implicit representations gave
rise to methods that can generate 3D shapes of high quality. However, being
able to locally control and edit shapes is another essential property that can
unlock several content creation applications. Local control can be achieved
with part-aware models, but existing methods require 3D supervision and cannot
produce textures. In this work, we devise PartNeRF, a novel part-aware
generative model for editable 3D shape synthesis that does not require any
explicit 3D supervision. Our model generates objects as a set of locally
defined NeRFs, augmented with an affine transformation. This enables several
editing operations such as applying transformations on parts, mixing parts from
different objects etc. To ensure distinct, manipulable parts we enforce a hard
assignment of rays to parts that makes sure that the color of each ray is only
determined by a single NeRF. As a result, altering one part does not affect the
appearance of the others. Evaluations on various ShapeNet categories
demonstrate the ability of our model to generate editable 3D objects of
improved fidelity, compared to previous part-based generative approaches that
require 3D supervision or models relying on NeRFs.

Comments:
- To appear in CVPR 2023, Project Page:
  https://ktertikas.github.io/part_nerf

---

## Mesh Strikes Back: Fast and Efficient Human Reconstruction from RGB  videos

nerf{: .label .label-blue }

2023-03-15 | Rohit Jena, Pratik Chaudhari, James Gee, Ganesh Iyer, Siddharth Choudhary, Brandon M. Smith | cs.CV | [PDF](http://arxiv.org/pdf/2303.08808v1){: .btn .btn-green }

**Abstract**: Human reconstruction and synthesis from monocular RGB videos is a challenging
problem due to clothing, occlusion, texture discontinuities and sharpness, and
framespecific pose changes. Many methods employ deferred rendering, NeRFs and
implicit methods to represent clothed humans, on the premise that mesh-based
representations cannot capture complex clothing and textures from RGB,
silhouettes, and keypoints alone. We provide a counter viewpoint to this
fundamental premise by optimizing a SMPL+D mesh and an efficient,
multi-resolution texture representation using only RGB images, binary
silhouettes and sparse 2D keypoints. Experimental results demonstrate that our
approach is more capable of capturing geometric details compared to visual
hull, mesh-based methods. We show competitive novel view synthesis and
improvements in novel pose synthesis compared to NeRF-based methods, which
introduce noticeable, unwanted artifacts. By restricting the solution space to
the SMPL+D model combined with differentiable rendering, we obtain dramatic
speedups in compute, training times (up to 24x) and inference times (up to
192x). Our method therefore can be used as is or as a fast initialization to
NeRF-based methods.

---

## Re-ReND: Real-time Rendering of NeRFs across Devices

nerf{: .label .label-blue }

2023-03-15 | Sara Rojas, Jesus Zarzar, Juan Camilo Perez, Artsiom Sanakoyeu, Ali Thabet, Albert Pumarola, Bernard Ghanem | cs.CV | [PDF](http://arxiv.org/pdf/2303.08717v1){: .btn .btn-green }

**Abstract**: This paper proposes a novel approach for rendering a pre-trained Neural
Radiance Field (NeRF) in real-time on resource-constrained devices. We
introduce Re-ReND, a method enabling Real-time Rendering of NeRFs across
Devices. Re-ReND is designed to achieve real-time performance by converting the
NeRF into a representation that can be efficiently processed by standard
graphics pipelines. The proposed method distills the NeRF by extracting the
learned density into a mesh, while the learned color information is factorized
into a set of matrices that represent the scene's light field. Factorization
implies the field is queried via inexpensive MLP-free matrix multiplications,
while using a light field allows rendering a pixel by querying the field a
single time-as opposed to hundreds of queries when employing a radiance field.
Since the proposed representation can be implemented using a fragment shader,
it can be directly integrated with standard rasterization frameworks. Our
flexible implementation can render a NeRF in real-time with low memory
requirements and on a wide range of resource-constrained devices, including
mobiles and AR/VR headsets. Notably, we find that Re-ReND can achieve over a
2.6-fold increase in rendering speed versus the state-of-the-art without
perceptible losses in quality.

---

## RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or  missing camera parameters

nerf{: .label .label-blue }

2023-03-15 | Shuja Khalid, Frank Rudzicz | cs.CV | [PDF](http://arxiv.org/pdf/2303.08695v1){: .btn .btn-green }

**Abstract**: Novel view synthesis (NVS) is a challenging task in computer vision that
involves synthesizing new views of a scene from a limited set of input images.
Neural Radiance Fields (NeRF) have emerged as a powerful approach to address
this problem, but they require accurate knowledge of camera \textit{intrinsic}
and \textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM)
and multi-view stereo (MVS) approaches have been used to extract camera
parameters, but these methods can be unreliable and may fail in certain cases.
In this paper, we propose a novel technique that leverages unposed images from
dynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera
parameters directly from data. Our approach is highly extensible and can be
integrated into existing NeRF architectures with minimal modifications. We
demonstrate the effectiveness of our method on a variety of static and dynamic
scenes and show that it outperforms traditional SfM and MVS approaches. The
code for our method is publicly available at
\href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}.
Our approach offers a promising new direction for improving the accuracy and
robustness of NVS using NeRF, and we anticipate that it will be a valuable tool
for a wide range of applications in computer vision and graphics.

---

## Harnessing Low-Frequency Neural Fields for Few-Shot View Synthesis

nerf{: .label .label-blue }

2023-03-15 | Liangchen Song, Zhong Li, Xuan Gong, Lele Chen, Zhang Chen, Yi Xu, Junsong Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2303.08370v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have led to breakthroughs in the novel view
synthesis problem. Positional Encoding (P.E.) is a critical factor that brings
the impressive performance of NeRF, where low-dimensional coordinates are
mapped to high-dimensional space to better recover scene details. However,
blindly increasing the frequency of P.E. leads to overfitting when the
reconstruction problem is highly underconstrained, \eg, few-shot images for
training. We harness low-frequency neural fields to regularize high-frequency
neural fields from overfitting to better address the problem of few-shot view
synthesis. We propose reconstructing with a low-frequency only field and then
finishing details with a high-frequency equipped field. Unlike most existing
solutions that regularize the output space (\ie, rendered images), our
regularization is conducted in the input space (\ie, signal frequency). We
further propose a simple-yet-effective strategy for tuning the frequency to
avoid overfitting few-shot inputs: enforcing consistency among the frequency
domain of rendered 2D images. Thanks to the input space regularizing scheme,
our method readily applies to inputs beyond spatial locations, such as the time
dimension in dynamic scenes. Comparisons with state-of-the-art on both
synthetic and natural datasets validate the effectiveness of our proposed
solution for few-shot view synthesis. Code is available at
\href{https://github.com/lsongx/halo}{https://github.com/lsongx/halo}.

---

## I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via  Raytracing in Neural SDFs



2023-03-14 | Jingsen Zhu, Yuchi Huo, Qi Ye, Fujun Luan, Jifan Li, Dianbing Xi, Lisha Wang, Rui Tang, Wei Hua, Hujun Bao, Rui Wang | cs.CV | [PDF](http://arxiv.org/pdf/2303.07634v2){: .btn .btn-green }

**Abstract**: In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene
reconstruction and editing using differentiable Monte Carlo raytracing on
neural signed distance fields (SDFs). Our holistic neural SDF-based framework
jointly recovers the underlying shapes, incident radiance and materials from
multi-view images. We introduce a novel bubble loss for fine-grained small
objects and error-guided adaptive sampling scheme to largely improve the
reconstruction quality on large-scale indoor scenes. Further, we propose to
decompose the neural radiance field into spatially-varying material of the
scene as a neural field through surface-based, differentiable Monte Carlo
raytracing and emitter semantic segmentations, which enables physically based
and photorealistic scene relighting and editing applications. Through a number
of qualitative and quantitative experiments, we demonstrate the superior
quality of our method on indoor scene reconstruction, novel view synthesis, and
scene editing compared to state-of-the-art baselines.

Comments:
- Accepted by CVPR 2023, project page:
  https://jingsenzhu.github.io/i2-sdf

---

## Frequency-Modulated Point Cloud Rendering with Easy Editing

nerf{: .label .label-blue }

2023-03-14 | Yi Zhang, Xiaoyang Huang, Bingbing Ni, Teng Li, Wenjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2303.07596v2){: .btn .btn-green }

**Abstract**: We develop an effective point cloud rendering pipeline for novel view
synthesis, which enables high fidelity local detail reconstruction, real-time
rendering and user-friendly editing. In the heart of our pipeline is an
adaptive frequency modulation module called Adaptive Frequency Net (AFNet),
which utilizes a hypernetwork to learn the local texture frequency encoding
that is consecutively injected into adaptive frequency activation layers to
modulate the implicit radiance signal. This mechanism improves the frequency
expressive ability of the network with richer frequency basis support, only at
a small computational budget. To further boost performance, a preprocessing
module is also proposed for point cloud geometry optimization via point opacity
estimation. In contrast to implicit rendering, our pipeline supports
high-fidelity interactive editing based on point cloud manipulation. Extensive
experimental results on NeRF-Synthetic, ScanNet, DTU and Tanks and Temples
datasets demonstrate the superior performances achieved by our method in terms
of PSNR, SSIM and LPIPS, in comparison to the state-of-the-art.

Comments:
- Accepted by CVPR 2023

---

## NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from  Multi-view Images

nerf{: .label .label-blue }

2023-03-14 | Yunfan Ye, Renjiao Yi, Zhirui Gao, Chenyang Zhu, Zhiping Cai, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2303.07653v2){: .btn .btn-green }

**Abstract**: We study the problem of reconstructing 3D feature curves of an object from a
set of calibrated multi-view images. To do so, we learn a neural implicit field
representing the density distribution of 3D edges which we refer to as Neural
Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based
rendering loss where a 2D edge map is rendered at a given view and is compared
to the ground-truth edge map extracted from the image of that view. The
rendering-based differentiable optimization of NEF fully exploits 2D edge
detection, without needing a supervision of 3D edges, a 3D geometric operator
or cross-view edge correspondence. Several technical designs are devised to
ensure learning a range-limited and view-independent NEF for robust edge
extraction. The final parametric 3D curves are extracted from NEF with an
iterative optimization method. On our benchmark with synthetic data, we
demonstrate that NEF outperforms existing state-of-the-art methods on all
metrics. Project page: https://yunfan1202.github.io/NEF/.

Comments:
- CVPR 2023

---

## Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D  Generation

nerf{: .label .label-blue }

2023-03-14 | Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2303.07937v3){: .btn .btn-green }

**Abstract**: Text-to-3D generation has shown rapid progress in recent days with the advent
of score distillation, a methodology of using pretrained text-to-2D diffusion
models to optimize neural radiance field (NeRF) in the zero-shot setting.
However, the lack of 3D awareness in the 2D diffusion models destabilizes score
distillation-based methods from reconstructing a plausible 3D scene. To address
this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness
into pretrained 2D diffusion models, enhancing the robustness and 3D
consistency of score distillation-based methods. We realize this by first
constructing a coarse 3D structure of a given text prompt and then utilizing
projected, view-specific depth map as a condition for the diffusion model.
Additionally, we introduce a training strategy that enables the 2D diffusion
model learns to handle the errors and sparsity within the coarse 3D structure
for robust generation, as well as a method for ensuring semantic consistency
throughout all viewpoints of the scene. Our framework surpasses the limitations
of prior arts, and has significant implications for 3D consistent generation of
2D diffusion models.

Comments:
- Project page https://ku-cvlab.github.io/3DFuse/

---

## MELON: NeRF with Unposed Images in SO(3)

nerf{: .label .label-blue }

2023-03-14 | Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein, Dmitry Lagun | cs.CV | [PDF](http://arxiv.org/pdf/2303.08096v2){: .btn .btn-green }

**Abstract**: Neural radiance fields enable novel-view synthesis and scene reconstruction
with photorealistic quality from a few images, but require known and accurate
camera poses. Conventional pose estimation algorithms fail on smooth or
self-similar scenes, while methods performing inverse rendering from unposed
views require a rough initialization of the camera orientations. The main
difficulty of pose estimation lies in real-life objects being almost invariant
under certain transformations, making the photometric distance between rendered
views non-convex with respect to the camera parameters. Using an equivalence
relation that matches the distribution of local minima in camera space, we
reduce this space to its quotient set, in which pose estimation becomes a more
convex problem. Using a neural-network to regularize pose estimation, we
demonstrate that our method - MELON - can reconstruct a neural radiance field
from unposed images with state-of-the-art accuracy while requiring ten times
fewer views than adversarial approaches.

---

## FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency  Regularization

nerf{: .label .label-blue }

2023-03-13 | Jiawei Yang, Marco Pavone, Yue Wang | cs.CV | [PDF](http://arxiv.org/pdf/2303.07418v1){: .btn .btn-green }

**Abstract**: Novel view synthesis with sparse inputs is a challenging problem for neural
radiance fields (NeRF). Recent efforts alleviate this challenge by introducing
external supervision, such as pre-trained models and extra depth signals, and
by non-trivial patch-based rendering. In this paper, we present Frequency
regularized NeRF (FreeNeRF), a surprisingly simple baseline that outperforms
previous methods with minimal modifications to the plain NeRF. We analyze the
key challenges in few-shot neural rendering and find that frequency plays an
important role in NeRF's training. Based on the analysis, we propose two
regularization terms. One is to regularize the frequency range of NeRF's
inputs, while the other is to penalize the near-camera density fields. Both
techniques are ``free lunches'' at no additional computational cost. We
demonstrate that even with one line of code change, the original NeRF can
achieve similar performance as other complicated methods in the few-shot
setting. FreeNeRF achieves state-of-the-art performance across diverse
datasets, including Blender, DTU, and LLFF. We hope this simple baseline will
motivate a rethinking of the fundamental role of frequency in NeRF's training
under the low-data regime and beyond.

Comments:
- Project page: https://jiawei-yang.github.io/FreeNeRF/, Code at:
  https://github.com/Jiawei-Yang/FreeNeRF

---

## NeRFLiX: High-Quality Neural View Synthesis by Learning a  Degradation-Driven Inter-viewpoint MiXer

nerf{: .label .label-blue }

2023-03-13 | Kun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, Jiangbo Lu | cs.CV | [PDF](http://arxiv.org/pdf/2303.06919v2){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) show great success in novel view synthesis.
However, in real-world scenes, recovering high-quality details from the source
images is still challenging for the existing NeRF-based approaches, due to the
potential imperfect calibration information and scene representation
inaccuracy. Even with high-quality training frames, the synthetic novel views
produced by NeRF models still suffer from notable rendering artifacts, such as
noise, blur, etc. Towards to improve the synthesis quality of NeRF-based
approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by
learning a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for existing deep neural networks. Moreover, beyond the degradation
removal, we propose an inter-viewpoint aggregation framework that is able to
fuse highly related high-quality training images, pushing the performance of
cutting-edge NeRF models to entirely new levels and producing highly
photo-realistic synthetic views.

Comments:
- Accepted to CVPR 2023; Project Page: see
  https://redrock303.github.io/nerflix/

---

## Just Flip: Flipped Observation Generation and Optimization for Neural  Radiance Fields to Cover Unobserved View

nerf{: .label .label-blue }

2023-03-11 | Minjae Lee, Kyeongsu Kang, Hyeonwoo Yu | cs.RO | [PDF](http://arxiv.org/pdf/2303.06335v3){: .btn .btn-green }

**Abstract**: With the advent of Neural Radiance Field (NeRF), representing 3D scenes
through multiple observations has shown remarkable improvements in performance.
Since this cutting-edge technique is able to obtain high-resolution renderings
by interpolating dense 3D environments, various approaches have been proposed
to apply NeRF for the spatial understanding of robot perception. However,
previous works are challenging to represent unobserved scenes or views on the
unexplored robot trajectory, as these works do not take into account 3D
reconstruction without observation information. To overcome this problem, we
propose a method to generate flipped observation in order to cover unexisting
observation for unexplored robot trajectory. To achieve this, we propose a data
augmentation method for 3D reconstruction using NeRF by flipping observed
images, and estimating flipped camera 6DOF poses. Our technique exploits the
property of objects being geometrically symmetric, making it simple but fast
and powerful, thereby making it suitable for robotic applications where
real-time performance is important. We demonstrate that our method
significantly improves three representative perceptual quality measures on the
NeRF synthetic dataset.

---

## Aleth-NeRF: Low-light Condition View Synthesis with Concealing Fields

nerf{: .label .label-blue }

2023-03-10 | Ziteng Cui, Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada | cs.CV | [PDF](http://arxiv.org/pdf/2303.05807v2){: .btn .btn-green }

**Abstract**: Common capture low-light scenes are challenging for most computer vision
techniques, including Neural Radiance Fields (NeRF). Vanilla NeRF is
viewer-centred simplifies the rendering process only as light emission from 3D
locations in the viewing direction, thus failing to model the low-illumination
induced darkness. Inspired by the emission theory of ancient Greeks that visual
perception is accomplished by rays casting from eyes, we make slight
modifications on vanilla NeRF to train on multiple views of low-light scenes,
we can thus render out the well-lit scene in an unsupervised manner. We
introduce a surrogate concept, Concealing Fields, that reduces the transport of
light during the volume rendering stage. Specifically, our proposed method,
Aleth-NeRF, directly learns from the dark image to understand volumetric object
representation and concealing field under priors. By simply eliminating
Concealing Fields, we can render a single or multi-view well-lit image(s) and
gain superior performance over other 2D low-light enhancement methods.
Additionally, we collect the first paired LOw-light and normal-light Multi-view
(LOM) datasets for future research. This version is invalid, please refer to
our new AAAI version: arXiv:2312.09093

Comments:
- website page: https://cuiziteng.github.io/Aleth_NeRF_web/, refer to
  new version: arXiv:2312.09093

---

## Hardware Acceleration of Neural Graphics

nerf{: .label .label-blue }

2023-03-10 | Muhammad Husnain Mubarik, Ramakrishna Kanungo, Tobias Zirr, Rakesh Kumar | cs.AR | [PDF](http://arxiv.org/pdf/2303.05735v6){: .btn .btn-green }

**Abstract**: Rendering and inverse-rendering algorithms that drive conventional computer
graphics have recently been superseded by neural representations (NR). NRs have
recently been used to learn the geometric and the material properties of the
scenes and use the information to synthesize photorealistic imagery, thereby
promising a replacement for traditional rendering algorithms with scalable
quality and predictable performance. In this work we ask the question: Does
neural graphics (NG) need hardware support? We studied representative NG
applications showing that, if we want to render 4k res. at 60FPS there is a gap
of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,
there is an even larger gap of 2-4 OOM between the desired performance and the
required system power. We identify that the input encoding and the MLP kernels
are the performance bottlenecks, consuming 72%,60% and 59% of application time
for multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,
respectively. We propose a NG processing cluster, a scalable and flexible
hardware architecture that directly accelerates the input encoding and MLP
kernels through dedicated engines and supports a wide range of NG applications.
We also accelerate the rest of the kernels by fusing them together in Vulkan,
which leads to 9.94X kernel-level performance improvement compared to un-fused
implementation of the pre-processing and the post-processing kernels. Our
results show that, NGPC gives up to 58X end-to-end application-level
performance improvement, for multi res. hashgrid encoding on average across the
four NG applications, the performance benefits are 12X,20X,33X and 39X for the
scaling factor of 8,16,32 and 64, respectively. Our results show that with
multi res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS
for NeRF and 8k res. at 120FPS for all our other NG applications.

---

## Self-NeRF: A Self-Training Pipeline for Few-Shot Neural Radiance Fields

nerf{: .label .label-blue }

2023-03-10 | Jiayang Bai, Letian Huang, Wen Gong, Jie Guo, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2303.05775v1){: .btn .btn-green }

**Abstract**: Recently, Neural Radiance Fields (NeRF) have emerged as a potent method for
synthesizing novel views from a dense set of images. Despite its impressive
performance, NeRF is plagued by its necessity for numerous calibrated views and
its accuracy diminishes significantly in a few-shot setting. To address this
challenge, we propose Self-NeRF, a self-evolved NeRF that iteratively refines
the radiance fields with very few number of input views, without incorporating
additional priors. Basically, we train our model under the supervision of
reference and unseen views simultaneously in an iterative procedure. In each
iteration, we label unseen views with the predicted colors or warped pixels
generated by the model from the preceding iteration. However, these expanded
pseudo-views are afflicted by imprecision in color and warping artifacts, which
degrades the performance of NeRF. To alleviate this issue, we construct an
uncertainty-aware NeRF with specialized embeddings. Some techniques such as
cone entropy regularization are further utilized to leverage the pseudo-views
in the most efficient manner. Through experiments under various settings, we
verified that our Self-NeRF is robust to input with uncertainty and surpasses
existing methods when trained on limited training data.

Comments:
- 11 pages, 11 figures

---

## MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field

nerf{: .label .label-blue }

2023-03-10 | Kaizhi Yang, Xiaoshuai Zhang, Zhiao Huang, Xuejin Chen, Zexiang Xu, Hao Su | cs.CV | [PDF](http://arxiv.org/pdf/2303.05703v2){: .btn .btn-green }

**Abstract**: We present MovingParts, a NeRF-based method for dynamic scene reconstruction
and part discovery. We consider motion as an important cue for identifying
parts, that all particles on the same part share the common motion pattern.
From the perspective of fluid simulation, existing deformation-based methods
for dynamic NeRF can be seen as parameterizing the scene motion under the
Eulerian view, i.e., focusing on specific locations in space through which the
fluid flows as time passes. However, it is intractable to extract the motion of
constituting objects or parts using the Eulerian view representation. In this
work, we introduce the dual Lagrangian view and enforce representations under
the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian
view, we parameterize the scene motion by tracking the trajectory of particles
on objects. The Lagrangian view makes it convenient to discover parts by
factorizing the scene motion as a composition of part-level rigid motions.
Experimentally, our method can achieve fast and high-quality dynamic scene
reconstruction from even a single moving camera, and the induced part-based
representation allows direct applications of part tracking, animation, 3D scene
editing, etc.

Comments:
- Project Page: https://silenkzyoung.github.io/MovingParts-WebPage/

---

## You Only Train Once: Multi-Identity Free-Viewpoint Neural Human  Rendering from Monocular Videos

nerf{: .label .label-blue }

2023-03-10 | Jaehyeok Kim, Dongyoon Wee, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2303.05835v1){: .btn .btn-green }

**Abstract**: We introduce You Only Train Once (YOTO), a dynamic human generation
framework, which performs free-viewpoint rendering of different human
identities with distinct motions, via only one-time training from monocular
videos. Most prior works for the task require individualized optimization for
each input video that contains a distinct human identity, leading to a
significant amount of time and resources for the deployment, thereby impeding
the scalability and the overall application potential of the system. In this
paper, we tackle this problem by proposing a set of learnable identity codes to
expand the capability of the framework for multi-identity free-viewpoint
rendering, and an effective pose-conditioned code query mechanism to finely
model the pose-dependent non-rigid motions. YOTO optimizes neural radiance
fields (NeRF) by utilizing designed identity codes to condition the model for
learning various canonical T-pose appearances in a single shared volumetric
representation. Besides, our joint learning of multiple identities within a
unified model incidentally enables flexible motion transfer in high-quality
photo-realistic renderings for all learned appearances. This capability expands
its potential use in important applications, including Virtual Reality. We
present extensive experimental results on ZJU-MoCap and PeopleSnapshot to
clearly demonstrate the effectiveness of our proposed model. YOTO shows
state-of-the-art performance on all evaluation metrics while showing
significant benefits in training and inference efficiency as well as rendering
quality. The code and model will be made publicly available soon.

---

## Learning Object-Centric Neural Scattering Functions for Free-Viewpoint  Relighting and Scene Composition



2023-03-10 | Hong-Xing Yu, Michelle Guo, Alireza Fathi, Yen-Yu Chang, Eric Ryan Chan, Ruohan Gao, Thomas Funkhouser, Jiajun Wu | cs.CV | [PDF](http://arxiv.org/pdf/2303.06138v4){: .btn .btn-green }

**Abstract**: Photorealistic object appearance modeling from 2D images is a constant topic
in vision and graphics. While neural implicit methods (such as Neural Radiance
Fields) have shown high-fidelity view synthesis results, they cannot relight
the captured objects. More recent neural inverse rendering approaches have
enabled object relighting, but they represent surface properties as simple
BRDFs, and therefore cannot handle translucent objects. We propose
Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct
object appearance from only images. OSFs not only support free-viewpoint object
relighting, but also can model both opaque and translucent objects. While
accurately modeling subsurface light transport for translucent objects can be
highly complex and even intractable for neural methods, OSFs learn to
approximate the radiance transfer from a distant light to an outgoing direction
at any spatial location. This approximation avoids explicitly modeling complex
subsurface scattering, making learning a neural implicit model tractable.
Experiments on real and synthetic data show that OSFs accurately reconstruct
appearances for both opaque and translucent objects, allowing faithful
free-viewpoint relighting as well as scene composition.

Comments:
- Journal extension of arXiv:2012.08503 (TMLR 2023). The first two
  authors contributed equally to this work. Project page:
  https://kovenyu.com/osf/

---

## NeRFlame: FLAME-based conditioning of NeRF for 3D face rendering

nerf{: .label .label-blue }

2023-03-10 | Wojciech Zając, Joanna Waczyńska, Piotr Borycki, Jacek Tabor, Maciej Zięba, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2303.06226v2){: .btn .btn-green }

**Abstract**: Traditional 3D face models are based on mesh representations with texture.
One of the most important models is FLAME (Faces Learned with an Articulated
Model and Expressions), which produces meshes of human faces that are fully
controllable. Unfortunately, such models have problems with capturing geometric
and appearance details. In contrast to mesh representation, the neural radiance
field (NeRF) produces extremely sharp renders. However, implicit methods are
hard to animate and do not generalize well to unseen expressions. It is not
trivial to effectively control NeRF models to obtain face manipulation.
  The present paper proposes a novel approach, named NeRFlame, which combines
the strengths of both NeRF and FLAME methods. Our method enables high-quality
rendering capabilities of NeRF while also offering complete control over the
visual appearance, similar to FLAME. In contrast to traditional NeRF-based
structures that use neural networks for RGB color and volume density modeling,
our approach utilizes the FLAME mesh as a distinct density volume.
Consequently, color values exist only in the vicinity of the FLAME mesh. This
FLAME framework is seamlessly incorporated into the NeRF architecture for
predicting RGB colors, enabling our model to explicitly represent volume
density and implicitly capture RGB colors.

---

## PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for  Geometry-Agnostic System Identification

nerf{: .label .label-blue }

2023-03-09 | Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy Jatavallabhula, Ming Lin, Chenfanfu Jiang, Chuang Gan | cs.CV | [PDF](http://arxiv.org/pdf/2303.05512v1){: .btn .btn-green }

**Abstract**: Existing approaches to system identification (estimating the physical
parameters of an object) from videos assume known object geometries. This
precludes their applicability in a vast majority of scenes where object
geometries are complex or unknown. In this work, we aim to identify parameters
characterizing a physical system from a set of multi-view videos without any
assumption on object geometry or topology. To this end, we propose "Physics
Augmented Continuum Neural Radiance Fields" (PAC-NeRF), to estimate both the
unknown geometry and physical parameters of highly dynamic objects from
multi-view videos. We design PAC-NeRF to only ever produce physically plausible
states by enforcing the neural radiance field to follow the conservation laws
of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian
representation of the neural radiance field, i.e., we use the Eulerian grid
representation for NeRF density and color fields, while advecting the neural
radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian
representation seamlessly blends efficient neural rendering with the material
point method (MPM) for robust differentiable physics simulation. We validate
the effectiveness of our proposed framework on geometry and physical parameter
estimation over a vast range of materials, including elastic bodies,
plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate
significant performance gain on most tasks.

Comments:
- ICLR 2023 Spotlight. Project page:
  https://sites.google.com/view/PAC-NeRF

---

## DroNeRF: Real-time Multi-agent Drone Pose Optimization for Computing  Neural Radiance Fields

nerf{: .label .label-blue }

2023-03-08 | Dipam Patel, Phu Pham, Aniket Bera | cs.RO | [PDF](http://arxiv.org/pdf/2303.04322v2){: .btn .btn-green }

**Abstract**: We present a novel optimization algorithm called DroNeRF for the autonomous
positioning of monocular camera drones around an object for real-time 3D
reconstruction using only a few images. Neural Radiance Fields or NeRF, is a
novel view synthesis technique used to generate new views of an object or scene
from a set of input images. Using drones in conjunction with NeRF provides a
unique and dynamic way to generate novel views of a scene, especially with
limited scene capabilities of restricted movements. Our approach focuses on
calculating optimized pose for individual drones while solely depending on the
object geometry without using any external localization system. The unique
camera positioning during the data-capturing phase significantly impacts the
quality of the 3D model. To evaluate the quality of our generated novel views,
we compute different perceptual metrics like the Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure(SSIM). Our work demonstrates the
benefit of using an optimal placement of various drones with limited mobility
to generate perceptually better results.

Comments:
- To appear in 2023 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2023)

---

## CROSSFIRE: Camera Relocalization On Self-Supervised Features from an  Implicit Representation



2023-03-08 | Arthur Moreau, Nathan Piasco, Moussab Bennehar, Dzmitry Tsishkou, Bogdan Stanciulescu, Arnaud de La Fortelle | cs.CV | [PDF](http://arxiv.org/pdf/2303.04869v2){: .btn .btn-green }

**Abstract**: Beyond novel view synthesis, Neural Radiance Fields are useful for
applications that interact with the real world. In this paper, we use them as
an implicit map of a given scene and propose a camera relocalization algorithm
tailored for this representation. The proposed method enables to compute in
real-time the precise position of a device using a single RGB camera, during
its navigation. In contrast with previous work, we do not rely on pose
regression or photometric alignment but rather use dense local features
obtained through volumetric rendering which are specialized on the scene with a
self-supervised objective. As a result, our algorithm is more accurate than
competitors, able to operate in dynamic outdoor environments with changing
lightning conditions and can be readily integrated in any volumetric neural
renderer.

Comments:
- Accepted to ICCV 2023

---

## InFusionSurf: Refining Neural RGB-D Surface Reconstruction Using  Per-Frame Intrinsic Refinement and TSDF Fusion Prior Learning

nerf{: .label .label-blue }

2023-03-08 | Seunghwan Lee, Gwanmo Park, Hyewon Son, Jiwon Ryu, Han Joo Chae | cs.CV | [PDF](http://arxiv.org/pdf/2303.04508v2){: .btn .btn-green }

**Abstract**: We introduce InFusionSurf, a novel approach to enhance the fidelity of neural
radiance field (NeRF) frameworks for 3D surface reconstruction using RGB-D
video frames. Building upon previous methods that have employed feature
encoding to improve optimization speed, we further improve the reconstruction
quality with minimal impact on optimization time by refining depth information.
Our per-frame intrinsic refinement scheme addresses frame-specific blurs caused
by camera motion in each depth frame. Furthermore, InFusionSurf utilizes a
classical real-time 3D surface reconstruction method, the truncated signed
distance field (TSDF) Fusion, as prior knowledge to pretrain the feature grid
to support reconstruction details while accelerating the training. The
quantitative and qualitative experiments comparing the performances of
InFusionSurf against prior work indicate that our method is capable of
accurately reconstructing a scene without sacrificing optimization speed. We
also demonstrate the effectiveness of our per-frame intrinsic refinement and
TSDF Fusion prior learning techniques via an ablation study.

---

## NEPHELE: A Neural Platform for Highly Realistic Cloud Radiance Rendering

nerf{: .label .label-blue }

2023-03-07 | Haimin Luo, Siyuan Zhang, Fuqiang Zhao, Haotian Jing, Penghao Wang, Zhenxiao Yu, Dongxue Yan, Junran Ding, Boyuan Zhang, Qiang Hu, Shu Yin, Lan Xu, JIngyi Yu | cs.GR | [PDF](http://arxiv.org/pdf/2303.04086v1){: .btn .btn-green }

**Abstract**: We have recently seen tremendous progress in neural rendering (NR) advances,
i.e., NeRF, for photo-real free-view synthesis. Yet, as a local technique based
on a single computer/GPU, even the best-engineered Instant-NGP or i-NGP cannot
reach real-time performance when rendering at a high resolution, and often
requires huge local computing resources. In this paper, we resort to cloud
rendering and present NEPHELE, a neural platform for highly realistic cloud
radiance rendering. In stark contrast with existing NR approaches, our NEPHELE
allows for more powerful rendering capabilities by combining multiple remote
GPUs and facilitates collaboration by allowing multiple people to view the same
NeRF scene simultaneously. We introduce i-NOLF to employ opacity light fields
for ultra-fast neural radiance rendering in a one-query-per-ray manner. We
further resemble the Lumigraph with geometry proxies for fast ray querying and
subsequently employ a small MLP to model the local opacity lumishperes for
high-quality rendering. We also adopt Perfect Spatial Hashing in i-NOLF to
enhance cache coherence. As a result, our i-NOLF achieves an order of magnitude
performance gain in terms of efficiency than i-NGP, especially for the
multi-user multi-viewpoint setting under cloud rendering scenarios. We further
tailor a task scheduler accompanied by our i-NOLF representation and
demonstrate the advance of our methodological design through a comprehensive
cloud platform, consisting of a series of cooperated modules, i.e., render
farms, task assigner, frame composer, and detailed streaming strategies. Using
such a cloud platform compatible with neural rendering, we further showcase the
capabilities of our cloud radiance rendering through a series of applications,
ranging from cloud VR/AR rendering.

---

## Multiscale Tensor Decomposition and Rendering Equation Encoding for View  Synthesis

nerf{: .label .label-blue }

2023-03-07 | Kang Han, Wei Xiang | cs.CV | [PDF](http://arxiv.org/pdf/2303.03808v2){: .btn .btn-green }

**Abstract**: Rendering novel views from captured multi-view images has made considerable
progress since the emergence of the neural radiance field. This paper aims to
further advance the quality of view synthesis by proposing a novel approach
dubbed the neural radiance feature field (NRFF). We first propose a multiscale
tensor decomposition scheme to organize learnable features so as to represent
scenes from coarse to fine scales. We demonstrate many benefits of the proposed
multiscale representation, including more accurate scene shape and appearance
reconstruction, and faster convergence compared with the single-scale
representation. Instead of encoding view directions to model view-dependent
effects, we further propose to encode the rendering equation in the feature
space by employing the anisotropic spherical Gaussian mixture predicted from
the proposed multiscale representation. The proposed NRFF improves
state-of-the-art rendering results by over 1 dB in PSNR on both the NeRF and
NSVF synthetic datasets. A significant improvement has also been observed on
the real-world Tanks & Temples dataset. Code can be found at
https://github.com/imkanghan/nrff.

---

## Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene  Representation from 2D Supervision

nerf{: .label .label-blue }

2023-03-06 | Xiaoshuai Zhang, Abhijit Kundu, Thomas Funkhouser, Leonidas Guibas, Hao Su, Kyle Genova | cs.CV | [PDF](http://arxiv.org/pdf/2303.03361v2){: .btn .btn-green }

**Abstract**: We address efficient and structure-aware 3D scene representation from images.
Nerflets are our key contribution -- a set of local neural radiance fields that
together represent a scene. Each nerflet maintains its own spatial position,
orientation, and extent, within which it contributes to panoptic, density, and
radiance reconstructions. By leveraging only photometric and inferred panoptic
image supervision, we can directly and jointly optimize the parameters of a set
of nerflets so as to form a decomposed representation of the scene, where each
object instance is represented by a group of nerflets. During experiments with
indoor and outdoor environments, we find that nerflets: (1) fit and approximate
the scene more efficiently than traditional global NeRFs, (2) allow the
extraction of panoptic and photometric renderings from arbitrary views, and (3)
enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive
editing.

Comments:
- accepted by CVPR 2023

---

## MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal  calibration

nerf{: .label .label-blue }

2023-03-06 | Quentin Herau, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux | cs.CV | [PDF](http://arxiv.org/pdf/2303.03056v3){: .btn .btn-green }

**Abstract**: With the recent advances in autonomous driving and the decreasing cost of
LiDARs, the use of multimodal sensor systems is on the rise. However, in order
to make use of the information provided by a variety of complimentary sensors,
it is necessary to accurately calibrate them. We take advantage of recent
advances in computer graphics and implicit volumetric scene representation to
tackle the problem of multi-sensor spatial and temporal calibration. Thanks to
a new formulation of the Neural Radiance Field (NeRF) optimization, we are able
to jointly optimize calibration parameters along with scene representation
based on radiometric and geometric measurements. Our method enables accurate
and robust calibration from data captured in uncontrolled and unstructured
urban environments, making our solution more scalable than existing calibration
solutions. We demonstrate the accuracy and robustness of our method in urban
scenes typically encountered in autonomous driving scenarios.

Comments:
- Accepted at IROS2023 Project site: https://qherau.github.io/MOISST/

---

## Efficient Large-scale Scene Representation with a Hybrid of  High-resolution Grid and Plane Features

nerf{: .label .label-blue }

2023-03-06 | Yuqi Zhang, Guanying Chen, Shuguang Cui | cs.CV | [PDF](http://arxiv.org/pdf/2303.03003v2){: .btn .btn-green }

**Abstract**: Existing neural radiance fields (NeRF) methods for large-scale scene modeling
require days of training using multiple GPUs, hindering their applications in
scenarios with limited computing resources. Despite fast optimization NeRF
variants have been proposed based on the explicit dense or hash grid features,
their effectivenesses are mainly demonstrated in object-scale scene
representation. In this paper, we point out that the low feature resolution in
explicit representation is the bottleneck for large-scale unbounded scene
representation. To address this problem, we introduce a new and efficient
hybrid feature representation for NeRF that fuses the 3D hash-grids and
high-resolution 2D dense plane features. Compared with the dense-grid
representation, the resolution of a dense 2D plane can be scaled up more
efficiently. Based on this hybrid representation, we propose a fast
optimization NeRF variant, called GP-NeRF, that achieves better rendering
results while maintaining a compact model size. Extensive experiments on
multiple large-scale unbounded scene datasets show that our model can converge
in 1.5 hours using a single GPU while achieving results comparable to or even
better than the existing method that requires about one day's training with 8
GPUs.

---

## Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild

nerf{: .label .label-blue }

2023-03-05 | Jaewon Lee, Injae Kim, Hwan Heo, Hyunwoo J. Kim | cs.CV | [PDF](http://arxiv.org/pdf/2303.03966v1){: .btn .btn-green }

**Abstract**: We present a learning framework for reconstructing neural scene
representations from a small number of unconstrained tourist photos. Since each
image contains transient occluders, decomposing the static and transient
components is necessary to construct radiance fields with such in-the-wild
photographs where existing methods require a lot of training data. We introduce
SF-NeRF, aiming to disentangle those two components with only a few images
given, which exploits semantic information without any supervision. The
proposed method contains an occlusion filtering module that predicts the
transient color and its opacity for each pixel, which enables the NeRF model to
solely learn the static scene representation. This filtering module learns the
transient phenomena guided by pixel-wise semantic features obtained by a
trainable image encoder that can be trained across multiple scenes to learn the
prior of transient objects. Furthermore, we present two techniques to prevent
ambiguous decomposition and noisy results of the filtering module. We
demonstrate that our method outperforms state-of-the-art novel view synthesis
methods on Phototourism dataset in a few-shot setting.

Comments:
- 11 pages, 5 figures

---

## Delicate Textured Mesh Recovery from NeRF via Adaptive Surface  Refinement

nerf{: .label .label-blue }

2023-03-03 | Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, Gang Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2303.02091v2){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) have constituted a remarkable breakthrough in
image-based 3D reconstruction. However, their implicit volumetric
representations differ significantly from the widely-adopted polygonal meshes
and lack support from common 3D software and hardware, making their rendering
and manipulation inefficient. To overcome this limitation, we present a novel
framework that generates textured surface meshes from images. Our approach
begins by efficiently initializing the geometry and view-dependency decomposed
appearance with a NeRF. Subsequently, a coarse mesh is extracted, and an
iterative surface refining algorithm is developed to adaptively adjust both
vertex positions and face density based on re-projected rendering errors. We
jointly refine the appearance with geometry and bake it into texture images for
real-time rendering. Extensive experiments demonstrate that our method achieves
superior mesh quality and competitive rendering quality.

Comments:
- ICCV 2023 camera-ready, Project Page: https://me.kiui.moe/nerf2mesh

---

## Multi-Plane Neural Radiance Fields for Novel View Synthesis

nerf{: .label .label-blue }

2023-03-03 | Youssef Abdelkareem, Shady Shehata, Fakhri Karray | cs.CV | [PDF](http://arxiv.org/pdf/2303.01736v1){: .btn .btn-green }

**Abstract**: Novel view synthesis is a long-standing problem that revolves around
rendering frames of scenes from novel camera viewpoints. Volumetric approaches
provide a solution for modeling occlusions through the explicit 3D
representation of the camera frustum. Multi-plane Images (MPI) are volumetric
methods that represent the scene using front-parallel planes at distinct depths
but suffer from depth discretization leading to a 2.D scene representation.
Another line of approach relies on implicit 3D scene representations. Neural
Radiance Fields (NeRF) utilize neural networks for encapsulating the continuous
3D scene structure within the network weights achieving photorealistic
synthesis results, however, methods are constrained to per-scene optimization
settings which are inefficient in practice. Multi-plane Neural Radiance Fields
(MINE) open the door for combining implicit and explicit scene representations.
It enables continuous 3D scene representations, especially in the depth
dimension, while utilizing the input image features to avoid per-scene
optimization. The main drawback of the current literature work in this domain
is being constrained to single-view input, limiting the synthesis ability to
narrow viewpoint ranges. In this work, we thoroughly examine the performance,
generalization, and efficiency of single-view multi-plane neural radiance
fields. In addition, we propose a new multiplane NeRF architecture that accepts
multiple views to improve the synthesis results and expand the viewing range.
Features from the input source frames are effectively fused through a proposed
attention-aware fusion module to highlight important information from different
viewpoints. Experiments show the effectiveness of attention-based fusion and
the promising outcomes of our proposed method when compared to multi-view NeRF
and MPI techniques.

Comments:
- ICDIPV 2023

---

## S-NeRF: Neural Radiance Fields for Street Views

nerf{: .label .label-blue }

2023-03-01 | Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2303.00749v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) aim to synthesize novel views of objects and
scenes, given the object-centric camera views with large overlaps. However, we
conjugate that this paradigm does not fit the nature of the street views that
are collected by many self-driving cars from the large-scale unbounded scenes.
Also, the onboard cameras perceive scenes without much overlapping. Thus,
existing NeRFs often produce blurs, 'floaters' and other artifacts on
street-view synthesis. In this paper, we propose a new street-view NeRF
(S-NeRF) that considers novel view synthesis of both the large-scale background
scenes and the foreground moving vehicles jointly. Specifically, we improve the
scene parameterization function and the camera poses for learning better neural
representations from street views. We also use the the noisy and sparse LiDAR
points to boost the training and learn a robust geometry and reprojection based
confidence to address the depth outliers. Moreover, we extend our S-NeRF for
reconstructing moving vehicles that is impracticable for conventional NeRFs.
Thorough experiments on the large-scale driving datasets (e.g., nuScenes and
Waymo) demonstrate that our method beats the state-of-the-art rivals by
reducing 7% to 40% of the mean-squared error in the street-view synthesis and a
45% PSNR gain for the moving vehicles rendering.

Comments:
- ICLR 2023

---

## Renderable Neural Radiance Map for Visual Navigation



2023-03-01 | Obin Kwon, Jeongho Park, Songhwai Oh | cs.CV | [PDF](http://arxiv.org/pdf/2303.00304v4){: .btn .btn-green }

**Abstract**: We propose a novel type of map for visual navigation, a renderable neural
radiance map (RNR-Map), which is designed to contain the overall visual
information of a 3D environment. The RNR-Map has a grid form and consists of
latent codes at each pixel. These latent codes are embedded from image
observations, and can be converted to the neural radiance field which enables
image rendering given a camera pose. The recorded latent codes implicitly
contain visual information about the environment, which makes the RNR-Map
visually descriptive. This visual information in RNR-Map can be a useful
guideline for visual localization and navigation. We develop localization and
navigation frameworks that can effectively utilize the RNR-Map. We evaluate the
proposed frameworks on camera tracking, visual localization, and image-goal
navigation. Experimental results show that the RNR-Map-based localization
framework can find the target location based on a single query image with fast
speed and competitive accuracy compared to other baselines. Also, this
localization framework is robust to environmental changes, and even finds the
most visually similar places when a query image from a different environment is
given. The proposed navigation framework outperforms the existing image-goal
navigation methods in difficult scenarios, under odometry and actuation noises.
The navigation framework shows 65.7% success rate in curved scenarios of the
NRNS dataset, which is an improvement of 18.6% over the current
state-of-the-art. Project page: https://rllab-snu.github.io/projects/RNR-Map/

Comments:
- Preprint version. CVPR 2023 accepted, highlight paper. Project page:
  https://rllab-snu.github.io/projects/RNR-Map/

---

## Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface



2023-02-28 | Decai Chen, Haofei Lu, Ingo Feldmann, Oliver Schreer, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2303.00050v1){: .btn .btn-green }

**Abstract**: Reconstructing general dynamic scenes is important for many computer vision
and graphics applications. Recent works represent the dynamic scene with neural
radiance fields for photorealistic view synthesis, while their surface geometry
is under-constrained and noisy. Other works introduce surface constraints to
the implicit neural representation to disentangle the ambiguity of geometry and
appearance field for static scene reconstruction. To bridge the gap between
rendering dynamic scenes and recovering static surface geometry, we propose a
template-free method to reconstruct surface geometry and appearance using
neural implicit representations from multi-view videos. We leverage
topology-aware deformation and the signed distance field to learn complex
dynamic surfaces via differentiable volume rendering without scene-specific
prior knowledge like template models. Furthermore, we propose a novel
mask-based ray selection strategy to significantly boost the optimization on
challenging time-varying regions. Experiments on different multi-view video
datasets demonstrate that our method achieves high-fidelity surface
reconstruction as well as photorealistic novel view synthesis.

Comments:
- 5 pages, accepted by ICASSP 2023

---

## IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF

nerf{: .label .label-blue }

2023-02-28 | Bo Peng, Jun Hu, Jingtao Zhou, Xuan Gao, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2302.14683v2){: .btn .btn-green }

**Abstract**: Recently, many works have been proposed to utilize the neural radiance field
for novel view synthesis of human performers. However, most of these methods
require hours of training, making them difficult for practical use. To address
this challenging problem, we propose IntrinsicNGP, which can train from scratch
and achieve high-fidelity results in few minutes with videos of a human
performer. To achieve this target, we introduce a continuous and optimizable
intrinsic coordinate rather than the original explicit Euclidean coordinate in
the hash encoding module of instant-NGP. With this novel intrinsic coordinate,
IntrinsicNGP can aggregate inter-frame information for dynamic objects with the
help of proxy geometry shapes. Moreover, the results trained with the given
rough geometry shapes can be further refined with an optimizable offset field
based on the intrinsic coordinate.Extensive experimental results on several
datasets demonstrate the effectiveness and efficiency of IntrinsicNGP. We also
illustrate our approach's ability to edit the shape of reconstructed subjects.

Comments:
- Project page:https://ustc3dv.github.io/IntrinsicNGP/. arXiv admin
  note: substantial text overlap with arXiv:2210.01651

---

## BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling

nerf{: .label .label-blue }

2023-02-27 | Sameera Ramasinghe, Violetta Shevchenko, Gil Avraham, Anton Van Den Hengel | cs.CV | [PDF](http://arxiv.org/pdf/2302.13543v3){: .btn .btn-green }

**Abstract**: Reasoning the 3D structure of a non-rigid dynamic scene from a single moving
camera is an under-constrained problem. Inspired by the remarkable progress of
neural radiance fields (NeRFs) in photo-realistic novel view synthesis of
static scenes, extensions have been proposed for dynamic settings. These
methods heavily rely on neural priors in order to regularize the problem. In
this work, we take a step back and reinvestigate how current implementations
may entail deleterious effects, including limited expressiveness, entanglement
of light and density fields, and sub-optimal motion localization. As a remedy,
we advocate for a bridge between classic non-rigid-structure-from-motion
(\nrsfm) and NeRF, enabling the well-studied priors of the former to constrain
the latter. To this end, we propose a framework that factorizes time and space
by formulating a scene as a composition of bandlimited, high-dimensional
signals. We demonstrate compelling results across complex dynamic scenes that
involve changes in lighting, texture and long-range dynamics.

---

## Efficient physics-informed neural networks using hash encoding

nerf{: .label .label-blue }

2023-02-26 | Xinquan Huang, Tariq Alkhalifah | cs.LG | [PDF](http://arxiv.org/pdf/2302.13397v1){: .btn .btn-green }

**Abstract**: Physics-informed neural networks (PINNs) have attracted a lot of attention in
scientific computing as their functional representation of partial differential
equation (PDE) solutions offers flexibility and accuracy features. However,
their training cost has limited their practical use as a real alternative to
classic numerical methods. Thus, we propose to incorporate multi-resolution
hash encoding into PINNs to improve the training efficiency, as such encoding
offers a locally-aware (at multi resolution) coordinate inputs to the neural
network. Borrowed from the neural representation field community (NeRF), we
investigate the robustness of calculating the derivatives of such hash encoded
neural networks with respect to the input coordinates, which is often needed by
the PINN loss terms. We propose to replace the automatic differentiation with
finite-difference calculations of the derivatives to address the discontinuous
nature of such derivatives. We also share the appropriate ranges for the hash
encoding hyperparameters to obtain robust derivatives. We test the proposed
method on three problems, including Burgers equation, Helmholtz equation, and
Navier-Stokes equation. The proposed method admits about a 10-fold improvement
in efficiency over the vanilla PINN implementation.

---

## CATNIPS: Collision Avoidance Through Neural Implicit Probabilistic  Scenes

nerf{: .label .label-blue }

2023-02-24 | Timothy Chen, Preston Culbertson, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2302.12931v2){: .btn .btn-green }

**Abstract**: We introduce a transformation of a Neural Radiance Field (NeRF) to an
equivalent Poisson Point Process (PPP). This PPP transformation allows for
rigorous quantification of uncertainty in NeRFs, in particular, for computing
collision probabilities for a robot navigating through a NeRF environment. The
PPP is a generalization of a probabilistic occupancy grid to the continuous
volume and is fundamental to the volumetric ray-tracing model underlying
radiance fields. Building upon this PPP representation, we present a
chance-constrained trajectory optimization method for safe robot navigation in
NeRFs. Our method relies on a voxel representation called the Probabilistic
Unsafe Robot Region (PURR) that spatially fuses the chance constraint with the
NeRF model to facilitate fast trajectory optimization. We then combine a
graph-based search with a spline-based trajectory optimization to yield robot
trajectories through the NeRF that are guaranteed to satisfy a user-specific
collision probability. We validate our chance constrained planning method
through simulations and hardware experiments, showing superior performance
compared to prior works on trajectory planning in NeRF environments.

Comments:
- Under Review in IEEE Transactions on Robotics

---

## DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising  Diffusion Models

nerf{: .label .label-blue }

2023-02-23 | Jamie Wynn, Daniyar Turmukhambetov | cs.CV | [PDF](http://arxiv.org/pdf/2302.12231v3){: .btn .btn-green }

**Abstract**: Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive
results on novel view synthesis tasks. NeRFs learn a scene's color and density
fields by minimizing the photometric discrepancy between training views and
differentiable renderings of the scene. Once trained from a sufficient set of
views, NeRFs can generate novel views from arbitrary camera positions. However,
the scene geometry and color fields are severely under-constrained, which can
lead to artifacts, especially when trained with few input views.
  To alleviate this problem we learn a prior over scene geometry and color,
using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of
the synthetic Hypersim dataset and can be used to predict the gradient of the
logarithm of a joint probability distribution of color and depth patches. We
show that, these gradients of logarithms of RGBD patch priors serve to
regularize geometry and color of a scene. During NeRF training, random RGBD
patches are rendered and the estimated gradient of the log-likelihood is
backpropagated to the color and density fields. Evaluations on LLFF, the most
relevant dataset, show that our learned prior achieves improved quality in the
reconstructed geometry and improved generalization to novel views. Evaluations
on DTU show improved reconstruction quality among NeRF methods.

Comments:
- CVPR 2023. Updated LPIPS scores in Table 1

---

## MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in  Unbounded Scenes



2023-02-23 | Christian Reiser, Richard Szeliski, Dor Verbin, Pratul P. Srinivasan, Ben Mildenhall, Andreas Geiger, Jonathan T. Barron, Peter Hedman | cs.CV | [PDF](http://arxiv.org/pdf/2302.12249v1){: .btn .btn-green }

**Abstract**: Neural radiance fields enable state-of-the-art photorealistic view synthesis.
However, existing radiance field representations are either too
compute-intensive for real-time rendering or require too much memory to scale
to large scenes. We present a Memory-Efficient Radiance Field (MERF)
representation that achieves real-time rendering of large-scale scenes in a
browser. MERF reduces the memory consumption of prior sparse volumetric
radiance fields using a combination of a sparse feature grid and
high-resolution 2D feature planes. To support large-scale unbounded scenes, we
introduce a novel contraction function that maps scene coordinates into a
bounded volume while still allowing for efficient ray-box intersection. We
design a lossless procedure for baking the parameterization used during
training into a model that achieves real-time rendering while still preserving
the photorealistic view synthesis quality of a volumetric radiance field.

Comments:
- Video and interactive web demo available at https://merf42.github.io

---

## Learning Neural Volumetric Representations of Dynamic Humans in Minutes

nerf{: .label .label-blue }

2023-02-23 | Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, Xiaowei Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2302.12237v2){: .btn .btn-green }

**Abstract**: This paper addresses the challenge of quickly reconstructing free-viewpoint
videos of dynamic humans from sparse multi-view videos. Some recent works
represent the dynamic human as a canonical neural radiance field (NeRF) and a
motion field, which are learned from videos through differentiable rendering.
But the per-scene optimization generally requires hours. Other generalizable
NeRF models leverage learned prior from datasets and reduce the optimization
time by only finetuning on new scenes at the cost of visual fidelity. In this
paper, we propose a novel method for learning neural volumetric videos of
dynamic humans from sparse view videos in minutes with competitive visual
quality. Specifically, we define a novel part-based voxelized human
representation to better distribute the representational power of the network
to different human parts. Furthermore, we propose a novel 2D motion
parameterization scheme to increase the convergence rate of deformation field
learning. Experiments demonstrate that our model can be learned 100 times
faster than prior per-scene optimization methods while being competitive in the
rendering quality. Training our model on a $512 \times 512$ video with 100
frames typically takes about 5 minutes on a single RTX 3090 GPU. The code will
be released on our project page: https://zju3dv.github.io/instant_nvr

Comments:
- Project page: https://zju3dv.github.io/instant_nvr

---

## Differentiable Rendering with Reparameterized Volume Sampling

nerf{: .label .label-blue }

2023-02-21 | Nikita Morozov, Denis Rakitin, Oleg Desheulin, Dmitry Vetrov, Kirill Struminsky | cs.CV | [PDF](http://arxiv.org/pdf/2302.10970v2){: .btn .btn-green }

**Abstract**: In view synthesis, a neural radiance field approximates underlying density
and radiance fields based on a sparse set of scene pictures. To generate a
pixel of a novel view, it marches a ray through the pixel and computes a
weighted sum of radiance emitted from a dense set of ray points. This rendering
algorithm is fully differentiable and facilitates gradient-based optimization
of the fields. However, in practice, only a tiny opaque portion of the ray
contributes most of the radiance to the sum. We propose a simple end-to-end
differentiable sampling algorithm based on inverse transform sampling. It
generates samples according to the probability distribution induced by the
density field and picks non-transparent points on the ray. We utilize the
algorithm in two ways. First, we propose a novel rendering approach based on
Monte Carlo estimates. This approach allows for evaluating and optimizing a
neural radiance field with just a few radiance field calls per ray. Second, we
use the sampling algorithm to modify the hierarchical scheme proposed in the
original NeRF work. We show that our modification improves reconstruction
quality of hierarchical models, at the same time simplifying the training
procedure by removing the need for auxiliary proposal network losses.

Comments:
- Preprint

---

## RealFusion: 360° Reconstruction of Any Object from a Single Image



2023-02-21 | Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi | cs.CV | [PDF](http://arxiv.org/pdf/2302.10663v2){: .btn .btn-green }

**Abstract**: We consider the problem of reconstructing a full 360{\deg} photographic model
of an object from a single image of it. We do so by fitting a neural radiance
field to the image, but find this problem to be severely ill-posed. We thus
take an off-the-self conditional image generator based on diffusion and
engineer a prompt that encourages it to "dream up" novel views of the object.
Using an approach inspired by DreamFields and DreamFusion, we fuse the given
input view, the conditional prior, and other regularizers in a final,
consistent reconstruction. We demonstrate state-of-the-art reconstruction
results on benchmark images when compared to prior methods for monocular 3D
reconstruction of objects. Qualitatively, our reconstructions provide a
faithful match of the input view and a plausible extrapolation of its
appearance and 3D shape, including to the side of the object not visible in the
image.

Comments:
- Project page: https://lukemelas.github.io/realfusion

---

## USR: Unsupervised Separated 3D Garment and Human Reconstruction via  Geometry and Semantic Consistency



2023-02-21 | Yue Shi, Yuxuan Xiong, Jingyi Chai, Bingbing Ni, Wenjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2302.10518v3){: .btn .btn-green }

**Abstract**: Dressed people reconstruction from images is a popular task with promising
applications in the creative media and game industry. However, most existing
methods reconstruct the human body and garments as a whole with the supervision
of 3D models, which hinders the downstream interaction tasks and requires
hard-to-obtain data. To address these issues, we propose an unsupervised
separated 3D garments and human reconstruction model (USR), which reconstructs
the human body and authentic textured clothes in layers without 3D models. More
specifically, our method proposes a generalized surface-aware neural radiance
field to learn the mapping between sparse multi-view images and geometries of
the dressed people. Based on the full geometry, we introduce a Semantic and
Confidence Guided Separation strategy (SCGS) to detect, segment, and
reconstruct the clothes layer, leveraging the consistency between 2D semantic
and 3D geometry. Moreover, we propose a Geometry Fine-tune Module to smooth
edges. Extensive experiments on our dataset show that comparing with
state-of-the-art methods, USR achieves improvements on both geometry and
appearance reconstruction while supporting generalizing to unseen people in
real time. Besides, we also introduce SMPL-D model to show the benefit of the
separated modeling of clothes and the human body that allows swapping clothes
and virtual try-on.

---

## NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from  3D-aware Diffusion

nerf{: .label .label-blue }

2023-02-20 | Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2302.10109v1){: .btn .btn-green }

**Abstract**: Novel view synthesis from a single image requires inferring occluded regions
of objects and scenes whilst simultaneously maintaining semantic and physical
consistency with the input. Existing approaches condition neural radiance
fields (NeRF) on local image features, projecting points to the input image
plane, and aggregating 2D features to perform volume rendering. However, under
severe occlusion, this projection fails to resolve uncertainty, resulting in
blurry renderings that lack details. In this work, we propose NerfDiff, which
addresses this issue by distilling the knowledge of a 3D-aware conditional
diffusion model (CDM) into NeRF through synthesizing and refining a set of
virtual views at test time. We further propose a novel NeRF-guided distillation
algorithm that simultaneously generates 3D consistent virtual views from the
CDM samples, and finetunes the NeRF based on the improved virtual views. Our
approach significantly outperforms existing NeRF-based and geometry-free
approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.

Comments:
- Project page: https://jiataogu.me/nerfdiff/

---

## LC-NeRF: Local Controllable Face Generation in Neural Randiance Field

nerf{: .label .label-blue }

2023-02-19 | Wenyang Zhou, Lu Yuan, Shuyu Chen, Lin Gao, Shimin Hu | cs.CV | [PDF](http://arxiv.org/pdf/2302.09486v1){: .btn .btn-green }

**Abstract**: 3D face generation has achieved high visual quality and 3D consistency thanks
to the development of neural radiance fields (NeRF). Recently, to generate and
edit 3D faces with NeRF representation, some methods are proposed and achieve
good results in decoupling geometry and texture. The latent codes of these
generative models affect the whole face, and hence modifications to these codes
cause the entire face to change. However, users usually edit a local region
when editing faces and do not want other regions to be affected. Since changes
to the latent code affect global generation results, these methods do not allow
for fine-grained control of local facial regions. To improve local
controllability in NeRF-based face editing, we propose LC-NeRF, which is
composed of a Local Region Generators Module and a Spatial-Aware Fusion Module,
allowing for local geometry and texture control of local facial regions.
Qualitative and quantitative evaluations show that our method provides better
local editing than state-of-the-art face editing methods. Our method also
performs well in downstream tasks, such as text-driven facial image editing.

---

## Temporal Interpolation Is All You Need for Dynamic Neural Radiance  Fields



2023-02-18 | Sungheon Park, Minjung Son, Seokhwan Jang, Young Chun Ahn, Ji-Yeon Kim, Nahyup Kang | cs.CV | [PDF](http://arxiv.org/pdf/2302.09311v2){: .btn .btn-green }

**Abstract**: Temporal interpolation often plays a crucial role to learn meaningful
representations in dynamic scenes. In this paper, we propose a novel method to
train spatiotemporal neural radiance fields of dynamic scenes based on temporal
interpolation of feature vectors. Two feature interpolation methods are
suggested depending on underlying representations, neural networks or grids. In
the neural representation, we extract features from space-time inputs via
multiple neural network modules and interpolate them based on time frames. The
proposed multi-level feature interpolation network effectively captures
features of both short-term and long-term time ranges. In the grid
representation, space-time features are learned via four-dimensional hash
grids, which remarkably reduces training time. The grid representation shows
more than 100 times faster training speed than the previous neural-net-based
methods while maintaining the rendering quality. Concatenating static and
dynamic features and adding a simple smoothness term further improve the
performance of our proposed models. Despite the simplicity of the model
architectures, our method achieved state-of-the-art performance both in
rendering quality for the neural representation and in training speed for the
grid representation.

Comments:
- CVPR 2023. Project page:
  https://sungheonpark.github.io/tempinterpnerf

---

## MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis  from Sparse Inputs

nerf{: .label .label-blue }

2023-02-17 | Seunghyeon Seo, Donghoon Han, Yeonjin Chang, Nojun Kwak | cs.CV | [PDF](http://arxiv.org/pdf/2302.08788v2){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) has broken new ground in the novel view
synthesis due to its simple concept and state-of-the-art quality. However, it
suffers from severe performance degradation unless trained with a dense set of
images with different camera poses, which hinders its practical applications.
Although previous methods addressing this problem achieved promising results,
they relied heavily on the additional training resources, which goes against
the philosophy of sparse-input novel-view synthesis pursuing the training
efficiency. In this work, we propose MixNeRF, an effective training strategy
for novel view synthesis from sparse inputs by modeling a ray with a mixture
density model. Our MixNeRF estimates the joint distribution of RGB colors along
the ray samples by modeling it with mixture of distributions. We also propose a
new task of ray depth estimation as a useful training objective, which is
highly correlated with 3D scene geometry. Moreover, we remodel the colors with
regenerated blending weights based on the estimated ray depth and further
improves the robustness for colors and viewpoints. Our MixNeRF outperforms
other state-of-the-art methods in various standard benchmarks with superior
efficiency of training and inference.

Comments:
- CVPR 2023. Project Page: https://shawn615.github.io/mixnerf/

---

## 3D-aware Conditional Image Synthesis



2023-02-16 | Kangle Deng, Gengshan Yang, Deva Ramanan, Jun-Yan Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2302.08509v2){: .btn .btn-green }

**Abstract**: We propose pix2pix3D, a 3D-aware conditional generative model for
controllable photorealistic image synthesis. Given a 2D label map, such as a
segmentation or edge map, our model learns to synthesize a corresponding image
from different viewpoints. To enable explicit 3D user control, we extend
conditional generative models with neural radiance fields. Given
widely-available monocular images and label map pairs, our model learns to
assign a label to every 3D point in addition to color and density, which
enables it to render the image and pixel-aligned label map simultaneously.
Finally, we build an interactive system that allows users to edit the label map
from any viewpoint and generate outputs accordingly.

Comments:
- Project Page: https://www.cs.cmu.edu/~pix2pix3D/

---

## LiveHand: Real-time and Photorealistic Neural Hand Rendering



2023-02-15 | Akshay Mundra, Mallikarjun B R, Jiayi Wang, Marc Habermann, Christian Theobalt, Mohamed Elgharib | cs.GR | [PDF](http://arxiv.org/pdf/2302.07672v3){: .btn .btn-green }

**Abstract**: The human hand is the main medium through which we interact with our
surroundings, making its digitization an important problem. While there are
several works modeling the geometry of hands, little attention has been paid to
capturing photo-realistic appearance. Moreover, for applications in extended
reality and gaming, real-time rendering is critical. We present the first
neural-implicit approach to photo-realistically render hands in real-time. This
is a challenging problem as hands are textured and undergo strong articulations
with pose-dependent effects. However, we show that this aim is achievable
through our carefully designed method. This includes training on a
low-resolution rendering of a neural radiance field, together with a
3D-consistent super-resolution module and mesh-guided sampling and space
canonicalization. We demonstrate a novel application of perceptual loss on the
image space, which is critical for learning details accurately. We also show a
live demo where we photo-realistically render the human hand in real-time for
the first time, while also modeling pose- and view-dependent appearance
effects. We ablate all our design choices and show that they optimize for
rendering speed and quality. Video results and our code can be accessed from
https://vcai.mpi-inf.mpg.de/projects/LiveHand/

Comments:
- Project page: https://vcai.mpi-inf.mpg.de/projects/LiveHand/ |
  Accepted at ICCV '23 | 11 pages, 7 figures

---

## VQ3D: Learning a 3D-Aware Generative Model on ImageNet

nerf{: .label .label-blue }

2023-02-14 | Kyle Sargent, Jing Yu Koh, Han Zhang, Huiwen Chang, Charles Herrmann, Pratul Srinivasan, Jiajun Wu, Deqing Sun | cs.CV | [PDF](http://arxiv.org/pdf/2302.06833v1){: .btn .btn-green }

**Abstract**: Recent work has shown the possibility of training generative models of 3D
content from 2D image collections on small datasets corresponding to a single
object class, such as human faces, animal faces, or cars. However, these models
struggle on larger, more complex datasets. To model diverse and unconstrained
image collections such as ImageNet, we present VQ3D, which introduces a
NeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1
allows for the reconstruction of an input image and the ability to change the
camera position around the image, and our Stage 2 allows for the generation of
new 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images
from the 1000-class ImageNet dataset of 1.2 million training images. We achieve
an ImageNet generation FID score of 16.8, compared to 69.8 for the next best
baseline method.

Comments:
- 15 pages. For visual results, please visit the project webpage at
  http://kylesargent.github.io/vq3d

---

## 3D-aware Blending with Generative NeRFs

nerf{: .label .label-blue }

2023-02-13 | Hyunsu Kim, Gayoung Lee, Yunjey Choi, Jin-Hwa Kim, Jun-Yan Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2302.06608v3){: .btn .btn-green }

**Abstract**: Image blending aims to combine multiple images seamlessly. It remains
challenging for existing 2D-based methods, especially when input images are
misaligned due to differences in 3D camera poses and object shapes. To tackle
these issues, we propose a 3D-aware blending method using generative Neural
Radiance Fields (NeRF), including two key components: 3D-aware alignment and
3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of
the reference image with respect to generative NeRFs and then perform 3D local
alignment for each part. To further leverage 3D information of the generative
NeRF, we propose 3D-aware blending that directly blends images on the NeRF's
latent representation space, rather than raw pixel space. Collectively, our
method outperforms existing 2D baselines, as validated by extensive
quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.

Comments:
- ICCV 2023, Project page: https://blandocs.github.io/blendnerf

---

## 3D Colored Shape Reconstruction from a Single RGB Image through  Diffusion

nerf{: .label .label-blue }

2023-02-11 | Bo Li, Xiaolin Wei, Fengwei Chen, Bin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2302.05573v1){: .btn .btn-green }

**Abstract**: We propose a novel 3d colored shape reconstruction method from a single RGB
image through diffusion model. Diffusion models have shown great development
potentials for high-quality 3D shape generation. However, most existing work
based on diffusion models only focus on geometric shape generation, they cannot
either accomplish 3D reconstruction from a single image, or produce 3D
geometric shape with color information. In this work, we propose to reconstruct
a 3D colored shape from a single RGB image through a novel conditional
diffusion model. The reverse process of the proposed diffusion model is
consisted of three modules, shape prediction module, color prediction module
and NeRF-like rendering module. In shape prediction module, the reference RGB
image is first encoded into a high-level shape feature and then the shape
feature is utilized as a condition to predict the reverse geometric noise in
diffusion model. Then the color of each 3D point updated in shape prediction
module is predicted by color prediction module. Finally, a NeRF-like rendering
module is designed to render the colored point cloud predicted by the former
two modules to 2D image space to guide the training conditioned only on a
reference image. As far as the authors know, the proposed method is the first
diffusion model for 3D colored shape reconstruction from a single RGB image.
Experimental results demonstrate that the proposed method achieves competitive
performance on colored 3D shape reconstruction, and the ablation study
validates the positive role of the color prediction module in improving the
reconstruction quality of 3D geometric point cloud.

Comments:
- 9 pages, 8 figures

---

## In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for  Face Editing



2023-02-09 | Yiran Xu, Zhixin Shu, Cameron Smith, Seoung Wug Oh, Jia-Bin Huang | cs.CV | [PDF](http://arxiv.org/pdf/2302.04871v3){: .btn .btn-green }

**Abstract**: 3D-aware GANs offer new capabilities for view synthesis while preserving the
editing functionalities of their 2D counterparts. GAN inversion is a crucial
step that seeks the latent code to reconstruct input images or videos,
subsequently enabling diverse editing tasks through manipulation of this latent
code. However, a model pre-trained on a particular dataset (e.g., FFHQ) often
has difficulty reconstructing images with out-of-distribution (OOD) objects
such as faces with heavy make-up or occluding objects. We address this issue by
explicitly modeling OOD objects from the input in 3D-aware GANs. Our core idea
is to represent the image using two individual neural radiance fields: one for
the in-distribution content and the other for the out-of-distribution object.
The final reconstruction is achieved by optimizing the composition of these two
radiance fields with carefully designed regularization. We demonstrate that our
explicit decomposition alleviates the inherent trade-off between reconstruction
fidelity and editability. We evaluate reconstruction accuracy and editability
of our method on challenging real face images and videos and showcase favorable
results against other baselines.

Comments:
- Project page: https://in-n-out-3d.github.io/

---

## Nerfstudio: A Modular Framework for Neural Radiance Field Development

nerf{: .label .label-blue }

2023-02-08 | Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2302.04264v4){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) are a rapidly growing area of research with
wide-ranging applications in computer vision, graphics, robotics, and more. In
order to streamline the development and deployment of NeRF research, we propose
a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play
components for implementing NeRF-based methods, which make it easy for
researchers and practitioners to incorporate NeRF into their projects.
Additionally, the modular design enables support for extensive real-time
visualization tools, streamlined pipelines for importing captured in-the-wild
data, and tools for exporting to video, point cloud and mesh representations.
The modularity of Nerfstudio enables the development of Nerfacto, our method
that combines components from recent papers to achieve a balance between speed
and quality, while also remaining flexible to future modifications. To promote
community-driven development, all associated code and data are made publicly
available with open-source licensing at https://nerf.studio.

Comments:
- Project page at https://nerf.studio

---

## AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene  Synthesis

nerf{: .label .label-blue }

2023-02-04 | Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2302.02088v3){: .btn .btn-green }

**Abstract**: Can machines recording an audio-visual scene produce realistic, matching
audio-visual experiences at novel positions and novel view directions? We
answer it by studying a new task -- real-world audio-visual scene synthesis --
and a first-of-its-kind NeRF-based approach for multimodal learning.
Concretely, given a video recording of an audio-visual scene, the task is to
synthesize new videos with spatial audios along arbitrary novel camera
trajectories in that scene. We propose an acoustic-aware audio generation
module that integrates prior knowledge of audio propagation into NeRF, in which
we implicitly associate audio generation with the 3D geometry and material
properties of a visual environment. Furthermore, we present a coordinate
transformation module that expresses a view direction relative to the sound
source, enabling the model to learn sound source-centric acoustic fields. To
facilitate the study of this new task, we collect a high-quality Real-World
Audio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method
on this real-world dataset and the simulation-based SoundSpaces dataset.

Comments:
- NeurIPS 2023

---

## Robust Camera Pose Refinement for Multi-Resolution Hash Encoding

nerf{: .label .label-blue }

2023-02-03 | Hwan Heo, Taekyung Kim, Jiyoung Lee, Jaewon Lee, Soohyun Kim, Hyunwoo J. Kim, Jin-Hwa Kim | cs.CV | [PDF](http://arxiv.org/pdf/2302.01571v1){: .btn .btn-green }

**Abstract**: Multi-resolution hash encoding has recently been proposed to reduce the
computational cost of neural renderings, such as NeRF. This method requires
accurate camera poses for the neural renderings of given scenes. However,
contrary to previous methods jointly optimizing camera poses and 3D scenes, the
naive gradient-based camera pose refinement method using multi-resolution hash
encoding severely deteriorates performance. We propose a joint optimization
algorithm to calibrate the camera pose and learn a geometric representation
using efficient multi-resolution hash encoding. Showing that the oscillating
gradient flows of hash encoding interfere with the registration of camera
poses, our method addresses the issue by utilizing smooth interpolation
weighting to stabilize the gradient oscillation for the ray samplings across
hash grids. Moreover, the curriculum training procedure helps to learn the
level-wise hash encoding, further increasing the pose refinement. Experiments
on the novel-view synthesis datasets validate that our learning frameworks
achieve state-of-the-art performance and rapid convergence of neural rendering,
even when initial camera poses are unknown.

---

## INV: Towards Streaming Incremental Neural Videos

nerf{: .label .label-blue }

2023-02-03 | Shengze Wang, Alexey Supikov, Joshua Ratcliff, Henry Fuchs, Ronald Azuma | cs.CV | [PDF](http://arxiv.org/pdf/2302.01532v1){: .btn .btn-green }

**Abstract**: Recent works in spatiotemporal radiance fields can produce photorealistic
free-viewpoint videos. However, they are inherently unsuitable for interactive
streaming scenarios (e.g. video conferencing, telepresence) because have an
inevitable lag even if the training is instantaneous. This is because these
approaches consume videos and thus have to buffer chunks of frames (often
seconds) before processing. In this work, we take a step towards interactive
streaming via a frame-by-frame approach naturally free of lag. Conventional
wisdom believes that per-frame NeRFs are impractical due to prohibitive
training costs and storage. We break this belief by introducing Incremental
Neural Videos (INV), a per-frame NeRF that is efficiently trained and
streamable. We designed INV based on two insights: (1) Our main finding is that
MLPs naturally partition themselves into Structure and Color Layers, which
store structural and color/texture information respectively. (2) We leverage
this property to retain and improve upon knowledge from previous frames, thus
amortizing training across frames and reducing redundant learning. As a result,
with negligible changes to NeRF, INV can achieve good qualities (>28.6db) in
8min/frame. It can also outperform prior SOTA in 19% less training time.
Additionally, our Temporal Weight Compression reduces the per-frame size to
0.3MB/frame (6.6% of NeRF). More importantly, INV is free from buffer lag and
is naturally fit for streaming. While this work does not achieve real-time
training, it shows that incremental approaches like INV present new
possibilities in interactive 3D streaming. Moreover, our discovery of natural
information partition leads to a better understanding and manipulation of MLPs.
Code and dataset will be released soon.

---

## Semantic 3D-aware Portrait Synthesis and Manipulation Based on  Compositional Neural Radiance Field

nerf{: .label .label-blue }

2023-02-03 | Tianxiang Ma, Bingchuan Li, Qian He, Jing Dong, Tieniu Tan | cs.CV | [PDF](http://arxiv.org/pdf/2302.01579v2){: .btn .btn-green }

**Abstract**: Recently 3D-aware GAN methods with neural radiance field have developed
rapidly. However, current methods model the whole image as an overall neural
radiance field, which limits the partial semantic editability of synthetic
results. Since NeRF renders an image pixel by pixel, it is possible to split
NeRF in the spatial dimension. We propose a Compositional Neural Radiance Field
(CNeRF) for semantic 3D-aware portrait synthesis and manipulation. CNeRF
divides the image by semantic regions and learns an independent neural radiance
field for each region, and finally fuses them and renders the complete image.
Thus we can manipulate the synthesized semantic regions independently, while
fixing the other parts unchanged. Furthermore, CNeRF is also designed to
decouple shape and texture within each semantic region. Compared to
state-of-the-art 3D-aware GAN methods, our approach enables fine-grained
semantic region manipulation, while maintaining high-quality 3D-consistent
synthesis. The ablation studies show the effectiveness of the structure and
loss function used by our method. In addition real image inversion and cartoon
portrait 3D editing experiments demonstrate the application potential of our
method.

Comments:
- Accepted by AAAI2023 Oral

---

## RobustNeRF: Ignoring Distractors with Robust Losses

nerf{: .label .label-blue }

2023-02-02 | Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin, David J. Fleet, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2302.00833v1){: .btn .btn-green }

**Abstract**: Neural radiance fields (NeRF) excel at synthesizing new views given
multi-view, calibrated images of a static scene. When scenes include
distractors, which are not persistent during image capture (moving objects,
lighting variations, shadows), artifacts appear as view-dependent effects or
'floaters'. To cope with distractors, we advocate a form of robust estimation
for NeRF training, modeling distractors in training data as outliers of an
optimization problem. Our method successfully removes outliers from a scene and
improves upon our baselines, on synthetic and real-world scenes. Our technique
is simple to incorporate in modern NeRF frameworks, with few hyper-parameters.
It does not assume a priori knowledge of the types of distractors, and is
instead focused on the optimization problem rather than pre-processing or
modeling transient objects. More results on our page
https://robustnerf.github.io/public.

---

## Factor Fields: A Unified Framework for Neural Fields and Beyond

nerf{: .label .label-blue }

2023-02-02 | Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, Andreas Geiger | cs.CV | [PDF](http://arxiv.org/pdf/2302.01226v3){: .btn .btn-green }

**Abstract**: We present Factor Fields, a novel framework for modeling and representing
signals. Factor Fields decomposes a signal into a product of factors, each
represented by a classical or neural field representation which operates on
transformed input coordinates. This decomposition results in a unified
framework that accommodates several recent signal representations including
NeRF, Plenoxels, EG3D, Instant-NGP, and TensoRF. Additionally, our framework
allows for the creation of powerful new signal representations, such as the
"Dictionary Field" (DiF) which is a second contribution of this paper. Our
experiments show that DiF leads to improvements in approximation quality,
compactness, and training time when compared to previous fast reconstruction
methods. Experimentally, our representation achieves better image approximation
quality on 2D image regression tasks, higher geometric quality when
reconstructing 3D signed distance fields, and higher compactness for radiance
field reconstruction tasks. Furthermore, DiF enables generalization to unseen
images/3D scenes by sharing bases across signals during training which greatly
benefits use cases such as image regression from sparse observations and
few-shot radiance field reconstruction.

Comments:
- 13 pages, 7 figures; Project Page:
  https://apchenstu.github.io/FactorFields/

---

## GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face  Synthesis

nerf{: .label .label-blue }

2023-01-31 | Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, JinZheng He, Zhou Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2301.13430v1){: .btn .btn-green }

**Abstract**: Generating photo-realistic video portrait with arbitrary speech audio is a
crucial problem in film-making and virtual reality. Recently, several works
explore the usage of neural radiance field in this task to improve 3D realness
and image fidelity. However, the generalizability of previous NeRF-based
methods to out-of-domain audio is limited by the small scale of training data.
In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based
talking face generation method, which can generate natural results
corresponding to various out-of-domain audio. Specifically, we learn a
variaitional motion generator on a large lip-reading corpus, and introduce a
domain adaptative post-net to calibrate the result. Moreover, we learn a
NeRF-based renderer conditioned on the predicted facial motion. A head-aware
torso-NeRF is proposed to eliminate the head-torso separation problem.
Extensive experiments show that our method achieves more generalized and
high-fidelity talking face generation compared to previous methods.

Comments:
- Accepted by ICLR2023. Project page: https://geneface.github.io/

---

## Equivariant Architectures for Learning in Deep Weight Spaces

nerf{: .label .label-blue }

2023-01-30 | Aviv Navon, Aviv Shamsian, Idan Achituve, Ethan Fetaya, Gal Chechik, Haggai Maron | cs.LG | [PDF](http://arxiv.org/pdf/2301.12780v2){: .btn .btn-green }

**Abstract**: Designing machine learning architectures for processing neural networks in
their raw weight matrix form is a newly introduced research direction.
Unfortunately, the unique symmetry structure of deep weight spaces makes this
design very challenging. If successful, such architectures would be capable of
performing a wide range of intriguing tasks, from adapting a pre-trained
network to a new domain to editing objects represented as functions (INRs or
NeRFs). As a first step towards this goal, we present here a novel network
architecture for learning in deep weight spaces. It takes as input a
concatenation of weights and biases of a pre-trained MLP and processes it using
a composition of layers that are equivariant to the natural permutation
symmetry of the MLP's weights: Changing the order of neurons in intermediate
layers of the MLP does not affect the function it represents. We provide a full
characterization of all affine equivariant and invariant layers for these
symmetries and show how these layers can be implemented using three basic
operations: pooling, broadcasting, and fully connected layers applied to the
input in an appropriate manner. We demonstrate the effectiveness of our
architecture and its advantages over natural baselines in a variety of learning
tasks.

Comments:
- ICML 2023

---

## HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN

nerf{: .label .label-blue }

2023-01-27 | Adam Kania, Artur Kasymov, Maciej Zięba, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2301.11631v1){: .btn .btn-green }

**Abstract**: Recently, generative models for 3D objects are gaining much popularity in VR
and augmented reality applications. Training such models using standard 3D
representations, like voxels or point clouds, is challenging and requires
complex tools for proper color rendering. In order to overcome this limitation,
Neural Radiance Fields (NeRFs) offer a state-of-the-art quality in synthesizing
novel views of complex 3D scenes from a small subset of 2D images.
  In the paper, we propose a generative model called HyperNeRFGAN, which uses
hypernetworks paradigm to produce 3D objects represented by NeRF. Our GAN
architecture leverages a hypernetwork paradigm to transfer gaussian noise into
weights of NeRF model. The model is further used to render 2D novel views, and
a classical 2D discriminator is utilized for training the entire GAN-based
structure. Our architecture produces 2D images, but we use 3D-aware NeRF
representation, which forces the model to produce correct 3D objects. The
advantage of the model over existing approaches is that it produces a dedicated
NeRF representation for the object without sharing some global parameters of
the rendering component. We show the superiority of our approach compared to
reference baselines on three challenging datasets from various domains.

---

## A Comparison of Tiny-nerf versus Spatial Representations for 3d  Reconstruction

nerf{: .label .label-blue }

2023-01-27 | Saulo Abraham Gante, Juan Irving Vasquez, Marco Antonio Valencia, Mauricio Olguín Carbajal | cs.AI | [PDF](http://arxiv.org/pdf/2301.11522v1){: .btn .btn-green }

**Abstract**: Neural rendering has emerged as a powerful paradigm for synthesizing images,
offering many benefits over classical rendering by using neural networks to
reconstruct surfaces, represent shapes, and synthesize novel views, either for
objects or scenes. In this neural rendering, the environment is encoded into a
neural network. We believe that these new representations can be used to codify
the scene for a mobile robot. Therefore, in this work, we perform a comparison
between a trending neural rendering, called tiny-NeRF, and other volume
representations that are commonly used as maps in robotics, such as voxel maps,
point clouds, and triangular meshes. The target is to know the advantages and
disadvantages of neural representations in the robotics context. The comparison
is made in terms of spatial complexity and processing time to obtain a model.
Experiments show that tiny-NeRF requires three times less memory space compared
to other representations. In terms of processing time, tiny-NeRF takes about
six times more to compute the model.

---

## SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning

nerf{: .label .label-blue }

2023-01-27 | Dongseok Shim, Seungjae Lee, H. Jin Kim | cs.LG | [PDF](http://arxiv.org/pdf/2301.11520v3){: .btn .btn-green }

**Abstract**: As previous representations for reinforcement learning cannot effectively
incorporate a human-intuitive understanding of the 3D environment, they usually
suffer from sub-optimal performances. In this paper, we present Semantic-aware
Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly
optimizes semantic-aware neural radiance fields (NeRF) with a convolutional
encoder to learn 3D-aware neural implicit representation from multi-view
images. We introduce 3D semantic and distilled feature fields in parallel to
the RGB radiance fields in NeRF to learn semantic and object-centric
representation for reinforcement learning. SNeRL outperforms not only previous
pixel-based representations but also recent 3D-aware representations both in
model-free and model-based reinforcement learning.

Comments:
- ICML 2023. First two authors contributed equally. Order was
  determined by coin flip

---

## Text-To-4D Dynamic Scene Generation

nerf{: .label .label-blue }

2023-01-26 | Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, Yaniv Taigman | cs.CV | [PDF](http://arxiv.org/pdf/2301.11280v1){: .btn .btn-green }

**Abstract**: We present MAV3D (Make-A-Video3D), a method for generating three-dimensional
dynamic scenes from text descriptions. Our approach uses a 4D dynamic Neural
Radiance Field (NeRF), which is optimized for scene appearance, density, and
motion consistency by querying a Text-to-Video (T2V) diffusion-based model. The
dynamic video output generated from the provided text can be viewed from any
camera location and angle, and can be composited into any 3D environment. MAV3D
does not require any 3D or 4D data and the T2V model is trained only on
Text-Image pairs and unlabeled videos. We demonstrate the effectiveness of our
approach using comprehensive quantitative and qualitative experiments and show
an improvement over previously established internal baselines. To the best of
our knowledge, our method is the first to generate 3D dynamic scenes given a
text description.

---

## GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency

nerf{: .label .label-blue }

2023-01-26 | Min-seop Kwak, Jiuhn Song, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2301.10941v3){: .btn .btn-green }

**Abstract**: We present a novel framework to regularize Neural Radiance Field (NeRF) in a
few-shot setting with a geometry-aware consistency regularization. The proposed
approach leverages a rendered depth map at unobserved viewpoint to warp sparse
input images to the unobserved viewpoint and impose them as pseudo ground
truths to facilitate learning of NeRF. By encouraging such geometry-aware
consistency at a feature-level instead of using pixel-level reconstruction
loss, we regularize the NeRF at semantic and structural levels while allowing
for modeling view dependent radiance to account for color variations across
viewpoints. We also propose an effective method to filter out erroneous warped
solutions, along with training strategies to stabilize training during
optimization. We show that our model achieves competitive results compared to
state-of-the-art few-shot NeRF models. Project page is available at
https://ku-cvlab.github.io/GeCoNeRF/.

Comments:
- ICML 2023

---

## Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging

nerf{: .label .label-blue }

2023-01-25 | Magdalena Wysocki, Mohammad Farid Azampour, Christine Eilers, Benjamin Busam, Mehrdad Salehi, Nassir Navab | eess.IV | [PDF](http://arxiv.org/pdf/2301.10520v2){: .btn .btn-green }

**Abstract**: We present a physics-enhanced implicit neural representation (INR) for
ultrasound (US) imaging that learns tissue properties from overlapping US
sweeps. Our proposed method leverages a ray-tracing-based neural rendering for
novel view US synthesis. Recent publications demonstrated that INR models could
encode a representation of a three-dimensional scene from a set of
two-dimensional US frames. However, these models fail to consider the
view-dependent changes in appearance and geometry intrinsic to US imaging. In
our work, we discuss direction-dependent changes in the scene and show that a
physics-inspired rendering improves the fidelity of US image synthesis. In
particular, we demonstrate experimentally that our proposed method generates
geometrically accurate B-mode images for regions with ambiguous representation
owing to view-dependent differences of the US images. We conduct our
experiments using simulated B-mode US sweeps of the liver and acquired US
sweeps of a spine phantom tracked with a robotic arm. The experiments
corroborate that our method generates US frames that enable consistent volume
compounding from previously unseen views. To the best of our knowledge, the
presented work is the first to address view-dependent US image synthesis using
INR.

Comments:
- accepted for oral presentation at MIDL 2023
  (https://openreview.net/forum?id=x4McMBwVyi)

---

## HexPlane: A Fast Representation for Dynamic Scenes

nerf{: .label .label-blue }

2023-01-23 | Ang Cao, Justin Johnson | cs.CV | [PDF](http://arxiv.org/pdf/2301.09632v2){: .btn .btn-green }

**Abstract**: Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D
vision. Prior approaches build on NeRF and rely on implicit representations.
This is slow since it requires many MLP evaluations, constraining real-world
applications. We show that dynamic 3D scenes can be explicitly represented by
six planes of learned features, leading to an elegant solution we call
HexPlane. A HexPlane computes features for points in spacetime by fusing
vectors extracted from each plane, which is highly efficient. Pairing a
HexPlane with a tiny MLP to regress output colors and training via volume
rendering gives impressive results for novel view synthesis on dynamic scenes,
matching the image quality of prior work but reducing training time by more
than $100\times$. Extensive ablations confirm our HexPlane design and show that
it is robust to different feature fusion mechanisms, coordinate systems, and
decoding mechanisms. HexPlane is a simple and effective solution for
representing 4D volumes, and we hope they can broadly contribute to modeling
spacetime for dynamic 3D scenes.

Comments:
- CVPR 2023, Camera Ready Project page:
  https://caoang327.github.io/HexPlane

---

## 3D Reconstruction of Non-cooperative Resident Space Objects using  Instant NGP-accelerated NeRF and D-NeRF

nerf{: .label .label-blue }

2023-01-22 | Basilio Caruso, Trupti Mahendrakar, Van Minh Nguyen, Ryan T. White, Todd Steffen | cs.CV | [PDF](http://arxiv.org/pdf/2301.09060v3){: .btn .btn-green }

**Abstract**: The proliferation of non-cooperative resident space objects (RSOs) in orbit
has spurred the demand for active space debris removal, on-orbit servicing
(OOS), classification, and functionality identification of these RSOs. Recent
advances in computer vision have enabled high-definition 3D modeling of objects
based on a set of 2D images captured from different viewing angles. This work
adapts Instant NeRF and D-NeRF, variations of the neural radiance field (NeRF)
algorithm to the problem of mapping RSOs in orbit for the purposes of
functionality identification and assisting with OOS. The algorithms are
evaluated for 3D reconstruction quality and hardware requirements using
datasets of images of a spacecraft mock-up taken under two different lighting
and motion conditions at the Orbital Robotic Interaction, On-Orbit Servicing
and Navigation (ORION) Laboratory at Florida Institute of Technology. Instant
NeRF is shown to learn high-fidelity 3D models with a computational cost that
could feasibly be trained on on-board computers.

Comments:
- Presented at AAS/AIAA Spaceflight Mechanics Conference 2023, 14
  pages, 10 figures, 2 tables

---

## RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color  Editing of 3D Scenes

nerf{: .label .label-blue }

2023-01-19 | Bingchen Gong, Yuehao Wang, Xiaoguang Han, Qi Dou | cs.CV | [PDF](http://arxiv.org/pdf/2301.07958v3){: .btn .btn-green }

**Abstract**: Radiance fields have gradually become a main representation of media.
Although its appearance editing has been studied, how to achieve
view-consistent recoloring in an efficient manner is still under explored. We
present RecolorNeRF, a novel user-friendly color editing approach for the
neural radiance fields. Our key idea is to decompose the scene into a set of
pure-colored layers, forming a palette. By this means, color manipulation can
be conducted by altering the color components of the palette directly. To
support efficient palette-based editing, the color of each layer needs to be as
representative as possible. In the end, the problem is formulated as an
optimization problem, where the layers and their blending weights are jointly
optimized with the NeRF itself. Extensive experiments show that our
jointly-optimized layer decomposition can be used against multiple backbones
and produce photo-realistic recolored novel-view renderings. We demonstrate
that RecolorNeRF outperforms baseline methods both quantitatively and
qualitatively for color editing even in complex real-world scenes.

Comments:
- To appear in ACM Multimedia 2023. Project website is accessible at
  https://sites.google.com/view/recolornerf

---

## NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via  Novel-View Synthesis

nerf{: .label .label-blue }

2023-01-18 | Allan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence, Chelsea Finn | cs.LG | [PDF](http://arxiv.org/pdf/2301.08556v1){: .btn .btn-green }

**Abstract**: Expert demonstrations are a rich source of supervision for training visual
robotic manipulation policies, but imitation learning methods often require
either a large number of demonstrations or expensive online expert supervision
to learn reactive closed-loop behaviors. In this work, we introduce SPARTN
(Synthetic Perturbations for Augmenting Robot Trajectories via NeRF): a
fully-offline data augmentation scheme for improving robot policies that use
eye-in-hand cameras. Our approach leverages neural radiance fields (NeRFs) to
synthetically inject corrective noise into visual demonstrations, using NeRFs
to generate perturbed viewpoints while simultaneously calculating the
corrective actions. This requires no additional expert supervision or
environment interaction, and distills the geometric information in NeRFs into a
real-time reactive RGB-only policy. In a simulated 6-DoF visual grasping
benchmark, SPARTN improves success rates by 2.8$\times$ over imitation learning
without the corrective augmentations and even outperforms some methods that use
online supervision. It additionally closes the gap between RGB-only and RGB-D
success rates, eliminating the previous need for depth sensors. In real-world
6-DoF robotic grasping experiments from limited human demonstrations, our
method improves absolute success rates by $22.5\%$ on average, including
objects that are traditionally challenging for depth-based methods. See video
results at \url{https://bland.website/spartn}.

---

## Behind the Scenes: Density Fields for Single View Reconstruction

nerf{: .label .label-blue }

2023-01-18 | Felix Wimbauer, Nan Yang, Christian Rupprecht, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2301.07668v3){: .btn .btn-green }

**Abstract**: Inferring a meaningful geometric scene representation from a single image is
a fundamental problem in computer vision. Approaches based on traditional depth
map prediction can only reason about areas that are visible in the image.
Currently, neural radiance fields (NeRFs) can capture true 3D including color,
but are too complex to be generated from a single image. As an alternative, we
propose to predict implicit density fields. A density field maps every location
in the frustum of the input image to volumetric density. By directly sampling
color from the available views instead of storing color in the density field,
our scene representation becomes significantly less complex compared to NeRFs,
and a neural network can predict it in a single forward pass. The prediction
network is trained through self-supervision from only video data. Our
formulation allows volume rendering to perform both depth prediction and novel
view synthesis. Through experiments, we show that our method is able to predict
meaningful geometry for regions that are occluded in the input image.
Additionally, we demonstrate the potential of our approach on three datasets
for depth prediction and novel-view synthesis.

Comments:
- Project Page: https://fwmb.github.io/bts/

---

## A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View  Synthesis and Implicit Scene Reconstruction

nerf{: .label .label-blue }

2023-01-17 | Chongshan Lu, Fukun Yin, Xin Chen, Tao Chen, Gang YU, Jiayuan Fan | cs.CV | [PDF](http://arxiv.org/pdf/2301.06782v1){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRF) has achieved impressive results in single
object scene reconstruction and novel view synthesis, which have been
demonstrated on many single modality and single object focused indoor scene
datasets like DTU, BMVS, and NeRF Synthetic.However, the study of NeRF on
large-scale outdoor scene reconstruction is still limited, as there is no
unified outdoor scene dataset for large-scale NeRF evaluation due to expensive
data acquisition and calibration costs. In this paper, we propose a large-scale
outdoor multi-modal dataset, OMMO dataset, containing complex land objects and
scenes with calibrated images, point clouds and prompt annotations. Meanwhile,
a new benchmark for several outdoor NeRF-based tasks is established, such as
novel view synthesis, surface reconstruction, and multi-modal NeRF. To create
the dataset, we capture and collect a large number of real fly-view videos and
select high-quality and high-resolution clips from them. Then we design a
quality review module to refine images, remove low-quality frames and
fail-to-calibrate scenes through a learning-based automatic evaluation plus
manual review. Finally, a number of volunteers are employed to add the text
descriptions for each scene and key-frame to meet the potential multi-modal
requirements in the future. Compared with existing NeRF datasets, our dataset
contains abundant real-world urban and natural scenes with various scales,
camera trajectories, and lighting conditions. Experiments show that our dataset
can benchmark most state-of-the-art NeRF methods on different tasks. We will
release the dataset and model weights very soon.

---

## Laser: Latent Set Representations for 3D Generative Modeling

nerf{: .label .label-blue }

2023-01-13 | Pol Moreno, Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Rosalia G. Schneider, Björn Winckler, Larisa Markeeva, Théophane Weber, Danilo J. Rezende | cs.CV | [PDF](http://arxiv.org/pdf/2301.05747v1){: .btn .btn-green }

**Abstract**: NeRF provides unparalleled fidelity of novel view synthesis: rendering a 3D
scene from an arbitrary viewpoint. NeRF requires training on a large number of
views that fully cover a scene, which limits its applicability. While these
issues can be addressed by learning a prior over scenes in various forms,
previous approaches have been either applied to overly simple scenes or
struggling to render unobserved parts. We introduce Laser-NV: a generative
model which achieves high modelling capacity, and which is based on a
set-valued latent representation modelled by normalizing flows. Similarly to
previous amortized approaches, Laser-NV learns structure from multiple scenes
and is capable of fast, feed-forward inference from few views. To encourage
higher rendering fidelity and consistency with observed views, Laser-NV further
incorporates a geometry-informed attention mechanism over the observed views.
Laser-NV further produces diverse and plausible completions of occluded parts
of a scene while remaining consistent with observations. Laser-NV shows
state-of-the-art novel-view synthesis quality when evaluated on ShapeNet and on
a novel simulated City dataset, which features high uncertainty in the
unobserved regions of the scene.

Comments:
- See https://laser-nv-paper.github.io/ for video results

---

## Neural Radiance Field Codebooks



2023-01-10 | Matthew Wallingford, Aditya Kusupati, Alex Fang, Vivek Ramanujan, Aniruddha Kembhavi, Roozbeh Mottaghi, Ali Farhadi | cs.CV | [PDF](http://arxiv.org/pdf/2301.04101v2){: .btn .btn-green }

**Abstract**: Compositional representations of the world are a promising step towards
enabling high-level scene understanding and efficient transfer to downstream
tasks. Learning such representations for complex scenes and tasks remains an
open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks
(NRC), a scalable method for learning object-centric representations through
novel view reconstruction. NRC learns to reconstruct scenes from novel views
using a dictionary of object codes which are decoded through a volumetric
renderer. This enables the discovery of reoccurring visual and geometric
patterns across scenes which are transferable to downstream tasks. We show that
NRC representations transfer well to object navigation in THOR, outperforming
2D and 3D representation learning methods by 3.1% success rate. We demonstrate
that our approach is able to perform unsupervised segmentation for more complex
synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29%
relative improvement). Finally, we show that NRC improves on the task of depth
ordering by 5.5% accuracy in THOR.

Comments:
- 19 pages, 8 figures, 9 tables

---

## Benchmarking Robustness in Neural Radiance Fields

nerf{: .label .label-blue }

2023-01-10 | Chen Wang, Angtian Wang, Junbo Li, Alan Yuille, Cihang Xie | cs.CV | [PDF](http://arxiv.org/pdf/2301.04075v1){: .btn .btn-green }

**Abstract**: Neural Radiance Field (NeRF) has demonstrated excellent quality in novel view
synthesis, thanks to its ability to model 3D object geometries in a concise
formulation. However, current approaches to NeRF-based models rely on clean
images with accurate camera calibration, which can be difficult to obtain in
the real world, where data is often subject to corruption and distortion. In
this work, we provide the first comprehensive analysis of the robustness of
NeRF-based novel view synthesis algorithms in the presence of different types
of corruptions.
  We find that NeRF-based models are significantly degraded in the presence of
corruption, and are more sensitive to a different set of corruptions than image
recognition models. Furthermore, we analyze the robustness of the feature
encoder in generalizable methods, which synthesize images using neural features
extracted via convolutional neural networks or transformers, and find that it
only contributes marginally to robustness. Finally, we reveal that standard
data augmentation techniques, which can significantly improve the robustness of
recognition models, do not help the robustness of NeRF-based models. We hope
that our findings will attract more researchers to study the robustness of
NeRF-based approaches and help to improve their performance in the real world.

---

## Traditional Readability Formulas Compared for English

nerf{: .label .label-blue }

2023-01-08 | Bruce W. Lee, Jason Hyung-Jong Lee | cs.CL | [PDF](http://arxiv.org/pdf/2301.02975v2){: .btn .btn-green }

**Abstract**: Traditional English readability formulas, or equations, were largely
developed in the 20th century. Nonetheless, many researchers still rely on them
for various NLP applications. This phenomenon is presumably due to the
convenience and straightforwardness of readability formulas. In this work, we
contribute to the NLP community by 1. introducing New English Readability
Formula (NERF), 2. recalibrating the coefficients of old readability formulas
(Flesch-Kincaid Grade Level, Fog Index, SMOG Index, Coleman-Liau Index, and
Automated Readability Index), 3. evaluating the readability formulas, for use
in text simplification studies and medical texts, and 4. developing a
Python-based program for the wide application to various NLP projects.

Comments:
- Submitted to EMNLP 2022

---

## Towards Open World NeRF-Based SLAM

nerf{: .label .label-blue }

2023-01-08 | Daniil Lisus, Connor Holmes, Steven Waslander | cs.RO | [PDF](http://arxiv.org/pdf/2301.03102v4){: .btn .btn-green }

**Abstract**: Neural Radiance Fields (NeRFs) offer versatility and robustness in map
representations for Simultaneous Localization and Mapping (SLAM) tasks. This
paper extends NICE-SLAM, a recent state-of-the-art NeRF-based SLAM algorithm
capable of producing high quality NeRF maps. However, depending on the hardware
used, the required number of iterations to produce these maps often makes
NICE-SLAM run at less than real time. Additionally, the estimated trajectories
fail to be competitive with classical SLAM approaches. Finally, NICE-SLAM
requires a grid covering the considered environment to be defined prior to
runtime, making it difficult to extend into previously unseen scenes. This
paper seeks to make NICE-SLAM more open-world-capable by improving the
robustness and tracking accuracy, and generalizing the map representation to
handle unconstrained environments. This is done by improving measurement
uncertainty handling, incorporating motion information, and modelling the map
as having an explicit foreground and background. It is shown that these changes
are able to improve tracking accuracy by 85% to 97% depending on the available
resources, while also improving mapping in environments with visual information
extending outside of the predefined grid.

Comments:
- Presented at Conference on Robots and Vision (CRV) 2023. 8 pages, 2
  figures, 2 tables

---

## WIRE: Wavelet Implicit Neural Representations



2023-01-05 | Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha Balakrishnan, Ashok Veeraraghavan, Richard G. Baraniuk | cs.CV | [PDF](http://arxiv.org/pdf/2301.05187v1){: .btn .btn-green }

**Abstract**: Implicit neural representations (INRs) have recently advanced numerous
vision-related areas. INR performance depends strongly on the choice of the
nonlinear activation function employed in its multilayer perceptron (MLP)
network. A wide range of nonlinearities have been explored, but, unfortunately,
current INRs designed to have high accuracy also suffer from poor robustness
(to signal noise, parameter variation, etc.). Inspired by harmonic analysis, we
develop a new, highly accurate and robust INR that does not exhibit this
tradeoff. Wavelet Implicit neural REpresentation (WIRE) uses a continuous
complex Gabor wavelet activation function that is well-known to be optimally
concentrated in space-frequency and to have excellent biases for representing
images. A wide range of experiments (image denoising, image inpainting,
super-resolution, computed tomography reconstruction, image overfitting, and
novel view synthesis with neural radiance fields) demonstrate that WIRE defines
the new state of the art in INR accuracy, training time, and robustness.

---

## Class-Continuous Conditional Generative Neural Radiance Field

nerf{: .label .label-blue }

2023-01-03 | Jiwook Kim, Minhyeok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2301.00950v3){: .btn .btn-green }

**Abstract**: The 3D-aware image synthesis focuses on conserving spatial consistency
besides generating high-resolution images with fine details. Recently, Neural
Radiance Field (NeRF) has been introduced for synthesizing novel views with low
computational cost and superior performance. While several works investigate a
generative NeRF and show remarkable achievement, they cannot handle conditional
and continuous feature manipulation in the generation procedure. In this work,
we introduce a novel model, called Class-Continuous Conditional Generative NeRF
($\text{C}^{3}$G-NeRF), which can synthesize conditionally manipulated
photorealistic 3D-consistent images by projecting conditional features to the
generator and the discriminator. The proposed $\text{C}^{3}$G-NeRF is evaluated
with three image datasets, AFHQ, CelebA, and Cars. As a result, our model shows
strong 3D-consistency with fine details and smooth interpolation in conditional
feature manipulation. For instance, $\text{C}^{3}$G-NeRF exhibits a Fr\'echet
Inception Distance (FID) of 7.64 in 3D-aware face image synthesis with a
$\text{128}^{2}$ resolution. Additionally, we provide FIDs of generated
3D-aware images of each class of the datasets as it is possible to synthesize
class-conditional images with $\text{C}^{3}$G-NeRF.

Comments:
- BMVC 2023 (Accepted)

---

## Detachable Novel Views Synthesis of Dynamic Scenes Using  Distribution-Driven Neural Radiance Fields

nerf{: .label .label-blue }

2023-01-01 | Boyu Zhang, Wenbo Xu, Zheng Zhu, Guan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2301.00411v2){: .btn .btn-green }

**Abstract**: Representing and synthesizing novel views in real-world dynamic scenes from
casual monocular videos is a long-standing problem. Existing solutions
typically approach dynamic scenes by applying geometry techniques or utilizing
temporal information between several adjacent frames without considering the
underlying background distribution in the entire scene or the transmittance
over the ray dimension, limiting their performance on static and occlusion
areas. Our approach $\textbf{D}$istribution-$\textbf{D}$riven neural radiance
fields offers high-quality view synthesis and a 3D solution to
$\textbf{D}$etach the background from the entire $\textbf{D}$ynamic scene,
which is called $\text{D}^4$NeRF. Specifically, it employs a neural
representation to capture the scene distribution in the static background and a
6D-input NeRF to represent dynamic objects, respectively. Each ray sample is
given an additional occlusion weight to indicate the transmittance lying in the
static and dynamic components. We evaluate $\text{D}^4$NeRF on public dynamic
scenes and our urban driving scenes acquired from an autonomous-driving
dataset. Extensive experiments demonstrate that our approach outperforms
previous methods in rendering texture details and motion areas while also
producing a clean static background. Our code will be released at
https://github.com/Luciferbobo/D4NeRF.