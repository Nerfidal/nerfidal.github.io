---
layout: default
title: September 2024
parent: Papers
nav_order: 202409
---

<!---metadata--->


## UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from  Endoscopic Sparse Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Jiaxin Guo, Jiangliu Wang, Ruofeng Wei, Di Kang, Qi Dou, Yun-hui Liu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02917v1){: .btn .btn-green } |

**Abstract**: Visualizing surgical scenes is crucial for revealing internal anatomical
structures during minimally invasive procedures. Novel View Synthesis is a
vital technique that offers geometry and appearance reconstruction, enhancing
understanding, planning, and decision-making in surgical scenes. Despite the
impressive achievements of Neural Radiance Field (NeRF), its direct application
to surgical scenes produces unsatisfying results due to two challenges:
endoscopic sparse views and significant photometric inconsistencies. In this
paper, we propose uncertainty-aware conditional NeRF for novel view synthesis
to tackle the severe shape-radiance ambiguity from sparse surgical views. The
core of UC-NeRF is to incorporate the multi-view uncertainty estimation to
condition the neural radiance field for modeling the severe photometric
inconsistencies adaptively. Specifically, our UC-NeRF first builds a
consistency learner in the form of multi-view stereo network, to establish the
geometric correspondence from sparse views and generate uncertainty estimation
and feature priors. In neural rendering, we design a base-adaptive NeRF network
to exploit the uncertainty estimation for explicitly handling the photometric
inconsistencies. Furthermore, an uncertainty-guided geometry distillation is
employed to enhance geometry learning. Experiments on the SCARED and Hamlyn
datasets demonstrate our superior performance in rendering appearance and
geometry, consistently outperforming the current state-of-the-art approaches.
Our code will be released at \url{https://github.com/wrld/UC-NeRF}.



---

## GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous  Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Huasong Han, Kaixuan Zhou, Xiaoxiao Long, Yusen Wang, Chunxia Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2409.02382v1){: .btn .btn-green } |

**Abstract**: We propose GGS, a Generalizable Gaussian Splatting method for Autonomous
Driving which can achieve realistic rendering under large viewpoint changes.
Previous generalizable 3D gaussian splatting methods are limited to rendering
novel views that are very close to the original pair of images, which cannot
handle large differences in viewpoint. Especially in autonomous driving
scenarios, images are typically collected from a single lane. The limited
training perspective makes rendering images of a different lane very
challenging. To further improve the rendering capability of GGS under large
viewpoint changes, we introduces a novel virtual lane generation module into
GSS method to enables high-quality lane switching even without a multi-lane
dataset. Besides, we design a diffusion loss to supervise the generation of
virtual lane image to further address the problem of lack of data in the
virtual lanes. Finally, we also propose a depth refinement module to optimize
depth estimation in the GSS model. Extensive validation of our method, compared
to existing approaches, demonstrates state-of-the-art performance.



---

## Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video  Diffusion Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Zhibin Liu, Haoye Dong, Aviral Chharia, Hefeng Wu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02851v1){: .btn .btn-green } |

**Abstract**: Generating lifelike 3D humans from a single RGB image remains a challenging
task in computer vision, as it requires accurate modeling of geometry,
high-quality texture, and plausible unseen parts. Existing methods typically
use multi-view diffusion models for 3D generation, but they often face
inconsistent view issues, which hinder high-quality 3D human generation. To
address this, we propose Human-VDM, a novel method for generating 3D human from
a single RGB image using Video Diffusion Models. Human-VDM provides temporally
consistent views for 3D human generation using Gaussian Splatting. It consists
of three modules: a view-consistent human video diffusion module, a video
augmentation module, and a Gaussian Splatting module. First, a single image is
fed into a human video diffusion module to generate a coherent human video.
Next, the video augmentation module applies super-resolution and video
interpolation to enhance the textures and geometric smoothness of the generated
video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans
under the guidance of these high-resolution and view-consistent images.
Experiments demonstrate that Human-VDM achieves high-quality 3D human from a
single image, outperforming state-of-the-art methods in both generation quality
and quantity. Project page: https://human-vdm.github.io/Human-VDM/

Comments:
- 14 Pages, 8 figures, Project page:
  https://human-vdm.github.io/Human-VDM/

---

## Object Gaussian for Monocular 6D Pose Estimation from Sparse Views


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Luqing Luo, Shichu Sun, Jiangang Yang, Linfang Zheng, Jinwei Du, Jian Liu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02581v1){: .btn .btn-green } |

**Abstract**: Monocular object pose estimation, as a pivotal task in computer vision and
robotics, heavily depends on accurate 2D-3D correspondences, which often demand
costly CAD models that may not be readily available. Object 3D reconstruction
methods offer an alternative, among which recent advancements in 3D Gaussian
Splatting (3DGS) afford a compelling potential. Yet its performance still
suffers and tends to overfit with fewer input views. Embracing this challenge,
we introduce SGPose, a novel framework for sparse view object pose estimation
using Gaussian-based methods. Given as few as ten views, SGPose generates a
geometric-aware representation by starting with a random cuboid initialization,
eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as
required by traditional 3DGS methods. SGPose removes the dependence on CAD
models by regressing dense 2D-3D correspondences between images and the
reconstructed model from sparse input and random initialization, while the
geometric-consistent depth supervision and online synthetic view warping are
key to the success. Experiments on typical benchmarks, especially on the
Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods
even under sparse view constraints, under-scoring its potential in real-world
applications.



---

## $S^2$NeRF: Privacy-preserving Training Framework for NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Bokang Zhang, Yanglin Zhang, Zhikun Zhang, Jinglan Yang, Lingying Huang, Junfeng Wu | cs.CR | [PDF](http://arxiv.org/pdf/2409.01661v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and
graphics, facilitating novel view synthesis and influencing sectors like
extended reality and e-commerce. However, NeRF's dependence on extensive data
collection, including sensitive scene image data, introduces significant
privacy risks when users upload this data for model training. To address this
concern, we first propose SplitNeRF, a training framework that incorporates
split learning (SL) techniques to enable privacy-preserving collaborative model
training between clients and servers without sharing local data. Despite its
benefits, we identify vulnerabilities in SplitNeRF by developing two attack
methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which
exploit the shared gradient data and a few leaked scene images to reconstruct
private scene information. To counter these threats, we introduce $S^2$NeRF,
secure SplitNeRF that integrates effective defense mechanisms. By introducing
decaying noise related to the gradient norm into the shared gradient
information, $S^2$NeRF preserves privacy while maintaining a high utility of
the NeRF model. Our extensive evaluations across multiple datasets demonstrate
the effectiveness of $S^2$NeRF against privacy breaches, confirming its
viability for secure NeRF training in sensitive applications.

Comments:
- To appear in the ACM Conference on Computer and Communications
  Security (CCS'24), October 14-18, 2024, Salt Lake City, UT, USA

---

## GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color  Point Clouds via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Zixuan Guo, Yifan Xie, Weijing Xie, Peng Huang, Fei Ma, Fei Richard Yu | cs.RO | [PDF](http://arxiv.org/pdf/2409.01581v1){: .btn .btn-green } |

**Abstract**: Dense colored point clouds enhance visual perception and are of significant
value in various robotic applications. However, existing learning-based point
cloud upsampling methods are constrained by computational resources and batch
processing strategies, which often require subdividing point clouds into
smaller patches, leading to distortions that degrade perceptual quality. To
address this challenge, we propose a novel 2D-3D hybrid colored point cloud
upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for
robotic perception. This approach leverages 3DGS to bridge 3D point clouds with
their 2D rendered images in robot vision systems. A dual scale rendered image
restoration network transforms sparse point cloud renderings into dense
representations, which are then input into 3DGS along with precise robot camera
poses and interpolated sparse point clouds to reconstruct dense 3D point
clouds. We have made a series of enhancements to the vanilla 3DGS, enabling
precise control over the number of points and significantly boosting the
quality of the upsampled point cloud for robotic scene understanding. Our
framework supports processing entire point clouds on a single consumer-grade
GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation
and thus producing high-quality, dense colored point clouds with millions of
points for robot navigation and manipulation tasks. Extensive experimental
results on generating million-level point cloud data validate the effectiveness
of our method, substantially improving the quality of colored point clouds and
demonstrating significant potential for applications involving large-scale
point clouds in autonomous robotics and human-robot interaction scenarios.

Comments:
- 7 pages, 5 figures

---

## DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, Laura Leal-Taix√© | cs.CV | [PDF](http://arxiv.org/pdf/2409.02104v1){: .btn .btn-green } |

**Abstract**: Reconstructing scenes and tracking motion are two sides of the same coin.
Tracking points allow for geometric reconstruction [14], while geometric
reconstruction of (dynamic) scenes allows for 3D tracking of points over time
[24, 39]. The latter was recently also exploited for 2D point tracking to
overcome occlusion ambiguities by lifting tracking directly into 3D [38].
However, above approaches either require offline processing or multi-view
camera setups both unrealistic for real-world applications like robot
navigation or mixed reality. We target the challenge of online 2D and 3D point
tracking from unposed monocular camera input introducing Dynamic Online
Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to
reconstruct dynamic scenes in an online fashion. Our approach extends 3D
Gaussians to capture new content and object motions while estimating camera
movements from a single RGB frame. DynOMo stands out by enabling emergence of
point trajectories through robust image feature reconstruction and a novel
similarity-enhanced regularization term, without requiring any
correspondence-level supervision. It sets the first baseline for online point
tracking with monocular unposed cameras, achieving performance on par with
existing methods. We aim to inspire the community to advance online point
tracking and reconstruction, expanding the applicability to diverse real-world
scenarios.



---

## PRoGS: Progressive Rendering of Gaussian Splats

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels | cs.CV | [PDF](http://arxiv.org/pdf/2409.01761v1){: .btn .btn-green } |

**Abstract**: Over the past year, 3D Gaussian Splatting (3DGS) has received significant
attention for its ability to represent 3D scenes in a perceptually accurate
manner. However, it can require a substantial amount of storage since each
splat's individual data must be stored. While compression techniques offer a
potential solution by reducing the memory footprint, they still necessitate
retrieving the entire scene before any part of it can be rendered. In this
work, we introduce a novel approach for progressively rendering such scenes,
aiming to display visible content that closely approximates the final scene as
early as possible without loading the entire scene into memory. This approach
benefits both on-device rendering applications limited by memory constraints
and streaming applications where minimal bandwidth usage is preferred. To
achieve this, we approximate the contribution of each Gaussian to the final
scene and construct an order of prioritization on their inclusion in the
rendering process. Additionally, we demonstrate that our approach can be
combined with existing compression methods to progressively render (and stream)
3DGS scenes, optimizing bandwidth usage by focusing on the most important
splats within a scene. Overall, our work establishes a foundation for making
remotely hosted 3DGS content more quickly accessible to end-users in
over-the-top consumption scenarios, with our results showing significant
improvements in quality across all metrics compared to existing methods.



---

## GraspSplats: Efficient Manipulation with 3D Feature Splatting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Mazeyu Ji, Ri-Zhao Qiu, Xueyan Zou, Xiaolong Wang | cs.RO | [PDF](http://arxiv.org/pdf/2409.02084v1){: .btn .btn-green } |

**Abstract**: The ability for robots to perform efficient and zero-shot grasping of object
parts is crucial for practical applications and is becoming prevalent with
recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap
for representations to support such a capability, existing methods rely on
neural fields (NeRFs) via differentiable rendering or point-based projection
methods. However, we demonstrate that NeRFs are inappropriate for scene changes
due to their implicitness and point-based methods are inaccurate for part
localization without rendering-based optimization. To amend these issues, we
propose GraspSplats. Using depth supervision and a novel reference feature
computation method, GraspSplats generates high-quality scene representations in
under 60 seconds. We further validate the advantages of Gaussian-based
representation by showing that the explicit and optimized geometry in
GraspSplats is sufficient to natively support (1) real-time grasp sampling and
(2) dynamic and articulated object manipulation with point trackers. With
extensive experiments on a Franka robot, we demonstrate that GraspSplats
significantly outperforms existing methods under diverse task settings. In
particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO,
and 2D detection methods.

Comments:
- Project webpage: https://graspsplats.github.io/

---

## Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian  Splatting for Dynamic Surgical Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-02 | Qian Li, Shuojue Yang, Daiyun Shen, Yueming Jin | cs.CV | [PDF](http://arxiv.org/pdf/2409.01003v1){: .btn .btn-green } |

**Abstract**: Reconstructing endoscopic videos is crucial for high-fidelity visualization
and the efficiency of surgical operations. Despite the importance, existing 3D
reconstruction methods encounter several challenges, including stringent
demands for accuracy, imprecise camera positioning, intricate dynamic scenes,
and the necessity for rapid reconstruction. Addressing these issues, this paper
presents the first camera-pose-free scene reconstruction framework, Free-DyGS,
tailored for dynamic surgical videos, leveraging 3D Gaussian splatting
technology. Our approach employs a frame-by-frame reconstruction strategy and
is delineated into four distinct phases: Scene Initialization, Joint Learning,
Scene Expansion, and Retrospective Learning. We introduce a Generalizable
Gaussians Parameterization module within the Scene Initialization and Expansion
phases to proficiently generate Gaussian attributes for each pixel from the
RGBD frames. The Joint Learning phase is crafted to concurrently deduce scene
deformation and camera pose, facilitated by an innovative flexible deformation
module. In the scene expansion stage, the Gaussian points gradually grow as the
camera moves. The Retrospective Learning phase is dedicated to enhancing the
precision of scene deformation through the reassessment of prior frames. The
efficacy of the proposed Free-DyGS is substantiated through experiments on two
datasets: the StereoMIS and Hamlyn datasets. The experimental outcomes
underscore that Free-DyGS surpasses conventional baseline models in both
rendering fidelity and computational efficiency.


