---
layout: default
title: September 2024
parent: Papers
nav_order: 202409
---

<!---metadata--->


## Space-time 2D Gaussian Splatting for Accurate Surface Reconstruction  under Complex Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-27 | Shuo Wang, Binbin Huang, Ruoyu Wang, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2409.18852v1){: .btn .btn-green } |

**Abstract**: Previous surface reconstruction methods either suffer from low geometric
accuracy or lengthy training times when dealing with real-world complex dynamic
scenes involving multi-person activities, and human-object interactions. To
tackle the dynamic contents and the occlusions in complex scenes, we present a
space-time 2D Gaussian Splatting approach. Specifically, to improve geometric
quality in dynamic scenes, we learn canonical 2D Gaussian splats and deform
these 2D Gaussian splats while enforcing the disks of the Gaussian located on
the surface of the objects by introducing depth and normal regularizers.
Further, to tackle the occlusion issues in complex scenes, we introduce a
compositional opacity deformation strategy, which further reduces the surface
recovery of those occluded areas. Experiments on real-world sparse-view video
datasets and monocular dynamic datasets demonstrate that our reconstructions
outperform state-of-the-art methods, especially for the surface of the details.
The project page and more visualizations can be found at:
https://tb2-sy.github.io/st-2dgs/.

Comments:
- Project page: https://tb2-sy.github.io/st-2dgs/

---

## LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-26 | Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu | cs.CV | [PDF](http://arxiv.org/pdf/2409.18057v1){: .btn .btn-green } |

**Abstract**: Recent works have shown that neural radiance fields (NeRFs) on top of
parametric models have reached SOTA quality to build photorealistic head
avatars from a monocular video. However, one major limitation of the NeRF-based
avatars is the slow rendering speed due to the dense point sampling of NeRF,
preventing them from broader utility on resource-constrained devices. We
introduce LightAvatar, the first head avatar model based on neural light fields
(NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose
via a single network forward pass, without using mesh or volume rendering. The
proposed approach, while being conceptually appealing, poses a significant
challenge towards real-time efficiency and training stability. To resolve them,
we introduce dedicated network designs to obtain proper representations for the
NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a
distillation-based training strategy that uses a pretrained avatar model as
teacher to synthesize abundant pseudo data for training. A warping field
network is introduced to correct the fitting error in the real data so that the
model can learn better. Extensive experiments suggest that our method can
achieve new SOTA image quality quantitatively or qualitatively, while being
significantly faster than the counterparts, reporting 174.1 FPS (512x512
resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.

Comments:
- Appear in ECCV'24 CADL Workshop. Code:
  https://github.com/MingSun-Tse/LightAvatar-TensorFlow

---

## WaSt-3D: Wasserstein-2 Distance for Scene-to-Scene Stylization on 3D  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-26 | Dmytro Kotovenko, Olga Grebenkova, Nikolaos Sarafianos, Avinash Paliwal, Pingchuan Ma, Omid Poursaeed, Sreyas Mohan, Yuchen Fan, Yilei Li, Rakesh Ranjan, Bj√∂rn Ommer | cs.CV | [PDF](http://arxiv.org/pdf/2409.17917v1){: .btn .btn-green } |

**Abstract**: While style transfer techniques have been well-developed for 2D image
stylization, the extension of these methods to 3D scenes remains relatively
unexplored. Existing approaches demonstrate proficiency in transferring colors
and textures but often struggle with replicating the geometry of the scenes. In
our work, we leverage an explicit Gaussian Splatting (GS) representation and
directly match the distributions of Gaussians between style and content scenes
using the Earth Mover's Distance (EMD). By employing the entropy-regularized
Wasserstein-2 distance, we ensure that the transformation maintains spatial
smoothness. Additionally, we decompose the scene stylization problem into
smaller chunks to enhance efficiency. This paradigm shift reframes stylization
from a pure generative process driven by latent space losses to an explicit
matching of distributions between two Gaussian representations. Our method
achieves high-resolution 3D stylization by faithfully transferring details from
3D style scenes onto the content scene. Furthermore, WaSt-3D consistently
delivers results across diverse content and style scenes without necessitating
any training, as it relies solely on optimization-based techniques. See our
project page for additional results and source code:
$\href{https://compvis.github.io/wast3d/}{https://compvis.github.io/wast3d/}$.



---

## Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or  Low-light Conditions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-26 | Weng Fei Low, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2409.17988v1){: .btn .btn-green } |

**Abstract**: The stark contrast in the design philosophy of an event camera makes it
particularly ideal for operating under high-speed, high dynamic range and
low-light conditions, where standard cameras underperform. Nonetheless, event
cameras still suffer from some amount of motion blur, especially under these
challenging conditions, in contrary to what most think. This is attributed to
the limited bandwidth of the event sensor pixel, which is mostly proportional
to the light intensity. Thus, to ensure that event cameras can truly excel in
such conditions where it has an edge over standard cameras, it is crucial to
account for event motion blur in downstream applications, especially
reconstruction. However, none of the recent works on reconstructing Neural
Radiance Fields (NeRFs) from events, nor event simulators, have considered the
full effects of event motion blur. To this end, we propose, Deblur e-NeRF, a
novel method to directly and effectively reconstruct blur-minimal NeRFs from
motion-blurred events generated under high-speed motion or low-light
conditions. The core component of this work is a physically-accurate pixel
bandwidth model proposed to account for event motion blur under arbitrary speed
and lighting conditions. We also introduce a novel threshold-normalized total
variation loss to improve the regularization of large textureless patches.
Experiments on real and novel realistically simulated sequences verify our
effectiveness. Our code, event simulator and synthetic event dataset will be
open-sourced.

Comments:
- Accepted to ECCV 2024. Project website is accessible at
  https://wengflow.github.io/deblur-e-nerf. arXiv admin note: text overlap with
  arXiv:2006.07722 by other authors

---

## Neural Implicit Representation for Highly Dynamic LiDAR Mapping and  Odometry

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-26 | Qi Zhang, He Wang, Ru Li, Wenbin Li | cs.CV | [PDF](http://arxiv.org/pdf/2409.17729v1){: .btn .btn-green } |

**Abstract**: Recent advancements in Simultaneous Localization and Mapping (SLAM) have
increasingly highlighted the robustness of LiDAR-based techniques. At the same
time, Neural Radiance Fields (NeRF) have introduced new possibilities for 3D
scene reconstruction, exemplified by SLAM systems. Among these, NeRF-LOAM has
shown notable performance in NeRF-based SLAM applications. However, despite its
strengths, these systems often encounter difficulties in dynamic outdoor
environments due to their inherent static assumptions. To address these
limitations, this paper proposes a novel method designed to improve
reconstruction in highly dynamic outdoor scenes. Based on NeRF-LOAM, the
proposed approach consists of two primary components. First, we separate the
scene into static background and dynamic foreground. By identifying and
excluding dynamic elements from the mapping process, this segmentation enables
the creation of a dense 3D map that accurately represents the static background
only. The second component extends the octree structure to support
multi-resolution representation. This extension not only enhances
reconstruction quality but also aids in the removal of dynamic objects
identified by the first module. Additionally, Fourier feature encoding is
applied to the sampled points, capturing high-frequency information and leading
to more complete reconstruction results. Evaluations on various datasets
demonstrate that our method achieves more competitive results compared to
current state-of-the-art approaches.



---

## RT-GuIDE: Real-Time Gaussian splatting for Information-Driven  Exploration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-26 | Yuezhan Tao, Dexter Ong, Varun Murali, Igor Spasojevic, Pratik Chaudhari, Vijay Kumar | cs.RO | [PDF](http://arxiv.org/pdf/2409.18122v1){: .btn .btn-green } |

**Abstract**: We propose a framework for active mapping and exploration that leverages
Gaussian splatting for constructing information-rich maps. Further, we develop
a parallelized motion planning algorithm that can exploit the Gaussian map for
real-time navigation. The Gaussian map constructed onboard the robot is
optimized for both photometric and geometric quality while enabling real-time
situational awareness for autonomy. We show through simulation experiments that
our method is competitive with approaches that use alternate information gain
metrics, while being orders of magnitude faster to compute. In real-world
experiments, our algorithm achieves better map quality (10% higher Peak
Signal-to-Noise Ratio (PSNR) and 30% higher geometric reconstruction accuracy)
than Gaussian maps constructed by traditional exploration baselines. Experiment
videos and more details can be found on our project page:
https://tyuezhan.github.io/RT_GuIDE/

Comments:
- Submitted to ICRA2025

---

## TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic  Scene

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-26 | Sandika Biswas, Qianyi Wu, Biplab Banerjee, Hamid Rezatofighi | cs.CV | [PDF](http://arxiv.org/pdf/2409.17459v1){: .btn .btn-green } |

**Abstract**: Despite advancements in Neural Implicit models for 3D surface reconstruction,
handling dynamic environments with arbitrary rigid, non-rigid, or deformable
entities remains challenging. Many template-based methods are entity-specific,
focusing on humans, while generic reconstruction methods adaptable to such
dynamic scenes often require additional inputs like depth or optical flow or
rely on pre-trained image features for reasonable outcomes. These methods
typically use latent codes to capture frame-by-frame deformations. In contrast,
some template-free methods bypass these requirements and adopt traditional LBS
(Linear Blend Skinning) weights for a detailed representation of deformable
object motions, although they involve complex optimizations leading to lengthy
training times. To this end, as a remedy, this paper introduces TFS-NeRF, a
template-free 3D semantic NeRF for dynamic scenes captured from sparse or
single-view RGB videos, featuring interactions among various entities and more
time-efficient than other LBS-based approaches. Our framework uses an
Invertible Neural Network (INN) for LBS prediction, simplifying the training
process. By disentangling the motions of multiple entities and optimizing
per-entity skinning weights, our method efficiently generates accurate,
semantically separable geometries. Extensive experiments demonstrate that our
approach produces high-quality reconstructions of both deformable and
non-deformable objects in complex interactions, with improved training
efficiency compared to existing methods.

Comments:
- Accepted in NeuRIPS 2024

---

## HGS-Planner: Hierarchical Planning Framework for Active Scene  Reconstruction Using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-26 | Zijun Xu, Rui Jin, Ke Wu, Yi Zhao, Zhiwei Zhang, Jieru Zhao, Zhongxue Gan, Wenchao Ding | cs.RO | [PDF](http://arxiv.org/pdf/2409.17624v1){: .btn .btn-green } |

**Abstract**: In complex missions such as search and rescue,robots must make intelligent
decisions in unknown environments, relying on their ability to perceive and
understand their surroundings. High-quality and real-time reconstruction
enhances situational awareness and is crucial for intelligent robotics.
Traditional methods often struggle with poor scene representation or are too
slow for real-time use. Inspired by the efficacy of 3D Gaussian Splatting
(3DGS), we propose a hierarchical planning framework for fast and high-fidelity
active reconstruction. Our method evaluates completion and quality gain to
adaptively guide reconstruction, integrating global and local planning for
efficiency. Experiments in simulated and real-world environments show our
approach outperforms existing real-time methods.



---

## Language-Embedded Gaussian Splats (LEGS): Incrementally Building  Room-Scale Representations with a Mobile Robot


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-26 | Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung Min Kim, Justin Kerr, Richard Cheng, Muhammad Zubair Irshad, Ashwin Balakrishna, Thomas Kollar, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2409.18108v1){: .btn .btn-green } |

**Abstract**: Building semantic 3D maps is valuable for searching for objects of interest
in offices, warehouses, stores, and homes. We present a mapping system that
incrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D
scene representation that encodes both appearance and semantics in a unified
representation. LEGS is trained online as a robot traverses its environment to
enable localization of open-vocabulary object queries. We evaluate LEGS on 4
room-scale scenes where we query for objects in the scene to assess how LEGS
can capture semantic meaning. We compare LEGS to LERF and find that while both
systems have comparable object query success rates, LEGS trains over 3.5x
faster than LERF. Results suggest that a multi-camera setup and incremental
bundle adjustment can boost visual reconstruction quality in constrained robot
trajectories, and suggest LEGS can localize open-vocabulary and long-tail
object queries with up to 66% accuracy.



---

## Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized  Gaussian Splat

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-25 | Jonathan Michaux, Seth Isaacson, Challen Enninful Adu, Adam Li, Rahul Kashyap Swayampakula, Parker Ewen, Sean Rice, Katherine A. Skinner, Ram Vasudevan | cs.RO | [PDF](http://arxiv.org/pdf/2409.16915v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields and Gaussian Splatting have transformed the field of
computer vision by enabling photo-realistic representation of complex scenes.
Despite this success, they have seen only limited use in real-world robotics
tasks such as trajectory optimization. Two key factors have contributed to this
limited success. First, it is challenging to reason about collisions in
radiance models. Second, it is difficult to perform inference of radiance
models fast enough for real-time trajectory synthesis. This paper addresses
these challenges by proposing SPLANNING, a risk-aware trajectory optimizer that
operates in a Gaussian Splatting model. This paper first derives a method for
rigorously upper-bounding the probability of collision between a robot and a
radiance field. Second, this paper introduces a normalized reformulation of
Gaussian Splatting that enables the efficient computation of the collision
bound in a Gaussian Splat. Third, a method is presented to optimize
trajectories while avoiding collisions with a scene represented by a Gaussian
Splat. Experiments demonstrate that SPLANNING outperforms state-of-the-art
methods in generating collision-free trajectories in highly cluttered
environments. The proposed system is also tested on a real-world robot
manipulator. A project page is available at
https://roahmlab.github.io/splanning.

Comments:
- First two authors contributed equally. Project Page:
  https://roahmlab.github.io/splanning

---

## Disco4D: Disentangled 4D Human Generation and Animation from a Single  Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-25 | Hui En Pang, Shuai Liu, Zhongang Cai, Lei Yang, Tianwei Zhang, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2409.17280v1){: .btn .btn-green } |

**Abstract**: We present \textbf{Disco4D}, a novel Gaussian Splatting framework for 4D
human generation and animation from a single image. Different from existing
methods, Disco4D distinctively disentangles clothings (with Gaussian models)
from the human body (with SMPL-X model), significantly enhancing the generation
details and flexibility. It has the following technical innovations.
\textbf{1)} Disco4D learns to efficiently fit the clothing Gaussians over the
SMPL-X Gaussians. \textbf{2)} It adopts diffusion models to enhance the 3D
generation process, \textit{e.g.}, modeling occluded parts not visible in the
input image. \textbf{3)} It learns an identity encoding for each clothing
Gaussian to facilitate the separation and extraction of clothing assets.
Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics.
Extensive experiments demonstrate the superiority of Disco4D on 4D human
generation and animation tasks. Our visualizations can be found in
\url{https://disco-4d.github.io/}.



---

## SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and  a Physically Grounded Image Formation Model

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-25 | Daniel Yang, John J. Leonard, Yogesh Girdhar | cs.CV | [PDF](http://arxiv.org/pdf/2409.17345v1){: .btn .btn-green } |

**Abstract**: We introduce SeaSplat, a method to enable real-time rendering of underwater
scenes leveraging recent advances in 3D radiance fields. Underwater scenes are
challenging visual environments, as rendering through a medium such as water
introduces both range and color dependent effects on image capture. We
constrain 3D Gaussian Splatting (3DGS), a recent advance in radiance fields
enabling rapid training and real-time rendering of full 3D scenes, with a
physically grounded underwater image formation model. Applying SeaSplat to the
real-world scenes from SeaThru-NeRF dataset, a scene collected by an underwater
vehicle in the US Virgin Islands, and simulation-degraded real-world scenes,
not only do we see increased quantitative performance on rendering novel
viewpoints from the scene with the medium present, but are also able to recover
the underlying true color of the scene and restore renders to be without the
presence of the intervening medium. We show that the underwater image formation
helps learn scene structure, with better depth maps, as well as show that our
improvements maintain the significant computational improvements afforded by
leveraging a 3D Gaussian representation.

Comments:
- Project page here: https://seasplat.github.io

---

## Generative Object Insertion in Gaussian Splatting with a Multi-View  Diffusion Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-25 | Hongliang Zhong, Can Wang, Jingbo Zhang, Jing Liao | cs.CV | [PDF](http://arxiv.org/pdf/2409.16938v1){: .btn .btn-green } |

**Abstract**: Generating and inserting new objects into 3D content is a compelling approach
for achieving versatile scene recreation. Existing methods, which rely on SDS
optimization or single-view inpainting, often struggle to produce high-quality
results. To address this, we propose a novel method for object insertion in 3D
content represented by Gaussian Splatting. Our approach introduces a multi-view
diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable
video diffusion model to facilitate view-consistent object inpainting. Within
MVInpainter, we incorporate a ControlNet-based conditional injection module to
enable controlled and more predictable multi-view generation. After generating
the multi-view inpainted results, we further propose a mask-aware 3D
reconstruction technique to refine Gaussian Splatting reconstruction from these
sparse inpainted views. By leveraging these fabricate techniques, our approach
yields diverse results, ensures view-consistent and harmonious insertions, and
produces better object quality. Extensive experiments demonstrate that our
approach outperforms existing methods.

Comments:
- Project Page: https://github.com/JiuTongBro/MultiView_Inpaint

---

## TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-25 | Aggelina Chatziagapi, Bindita Chaudhuri, Amit Kumar, Rakesh Ranjan, Dimitris Samaras, Nikolaos Sarafianos | cs.CV | [PDF](http://arxiv.org/pdf/2409.16666v1){: .btn .btn-green } |

**Abstract**: We introduce a novel framework that learns a dynamic neural radiance field
(NeRF) for full-body talking humans from monocular videos. Prior work
represents only the body pose or the face. However, humans communicate with
their full body, combining body pose, hand gestures, as well as facial
expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network
that represents the holistic 4D human motion. Given a monocular video of a
subject, we learn corresponding modules for the body, face, and hands, that are
combined together to generate the final result. To capture complex finger
articulation, we learn an additional deformation field for the hands. Our
multi-identity representation enables simultaneous training for multiple
subjects, as well as robust animation under completely unseen poses. It can
also generalize to novel identities, given only a short video as input. We
demonstrate state-of-the-art performance for animating full-body talking
humans, with fine-grained hand articulation and facial expressions.

Comments:
- Accepted by ECCVW 2024. Project page:
  https://aggelinacha.github.io/TalkinNeRF/

---

## Go-SLAM: Grounded Object Segmentation and Localization with Gaussian  Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-25 | Phu Pham, Dipam Patel, Damon Conover, Aniket Bera | cs.RO | [PDF](http://arxiv.org/pdf/2409.16944v1){: .btn .btn-green } |

**Abstract**: We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting
SLAM to reconstruct dynamic environments while embedding object-level
information within the scene representations. This framework employs advanced
object segmentation techniques, assigning a unique identifier to each Gaussian
splat that corresponds to the object it represents. Consequently, our system
facilitates open-vocabulary querying, allowing users to locate objects using
natural language descriptions. Furthermore, the framework features an optimal
path generation module that calculates efficient navigation paths for robots
toward queried objects, considering obstacles and environmental uncertainties.
Comprehensive evaluations in various scene settings demonstrate the
effectiveness of our approach in delivering high-fidelity scene
reconstructions, precise object segmentation, flexible object querying, and
efficient robot path planning. This work represents an additional step forward
in bridging the gap between 3D scene reconstruction, semantic object
understanding, and real-time environment interactions.



---

## Disentangled Generation and Aggregation for Robust Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-24 | Shihe Shen, Huachen Gao, Wangze Xu, Rui Peng, Luyang Tang, Kaiqiang Xiong, Jianbo Jiao, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.15715v1){: .btn .btn-green } |

**Abstract**: The utilization of the triplane-based radiance fields has gained attention in
recent years due to its ability to effectively disentangle 3D scenes with a
high-quality representation and low computation cost. A key requirement of this
method is the precise input of camera poses. However, due to the local update
property of the triplane, a similar joint estimation as previous joint
pose-NeRF optimization works easily results in local minima. To this end, we
propose the Disentangled Triplane Generation module to introduce global feature
context and smoothness into triplane learning, which mitigates errors caused by
local updating. Then, we propose the Disentangled Plane Aggregation to mitigate
the entanglement caused by the common triplane feature aggregation during
camera pose updating. In addition, we introduce a two-stage warm-start training
strategy to reduce the implicit constraints caused by the triplane generator.
Quantitative and qualitative results demonstrate that our proposed method
achieves state-of-the-art performance in novel view synthesis with noisy or
unknown camera poses, as well as efficient convergence of optimization. Project
page: https://gaohchen.github.io/DiGARR/.

Comments:
- 27 pages, 11 figures, Accepted by ECCV'2024

---

## Plenoptic PNG: Real-Time Neural Radiance Fields in 150 KB

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-24 | Jae Yong Lee, Yuqun Wu, Chuhang Zou, Derek Hoiem, Shenlong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.15689v1){: .btn .btn-green } |

**Abstract**: The goal of this paper is to encode a 3D scene into an extremely compact
representation from 2D images and to enable its transmittance, decoding and
rendering in real-time across various platforms. Despite the progress in NeRFs
and Gaussian Splats, their large model size and specialized renderers make it
challenging to distribute free-viewpoint 3D content as easily as images. To
address this, we have designed a novel 3D representation that encodes the
plenoptic function into sinusoidal function indexed dense volumes. This
approach facilitates feature sharing across different locations, improving
compactness over traditional spatial voxels. The memory footprint of the dense
3D feature grid can be further reduced using spatial decomposition techniques.
This design combines the strengths of spatial hashing functions and voxel
decomposition, resulting in a model size as small as 150 KB for each 3D scene.
Moreover, PPNG features a lightweight rendering pipeline with only 300 lines of
code that decodes its representation into standard GL textures and fragment
shaders. This enables real-time rendering using the traditional GL pipeline,
ensuring universal compatibility and efficiency across various platforms
without additional dependencies.



---

## GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for  Improved Visual Localization

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-24 | Gennady Sidorov, Malik Mohrat, Ksenia Lebedeva, Ruslan Rakhimov, Sergey Kolyubin | cs.CV | [PDF](http://arxiv.org/pdf/2409.16502v1){: .btn .btn-green } |

**Abstract**: Although various visual localization approaches exist, such as scene
coordinate and pose regression, these methods often struggle with high memory
consumption or extensive optimization requirements. To address these
challenges, we utilize recent advancements in novel view synthesis,
particularly 3D Gaussian Splatting (3DGS), to enhance localization. 3DGS allows
for the compact encoding of both 3D geometry and scene appearance with its
spatial features. Our method leverages the dense description maps produced by
XFeat's lightweight keypoint detection and description model. We propose
distilling these dense keypoint descriptors into 3DGS to improve the model's
spatial understanding, leading to more accurate camera pose predictions through
2D-3D correspondences. After estimating an initial pose, we refine it using a
photometric warping loss. Benchmarking on popular indoor and outdoor datasets
shows that our approach surpasses state-of-the-art Neural Render Pose (NRP)
methods, including NeRFMatch and PNeRFLoc.

Comments:
- Project website at https://gsplatloc.github.io/

---

## Semantics-Controlled Gaussian Splatting for Outdoor Scene Reconstruction  and Rendering in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-24 | Hannah Schieber, Jacob Young, Tobias Langlotz, Stefanie Zollmann, Daniel Roth | cs.CV | [PDF](http://arxiv.org/pdf/2409.15959v1){: .btn .btn-green } |

**Abstract**: Advancements in 3D rendering like Gaussian Splatting (GS) allow novel view
synthesis and real-time rendering in virtual reality (VR). However, GS-created
3D environments are often difficult to edit. For scene enhancement or to
incorporate 3D assets, segmenting Gaussians by class is essential. Existing
segmentation approaches are typically limited to certain types of scenes, e.g.,
''circular'' scenes, to determine clear object boundaries. However, this method
is ineffective when removing large objects in non-''circling'' scenes such as
large outdoor scenes. We propose Semantics-Controlled GS (SCGS), a
segmentation-driven GS approach, enabling the separation of large scene parts
in uncontrolled, natural environments. SCGS allows scene editing and the
extraction of scene parts for VR. Additionally, we introduce a challenging
outdoor dataset, overcoming the ''circling'' setup. We outperform the
state-of-the-art in visual quality on our dataset and in segmentation quality
on the 3D-OVS dataset. We conducted an exploratory user study, comparing a
360-video, plain GS, and SCGS in VR with a fixed viewpoint. In our subsequent
main study, users were allowed to move freely, evaluating plain GS and SCGS.
Our main study results show that participants clearly prefer SCGS over plain
GS. We overall present an innovative approach that surpasses the
state-of-the-art both technically and in user experience.



---

## Frequency-based View Selection in Gaussian Splatting Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-24 | Monica M. Q. Li, Pierre-Yves Lajoie, Giovanni Beltrame | cs.CV | [PDF](http://arxiv.org/pdf/2409.16470v1){: .btn .btn-green } |

**Abstract**: Three-dimensional reconstruction is a fundamental problem in robotics
perception. We examine the problem of active view selection to perform 3D
Gaussian Splatting reconstructions with as few input images as possible.
Although 3D Gaussian Splatting has made significant progress in image rendering
and 3D reconstruction, the quality of the reconstruction is strongly impacted
by the selection of 2D images and the estimation of camera poses through
Structure-from-Motion (SfM) algorithms. Current methods to select views that
rely on uncertainties from occlusions, depth ambiguities, or neural network
predictions directly are insufficient to handle the issue and struggle to
generalize to new scenes. By ranking the potential views in the frequency
domain, we are able to effectively estimate the potential information gain of
new viewpoints without ground truth data. By overcoming current constraints on
model architecture and efficacy, our method achieves state-of-the-art results
in view selection, demonstrating its potential for efficient image-based 3D
reconstruction.

Comments:
- 8 pages, 4 figures

---

## Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with  Enhanced Generalization and Personalization Abilities

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-23 | Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du | cs.CV | [PDF](http://arxiv.org/pdf/2409.16147v2){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant
potential for modeling 3D head avatars, providing greater flexibility than
mesh-based methods and more efficient rendering compared to NeRF-based
approaches. Despite these advancements, the creation of controllable 3DGS-based
head avatars remains time-intensive, often requiring tens of minutes to hours.
To expedite this process, we here introduce the ``Gaussian D\'ej\`a-vu"
framework, which first obtains a generalized model of the head avatar and then
personalizes the result. The generalized model is trained on large 2D
(synthetic and real) image datasets. This model provides a well-initialized 3D
Gaussian head that is further refined using a monocular video to achieve the
personalized head avatar. For personalizing, we propose learnable
expression-aware rectification blendmaps to correct the initial 3D Gaussians,
ensuring rapid convergence without the reliance on neural networks. Experiments
demonstrate that the proposed method meets its objectives. It outperforms
state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as
well as reduces training time consumption to at least a quarter of the existing
methods, producing the avatar in minutes.

Comments:
- 11 pages, Accepted by WACV 2025 in Round 1

---

## AgriNeRF: Neural Radiance Fields for Agriculture in Challenging Lighting  Conditions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-23 | Samarth Chopra, Fernando Cladera, Varun Murali, Vijay Kumar | cs.RO | [PDF](http://arxiv.org/pdf/2409.15487v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have shown significant promise in 3D scene
reconstruction and novel view synthesis. In agricultural settings, NeRFs can
serve as digital twins, providing critical information about fruit detection
for yield estimation and other important metrics for farmers. However,
traditional NeRFs are not robust to challenging lighting conditions, such as
low-light, extreme bright light and varying lighting. To address these issues,
this work leverages three different sensors: an RGB camera, an event camera and
a thermal camera. Our RGB scene reconstruction shows an improvement in PSNR and
SSIM by +2.06 dB and +8.3% respectively. Our cross-spectral scene
reconstruction enhances downstream fruit detection by +43.0% in mAP50 and
+61.1% increase in mAP50-95. The integration of additional sensors leads to a
more robust and informative NeRF. We demonstrate that our multi-modal system
yields high quality photo-realistic reconstructions under various tree canopy
covers and at different times of the day. This work results in the development
of a resilient NeRF, capable of performing well in visibly degraded scenarios,
as well as a learnt cross-spectral representation, that is used for automated
fruit detection.

Comments:
- 7 pages, 5 figures

---

## Human Hair Reconstruction with Strand-Aligned 3D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-23 | Egor Zakharov, Vanessa Sklyarova, Michael Black, Giljoo Nam, Justus Thies, Otmar Hilliges | cs.CV | [PDF](http://arxiv.org/pdf/2409.14778v1){: .btn .btn-green } |

**Abstract**: We introduce a new hair modeling method that uses a dual representation of
classical hair strands and 3D Gaussians to produce accurate and realistic
strand-based reconstructions from multi-view data. In contrast to recent
approaches that leverage unstructured Gaussians to model human avatars, our
method reconstructs the hair using 3D polylines, or strands. This fundamental
difference allows the use of the resulting hairstyles out-of-the-box in modern
computer graphics engines for editing, rendering, and simulation. Our 3D
lifting method relies on unstructured Gaussians to generate multi-view ground
truth data to supervise the fitting of hair strands. The hairstyle itself is
represented in the form of the so-called strand-aligned 3D Gaussians. This
representation allows us to combine strand-based hair priors, which are
essential for realistic modeling of the inner structure of hairstyles, with the
differentiable rendering capabilities of 3D Gaussian Splatting. Our method,
named Gaussian Haircut, is evaluated on synthetic and real scenes and
demonstrates state-of-the-art performance in the task of strand-based hair
reconstruction.



---

## MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse  Input Views

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-22 | Wangze Xu, Huachen Gao, Shihe Shen, Rui Peng, Jianbo Jiao, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.14316v1){: .btn .btn-green } |

**Abstract**: Recently, the Neural Radiance Field (NeRF) advancement has facilitated
few-shot Novel View Synthesis (NVS), which is a significant challenge in 3D
vision applications. Despite numerous attempts to reduce the dense input
requirement in NeRF, it still suffers from time-consumed training and rendering
processes. More recently, 3D Gaussian Splatting (3DGS) achieves real-time
high-quality rendering with an explicit point-based representation. However,
similar to NeRF, it tends to overfit the train views for lack of constraints.
In this paper, we propose \textbf{MVPGS}, a few-shot NVS method that excavates
the multi-view priors based on 3D Gaussian Splatting. We leverage the recent
learning-based Multi-view Stereo (MVS) to enhance the quality of geometric
initialization for 3DGS. To mitigate overfitting, we propose a forward-warping
method for additional appearance constraints conforming to scenes based on the
computed geometry. Furthermore, we introduce a view-consistent geometry
constraint for Gaussian parameters to facilitate proper optimization
convergence and utilize a monocular depth regularization as compensation.
Experiments show that the proposed method achieves state-of-the-art performance
with real-time rendering speed. Project page:
https://zezeaaa.github.io/projects/MVPGS/

Comments:
- Accepted by ECCV 2024, Project page:
  https://zezeaaa.github.io/projects/MVPGS/

---

## SplatLoc: 3D Gaussian Splatting-based Visual Localization for Augmented  Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-21 | Hongjia Zhai, Xiyu Zhang, Boming Zhao, Hai Li, Yijia He, Zhaopeng Cui, Hujun Bao, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2409.14067v1){: .btn .btn-green } |

**Abstract**: Visual localization plays an important role in the applications of Augmented
Reality (AR), which enable AR devices to obtain their 6-DoF pose in the
pre-build map in order to render virtual content in real scenes. However, most
existing approaches can not perform novel view rendering and require large
storage capacities for maps. To overcome these limitations, we propose an
efficient visual localization method capable of high-quality rendering with
fewer parameters. Specifically, our approach leverages 3D Gaussian primitives
as the scene representation. To ensure precise 2D-3D correspondences for pose
estimation, we develop an unbiased 3D scene-specific descriptor decoder for
Gaussian primitives, distilled from a constructed feature volume. Additionally,
we introduce a salient 3D landmark selection algorithm that selects a suitable
primitive subset based on the saliency score for localization. We further
regularize key Gaussian primitives to prevent anisotropic effects, which also
improves localization performance. Extensive experiments on two widely used
datasets demonstrate that our method achieves superior or comparable rendering
and localization performance to state-of-the-art implicit-based visual
localization approaches. Project page:
\href{https://zju3dv.github.io/splatloc}{https://zju3dv.github.io/splatloc}.



---

## MOSE: Monocular Semantic Reconstruction Using NeRF-Lifted Noisy Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-21 | Zhenhua Du, Binbin Xu, Haoyu Zhang, Kai Huo, Shuaifeng Zhi | cs.CV | [PDF](http://arxiv.org/pdf/2409.14019v1){: .btn .btn-green } |

**Abstract**: Accurately reconstructing dense and semantically annotated 3D meshes from
monocular images remains a challenging task due to the lack of geometry
guidance and imperfect view-dependent 2D priors. Though we have witnessed
recent advancements in implicit neural scene representations enabling precise
2D rendering simply from multi-view images, there have been few works
addressing 3D scene understanding with monocular priors alone. In this paper,
we propose MOSE, a neural field semantic reconstruction approach to lift
inferred image-level noisy priors to 3D, producing accurate semantics and
geometry in both 3D and 2D space. The key motivation for our method is to
leverage generic class-agnostic segment masks as guidance to promote local
consistency of rendered semantics during training. With the help of semantics,
we further apply a smoothness regularization to texture-less regions for better
geometric quality, thus achieving mutual benefits of geometry and semantics.
Experiments on the ScanNet dataset show that our MOSE outperforms relevant
baselines across all metrics on tasks of 3D semantic segmentation, 2D semantic
segmentation and 3D surface reconstruction.

Comments:
- 8 pages, 10 figures

---

## 3D-GSW: 3D Gaussian Splatting Watermark for Protecting Copyrights in  Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-20 | Youngdong Jang, Hyunje Park, Feng Yang, Heeju Ko, Euijin Choo, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2409.13222v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian splatting has been getting a lot of attention as an
innovative method for representing 3D space due to rapid rendering and image
quality. However, copyright protection for the 3D Gaussian splatting has not
yet been introduced. In this paper, we present a novel watermarking method for
3D Gaussian splatting. The proposed method embeds a binary message into 3D
Gaussians by fine-tuning the pre-trained 3D Gaussian splatting model. To
achieve this, we present Frequency-Guided Densification (FGD) that utilizes
Discrete Fourier Transform to find patches with high-frequencies and split 3D
Gaussians based on 3D Gaussian Contribution Vector. It is each 3D Gaussian
contribution to rendered pixel colors, improving both rendering quality and bit
accuracy. Furthermore, we modify an adaptive gradient mask to enhance rendering
quality. Our experiments show that our method can embed a watermark in 3D
Gaussians imperceptibly with increased capacity and robustness against attacks.
Our method reduces optimization cost and achieves state-of-the-art performance
compared to other methods.



---

## Elite-EvGS: Learning Event-based 3D Gaussian Splatting by Distilling  Event-to-Video Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-20 | Zixin Zhang, Kanghao Chen, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.13392v1){: .btn .btn-green } |

**Abstract**: Event cameras are bio-inspired sensors that output asynchronous and sparse
event streams, instead of fixed frames. Benefiting from their distinct
advantages, such as high dynamic range and high temporal resolution, event
cameras have been applied to address 3D reconstruction, important for robotic
mapping. Recently, neural rendering techniques, such as 3D Gaussian splatting
(3DGS), have been shown successful in 3D reconstruction. However, it still
remains under-explored how to develop an effective event-based 3DGS pipeline.
In particular, as 3DGS typically depends on high-quality initialization and
dense multiview constraints, a potential problem appears for the 3DGS
optimization with events given its inherent sparse property. To this end, we
propose a novel event-based 3DGS framework, named Elite-EvGS. Our key idea is
to distill the prior knowledge from the off-the-shelf event-to-video (E2V)
models to effectively reconstruct 3D scenes from events in a coarse-to-fine
optimization manner. Specifically, to address the complexity of 3DGS
initialization from events, we introduce a novel warm-up initialization
strategy that optimizes a coarse 3DGS from the frames generated by E2V models
and then incorporates events to refine the details. Then, we propose a
progressive event supervision strategy that employs the window-slicing
operation to progressively reduce the number of events used for supervision.
This subtly relives the temporal randomness of the event frames, benefiting the
optimization of local textural and global structural details. Experiments on
the benchmark datasets demonstrate that Elite-EvGS can reconstruct 3D scenes
with better textural and structural details. Meanwhile, our method yields
plausible performance on the captured real-world data, including diverse
challenging conditions, such as fast motion and low light scenes.



---

## Spectral-GS: Taming 3D Gaussian Splatting with Spectral Entropy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Letian Huang, Jie Guo, Jialin Dan, Ruoyu Fu, Shujie Wang, Yuanqi Li, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2409.12771v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3D-GS) has achieved impressive results in
novel view synthesis, demonstrating high fidelity and efficiency. However, it
easily exhibits needle-like artifacts, especially when increasing the sampling
rate. Mip-Splatting tries to remove these artifacts with a 3D smoothing filter
for frequency constraints and a 2D Mip filter for approximated supersampling.
Unfortunately, it tends to produce over-blurred results, and sometimes
needle-like Gaussians still persist. Our spectral analysis of the covariance
matrix during optimization and densification reveals that current 3D-GS lacks
shape awareness, relying instead on spectral radius and view positional
gradients to determine splitting. As a result, needle-like Gaussians with small
positional gradients and low spectral entropy fail to split and overfit
high-frequency details. Furthermore, both the filters used in 3D-GS and
Mip-Splatting reduce the spectral entropy and increase the condition number
during zooming in to synthesize novel view, causing view inconsistencies and
more pronounced artifacts. Our Spectral-GS, based on spectral analysis,
introduces 3D shape-aware splitting and 2D view-consistent filtering
strategies, effectively addressing these issues, enhancing 3D-GS's capability
to represent high-frequency details without noticeable artifacts, and achieving
high-quality photorealistic rendering.



---

## GaRField++: Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Hanyue Zhang, Zhiliu Yang, Xinhe Zuo, Yuxin Tong, Ying Long, Chen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2409.12774v3){: .btn .btn-green } |

**Abstract**: This paper proposes a novel framework for large-scale scene reconstruction
based on 3D Gaussian splatting (3DGS) and aims to address the scalability and
accuracy challenges faced by existing methods. For tackling the scalability
issue, we split the large scene into multiple cells, and the candidate
point-cloud and camera views of each cell are correlated through a
visibility-based camera selection and a progressive point-cloud extension. To
reinforce the rendering quality, three highlighted improvements are made in
comparison with vanilla 3DGS, which are a strategy of the ray-Gaussian
intersection and the novel Gaussians density control for learning efficiency,
an appearance decoupling module based on ConvKAN network to solve uneven
lighting conditions in large-scale scenes, and a refined final loss with the
color loss, the depth distortion loss, and the normal consistency loss.
Finally, the seamless stitching procedure is executed to merge the individual
Gaussian radiance field for novel view synthesis across different cells.
Evaluation of Mill19, Urban3D, and MatrixCity datasets shows that our method
consistently generates more high-fidelity rendering results than
state-of-the-art methods of large-scale scene reconstruction. We further
validate the generalizability of the proposed approach by rendering on
self-collected video clips recorded by a commercial drone.



---

## Hi-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Boying Li, Zhixi Cai, Yuan-Fang Li, Ian Reid, Hamid Rezatofighi | cs.RO | [PDF](http://arxiv.org/pdf/2409.12518v1){: .btn .btn-green } |

**Abstract**: We propose Hi-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a
novel hierarchical categorical representation, which enables accurate global 3D
semantic mapping, scaling-up capability, and explicit semantic label prediction
in the 3D world. The parameter usage in semantic SLAM systems increases
significantly with the growing complexity of the environment, making it
particularly challenging and costly for scene understanding. To address this
problem, we introduce a novel hierarchical representation that encodes semantic
information in a compact form into 3D Gaussian Splatting, leveraging the
capabilities of large language models (LLMs). We further introduce a novel
semantic loss designed to optimize hierarchical semantic information through
both inter-level and cross-level optimization. Furthermore, we enhance the
whole SLAM system, resulting in improved tracking and mapping performance. Our
Hi-SLAM outperforms existing dense SLAM methods in both mapping and tracking
accuracy, while achieving a 2x operation speed-up. Additionally, it exhibits
competitive performance in rendering semantic segmentation in small synthetic
scenes, with significantly reduced storage and training time requirements.
Rendering FPS impressively reaches 2,000 with semantic information and 3,000
without it. Most notably, it showcases the capability of handling the complex
real-world scene with more than 500 semantic classes, highlighting its valuable
scaling-up capability.

Comments:
- 6 pages, 4 figures

---

## CrossRT: A cross platform programming technology for  hardware-accelerated ray tracing in CG and CV applications

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Vladimir Frolov, Vadim Sanzharov, Garifullin Albert, Maxim Raenchuk, Alexei Voloboy | cs.GR | [PDF](http://arxiv.org/pdf/2409.12617v1){: .btn .btn-green } |

**Abstract**: We propose a programming technology that bridges cross-platform compatibility
and hardware acceleration in ray tracing applications. Our methodology enables
developers to define algorithms while our translator manages implementation
specifics for different hardware or APIs. Features include: generating
hardware-accelerated code from hardware-agnostic, object-oriented C++ algorithm
descriptions; enabling users to define software fallbacks for
non-hardware-accelerated CPUs and GPUs; producing GPU programming API-based
algorithm implementations resembling manually ported C++ versions. The
generated code is editable and readable, allowing for additional hardware
acceleration. Our translator supports single megakernel and multiple kernel
path tracing implementations without altering the programming model or input
source code. Wavefront mode is crucial for NeRF and SDF, ensuring efficient
evaluation with multiple kernels. Validation on tasks such as BVH tree
build/traversal, ray-surface intersection for SDF, ray-volume intersection for
3D Gaussian Splatting, and complex Path Tracing models showed comparable
performance levels to expert-written implementations for GPUs. Our technology
outperformed existing Path Tracing implementations.



---

## DrivingForward: Feed-forward 3D Gaussian Splatting for Driving Scene  Reconstruction from Flexible Surround-view Input

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Qijian Tian, Xin Tan, Yuan Xie, Lizhuang Ma | cs.CV | [PDF](http://arxiv.org/pdf/2409.12753v1){: .btn .btn-green } |

**Abstract**: We propose DrivingForward, a feed-forward Gaussian Splatting model that
reconstructs driving scenes from flexible surround-view input. Driving scene
images from vehicle-mounted cameras are typically sparse, with limited overlap,
and the movement of the vehicle further complicates the acquisition of camera
extrinsics. To tackle these challenges and achieve real-time reconstruction, we
jointly train a pose network, a depth network, and a Gaussian network to
predict the Gaussian primitives that represent the driving scenes. The pose
network and depth network determine the position of the Gaussian primitives in
a self-supervised manner, without using depth ground truth and camera
extrinsics during training. The Gaussian network independently predicts
primitive parameters from each input image, including covariance, opacity, and
spherical harmonics coefficients. At the inference stage, our model can achieve
feed-forward reconstruction from flexible multi-frame surround-view input.
Experiments on the nuScenes dataset show that our model outperforms existing
state-of-the-art feed-forward and scene-optimized reconstruction methods in
terms of reconstruction.

Comments:
- Project page: https://fangzhou2000.github.io/projects/drivingforward/

---

## 3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Lukas H√∂llein, Alja≈æ Bo≈æiƒç, Michael Zollh√∂fer, Matthias Nie√üner | cs.CV | [PDF](http://arxiv.org/pdf/2409.12892v1){: .btn .btn-green } |

**Abstract**: We present 3DGS-LM, a new method that accelerates the reconstruction of 3D
Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored
Levenberg-Marquardt (LM). Existing methods reduce the optimization time by
decreasing the number of Gaussians or by improving the implementation of the
differentiable rasterizer. However, they still rely on the ADAM optimizer to
fit Gaussian parameters of a scene in thousands of iterations, which can take
up to an hour. To this end, we change the optimizer to LM that runs in
conjunction with the 3DGS differentiable rasterizer. For efficient GPU
parallization, we propose a caching data structure for intermediate gradients
that allows us to efficiently calculate Jacobian-vector products in custom CUDA
kernels. In every LM iteration, we calculate update directions from multiple
image subsets using these kernels and combine them in a weighted mean. Overall,
our method is 30% faster than the original 3DGS while obtaining the same
reconstruction quality. Our optimization is also agnostic to other methods that
acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.

Comments:
- project page: https://lukashoel.github.io/3DGS-LM, video:
  https://www.youtube.com/watch?v=tDiGuGMssg8, code:
  https://github.com/lukasHoel/3DGS-LM

---

## LI-GS: Gaussian Splatting with LiDAR Incorporated for Accurate  Large-Scale Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Changjian Jiang, Ruilan Gao, Kele Shao, Yue Wang, Rong Xiong, Yu Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2409.12899v1){: .btn .btn-green } |

**Abstract**: Large-scale 3D reconstruction is critical in the field of robotics, and the
potential of 3D Gaussian Splatting (3DGS) for achieving accurate object-level
reconstruction has been demonstrated. However, ensuring geometric accuracy in
outdoor and unbounded scenes remains a significant challenge. This study
introduces LI-GS, a reconstruction system that incorporates LiDAR and Gaussian
Splatting to enhance geometric accuracy in large-scale scenes. 2D Gaussain
surfels are employed as the map representation to enhance surface alignment.
Additionally, a novel modeling method is proposed to convert LiDAR point clouds
to plane-constrained multimodal Gaussian Mixture Models (GMMs). The GMMs are
utilized during both initialization and optimization stages to ensure
sufficient and continuous supervision over the entire scene while mitigating
the risk of over-fitting. Furthermore, GMMs are employed in mesh extraction to
eliminate artifacts and improve the overall geometric quality. Experiments
demonstrate that our method outperforms state-of-the-art methods in large-scale
3D reconstruction, achieving higher accuracy compared to both LiDAR-based
methods and Gaussian-based methods with improvements of 52.6% and 68.7%,
respectively.



---

## GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled  Appearance and Geometry Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Victor Rong, Jingxiang Chen, Sherwin Bahmani, Kiriakos N. Kutulakos, David B. Lindell | cs.CV | [PDF](http://arxiv.org/pdf/2409.12954v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has demonstrated excellent performance for view synthesis
and scene reconstruction. The representation achieves photorealistic quality by
optimizing the position, scale, color, and opacity of thousands to millions of
2D or 3D Gaussian primitives within a scene. However, since each Gaussian
primitive encodes both appearance and geometry, these attributes are strongly
coupled--thus, high-fidelity appearance modeling requires a large number of
Gaussian primitives, even when the scene geometry is simple (e.g., for a
textured planar surface). We propose to texture each 2D Gaussian primitive so
that even a single Gaussian can be used to capture appearance details. By
employing per-primitive texturing, our appearance representation is agnostic to
the topology and complexity of the scene's geometry. We show that our approach,
GStex, yields improved visual quality over prior work in texturing Gaussian
splats. Furthermore, we demonstrate that our decoupling enables improved novel
view synthesis performance compared to 2D Gaussian splatting when reducing the
number of Gaussian primitives, and that GStex can be used for scene appearance
editing and re-texturing.

Comments:
- Project page: https://lessvrong.com/cs/gstex

---

## MGSO: Monocular Real-time Photometric SLAM with Efficient 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Yan Song Hu, Nicolas Abboud, Muhammad Qasim Ali, Adam Srebrnjak Yang, Imad Elhajj, Daniel Asmar, Yuhao Chen, John S. Zelek | cs.RO | [PDF](http://arxiv.org/pdf/2409.13055v1){: .btn .btn-green } |

**Abstract**: Real-time SLAM with dense 3D mapping is computationally challenging,
especially on resource-limited devices. The recent development of 3D Gaussian
Splatting (3DGS) offers a promising approach for real-time dense 3D
reconstruction. However, existing 3DGS-based SLAM systems struggle to balance
hardware simplicity, speed, and map quality. Most systems excel in one or two
of the aforementioned aspects but rarely achieve all. A key issue is the
difficulty of initializing 3D Gaussians while concurrently conducting SLAM. To
address these challenges, we present Monocular GSO (MGSO), a novel real-time
SLAM system that integrates photometric SLAM with 3DGS. Photometric SLAM
provides dense structured point clouds for 3DGS initialization, accelerating
optimization and producing more efficient maps with fewer Gaussians. As a
result, experiments show that our system generates reconstructions with a
balance of quality, memory efficiency, and speed that outperforms the
state-of-the-art. Furthermore, our system achieves all results using RGB
inputs. We evaluate the Replica, TUM-RGBD, and EuRoC datasets against current
live dense reconstruction systems. Not only do we surpass contemporary systems,
but experiments also show that we maintain our performance on laptop hardware,
making it a practical solution for robotics, A/R, and other real-time
applications.

Comments:
- Paper Contribution to the ICRA 2025 Conference. Currently being
  reviewed

---

## EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-19 | Kunal Chelani, Assia Benbihi, Torsten Sattler, Fredrik Kahl | cs.CV | [PDF](http://arxiv.org/pdf/2409.12886v1){: .btn .btn-green } |

**Abstract**: With their meaningful geometry and their omnipresence in the 3D world, edges
are extremely useful primitives in computer vision. 3D edges comprise of lines
and curves, and methods to reconstruct them use either multi-view images or
point clouds as input. State-of-the-art image-based methods first learn a 3D
edge point cloud then fit 3D edges to it. The edge point cloud is obtained by
learning a 3D neural implicit edge field from which the 3D edge points are
sampled on a specific level set (0 or 1). However, such methods present two
important drawbacks: i) it is not realistic to sample points on exact level
sets due to float imprecision and training inaccuracies. Instead, they are
sampled within a range of levels so the points do not lie accurately on the 3D
edges and require further processing. ii) Such implicit representations are
computationally expensive and require long training times. In this paper, we
address these two limitations and propose a 3D edge mapping that is simpler,
more efficient, and preserves accuracy. Our method learns explicitly the 3D
edge points and their edge direction hence bypassing the need for point
sampling. It casts a 3D edge point as the center of a 3D Gaussian and the edge
direction as the principal axis of the Gaussian. Such a representation has the
advantage of being not only geometrically meaningful but also compatible with
the efficient training optimization defined in Gaussian Splatting. Results show
that the proposed method produces edges as accurate and complete as the
state-of-the-art while being an order of magnitude faster. Code is released at
https://github.com/kunalchelani/EdgeGaussians.



---

## Depth Estimation Based on 3D Gaussian Splatting Siamese Defocus

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-18 | Jinchang Zhang, Ningning Xu, Hao Zhang, Guoyu Lu | cs.CV | [PDF](http://arxiv.org/pdf/2409.12323v1){: .btn .btn-green } |

**Abstract**: Depth estimation is a fundamental task in 3D geometry. While stereo depth
estimation can be achieved through triangulation methods, it is not as
straightforward for monocular methods, which require the integration of global
and local information. The Depth from Defocus (DFD) method utilizes camera lens
models and parameters to recover depth information from blurred images and has
been proven to perform well. However, these methods rely on All-In-Focus (AIF)
images for depth estimation, which is nearly impossible to obtain in real-world
applications. To address this issue, we propose a self-supervised framework
based on 3D Gaussian splatting and Siamese networks. By learning the blur
levels at different focal distances of the same scene in the focal stack, the
framework predicts the defocus map and Circle of Confusion (CoC) from a single
defocused image, using the defocus map as input to DepthNet for monocular depth
estimation. The 3D Gaussian splatting model renders defocused images using the
predicted CoC, and the differences between these and the real defocused images
provide additional supervision signals for the Siamese Defocus self-supervised
network. This framework has been validated on both artificially synthesized and
real blurred datasets. Subsequent quantitative and visualization experiments
demonstrate that our proposed framework is highly effective as a DFD method.



---

## BRDF-NeRF: Neural Radiance Fields with Optical Satellite Images and BRDF  Modelling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-18 | Lulin Zhang, Ewelina Rupnik, Tri Dung Nguyen, St√©phane Jacquemoud, Yann Klinger | cs.CV | [PDF](http://arxiv.org/pdf/2409.12014v2){: .btn .btn-green } |

**Abstract**: Understanding the anisotropic reflectance of complex Earth surfaces from
satellite imagery is crucial for numerous applications. Neural radiance fields
(NeRF) have become popular as a machine learning technique capable of deducing
the bidirectional reflectance distribution function (BRDF) of a scene from
multiple images. However, prior research has largely concentrated on applying
NeRF to close-range imagery, estimating basic Microfacet BRDF models, which
fall short for many Earth surfaces. Moreover, high-quality NeRFs generally
require several images captured simultaneously, a rare occurrence in satellite
imaging. To address these limitations, we propose BRDF-NeRF, developed to
explicitly estimate the Rahman-Pinty-Verstraete (RPV) model, a semi-empirical
BRDF model commonly employed in remote sensing. We assess our approach using
two datasets: (1) Djibouti, captured in a single epoch at varying viewing
angles with a fixed Sun position, and (2) Lanzhou, captured over multiple
epochs with different viewing angles and Sun positions. Our results, based on
only three to four satellite images for training, demonstrate that BRDF-NeRF
can effectively synthesize novel views from directions far removed from the
training data and produce high-quality digital surface models (DSMs).



---

## Vista3D: Unravel the 3D Darkside of a Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-18 | Qiuhong Shen, Xingyi Yang, Michael Bi Mi, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.12193v1){: .btn .btn-green } |

**Abstract**: We embark on the age-old quest: unveiling the hidden dimensions of objects
from mere glimpses of their visible parts. To address this, we present Vista3D,
a framework that realizes swift and consistent 3D generation within a mere 5
minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase
and the fine phase. In the coarse phase, we rapidly generate initial geometry
with Gaussian Splatting from a single image. In the fine phase, we extract a
Signed Distance Function (SDF) directly from learned Gaussian Splatting,
optimizing it with a differentiable isosurface representation. Furthermore, it
elevates the quality of generation by using a disentangled representation with
two independent implicit functions to capture both visible and obscured aspects
of objects. Additionally, it harmonizes gradients from 2D diffusion prior with
3D-aware diffusion priors by angular diffusion prior composition. Through
extensive evaluation, we demonstrate that Vista3D effectively sustains a
balance between the consistency and diversity of the generated 3D objects.
Demos and code will be available at https://github.com/florinshen/Vista3D.

Comments:
- ECCV'2024

---

## Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian  Splatting Using 2D Masks

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-18 | Joji Joseph, Bharadwaj Amrutur, Shalabh Bhatnagar | cs.CV | [PDF](http://arxiv.org/pdf/2409.11681v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as a powerful 3D scene representation
technique, capturing fine details with high efficiency. In this paper, we
introduce a novel voting-based method that extends 2D segmentation models to 3D
Gaussian splats. Our approach leverages masked gradients, where gradients are
filtered by input 2D masks, and these gradients are used as votes to achieve
accurate segmentation. As a byproduct, we discovered that inference-time
gradients can also be used to prune Gaussians, resulting in up to 21%
compression. Additionally, we explore few-shot affordance transfer, allowing
annotations from 2D images to be effectively transferred onto 3D Gaussian
splats. The robust yet straightforward mathematical formulation underlying this
approach makes it a highly effective tool for numerous downstream applications,
such as augmented reality (AR), object editing, and robotics. The project code
and additional resources are available at
https://jojijoseph.github.io/3dgs-segmentation.

Comments:
- Preprint, Under review for ICRA 2025

---

## JEAN: Joint Expression and Audio-guided NeRF-based Talking Face  Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-18 | Sai Tanmay Reddy Chakkera, Aggelina Chatziagapi, Dimitris Samaras | cs.CV | [PDF](http://arxiv.org/pdf/2409.12156v1){: .btn .btn-green } |

**Abstract**: We introduce a novel method for joint expression and audio-guided talking
face generation. Recent approaches either struggle to preserve the speaker
identity or fail to produce faithful facial expressions. To address these
challenges, we propose a NeRF-based network. Since we train our network on
monocular videos without any ground truth, it is essential to learn
disentangled representations for audio and expression. We first learn audio
features in a self-supervised manner, given utterances from multiple subjects.
By incorporating a contrastive learning technique, we ensure that the learned
audio features are aligned to the lip motion and disentangled from the muscle
motion of the rest of the face. We then devise a transformer-based architecture
that learns expression features, capturing long-range facial expressions and
disentangling them from the speech-specific mouth movements. Through
quantitative and qualitative evaluation, we demonstrate that our method can
synthesize high-fidelity talking face videos, achieving state-of-the-art facial
expression transfer along with lip synchronization to unseen audio.

Comments:
- Accepted by BMVC 2024. Project Page:
  https://starc52.github.io/publications/2024-07-19-JEAN

---

## SRIF: Semantic Shape Registration Empowered by Diffusion-based Image  Morphing and Flow Estimation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-18 | Mingze Sun, Chen Guo, Puhua Jiang, Shiwei Mao, Yurun Chen, Ruqi Huang | cs.CV | [PDF](http://arxiv.org/pdf/2409.11682v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose SRIF, a novel Semantic shape Registration framework
based on diffusion-based Image morphing and Flow estimation. More concretely,
given a pair of extrinsically aligned shapes, we first render them from
multi-views, and then utilize an image interpolation framework based on
diffusion models to generate sequences of intermediate images between them. The
images are later fed into a dynamic 3D Gaussian splatting framework, with which
we reconstruct and post-process for intermediate point clouds respecting the
image morphing processing. In the end, tailored for the above, we propose a
novel registration module to estimate continuous normalizing flow, which
deforms source shape consistently towards the target, with intermediate point
clouds as weak guidance. Our key insight is to leverage large vision models
(LVMs) to associate shapes and therefore obtain much richer semantic
information on the relationship between shapes than the ad-hoc feature
extraction and alignment. As a consequence, SRIF achieves high-quality dense
correspondences on challenging shape pairs, but also delivers smooth,
semantically meaningful interpolation in between. Empirical evidence justifies
the effectiveness and superiority of our method as well as specific design
choices. The code is released at https://github.com/rqhuang88/SRIF.



---

## HGSLoc: 3DGS-based Heuristic Camera Pose Refinement

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-17 | Zhongyan Niu, Zhen Tan, Jinpu Zhang, Xueliang Yang, Dewen Hu | cs.CV | [PDF](http://arxiv.org/pdf/2409.10925v2){: .btn .btn-green } |

**Abstract**: Visual localization refers to the process of determining camera poses and
orientation within a known scene representation. This task is often complicated
by factors such as illumination changes and variations in viewing angles. In
this paper, we propose HGSLoc, a novel lightweight, plug and-play pose
optimization framework, which integrates 3D reconstruction with a heuristic
refinement strategy to achieve higher pose estimation accuracy. Specifically,
we introduce an explicit geometric map for 3D representation and high-fidelity
rendering, allowing the generation of high-quality synthesized views to support
accurate visual localization. Our method demonstrates a faster rendering speed
and higher localization accuracy compared to NeRF-based neural rendering
localization approaches. We introduce a heuristic refinement strategy, its
efficient optimization capability can quickly locate the target node, while we
set the step-level optimization step to enhance the pose accuracy in the
scenarios with small errors. With carefully designed heuristic functions, it
offers efficient optimization capabilities, enabling rapid error reduction in
rough localization estimations. Our method mitigates the dependence on complex
neural network models while demonstrating improved robustness against noise and
higher localization accuracy in challenging environments, as compared to neural
network joint optimization strategies. The optimization framework proposed in
this paper introduces novel approaches to visual localization by integrating
the advantages of 3D reconstruction and heuristic refinement strategy, which
demonstrates strong performance across multiple benchmark datasets, including
7Scenes and DB dataset.



---

## RenderWorld: World Model with Self-Supervised 3D Label

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-17 | Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Liu Haiyang, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Wang, Fabio Remondino, Yuexin Ma | cs.CV | [PDF](http://arxiv.org/pdf/2409.11356v1){: .btn .btn-green } |

**Abstract**: End-to-end autonomous driving with vision-only is not only more
cost-effective compared to LiDAR-vision fusion but also more reliable than
traditional methods. To achieve a economical and robust purely visual
autonomous driving system, we propose RenderWorld, a vision-only end-to-end
autonomous driving framework, which generates 3D occupancy labels using a
self-supervised gaussian-based Img2Occ Module, then encodes the labels by
AM-VAE, and uses world model for forecasting and planning. RenderWorld employs
Gaussian Splatting to represent 3D scenes and render 2D images greatly improves
segmentation accuracy and reduces GPU memory consumption compared with
NeRF-based methods. By applying AM-VAE to encode air and non-air separately,
RenderWorld achieves more fine-grained scene element representation, leading to
state-of-the-art performance in both 4D occupancy forecasting and motion
planning from autoregressive world model.



---

## GLC-SLAM: Gaussian Splatting SLAM with Efficient Loop Closure

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-17 | Ziheng Xu, Qingfeng Li, Chen Chen, Xuefeng Liu, Jianwei Niu | cs.RO | [PDF](http://arxiv.org/pdf/2409.10982v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for its
application in dense Simultaneous Localization and Mapping (SLAM), enabling
real-time rendering and high-fidelity mapping. However, existing 3DGS-based
SLAM methods often suffer from accumulated tracking errors and map drift,
particularly in large-scale environments. To address these issues, we introduce
GLC-SLAM, a Gaussian Splatting SLAM system that integrates global optimization
of camera poses and scene models. Our approach employs frame-to-model tracking
and triggers hierarchical loop closure using a global-to-local strategy to
minimize drift accumulation. By dividing the scene into 3D Gaussian submaps, we
facilitate efficient map updates following loop corrections in large scenes.
Additionally, our uncertainty-minimized keyframe selection strategy prioritizes
keyframes observing more valuable 3D Gaussians to enhance submap optimization.
Experimental results on various datasets demonstrate that GLC-SLAM achieves
superior or competitive tracking and mapping performance compared to
state-of-the-art dense RGB-D SLAM systems.



---

## SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-17 | Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Robert Maier, Federica Bogo, Tony Tung, Edmond Boyer | cs.CV | [PDF](http://arxiv.org/pdf/2409.11211v1){: .btn .btn-green } |

**Abstract**: Digitizing 3D static scenes and 4D dynamic events from multi-view images has
long been a challenge in computer vision and graphics. Recently, 3D Gaussian
Splatting (3DGS) has emerged as a practical and scalable reconstruction method,
gaining popularity due to its impressive reconstruction quality, real-time
rendering capabilities, and compatibility with widely used visualization tools.
However, the method requires a substantial number of input views to achieve
high-quality scene reconstruction, introducing a significant practical
bottleneck. This challenge is especially severe in capturing dynamic scenes,
where deploying an extensive camera array can be prohibitively costly. In this
work, we identify the lack of spatial autocorrelation of splat features as one
of the factors contributing to the suboptimal performance of the 3DGS technique
in sparse reconstruction settings. To address the issue, we propose an
optimization strategy that effectively regularizes splat features by modeling
them as the outputs of a corresponding implicit neural field. This results in a
consistent enhancement of reconstruction quality across various scenarios. Our
approach effectively handles static and dynamic cases, as demonstrated by
extensive testing across different setups and scene complexities.

Comments:
- ECCV 2024 paper. The project page and code are available at
  https://markomih.github.io/SplatFields/

---

## GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-17 | Yichen Zhang, Zihan Wang, Jiali Han, Peilin Li, Jiaxun Zhang, Jianqiang Wang, Lei He, Keqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2409.11307v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) integrates the strengths of primitive-based
representations and volumetric rendering techniques, enabling real-time,
high-quality rendering. However, 3DGS models typically overfit to single-scene
training and are highly sensitive to the initialization of Gaussian ellipsoids,
heuristically derived from Structure from Motion (SfM) point clouds, which
limits both generalization and practicality. To address these limitations, we
propose GS-Net, a generalizable, plug-and-play 3DGS module that densifies
Gaussian ellipsoids from sparse SfM point clouds, enhancing geometric structure
representation. To the best of our knowledge, GS-Net is the first plug-and-play
3DGS module with cross-scene generalization capabilities. Additionally, we
introduce the CARLA-NVS dataset, which incorporates additional camera
viewpoints to thoroughly evaluate reconstruction and rendering quality.
Extensive experiments demonstrate that applying GS-Net to 3DGS yields a PSNR
improvement of 2.08 dB for conventional viewpoints and 1.86 dB for novel
viewpoints, confirming the method's effectiveness and robustness.



---

## Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-16 | Euntae Choi, Sungjoo Yoo | cs.GR | [PDF](http://arxiv.org/pdf/2409.10335v1){: .btn .btn-green } |

**Abstract**: We propose two novel ideas (adoption of deferred rendering and mesh-based
representation) to improve the quality of 3D Gaussian splatting (3DGS) based
inverse rendering. We first report a problem incurred by hidden Gaussians,
where Gaussians beneath the surface adversely affect the pixel color in the
volume rendering adopted by the existing methods. In order to resolve the
problem, we propose applying deferred rendering and report new problems
incurred in a naive application of deferred rendering to the existing
3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based
inverse rendering under deferred rendering, we propose a novel two-step
training approach which (1) exploits mesh extraction and utilizes a hybrid
mesh-3DGS representation and (2) applies novel regularization methods to better
exploit the mesh. Our experiments show that, under relighting, the proposed
method offers significantly better rendering quality than the existing
3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based
inverse rendering method, it gives better rendering quality while offering
real-time rendering.

Comments:
- Under review

---

## Baking Relightable NeRF for Real-time Direct/Indirect Illumination  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-16 | Euntae Choi, Vincent Carpentier, Seunghun Shin, Sungjoo Yoo | cs.CV | [PDF](http://arxiv.org/pdf/2409.10327v1){: .btn .btn-green } |

**Abstract**: Relighting, which synthesizes a novel view under a given lighting condition
(unseen in training time), is a must feature for immersive photo-realistic
experience. However, real-time relighting is challenging due to high
computation cost of the rendering equation which requires shape and material
decomposition and visibility test to model shadow. Additionally, for indirect
illumination, additional computation of rendering equation on each secondary
surface point (where reflection occurs) is required rendering real-time
relighting challenging. We propose a novel method that executes a CNN renderer
to compute primary surface points and rendering parameters, required for direct
illumination. We also present a lightweight hash grid-based renderer, for
indirect illumination, which is recursively executed to perform the secondary
ray tracing process. Both renderers are trained in a distillation from a
pre-trained teacher model and provide real-time physically-based rendering
under unseen lighting condition at a negligible loss of rendering quality.

Comments:
- Under review

---

## BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-16 | Wugang Meng, Tianfu Wu, Huan Yin, Fumin Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2409.10216v1){: .btn .btn-green } |

**Abstract**: Image-goal navigation enables a robot to reach the location where a target
image was captured, using visual cues for guidance. However, current methods
either rely heavily on data and computationally expensive learning-based
approaches or lack efficiency in complex environments due to insufficient
exploration strategies. To address these limitations, we propose Bayesian
Embodied Image-goal Navigation Using Gaussian Splatting, a novel method that
formulates ImageNav as an optimal control problem within a model predictive
control framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to
predict future observations, enabling efficient, real-time navigation decisions
grounded in the robot's sensory experiences. By integrating Bayesian updates,
our method dynamically refines the robot's strategy without requiring extensive
prior experience or data. Our algorithm is validated through extensive
simulations and physical experiments, showcasing its potential for embodied
robot systems in visually complex scenarios.



---

## Adaptive Segmentation-Based Initialization for Steered Mixture of  Experts Image Regression


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-16 | Yi-Hsin Li, Sebastian Knorr, M√•rten Sj√∂str√∂m, Thomas Sikora | cs.CV | [PDF](http://arxiv.org/pdf/2409.10101v1){: .btn .btn-green } |

**Abstract**: Kernel image regression methods have shown to provide excellent efficiency in
many image processing task, such as image and light-field compression, Gaussian
Splatting, denoising and super-resolution. The estimation of parameters for
these methods frequently employ gradient descent iterative optimization, which
poses significant computational burden for many applications. In this paper, we
introduce a novel adaptive segmentation-based initialization method targeted
for optimizing Steered-Mixture-of Experts (SMoE) gating networks and
Radial-Basis-Function (RBF) networks with steering kernels. The novel
initialization method allocates kernels into pre-calculated image segments. The
optimal number of kernels, kernel positions, and steering parameters are
derived per segment in an iterative optimization and kernel sparsification
procedure. The kernel information from "local" segments is then transferred
into a "global" initialization, ready for use in iterative optimization of
SMoE, RBF, and related kernel image regression methods. Results show that
drastic objective and subjective quality improvements are achievable compared
to widely used regular grid initialization, "state-of-the-art" K-Means
initialization and previously introduced segmentation-based initialization
methods, while also drastically improving the sparsity of the regression
models. For same quality, the novel initialization results in models with
around 50% reduction of kernels. In addition, a significant reduction of
convergence time is achieved, with overall run-time savings of up to 50%. The
segmentation-based initialization strategy itself admits heavy parallel
computation; in theory, it may be divided into as many tasks as there are
segments in the images. By accessing only four parallel GPUs, run-time savings
of already 50% for initialization are achievable.



---

## SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-16 | Mohammad Nomaan Qureshi, Sparsh Garg, Francisco Yandun, David Held, George Kantor, Abhishesh Silwal | cs.RO | [PDF](http://arxiv.org/pdf/2409.10161v1){: .btn .btn-green } |

**Abstract**: Sim2Real transfer, particularly for manipulation policies relying on RGB
images, remains a critical challenge in robotics due to the significant domain
shift between synthetic and real-world visual data. In this paper, we propose
SplatSim, a novel framework that leverages Gaussian Splatting as the primary
rendering primitive to reduce the Sim2Real gap for RGB-based manipulation
policies. By replacing traditional mesh representations with Gaussian Splats in
simulators, SplatSim produces highly photorealistic synthetic data while
maintaining the scalability and cost-efficiency of simulation. We demonstrate
the effectiveness of our framework by training manipulation policies within
SplatSim}and deploying them in the real world in a zero-shot manner, achieving
an average success rate of 86.25%, compared to 97.5% for policies trained on
real-world data.



---

## DENSER: 3D Gaussians Splatting for Scene Reconstruction of Dynamic Urban  Environments

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-16 | Mahmud A. Mohamad, Gamal Elghazaly, Arthur Hubert, Raphael Frank | cs.CV | [PDF](http://arxiv.org/pdf/2409.10041v1){: .btn .btn-green } |

**Abstract**: This paper presents DENSER, an efficient and effective approach leveraging 3D
Gaussian splatting (3DGS) for the reconstruction of dynamic urban environments.
While several methods for photorealistic scene representations, both implicitly
using neural radiance fields (NeRF) and explicitly using 3DGS have shown
promising results in scene reconstruction of relatively complex dynamic scenes,
modeling the dynamic appearance of foreground objects tend to be challenging,
limiting the applicability of these methods to capture subtleties and details
of the scenes, especially far dynamic objects. To this end, we propose DENSER,
a framework that significantly enhances the representation of dynamic objects
and accurately models the appearance of dynamic objects in the driving scene.
Instead of directly using Spherical Harmonics (SH) to model the appearance of
dynamic objects, we introduce and integrate a new method aiming at dynamically
estimating SH bases using wavelets, resulting in better representation of
dynamic objects appearance in both space and time. Besides object appearance,
DENSER enhances object shape representation through densification of its point
cloud across multiple scene frames, resulting in faster convergence of model
training. Extensive evaluations on KITTI dataset show that the proposed
approach significantly outperforms state-of-the-art methods by a wide margin.
Source codes and models will be uploaded to this repository
https://github.com/sntubix/denser



---

## MesonGS: Post-training Compression of 3D Gaussians via Efficient  Attribute Transformation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-15 | Shuzhao Xie, Weixiang Zhang, Chen Tang, Yunpeng Bai, Rongwei Lu, Shijia Ge, Zhi Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.09756v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting demonstrates excellent quality and speed in novel view
synthesis. Nevertheless, the huge file size of the 3D Gaussians presents
challenges for transmission and storage. Current works design compact models to
replace the substantial volume and attributes of 3D Gaussians, along with
intensive training to distill information. These endeavors demand considerable
training time, presenting formidable hurdles for practical deployment. To this
end, we propose MesonGS, a codec for post-training compression of 3D Gaussians.
Initially, we introduce a measurement criterion that considers both
view-dependent and view-independent factors to assess the impact of each
Gaussian point on the rendering output, enabling the removal of insignificant
points. Subsequently, we decrease the entropy of attributes through two
transformations that complement subsequent entropy coding techniques to enhance
the file compression rate. More specifically, we first replace rotation
quaternions with Euler angles; then, we apply region adaptive hierarchical
transform to key attributes to reduce entropy. Lastly, we adopt finer-grained
quantization to avoid excessive information loss. Moreover, a well-crafted
finetune scheme is devised to restore quality. Extensive experiments
demonstrate that MesonGS significantly reduces the size of 3D Gaussians while
preserving competitive quality.

Comments:
- 18 pages, 8 figures, ECCV 2024

---

## NARF24: Estimating Articulated Object Structure for Implicit Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-15 | Stanley Lewis, Tom Gao, Odest Chadwicke Jenkins | cs.RO | [PDF](http://arxiv.org/pdf/2409.09829v1){: .btn .btn-green } |

**Abstract**: Articulated objects and their representations pose a difficult problem for
robots. These objects require not only representations of geometry and texture,
but also of the various connections and joint parameters that make up each
articulation. We propose a method that learns a common Neural Radiance Field
(NeRF) representation across a small number of collected scenes. This
representation is combined with a parts-based image segmentation to produce an
implicit space part localization, from which the connectivity and joint
parameters of the articulated object can be estimated, thus enabling
configuration-conditioned rendering.

Comments:
- extended abstract as submitted to ICRA@40 anniversary conference

---

## SAFER-Splat: A Control Barrier Function for Safe Navigation with Online  Gaussian Splatting Maps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-15 | Timothy Chen, Aiden Swann, Javier Yu, Ola Shorinwa, Riku Murai, Monroe Kennedy III, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2409.09868v1){: .btn .btn-green } |

**Abstract**: SAFER-Splat (Simultaneous Action Filtering and Environment Reconstruction) is
a real-time, scalable, and minimally invasive action filter, based on control
barrier functions, for safe robotic navigation in a detailed map constructed at
runtime using Gaussian Splatting (GSplat). We propose a novel Control Barrier
Function (CBF) that not only induces safety with respect to all Gaussian
primitives in the scene, but when synthesized into a controller, is capable of
processing hundreds of thousands of Gaussians while maintaining a minimal
memory footprint and operating at 15 Hz during online Splat training. Of the
total compute time, a small fraction of it consumes GPU resources, enabling
uninterrupted training. The safety layer is minimally invasive, correcting
robot actions only when they are unsafe. To showcase the safety filter, we also
introduce SplatBridge, an open-source software package built with ROS for
real-time GSplat mapping for robots. We demonstrate the safety and robustness
of our pipeline first in simulation, where our method is 20-50x faster, safer,
and less conservative than competing methods based on neural radiance fields.
Further, we demonstrate simultaneous GSplat mapping and safety filtering on a
drone hardware platform using only on-board perception. We verify that under
teleoperation a human pilot cannot invoke a collision. Our videos and codebase
can be found at https://chengine.github.io/safer-splat.



---

## GEVO: Memory-Efficient Monocular Visual Odometry Using Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-14 | Dasong Gao, Peter Zhi Xuan Li, Vivienne Sze, Sertac Karaman | cs.RO | [PDF](http://arxiv.org/pdf/2409.09295v1){: .btn .btn-green } |

**Abstract**: Constructing a high-fidelity representation of the 3D scene using a monocular
camera can enable a wide range of applications on mobile devices, such as
micro-robots, smartphones, and AR/VR headsets. On these devices, memory is
often limited in capacity and its access often dominates the consumption of
compute energy. Although Gaussian Splatting (GS) allows for high-fidelity
reconstruction of 3D scenes, current GS-based SLAM is not memory efficient as a
large number of past images is stored to retrain Gaussians for reducing
catastrophic forgetting. These images often require two-orders-of-magnitude
higher memory than the map itself and thus dominate the total memory usage. In
this work, we present GEVO, a GS-based monocular SLAM framework that achieves
comparable fidelity as prior methods by rendering (instead of storing) them
from the existing map. Novel Gaussian initialization and optimization
techniques are proposed to remove artifacts from the map and delay the
degradation of the rendered images over time. Across a variety of environments,
GEVO achieves comparable map fidelity while reducing the memory overhead to
around 58 MBs, which is up to 94x lower than prior works.

Comments:
- 8 pages

---

## CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-13 | Runze Chen, Mingyu Xiao, Haiyong Luo, Fang Zhao, Fan Wu, Hao Xiong, Qi Liu, Meng Song | cs.CV | [PDF](http://arxiv.org/pdf/2409.08562v1){: .btn .btn-green } |

**Abstract**: We introduce Crowd-Sourced Splatting (CSS), a novel 3D Gaussian Splatting
(3DGS) pipeline designed to overcome the challenges of pose-free scene
reconstruction using crowd-sourced imagery. The dream of reconstructing
historically significant but inaccessible scenes from collections of
photographs has long captivated researchers. However, traditional 3D techniques
struggle with missing camera poses, limited viewpoints, and inconsistent
lighting. CSS addresses these challenges through robust geometric priors and
advanced illumination modeling, enabling high-quality novel view synthesis
under complex, real-world conditions. Our method demonstrates clear
improvements over existing approaches, paving the way for more accurate and
flexible applications in AR, VR, and large-scale 3D reconstruction.



---

## AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-13 | Xinzhe Wang, Ran Yi, Lizhuang Ma | cs.CV | [PDF](http://arxiv.org/pdf/2409.08669v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a recent explicit 3D representation that has
achieved high-quality reconstruction and real-time rendering of complex scenes.
However, the rasterization pipeline still suffers from unnecessary overhead
resulting from avoidable serial Gaussian culling, and uneven load due to the
distinct number of Gaussian to be rendered across pixels, which hinders wider
promotion and application of 3DGS. In order to accelerate Gaussian splatting,
we propose AdR-Gaussian, which moves part of serial culling in Render stage
into the earlier Preprocess stage to enable parallel culling, employing
adaptive radius to narrow the rendering pixel range for each Gaussian, and
introduces a load balancing method to minimize thread waiting time during the
pixel-parallel rendering. Our contributions are threefold, achieving a
rendering speed of 310% while maintaining equivalent or even better quality
than the state-of-the-art. Firstly, we propose to early cull Gaussian-Tile
pairs of low splatting opacity based on an adaptive radius in the
Gaussian-parallel Preprocess stage, which reduces the number of affected tile
through the Gaussian bounding circle, thus reducing unnecessary overhead and
achieving faster rendering speed. Secondly, we further propose early culling
based on axis-aligned bounding box for Gaussian splatting, which achieves a
more significant reduction in ineffective expenses by accurately calculating
the Gaussian size in the 2D directions. Thirdly, we propose a balancing
algorithm for pixel thread load, which compresses the information of heavy-load
pixels to reduce thread waiting time, and enhance information of light-load
pixels to hedge against rendering quality loss. Experiments on three datasets
demonstrate that our algorithm can significantly improve the Gaussian Splatting
rendering speed.

Comments:
- SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers '24),
  December 03-06, 2024, Tokyo, Japan

---

## Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse  Viewpoints

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-13 | Shan Chen, Jiale Zhou, Lei Li | cs.CV | [PDF](http://arxiv.org/pdf/2409.08613v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in scene
synthesis and novel view synthesis tasks. Typically, the initialization of 3D
Gaussian primitives relies on point clouds derived from Structure-from-Motion
(SfM) methods. However, in scenarios requiring scene reconstruction from sparse
viewpoints, the effectiveness of 3DGS is significantly constrained by the
quality of these initial point clouds and the limited number of input images.
In this study, we present Dust-GS, a novel framework specifically designed to
overcome the limitations of 3DGS in sparse viewpoint conditions. Instead of
relying solely on SfM, Dust-GS introduces an innovative point cloud
initialization technique that remains effective even with sparse input data.
Our approach leverages a hybrid strategy that integrates an adaptive
depth-based masking technique, thereby enhancing the accuracy and detail of
reconstructed scenes. Extensive experiments conducted on several benchmark
datasets demonstrate that Dust-GS surpasses traditional 3DGS methods in
scenarios with sparse viewpoints, achieving superior scene reconstruction
quality with a reduced number of input images.



---

## A Diffusion Approach to Radiance Field Relighting using  Multi-Illumination Synthesis


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-13 | Yohan Poirier-Ginter, Alban Gauthier, Julien Philip, Jean-Francois Lalonde, George Drettakis | cs.CV | [PDF](http://arxiv.org/pdf/2409.08947v2){: .btn .btn-green } |

**Abstract**: Relighting radiance fields is severely underconstrained for multi-view data,
which is most often captured under a single illumination condition; It is
especially hard for full scenes containing multiple objects. We introduce a
method to create relightable radiance fields using such single-illumination
data by exploiting priors extracted from 2D image diffusion models. We first
fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by
light direction, allowing us to augment a single-illumination capture into a
realistic -- but possibly inconsistent -- multi-illumination dataset from
directly defined light directions. We use this augmented data to create a
relightable radiance field represented by 3D Gaussian splats. To allow direct
control of light direction for low-frequency lighting, we represent appearance
with a multi-layer perceptron parameterized on light direction. To enforce
multi-view consistency and overcome inaccuracies we optimize a per-image
auxiliary feature vector. We show results on synthetic and real multi-view data
under single illumination, demonstrating that our method successfully exploits
2D diffusion model priors to allow realistic 3D relighting for complete scenes.
Project site
https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/

Comments:
- Project site
  https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/

---

## FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Qiuhong Shen, Xingyi Yang, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.08270v1){: .btn .btn-green } |

**Abstract**: This study addresses the challenge of accurately segmenting 3D Gaussian
Splatting from 2D masks. Conventional methods often rely on iterative gradient
descent to assign each Gaussian a unique label, leading to lengthy optimization
and sub-optimal solutions. Instead, we propose a straightforward yet globally
optimal solver for 3D-GS segmentation. The core insight of our method is that,
with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially
a linear function with respect to the labels of each Gaussian. As such, the
optimal label assignment can be solved via linear programming in closed form.
This solution capitalizes on the alpha blending characteristic of the splatting
process for single step optimization. By incorporating the background bias in
our objective function, our method shows superior robustness in 3D segmentation
against noises. Remarkably, our optimization completes within 30 seconds, about
50$\times$ faster than the best existing methods. Extensive experiments
demonstrate the efficiency and robustness of our method in segmenting various
scenes, and its superior performance in downstream tasks such as object removal
and inpainting. Demos and code will be available at
https://github.com/florinshen/FlashSplat.

Comments:
- ECCV'2024

---

## DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with  Diffusion Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Thomas Hanwen Zhu, Ruining Li, Tomas Jakab | cs.CV | [PDF](http://arxiv.org/pdf/2409.08278v1){: .btn .btn-green } |

**Abstract**: We present DreamHOI, a novel method for zero-shot synthesis of human-object
interactions (HOIs), enabling a 3D human model to realistically interact with
any given object based on a textual description. This task is complicated by
the varying categories and geometries of real-world objects and the scarcity of
datasets encompassing diverse HOIs. To circumvent the need for extensive data,
we leverage text-to-image diffusion models trained on billions of image-caption
pairs. We optimize the articulation of a skinned human mesh using Score
Distillation Sampling (SDS) gradients obtained from these models, which predict
image-space edits. However, directly backpropagating image-space gradients into
complex articulation parameters is ineffective due to the local nature of such
gradients. To overcome this, we introduce a dual implicit-explicit
representation of a skinned mesh, combining (implicit) neural radiance fields
(NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization,
we transition between implicit and explicit forms, grounding the NeRF
generation while refining the mesh articulation. We validate our approach
through extensive experiments, demonstrating its effectiveness in generating
realistic HOIs.

Comments:
- Project page: https://DreamHOI.github.io/

---

## Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared  Novel-view Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Qian Chen, Shihao Shu, Xiangzhi Bai | cs.CV | [PDF](http://arxiv.org/pdf/2409.08042v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis based on visible light has been extensively studied. In
comparison to visible light imaging, thermal infrared imaging offers the
advantage of all-weather imaging and strong penetration, providing increased
possibilities for reconstruction in nighttime and adverse weather scenarios.
However, thermal infrared imaging is influenced by physical characteristics
such as atmospheric transmission effects and thermal conduction, hindering the
precise reconstruction of intricate details in thermal infrared scenes,
manifesting as issues of floaters and indistinct edge features in synthesized
images. To address these limitations, this paper introduces a physics-induced
3D Gaussian splatting method named Thermal3D-GS. Thermal3D-GS begins by
modeling atmospheric transmission effects and thermal conduction in
three-dimensional media using neural networks. Additionally, a temperature
consistency constraint is incorporated into the optimization objective to
enhance the reconstruction accuracy of thermal infrared images. Furthermore, to
validate the effectiveness of our method, the first large-scale benchmark
dataset for this field named Thermal Infrared Novel-view Synthesis Dataset
(TI-NSD) is created. This dataset comprises 20 authentic thermal infrared video
scenes, covering indoor, outdoor, and UAV(Unmanned Aerial Vehicle) scenarios,
totaling 6,664 frames of thermal infrared image data. Based on this dataset,
this paper experimentally verifies the effectiveness of Thermal3D-GS. The
results indicate that our method outperforms the baseline method with a 3.03 dB
improvement in PSNR and significantly addresses the issues of floaters and
indistinct edge features present in the baseline method. Our dataset and
codebase will be released in
\href{https://github.com/mzzcdf/Thermal3DGS}{\textcolor{red}{Thermal3DGS}}.

Comments:
- 17 pages, 4 figures, 3 tables

---

## Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Yuheng Jiang, Zhehao Shen, Yu Hong, Chengcheng Guo, Yize Wu, Yingliang Zhang, Jingyi Yu, Lan Xu | cs.GR | [PDF](http://arxiv.org/pdf/2409.08353v1){: .btn .btn-green } |

**Abstract**: Volumetric video represents a transformative advancement in visual media,
enabling users to freely navigate immersive virtual experiences and narrowing
the gap between digital and real worlds. However, the need for extensive manual
intervention to stabilize mesh sequences and the generation of excessively
large assets in existing workflows impedes broader adoption. In this paper, we
present a novel Gaussian-based approach, dubbed \textit{DualGS}, for real-time
and high-fidelity playback of complex human performance with excellent
compression ratios. Our key idea in DualGS is to separately represent motion
and appearance using the corresponding skin and joint Gaussians. Such an
explicit disentanglement can significantly reduce motion redundancy and enhance
temporal coherence. We begin by initializing the DualGS and anchoring skin
Gaussians to joint Gaussians at the first frame. Subsequently, we employ a
coarse-to-fine training strategy for frame-by-frame human performance modeling.
It includes a coarse alignment phase for overall motion prediction as well as a
fine-grained optimization for robust tracking and high-fidelity rendering. To
integrate volumetric video seamlessly into VR environments, we efficiently
compress motion using entropy encoding and appearance using codec compression
coupled with a persistent codebook. Our approach achieves a compression ratio
of up to 120 times, only requiring approximately 350KB of storage per frame. We
demonstrate the efficacy of our representation through photo-realistic,
free-view experiences on VR headsets, enabling users to immersively watch
musicians in performance and feel the rhythm of the notes at the performers'
fingertips.

Comments:
- Accepted at SIGGRAPH Asia 2024. Project page:
  https://nowheretrix.github.io/DualGS/

---

## SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming  with Arbitrary Length

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Bangya Liu, Suman Banerjee | cs.MM | [PDF](http://arxiv.org/pdf/2409.07759v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant
attention in computer vision and computer graphics due to its high rendering
speed and remarkable quality. While extant research has endeavored to extend
the application of 3DGS from static to dynamic scenes, such efforts have been
consistently impeded by excessive model sizes, constraints on video duration,
and content deviation. These limitations significantly compromise the
streamability of dynamic 3D Gaussian models, thereby restricting their utility
in downstream applications, including volumetric video, autonomous vehicle, and
immersive technologies such as virtual, augmented, and mixed reality.
  This paper introduces SwinGS, a novel framework for training, delivering, and
rendering volumetric video in a real-time streaming fashion. To address the
aforementioned challenges and enhance streamability, SwinGS integrates
spacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to
fit various 3D scenes across frames, in the meantime employing a sliding window
captures Gaussian snapshots for each frame in an accumulative way. We implement
a prototype of SwinGS and demonstrate its streamability across various datasets
and scenes. Additionally, we develop an interactive WebGL viewer enabling
real-time volumetric video playback on most devices with modern browsers,
including smartphones and tablets. Experimental results show that SwinGS
reduces transmission costs by 83.6% compared to previous work with ignorable
compromise in PSNR. Moreover, SwinGS easily scales to long video sequences
without compromising quality.



---

## ThermalGaussian: Thermal 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Rongfeng Lu, Hangyu Chen, Zunjie Zhu, Yuhang Qin, Ming Lu, Le Zhang, Chenggang Yan, Anke Xue | cs.CV | [PDF](http://arxiv.org/pdf/2409.07200v1){: .btn .btn-green } |

**Abstract**: Thermography is especially valuable for the military and other users of
surveillance cameras. Some recent methods based on Neural Radiance Fields
(NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of
thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS)
prevails due to its rapid training and real-time rendering. In this work, we
propose ThermalGaussian, the first thermal 3DGS approach capable of rendering
high-quality images in RGB and thermal modalities. We first calibrate the RGB
camera and the thermal camera to ensure that both modalities are accurately
aligned. Subsequently, we use the registered images to learn the multimodal 3D
Gaussians. To prevent the overfitting of any single modality, we introduce
several multimodal regularization constraints. We also develop smoothing
constraints tailored to the physical characteristics of the thermal modality.
Besides, we contribute a real-world dataset named RGBT-Scenes, captured by a
hand-hold thermal-infrared camera, facilitating future research on thermal
scene reconstruction. We conduct comprehensive experiments to show that
ThermalGaussian achieves photorealistic rendering of thermal images and
improves the rendering quality of RGB images. With the proposed multimodal
regularization constraints, we also reduced the model's storage cost by 90\%.
The code and dataset will be released.

Comments:
- 10 pages, 7 figures

---

## Instant Facial Gaussians Translator for Relightable and Interactable  Facial Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Dafei Qin, Hongyang Lin, Qixuan Zhang, Kaichun Qiao, Longwen Zhang, Zijun Zhao, Jun Saito, Jingyi Yu, Lan Xu, Taku Komura | cs.GR | [PDF](http://arxiv.org/pdf/2409.07441v1){: .btn .btn-green } |

**Abstract**: We propose GauFace, a novel Gaussian Splatting representation, tailored for
efficient animation and rendering of physically-based facial assets. Leveraging
strong geometric priors and constrained optimization, GauFace ensures a neat
and structured Gaussian representation, delivering high fidelity and real-time
facial interaction of 30fps@1440p on a Snapdragon 8 Gen 2 mobile platform.
  Then, we introduce TransGS, a diffusion transformer that instantly translates
physically-based facial assets into the corresponding GauFace representations.
Specifically, we adopt a patch-based pipeline to handle the vast number of
Gaussians effectively. We also introduce a novel pixel-aligned sampling scheme
with UV positional encoding to ensure the throughput and rendering quality of
GauFace assets generated by our TransGS. Once trained, TransGS can instantly
translate facial assets with lighting conditions to GauFace representation,
With the rich conditioning modalities, it also enables editing and animation
capabilities reminiscent of traditional CG pipelines.
  We conduct extensive evaluations and user studies, compared to traditional
offline and online renderers, as well as recent neural rendering methods, which
demonstrate the superior performance of our approach for facial asset
rendering. We also showcase diverse immersive applications of facial assets
using our TransGS approach and GauFace representation, across various platforms
like PCs, phones and even VR headsets.

Comments:
- Project Page: https://dafei-qin.github.io/TransGS.github.io/

---

## Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting  Networks


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Ruihan Xu, Anthony Opipari, Joshua Mah, Stanley Lewis, Haoran Zhang, Hanzhe Guo, Odest Chadwicke Jenkins | cs.CV | [PDF](http://arxiv.org/pdf/2409.07245v1){: .btn .btn-green } |

**Abstract**: This paper introduces SO(2)-Equivariant Gaussian Sculpting Networks (GSNs) as
an approach for SO(2)-Equivariant 3D object reconstruction from single-view
image observations.
  GSNs take a single observation as input to generate a Gaussian splat
representation describing the observed object's geometry and texture. By using
a shared feature extractor before decoding Gaussian colors, covariances,
positions, and opacities, GSNs achieve extremely high throughput (>150FPS).
Experiments demonstrate that GSNs can be trained efficiently using a multi-view
rendering loss and are competitive, in quality, with expensive diffusion-based
reconstruction algorithms. The GSN model is validated on multiple benchmark
experiments. Moreover, we demonstrate the potential for GSNs to be used within
a robotic manipulation pipeline for object-centric grasping.

Comments:
- Accepted to RSS 2024 Workshop on Geometric and Algebraic Structure in
  Robot Learning

---

## DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for  Text-to-3D Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Zuxuan Wu, Yu-Gang Jiang, Tao Mei | cs.CV | [PDF](http://arxiv.org/pdf/2409.07454v1){: .btn .btn-green } |

**Abstract**: Learning radiance fields (NeRF) with powerful 2D diffusion models has
garnered popularity for text-to-3D generation. Nevertheless, the implicit 3D
representations of NeRF lack explicit modeling of meshes and textures over
surfaces, and such surface-undefined way may suffer from the issues, e.g.,
noisy surfaces with ambiguous texture details or cross-view inconsistency. To
alleviate this, we present DreamMesh, a novel text-to-3D architecture that
pivots on well-defined surfaces (triangle meshes) to generate high-fidelity
explicit 3D model. Technically, DreamMesh capitalizes on a distinctive
coarse-to-fine scheme. In the coarse stage, the mesh is first deformed by
text-guided Jacobians and then DreamMesh textures the mesh with an interlaced
use of 2D diffusion models in a tuning free manner from multiple viewpoints. In
the fine stage, DreamMesh jointly manipulates the mesh and refines the texture
map, leading to high-quality triangle meshes with high-fidelity textured
materials. Extensive experiments demonstrate that DreamMesh significantly
outperforms state-of-the-art text-to-3D methods in faithfully generating 3D
content with richer textual details and enhanced geometry. Our project page is
available at https://dreammesh.github.io.

Comments:
- ECCV 2024. Project page is available at
  \url{https://dreammesh.github.io}

---

## Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered  Stereo Pairs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Sadra Safadoust, Fabio Tosi, Fatma G√ºney, Matteo Poggi | cs.CV | [PDF](http://arxiv.org/pdf/2409.07456v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (GS) significantly struggles to accurately represent
the underlying 3D scene geometry, resulting in inaccuracies and floating
artifacts when rendering depth maps. In this paper, we address this limitation,
undertaking a comprehensive analysis of the integration of depth priors
throughout the optimization process of Gaussian primitives, and present a novel
strategy for this purpose. This latter dynamically exploits depth cues from a
readily available stereo network, processing virtual stereo pairs rendered by
the GS model itself during training and achieving consistent self-improvement
of the scene representation. Experimental results on three popular datasets,
breaking ground as the first to assess depth accuracy for these models,
validate our findings.

Comments:
- BMVC 2024. Project page: https://kuis-ai.github.io/StereoGS/

---

## Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video  Diffusion Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Chong-Wah Ngo, Tao Mei | cs.CV | [PDF](http://arxiv.org/pdf/2409.07452v1){: .btn .btn-green } |

**Abstract**: Despite having tremendous progress in image-to-3D generation, existing
methods still struggle to produce multi-view consistent images with
high-resolution textures in detail, especially in the paradigm of 2D diffusion
that lacks 3D awareness. In this work, we present High-resolution Image-to-3D
model (Hi3D), a new video diffusion based paradigm that redefines a single
image to multi-view images as 3D-aware sequential image generation (i.e.,
orbital video generation). This methodology delves into the underlying temporal
consistency knowledge in video diffusion model that generalizes well to
geometry consistency across multiple views in 3D generation. Technically, Hi3D
first empowers the pre-trained video diffusion model with 3D-aware prior
(camera pose condition), yielding multi-view images with low-resolution texture
details. A 3D-aware video-to-video refiner is learnt to further scale up the
multi-view images with high-resolution texture details. Such high-resolution
multi-view images are further augmented with novel views through 3D Gaussian
Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D
reconstruction. Extensive experiments on both novel view synthesis and single
view reconstruction demonstrate that our Hi3D manages to produce superior
multi-view consistency images with highly-detailed textures. Source code and
data are available at \url{https://github.com/yanghb22-fdu/Hi3D-Official}.

Comments:
- ACM Multimedia 2024. Source code is available at
  \url{https://github.com/yanghb22-fdu/Hi3D-Official}

---

## gsplat: An Open-Source Library for Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-10 | Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2409.06765v1){: .btn .btn-green } |

**Abstract**: gsplat is an open-source library designed for training and developing
Gaussian Splatting methods. It features a front-end with Python bindings
compatible with the PyTorch library and a back-end with highly optimized CUDA
kernels. gsplat offers numerous features that enhance the optimization of
Gaussian Splatting models, which include optimization improvements for speed,
memory, and convergence times. Experimental results demonstrate that gsplat
achieves up to 10% less training time and 4x less memory than the original
implementation. Utilized in several research projects, gsplat is actively
maintained on GitHub. Source code is available at
https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We
welcome contributions from the open-source community.

Comments:
- 17 pages, 2 figures, JMLR MLOSS

---

## Sources of Uncertainty in 3D Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-10 | Marcus Klasson, Riccardo Mereu, Juho Kannala, Arno Solin | cs.CV | [PDF](http://arxiv.org/pdf/2409.06407v1){: .btn .btn-green } |

**Abstract**: The process of 3D scene reconstruction can be affected by numerous
uncertainty sources in real-world scenes. While Neural Radiance Fields (NeRFs)
and 3D Gaussian Splatting (GS) achieve high-fidelity rendering, they lack
built-in mechanisms to directly address or quantify uncertainties arising from
the presence of noise, occlusions, confounding outliers, and imprecise camera
pose inputs. In this paper, we introduce a taxonomy that categorizes different
sources of uncertainty inherent in these methods. Moreover, we extend NeRF- and
GS-based methods with uncertainty estimation techniques, including learning
uncertainty outputs and ensembles, and perform an empirical study to assess
their ability to capture the sensitivity of the reconstruction. Our study
highlights the need for addressing various uncertainty aspects when designing
NeRF/GS-based methods for uncertainty-aware 3D reconstruction.

Comments:
- To appear in ECCV 2024 Workshop Proceedings. Project page at
  https://aaltoml.github.io/uncertainty-nerf-gs/

---

## GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-10 | Junyi Chen, Weicai Ye, Yifan Wang, Danpeng Chen, Di Huang, Wanli Ouyang, Guofeng Zhang, Yu Qiao, Tong He | cs.CV | [PDF](http://arxiv.org/pdf/2409.06685v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown promising performance in novel view
synthesis. Previous methods adapt it to obtaining surfaces of either individual
3D objects or within limited scenes. In this paper, we make the first attempt
to tackle the challenging task of large-scale scene surface reconstruction.
This task is particularly difficult due to the high GPU memory consumption,
different levels of details for geometric representation, and noticeable
inconsistencies in appearance. To this end, we propose GigaGS, the first work
for high-quality surface reconstruction for large-scale scenes using 3DGS.
GigaGS first applies a partitioning strategy based on the mutual visibility of
spatial regions, which effectively grouping cameras for parallel processing. To
enhance the quality of the surface, we also propose novel multi-view
photometric and geometric consistency constraints based on Level-of-Detail
representation. In doing so, our method can reconstruct detailed surface
structures. Comprehensive experiments are conducted on various datasets. The
consistent improvement demonstrates the superiority of GigaGS.



---

## LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-10 | Archana Swaminathan, Anubhav Gupta, Kamal Gupta, Shishira R. Maiya, Vatsal Agarwal, Abhinav Shrivastava | cs.CV | [PDF](http://arxiv.org/pdf/2409.06703v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of
static scenes and objects in 3D, offering unprecedented quality. However,
extending NeRFs to model dynamic objects or object articulations remains a
challenging problem. Previous works have tackled this issue by focusing on
part-level reconstruction and motion estimation for objects, but they often
rely on heuristics regarding the number of moving parts or object categories,
which can limit their practical use. In this work, we introduce LEIA, a novel
approach for representing dynamic 3D objects. Our method involves observing the
object at distinct time steps or "states" and conditioning a hypernetwork on
the current state, using this to parameterize our NeRF. This approach allows us
to learn a view-invariant latent representation for each state. We further
demonstrate that by interpolating between these states, we can generate novel
articulation configurations in 3D space that were previously unseen. Our
experimental results highlight the effectiveness of our method in articulating
objects in a manner that is independent of the viewing angle and joint
configuration. Notably, our approach outperforms previous methods that rely on
motion information for articulation registration.

Comments:
- Accepted to ECCV 2024. Project Website at
  https://archana1998.github.io/leia/

---

## GASP: Gaussian Splatting for Physic-Based Simulations

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Piotr Borycki, Weronika Smolak, Joanna Waczy≈Ñska, Marcin Mazur, S≈Çawomir Tadeja, Przemys≈Çaw Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2409.05819v1){: .btn .btn-green } |

**Abstract**: Physics simulation is paramount for modeling and utilization of 3D scenes in
various real-world applications. However, its integration with state-of-the-art
3D scene rendering techniques such as Gaussian Splatting (GS) remains
challenging. Existing models use additional meshing mechanisms, including
triangle or tetrahedron meshing, marching cubes, or cage meshes. As an
alternative, we can modify the physics grounded Newtonian dynamics to align
with 3D Gaussian components. Current models take the first-order approximation
of a deformation map, which locally approximates the dynamics by linear
transformations. In contrast, our Gaussian Splatting for Physics-Based
Simulations (GASP) model uses such a map (without any modifications) and flat
Gaussian distributions, which are parameterized by three points (mesh faces).
Subsequently, each 3D point (mesh face node) is treated as a discrete entity
within a 3D space. Consequently, the problem of modeling Gaussian components is
reduced to working with 3D points. Additionally, the information on mesh faces
can be used to incorporate further properties into the physics model,
facilitating the use of triangles. Resulting solution can be integrated into
any physics engine that can be treated as a black box. As demonstrated in our
studies, the proposed model exhibits superior performance on a diverse range of
benchmark datasets designed for 3D object rendering.



---

## LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance  Fields with RGB-Event Stereo

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Wei Zhi Tang, Daniel Rebain, Kostantinos G. Derpanis, Kwang Moo Yi | cs.CV | [PDF](http://arxiv.org/pdf/2409.06104v1){: .btn .btn-green } |

**Abstract**: We present a method for reconstructing a clear Neural Radiance Field (NeRF)
even with fast camera motions. To address blur artifacts, we leverage both
(blurry) RGB images and event camera data captured in a binocular
configuration. Importantly, when reconstructing our clear NeRF, we consider the
camera modeling imperfections that arise from the simple pinhole camera model
as learned embeddings for each camera measurement, and further learn a mapper
that connects event camera measurements with RGB data. As no previous dataset
exists for our binocular setting, we introduce an event camera dataset with
captures from a 3D-printed stereo configuration between RGB and event cameras.
Empirically, we evaluate our introduced dataset and EVIMOv2 and show that our
method leads to improved reconstructions. Our code and dataset are available at
https://github.com/ubc-vision/LSENeRF.



---

## KRONC: Keypoint-based Robust Camera Optimization for 3D Car  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Davide Di Nucci, Alessandro Simoni, Matteo Tomei, Luca Ciuffreda, Roberto Vezzani, Rita Cucchiara | cs.CV | [PDF](http://arxiv.org/pdf/2409.05407v1){: .btn .btn-green } |

**Abstract**: The three-dimensional representation of objects or scenes starting from a set
of images has been a widely discussed topic for years and has gained additional
attention after the diffusion of NeRF-based approaches. However, an
underestimated prerequisite is the knowledge of camera poses or, more
specifically, the estimation of the extrinsic calibration parameters. Although
excellent general-purpose Structure-from-Motion methods are available as a
pre-processing step, their computational load is high and they require a lot of
frames to guarantee sufficient overlapping among the views. This paper
introduces KRONC, a novel approach aimed at inferring view poses by leveraging
prior knowledge about the object to reconstruct and its representation through
semantic keypoints. With a focus on vehicle scenes, KRONC is able to estimate
the position of the views as a solution to a light optimization problem
targeting the convergence of keypoints' back-projections to a singular point.
To validate the method, a specific dataset of real-world car scenes has been
collected. Experiments confirm KRONC's ability to generate excellent estimates
of camera poses starting from very coarse initialization. Results are
comparable with Structure-from-Motion methods with huge savings in computation.
Code and data will be made publicly available.

Comments:
- Accepted at ECCVW

---

## Online 3D reconstruction and dense tracking in endoscopic videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Michel Hayoz, Christopher Hahne, Thomas Kurmann, Max Allan, Guido Beldi, Daniel Candinas, ablo M√°rquez-Neila, Raphael Sznitman | cs.CV | [PDF](http://arxiv.org/pdf/2409.06037v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction from stereo endoscopic video data is crucial for
advancing surgical interventions. In this work, we present an online framework
for online, dense 3D scene reconstruction and tracking, aimed at enhancing
surgical scene understanding and assisting interventions. Our method
dynamically extends a canonical scene representation using Gaussian splatting,
while modeling tissue deformations through a sparse set of control points. We
introduce an efficient online fitting algorithm that optimizes the scene
parameters, enabling consistent tracking and accurate reconstruction. Through
experiments on the StereoMIS dataset, we demonstrate the effectiveness of our
approach, outperforming state-of-the-art tracking methods and achieving
comparable performance to offline reconstruction techniques. Our work enables
various downstream applications thus contributing to advancing the capabilities
of surgical assistance systems.



---

## From Words to Poses: Enhancing Novel Object Pose Estimation with Vision  Language Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Tessa Pulli, Stefan Thalhammer, Simon Schwaiger, Markus Vincze | cs.CV | [PDF](http://arxiv.org/pdf/2409.05413v1){: .btn .btn-green } |

**Abstract**: Robots are increasingly envisioned to interact in real-world scenarios, where
they must continuously adapt to new situations. To detect and grasp novel
objects, zero-shot pose estimators determine poses without prior knowledge.
Recently, vision language models (VLMs) have shown considerable advances in
robotics applications by establishing an understanding between language input
and image input. In our work, we take advantage of VLMs zero-shot capabilities
and translate this ability to 6D object pose estimation. We propose a novel
framework for promptable zero-shot 6D object pose estimation using language
embeddings. The idea is to derive a coarse location of an object based on the
relevancy map of a language-embedded NeRF reconstruction and to compute the
pose estimate with a point cloud registration method. Additionally, we provide
an analysis of LERF's suitability for open-set object pose estimation. We
examine hyperparameters, such as activation thresholds for relevancy maps and
investigate the zero-shot capabilities on an instance- and category-level.
Furthermore, we plan to conduct robotic grasping experiments in a real-world
setting.



---

## G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel  View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Lutao Jiang, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.05617v1){: .btn .btn-green } |

**Abstract**: Following the burgeoning interest in implicit neural representation, Neural
Light Field (NeLF) has been introduced to predict the color of a ray directly.
Unlike Neural Radiance Field (NeRF), NeLF does not create a point-wise
representation by predicting color and volume density for each point in space.
However, the current NeLF methods face a challenge as they need to train a NeRF
model first and then synthesize over 10K views to train NeLF for improved
performance. Additionally, the rendering quality of NeLF methods is lower
compared to NeRF methods. In this paper, we propose G-NeLF, a versatile
grid-based NeLF approach that utilizes spatial-aware features to unleash the
potential of the neural network's inference capability, and consequently
overcome the difficulties of NeLF training. Specifically, we employ a
spatial-aware feature sequence derived from a meticulously crafted grid as the
ray's representation. Drawing from our empirical studies on the adaptability of
multi-resolution hash tables, we introduce a novel grid-based ray
representation for NeLF that can represent the entire space with a very limited
number of parameters. To better utilize the sequence feature, we design a
lightweight ray color decoder that simulates the ray propagation process,
enabling a more efficient inference of the ray's color. G-NeLF can be trained
without necessitating significant storage overhead and with the model size of
only 0.95 MB to surpass previous state-of-the-art NeLF. Moreover, compared with
grid-based NeRF methods, e.g., Instant-NGP, we only utilize one-tenth of its
parameters to achieve higher performance. Our code will be released upon
acceptance.



---

## Lagrangian Hashing for Compressed Neural Field Representations

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Shrisudhan Govindarajan, Zeno Sambugaro,  Akhmedkhan,  Shabanov, Towaki Takikawa, Daniel Rebain, Weiwei Sun, Nicola Conci, Kwang Moo Yi, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2409.05334v1){: .btn .btn-green } |

**Abstract**: We present Lagrangian Hashing, a representation for neural fields combining
the characteristics of fast training NeRF methods that rely on Eulerian grids
(i.e.~InstantNGP), with those that employ points equipped with features as a
way to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We
achieve this by incorporating a point-based representation into the
high-resolution layers of the hierarchical hash tables of an InstantNGP
representation. As our points are equipped with a field of influence, our
representation can be interpreted as a mixture of Gaussians stored within the
hash table. We propose a loss that encourages the movement of our Gaussians
towards regions that require more representation budget to be sufficiently well
represented. Our main finding is that our representation allows the
reconstruction of signals using a more compact representation without
compromising quality.

Comments:
- Project page: https://theialab.github.io/laghashes/

---

## Neural Surface Reconstruction and Rendering for LiDAR-Visual Systems

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Jianheng Liu, Chunran Zheng, Yunfei Wan, Bowen Wang, Yixi Cai, Fu Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2409.05310v1){: .btn .btn-green } |

**Abstract**: This paper presents a unified surface reconstruction and rendering framework
for LiDAR-visual systems, integrating Neural Radiance Fields (NeRF) and Neural
Distance Fields (NDF) to recover both appearance and structural information
from posed images and point clouds. We address the structural visible gap
between NeRF and NDF by utilizing a visible-aware occupancy map to classify
space into the free, occupied, visible unknown, and background regions. This
classification facilitates the recovery of a complete appearance and structure
of the scene. We unify the training of the NDF and NeRF using a spatial-varying
scale SDF-to-density transformation for levels of detail for both structure and
appearance. The proposed method leverages the learned NDF for structure-aware
NeRF training by an adaptive sphere tracing sampling strategy for accurate
structure rendering. In return, NeRF further refines structural in recovering
missing or fuzzy structures in the NDF. Extensive experiments demonstrate the
superior quality and versatility of the proposed method across various
scenarios. To benefit the community, the codes will be released at
\url{https://github.com/hku-mars/M2Mapping}.



---

## DreamMapping: High-Fidelity Text-to-3D Generation via Variational  Distribution Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-08 | Zeyu Cai, Duotun Wang, Yixun Liang, Zhijing Shao, Ying-Cong Chen, Xiaohang Zhan, Zeyu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.05099v3){: .btn .btn-green } |

**Abstract**: Score Distillation Sampling (SDS) has emerged as a prevalent technique for
text-to-3D generation, enabling 3D content creation by distilling
view-dependent information from text-to-2D guidance. However, they frequently
exhibit shortcomings such as over-saturated color and excess smoothness. In
this paper, we conduct a thorough analysis of SDS and refine its formulation,
finding that the core design is to model the distribution of rendered images.
Following this insight, we introduce a novel strategy called Variational
Distribution Mapping (VDM), which expedites the distribution modeling process
by regarding the rendered images as instances of degradation from
diffusion-based generation. This special design enables the efficient training
of variational distribution by skipping the calculations of the Jacobians in
the diffusion U-Net. We also introduce timestep-dependent Distribution
Coefficient Annealing (DCA) to further improve distilling precision. Leveraging
VDM and DCA, we use Gaussian Splatting as the 3D representation and build a
text-to-3D generation framework. Extensive experiments and evaluations
demonstrate the capability of VDM and DCA to generate high-fidelity and
realistic assets with optimization efficiency.

Comments:
- 15 pages, 14 figures

---

## GS-PT: Exploiting 3D Gaussian Splatting for Comprehensive Point Cloud  Understanding via Self-supervised Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-08 | Keyi Liu, Yeqi Luo, Weidong Yang, Jingyi Xu, Zhijun Li, Wen-Ming Chen, Ben Fei | cs.CV | [PDF](http://arxiv.org/pdf/2409.04963v1){: .btn .btn-green } |

**Abstract**: Self-supervised learning of point cloud aims to leverage unlabeled 3D data to
learn meaningful representations without reliance on manual annotations.
However, current approaches face challenges such as limited data diversity and
inadequate augmentation for effective feature learning. To address these
challenges, we propose GS-PT, which integrates 3D Gaussian Splatting (3DGS)
into point cloud self-supervised learning for the first time. Our pipeline
utilizes transformers as the backbone for self-supervised pre-training and
introduces novel contrastive learning tasks through 3DGS. Specifically, the
transformers aim to reconstruct the masked point cloud. 3DGS utilizes
multi-view rendered images as input to generate enhanced point cloud
distributions and novel view images, facilitating data augmentation and
cross-modal contrastive learning. Additionally, we incorporate features from
depth maps. By optimizing these tasks collectively, our method enriches the
tri-modal self-supervised learning process, enabling the model to leverage the
correlation across 3D point clouds and 2D images from various modalities. We
freeze the encoder after pre-training and test the model's performance on
multiple downstream tasks. Experimental results indicate that GS-PT outperforms
the off-the-shelf self-supervised learning methods on various downstream tasks
including 3D object classification, real-world classifications, and few-shot
learning and segmentation.



---

## Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for  Fisheye Cameras

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-07 | Zimu Liao, Siyan Chen, Rong Fu, Yi Wang, Zhongling Su, Hao Luo, Li Ma, Linning Xu, Bo Dai, Hengjie Li, Zhilin Pei, Xingcheng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2409.04751v2){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high
fidelity and real-time rendering. However, adapting 3DGS to different camera
models, particularly fisheye lenses, poses challenges due to the unique 3D to
2D projection calculation. Additionally, there are inefficiencies in the
tile-based splatting, especially for the extreme curvature and wide field of
view of fisheye lenses, which are crucial for its broader real-life
applications. To tackle these challenges, we introduce Fisheye-GS.This
innovative method recalculates the projection transformation and its gradients
for fisheye cameras. Our approach can be seamlessly integrated as a module into
other efficient 3D rendering methods, emphasizing its extensibility,
lightweight nature, and modular design. Since we only modified the projection
component, it can also be easily adapted for use with different camera models.
Compared to methods that train after undistortion, our approach demonstrates a
clear improvement in visual quality.



---

## 3D-GP-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian  Geometric Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-06 | Yujun Huang, Bin Chen, Niu Lian, Baoyi An, Shu-Tao Xia | cs.CV | [PDF](http://arxiv.org/pdf/2409.04013v1){: .btn .btn-green } |

**Abstract**: Multi-view image compression is vital for 3D-related applications. To
effectively model correlations between views, existing methods typically
predict disparity between two views on a 2D plane, which works well for small
disparities, such as in stereo images, but struggles with larger disparities
caused by significant view changes. To address this, we propose a novel
approach: learning-based multi-view image coding with 3D Gaussian geometric
priors (3D-GP-LMVIC). Our method leverages 3D Gaussian Splatting to derive
geometric priors of the 3D scene, enabling more accurate disparity estimation
across views within the compression model. Additionally, we introduce a depth
map compression model to reduce redundancy in geometric information between
views. A multi-view sequence ordering method is also proposed to enhance
correlations between adjacent views. Experimental results demonstrate that
3D-GP-LMVIC surpasses both traditional and learning-based methods in
performance, while maintaining fast encoding and decoding speed.

Comments:
- 19pages, 8 figures, conference

---

## SCARF: Scalable Continual Learning Framework for Memory-efficient  Multiple Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-06 | Yuze Wang, Junyi Wang, Chen Wang, Wantong Duan, Yongtang Bao, Yue Qi | cs.CV | [PDF](http://arxiv.org/pdf/2409.04482v1){: .btn .btn-green } |

**Abstract**: This paper introduces a novel continual learning framework for synthesising
novel views of multiple scenes, learning multiple 3D scenes incrementally, and
updating the network parameters only with the training data of the upcoming new
scene. We build on Neural Radiance Fields (NeRF), which uses multi-layer
perceptron to model the density and radiance field of a scene as the implicit
function. While NeRF and its extensions have shown a powerful capability of
rendering photo-realistic novel views in a single 3D scene, managing these
growing 3D NeRF assets efficiently is a new scientific problem. Very few works
focus on the efficient representation or continuous learning capability of
multiple scenes, which is crucial for the practical applications of NeRF. To
achieve these goals, our key idea is to represent multiple scenes as the linear
combination of a cross-scene weight matrix and a set of scene-specific weight
matrices generated from a global parameter generator. Furthermore, we propose
an uncertain surface knowledge distillation strategy to transfer the radiance
field knowledge of previous scenes to the new model. Representing multiple 3D
scenes with such weight matrices significantly reduces memory requirements. At
the same time, the uncertain surface distillation strategy greatly overcomes
the catastrophic forgetting problem and maintains the photo-realistic rendering
quality of previous scenes. Experiments show that the proposed approach
achieves state-of-the-art rendering quality of continual learning NeRF on
NeRF-Synthetic, LLFF, and TanksAndTemples datasets while preserving extra low
storage cost.



---

## GST: Precise 3D Human Body from a Single Image with Gaussian Splatting  Transformers

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-06 | Lorenza Prospero, Abdullah Hamdi, Joao F. Henriques, Christian Rupprecht | cs.CV | [PDF](http://arxiv.org/pdf/2409.04196v1){: .btn .btn-green } |

**Abstract**: Reconstructing realistic 3D human models from monocular images has
significant applications in creative industries, human-computer interfaces, and
healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene
representation composed of a mixture of Gaussians. Predicting such mixtures for
a human from a single input image is challenging, as it is a non-uniform
density (with a many-to-one relationship with input pixels) with strict
physical constraints. At the same time, it needs to be flexible to accommodate
a variety of clothes and poses. Our key observation is that the vertices of
standardized human meshes (such as SMPL) can provide an adequate density and
approximate initial position for Gaussians. We can then train a transformer
model to jointly predict comparatively small adjustments to these positions, as
well as the other Gaussians' attributes and the SMPL parameters. We show
empirically that this combination (using only multi-view supervision) can
achieve fast inference of 3D human models from a single image without test-time
optimization, expensive diffusion models, or 3D points supervision. We also
show that it can improve 3D pose estimation by better fitting human models that
account for clothes and other variations. The code is available on the project
website https://abdullahamdi.com/gst/ .

Comments:
- preprint

---

## LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model  Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-05 | Hanyang Yu, Xiaoxiao Long, Ping Tan | cs.CV | [PDF](http://arxiv.org/pdf/2409.03456v1){: .btn .btn-green } |

**Abstract**: We aim to address sparse-view reconstruction of a 3D scene by leveraging
priors from large-scale vision models. While recent advancements such as 3D
Gaussian Splatting (3DGS) have demonstrated remarkable successes in 3D
reconstruction, these methods typically necessitate hundreds of input images
that densely capture the underlying scene, making them time-consuming and
impractical for real-world applications. However, sparse-view reconstruction is
inherently ill-posed and under-constrained, often resulting in inferior and
incomplete outcomes. This is due to issues such as failed initialization,
overfitting on input images, and a lack of details. To mitigate these
challenges, we introduce LM-Gaussian, a method capable of generating
high-quality reconstructions from a limited number of images. Specifically, we
propose a robust initialization module that leverages stereo priors to aid in
the recovery of camera poses and the reliable point clouds. Additionally, a
diffusion-based refinement is iteratively applied to incorporate image
diffusion priors into the Gaussian optimization process to preserve intricate
scene details. Finally, we utilize video diffusion priors to further enhance
the rendered images for realistic visual effects. Overall, our approach
significantly reduces the data acquisition requirements compared to previous
3DGS methods. We validate the effectiveness of our framework through
experiments on various public datasets, demonstrating its potential for
high-quality 360-degree scene reconstruction. Visual results are on our
website.

Comments:
- Project page: https://hanyangyu1021.github.io/lm-gaussian.github.io/

---

## Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-05 | Shen Chen, Jiale Zhou, Lei Li | cs.CV | [PDF](http://arxiv.org/pdf/2409.03213v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for 3D scene
representation, offering a reduction in computational overhead compared to
Neural Radiance Fields (NeRF). However, 3DGS is susceptible to high-frequency
artifacts and demonstrates suboptimal performance under sparse viewpoint
conditions, thereby limiting its applicability in robotics and computer vision.
To address these limitations, we introduce SVS-GS, a novel framework for Sparse
Viewpoint Scene reconstruction that integrates a 3D Gaussian smoothing filter
to suppress artifacts. Furthermore, our approach incorporates a Depth Gradient
Profile Prior (DGPP) loss with a dynamic depth mask to sharpen edges and 2D
diffusion with Score Distillation Sampling (SDS) loss to enhance geometric
consistency in novel view synthesis. Experimental evaluations on the
MipNeRF-360 and SeaThru-NeRF datasets demonstrate that SVS-GS markedly improves
3D reconstruction from sparse viewpoints, offering a robust and efficient
solution for scene understanding in robotics and computer vision applications.



---

## Weight Conditioning for Smooth Optimization of Neural Networks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-05 | Hemanth Saratchandran, Thomas X. Wang, Simon Lucey | cs.CV | [PDF](http://arxiv.org/pdf/2409.03424v1){: .btn .btn-green } |

**Abstract**: In this article, we introduce a novel normalization technique for neural
network weight matrices, which we term weight conditioning. This approach aims
to narrow the gap between the smallest and largest singular values of the
weight matrices, resulting in better-conditioned matrices. The inspiration for
this technique partially derives from numerical linear algebra, where
well-conditioned matrices are known to facilitate stronger convergence results
for iterative solvers. We provide a theoretical foundation demonstrating that
our normalization technique smoothens the loss landscape, thereby enhancing
convergence of stochastic gradient descent algorithms. Empirically, we validate
our normalization across various neural network architectures, including
Convolutional Neural Networks (CNNs), Vision Transformers (ViT), Neural
Radiance Fields (NeRF), and 3D shape modeling. Our findings indicate that our
normalization method is not only competitive but also outperforms existing
weight normalization techniques from the literature.

Comments:
- ECCV 2024

---

## GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous  Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Huasong Han, Kaixuan Zhou, Xiaoxiao Long, Yusen Wang, Chunxia Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2409.02382v1){: .btn .btn-green } |

**Abstract**: We propose GGS, a Generalizable Gaussian Splatting method for Autonomous
Driving which can achieve realistic rendering under large viewpoint changes.
Previous generalizable 3D gaussian splatting methods are limited to rendering
novel views that are very close to the original pair of images, which cannot
handle large differences in viewpoint. Especially in autonomous driving
scenarios, images are typically collected from a single lane. The limited
training perspective makes rendering images of a different lane very
challenging. To further improve the rendering capability of GGS under large
viewpoint changes, we introduces a novel virtual lane generation module into
GSS method to enables high-quality lane switching even without a multi-lane
dataset. Besides, we design a diffusion loss to supervise the generation of
virtual lane image to further address the problem of lack of data in the
virtual lanes. Finally, we also propose a depth refinement module to optimize
depth estimation in the GSS model. Extensive validation of our method, compared
to existing approaches, demonstrates state-of-the-art performance.



---

## Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video  Diffusion Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Zhibin Liu, Haoye Dong, Aviral Chharia, Hefeng Wu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02851v1){: .btn .btn-green } |

**Abstract**: Generating lifelike 3D humans from a single RGB image remains a challenging
task in computer vision, as it requires accurate modeling of geometry,
high-quality texture, and plausible unseen parts. Existing methods typically
use multi-view diffusion models for 3D generation, but they often face
inconsistent view issues, which hinder high-quality 3D human generation. To
address this, we propose Human-VDM, a novel method for generating 3D human from
a single RGB image using Video Diffusion Models. Human-VDM provides temporally
consistent views for 3D human generation using Gaussian Splatting. It consists
of three modules: a view-consistent human video diffusion module, a video
augmentation module, and a Gaussian Splatting module. First, a single image is
fed into a human video diffusion module to generate a coherent human video.
Next, the video augmentation module applies super-resolution and video
interpolation to enhance the textures and geometric smoothness of the generated
video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans
under the guidance of these high-resolution and view-consistent images.
Experiments demonstrate that Human-VDM achieves high-quality 3D human from a
single image, outperforming state-of-the-art methods in both generation quality
and quantity. Project page: https://human-vdm.github.io/Human-VDM/

Comments:
- 14 Pages, 8 figures, Project page:
  https://human-vdm.github.io/Human-VDM/

---

## Object Gaussian for Monocular 6D Pose Estimation from Sparse Views


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Luqing Luo, Shichu Sun, Jiangang Yang, Linfang Zheng, Jinwei Du, Jian Liu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02581v1){: .btn .btn-green } |

**Abstract**: Monocular object pose estimation, as a pivotal task in computer vision and
robotics, heavily depends on accurate 2D-3D correspondences, which often demand
costly CAD models that may not be readily available. Object 3D reconstruction
methods offer an alternative, among which recent advancements in 3D Gaussian
Splatting (3DGS) afford a compelling potential. Yet its performance still
suffers and tends to overfit with fewer input views. Embracing this challenge,
we introduce SGPose, a novel framework for sparse view object pose estimation
using Gaussian-based methods. Given as few as ten views, SGPose generates a
geometric-aware representation by starting with a random cuboid initialization,
eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as
required by traditional 3DGS methods. SGPose removes the dependence on CAD
models by regressing dense 2D-3D correspondences between images and the
reconstructed model from sparse input and random initialization, while the
geometric-consistent depth supervision and online synthetic view warping are
key to the success. Experiments on typical benchmarks, especially on the
Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods
even under sparse view constraints, under-scoring its potential in real-world
applications.



---

## UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from  Endoscopic Sparse Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Jiaxin Guo, Jiangliu Wang, Ruofeng Wei, Di Kang, Qi Dou, Yun-hui Liu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02917v1){: .btn .btn-green } |

**Abstract**: Visualizing surgical scenes is crucial for revealing internal anatomical
structures during minimally invasive procedures. Novel View Synthesis is a
vital technique that offers geometry and appearance reconstruction, enhancing
understanding, planning, and decision-making in surgical scenes. Despite the
impressive achievements of Neural Radiance Field (NeRF), its direct application
to surgical scenes produces unsatisfying results due to two challenges:
endoscopic sparse views and significant photometric inconsistencies. In this
paper, we propose uncertainty-aware conditional NeRF for novel view synthesis
to tackle the severe shape-radiance ambiguity from sparse surgical views. The
core of UC-NeRF is to incorporate the multi-view uncertainty estimation to
condition the neural radiance field for modeling the severe photometric
inconsistencies adaptively. Specifically, our UC-NeRF first builds a
consistency learner in the form of multi-view stereo network, to establish the
geometric correspondence from sparse views and generate uncertainty estimation
and feature priors. In neural rendering, we design a base-adaptive NeRF network
to exploit the uncertainty estimation for explicitly handling the photometric
inconsistencies. Furthermore, an uncertainty-guided geometry distillation is
employed to enhance geometry learning. Experiments on the SCARED and Hamlyn
datasets demonstrate our superior performance in rendering appearance and
geometry, consistently outperforming the current state-of-the-art approaches.
Our code will be released at \url{https://github.com/wrld/UC-NeRF}.



---

## DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, Laura Leal-Taix√© | cs.CV | [PDF](http://arxiv.org/pdf/2409.02104v1){: .btn .btn-green } |

**Abstract**: Reconstructing scenes and tracking motion are two sides of the same coin.
Tracking points allow for geometric reconstruction [14], while geometric
reconstruction of (dynamic) scenes allows for 3D tracking of points over time
[24, 39]. The latter was recently also exploited for 2D point tracking to
overcome occlusion ambiguities by lifting tracking directly into 3D [38].
However, above approaches either require offline processing or multi-view
camera setups both unrealistic for real-world applications like robot
navigation or mixed reality. We target the challenge of online 2D and 3D point
tracking from unposed monocular camera input introducing Dynamic Online
Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to
reconstruct dynamic scenes in an online fashion. Our approach extends 3D
Gaussians to capture new content and object motions while estimating camera
movements from a single RGB frame. DynOMo stands out by enabling emergence of
point trajectories through robust image feature reconstruction and a novel
similarity-enhanced regularization term, without requiring any
correspondence-level supervision. It sets the first baseline for online point
tracking with monocular unposed cameras, achieving performance on par with
existing methods. We aim to inspire the community to advance online point
tracking and reconstruction, expanding the applicability to diverse real-world
scenarios.



---

## PRoGS: Progressive Rendering of Gaussian Splats

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels | cs.CV | [PDF](http://arxiv.org/pdf/2409.01761v1){: .btn .btn-green } |

**Abstract**: Over the past year, 3D Gaussian Splatting (3DGS) has received significant
attention for its ability to represent 3D scenes in a perceptually accurate
manner. However, it can require a substantial amount of storage since each
splat's individual data must be stored. While compression techniques offer a
potential solution by reducing the memory footprint, they still necessitate
retrieving the entire scene before any part of it can be rendered. In this
work, we introduce a novel approach for progressively rendering such scenes,
aiming to display visible content that closely approximates the final scene as
early as possible without loading the entire scene into memory. This approach
benefits both on-device rendering applications limited by memory constraints
and streaming applications where minimal bandwidth usage is preferred. To
achieve this, we approximate the contribution of each Gaussian to the final
scene and construct an order of prioritization on their inclusion in the
rendering process. Additionally, we demonstrate that our approach can be
combined with existing compression methods to progressively render (and stream)
3DGS scenes, optimizing bandwidth usage by focusing on the most important
splats within a scene. Overall, our work establishes a foundation for making
remotely hosted 3DGS content more quickly accessible to end-users in
over-the-top consumption scenarios, with our results showing significant
improvements in quality across all metrics compared to existing methods.



---

## GraspSplats: Efficient Manipulation with 3D Feature Splatting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Mazeyu Ji, Ri-Zhao Qiu, Xueyan Zou, Xiaolong Wang | cs.RO | [PDF](http://arxiv.org/pdf/2409.02084v1){: .btn .btn-green } |

**Abstract**: The ability for robots to perform efficient and zero-shot grasping of object
parts is crucial for practical applications and is becoming prevalent with
recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap
for representations to support such a capability, existing methods rely on
neural fields (NeRFs) via differentiable rendering or point-based projection
methods. However, we demonstrate that NeRFs are inappropriate for scene changes
due to their implicitness and point-based methods are inaccurate for part
localization without rendering-based optimization. To amend these issues, we
propose GraspSplats. Using depth supervision and a novel reference feature
computation method, GraspSplats generates high-quality scene representations in
under 60 seconds. We further validate the advantages of Gaussian-based
representation by showing that the explicit and optimized geometry in
GraspSplats is sufficient to natively support (1) real-time grasp sampling and
(2) dynamic and articulated object manipulation with point trackers. With
extensive experiments on a Franka robot, we demonstrate that GraspSplats
significantly outperforms existing methods under diverse task settings. In
particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO,
and 2D detection methods.

Comments:
- Project webpage: https://graspsplats.github.io/

---

## GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color  Point Clouds via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Zixuan Guo, Yifan Xie, Weijing Xie, Peng Huang, Fei Ma, Fei Richard Yu | cs.RO | [PDF](http://arxiv.org/pdf/2409.01581v1){: .btn .btn-green } |

**Abstract**: Dense colored point clouds enhance visual perception and are of significant
value in various robotic applications. However, existing learning-based point
cloud upsampling methods are constrained by computational resources and batch
processing strategies, which often require subdividing point clouds into
smaller patches, leading to distortions that degrade perceptual quality. To
address this challenge, we propose a novel 2D-3D hybrid colored point cloud
upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for
robotic perception. This approach leverages 3DGS to bridge 3D point clouds with
their 2D rendered images in robot vision systems. A dual scale rendered image
restoration network transforms sparse point cloud renderings into dense
representations, which are then input into 3DGS along with precise robot camera
poses and interpolated sparse point clouds to reconstruct dense 3D point
clouds. We have made a series of enhancements to the vanilla 3DGS, enabling
precise control over the number of points and significantly boosting the
quality of the upsampled point cloud for robotic scene understanding. Our
framework supports processing entire point clouds on a single consumer-grade
GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation
and thus producing high-quality, dense colored point clouds with millions of
points for robot navigation and manipulation tasks. Extensive experimental
results on generating million-level point cloud data validate the effectiveness
of our method, substantially improving the quality of colored point clouds and
demonstrating significant potential for applications involving large-scale
point clouds in autonomous robotics and human-robot interaction scenarios.

Comments:
- 7 pages, 5 figures

---

## $S^2$NeRF: Privacy-preserving Training Framework for NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Bokang Zhang, Yanglin Zhang, Zhikun Zhang, Jinglan Yang, Lingying Huang, Junfeng Wu | cs.CR | [PDF](http://arxiv.org/pdf/2409.01661v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and
graphics, facilitating novel view synthesis and influencing sectors like
extended reality and e-commerce. However, NeRF's dependence on extensive data
collection, including sensitive scene image data, introduces significant
privacy risks when users upload this data for model training. To address this
concern, we first propose SplitNeRF, a training framework that incorporates
split learning (SL) techniques to enable privacy-preserving collaborative model
training between clients and servers without sharing local data. Despite its
benefits, we identify vulnerabilities in SplitNeRF by developing two attack
methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which
exploit the shared gradient data and a few leaked scene images to reconstruct
private scene information. To counter these threats, we introduce $S^2$NeRF,
secure SplitNeRF that integrates effective defense mechanisms. By introducing
decaying noise related to the gradient norm into the shared gradient
information, $S^2$NeRF preserves privacy while maintaining a high utility of
the NeRF model. Our extensive evaluations across multiple datasets demonstrate
the effectiveness of $S^2$NeRF against privacy breaches, confirming its
viability for secure NeRF training in sensitive applications.

Comments:
- To appear in the ACM Conference on Computer and Communications
  Security (CCS'24), October 14-18, 2024, Salt Lake City, UT, USA

---

## Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian  Splatting for Dynamic Surgical Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-02 | Qian Li, Shuojue Yang, Daiyun Shen, Yueming Jin | cs.CV | [PDF](http://arxiv.org/pdf/2409.01003v1){: .btn .btn-green } |

**Abstract**: Reconstructing endoscopic videos is crucial for high-fidelity visualization
and the efficiency of surgical operations. Despite the importance, existing 3D
reconstruction methods encounter several challenges, including stringent
demands for accuracy, imprecise camera positioning, intricate dynamic scenes,
and the necessity for rapid reconstruction. Addressing these issues, this paper
presents the first camera-pose-free scene reconstruction framework, Free-DyGS,
tailored for dynamic surgical videos, leveraging 3D Gaussian splatting
technology. Our approach employs a frame-by-frame reconstruction strategy and
is delineated into four distinct phases: Scene Initialization, Joint Learning,
Scene Expansion, and Retrospective Learning. We introduce a Generalizable
Gaussians Parameterization module within the Scene Initialization and Expansion
phases to proficiently generate Gaussian attributes for each pixel from the
RGBD frames. The Joint Learning phase is crafted to concurrently deduce scene
deformation and camera pose, facilitated by an innovative flexible deformation
module. In the scene expansion stage, the Gaussian points gradually grow as the
camera moves. The Retrospective Learning phase is dedicated to enhancing the
precision of scene deformation through the reassessment of prior frames. The
efficacy of the proposed Free-DyGS is substantiated through experiments on two
datasets: the StereoMIS and Hamlyn datasets. The experimental outcomes
underscore that Free-DyGS surpasses conventional baseline models in both
rendering fidelity and computational efficiency.


