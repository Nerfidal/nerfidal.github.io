---
layout: default
title: September 2024
parent: Papers
nav_order: 202409
---

<!---metadata--->


## DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with  Diffusion Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Thomas Hanwen Zhu, Ruining Li, Tomas Jakab | cs.CV | [PDF](http://arxiv.org/pdf/2409.08278v1){: .btn .btn-green } |

**Abstract**: We present DreamHOI, a novel method for zero-shot synthesis of human-object
interactions (HOIs), enabling a 3D human model to realistically interact with
any given object based on a textual description. This task is complicated by
the varying categories and geometries of real-world objects and the scarcity of
datasets encompassing diverse HOIs. To circumvent the need for extensive data,
we leverage text-to-image diffusion models trained on billions of image-caption
pairs. We optimize the articulation of a skinned human mesh using Score
Distillation Sampling (SDS) gradients obtained from these models, which predict
image-space edits. However, directly backpropagating image-space gradients into
complex articulation parameters is ineffective due to the local nature of such
gradients. To overcome this, we introduce a dual implicit-explicit
representation of a skinned mesh, combining (implicit) neural radiance fields
(NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization,
we transition between implicit and explicit forms, grounding the NeRF
generation while refining the mesh articulation. We validate our approach
through extensive experiments, demonstrating its effectiveness in generating
realistic HOIs.

Comments:
- Project page: https://DreamHOI.github.io/

---

## FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Qiuhong Shen, Xingyi Yang, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.08270v1){: .btn .btn-green } |

**Abstract**: This study addresses the challenge of accurately segmenting 3D Gaussian
Splatting from 2D masks. Conventional methods often rely on iterative gradient
descent to assign each Gaussian a unique label, leading to lengthy optimization
and sub-optimal solutions. Instead, we propose a straightforward yet globally
optimal solver for 3D-GS segmentation. The core insight of our method is that,
with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially
a linear function with respect to the labels of each Gaussian. As such, the
optimal label assignment can be solved via linear programming in closed form.
This solution capitalizes on the alpha blending characteristic of the splatting
process for single step optimization. By incorporating the background bias in
our objective function, our method shows superior robustness in 3D segmentation
against noises. Remarkably, our optimization completes within 30 seconds, about
50$\times$ faster than the best existing methods. Extensive experiments
demonstrate the efficiency and robustness of our method in segmenting various
scenes, and its superior performance in downstream tasks such as object removal
and inpainting. Demos and code will be available at
https://github.com/florinshen/FlashSplat.

Comments:
- ECCV'2024

---

## Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared  Novel-view Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Qian Chen, Shihao Shu, Xiangzhi Bai | cs.CV | [PDF](http://arxiv.org/pdf/2409.08042v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis based on visible light has been extensively studied. In
comparison to visible light imaging, thermal infrared imaging offers the
advantage of all-weather imaging and strong penetration, providing increased
possibilities for reconstruction in nighttime and adverse weather scenarios.
However, thermal infrared imaging is influenced by physical characteristics
such as atmospheric transmission effects and thermal conduction, hindering the
precise reconstruction of intricate details in thermal infrared scenes,
manifesting as issues of floaters and indistinct edge features in synthesized
images. To address these limitations, this paper introduces a physics-induced
3D Gaussian splatting method named Thermal3D-GS. Thermal3D-GS begins by
modeling atmospheric transmission effects and thermal conduction in
three-dimensional media using neural networks. Additionally, a temperature
consistency constraint is incorporated into the optimization objective to
enhance the reconstruction accuracy of thermal infrared images. Furthermore, to
validate the effectiveness of our method, the first large-scale benchmark
dataset for this field named Thermal Infrared Novel-view Synthesis Dataset
(TI-NSD) is created. This dataset comprises 20 authentic thermal infrared video
scenes, covering indoor, outdoor, and UAV(Unmanned Aerial Vehicle) scenarios,
totaling 6,664 frames of thermal infrared image data. Based on this dataset,
this paper experimentally verifies the effectiveness of Thermal3D-GS. The
results indicate that our method outperforms the baseline method with a 3.03 dB
improvement in PSNR and significantly addresses the issues of floaters and
indistinct edge features present in the baseline method. Our dataset and
codebase will be released in
\href{https://github.com/mzzcdf/Thermal3DGS}{\textcolor{red}{Thermal3DGS}}.

Comments:
- 17 pages, 4 figures, 3 tables

---

## SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming  with Arbitrary Length

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-12 | Bangya Liu, Suman Banerjee | cs.MM | [PDF](http://arxiv.org/pdf/2409.07759v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant
attention in computer vision and computer graphics due to its high rendering
speed and remarkable quality. While extant research has endeavored to extend
the application of 3DGS from static to dynamic scenes, such efforts have been
consistently impeded by excessive model sizes, constraints on video duration,
and content deviation. These limitations significantly compromise the
streamability of dynamic 3D Gaussian models, thereby restricting their utility
in downstream applications, including volumetric video, autonomous vehicle, and
immersive technologies such as virtual, augmented, and mixed reality.
  This paper introduces SwinGS, a novel framework for training, delivering, and
rendering volumetric video in a real-time streaming fashion. To address the
aforementioned challenges and enhance streamability, SwinGS integrates
spacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to
fit various 3D scenes across frames, in the meantime employing a sliding window
captures Gaussian snapshots for each frame in an accumulative way. We implement
a prototype of SwinGS and demonstrate its streamability across various datasets
and scenes. Additionally, we develop an interactive WebGL viewer enabling
real-time volumetric video playback on most devices with modern browsers,
including smartphones and tablets. Experimental results show that SwinGS
reduces transmission costs by 83.6% compared to previous work with ignorable
compromise in PSNR. Moreover, SwinGS easily scales to long video sequences
without compromising quality.



---

## Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered  Stereo Pairs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Sadra Safadoust, Fabio Tosi, Fatma GÃ¼ney, Matteo Poggi | cs.CV | [PDF](http://arxiv.org/pdf/2409.07456v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (GS) significantly struggles to accurately represent
the underlying 3D scene geometry, resulting in inaccuracies and floating
artifacts when rendering depth maps. In this paper, we address this limitation,
undertaking a comprehensive analysis of the integration of depth priors
throughout the optimization process of Gaussian primitives, and present a novel
strategy for this purpose. This latter dynamically exploits depth cues from a
readily available stereo network, processing virtual stereo pairs rendered by
the GS model itself during training and achieving consistent self-improvement
of the scene representation. Experimental results on three popular datasets,
breaking ground as the first to assess depth accuracy for these models,
validate our findings.

Comments:
- BMVC 2024. Project page: https://kuis-ai.github.io/StereoGS/

---

## ThermalGaussian: Thermal 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Rongfeng Lu, Hangyu Chen, Zunjie Zhu, Yuhang Qin, Ming Lu, Le Zhang, Chenggang Yan, Anke Xue | cs.CV | [PDF](http://arxiv.org/pdf/2409.07200v1){: .btn .btn-green } |

**Abstract**: Thermography is especially valuable for the military and other users of
surveillance cameras. Some recent methods based on Neural Radiance Fields
(NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of
thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS)
prevails due to its rapid training and real-time rendering. In this work, we
propose ThermalGaussian, the first thermal 3DGS approach capable of rendering
high-quality images in RGB and thermal modalities. We first calibrate the RGB
camera and the thermal camera to ensure that both modalities are accurately
aligned. Subsequently, we use the registered images to learn the multimodal 3D
Gaussians. To prevent the overfitting of any single modality, we introduce
several multimodal regularization constraints. We also develop smoothing
constraints tailored to the physical characteristics of the thermal modality.
Besides, we contribute a real-world dataset named RGBT-Scenes, captured by a
hand-hold thermal-infrared camera, facilitating future research on thermal
scene reconstruction. We conduct comprehensive experiments to show that
ThermalGaussian achieves photorealistic rendering of thermal images and
improves the rendering quality of RGB images. With the proposed multimodal
regularization constraints, we also reduced the model's storage cost by 90\%.
The code and dataset will be released.

Comments:
- 10 pages, 7 figures

---

## DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for  Text-to-3D Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Zuxuan Wu, Yu-Gang Jiang, Tao Mei | cs.CV | [PDF](http://arxiv.org/pdf/2409.07454v1){: .btn .btn-green } |

**Abstract**: Learning radiance fields (NeRF) with powerful 2D diffusion models has
garnered popularity for text-to-3D generation. Nevertheless, the implicit 3D
representations of NeRF lack explicit modeling of meshes and textures over
surfaces, and such surface-undefined way may suffer from the issues, e.g.,
noisy surfaces with ambiguous texture details or cross-view inconsistency. To
alleviate this, we present DreamMesh, a novel text-to-3D architecture that
pivots on well-defined surfaces (triangle meshes) to generate high-fidelity
explicit 3D model. Technically, DreamMesh capitalizes on a distinctive
coarse-to-fine scheme. In the coarse stage, the mesh is first deformed by
text-guided Jacobians and then DreamMesh textures the mesh with an interlaced
use of 2D diffusion models in a tuning free manner from multiple viewpoints. In
the fine stage, DreamMesh jointly manipulates the mesh and refines the texture
map, leading to high-quality triangle meshes with high-fidelity textured
materials. Extensive experiments demonstrate that DreamMesh significantly
outperforms state-of-the-art text-to-3D methods in faithfully generating 3D
content with richer textual details and enhanced geometry. Our project page is
available at https://dreammesh.github.io.

Comments:
- ECCV 2024. Project page is available at
  \url{https://dreammesh.github.io}

---

## Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video  Diffusion Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Chong-Wah Ngo, Tao Mei | cs.CV | [PDF](http://arxiv.org/pdf/2409.07452v1){: .btn .btn-green } |

**Abstract**: Despite having tremendous progress in image-to-3D generation, existing
methods still struggle to produce multi-view consistent images with
high-resolution textures in detail, especially in the paradigm of 2D diffusion
that lacks 3D awareness. In this work, we present High-resolution Image-to-3D
model (Hi3D), a new video diffusion based paradigm that redefines a single
image to multi-view images as 3D-aware sequential image generation (i.e.,
orbital video generation). This methodology delves into the underlying temporal
consistency knowledge in video diffusion model that generalizes well to
geometry consistency across multiple views in 3D generation. Technically, Hi3D
first empowers the pre-trained video diffusion model with 3D-aware prior
(camera pose condition), yielding multi-view images with low-resolution texture
details. A 3D-aware video-to-video refiner is learnt to further scale up the
multi-view images with high-resolution texture details. Such high-resolution
multi-view images are further augmented with novel views through 3D Gaussian
Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D
reconstruction. Extensive experiments on both novel view synthesis and single
view reconstruction demonstrate that our Hi3D manages to produce superior
multi-view consistency images with highly-detailed textures. Source code and
data are available at \url{https://github.com/yanghb22-fdu/Hi3D-Official}.

Comments:
- ACM Multimedia 2024. Source code is available at
  \url{https://github.com/yanghb22-fdu/Hi3D-Official}

---

## Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting  Networks


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Ruihan Xu, Anthony Opipari, Joshua Mah, Stanley Lewis, Haoran Zhang, Hanzhe Guo, Odest Chadwicke Jenkins | cs.CV | [PDF](http://arxiv.org/pdf/2409.07245v1){: .btn .btn-green } |

**Abstract**: This paper introduces SO(2)-Equivariant Gaussian Sculpting Networks (GSNs) as
an approach for SO(2)-Equivariant 3D object reconstruction from single-view
image observations.
  GSNs take a single observation as input to generate a Gaussian splat
representation describing the observed object's geometry and texture. By using
a shared feature extractor before decoding Gaussian colors, covariances,
positions, and opacities, GSNs achieve extremely high throughput (>150FPS).
Experiments demonstrate that GSNs can be trained efficiently using a multi-view
rendering loss and are competitive, in quality, with expensive diffusion-based
reconstruction algorithms. The GSN model is validated on multiple benchmark
experiments. Moreover, we demonstrate the potential for GSNs to be used within
a robotic manipulation pipeline for object-centric grasping.

Comments:
- Accepted to RSS 2024 Workshop on Geometric and Algebraic Structure in
  Robot Learning

---

## Instant Facial Gaussians Translator for Relightable and Interactable  Facial Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-11 | Dafei Qin, Hongyang Lin, Qixuan Zhang, Kaichun Qiao, Longwen Zhang, Zijun Zhao, Jun Saito, Jingyi Yu, Lan Xu, Taku Komura | cs.GR | [PDF](http://arxiv.org/pdf/2409.07441v1){: .btn .btn-green } |

**Abstract**: We propose GauFace, a novel Gaussian Splatting representation, tailored for
efficient animation and rendering of physically-based facial assets. Leveraging
strong geometric priors and constrained optimization, GauFace ensures a neat
and structured Gaussian representation, delivering high fidelity and real-time
facial interaction of 30fps@1440p on a Snapdragon 8 Gen 2 mobile platform.
  Then, we introduce TransGS, a diffusion transformer that instantly translates
physically-based facial assets into the corresponding GauFace representations.
Specifically, we adopt a patch-based pipeline to handle the vast number of
Gaussians effectively. We also introduce a novel pixel-aligned sampling scheme
with UV positional encoding to ensure the throughput and rendering quality of
GauFace assets generated by our TransGS. Once trained, TransGS can instantly
translate facial assets with lighting conditions to GauFace representation,
With the rich conditioning modalities, it also enables editing and animation
capabilities reminiscent of traditional CG pipelines.
  We conduct extensive evaluations and user studies, compared to traditional
offline and online renderers, as well as recent neural rendering methods, which
demonstrate the superior performance of our approach for facial asset
rendering. We also showcase diverse immersive applications of facial assets
using our TransGS approach and GauFace representation, across various platforms
like PCs, phones and even VR headsets.

Comments:
- Project Page: https://dafei-qin.github.io/TransGS.github.io/

---

## LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-10 | Archana Swaminathan, Anubhav Gupta, Kamal Gupta, Shishira R. Maiya, Vatsal Agarwal, Abhinav Shrivastava | cs.CV | [PDF](http://arxiv.org/pdf/2409.06703v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of
static scenes and objects in 3D, offering unprecedented quality. However,
extending NeRFs to model dynamic objects or object articulations remains a
challenging problem. Previous works have tackled this issue by focusing on
part-level reconstruction and motion estimation for objects, but they often
rely on heuristics regarding the number of moving parts or object categories,
which can limit their practical use. In this work, we introduce LEIA, a novel
approach for representing dynamic 3D objects. Our method involves observing the
object at distinct time steps or "states" and conditioning a hypernetwork on
the current state, using this to parameterize our NeRF. This approach allows us
to learn a view-invariant latent representation for each state. We further
demonstrate that by interpolating between these states, we can generate novel
articulation configurations in 3D space that were previously unseen. Our
experimental results highlight the effectiveness of our method in articulating
objects in a manner that is independent of the viewing angle and joint
configuration. Notably, our approach outperforms previous methods that rely on
motion information for articulation registration.

Comments:
- Accepted to ECCV 2024. Project Website at
  https://archana1998.github.io/leia/

---

## GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-10 | Junyi Chen, Weicai Ye, Yifan Wang, Danpeng Chen, Di Huang, Wanli Ouyang, Guofeng Zhang, Yu Qiao, Tong He | cs.CV | [PDF](http://arxiv.org/pdf/2409.06685v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown promising performance in novel view
synthesis. Previous methods adapt it to obtaining surfaces of either individual
3D objects or within limited scenes. In this paper, we make the first attempt
to tackle the challenging task of large-scale scene surface reconstruction.
This task is particularly difficult due to the high GPU memory consumption,
different levels of details for geometric representation, and noticeable
inconsistencies in appearance. To this end, we propose GigaGS, the first work
for high-quality surface reconstruction for large-scale scenes using 3DGS.
GigaGS first applies a partitioning strategy based on the mutual visibility of
spatial regions, which effectively grouping cameras for parallel processing. To
enhance the quality of the surface, we also propose novel multi-view
photometric and geometric consistency constraints based on Level-of-Detail
representation. In doing so, our method can reconstruct detailed surface
structures. Comprehensive experiments are conducted on various datasets. The
consistent improvement demonstrates the superiority of GigaGS.



---

## Sources of Uncertainty in 3D Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-10 | Marcus Klasson, Riccardo Mereu, Juho Kannala, Arno Solin | cs.CV | [PDF](http://arxiv.org/pdf/2409.06407v1){: .btn .btn-green } |

**Abstract**: The process of 3D scene reconstruction can be affected by numerous
uncertainty sources in real-world scenes. While Neural Radiance Fields (NeRFs)
and 3D Gaussian Splatting (GS) achieve high-fidelity rendering, they lack
built-in mechanisms to directly address or quantify uncertainties arising from
the presence of noise, occlusions, confounding outliers, and imprecise camera
pose inputs. In this paper, we introduce a taxonomy that categorizes different
sources of uncertainty inherent in these methods. Moreover, we extend NeRF- and
GS-based methods with uncertainty estimation techniques, including learning
uncertainty outputs and ensembles, and perform an empirical study to assess
their ability to capture the sensitivity of the reconstruction. Our study
highlights the need for addressing various uncertainty aspects when designing
NeRF/GS-based methods for uncertainty-aware 3D reconstruction.

Comments:
- To appear in ECCV 2024 Workshop Proceedings. Project page at
  https://aaltoml.github.io/uncertainty-nerf-gs/

---

## gsplat: An Open-Source Library for Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-10 | Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2409.06765v1){: .btn .btn-green } |

**Abstract**: gsplat is an open-source library designed for training and developing
Gaussian Splatting methods. It features a front-end with Python bindings
compatible with the PyTorch library and a back-end with highly optimized CUDA
kernels. gsplat offers numerous features that enhance the optimization of
Gaussian Splatting models, which include optimization improvements for speed,
memory, and convergence times. Experimental results demonstrate that gsplat
achieves up to 10% less training time and 4x less memory than the original
implementation. Utilized in several research projects, gsplat is actively
maintained on GitHub. Source code is available at
https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We
welcome contributions from the open-source community.

Comments:
- 17 pages, 2 figures, JMLR MLOSS

---

## LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance  Fields with RGB-Event Stereo

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Wei Zhi Tang, Daniel Rebain, Kostantinos G. Derpanis, Kwang Moo Yi | cs.CV | [PDF](http://arxiv.org/pdf/2409.06104v1){: .btn .btn-green } |

**Abstract**: We present a method for reconstructing a clear Neural Radiance Field (NeRF)
even with fast camera motions. To address blur artifacts, we leverage both
(blurry) RGB images and event camera data captured in a binocular
configuration. Importantly, when reconstructing our clear NeRF, we consider the
camera modeling imperfections that arise from the simple pinhole camera model
as learned embeddings for each camera measurement, and further learn a mapper
that connects event camera measurements with RGB data. As no previous dataset
exists for our binocular setting, we introduce an event camera dataset with
captures from a 3D-printed stereo configuration between RGB and event cameras.
Empirically, we evaluate our introduced dataset and EVIMOv2 and show that our
method leads to improved reconstructions. Our code and dataset are available at
https://github.com/ubc-vision/LSENeRF.



---

## GASP: Gaussian Splatting for Physic-Based Simulations

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Piotr Borycki, Weronika Smolak, Joanna WaczyÅska, Marcin Mazur, SÅawomir Tadeja, PrzemysÅaw Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2409.05819v1){: .btn .btn-green } |

**Abstract**: Physics simulation is paramount for modeling and utilization of 3D scenes in
various real-world applications. However, its integration with state-of-the-art
3D scene rendering techniques such as Gaussian Splatting (GS) remains
challenging. Existing models use additional meshing mechanisms, including
triangle or tetrahedron meshing, marching cubes, or cage meshes. As an
alternative, we can modify the physics grounded Newtonian dynamics to align
with 3D Gaussian components. Current models take the first-order approximation
of a deformation map, which locally approximates the dynamics by linear
transformations. In contrast, our Gaussian Splatting for Physics-Based
Simulations (GASP) model uses such a map (without any modifications) and flat
Gaussian distributions, which are parameterized by three points (mesh faces).
Subsequently, each 3D point (mesh face node) is treated as a discrete entity
within a 3D space. Consequently, the problem of modeling Gaussian components is
reduced to working with 3D points. Additionally, the information on mesh faces
can be used to incorporate further properties into the physics model,
facilitating the use of triangles. Resulting solution can be integrated into
any physics engine that can be treated as a black box. As demonstrated in our
studies, the proposed model exhibits superior performance on a diverse range of
benchmark datasets designed for 3D object rendering.



---

## Lagrangian Hashing for Compressed Neural Field Representations

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Shrisudhan Govindarajan, Zeno Sambugaro,  Akhmedkhan,  Shabanov, Towaki Takikawa, Daniel Rebain, Weiwei Sun, Nicola Conci, Kwang Moo Yi, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2409.05334v1){: .btn .btn-green } |

**Abstract**: We present Lagrangian Hashing, a representation for neural fields combining
the characteristics of fast training NeRF methods that rely on Eulerian grids
(i.e.~InstantNGP), with those that employ points equipped with features as a
way to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We
achieve this by incorporating a point-based representation into the
high-resolution layers of the hierarchical hash tables of an InstantNGP
representation. As our points are equipped with a field of influence, our
representation can be interpreted as a mixture of Gaussians stored within the
hash table. We propose a loss that encourages the movement of our Gaussians
towards regions that require more representation budget to be sufficiently well
represented. Our main finding is that our representation allows the
reconstruction of signals using a more compact representation without
compromising quality.

Comments:
- Project page: https://theialab.github.io/laghashes/

---

## Neural Surface Reconstruction and Rendering for LiDAR-Visual Systems

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Jianheng Liu, Chunran Zheng, Yunfei Wan, Bowen Wang, Yixi Cai, Fu Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2409.05310v1){: .btn .btn-green } |

**Abstract**: This paper presents a unified surface reconstruction and rendering framework
for LiDAR-visual systems, integrating Neural Radiance Fields (NeRF) and Neural
Distance Fields (NDF) to recover both appearance and structural information
from posed images and point clouds. We address the structural visible gap
between NeRF and NDF by utilizing a visible-aware occupancy map to classify
space into the free, occupied, visible unknown, and background regions. This
classification facilitates the recovery of a complete appearance and structure
of the scene. We unify the training of the NDF and NeRF using a spatial-varying
scale SDF-to-density transformation for levels of detail for both structure and
appearance. The proposed method leverages the learned NDF for structure-aware
NeRF training by an adaptive sphere tracing sampling strategy for accurate
structure rendering. In return, NeRF further refines structural in recovering
missing or fuzzy structures in the NDF. Extensive experiments demonstrate the
superior quality and versatility of the proposed method across various
scenarios. To benefit the community, the codes will be released at
\url{https://github.com/hku-mars/M2Mapping}.



---

## G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel  View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Lutao Jiang, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.05617v1){: .btn .btn-green } |

**Abstract**: Following the burgeoning interest in implicit neural representation, Neural
Light Field (NeLF) has been introduced to predict the color of a ray directly.
Unlike Neural Radiance Field (NeRF), NeLF does not create a point-wise
representation by predicting color and volume density for each point in space.
However, the current NeLF methods face a challenge as they need to train a NeRF
model first and then synthesize over 10K views to train NeLF for improved
performance. Additionally, the rendering quality of NeLF methods is lower
compared to NeRF methods. In this paper, we propose G-NeLF, a versatile
grid-based NeLF approach that utilizes spatial-aware features to unleash the
potential of the neural network's inference capability, and consequently
overcome the difficulties of NeLF training. Specifically, we employ a
spatial-aware feature sequence derived from a meticulously crafted grid as the
ray's representation. Drawing from our empirical studies on the adaptability of
multi-resolution hash tables, we introduce a novel grid-based ray
representation for NeLF that can represent the entire space with a very limited
number of parameters. To better utilize the sequence feature, we design a
lightweight ray color decoder that simulates the ray propagation process,
enabling a more efficient inference of the ray's color. G-NeLF can be trained
without necessitating significant storage overhead and with the model size of
only 0.95 MB to surpass previous state-of-the-art NeLF. Moreover, compared with
grid-based NeRF methods, e.g., Instant-NGP, we only utilize one-tenth of its
parameters to achieve higher performance. Our code will be released upon
acceptance.



---

## KRONC: Keypoint-based Robust Camera Optimization for 3D Car  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Davide Di Nucci, Alessandro Simoni, Matteo Tomei, Luca Ciuffreda, Roberto Vezzani, Rita Cucchiara | cs.CV | [PDF](http://arxiv.org/pdf/2409.05407v1){: .btn .btn-green } |

**Abstract**: The three-dimensional representation of objects or scenes starting from a set
of images has been a widely discussed topic for years and has gained additional
attention after the diffusion of NeRF-based approaches. However, an
underestimated prerequisite is the knowledge of camera poses or, more
specifically, the estimation of the extrinsic calibration parameters. Although
excellent general-purpose Structure-from-Motion methods are available as a
pre-processing step, their computational load is high and they require a lot of
frames to guarantee sufficient overlapping among the views. This paper
introduces KRONC, a novel approach aimed at inferring view poses by leveraging
prior knowledge about the object to reconstruct and its representation through
semantic keypoints. With a focus on vehicle scenes, KRONC is able to estimate
the position of the views as a solution to a light optimization problem
targeting the convergence of keypoints' back-projections to a singular point.
To validate the method, a specific dataset of real-world car scenes has been
collected. Experiments confirm KRONC's ability to generate excellent estimates
of camera poses starting from very coarse initialization. Results are
comparable with Structure-from-Motion methods with huge savings in computation.
Code and data will be made publicly available.

Comments:
- Accepted at ECCVW

---

## From Words to Poses: Enhancing Novel Object Pose Estimation with Vision  Language Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Tessa Pulli, Stefan Thalhammer, Simon Schwaiger, Markus Vincze | cs.CV | [PDF](http://arxiv.org/pdf/2409.05413v1){: .btn .btn-green } |

**Abstract**: Robots are increasingly envisioned to interact in real-world scenarios, where
they must continuously adapt to new situations. To detect and grasp novel
objects, zero-shot pose estimators determine poses without prior knowledge.
Recently, vision language models (VLMs) have shown considerable advances in
robotics applications by establishing an understanding between language input
and image input. In our work, we take advantage of VLMs zero-shot capabilities
and translate this ability to 6D object pose estimation. We propose a novel
framework for promptable zero-shot 6D object pose estimation using language
embeddings. The idea is to derive a coarse location of an object based on the
relevancy map of a language-embedded NeRF reconstruction and to compute the
pose estimate with a point cloud registration method. Additionally, we provide
an analysis of LERF's suitability for open-set object pose estimation. We
examine hyperparameters, such as activation thresholds for relevancy maps and
investigate the zero-shot capabilities on an instance- and category-level.
Furthermore, we plan to conduct robotic grasping experiments in a real-world
setting.



---

## Online 3D reconstruction and dense tracking in endoscopic videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-09 | Michel Hayoz, Christopher Hahne, Thomas Kurmann, Max Allan, Guido Beldi, Daniel Candinas, ablo MÃ¡rquez-Neila, Raphael Sznitman | cs.CV | [PDF](http://arxiv.org/pdf/2409.06037v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction from stereo endoscopic video data is crucial for
advancing surgical interventions. In this work, we present an online framework
for online, dense 3D scene reconstruction and tracking, aimed at enhancing
surgical scene understanding and assisting interventions. Our method
dynamically extends a canonical scene representation using Gaussian splatting,
while modeling tissue deformations through a sparse set of control points. We
introduce an efficient online fitting algorithm that optimizes the scene
parameters, enabling consistent tracking and accurate reconstruction. Through
experiments on the StereoMIS dataset, we demonstrate the effectiveness of our
approach, outperforming state-of-the-art tracking methods and achieving
comparable performance to offline reconstruction techniques. Our work enables
various downstream applications thus contributing to advancing the capabilities
of surgical assistance systems.



---

## GS-PT: Exploiting 3D Gaussian Splatting for Comprehensive Point Cloud  Understanding via Self-supervised Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-08 | Keyi Liu, Yeqi Luo, Weidong Yang, Jingyi Xu, Zhijun Li, Wen-Ming Chen, Ben Fei | cs.CV | [PDF](http://arxiv.org/pdf/2409.04963v1){: .btn .btn-green } |

**Abstract**: Self-supervised learning of point cloud aims to leverage unlabeled 3D data to
learn meaningful representations without reliance on manual annotations.
However, current approaches face challenges such as limited data diversity and
inadequate augmentation for effective feature learning. To address these
challenges, we propose GS-PT, which integrates 3D Gaussian Splatting (3DGS)
into point cloud self-supervised learning for the first time. Our pipeline
utilizes transformers as the backbone for self-supervised pre-training and
introduces novel contrastive learning tasks through 3DGS. Specifically, the
transformers aim to reconstruct the masked point cloud. 3DGS utilizes
multi-view rendered images as input to generate enhanced point cloud
distributions and novel view images, facilitating data augmentation and
cross-modal contrastive learning. Additionally, we incorporate features from
depth maps. By optimizing these tasks collectively, our method enriches the
tri-modal self-supervised learning process, enabling the model to leverage the
correlation across 3D point clouds and 2D images from various modalities. We
freeze the encoder after pre-training and test the model's performance on
multiple downstream tasks. Experimental results indicate that GS-PT outperforms
the off-the-shelf self-supervised learning methods on various downstream tasks
including 3D object classification, real-world classifications, and few-shot
learning and segmentation.



---

## DreamMapping: High-Fidelity Text-to-3D Generation via Variational  Distribution Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-08 | Zeyu Cai, Duotun Wang, Yixun Liang, Zhijing Shao, Ying-Cong Chen, Xiaohang Zhan, Zeyu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2409.05099v3){: .btn .btn-green } |

**Abstract**: Score Distillation Sampling (SDS) has emerged as a prevalent technique for
text-to-3D generation, enabling 3D content creation by distilling
view-dependent information from text-to-2D guidance. However, they frequently
exhibit shortcomings such as over-saturated color and excess smoothness. In
this paper, we conduct a thorough analysis of SDS and refine its formulation,
finding that the core design is to model the distribution of rendered images.
Following this insight, we introduce a novel strategy called Variational
Distribution Mapping (VDM), which expedites the distribution modeling process
by regarding the rendered images as instances of degradation from
diffusion-based generation. This special design enables the efficient training
of variational distribution by skipping the calculations of the Jacobians in
the diffusion U-Net. We also introduce timestep-dependent Distribution
Coefficient Annealing (DCA) to further improve distilling precision. Leveraging
VDM and DCA, we use Gaussian Splatting as the 3D representation and build a
text-to-3D generation framework. Extensive experiments and evaluations
demonstrate the capability of VDM and DCA to generate high-fidelity and
realistic assets with optimization efficiency.

Comments:
- 15 pages, 14 figures

---

## Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for  Fisheye Cameras

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-07 | Zimu Liao, Siyan Chen, Rong Fu, Yi Wang, Zhongling Su, Hao Luo, Li Ma, Linning Xu, Bo Dai, Hengjie Li, Zhilin Pei, Xingcheng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2409.04751v2){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high
fidelity and real-time rendering. However, adapting 3DGS to different camera
models, particularly fisheye lenses, poses challenges due to the unique 3D to
2D projection calculation. Additionally, there are inefficiencies in the
tile-based splatting, especially for the extreme curvature and wide field of
view of fisheye lenses, which are crucial for its broader real-life
applications. To tackle these challenges, we introduce Fisheye-GS.This
innovative method recalculates the projection transformation and its gradients
for fisheye cameras. Our approach can be seamlessly integrated as a module into
other efficient 3D rendering methods, emphasizing its extensibility,
lightweight nature, and modular design. Since we only modified the projection
component, it can also be easily adapted for use with different camera models.
Compared to methods that train after undistortion, our approach demonstrates a
clear improvement in visual quality.



---

## 3D-GP-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian  Geometric Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-06 | Yujun Huang, Bin Chen, Niu Lian, Baoyi An, Shu-Tao Xia | cs.CV | [PDF](http://arxiv.org/pdf/2409.04013v1){: .btn .btn-green } |

**Abstract**: Multi-view image compression is vital for 3D-related applications. To
effectively model correlations between views, existing methods typically
predict disparity between two views on a 2D plane, which works well for small
disparities, such as in stereo images, but struggles with larger disparities
caused by significant view changes. To address this, we propose a novel
approach: learning-based multi-view image coding with 3D Gaussian geometric
priors (3D-GP-LMVIC). Our method leverages 3D Gaussian Splatting to derive
geometric priors of the 3D scene, enabling more accurate disparity estimation
across views within the compression model. Additionally, we introduce a depth
map compression model to reduce redundancy in geometric information between
views. A multi-view sequence ordering method is also proposed to enhance
correlations between adjacent views. Experimental results demonstrate that
3D-GP-LMVIC surpasses both traditional and learning-based methods in
performance, while maintaining fast encoding and decoding speed.

Comments:
- 19pages, 8 figures, conference

---

## SCARF: Scalable Continual Learning Framework for Memory-efficient  Multiple Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-06 | Yuze Wang, Junyi Wang, Chen Wang, Wantong Duan, Yongtang Bao, Yue Qi | cs.CV | [PDF](http://arxiv.org/pdf/2409.04482v1){: .btn .btn-green } |

**Abstract**: This paper introduces a novel continual learning framework for synthesising
novel views of multiple scenes, learning multiple 3D scenes incrementally, and
updating the network parameters only with the training data of the upcoming new
scene. We build on Neural Radiance Fields (NeRF), which uses multi-layer
perceptron to model the density and radiance field of a scene as the implicit
function. While NeRF and its extensions have shown a powerful capability of
rendering photo-realistic novel views in a single 3D scene, managing these
growing 3D NeRF assets efficiently is a new scientific problem. Very few works
focus on the efficient representation or continuous learning capability of
multiple scenes, which is crucial for the practical applications of NeRF. To
achieve these goals, our key idea is to represent multiple scenes as the linear
combination of a cross-scene weight matrix and a set of scene-specific weight
matrices generated from a global parameter generator. Furthermore, we propose
an uncertain surface knowledge distillation strategy to transfer the radiance
field knowledge of previous scenes to the new model. Representing multiple 3D
scenes with such weight matrices significantly reduces memory requirements. At
the same time, the uncertain surface distillation strategy greatly overcomes
the catastrophic forgetting problem and maintains the photo-realistic rendering
quality of previous scenes. Experiments show that the proposed approach
achieves state-of-the-art rendering quality of continual learning NeRF on
NeRF-Synthetic, LLFF, and TanksAndTemples datasets while preserving extra low
storage cost.



---

## GST: Precise 3D Human Body from a Single Image with Gaussian Splatting  Transformers

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-06 | Lorenza Prospero, Abdullah Hamdi, Joao F. Henriques, Christian Rupprecht | cs.CV | [PDF](http://arxiv.org/pdf/2409.04196v1){: .btn .btn-green } |

**Abstract**: Reconstructing realistic 3D human models from monocular images has
significant applications in creative industries, human-computer interfaces, and
healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene
representation composed of a mixture of Gaussians. Predicting such mixtures for
a human from a single input image is challenging, as it is a non-uniform
density (with a many-to-one relationship with input pixels) with strict
physical constraints. At the same time, it needs to be flexible to accommodate
a variety of clothes and poses. Our key observation is that the vertices of
standardized human meshes (such as SMPL) can provide an adequate density and
approximate initial position for Gaussians. We can then train a transformer
model to jointly predict comparatively small adjustments to these positions, as
well as the other Gaussians' attributes and the SMPL parameters. We show
empirically that this combination (using only multi-view supervision) can
achieve fast inference of 3D human models from a single image without test-time
optimization, expensive diffusion models, or 3D points supervision. We also
show that it can improve 3D pose estimation by better fitting human models that
account for clothes and other variations. The code is available on the project
website https://abdullahamdi.com/gst/ .

Comments:
- preprint

---

## LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model  Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-05 | Hanyang Yu, Xiaoxiao Long, Ping Tan | cs.CV | [PDF](http://arxiv.org/pdf/2409.03456v1){: .btn .btn-green } |

**Abstract**: We aim to address sparse-view reconstruction of a 3D scene by leveraging
priors from large-scale vision models. While recent advancements such as 3D
Gaussian Splatting (3DGS) have demonstrated remarkable successes in 3D
reconstruction, these methods typically necessitate hundreds of input images
that densely capture the underlying scene, making them time-consuming and
impractical for real-world applications. However, sparse-view reconstruction is
inherently ill-posed and under-constrained, often resulting in inferior and
incomplete outcomes. This is due to issues such as failed initialization,
overfitting on input images, and a lack of details. To mitigate these
challenges, we introduce LM-Gaussian, a method capable of generating
high-quality reconstructions from a limited number of images. Specifically, we
propose a robust initialization module that leverages stereo priors to aid in
the recovery of camera poses and the reliable point clouds. Additionally, a
diffusion-based refinement is iteratively applied to incorporate image
diffusion priors into the Gaussian optimization process to preserve intricate
scene details. Finally, we utilize video diffusion priors to further enhance
the rendered images for realistic visual effects. Overall, our approach
significantly reduces the data acquisition requirements compared to previous
3DGS methods. We validate the effectiveness of our framework through
experiments on various public datasets, demonstrating its potential for
high-quality 360-degree scene reconstruction. Visual results are on our
website.

Comments:
- Project page: https://hanyangyu1021.github.io/lm-gaussian.github.io/

---

## Weight Conditioning for Smooth Optimization of Neural Networks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-05 | Hemanth Saratchandran, Thomas X. Wang, Simon Lucey | cs.CV | [PDF](http://arxiv.org/pdf/2409.03424v1){: .btn .btn-green } |

**Abstract**: In this article, we introduce a novel normalization technique for neural
network weight matrices, which we term weight conditioning. This approach aims
to narrow the gap between the smallest and largest singular values of the
weight matrices, resulting in better-conditioned matrices. The inspiration for
this technique partially derives from numerical linear algebra, where
well-conditioned matrices are known to facilitate stronger convergence results
for iterative solvers. We provide a theoretical foundation demonstrating that
our normalization technique smoothens the loss landscape, thereby enhancing
convergence of stochastic gradient descent algorithms. Empirically, we validate
our normalization across various neural network architectures, including
Convolutional Neural Networks (CNNs), Vision Transformers (ViT), Neural
Radiance Fields (NeRF), and 3D shape modeling. Our findings indicate that our
normalization method is not only competitive but also outperforms existing
weight normalization techniques from the literature.

Comments:
- ECCV 2024

---

## Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-05 | Shen Chen, Jiale Zhou, Lei Li | cs.CV | [PDF](http://arxiv.org/pdf/2409.03213v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for 3D scene
representation, offering a reduction in computational overhead compared to
Neural Radiance Fields (NeRF). However, 3DGS is susceptible to high-frequency
artifacts and demonstrates suboptimal performance under sparse viewpoint
conditions, thereby limiting its applicability in robotics and computer vision.
To address these limitations, we introduce SVS-GS, a novel framework for Sparse
Viewpoint Scene reconstruction that integrates a 3D Gaussian smoothing filter
to suppress artifacts. Furthermore, our approach incorporates a Depth Gradient
Profile Prior (DGPP) loss with a dynamic depth mask to sharpen edges and 2D
diffusion with Score Distillation Sampling (SDS) loss to enhance geometric
consistency in novel view synthesis. Experimental evaluations on the
MipNeRF-360 and SeaThru-NeRF datasets demonstrate that SVS-GS markedly improves
3D reconstruction from sparse viewpoints, offering a robust and efficient
solution for scene understanding in robotics and computer vision applications.



---

## GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous  Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Huasong Han, Kaixuan Zhou, Xiaoxiao Long, Yusen Wang, Chunxia Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2409.02382v1){: .btn .btn-green } |

**Abstract**: We propose GGS, a Generalizable Gaussian Splatting method for Autonomous
Driving which can achieve realistic rendering under large viewpoint changes.
Previous generalizable 3D gaussian splatting methods are limited to rendering
novel views that are very close to the original pair of images, which cannot
handle large differences in viewpoint. Especially in autonomous driving
scenarios, images are typically collected from a single lane. The limited
training perspective makes rendering images of a different lane very
challenging. To further improve the rendering capability of GGS under large
viewpoint changes, we introduces a novel virtual lane generation module into
GSS method to enables high-quality lane switching even without a multi-lane
dataset. Besides, we design a diffusion loss to supervise the generation of
virtual lane image to further address the problem of lack of data in the
virtual lanes. Finally, we also propose a depth refinement module to optimize
depth estimation in the GSS model. Extensive validation of our method, compared
to existing approaches, demonstrates state-of-the-art performance.



---

## Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video  Diffusion Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Zhibin Liu, Haoye Dong, Aviral Chharia, Hefeng Wu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02851v1){: .btn .btn-green } |

**Abstract**: Generating lifelike 3D humans from a single RGB image remains a challenging
task in computer vision, as it requires accurate modeling of geometry,
high-quality texture, and plausible unseen parts. Existing methods typically
use multi-view diffusion models for 3D generation, but they often face
inconsistent view issues, which hinder high-quality 3D human generation. To
address this, we propose Human-VDM, a novel method for generating 3D human from
a single RGB image using Video Diffusion Models. Human-VDM provides temporally
consistent views for 3D human generation using Gaussian Splatting. It consists
of three modules: a view-consistent human video diffusion module, a video
augmentation module, and a Gaussian Splatting module. First, a single image is
fed into a human video diffusion module to generate a coherent human video.
Next, the video augmentation module applies super-resolution and video
interpolation to enhance the textures and geometric smoothness of the generated
video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans
under the guidance of these high-resolution and view-consistent images.
Experiments demonstrate that Human-VDM achieves high-quality 3D human from a
single image, outperforming state-of-the-art methods in both generation quality
and quantity. Project page: https://human-vdm.github.io/Human-VDM/

Comments:
- 14 Pages, 8 figures, Project page:
  https://human-vdm.github.io/Human-VDM/

---

## Object Gaussian for Monocular 6D Pose Estimation from Sparse Views


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Luqing Luo, Shichu Sun, Jiangang Yang, Linfang Zheng, Jinwei Du, Jian Liu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02581v1){: .btn .btn-green } |

**Abstract**: Monocular object pose estimation, as a pivotal task in computer vision and
robotics, heavily depends on accurate 2D-3D correspondences, which often demand
costly CAD models that may not be readily available. Object 3D reconstruction
methods offer an alternative, among which recent advancements in 3D Gaussian
Splatting (3DGS) afford a compelling potential. Yet its performance still
suffers and tends to overfit with fewer input views. Embracing this challenge,
we introduce SGPose, a novel framework for sparse view object pose estimation
using Gaussian-based methods. Given as few as ten views, SGPose generates a
geometric-aware representation by starting with a random cuboid initialization,
eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as
required by traditional 3DGS methods. SGPose removes the dependence on CAD
models by regressing dense 2D-3D correspondences between images and the
reconstructed model from sparse input and random initialization, while the
geometric-consistent depth supervision and online synthetic view warping are
key to the success. Experiments on typical benchmarks, especially on the
Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods
even under sparse view constraints, under-scoring its potential in real-world
applications.



---

## UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from  Endoscopic Sparse Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-04 | Jiaxin Guo, Jiangliu Wang, Ruofeng Wei, Di Kang, Qi Dou, Yun-hui Liu | cs.CV | [PDF](http://arxiv.org/pdf/2409.02917v1){: .btn .btn-green } |

**Abstract**: Visualizing surgical scenes is crucial for revealing internal anatomical
structures during minimally invasive procedures. Novel View Synthesis is a
vital technique that offers geometry and appearance reconstruction, enhancing
understanding, planning, and decision-making in surgical scenes. Despite the
impressive achievements of Neural Radiance Field (NeRF), its direct application
to surgical scenes produces unsatisfying results due to two challenges:
endoscopic sparse views and significant photometric inconsistencies. In this
paper, we propose uncertainty-aware conditional NeRF for novel view synthesis
to tackle the severe shape-radiance ambiguity from sparse surgical views. The
core of UC-NeRF is to incorporate the multi-view uncertainty estimation to
condition the neural radiance field for modeling the severe photometric
inconsistencies adaptively. Specifically, our UC-NeRF first builds a
consistency learner in the form of multi-view stereo network, to establish the
geometric correspondence from sparse views and generate uncertainty estimation
and feature priors. In neural rendering, we design a base-adaptive NeRF network
to exploit the uncertainty estimation for explicitly handling the photometric
inconsistencies. Furthermore, an uncertainty-guided geometry distillation is
employed to enhance geometry learning. Experiments on the SCARED and Hamlyn
datasets demonstrate our superior performance in rendering appearance and
geometry, consistently outperforming the current state-of-the-art approaches.
Our code will be released at \url{https://github.com/wrld/UC-NeRF}.



---

## $S^2$NeRF: Privacy-preserving Training Framework for NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Bokang Zhang, Yanglin Zhang, Zhikun Zhang, Jinglan Yang, Lingying Huang, Junfeng Wu | cs.CR | [PDF](http://arxiv.org/pdf/2409.01661v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and
graphics, facilitating novel view synthesis and influencing sectors like
extended reality and e-commerce. However, NeRF's dependence on extensive data
collection, including sensitive scene image data, introduces significant
privacy risks when users upload this data for model training. To address this
concern, we first propose SplitNeRF, a training framework that incorporates
split learning (SL) techniques to enable privacy-preserving collaborative model
training between clients and servers without sharing local data. Despite its
benefits, we identify vulnerabilities in SplitNeRF by developing two attack
methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which
exploit the shared gradient data and a few leaked scene images to reconstruct
private scene information. To counter these threats, we introduce $S^2$NeRF,
secure SplitNeRF that integrates effective defense mechanisms. By introducing
decaying noise related to the gradient norm into the shared gradient
information, $S^2$NeRF preserves privacy while maintaining a high utility of
the NeRF model. Our extensive evaluations across multiple datasets demonstrate
the effectiveness of $S^2$NeRF against privacy breaches, confirming its
viability for secure NeRF training in sensitive applications.

Comments:
- To appear in the ACM Conference on Computer and Communications
  Security (CCS'24), October 14-18, 2024, Salt Lake City, UT, USA

---

## GraspSplats: Efficient Manipulation with 3D Feature Splatting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Mazeyu Ji, Ri-Zhao Qiu, Xueyan Zou, Xiaolong Wang | cs.RO | [PDF](http://arxiv.org/pdf/2409.02084v1){: .btn .btn-green } |

**Abstract**: The ability for robots to perform efficient and zero-shot grasping of object
parts is crucial for practical applications and is becoming prevalent with
recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap
for representations to support such a capability, existing methods rely on
neural fields (NeRFs) via differentiable rendering or point-based projection
methods. However, we demonstrate that NeRFs are inappropriate for scene changes
due to their implicitness and point-based methods are inaccurate for part
localization without rendering-based optimization. To amend these issues, we
propose GraspSplats. Using depth supervision and a novel reference feature
computation method, GraspSplats generates high-quality scene representations in
under 60 seconds. We further validate the advantages of Gaussian-based
representation by showing that the explicit and optimized geometry in
GraspSplats is sufficient to natively support (1) real-time grasp sampling and
(2) dynamic and articulated object manipulation with point trackers. With
extensive experiments on a Franka robot, we demonstrate that GraspSplats
significantly outperforms existing methods under diverse task settings. In
particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO,
and 2D detection methods.

Comments:
- Project webpage: https://graspsplats.github.io/

---

## GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color  Point Clouds via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Zixuan Guo, Yifan Xie, Weijing Xie, Peng Huang, Fei Ma, Fei Richard Yu | cs.RO | [PDF](http://arxiv.org/pdf/2409.01581v1){: .btn .btn-green } |

**Abstract**: Dense colored point clouds enhance visual perception and are of significant
value in various robotic applications. However, existing learning-based point
cloud upsampling methods are constrained by computational resources and batch
processing strategies, which often require subdividing point clouds into
smaller patches, leading to distortions that degrade perceptual quality. To
address this challenge, we propose a novel 2D-3D hybrid colored point cloud
upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for
robotic perception. This approach leverages 3DGS to bridge 3D point clouds with
their 2D rendered images in robot vision systems. A dual scale rendered image
restoration network transforms sparse point cloud renderings into dense
representations, which are then input into 3DGS along with precise robot camera
poses and interpolated sparse point clouds to reconstruct dense 3D point
clouds. We have made a series of enhancements to the vanilla 3DGS, enabling
precise control over the number of points and significantly boosting the
quality of the upsampled point cloud for robotic scene understanding. Our
framework supports processing entire point clouds on a single consumer-grade
GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation
and thus producing high-quality, dense colored point clouds with millions of
points for robot navigation and manipulation tasks. Extensive experimental
results on generating million-level point cloud data validate the effectiveness
of our method, substantially improving the quality of colored point clouds and
demonstrating significant potential for applications involving large-scale
point clouds in autonomous robotics and human-robot interaction scenarios.

Comments:
- 7 pages, 5 figures

---

## DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, Laura Leal-TaixÃ© | cs.CV | [PDF](http://arxiv.org/pdf/2409.02104v1){: .btn .btn-green } |

**Abstract**: Reconstructing scenes and tracking motion are two sides of the same coin.
Tracking points allow for geometric reconstruction [14], while geometric
reconstruction of (dynamic) scenes allows for 3D tracking of points over time
[24, 39]. The latter was recently also exploited for 2D point tracking to
overcome occlusion ambiguities by lifting tracking directly into 3D [38].
However, above approaches either require offline processing or multi-view
camera setups both unrealistic for real-world applications like robot
navigation or mixed reality. We target the challenge of online 2D and 3D point
tracking from unposed monocular camera input introducing Dynamic Online
Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to
reconstruct dynamic scenes in an online fashion. Our approach extends 3D
Gaussians to capture new content and object motions while estimating camera
movements from a single RGB frame. DynOMo stands out by enabling emergence of
point trajectories through robust image feature reconstruction and a novel
similarity-enhanced regularization term, without requiring any
correspondence-level supervision. It sets the first baseline for online point
tracking with monocular unposed cameras, achieving performance on par with
existing methods. We aim to inspire the community to advance online point
tracking and reconstruction, expanding the applicability to diverse real-world
scenarios.



---

## PRoGS: Progressive Rendering of Gaussian Splats

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-03 | Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels | cs.CV | [PDF](http://arxiv.org/pdf/2409.01761v1){: .btn .btn-green } |

**Abstract**: Over the past year, 3D Gaussian Splatting (3DGS) has received significant
attention for its ability to represent 3D scenes in a perceptually accurate
manner. However, it can require a substantial amount of storage since each
splat's individual data must be stored. While compression techniques offer a
potential solution by reducing the memory footprint, they still necessitate
retrieving the entire scene before any part of it can be rendered. In this
work, we introduce a novel approach for progressively rendering such scenes,
aiming to display visible content that closely approximates the final scene as
early as possible without loading the entire scene into memory. This approach
benefits both on-device rendering applications limited by memory constraints
and streaming applications where minimal bandwidth usage is preferred. To
achieve this, we approximate the contribution of each Gaussian to the final
scene and construct an order of prioritization on their inclusion in the
rendering process. Additionally, we demonstrate that our approach can be
combined with existing compression methods to progressively render (and stream)
3DGS scenes, optimizing bandwidth usage by focusing on the most important
splats within a scene. Overall, our work establishes a foundation for making
remotely hosted 3DGS content more quickly accessible to end-users in
over-the-top consumption scenarios, with our results showing significant
improvements in quality across all metrics compared to existing methods.



---

## Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian  Splatting for Dynamic Surgical Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-09-02 | Qian Li, Shuojue Yang, Daiyun Shen, Yueming Jin | cs.CV | [PDF](http://arxiv.org/pdf/2409.01003v1){: .btn .btn-green } |

**Abstract**: Reconstructing endoscopic videos is crucial for high-fidelity visualization
and the efficiency of surgical operations. Despite the importance, existing 3D
reconstruction methods encounter several challenges, including stringent
demands for accuracy, imprecise camera positioning, intricate dynamic scenes,
and the necessity for rapid reconstruction. Addressing these issues, this paper
presents the first camera-pose-free scene reconstruction framework, Free-DyGS,
tailored for dynamic surgical videos, leveraging 3D Gaussian splatting
technology. Our approach employs a frame-by-frame reconstruction strategy and
is delineated into four distinct phases: Scene Initialization, Joint Learning,
Scene Expansion, and Retrospective Learning. We introduce a Generalizable
Gaussians Parameterization module within the Scene Initialization and Expansion
phases to proficiently generate Gaussian attributes for each pixel from the
RGBD frames. The Joint Learning phase is crafted to concurrently deduce scene
deformation and camera pose, facilitated by an innovative flexible deformation
module. In the scene expansion stage, the Gaussian points gradually grow as the
camera moves. The Retrospective Learning phase is dedicated to enhancing the
precision of scene deformation through the reassessment of prior frames. The
efficacy of the proposed Free-DyGS is substantiated through experiments on two
datasets: the StereoMIS and Hamlyn datasets. The experimental outcomes
underscore that Free-DyGS surpasses conventional baseline models in both
rendering fidelity and computational efficiency.


