---
layout: default
title: December 2024
parent: Papers
nav_order: 202412
---

<!---metadata--->


## SuperGSeg: Open-Vocabulary 3D Segmentation with Structured  Super-Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2412.10231v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently gained traction for its efficient training
and real-time rendering. While the vanilla Gaussian Splatting representation is
mainly designed for view synthesis, more recent works investigated how to
extend it with scene understanding and language features. However, existing
methods lack a detailed comprehension of scenes, limiting their ability to
segment and interpret complex structures. To this end, We introduce SuperGSeg,
a novel approach that fosters cohesive, context-aware scene representation by
disentangling segmentation and language field distillation. SuperGSeg first
employs neural Gaussians to learn instance and hierarchical segmentation
features from multi-view images with the aid of off-the-shelf 2D masks. These
features are then leveraged to create a sparse set of what we call
Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language
features into 3D space. Through Super-Gaussians, our method enables
high-dimensional language feature rendering without extreme increases in GPU
memory. Extensive experiments demonstrate that SuperGSeg outperforms prior
works on both open-vocabulary object localization and semantic segmentation
tasks.

Comments:
- 13 pages, 8 figures

---

## Sharpening Your Density Fields: Spiking Neuron Aided Fast Geometry  Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Yi Gu, Zhaorui Wang, Dongjun Ye, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2412.09881v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have achieved remarkable progress in neural
rendering. Extracting geometry from NeRF typically relies on the Marching Cubes
algorithm, which uses a hand-crafted threshold to define the level set.
However, this threshold-based approach requires laborious and scenario-specific
tuning, limiting its practicality for real-world applications. In this work, we
seek to enhance the efficiency of this method during the training time. To this
end, we introduce a spiking neuron mechanism that dynamically adjusts the
threshold, eliminating the need for manual selection. Despite its promise,
directly training with the spiking neuron often results in model collapse and
noisy outputs. To overcome these challenges, we propose a round-robin strategy
that stabilizes the training process and enables the geometry network to
achieve a sharper and more precise density distribution with minimal
computational overhead. We validate our approach through extensive experiments
on both synthetic and real-world datasets. The results show that our method
significantly improves the performance of threshold-based techniques, offering
a more robust and efficient solution for NeRF geometry extraction.



---

## GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner | cs.CV | [PDF](http://arxiv.org/pdf/2412.10209v1){: .btn .btn-green } |

**Abstract**: We propose a novel approach for reconstructing animatable 3D Gaussian avatars
from monocular videos captured by commodity devices like smartphones.
Photorealistic 3D head avatar reconstruction from such recordings is
challenging due to limited observations, which leaves unobserved regions
under-constrained and can lead to artifacts in novel views. To address this
problem, we introduce a multi-view head diffusion model, leveraging its priors
to fill in missing regions and ensure view consistency in Gaussian splatting
renderings. To enable precise viewpoint control, we use normal maps rendered
from FLAME-based head reconstruction, which provides pixel-aligned inductive
biases. We also condition the diffusion model on VAE features extracted from
the input image to preserve details of facial identity and appearance. For
Gaussian avatar reconstruction, we distill multi-view diffusion priors by using
iteratively denoised images as pseudo-ground truths, effectively mitigating
over-saturation issues. To further improve photorealism, we apply latent
upsampling to refine the denoised latent before decoding it into an image. We
evaluate our method on the NeRSemble dataset, showing that GAF outperforms the
previous state-of-the-art methods in novel view synthesis by a 5.34\% higher
SSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions
from monocular videos captured on commodity devices.

Comments:
- Paper Video: https://youtu.be/QuIYTljvhyg Project Page:
  https://tangjiapeng.github.io/projects/GAF

---

## RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Lizhi Bai, Chunqi Tian, Jun Yang, Siyu Zhang, Masanori Suganuma, Takayuki Okatani | cs.RO | [PDF](http://arxiv.org/pdf/2412.09868v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as a promising technique for high-quality
3D rendering, leading to increasing interest in integrating 3DGS into realism
SLAM systems. However, existing methods face challenges such as Gaussian
primitives redundancy, forgetting problem during continuous optimization, and
difficulty in initializing primitives in monocular case due to lack of depth
information. In order to achieve efficient and photorealistic mapping, we
propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular
and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian
primitives optimization and consists of three key components. Firstly, we
propose an efficient incremental mapping approach to achieve a compact and
accurate representation of the scene through adaptive sampling and Gaussian
primitives filtering. Secondly, a dynamic window optimization method is
proposed to mitigate the forgetting problem and improve map consistency.
Finally, for the monocular case, a monocular keyframe initialization method
based on sparse point cloud is proposed to improve the initialization accuracy
of Gaussian primitives, which provides a geometric basis for subsequent
optimization. The results of numerous experiments demonstrate that RP-SLAM
achieves state-of-the-art map rendering accuracy while ensuring real-time
performance and model compactness.



---

## TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting  from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Liang Zhao, Zehan Bao, Yi Xie, Hong Chen, Yaohui Chen, Weifu Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.10051v1){: .btn .btn-green } |

**Abstract**: Recent advances in Gaussian Splatting have significantly advanced the field,
achieving both panoptic and interactive segmentation of 3D scenes. However,
existing methodologies often overlook the critical need for reconstructing
specified targets with complex structures from sparse views. To address this
issue, we introduce TSGaussian, a novel framework that combines semantic
constraints with depth priors to avoid geometry degradation in challenging
novel view synthesis tasks. Our approach prioritizes computational resources on
designated targets while minimizing background allocation. Bounding boxes from
YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask
predictions, ensuring semantic accuracy and cost efficiency. TSGaussian
effectively clusters 3D gaussians by introducing a compact identity encoding
for each Gaussian ellipsoid and incorporating 3D spatial consistency
regularization. Leveraging these modules, we propose a pruning strategy to
effectively reduce redundancy in 3D gaussians. Extensive experiments
demonstrate that TSGaussian outperforms state-of-the-art methods on three
standard datasets and a new challenging dataset we collected, achieving
superior results in novel view synthesis of specific objects. Code is available
at: https://github.com/leon2000-ai/TSGaussian.



---

## SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D  Gaussians from Monocular Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim | cs.CV | [PDF](http://arxiv.org/pdf/2412.09982v1){: .btn .btn-green } |

**Abstract**: Synthesizing novel views from in-the-wild monocular videos is challenging due
to scene dynamics and the lack of multi-view cues. To address this, we propose
SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for
high-quality reconstruction and fast rendering from monocular videos. At its
core is a novel Motion-Adaptive Spline (MAS) method, which represents
continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a
small number of control points. For MAS, we introduce a Motion-Adaptive Control
points Pruning (MACP) method to model the deformation of each dynamic 3D
Gaussian across varying motions, progressively pruning control points while
maintaining dynamic modeling integrity. Additionally, we present a joint
optimization strategy for camera parameter estimation and 3D Gaussian
attributes, leveraging photometric and geometric consistency. This eliminates
the need for Structure-from-Motion preprocessing and enhances SplineGS's
robustness in real-world conditions. Experiments show that SplineGS
significantly outperforms state-of-the-art methods in novel view synthesis
quality for dynamic scenes from monocular videos, achieving thousands times
faster rendering speed.

Comments:
- The first two authors contributed equally to this work (equal
  contribution). The last two authors advised equally to this work. Please
  visit our project page at this https://kaist-viclab.github.io/splinegs-site/

---

## NeRF-Texture: Synthesizing Neural Radiance Field Textures

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Yi-Hua Huang, Yan-Pei Cao, Yu-Kun Lai, Ying Shan, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2412.10004v1){: .btn .btn-green } |

**Abstract**: Texture synthesis is a fundamental problem in computer graphics that would
benefit various applications. Existing methods are effective in handling 2D
image textures. In contrast, many real-world textures contain meso-structure in
the 3D geometry space, such as grass, leaves, and fabrics, which cannot be
effectively modeled using only 2D image textures. We propose a novel texture
synthesis method with Neural Radiance Fields (NeRF) to capture and synthesize
textures from given multi-view images. In the proposed NeRF texture
representation, a scene with fine geometric details is disentangled into the
meso-structure textures and the underlying base shape. This allows textures
with meso-structure to be effectively learned as latent features situated on
the base shape, which are fed into a NeRF decoder trained simultaneously to
represent the rich view-dependent appearance. Using this implicit
representation, we can synthesize NeRF-based textures through patch matching of
latent features. However, inconsistencies between the metrics of the
reconstructed content space and the latent feature space may compromise the
synthesis quality. To enhance matching performance, we further regularize the
distribution of latent features by incorporating a clustering constraint. In
addition to generating NeRF textures over a planar domain, our method can also
synthesize NeRF textures over curved surfaces, which are practically useful.
Experimental results and evaluations demonstrate the effectiveness of our
approach.



---

## MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative  Ego-Motion and Photorealistic 3D Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Xiaohao Xu, Feng Xue, Shibo Zhao, Yike Pan, Sebastian Scherer, Xiaonan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2412.09723v1){: .btn .btn-green } |

**Abstract**: Real-time multi-agent collaboration for ego-motion estimation and
high-fidelity 3D reconstruction is vital for scalable spatial intelligence.
However, traditional methods produce sparse, low-detail maps, while recent
dense mapping approaches struggle with high latency. To overcome these
challenges, we present MAC-Ego3D, a novel framework for real-time collaborative
photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D
enables agents to independently construct, align, and iteratively refine local
maps using a unified Gaussian splat representation. Through Intra-Agent
Gaussian Consensus, it enforces spatial coherence among neighboring Gaussian
splats within an agent. For global alignment, parallelized Inter-Agent Gaussian
Consensus, which asynchronously aligns and optimizes local maps by regularizing
multi-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D
model. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D
rendering, enabling rapid inter-agent Gaussian association and alignment.
MAC-Ego3D bridges local precision and global coherence, delivering higher
efficiency, largely reducing localization error, and improving mapping
fidelity. It establishes a new SOTA on synthetic and real-world benchmarks,
achieving a 15x increase in inference speed, order-of-magnitude reductions in
ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10
dB. Our code will be made publicly available at
https://github.com/Xiaohao-Xu/MAC-Ego3D .

Comments:
- 27 pages, 25 figures

---

## Feat2GS: Probing Visual Foundation Models with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu | cs.CV | [PDF](http://arxiv.org/pdf/2412.09606v1){: .btn .btn-green } |

**Abstract**: Given that visual foundation models (VFMs) are trained on extensive datasets
but often limited to 2D images, a natural question arises: how well do they
understand the 3D world? With the differences in architecture and training
protocols (i.e., objectives, proxy tasks), a unified framework to fairly and
comprehensively probe their 3D awareness is urgently needed. Existing works on
3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or
two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately,
these tasks ignore texture awareness, and require 3D data as ground-truth,
which limits the scale and diversity of their evaluation set. To address these
issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM
features extracted from unposed images. This allows us to probe 3D awareness
for geometry and texture via novel view synthesis, without requiring 3D data.
Additionally, the disentanglement of 3DGS parameters - geometry
($\boldsymbol{x}, \alpha, \Sigma$) and texture ($\boldsymbol{c}$) - enables
separate analysis of texture and geometry awareness. Under Feat2GS, we conduct
extensive experiments to probe the 3D awareness of several VFMs, and
investigate the ingredients that lead to a 3D aware VFM. Building on these
findings, we develop several variants that achieve state-of-the-art across
diverse datasets. This makes Feat2GS useful for probing VFMs, and as a
simple-yet-effective baseline for novel-view synthesis. Code and data will be
made available at https://fanegg.github.io/Feat2GS/.

Comments:
- Project Page: https://fanegg.github.io/Feat2GS/

---

## GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Dongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2412.09511v1){: .btn .btn-green } |

**Abstract**: Identifying affordance regions on 3D objects from semantic cues is essential
for robotics and human-machine interaction. However, existing 3D affordance
learning methods struggle with generalization and robustness due to limited
annotated data and a reliance on 3D backbones focused on geometric encoding,
which often lack resilience to real-world noise and data corruption. We propose
GEAL, a novel framework designed to enhance the generalization and robustness
of 3D affordance learning by leveraging large-scale pre-trained 2D models. We
employ a dual-branch architecture with Gaussian splatting to establish
consistent mappings between 3D point clouds and 2D representations, enabling
realistic 2D renderings from sparse point clouds. A granularity-adaptive fusion
module and a 2D-3D consistency alignment module further strengthen cross-modal
alignment and knowledge transfer, allowing the 3D branch to benefit from the
rich semantics and generalization capacity of 2D models. To holistically assess
the robustness, we introduce two new corruption-based benchmarks: PIAD-C and
LASO-C. Extensive experiments on public datasets and our benchmarks show that
GEAL consistently outperforms existing methods across seen and novel object
categories, as well as corrupted data, demonstrating robust and adaptable
affordance prediction under diverse conditions. Code and corruption datasets
have been made publicly available.

Comments:
- 22 pages, 8 figures, 12 tables; Project Page at
  https://dylanorange.github.io/projects/geal

---

## LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Haotian Mao, Zhuoxiong Xu, Siyue Wei, Yule Quan, Nianchen Deng, Xubo Yang | cs.HC | [PDF](http://arxiv.org/pdf/2412.09176v1){: .btn .btn-green } |

**Abstract**: Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has
shown immense potential in VR content creation due to its high-quality
rendering and efficient production process. However, existing physics-based
interaction systems for 3DGS can only perform simple and non-realistic
simulations or demand extensive user input for complex scenes, primarily due to
the absence of scene understanding. In this paper, we propose LIVE-GS, a highly
realistic interactive VR system powered by LLM. After object-aware GS
reconstruction, we prompt GPT-4o to analyze the physical properties of objects
in the scene, which are used to guide physical simulations consistent with real
phenomena. We also design a GPT-assisted GS inpainting module to fill the
unseen area covered by manipulative objects. To perform a precise segmentation
of Gaussian kernels, we propose a feature-mask segmentation strategy. To enable
rich interaction, we further propose a computationally efficient physical
simulation framework through an PBD-based unified interpolation method,
supporting various physical forms such as rigid body, soft body, and granular
materials. Our experimental results show that with the help of LLM's
understanding and enhancement of scenes, our VR system can support complex and
realistic interactions without additional manual design and annotation.



---

## LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video  Generation Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Yabo Chen, Chen Yang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Wei Shen, Wenrui Dai, Hongkai Xiong, Qi Tian | cs.CV | [PDF](http://arxiv.org/pdf/2412.09597v1){: .btn .btn-green } |

**Abstract**: Single-image 3D reconstruction remains a fundamental challenge in computer
vision due to inherent geometric ambiguities and limited viewpoint information.
Recent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D
priors learned from large-scale video data. However, leveraging these priors
effectively faces three key challenges: (1) degradation in quality across large
camera motions, (2) difficulties in achieving precise camera control, and (3)
geometric distortions inherent to the diffusion process that damage 3D
consistency. We address these challenges by proposing LiftImage3D, a framework
that effectively releases LVDMs' generative priors while ensuring 3D
consistency. Specifically, we design an articulated trajectory strategy to
generate video frames, which decomposes video sequences with large camera
motions into ones with controllable small motions. Then we use robust neural
matching models, i.e. MASt3R, to calibrate the camera poses of generated frames
and produce corresponding point clouds. Finally, we propose a distortion-aware
3D Gaussian splatting representation, which can learn independent distortions
between frames and output undistorted canonical Gaussians. Extensive
experiments demonstrate that LiftImage3D achieves state-of-the-art performance
on two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and
generalizes well to diverse in-the-wild images, from cartoon illustrations to
complex real-world scenes.

Comments:
- Project page: https://liftimage3d.github.io/

---

## FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Jiale Xu, Shenghua Gao, Ying Shan | cs.CV | [PDF](http://arxiv.org/pdf/2412.09573v1){: .btn .btn-green } |

**Abstract**: Existing sparse-view reconstruction models heavily rely on accurate known
camera poses. However, deriving camera extrinsics and intrinsics from
sparse-view images presents significant challenges. In this work, we present
FreeSplatter, a highly scalable, feed-forward reconstruction framework capable
of generating high-quality 3D Gaussians from uncalibrated sparse-view images
and recovering their camera parameters in mere seconds. FreeSplatter is built
upon a streamlined transformer architecture, comprising sequential
self-attention blocks that facilitate information exchange among multi-view
image tokens and decode them into pixel-wise 3D Gaussian primitives. The
predicted Gaussian primitives are situated in a unified reference frame,
allowing for high-fidelity 3D modeling and instant camera parameter estimation
using off-the-shelf solvers. To cater to both object-centric and scene-level
reconstruction, we train two model variants of FreeSplatter on extensive
datasets. In both scenarios, FreeSplatter outperforms state-of-the-art
baselines in terms of reconstruction quality and pose estimation accuracy.
Furthermore, we showcase FreeSplatter's potential in enhancing the productivity
of downstream applications, such as text/image-to-3D content creation.

Comments:
- Project page: https://bluestyle97.github.io/projects/freesplatter/

---

## PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Sean Wu, Shamik Basu, Tim Broedermann, Luc Van Gool, Christos Sakaridis | cs.CV | [PDF](http://arxiv.org/pdf/2412.09680v1){: .btn .btn-green } |

**Abstract**: We tackle the ill-posed inverse rendering problem in 3D reconstruction with a
Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR)
theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and
3D Gaussian Splatting approaches: they estimate view-dependent appearance
without modeling scene materials and illumination. To address this limitation,
we present an inverse rendering (IR) model capable of jointly estimating scene
geometry, materials, and illumination. Our model builds upon recent NeRF-based
IR approaches, but crucially introduces two novel physics-based priors that
better constrain the IR estimation. Our priors are rigorously formulated as
intuitive loss terms and achieve state-of-the-art material estimation without
compromising novel view synthesis quality. Our method is easily adaptable to
other inverse rendering and 3D reconstruction frameworks that require material
estimation. We demonstrate the importance of extending current neural rendering
approaches to fully model scene properties beyond geometry and view-dependent
appearance. Code is publicly available at https://github.com/s3anwu/pbrnerf

Comments:
- 16 pages, 7 figures. Code is publicly available at
  https://github.com/s3anwu/pbrnerf

---

## DSplats: 3D Generation by Denoising Splats-Based Multiview Diffusion  Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Kevin Miao, Harsh Agrawal, Qihang Zhang, Federico Semeraro, Marco Cavallo, Jiatao Gu, Alexander Toshev | eess.IV | [PDF](http://arxiv.org/pdf/2412.09648v1){: .btn .btn-green } |

**Abstract**: Generating high-quality 3D content requires models capable of learning robust
distributions of complex scenes and the real-world objects within them. Recent
Gaussian-based 3D reconstruction techniques have achieved impressive results in
recovering high-fidelity 3D assets from sparse input images by predicting 3D
Gaussians in a feed-forward manner. However, these techniques often lack the
extensive priors and expressiveness offered by Diffusion Models. On the other
hand, 2D Diffusion Models, which have been successfully applied to denoise
multiview images, show potential for generating a wide range of photorealistic
3D outputs but still fall short on explicit 3D priors and consistency. In this
work, we aim to bridge these two approaches by introducing DSplats, a novel
method that directly denoises multiview images using Gaussian Splat-based
Reconstructors to produce a diverse array of realistic 3D assets. To harness
the extensive priors of 2D Diffusion Models, we incorporate a pretrained Latent
Diffusion Model into the reconstructor backbone to predict a set of 3D
Gaussians. Additionally, the explicit 3D representation embedded in the
denoising network provides a strong inductive bias, ensuring geometrically
consistent novel view generation. Our qualitative and quantitative experiments
demonstrate that DSplats not only produces high-quality, spatially consistent
outputs, but also sets a new standard in single-image to 3D reconstruction.
When evaluated on the Google Scanned Objects dataset, DSplats achieves a PSNR
of 20.38, an SSIM of 0.842, and an LPIPS of 0.109.



---

## SLGaussian: Fast Language Gaussian Splatting in Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Kangjie Chen, BingQuan Dai, Minghan Qin, Dongbin Zhang, Peihao Li, Yingshuang Zou, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.08331v1){: .btn .btn-green } |

**Abstract**: 3D semantic field learning is crucial for applications like autonomous
navigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from
limited viewpoints is essential. Existing methods struggle under sparse view
conditions, relying on inefficient per-scene multi-view optimizations, which
are impractical for many real-world tasks. To address this, we propose
SLGaussian, a feed-forward method for constructing 3D semantic fields from
sparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring
consistent SAM segmentations through video tracking and using low-dimensional
indexing for high-dimensional CLIP features, SLGaussian efficiently embeds
language information in 3D space, offering a robust solution for accurate 3D
scene understanding under sparse view conditions. In experiments on two-view
sparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,
SLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,
and mIoU. Moreover, our model achieves scene inference in under 30 seconds and
open-vocabulary querying in just 0.011 seconds per query.



---

## ProGDF: Progressive Gaussian Differential Field for Controllable and  Flexible 3D Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Yian Zhao, Wanshi Xu, Yang Wu, Weiheng Huang, Zhongqian Sun, Wei Yang | cs.GR | [PDF](http://arxiv.org/pdf/2412.08152v1){: .btn .btn-green } |

**Abstract**: 3D editing plays a crucial role in editing and reusing existing 3D assets,
thereby enhancing productivity. Recently, 3DGS-based methods have gained
increasing attention due to their efficient rendering and flexibility. However,
achieving desired 3D editing results often requires multiple adjustments in an
iterative loop, resulting in tens of minutes of training time cost for each
attempt and a cumbersome trial-and-error cycle for users. This in-the-loop
training paradigm results in a poor user experience. To address this issue, we
introduce the concept of process-oriented modelling for 3D editing and propose
the Progressive Gaussian Differential Field (ProGDF), an out-of-loop training
approach that requires only a single training session to provide users with
controllable editing capability and variable editing results through a
user-friendly interface in real-time. ProGDF consists of two key components:
Progressive Gaussian Splatting (PGS) and Gaussian Differential Field (GDF). PGS
introduces the progressive constraint to extract the diverse intermediate
results of the editing process and employs rendering quality regularization to
improve the quality of these results. Based on these intermediate results, GDF
leverages a lightweight neural network to model the editing process. Extensive
results on two novel applications, namely controllable 3D editing and flexible
fine-grained 3D manipulation, demonstrate the effectiveness, practicality and
flexibility of the proposed ProGDF.



---

## GN-FR:Generalizable Neural Radiance Fields for Flare Removal

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Gopi Raju Matta, Rahul Siddartha, Rongali Simhachala Venkata Girish, Sumit Sharma, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2412.08200v1){: .btn .btn-green } |

**Abstract**: Flare, an optical phenomenon resulting from unwanted scattering and
reflections within a lens system, presents a significant challenge in imaging.
The diverse patterns of flares, such as halos, streaks, color bleeding, and
haze, complicate the flare removal process. Existing traditional and
learning-based methods have exhibited limited efficacy due to their reliance on
single-image approaches, where flare removal is highly ill-posed. We address
this by framing flare removal as a multi-view image problem, taking advantage
of the view-dependent nature of flare artifacts. This approach leverages
information from neighboring views to recover details obscured by flare in
individual images. Our proposed framework, GN-FR (Generalizable Neural Radiance
Fields for Flare Removal), can render flare-free views from a sparse set of
input images affected by lens flare and generalizes across different scenes in
an unsupervised manner. GN-FR incorporates several modules within the
Generalizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation
(FMG), View Sampler (VS), and Point Sampler (PS). To overcome the
impracticality of capturing both flare-corrupted and flare-free data, we
introduce a masking loss function that utilizes mask information in an
unsupervised setting. Additionally, we present a 3D multi-view flare dataset,
comprising 17 real flare scenes with 782 images, 80 real flare patterns, and
their corresponding annotated flare-occupancy masks. To our knowledge, this is
the first work to address flare removal within a Neural Radiance Fields (NeRF)
framework.



---

## NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF  and Neural View Synthesis Methods

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Qiang Qu, Hanxue Liang, Xiaoming Chen, Yuk Ying Chung, Yiran Shen | cs.CV | [PDF](http://arxiv.org/pdf/2412.08029v1){: .btn .btn-green } |

**Abstract**: Neural View Synthesis (NVS) has demonstrated efficacy in generating
high-fidelity dense viewpoint videos using a image set with sparse views.
However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not
tailored for the scenes with dense viewpoints synthesized by NVS and NeRF
variants, thus, they often fall short in capturing the perceptual quality,
including spatial and angular aspects of NVS-synthesized scenes. Furthermore,
the lack of dense ground truth views makes the full reference quality
assessment on NVS-synthesized scenes challenging. For instance, datasets such
as LLFF provide only sparse images, insufficient for complete full-reference
assessments. To address the issues above, we propose NeRF-NQA, the first
no-reference quality assessment method for densely-observed scenes synthesized
from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment
strategy, integrating both viewwise and pointwise approaches, to evaluate the
quality of NVS-generated scenes. The viewwise approach assesses the spatial
quality of each individual synthesized view and the overall inter-views
consistency, while the pointwise approach focuses on the angular qualities of
scene surface points and their compound inter-point quality. Extensive
evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality
assessment methods (from fields of image, video, and light-field assessment).
The results demonstrate NeRF-NQA outperforms the existing assessment methods
significantly and it shows substantial superiority on assessing NVS-synthesized
scenes without references. An implementation of this paper are available at
https://github.com/VincentQQu/NeRF-NQA.



---

## EventSplat: 3D Gaussian Splatting from Moving Event Cameras for  Real-time Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Toshiya Yura, Ashkan Mirzaei, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2412.07293v1){: .btn .btn-green } |

**Abstract**: We introduce a method for using event camera data in novel view synthesis via
Gaussian Splatting. Event cameras offer exceptional temporal resolution and a
high dynamic range. Leveraging these capabilities allows us to effectively
address the novel view synthesis challenge in the presence of fast camera
motion. For initialization of the optimization process, our approach uses prior
knowledge encoded in an event-to-video model. We also use spline interpolation
for obtaining high quality poses along the event camera trajectory. This
enhances the reconstruction quality from fast-moving cameras while overcoming
the computational limitations traditionally associated with event-based Neural
Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that
our results achieve higher visual fidelity and better performance than existing
event-based NeRF approaches while being an order of magnitude faster to render.



---

## Proc-GS: Procedural Building Generation for City Assembly with 3D  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Yixuan Li, Xingjian Ran, Linning Xu, Tao Lu, Mulin Yu, Zhenzhi Wang, Yuanbo Xiangli, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2412.07660v1){: .btn .btn-green } |

**Abstract**: Buildings are primary components of cities, often featuring repeated elements
such as windows and doors. Traditional 3D building asset creation is
labor-intensive and requires specialized skills to develop design rules. Recent
generative models for building creation often overlook these patterns, leading
to low visual fidelity and limited scalability. Drawing inspiration from
procedural modeling techniques used in the gaming and visual effects industry,
our method, Proc-GS, integrates procedural code into the 3D Gaussian Splatting
(3D-GS) framework, leveraging their advantages in high-fidelity rendering and
efficient asset management from both worlds. By manipulating procedural code,
we can streamline this process and generate an infinite variety of buildings.
This integration significantly reduces model size by utilizing shared
foundational assets, enabling scalable generation with precise control over
building assembly. We showcase the potential for expansive cityscape generation
while maintaining high rendering fidelity and precise control on both real and
synthetic cases.

Comments:
- Project page: https://city-super.github.io/procgs/

---

## Faster and Better 3D Splatting via Group Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Chengbo Wang, Guozheng Ma, Yifei Xue, Yizhen Lao | cs.CV | [PDF](http://arxiv.org/pdf/2412.07608v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel
view synthesis, demonstrating remarkable capability in high-fidelity scene
reconstruction through its Gaussian primitive representations. However, the
computational overhead induced by the massive number of primitives poses a
significant bottleneck to training efficiency. To overcome this challenge, we
propose Group Training, a simple yet effective strategy that organizes Gaussian
primitives into manageable groups, optimizing training efficiency and improving
rendering quality. This approach shows universal compatibility with existing
3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently
achieving accelerated training while maintaining superior synthesis quality.
Extensive experiments reveal that our straightforward Group Training strategy
achieves up to 30% faster convergence and improved rendering quality across
diverse scenarios.



---

## ResGS: Residual Densification of 3D Gaussian for Efficient Detail  Recovery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Yanzhe Lyu, Kai Cheng, Xin Kang, Xuejin Chen | cs.CV | [PDF](http://arxiv.org/pdf/2412.07494v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view
synthesis, achieving high fidelity and efficiency. However, it often struggles
to capture rich details and complete geometry. Our analysis highlights a key
limitation of 3D-GS caused by the fixed threshold in densification, which
balances geometry coverage against detail recovery as the threshold varies. To
address this, we introduce a novel densification method, residual split, which
adds a downscaled Gaussian as a residual. Our approach is capable of adaptively
retrieving details and complementing missing geometry while enabling
progressive refinement. To further support this method, we propose a pipeline
named ResGS. Specifically, we integrate a Gaussian image pyramid for
progressive supervision and implement a selection scheme that prioritizes the
densification of coarse Gaussians over time. Extensive experiments demonstrate
that our method achieves SOTA rendering quality. Consistent performance
improvements can be achieved by applying our residual split on various 3D-GS
variants, underscoring its versatility and potential for broader application in
3D-GS-based applications.



---

## Diffusion-Based Attention Warping for Consistent 3D Scene Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Eyal Gomel, Lior Wolf | cs.CV | [PDF](http://arxiv.org/pdf/2412.07984v1){: .btn .btn-green } |

**Abstract**: We present a novel method for 3D scene editing using diffusion models,
designed to ensure view consistency and realism across perspectives. Our
approach leverages attention features extracted from a single reference image
to define the intended edits. These features are warped across multiple views
by aligning them with scene geometry derived from Gaussian splatting depth
estimates. Injecting these warped features into other viewpoints enables
coherent propagation of edits, achieving high fidelity and spatial alignment in
3D space. Extensive evaluations demonstrate the effectiveness of our method in
generating versatile edits of 3D scenes, significantly advancing the
capabilities of scene manipulation compared to the existing methods. Project
page: \url{https://attention-warp.github.io}



---

## GASP: Gaussian Avatars with Synthetic Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrusaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay P. Namboodiri, Benjamin E Lundell | cs.CV | [PDF](http://arxiv.org/pdf/2412.07739v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has changed the game for real-time photo-realistic
rendering. One of the most popular applications of Gaussian Splatting is to
create animatable avatars, known as Gaussian Avatars. Recent works have pushed
the boundaries of quality and rendering efficiency but suffer from two main
limitations. Either they require expensive multi-camera rigs to produce avatars
with free-view rendering, or they can be trained with a single camera but only
rendered at high quality from this fixed viewpoint. An ideal model would be
trained using a short monocular video or image from available hardware, such as
a webcam, and rendered from any view. To this end, we propose GASP: Gaussian
Avatars with Synthetic Priors. To overcome the limitations of existing
datasets, we exploit the pixel-perfect nature of synthetic data to train a
Gaussian Avatar prior. By fitting this prior model to a single photo or video
and fine-tuning it, we get a high-quality Gaussian Avatar, which supports
360$^\circ$ rendering. Our prior is only required for fitting, not inference,
enabling real-time application. Through our method, we obtain high-quality,
animatable Avatars from limited data which can be animated and rendered at
70fps on commercial hardware. See our project page
(https://microsoft.github.io/GASP/) for results.

Comments:
- Project page: https://microsoft.github.io/GASP/

---

## MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2  Seconds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan | cs.CV | [PDF](http://arxiv.org/pdf/2412.06974v1){: .btn .btn-green } |

**Abstract**: Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R
no longer require camera calibration and camera pose estimation. However, they
only process a pair of views at a time to infer pixel-aligned pointmaps. When
dealing with more than two views, a combinatorial number of error prone
pairwise reconstructions are usually followed by an expensive global
optimization, which often fails to rectify the pairwise reconstruction errors.
To handle more views, reduce errors, and improve inference time, we propose the
fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view
decoder blocks which exchange information across any number of views while
considering one reference view. To make our method robust to reference view
selection, we further propose MV-DUSt3R+, which employs cross-reference-view
blocks to fuse information across different reference view choices. To further
enable novel view synthesis, we extend both by adding and jointly training
Gaussian splatting heads. Experiments on multi-view stereo reconstruction,
multi-view pose estimation, and novel view synthesis confirm that our methods
improve significantly upon prior art. Code will be released.



---

## Advancing Extended Reality with 3D Gaussian Splatting: Innovations and  Prospects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng | cs.CV | [PDF](http://arxiv.org/pdf/2412.06257v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has attracted significant attention for its
potential to revolutionize 3D representation, rendering, and interaction.
Despite the rapid growth of 3DGS research, its direct application to Extended
Reality (XR) remains underexplored. Although many studies recognize the
potential of 3DGS for XR, few have explicitly focused on or demonstrated its
effectiveness within XR environments. In this paper, we aim to synthesize
innovations in 3DGS that show specific potential for advancing XR research and
development. We conduct a comprehensive review of publicly available 3DGS
papers, with a focus on those referencing XR-related concepts. Additionally, we
perform an in-depth analysis of innovations explicitly relevant to XR and
propose a taxonomy to highlight their significance. Building on these insights,
we propose several prospective XR research areas where 3DGS can make promising
contributions, yet remain rarely touched. By investigating the intersection of
3DGS and XR, this paper provides a roadmap to push the boundaries of XR using
cutting-edge 3DGS techniques.

Comments:
- IEEE AIxVR 2025

---

## Generative Densification: Learning to Densify Gaussians for  High-Fidelity Generalizable 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Seungtae Nam, Xiangyu Sun, Gyeongjin Kang, Younggeun Lee, Seungjun Oh, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2412.06234v2){: .btn .btn-green } |

**Abstract**: Generalized feed-forward Gaussian models have achieved significant progress
in sparse-view 3D reconstruction by leveraging prior knowledge from large
multi-view datasets. However, these models often struggle to represent
high-frequency details due to the limited number of Gaussians. While the
densification strategy used in per-scene 3D Gaussian splatting (3D-GS)
optimization can be adapted to the feed-forward models, it may not be ideally
suited for generalized scenarios. In this paper, we propose Generative
Densification, an efficient and generalizable method to densify Gaussians
generated by feed-forward models. Unlike the 3D-GS densification strategy,
which iteratively splits and clones raw Gaussian parameters, our method
up-samples feature representations from the feed-forward models and generates
their corresponding fine Gaussians in a single forward pass, leveraging the
embedded prior knowledge for enhanced generalization. Experimental results on
both object-level and scene-level reconstruction tasks demonstrate that our
method outperforms state-of-the-art approaches with comparable or smaller model
sizes, achieving notable improvements in representing fine details.

Comments:
- Project page: https://stnamjef.github.io/GenerativeDensification/

---

## Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view  Event Cameras

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Viktor Rudnev, Gereon Fox, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik | cs.CV | [PDF](http://arxiv.org/pdf/2412.06770v1){: .btn .btn-green } |

**Abstract**: Volumetric reconstruction of dynamic scenes is an important problem in
computer vision. It is especially challenging in poor lighting and with fast
motion. It is partly due to the limitations of RGB cameras: To capture fast
motion without much blur, the framerate must be increased, which in turn
requires more lighting. In contrast, event cameras, which record changes in
pixel brightness asynchronously, are much less dependent on lighting, making
them more suitable for recording fast motion. We hence propose the first method
to spatiotemporally reconstruct a scene from sparse multi-view event streams
and sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF
models, one per short recording segment. The individual segments are supervised
with a set of event- and RGB-based losses and sparse-view regularisation. We
assemble a real-world multi-view camera rig with six static event cameras
around the object and record a benchmark multi-view event stream dataset of
challenging motions. Our work outperforms RGB-based baselines, producing
state-of-the-art results, and opens up the topic of multi-view event-based
reconstruction as a new path for fast scene capture beyond RGB cameras. The
code and the data will be released soon at
https://4dqv.mpi-inf.mpg.de/DynEventNeRF/

Comments:
- 15 pages, 11 figures, 6 tables

---

## Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Renlong Wu, Zhilu Zhang, Mingyang Chen, Xiaopeng Fan, Zifei Yan, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2412.06424v1){: .btn .btn-green } |

**Abstract**: Recent 4D reconstruction methods have yielded impressive results but rely on
sharp videos as supervision. However, motion blur often occurs in videos due to
camera shake and object movement, while existing methods render blurry results
when using such videos for reconstructing 4D models. Although a few NeRF-based
approaches attempted to address the problem, they struggled to produce
high-quality results, due to the inaccuracy in estimating continuous dynamic
representations within the exposure time. Encouraged by recent works in 3D
motion trajectory modeling using 3D Gaussian Splatting (3DGS), we suggest
taking 3DGS as the scene representation manner, and propose the first 4D
Gaussian Splatting framework to reconstruct a high-quality 4D model from blurry
monocular video, named Deblur4DGS. Specifically, we transform continuous
dynamic representations estimation within an exposure time into the exposure
time estimation. Moreover, we introduce exposure regularization to avoid
trivial solutions, as well as multi-frame and multi-resolution consistency ones
to alleviate artifacts. Furthermore, to better represent objects with large
motion, we suggest blur-aware variable canonical Gaussians. Beyond novel-view
synthesis, Deblur4DGS can be applied to improve blurry video from multiple
perspectives, including deblurring, frame interpolation, and video
stabilization. Extensive experiments on the above four tasks show that
Deblur4DGS outperforms state-of-the-art 4D reconstruction methods. The codes
are available at https://github.com/ZcsrenlongZ/Deblur4DGS.

Comments:
- 17 pages

---

## Splatter-360: Generalizable 360$^{\circ}$ Gaussian Splatting for  Wide-baseline Panoramic Images

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Zheng Chen, Chenming Wu, Zhelun Shen, Chen Zhao, Weicai Ye, Haocheng Feng, Errui Ding, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.06250v1){: .btn .btn-green } |

**Abstract**: Wide-baseline panoramic images are frequently used in applications like VR
and simulations to minimize capturing labor costs and storage needs. However,
synthesizing novel views from these panoramic images in real time remains a
significant challenge, especially due to panoramic imagery's high resolution
and inherent distortions. Although existing 3D Gaussian splatting (3DGS)
methods can produce photo-realistic views under narrow baselines, they often
overfit the training views when dealing with wide-baseline panoramic images due
to the difficulty in learning precise geometry from sparse 360$^{\circ}$ views.
This paper presents \textit{Splatter-360}, a novel end-to-end generalizable
3DGS framework designed to handle wide-baseline panoramic images. Unlike
previous approaches, \textit{Splatter-360} performs multi-view matching
directly in the spherical domain by constructing a spherical cost volume
through a spherical sweep algorithm, enhancing the network's depth perception
and geometry estimation. Additionally, we introduce a 3D-aware bi-projection
encoder to mitigate the distortions inherent in panoramic images and integrate
cross-view attention to improve feature interactions across multiple
viewpoints. This enables robust 3D-aware feature representations and real-time
rendering capabilities. Experimental results on the HM3D~\cite{hm3d} and
Replica~\cite{replica} demonstrate that \textit{Splatter-360} significantly
outperforms state-of-the-art NeRF and 3DGS methods (e.g., PanoGRF, MVSplat,
DepthSplat, and HiSplat) in both synthesis quality and generalization
performance for wide-baseline panoramic images. Code and trained models are
available at \url{https://3d-aigc.github.io/Splatter-360/}.

Comments:
- Project page:https://3d-aigc.github.io/Splatter-360/. Code:
  https://github.com/thucz/splatter360

---

## 4D Gaussian Splatting with Scale-aware Residual Field and Adaptive  Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Jinbo Yan, Rui Peng, Luyang Tang, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.06299v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic scenes from video sequences is a highly promising task
in the multimedia domain. While previous methods have made progress, they often
struggle with slow rendering and managing temporal complexities such as
significant motion and object appearance/disappearance. In this paper, we
propose SaRO-GS as a novel dynamic scene representation capable of achieving
real-time rendering while effectively handling temporal complexities in dynamic
scenes. To address the issue of slow rendering speed, we adopt a Gaussian
primitive-based representation and optimize the Gaussians in 4D space, which
facilitates real-time rendering with the assistance of 3D Gaussian Splatting.
Additionally, to handle temporally complex dynamic scenes, we introduce a
Scale-aware Residual Field. This field considers the size information of each
Gaussian primitive while encoding its residual feature and aligns with the
self-splitting behavior of Gaussian primitives. Furthermore, we propose an
Adaptive Optimization Schedule, which assigns different optimization strategies
to Gaussian primitives based on their distinct temporal properties, thereby
expediting the reconstruction of dynamic regions. Through evaluations on
monocular and multi-view datasets, our method has demonstrated state-of-the-art
performance. Please see our project page at
https://yjb6.github.io/SaRO-GS.github.io.



---

## Diffusing Differentiable Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Yash Savani, Marc Finzi, J. Zico Kolter | cs.CV | [PDF](http://arxiv.org/pdf/2412.06981v1){: .btn .btn-green } |

**Abstract**: We introduce a novel, training-free method for sampling differentiable
representations (diffreps) using pretrained diffusion models. Rather than
merely mode-seeking, our method achieves sampling by "pulling back" the
dynamics of the reverse-time process--from the image space to the diffrep
parameter space--and updating the parameters according to this pulled-back
process. We identify an implicit constraint on the samples induced by the
diffrep and demonstrate that addressing this constraint significantly improves
the consistency and detail of the generated objects. Our method yields diffreps
with substantially improved quality and diversity for images, panoramas, and 3D
NeRFs compared to existing techniques. Our approach is a general-purpose method
for sampling diffreps, expanding the scope of problems that diffusion models
can tackle.

Comments:
- Published at NeurIPS 2024

---

## Efficient Semantic Splatting for Remote Sensing Multi-view Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-08 | Zipeng Qi, Hao Chen, Haotian Zhang, Zhengxia Zou, Zhenwei Shi | cs.CV | [PDF](http://arxiv.org/pdf/2412.05969v2){: .btn .btn-green } |

**Abstract**: In this paper, we propose a novel semantic splatting approach based on
Gaussian Splatting to achieve efficient and low-latency. Our method projects
the RGB attributes and semantic features of point clouds onto the image plane,
simultaneously rendering RGB images and semantic segmentation results.
Leveraging the explicit structure of point clouds and a one-time rendering
strategy, our approach significantly enhances efficiency during optimization
and rendering. Additionally, we employ SAM2 to generate pseudo-labels for
boundary regions, which often lack sufficient supervision, and introduce
two-level aggregation losses at the 2D feature map and 3D spatial levels to
improve the view-consistent and spatial continuity.



---

## GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting  and Meshing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-08 | Jianing Zhang, Yuchao Zheng, Ziwei Li, Qionghai Dai, Xiaoyun Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2412.05908v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has gained attention for its efficient representation and
rendering of 3D scenes using continuous Gaussian primitives. However, it
struggles with sparse-view inputs due to limited geometric and photometric
information, causing ambiguities in depth, shape, and texture.
  we propose GBR: Generative Bundle Refinement, a method for high-fidelity
Gaussian splatting and meshing using only 4-6 input views. GBR integrates a
neural bundle adjustment module to enhance geometry accuracy and a generative
depth refinement module to improve geometry fidelity. More specifically, the
neural bundle adjustment module integrates a foundation network to produce
initial 3D point maps and point matches from unposed images, followed by bundle
adjustment optimization to improve multiview consistency and point cloud
accuracy. The generative depth refinement module employs a diffusion-based
strategy to enhance geometric details and fidelity while preserving the scale.
Finally, for Gaussian splatting optimization, we propose a multimodal loss
function incorporating depth and normal consistency, geometric regularization,
and pseudo-view supervision, providing robust guidance under sparse-view
conditions. Experiments on widely used datasets show that GBR significantly
outperforms existing methods under sparse-view inputs. Additionally, GBR
demonstrates the ability to reconstruct and render large-scale real-world
scenes, such as the Pavilion of Prince Teng and the Great Wall, with remarkable
details using only 6 views.



---

## WATER-GS: Toward Copyright Protection for 3D Gaussian Splatting via  Universal Watermarking

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Yuqi Tan, Xiang Liu, Shuzhao Xie, Bin Chen, Shu-Tao Xia, Zhi Wang | cs.CR | [PDF](http://arxiv.org/pdf/2412.05695v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for 3D scene
representation, providing rapid rendering speeds and high fidelity. As 3DGS
gains prominence, safeguarding its intellectual property becomes increasingly
crucial since 3DGS could be used to imitate unauthorized scene creations and
raise copyright issues. Existing watermarking methods for implicit NeRFs cannot
be directly applied to 3DGS due to its explicit representation and real-time
rendering process, leaving watermarking for 3DGS largely unexplored. In
response, we propose WATER-GS, a novel method designed to protect 3DGS
copyrights through a universal watermarking strategy. First, we introduce a
pre-trained watermark decoder, treating raw 3DGS generative modules as
potential watermark encoders to ensure imperceptibility. Additionally, we
implement novel 3D distortion layers to enhance the robustness of the embedded
watermark against common real-world distortions of point cloud data.
Comprehensive experiments and ablation studies demonstrate that WATER-GS
effectively embeds imperceptible and robust watermarks into 3DGS without
compromising rendering efficiency and quality. Our experiments indicate that
the 3D distortion layers can yield up to a 20% improvement in accuracy rate.
Notably, our method is adaptable to different 3DGS variants, including 3DGS
compression frameworks and 2D Gaussian splatting.



---

## Radiant: Large-scale 3D Gaussian Rendering based on Hierarchical  Framework


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Haosong Peng, Tianyu Qi, Yufeng Zhan, Hao Li, Yalun Dai, Yuanqing Xia | cs.CV | [PDF](http://arxiv.org/pdf/2412.05546v1){: .btn .btn-green } |

**Abstract**: With the advancement of computer vision, the recently emerged 3D Gaussian
Splatting (3DGS) has increasingly become a popular scene reconstruction
algorithm due to its outstanding performance. Distributed 3DGS can efficiently
utilize edge devices to directly train on the collected images, thereby
offloading computational demands and enhancing efficiency. However, traditional
distributed frameworks often overlook computational and communication
challenges in real-world environments, hindering large-scale deployment and
potentially posing privacy risks. In this paper, we propose Radiant, a
hierarchical 3DGS algorithm designed for large-scale scene reconstruction that
considers system heterogeneity, enhancing the model performance and training
efficiency. Via extensive empirical study, we find that it is crucial to
partition the regions for each edge appropriately and allocate varying camera
positions to each device for image collection and training. The core of Radiant
is partitioning regions based on heterogeneous environment information and
allocating workloads to each device accordingly. Furthermore, we provide a 3DGS
model aggregation algorithm that enhances the quality and ensures the
continuity of models' boundaries. Finally, we develop a testbed, and
experiments demonstrate that Radiant improved reconstruction quality by up to
25.7\% and reduced up to 79.6\% end-to-end latency.



---

## Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Saqib Javed, Ahmad Jarrar Khan, Corentin Dumery, Chen Zhao, Mathieu Salzmann | cs.CV | [PDF](http://arxiv.org/pdf/2412.05700v1){: .btn .btn-green } |

**Abstract**: Recent advancements in high-fidelity dynamic scene reconstruction have
leveraged dynamic 3D Gaussians and 4D Gaussian Splatting for realistic scene
representation. However, to make these methods viable for real-time
applications such as AR/VR, gaming, and rendering on low-power devices,
substantial reductions in memory usage and improvements in rendering efficiency
are required. While many state-of-the-art methods prioritize lightweight
implementations, they struggle in handling scenes with complex motions or long
sequences. In this work, we introduce Temporally Compressed 3D Gaussian
Splatting (TC3DGS), a novel technique designed specifically to effectively
compress dynamic 3D Gaussian representations. TC3DGS selectively prunes
Gaussians based on their temporal relevance and employs gradient-aware
mixed-precision quantization to dynamically compress Gaussian parameters. It
additionally relies on a variation of the Ramer-Douglas-Peucker algorithm in a
post-processing step to further reduce storage by interpolating Gaussian
trajectories across frames. Our experiments across multiple datasets
demonstrate that TC3DGS achieves up to 67$\times$ compression with minimal or
no degradation in visual quality.

Comments:
- Code will be released soon

---

## Text-to-3D Gaussian Splatting with Physics-Grounded Motion Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Wenqing Wang, Yun Fu | cs.CV | [PDF](http://arxiv.org/pdf/2412.05560v1){: .btn .btn-green } |

**Abstract**: Text-to-3D generation is a valuable technology in virtual reality and digital
content creation. While recent works have pushed the boundaries of text-to-3D
generation, producing high-fidelity 3D objects with inefficient prompts and
simulating their physics-grounded motion accurately still remain unsolved
challenges. To address these challenges, we present an innovative framework
that utilizes the Large Language Model (LLM)-refined prompts and diffusion
priors-guided Gaussian Splatting (GS) for generating 3D models with accurate
appearances and geometric structures. We also incorporate a continuum
mechanics-based deformation map and color regularization to synthesize vivid
physics-grounded motion for the generated 3D Gaussians, adhering to the
conservation of mass and momentum. By integrating text-to-3D generation with
physics-grounded motion synthesis, our framework renders photo-realistic 3D
objects that exhibit physics-aware motion, accurately reflecting the behaviors
of the objects under various forces and constraints across different materials.
Extensive experiments demonstrate that our approach achieves high-quality 3D
generations with realistic physics-grounded motion.



---

## Template-free Articulated Gaussian Splatting for Real-time Reposable  Dynamic View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Diwen Wan, Yuxiang Wang, Ruijie Lu, Gang Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2412.05570v1){: .btn .btn-green } |

**Abstract**: While novel view synthesis for dynamic scenes has made significant progress,
capturing skeleton models of objects and re-posing them remains a challenging
task. To tackle this problem, in this paper, we propose a novel approach to
automatically discover the associated skeleton model for dynamic objects from
videos without the need for object-specific templates. Our approach utilizes 3D
Gaussian Splatting and superpoints to reconstruct dynamic objects. Treating
superpoints as rigid parts, we can discover the underlying skeleton model
through intuitive cues and optimize it using the kinematic model. Besides, an
adaptive control strategy is applied to avoid the emergence of redundant
superpoints. Extensive experiments demonstrate the effectiveness and efficiency
of our method in obtaining re-posable 3D objects. Not only can our approach
achieve excellent visual fidelity, but it also allows for the real-time
rendering of high-resolution images.

Comments:
- Accepted by NeurIPS 2024

---

## Extrapolated Urban View Synthesis Benchmark

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Xiangyu Han, Zhen Jia, Boyi Li, Yan Wang, Boris Ivanovic, Yurong You, Lingjie Liu, Yue Wang, Marco Pavone, Chen Feng, Yiming Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.05256v2){: .btn .btn-green } |

**Abstract**: Photorealistic simulators are essential for the training and evaluation of
vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis
(NVS), a crucial capability that generates diverse unseen viewpoints to
accommodate the broad and continuous pose distribution of AVs. Recent advances
in radiance fields, such as 3D Gaussian Splatting, achieve photorealistic
rendering at real-time speeds and have been widely used in modeling large-scale
driving scenes. However, their performance is commonly evaluated using an
interpolated setup with highly correlated training and test views. In contrast,
extrapolation, where test views largely deviate from training views, remains
underexplored, limiting progress in generalizable simulation technology. To
address this gap, we leverage publicly available AV datasets with multiple
traversals, multiple vehicles, and multiple cameras to build the first
Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct
quantitative and qualitative evaluations of state-of-the-art Gaussian Splatting
methods across different difficulty levels. Our results show that Gaussian
Splatting is prone to overfitting to training views. Besides, incorporating
diffusion priors and improving geometry cannot fundamentally improve NVS under
large view changes, highlighting the need for more robust approaches and
large-scale training. We have released our data to help advance self-driving
and urban robotics simulation technology.

Comments:
- Project page: https://ai4ce.github.io/EUVS-Benchmark/

---

## MixedGaussianAvatar: Realistically and Geometrically Accurate Head  Avatar via Mixed 2D-3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu | cs.CV | [PDF](http://arxiv.org/pdf/2412.04955v2){: .btn .btn-green } |

**Abstract**: Reconstructing high-fidelity 3D head avatars is crucial in various
applications such as virtual reality. The pioneering methods reconstruct
realistic head avatars with Neural Radiance Fields (NeRF), which have been
limited by training and rendering speed. Recent methods based on 3D Gaussian
Splatting (3DGS) significantly improve the efficiency of training and
rendering. However, the surface inconsistency of 3DGS results in subpar
geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy
at the expense of rendering fidelity. To leverage the benefits of both 2DGS and
3DGS, we propose a novel method named MixedGaussianAvatar for realistically and
geometrically accurate head avatar reconstruction. Our main idea is to utilize
2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric
accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model
and connect additional 3D Gaussians to those 2D Gaussians where the rendering
quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation.
These 2D-3D Gaussians can then be animated using FLAME parameters. We further
introduce a progressive training strategy that first trains the 2D Gaussians
and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority
of MixedGaussianAvatar through comprehensive experiments. The code will be
released at: https://github.com/ChenVoid/MGA/.

Comments:
- Project: https://chenvoid.github.io/MGA/

---

## Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Jixuan Fan, Wanhua Li, Yifei Han, Yansong Tang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04887v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has demonstrated notable success in large-scale scene
reconstruction, but challenges persist due to high training memory consumption
and storage overhead. Hybrid representations that integrate implicit and
explicit features offer a way to mitigate these limitations. However, when
applied in parallelized block-wise training, two critical issues arise since
reconstruction accuracy deteriorates due to reduced data diversity when
training each block independently, and parallel training restricts the number
of divided blocks to the available number of GPUs. To address these issues, we
propose Momentum-GS, a novel approach that leverages momentum-based
self-distillation to promote consistency and accuracy across the blocks while
decoupling the number of blocks from the physical GPU count. Our method
maintains a teacher Gaussian decoder updated with momentum, ensuring a stable
reference during training. This teacher provides each block with global
guidance in a self-distillation manner, promoting spatial consistency in
reconstruction. To further ensure consistency across the blocks, we incorporate
block weighting, dynamically adjusting each block's weight according to its
reconstruction accuracy. Extensive experiments on large-scale scenes show that
our method consistently outperforms existing techniques, achieving a 12.8%
improvement in LPIPS over CityGaussian with much fewer divided blocks and
establishing a new state of the art. Project page:
https://jixuan-fan.github.io/Momentum-GS_Page/



---

## Pushing Rendering Boundaries: Hard Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Qingshan Xu, Jiequan Cui, Xuanyu Yi, Yuxuan Wang, Yuan Zhou, Yew-Soon Ong, Hanwang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04826v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive Novel View Synthesis
(NVS) results in a real-time rendering manner. During training, it relies
heavily on the average magnitude of view-space positional gradients to grow
Gaussians to reduce rendering loss. However, this average operation smooths the
positional gradients from different viewpoints and rendering errors from
different pixels, hindering the growth and optimization of many defective
Gaussians. This leads to strong spurious artifacts in some areas. To address
this problem, we propose Hard Gaussian Splatting, dubbed HGS, which considers
multi-view significant positional gradients and rendering errors to grow hard
Gaussians that fill the gaps of classical Gaussian Splatting on 3D scenes, thus
achieving superior NVS results. In detail, we present positional gradient
driven HGS, which leverages multi-view significant positional gradients to
uncover hard Gaussians. Moreover, we propose rendering error guided HGS, which
identifies noticeable pixel rendering errors and potentially over-large
Gaussians to jointly mine hard Gaussians. By growing and optimizing these hard
Gaussians, our method helps to resolve blurring and needle-like artifacts.
Experiments on various datasets demonstrate that our method achieves
state-of-the-art rendering quality while maintaining real-time efficiency.



---

## Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Susung Hong, Johanna Karras, Ricardo Martin-Brualla, Ira Kemelmacher-Shlizerman | cs.CV | [PDF](http://arxiv.org/pdf/2412.05279v1){: .btn .btn-green } |

**Abstract**: The fields of 3D reconstruction and text-based 3D editing have advanced
significantly with the evolution of text-based diffusion models. While existing
3D editing methods excel at modifying color, texture, and style, they struggle
with extensive geometric or appearance changes, thus limiting their
applications. We propose Perturb-and-Revise, which makes possible a variety of
NeRF editing. First, we perturb the NeRF parameters with random initializations
to create a versatile initialization. We automatically determine the
perturbation magnitude through analysis of the local loss landscape. Then, we
revise the edited NeRF via generative trajectories. Combined with the
generative process, we impose identity-preserving gradients to refine the
edited NeRF. Extensive experiments demonstrate that Perturb-and-Revise
facilitates flexible, effective, and consistent editing of color, appearance,
and geometry in 3D. For 360{\deg} results, please visit our project page:
https://susunghong.github.io/Perturb-and-Revise.

Comments:
- Project page: https://susunghong.github.io/Perturb-and-Revise

---

## WRF-GS: Wireless Radiation Field Reconstruction with 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Chaozheng Wen, Jingwen Tong, Yingdong Hu, Zehong Lin, Jun Zhang | cs.NI | [PDF](http://arxiv.org/pdf/2412.04832v1){: .btn .btn-green } |

**Abstract**: Wireless channel modeling plays a pivotal role in designing, analyzing, and
optimizing wireless communication systems. Nevertheless, developing an
effective channel modeling approach has been a longstanding challenge. This
issue has been escalated due to the denser network deployment, larger antenna
arrays, and wider bandwidth in 5G and beyond networks. To address this
challenge, we put forth WRF-GS, a novel framework for channel modeling based on
wireless radiation field (WRF) reconstruction using 3D Gaussian splatting.
WRF-GS employs 3D Gaussian primitives and neural networks to capture the
interactions between the environment and radio signals, enabling efficient WRF
reconstruction and visualization of the propagation characteristics. The
reconstructed WRF can then be used to synthesize the spatial spectrum for
comprehensive wireless channel characterization. Notably, with a small number
of measurements, WRF-GS can synthesize new spatial spectra within milliseconds
for a given scene, thereby enabling latency-sensitive applications.
Experimental results demonstrate that WRF-GS outperforms existing methods for
spatial spectrum synthesis, such as ray tracing and other deep-learning
approaches. Moreover, WRF-GS achieves superior performance in the channel state
information prediction task, surpassing existing methods by a significant
margin of more than 2.43 dB.

Comments:
- accepted to the IEEE International Conference on Computer
  Communications (INFOCOM 2025)

---

## Multi-View Pose-Agnostic Change Localization with Zero Labels

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Chamuditha Jayanga Galappaththige, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko Suenderhauf, Dimity Miller | cs.CV | [PDF](http://arxiv.org/pdf/2412.03911v1){: .btn .btn-green } |

**Abstract**: Autonomous agents often require accurate methods for detecting and localizing
changes in their environment, particularly when observations are captured from
unconstrained and inconsistent viewpoints. We propose a novel label-free,
pose-agnostic change detection method that integrates information from multiple
viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS)
representation of the scene. With as few as 5 images of the post-change scene,
our approach can learn additional change channels in a 3DGS and produce change
masks that outperform single-view techniques. Our change-aware 3D scene
representation additionally enables the generation of accurate change masks for
unseen viewpoints. Experimental results demonstrate state-of-the-art
performance in complex multi-object scenes, achieving a 1.7$\times$ and
1.6$\times$ improvement in Mean Intersection Over Union and F1 score
respectively over other baselines. We also contribute a new real-world dataset
to benchmark change detection in diverse challenging scenes in the presence of
lighting variations.



---

## Turbo3D: Ultra-fast Text-to-3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Hanzhe Hu, Tianwei Yin, Fujun Luan, Yiwei Hu, Hao Tan, Zexiang Xu, Sai Bi, Shubham Tulsiani, Kai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04470v1){: .btn .btn-green } |

**Abstract**: We present Turbo3D, an ultra-fast text-to-3D system capable of generating
high-quality Gaussian splatting assets in under one second. Turbo3D employs a
rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian
reconstructor, both operating in latent space. The 4-step, 4-view generator is
a student model distilled through a novel Dual-Teacher approach, which
encourages the student to learn view consistency from a multi-view teacher and
photo-realism from a single-view teacher. By shifting the Gaussian
reconstructor's inputs from pixel space to latent space, we eliminate the extra
image decoding time and halve the transformer sequence length for maximum
efficiency. Our method demonstrates superior 3D generation results compared to
previous baselines, while operating in a fraction of their runtime.

Comments:
- project page: https://turbo-3d.github.io/

---

## PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human  Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Shota Sasaki, Jane Wu, Ko Nishino | cs.CV | [PDF](http://arxiv.org/pdf/2412.04433v2){: .btn .btn-green } |

**Abstract**: This paper introduces a novel clothed human model that can be learned from
multiview RGB videos, with a particular emphasis on recovering physically
accurate body and cloth movements. Our method, Position Based Dynamic Gaussians
(PBDyG), realizes ``movement-dependent'' cloth deformation via physical
simulation, rather than merely relying on ``pose-dependent'' rigid
transformations. We model the clothed human holistically but with two distinct
physical entities in contact: clothing modeled as 3D Gaussians, which are
attached to a skinned SMPL body that follows the movement of the person in the
input videos. The articulation of the SMPL body also drives physically-based
simulation of the clothes' Gaussians to transform the avatar to novel poses. In
order to run position based dynamics simulation, physical properties including
mass and material stiffness are estimated from the RGB videos through Dynamic
3D Gaussian Splatting. Experiments demonstrate that our method not only
accurately reproduces appearance but also enables the reconstruction of avatars
wearing highly deformable garments, such as skirts or coats, which have been
challenging to reconstruct using existing methods.



---

## Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth  Motion Helps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Yiqing Liang, Mikhail Okunev, Mikaela Angelina Uy, Runfeng Li, Leonidas Guibas, James Tompkin, Adam W. Harley | cs.CV | [PDF](http://arxiv.org/pdf/2412.04457v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting methods are emerging as a popular approach for converting
multi-view image data into scene representations that allow view synthesis. In
particular, there is interest in enabling view synthesis for dynamic scenes
using only monocular input data -- an ill-posed and challenging problem. The
fast pace of work in this area has produced multiple simultaneous papers that
claim to work best, which cannot all be true. In this work, we organize,
benchmark, and analyze many Gaussian-splatting-based methods, providing
apples-to-apples comparisons that prior works have lacked. We use multiple
existing datasets and a new instructive synthetic dataset designed to isolate
factors that affect reconstruction quality. We systematically categorize
Gaussian splatting methods into specific motion representation types and
quantify how their differences impact performance. Empirically, we find that
their rank order is well-defined in synthetic data, but the complexity of
real-world data currently overwhelms the differences. Furthermore, the fast
rendering speed of all Gaussian-based methods comes at the cost of brittleness
in optimization. We summarize our experiments into a list of findings that can
help to further progress in this lively problem setting. Project Webpage:
https://lynl7130.github.io/MonoDyGauBench.github.io/

Comments:
- 37 pages, 39 figures, 9 tables

---

## QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming  Free-viewpoint Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Sharath Girish, Tianye Li, Amrita Mazumdar, Abhinav Shrivastava, David Luebke, Shalini De Mello | cs.CV | [PDF](http://arxiv.org/pdf/2412.04469v1){: .btn .btn-green } |

**Abstract**: Online free-viewpoint video (FVV) streaming is a challenging problem, which
is relatively under-explored. It requires incremental on-the-fly updates to a
volumetric representation, fast training and rendering to satisfy real-time
constraints and a small memory footprint for efficient transmission. If
achieved, it can enhance user experience by enabling novel applications, e.g.,
3D video conferencing and live volumetric video broadcast, among others. In
this work, we propose a novel framework for QUantized and Efficient ENcoding
(QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly
learns Gaussian attribute residuals between consecutive frames at each
time-step without imposing any structural constraints on them, allowing for
high quality reconstruction and generalizability. To efficiently store the
residuals, we further propose a quantization-sparsity framework, which contains
a learned latent-decoder for effectively quantizing attribute residuals other
than Gaussian positions and a learned gating module to sparsify position
residuals. We propose to use the Gaussian viewspace gradient difference vector
as a signal to separate the static and dynamic content of the scene. It acts as
a guide for effective sparsity learning and speeds up training. On diverse FVV
benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all
metrics. Notably, for several highly dynamic scenes, it reduces the model size
to just 0.7 MB per frame while training in under 5 sec and rendering at 350
FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen

Comments:
- Accepted at NeurIPS 2024, Project website:
  https://research.nvidia.com/labs/amri/projects/queen

---

## HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, Jieping Ye | cs.CV | [PDF](http://arxiv.org/pdf/2412.03844v1){: .btn .btn-green } |

**Abstract**: Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS)
in scenes featuring transient objects is challenging. We propose a novel hybrid
representation, termed as HybridGS, using 2D Gaussians for transient objects
per image and maintaining traditional 3D Gaussians for the whole static scenes.
Note that, the 3DGS itself is better suited for modeling static scenes that
assume multi-view consistency, but the transient objects appear occasionally
and do not adhere to the assumption, thus we model them as planar objects from
a single view, represented with 2D Gaussians. Our novel representation
decomposes the scene from the perspective of fundamental viewpoint consistency,
making it more reasonable. Additionally, we present a novel multi-view
regulated supervision method for 3DGS that leverages information from
co-visible regions, further enhancing the distinctions between the transients
and statics. Then, we propose a straightforward yet effective multi-stage
training strategy to ensure robust training and high-quality view synthesis
across various settings. Experiments on benchmark datasets show our
state-of-the-art performance of novel view synthesis in both indoor and outdoor
scenes, even in the presence of distracting elements.

Comments:
- Project page: https://gujiaqivadin.github.io/hybridgs/

---

## Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04459v1){: .btn .btn-green } |

**Abstract**: We propose an efficient radiance field rendering algorithm that incorporates
a rasterization process on sparse voxels without neural networks or 3D
Gaussians. There are two key contributions coupled with the proposed system.
The first is to render sparse voxels in the correct depth order along pixel
rays by using dynamic Morton ordering. This avoids the well-known popping
artifact found in Gaussian splatting. Second, we adaptively fit sparse voxels
to different levels of detail within scenes, faithfully reproducing scene
details while achieving high rendering frame rates. Our method improves the
previous neural-free voxel grid representation by over 4db PSNR and more than
10x rendering FPS speedup, achieving state-of-the-art comparable novel-view
synthesis results. Additionally, our neural-free sparse voxels are seamlessly
compatible with grid-based 3D processing algorithms. We achieve promising mesh
reconstruction accuracy by integrating TSDF-Fusion and Marching Cubes into our
sparse grid system.

Comments:
- Code release in progress

---

## DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for  Monocular Dynamic 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Xuesong Li, Jinguang Tong, Jie Hong, Vivien Rolland, Lars Petersson | cs.CV | [PDF](http://arxiv.org/pdf/2412.03910v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction from monocular video is critical for real-world
applications. This paper tackles the dual challenges of dynamic novel-view
synthesis and 3D geometry reconstruction by introducing a hybrid framework:
Deformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both
modules can leverage each other for both tasks. During training, depth maps
generated by the deformable Gaussian splatting module guide the ray sampling
for faster processing and provide depth supervision within the dynamic neural
surface module to improve geometry reconstruction. Simultaneously, the dynamic
neural surface directs the distribution of Gaussian primitives around the
surface, enhancing rendering quality. To further refine depth supervision, we
introduce a depth-filtering process on depth maps derived from Gaussian
rasterization. Extensive experiments on public datasets demonstrate that DGNS
achieves state-of-the-art performance in both novel-view synthesis and 3D
reconstruction.



---

## Volumetrically Consistent 3D Gaussian Rasterization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Chinmay Talegaonkar, Yash Belhe, Ravi Ramamoorthi, Nicholas Antipa | cs.CV | [PDF](http://arxiv.org/pdf/2412.03378v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view
synthesis at high inference speeds. However, its splatting-based rendering
model makes several approximations to the rendering equation, reducing physical
accuracy. We show that splatting and its approximations are unnecessary, even
within a rasterizer; we instead volumetrically integrate 3D Gaussians directly
to compute the transmittance across them analytically. We use this analytic
transmittance to derive more physically-accurate alpha values than 3DGS, which
can directly be used within their framework. The result is a method that more
closely follows the volume rendering equation (similar to ray-tracing) while
enjoying the speed benefits of rasterization. Our method represents opaque
surfaces with higher accuracy and fewer points than 3DGS. This enables it to
outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being
volumetrically consistent also enables our method to work out of the box for
tomography. We match the state-of-the-art 3DGS-based tomography method with
fewer points. Being volumetrically consistent also enables our method to work
out of the box for tomography. We match the state-of-the-art 3DGS-based
tomography method with fewer points.



---

## Dense Scene Reconstruction from Light-Field Images Affected by Rolling  Shutter


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Hermes McGriff, Renato Martins, Nicolas Andreff, Cedric Demonceaux | cs.CV | [PDF](http://arxiv.org/pdf/2412.03518v1){: .btn .btn-green } |

**Abstract**: This paper presents a dense depth estimation approach from light-field (LF)
images that is able to compensate for strong rolling shutter (RS) effects. Our
method estimates RS compensated views and dense RS compensated disparity maps.
We present a two-stage method based on a 2D Gaussians Splatting that allows for
a ``render and compare" strategy with a point cloud formulation. In the first
stage, a subset of sub-aperture images is used to estimate an RS agnostic 3D
shape that is related to the scene target shape ``up to a motion". In the
second stage, the deformation of the 3D shape is computed by estimating an
admissible camera motion. We demonstrate the effectiveness and advantages of
this approach through several experiments conducted for different scenes and
types of motions. Due to lack of suitable datasets for evaluation, we also
present a new carefully designed synthetic dataset of RS LF images. The source
code, trained models and dataset will be made publicly available at:
https://github.com/ICB-Vision-AI/DenseRSLF



---

## Urban4D: Semantic-Guided 4D Gaussian Splatting for Urban Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Ziwen Li, Jiaxin Huang, Runnan Chen, Yunlong Che, Yandong Guo, Tongliang Liu, Fakhri Karray, Mingming Gong | cs.CV | [PDF](http://arxiv.org/pdf/2412.03473v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic urban scenes presents significant challenges due to
their intrinsic geometric structures and spatiotemporal dynamics. Existing
methods that attempt to model dynamic urban scenes without leveraging priors on
potentially moving regions often produce suboptimal results. Meanwhile,
approaches based on manual 3D annotations yield improved reconstruction quality
but are impractical due to labor-intensive labeling. In this paper, we revisit
the potential of 2D semantic maps for classifying dynamic and static Gaussians
and integrating spatial and temporal dimensions for urban scene representation.
We introduce Urban4D, a novel framework that employs a semantic-guided
decomposition strategy inspired by advances in deep 2D semantic map generation.
Our approach distinguishes potentially dynamic objects through reliable
semantic Gaussians. To explicitly model dynamic objects, we propose an
intuitive and effective 4D Gaussian splatting (4DGS) representation that
aggregates temporal information through learnable time embeddings for each
Gaussian, predicting their deformations at desired timestamps using a
multilayer perceptron (MLP). For more accurate static reconstruction, we also
design a k-nearest neighbor (KNN)-based consistency regularization to handle
the ground surface due to its low-texture characteristic. Extensive experiments
on real-world datasets demonstrate that Urban4D not only achieves comparable or
better quality than previous state-of-the-art methods but also effectively
captures dynamic objects while maintaining high visual fidelity for static
elements.



---

## 2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains  for High-Fidelity Indoor Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Wanting Zhang, Haodong Xiang, Zhichao Liao, Xiansong Lai, Xinghui Li, Long Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2412.03428v1){: .btn .btn-green } |

**Abstract**: The reconstruction of indoor scenes remains challenging due to the inherent
complexity of spatial structures and the prevalence of textureless regions.
Recent advancements in 3D Gaussian Splatting have improved novel view synthesis
with accelerated processing but have yet to deliver comparable performance in
surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method
leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction.
Specifically, we employ a seed-guided mechanism to control the distribution of
2D Gaussians, with the density of seed points dynamically optimized through
adaptive growth and pruning mechanisms. To further improve geometric accuracy,
we incorporate monocular depth and normal priors to provide constraints for
details and textureless regions respectively. Additionally, multi-view
consistency constraints are employed to mitigate artifacts and further enhance
reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets
demonstrate that our method achieves state-of-the-art performance in indoor
scene reconstruction.



---

## Splats in Splats: Embedding Invisible 3D Watermark within Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Yijia Guo, Wenkai Huang, Yang Li, Gaolei Li, Hang Zhang, Liwen Hu, Jianhua Li, Tiejun Huang, Lei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2412.03121v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction
performance with explicit scene representations. Given the widespread
application of 3DGS in 3D reconstruction and generation tasks, there is an
urgent need to protect the copyright of 3DGS assets. However, existing
copyright protection techniques for 3DGS overlook the usability of 3D assets,
posing challenges for practical deployment. Here we describe WaterGS, the first
3DGS watermarking framework that embeds 3D content in 3DGS itself without
modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep
insight into spherical harmonics (SH) and devise an importance-graded SH
coefficient encryption strategy to embed the hidden SH coefficients.
Furthermore, we employ a convolutional autoencoder to establish a mapping
between the original Gaussian primitives' opacity and the hidden Gaussian
primitives' opacity. Extensive experiments indicate that WaterGS significantly
outperforms existing 3D steganography techniques, with 5.31% higher scene
fidelity and 3X faster rendering speed, while ensuring security, robustness,
and user experience. Codes and data will be released at
https://water-gs.github.io.



---

## NeRF and Gaussian Splatting SLAM in the Wild

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Fabian Schmidt, Markus Enzweiler, Abhinav Valada | cs.RO | [PDF](http://arxiv.org/pdf/2412.03263v1){: .btn .btn-green } |

**Abstract**: Navigating outdoor environments with visual Simultaneous Localization and
Mapping (SLAM) systems poses significant challenges due to dynamic scenes,
lighting variations, and seasonal changes, requiring robust solutions. While
traditional SLAM methods struggle with adaptability, deep learning-based
approaches and emerging neural radiance fields as well as Gaussian
Splatting-based SLAM methods, offer promising alternatives. However, these
methods have primarily been evaluated in controlled indoor environments with
stable conditions, leaving a gap in understanding their performance in
unstructured and variable outdoor settings. This study addresses this gap by
evaluating these methods in natural outdoor environments, focusing on camera
tracking accuracy, robustness to environmental factors, and computational
efficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate
that neural SLAM methods achieve superior robustness, particularly under
challenging conditions such as low light, but at a high computational cost. At
the same time, traditional methods perform the best across seasons but are
highly sensitive to variations in lighting conditions. The code of the
benchmark is publicly available at
https://github.com/iis-esslingen/nerf-3dgs-benchmark.

Comments:
- 5 pages, 2 figures, 4 tables

---

## Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2412.03526v1){: .btn .btn-green } |

**Abstract**: Recent advancements in static feed-forward scene reconstruction have
demonstrated significant progress in high-quality novel view synthesis.
However, these models often struggle with generalizability across diverse
environments and fail to effectively handle dynamic content. We present BTimer
(short for BulletTimer), the first motion-aware feed-forward model for
real-time reconstruction and novel view synthesis of dynamic scenes. Our
approach reconstructs the full scene in a 3D Gaussian Splatting representation
at a given target ('bullet') timestamp by aggregating information from all the
context frames. Such a formulation allows BTimer to gain scalability and
generalization by leveraging both static and dynamic scene datasets. Given a
casual monocular dynamic video, BTimer reconstructs a bullet-time scene within
150ms while reaching state-of-the-art performance on both static and dynamic
scene datasets, even compared with optimization-based approaches.

Comments:
- Project website:
  https://research.nvidia.com/labs/toronto-ai/bullet-timer/

---

## SGSST: Scaling Gaussian Splatting StyleTransfer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Bruno Galerne, Jianling Wang, Lara Raad, Jean-Michel Morel | cs.CV | [PDF](http://arxiv.org/pdf/2412.03371v1){: .btn .btn-green } |

**Abstract**: Applying style transfer to a full 3D environment is a challenging task that
has seen many developments since the advent of neural rendering. 3D Gaussian
splatting (3DGS) has recently pushed further many limits of neural rendering in
terms of training speed and reconstruction quality. This work introduces SGSST:
Scaling Gaussian Splatting Style Transfer, an optimization-based method to
apply style transfer to pretrained 3DGS scenes. We demonstrate that a new
multiscale loss based on global neural statistics, that we name SOS for
Simultaneously Optimized Scales, enables style transfer to ultra-high
resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such
high image resolutions, it also produces superior visual quality as assessed by
thorough qualitative, quantitative and perceptual comparisons.



---

## RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Yoonwoo Jeong, Junmyeong Lee, Hoseung Choi, Minsu Cho | cs.CV | [PDF](http://arxiv.org/pdf/2412.03077v1){: .btn .btn-green } |

**Abstract**: Dynamic view synthesis (DVS) has advanced remarkably in recent years,
achieving high-fidelity rendering while reducing computational costs. Despite
the progress, optimizing dynamic neural fields from casual videos remains
challenging, as these videos do not provide direct 3D information, such as
camera trajectories or the underlying scene geometry. In this work, we present
RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual
videos. It effectively learns motion and underlying geometry of scenes by
separating dynamic and static primitives, and ensures that the learned motion
and geometry are physically plausible by incorporating motion and geometric
regularization terms. We also introduce a comprehensive benchmark, Kubric-MRig,
that provides extensive camera and object motion along with simultaneous
multi-view captures, features that are absent in previous benchmarks.
Experimental results demonstrate that the proposed method significantly
outperforms previous pose-free dynamic neural fields and achieves competitive
rendering quality compared to existing pose-free static neural fields. The code
and data are publicly available at https://rodygs.github.io/.

Comments:
- Project Page: https://rodygs.github.io/

---

## GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Zhiyuan Chen, Fan Lu, Guo Yu, Bin Li, Sanqing Qu, Yuan Huang, Changhong Fu, Guang Chen | cs.CV | [PDF](http://arxiv.org/pdf/2412.02267v1){: .btn .btn-green } |

**Abstract**: Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is
crucial for robotic manipulation. However, existing approaches typically rely
on accurate depth information, which is non-trivial to obtain in real-world
scenarios. Although depth estimation algorithms can be employed, geometric
inaccuracy can lead to failures in RGBD-based pose tracking methods. To address
this challenge, we introduce GSGTrack, a novel RGB-based pose tracking
framework that jointly optimizes geometry and pose. Specifically, we adopt 3D
Gaussian Splatting to create an optimizable 3D representation, which is learned
simultaneously with a graph-based geometry optimization to capture the object's
appearance features and refine its geometry. However, the joint optimization
process is susceptible to perturbations from noisy pose and geometry data.
Thus, we propose an object silhouette loss to address the issue of pixel-wise
loss being overly sensitive to pose noise during tracking. To mitigate the
geometric ambiguities caused by inaccurate depth information, we propose a
geometry-consistent image pair selection strategy, which filters out
low-confidence pairs and ensures robust geometric optimization. Extensive
experiments on the OnePose and HO3D datasets demonstrate the effectiveness of
GSGTrack in both 6DoF pose tracking and object reconstruction.



---

## SparseLGS: Sparse View Language Embedded Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Jun Hu, Zhang Chen, Zhong Li, Yi Xu, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.02245v2){: .btn .btn-green } |

**Abstract**: Recently, several studies have combined Gaussian Splatting to obtain scene
representations with language embeddings for open-vocabulary 3D scene
understanding. While these methods perform well, they essentially require very
dense multi-view inputs, limiting their applicability in real-world scenarios.
In this work, we propose SparseLGS to address the challenge of 3D scene
understanding with pose-free and sparse view input images. Our method leverages
a learning-based dense stereo model to handle pose-free and sparse inputs, and
a three-step region matching approach to address the multi-view semantic
inconsistency problem, which is especially important for sparse inputs.
Different from directly learning high-dimensional CLIP features, we extract
low-dimensional information and build bijections to avoid excessive learning
and storage costs. We introduce a reconstruction loss during semantic training
to improve Gaussian positions and shapes. To the best of our knowledge, we are
the first to address the 3D semantic field problem with sparse pose-free
inputs. Experimental results show that SparseLGS achieves comparable quality
when reconstructing semantic fields with fewer inputs (3-4 views) compared to
previous SOTA methods with dense input. Besides, when using the same sparse
input, SparseLGS leads significantly in quality and heavily improves the
computation speed (5$\times$speedup). Project page:
https://ustc3dv.github.io/SparseLGS

Comments:
- Project Page: https://ustc3dv.github.io/SparseLGS

---

## How to Use Diffusion Priors under Sparse Views?

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Qisen Wang, Yifan Zhao, Jiawei Ma, Jia Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.02225v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis under sparse views has been a long-term important
challenge in 3D reconstruction. Existing works mainly rely on introducing
external semantic or depth priors to supervise the optimization of 3D
representations. However, the diffusion model, as an external prior that can
directly provide visual supervision, has always underperformed in sparse-view
3D reconstruction using Score Distillation Sampling (SDS) due to the low
information entropy of sparse views compared to text, leading to optimization
challenges caused by mode deviation. To this end, we present a thorough
analysis of SDS from the mode-seeking perspective and propose Inline Prior
Guided Score Matching (IPSM), which leverages visual inline priors provided by
pose relationships between viewpoints to rectify the rendered image
distribution and decomposes the original optimization objective of SDS, thereby
offering effective diffusion visual guidance without any fine-tuning or
pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts
3D Gaussian Splatting as the backbone and supplements depth and geometry
consistency regularization based on IPSM to further improve inline priors and
rectified distribution. Experimental results on different public datasets show
that our method achieves state-of-the-art reconstruction quality. The code is
released at https://github.com/iCVTEAM/IPSM.



---

## Gaussian Object Carver: Object-Compositional Gaussian Splatting with  surfaces completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Liu Liu, Xinjie Wang, Jiaxiong Qiu, Tianwei Lin, Xiaolin Zhou, Zhizhong Su | cs.CV | [PDF](http://arxiv.org/pdf/2412.02075v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction is a foundational problem in computer vision. Despite
recent advancements in Neural Implicit Representations (NIR), existing methods
often lack editability and compositional flexibility, limiting their use in
scenarios requiring high interactivity and object-level manipulation. In this
paper, we introduce the Gaussian Object Carver (GOC), a novel, efficient, and
scalable framework for object-compositional 3D scene reconstruction. GOC
leverages 3D Gaussian Splatting (GS), enriched with monocular geometry priors
and multi-view geometry regularization, to achieve high-quality and flexible
reconstruction. Furthermore, we propose a zero-shot Object Surface Completion
(OSC) model, which uses 3D priors from 3d object data to reconstruct unobserved
surfaces, ensuring object completeness even in occluded areas. Experimental
results demonstrate that GOC improves reconstruction efficiency and geometric
fidelity. It holds promise for advancing the practical application of digital
twins in embodied AI, AR/VR, and interactive simulation environments.



---

## Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D  Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Abdurrahman Zeybey, Mehmet Ergezer, Tommy Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2412.02803v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has advanced radiance field reconstruction, enabling
high-quality view synthesis and fast rendering in 3D modeling. While
adversarial attacks on object detection models are well-studied for 2D images,
their impact on 3D models remains underexplored. This work introduces the
Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate
adversarial noise targeting the CLIP vision-language model. M-IFGSM
specifically alters the object of interest by focusing perturbations on masked
regions, degrading the performance of CLIP's zero-shot object detection
capability when applied to 3D models. Using eight objects from the Common
Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces
the accuracy and confidence of the model, with adversarial noise being nearly
imperceptible to human observers. The top-1 accuracy in original model renders
drops from 95.4\% to 12.5\% for train images and from 91.2\% to 35.4\% for test
images, with confidence levels reflecting this shift from true classification
to misclassification, underscoring the risks of adversarial attacks on 3D
models in applications such as autonomous driving, robotics, and surveillance.
The significance of this research lies in its potential to expose
vulnerabilities in modern 3D vision models, including radiance fields,
prompting the development of more robust defenses and security measures in
critical real-world applications.

Comments:
- Accepted to Safe Generative AI Workshop @ NeurIPS 2024:
  https://neurips.cc/virtual/2024/workshop/84705

---

## SparseGrasp: Robotic Grasping via 3D Semantic Gaussian Splatting from  Sparse Multi-View RGB Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Junqiu Yu, Xinlin Ren, Yongchong Gu, Haitao Lin, Tianyu Wang, Yi Zhu, Hang Xu, Yu-Gang Jiang, Xiangyang Xue, Yanwei Fu | cs.RO | [PDF](http://arxiv.org/pdf/2412.02140v1){: .btn .btn-green } |

**Abstract**: Language-guided robotic grasping is a rapidly advancing field where robots
are instructed using human language to grasp specific objects. However,
existing methods often depend on dense camera views and struggle to quickly
update scenes, limiting their effectiveness in changeable environments.
  In contrast, we propose SparseGrasp, a novel open-vocabulary robotic grasping
system that operates efficiently with sparse-view RGB images and handles scene
updates fastly. Our system builds upon and significantly enhances existing
computer vision modules in robotic learning. Specifically, SparseGrasp utilizes
DUSt3R to generate a dense point cloud as the initialization for 3D Gaussian
Splatting (3DGS), maintaining high fidelity even under sparse supervision.
Importantly, SparseGrasp incorporates semantic awareness from recent vision
foundation models. To further improve processing efficiency, we repurpose
Principal Component Analysis (PCA) to compress features from 2D models.
Additionally, we introduce a novel render-and-compare strategy that ensures
rapid scene updates, enabling multi-turn grasping in changeable environments.
  Experimental results show that SparseGrasp significantly outperforms
state-of-the-art methods in terms of both speed and adaptability, providing a
robust solution for multi-turn grasping in changeable environment.



---

## AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent  Gaussian Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong | cs.CV | [PDF](http://arxiv.org/pdf/2412.02684v1){: .btn .btn-green } |

**Abstract**: Generating animatable human avatars from a single image is essential for
various digital human modeling applications. Existing 3D reconstruction methods
often struggle to capture fine details in animatable models, while generative
approaches for controllable animation, though avoiding explicit 3D modeling,
suffer from viewpoint inconsistencies in extreme poses and computational
inefficiencies. In this paper, we address these challenges by leveraging the
power of generative models to produce detailed multi-view canonical pose
images, which help resolve ambiguities in animatable human reconstruction. We
then propose a robust method for 3D reconstruction of inconsistent images,
enabling real-time rendering during inference. Specifically, we adapt a
transformer-based video generation model to generate multi-view canonical pose
images and normal maps, pretraining on a large-scale video dataset to improve
generalization. To handle view inconsistencies, we recast the reconstruction
problem as a 4D task and introduce an efficient 3D modeling approach using 4D
Gaussian Splatting. Experiments demonstrate that our method achieves
photorealistic, real-time animation of 3D human avatars from in-the-wild
images, showcasing its effectiveness and generalization capability.

Comments:
- Project Page: https://lingtengqiu.github.io/2024/AniGS/

---

## TimeWalker: Personalized Neural Space for Lifelong Head Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Dongwei Pan, Yang Li, Hongsheng Li, Kwan-Yee Lin | cs.CV | [PDF](http://arxiv.org/pdf/2412.02421v1){: .btn .btn-green } |

**Abstract**: We present TimeWalker, a novel framework that models realistic, full-scale 3D
head avatars of a person on lifelong scale. Unlike current human head avatar
pipelines that capture identity at the momentary level(e.g., instant
photography or short videos), TimeWalker constructs a person's comprehensive
identity from unstructured data collection over his/her various life stages,
offering a paradigm to achieve full reconstruction and animation of that person
at different moments of life. At the heart of TimeWalker's success is a novel
neural parametric model that learns personalized representation with the
disentanglement of shape, expression, and appearance across ages. Central to
our methodology are the concepts of two aspects: (1) We track back to the
principle of modeling a person's identity in an additive combination of average
head representation in the canonical space, and moment-specific head attribute
representations driven from a set of neural head basis. To learn the set of
head basis that could represent the comprehensive head variations in a compact
manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It
dynamically adjusts the number and blend weights of neural head bases,
according to both shared and specific traits of the target person over ages.
(2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian
splatting representation, to model head motion deformations like facial
expressions without losing the realism of rendering and reconstruction.
DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that
utilize the priors from parametric model, and move/rotate with the change of
expression. Through extensive experimental evaluations, we show TimeWalker's
ability to reconstruct and animate avatars across decoupled dimensions with
realistic rendering effects, demonstrating a way to achieve personalized 'time
traveling' in a breeze.

Comments:
- Project Page: https://timewalker2024.github.io/timewalker.github.io/
  , Video: https://www.youtube.com/watch?v=x8cpOVMY_ko

---

## RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex  Motions via Relay Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Qiankun Gao, Yanmin Wu, Chengxiang Wen, Jiarui Meng, Luyang Tang, Jie Chen, Ronggang Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.02493v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic scenes with large-scale and complex motions remains a
significant challenge. Recent techniques like Neural Radiance Fields and 3D
Gaussian Splatting (3DGS) have shown promise but still struggle with scenes
involving substantial movement. This paper proposes RelayGS, a novel method
based on 3DGS, specifically designed to represent and reconstruct highly
dynamic scenes. Our RelayGS learns a complete 4D representation with canonical
3D Gaussians and a compact motion field, consisting of three stages. First, we
learn a fundamental 3DGS from all frames, ignoring temporal scene variations,
and use a learnable mask to separate the highly dynamic foreground from the
minimally moving background. Second, we replicate multiple copies of the
decoupled foreground Gaussians from the first stage, each corresponding to a
temporal segment, and optimize them using pseudo-views constructed from
multiple frames within each segment. These Gaussians, termed Relay Gaussians,
act as explicit relay nodes, simplifying and breaking down large-scale motion
trajectories into smaller, manageable segments. Finally, we jointly learn the
scene's temporal motion and refine the canonical Gaussians learned from the
first two stages. We conduct thorough experiments on two dynamic scene datasets
featuring large and complex motions, where our RelayGS outperforms
state-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs
real-world basketball game scenes in a much more complete and coherent manner,
whereas previous methods usually struggle to capture the complex motion of
players. Code will be publicly available at https://github.com/gqk/RelayGS

Comments:
- Technical Report. GitHub: https://github.com/gqk/RelayGS

---

## Multi-robot autonomous 3D reconstruction using Gaussian splatting with  Semantic guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Jing Zeng, Qi Ye, Tianle Liu, Yang Xu, Jin Li, Jinming Xu, Liang Li, Jiming Chen | cs.RO | [PDF](http://arxiv.org/pdf/2412.02249v1){: .btn .btn-green } |

**Abstract**: Implicit neural representations and 3D Gaussian splatting (3DGS) have shown
great potential for scene reconstruction. Recent studies have expanded their
applications in autonomous reconstruction through task assignment methods.
However, these methods are mainly limited to single robot, and rapid
reconstruction of large-scale scenes remains challenging. Additionally,
task-driven planning based on surface uncertainty is prone to being trapped in
local optima. To this end, we propose the first 3DGS-based centralized
multi-robot autonomous 3D reconstruction framework. To further reduce time cost
of task generation and improve reconstruction quality, we integrate online
open-vocabulary semantic segmentation with surface uncertainty of 3DGS,
focusing view sampling on regions with high instance uncertainty. Finally, we
develop a multi-robot collaboration strategy with mode and task assignments
improving reconstruction quality while ensuring planning efficiency. Our method
demonstrates the highest reconstruction quality among all planning methods and
superior planning efficiency compared to existing multi-robot methods. We
deploy our method on multiple robots, and results show that it can effectively
plan view paths and reconstruct scenes with high quality.



---

## 6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Yufeng Jin, Vignesh Prasad, Snehal Jauhri, Mathias Franzius, Georgia Chalvatzaki | cs.CV | [PDF](http://arxiv.org/pdf/2412.01543v1){: .btn .btn-green } |

**Abstract**: Efficient and accurate object pose estimation is an essential component for
modern vision systems in many applications such as Augmented Reality,
autonomous driving, and robotics. While research in model-based 6D object pose
estimation has delivered promising results, model-free methods are hindered by
the high computational load in rendering and inferring consistent poses of
arbitrary objects in a live RGB-D video stream. To address this issue, we
present 6DOPE-GS, a novel method for online 6D object pose estimation \&
tracking with a single RGB-D camera by effectively leveraging advances in
Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of
Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses
and 3D object reconstruction. To achieve the necessary efficiency and accuracy
for live tracking, our method uses incremental 2D Gaussian Splatting with an
intelligent dynamic keyframe selection procedure to achieve high spatial object
coverage and prevent erroneous pose updates. We also propose an opacity
statistic-based pruning mechanism for adaptive Gaussian density control, to
ensure training stability and efficiency. We evaluate our method on the HO3D
and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of
state-of-the-art baselines for model-free simultaneous 6D pose tracking and
reconstruction while providing a 5$\times$ speedup. We also demonstrate the
method's suitability for live, dynamic object tracking and reconstruction in a
real-world setting.



---

## 3DSceneEditor: Controllable 3D Scene Editing with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Ziyang Yan, Lei Li, Yihua Shao, Siyu Chen, Wuzong Kai, Jenq-Neng Hwang, Hao Zhao, Fabio Remondino | cs.CV | [PDF](http://arxiv.org/pdf/2412.01583v1){: .btn .btn-green } |

**Abstract**: The creation of 3D scenes has traditionally been both labor-intensive and
costly, requiring designers to meticulously configure 3D assets and
environments. Recent advancements in generative AI, including text-to-3D and
image-to-3D methods, have dramatically reduced the complexity and cost of this
process. However, current techniques for editing complex 3D scenes continue to
rely on generally interactive multi-step, 2D-to-3D projection methods and
diffusion-based techniques, which often lack precision in control and hamper
real-time performance. In this work, we propose 3DSceneEditor, a fully 3D-based
paradigm for real-time, precise editing of intricate 3D scenes using Gaussian
Splatting. Unlike conventional methods, 3DSceneEditor operates through a
streamlined 3D pipeline, enabling direct manipulation of Gaussians for
efficient, high-quality edits based on input prompts.The proposed framework (i)
integrates a pre-trained instance segmentation model for semantic labeling;
(ii) employs a zero-shot grounding approach with CLIP to align target objects
with user prompts; and (iii) applies scene modifications, such as object
addition, repositioning, recoloring, replacing, and deletion directly on
Gaussians. Extensive experimental results show that 3DSceneEditor achieves
superior editing precision and speed with respect to current SOTA 3D scene
editing approaches, establishing a new benchmark for efficient and interactive
3D scene customization.

Comments:
- Project Page: https://ziyangyan.github.io/3DSceneEditor

---

## ULSR-GS: Ultra Large-scale Surface Reconstruction Gaussian Splatting  with Multi-View Geometric Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zhuoxiao Li, Shanliang Yao, Qizhong Gao, Angel F. Garcia-Fernandez, Yong Yue, Xiaohui Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2412.01402v1){: .btn .btn-green } |

**Abstract**: While Gaussian Splatting (GS) demonstrates efficient and high-quality scene
rendering and small area surface extraction ability, it falls short in handling
large-scale aerial image surface extraction tasks. To overcome this, we present
ULSR-GS, a framework dedicated to high-fidelity surface extraction in
ultra-large-scale scenes, addressing the limitations of existing GS-based mesh
extraction methods. Specifically, we propose a point-to-photo partitioning
approach combined with a multi-view optimal view matching principle to select
the best training images for each sub-region. Additionally, during training,
ULSR-GS employs a densification strategy based on multi-view geometric
consistency to enhance surface extraction details. Experimental results
demonstrate that ULSR-GS outperforms other state-of-the-art GS-based works on
large-scale aerial photogrammetry benchmark datasets, significantly improving
surface extraction accuracy in complex urban environments. Project page:
https://ulsrgs.github.io.

Comments:
- Project page: https://ulsrgs.github.io

---

## RGBDS-SLAM: A RGB-D Semantic Dense SLAM Based on 3D Multi Level Pyramid  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zhenzhong Cao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01217v1){: .btn .btn-green } |

**Abstract**: High-quality reconstruction is crucial for dense SLAM. Recent popular
approaches utilize 3D Gaussian Splatting (3D GS) techniques for RGB, depth, and
semantic reconstruction of scenes. However, these methods often overlook issues
of detail and consistency in different parts of the scene. To address this, we
propose RGBDS-SLAM, a RGB-D semantic dense SLAM system based on 3D multi-level
pyramid gaussian splatting, which enables high-quality dense reconstruction of
scene RGB, depth, and semantics.In this system, we introduce a 3D multi-level
pyramid gaussian splatting method that restores scene details by extracting
multi-level image pyramids for gaussian splatting training, ensuring
consistency in RGB, depth, and semantic reconstructions. Additionally, we
design a tightly-coupled multi-features reconstruction optimization mechanism,
allowing the reconstruction accuracy of RGB, depth, and semantic maps to
mutually enhance each other during the rendering optimization process.
Extensive quantitative, qualitative, and ablation experiments on the Replica
and ScanNet public datasets demonstrate that our proposed method outperforms
current state-of-the-art methods. The open-source code will be available at:
https://github.com/zhenzhongcao/RGBDS-SLAM.



---

## CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Kai He, Chin-Hsuan Wu, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2412.01792v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D representations, such as Neural Radiance Fields and 3D
Gaussian Splatting, have greatly improved realistic scene modeling and
novel-view synthesis. However, achieving controllable and consistent editing in
dynamic 3D scenes remains a significant challenge. Previous work is largely
constrained by its editing backbones, resulting in inconsistent edits and
limited controllability. In our work, we introduce a novel framework that first
fine-tunes the InstructPix2Pix model, followed by a two-stage optimization of
the scene based on deformable 3D Gaussians. Our fine-tuning enables the model
to "learn" the editing ability from a single edited reference image,
transforming the complex task of dynamic scene editing into a simple 2D image
editing process. By directly learning editing regions and styles from the
reference, our approach enables consistent and precise local edits without the
need for tracking desired editing regions, effectively addressing key
challenges in dynamic scene editing. Then, our two-stage optimization
progressively edits the trained dynamic scene, using a designed edited image
buffer to accelerate convergence and improve temporal consistency. Compared to
state-of-the-art methods, our approach offers more flexible and controllable
local scene editing, achieving high-quality and consistent results.

Comments:
- Project page: https://ihe-kaii.github.io/CTRL-D/

---

## Diffusion Models with Anisotropic Gaussian Splatting for Image  Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Jacob Fein-Ashley, Benjamin Fein-Ashley | cs.CV | [PDF](http://arxiv.org/pdf/2412.01682v2){: .btn .btn-green } |

**Abstract**: Image inpainting is a fundamental task in computer vision, aiming to restore
missing or corrupted regions in images realistically. While recent deep
learning approaches have significantly advanced the state-of-the-art,
challenges remain in maintaining structural continuity and generating coherent
textures, particularly in large missing areas. Diffusion models have shown
promise in generating high-fidelity images but often lack the structural
guidance necessary for realistic inpainting. We propose a novel inpainting
method that combines diffusion models with anisotropic Gaussian splatting to
capture both local structures and global context effectively. By modeling
missing regions using anisotropic Gaussian functions that adapt to local image
gradients, our approach provides structural guidance to the diffusion-based
inpainting network. The Gaussian splat maps are integrated into the diffusion
process, enhancing the model's ability to generate high-fidelity and
structurally coherent inpainting results. Extensive experiments demonstrate
that our method outperforms state-of-the-art techniques, producing visually
plausible results with enhanced structural integrity and texture realism.



---

## SfM-Free 3D Gaussian Splatting via Hierarchical Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Bo Ji, Angela Yao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01553v1){: .btn .btn-green } |

**Abstract**: Standard 3D Gaussian Splatting (3DGS) relies on known or pre-computed camera
poses and a sparse point cloud, obtained from structure-from-motion (SfM)
preprocessing, to initialize and grow 3D Gaussians. We propose a novel SfM-Free
3DGS (SFGS) method for video input, eliminating the need for known camera poses
and SfM preprocessing. Our approach introduces a hierarchical training strategy
that trains and merges multiple 3D Gaussian representations -- each optimized
for specific scene regions -- into a single, unified 3DGS model representing
the entire scene. To compensate for large camera motions, we leverage video
frame interpolation models. Additionally, we incorporate multi-source
supervision to reduce overfitting and enhance representation. Experimental
results reveal that our approach significantly surpasses state-of-the-art
SfM-free novel view synthesis methods. On the Tanks and Temples dataset, we
improve PSNR by an average of 2.25dB, with a maximum gain of 3.72dB in the best
scene. On the CO3D-V2 dataset, we achieve an average PSNR boost of 1.74dB, with
a top gain of 3.90dB. The code is available at
https://github.com/jibo27/3DGS_Hierarchical_Training.



---

## GFreeDet: Exploiting Gaussian Splatting and Foundation Models for  Model-free Unseen Object Detection in the BOP Challenge 2024

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Xingyu Liu, Yingyue Li, Chengxi Li, Gu Wang, Chenyangguang Zhang, Ziqin Huang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2412.01552v2){: .btn .btn-green } |

**Abstract**: In this report, we provide the technical details of the submitted method
GFreeDet, which exploits Gaussian splatting and vision Foundation models for
the model-free unseen object Detection track in the BOP 2024 Challenge.



---

## HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Yunzhou Song, Heguang Lin, Jiahui Lei, Lingjie Liu, Kostas Daniilidis | cs.CV | [PDF](http://arxiv.org/pdf/2412.01823v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural rendering, particularly 2D Gaussian Splatting
(2DGS), have shown promising results for jointly reconstructing fine appearance
and geometry by leveraging 2D Gaussian surfels. However, current methods face
significant challenges when rendering at arbitrary viewpoints, such as
anti-aliasing for down-sampled rendering, and texture detail preservation for
high-resolution rendering. We proposed a novel method to align the 2D surfels
with texture maps and augment it with per-ray depth sorting and fisher-based
pruning for rendering consistency and efficiency. With correct order,
per-surfel texture maps significantly improve the capabilities to capture fine
details. Additionally, to render high-fidelity details in varying viewpoints,
we designed a frustum-based sampling method to mitigate the aliasing artifacts.
Experimental results on benchmarks and our custom texture-rich dataset
demonstrate that our method surpasses existing techniques, particularly in
detail preservation and anti-aliasing.

Comments:
- Project Page: https://timsong412.github.io/HDGS-ProjPage/

---

## Driving Scene Synthesis on Free-form Trajectories with Generative Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zeyu Yang, Zijie Pan, Yuankun Yang, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.01717v1){: .btn .btn-green } |

**Abstract**: Driving scene synthesis along free-form trajectories is essential for driving
simulations to enable closed-loop evaluation of end-to-end driving policies.
While existing methods excel at novel view synthesis on recorded trajectories,
they face challenges with novel trajectories due to limited views of driving
videos and the vastness of driving environments. To tackle this challenge, we
propose a novel free-form driving view synthesis approach, dubbed DriveX, by
leveraging video generative prior to optimize a 3D model across a variety of
trajectories. Concretely, we crafted an inverse problem that enables a video
diffusion model to be utilized as a prior for many-trajectory optimization of a
parametric 3D model (e.g., Gaussian splatting). To seamlessly use the
generative prior, we iteratively conduct this process during optimization. Our
resulting model can produce high-fidelity virtual driving environments outside
the recorded trajectory, enabling free-form trajectory driving simulation.
Beyond real driving scenes, DriveX can also be utilized to simulate virtual
driving worlds from AI-generated videos.



---

## Occam's LGS: A Simple Approach for Language Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Jiahuan Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2412.01807v1){: .btn .btn-green } |

**Abstract**: TL;DR: Gaussian Splatting is a widely adopted approach for 3D scene
representation that offers efficient, high-quality 3D reconstruction and
rendering. A major reason for the success of 3DGS is its simplicity of
representing a scene with a set of Gaussians, which makes it easy to interpret
and adapt. To enhance scene understanding beyond the visual representation,
approaches have been developed that extend 3D Gaussian Splatting with semantic
vision-language features, especially allowing for open-set tasks. In this
setting, the language features of 3D Gaussian Splatting are often aggregated
from multiple 2D views. Existing works address this aggregation problem using
cumbersome techniques that lead to high computational cost and training time.
  In this work, we show that the sophisticated techniques for language-grounded
3D Gaussian Splatting are simply unnecessary. Instead, we apply Occam's razor
to the task at hand and perform weighted multi-view feature aggregation using
the weights derived from the standard rendering process, followed by a simple
heuristic-based noisy Gaussian filtration. Doing so offers us state-of-the-art
results with a speed-up of two orders of magnitude. We showcase our results in
two commonly used benchmark datasets: LERF and 3D-OVS. Our simple approach
allows us to perform reasoning directly in the language features, without any
compression whatsoever. Such modeling in turn offers easy scene manipulation,
unlike the existing methods -- which we illustrate using an application of
object insertion in the scene. Furthermore, we provide a thorough discussion
regarding the significance of our contributions within the context of the
current literature. Project Page: https://insait-institute.github.io/OccamLGS/

Comments:
- Project Page: https://insait-institute.github.io/OccamLGS/

---

## Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale  Aerial-to-Ground Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Lihan Jiang, Kerui Ren, Mulin Yu, Linning Xu, Junting Dong, Tao Lu, Feng Zhao, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2412.01745v1){: .btn .btn-green } |

**Abstract**: Seamless integration of both aerial and street view images remains a
significant challenge in neural scene reconstruction and rendering. Existing
methods predominantly focus on single domain, limiting their applications in
immersive environments, which demand extensive free view exploration with large
view changes both horizontally and vertically. We introduce Horizon-GS, a novel
approach built upon Gaussian Splatting techniques, tackles the unified
reconstruction and rendering for aerial and street views. Our method addresses
the key challenges of combining these perspectives with a new training
strategy, overcoming viewpoint discrepancies to generate high-fidelity scenes.
We also curate a high-quality aerial-to-ground views dataset encompassing both
synthetic and real-world scene to advance further research. Experiments across
diverse urban scene datasets confirm the effectiveness of our method.



---

## Planar Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Farhad G. Zanjani, Hong Cai, Hanno Ackermann, Leila Mirvakhabova, Fatih Porikli | cs.CV | [PDF](http://arxiv.org/pdf/2412.01931v1){: .btn .btn-green } |

**Abstract**: This paper presents Planar Gaussian Splatting (PGS), a novel neural rendering
approach to learn the 3D geometry and parse the 3D planes of a scene, directly
from multiple RGB images. The PGS leverages Gaussian primitives to model the
scene and employ a hierarchical Gaussian mixture approach to group them.
Similar Gaussians are progressively merged probabilistically in the
tree-structured Gaussian mixtures to identify distinct 3D plane instances and
form the overall 3D scene geometry. In order to enable the grouping, the
Gaussian primitives contain additional parameters, such as plane descriptors
derived by lifting 2D masks from a general 2D segmentation model and surface
normals. Experiments show that the proposed PGS achieves state-of-the-art
performance in 3D planar reconstruction without requiring either 3D plane
labels or depth supervision. In contrast to existing supervised methods that
have limited generalizability and struggle under domain shift, PGS maintains
its performance across datasets thanks to its neural rendering and
scene-specific optimization mechanism, while also being significantly faster
than existing optimization-based approaches.

Comments:
- IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),
  2025

---

## HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Hongyu Zhou, Longzhong Lin, Jiabao Wang, Yichong Lu, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01718v1){: .btn .btn-green } |

**Abstract**: In the past few decades, autonomous driving algorithms have made significant
progress in perception, planning, and control. However, evaluating individual
components does not fully reflect the performance of entire systems,
highlighting the need for more holistic assessment methods. This motivates the
development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator
for evaluating autonomous driving algorithms. We achieve this by lifting
captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving
the rendering quality for closed-loop scenarios, and building the closed-loop
environment. In terms of rendering, We tackle challenges of novel view
synthesis in closed-loop scenarios, including viewpoint extrapolation and
360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further
enables the full closed simulation loop, dynamically updating the ego and actor
states and observations based on control commands. Moreover, HUGSIM offers a
comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo,
nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair
and realistic evaluation platform for existing autonomous driving algorithms.
HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks
the potential for fine-tuning autonomous driving algorithms in a photorealistic
closed-loop setting.

Comments:
- Our project page is at https://xdimlab.github.io/HUGSIM

---

## VR-Doh: Hands-on 3D Modeling in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Zhaofeng Luo, Zhitong Cui, Shijian Luo, Mengyu Chu, Minchen Li | cs.GR | [PDF](http://arxiv.org/pdf/2412.00814v1){: .btn .btn-green } |

**Abstract**: We present VR-Doh, a hands-on 3D modeling system designed for creating and
manipulating elastoplastic objects in virtual reality (VR). The system employs
the Material Point Method (MPM) for simulating realistic large deformations and
incorporates optimized Gaussian Splatting for seamless rendering. With direct,
hand-based interactions, users can naturally sculpt, deform, and edit objects
interactively. To achieve real-time performance, we developed localized
simulation techniques, optimized collision handling, and separated appearance
and physical representations, ensuring smooth and responsive user interaction.
The system supports both freeform creation and precise adjustments, catering to
diverse modeling tasks. A user study involving novice and experienced users
highlights the system's intuitive design, immersive feedback, and creative
potential. Compared to traditional geometry-based modeling tools, our approach
offers improved accessibility and natural interaction in specific contexts.



---

## FlashSLAM: Accelerated RGB-D SLAM for Real-Time 3D Scene Reconstruction  with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Phu Pham, Damon Conover, Aniket Bera | cs.CV | [PDF](http://arxiv.org/pdf/2412.00682v1){: .btn .btn-green } |

**Abstract**: We present FlashSLAM, a novel SLAM approach that leverages 3D Gaussian
Splatting for efficient and robust 3D scene reconstruction. Existing 3DGS-based
SLAM methods often fall short in sparse view settings and during large camera
movements due to their reliance on gradient descent-based optimization, which
is both slow and inaccurate. FlashSLAM addresses these limitations by combining
3DGS with a fast vision-based camera tracking technique, utilizing a pretrained
feature matching model and point cloud registration for precise pose estimation
in under 80 ms - a 90% reduction in tracking time compared to SplaTAM - without
costly iterative rendering. In sparse settings, our method achieves up to a 92%
improvement in average tracking accuracy over previous methods. Additionally,
it accounts for noise in depth sensors, enhancing robustness when using
unspecialized devices such as smartphones. Extensive experiments show that
FlashSLAM performs reliably across both sparse and dense settings, in synthetic
and real-world environments. Evaluations on benchmark datasets highlight its
superior accuracy and efficiency, establishing FlashSLAM as a versatile and
high-performance solution for SLAM, advancing the state-of-the-art in 3D
reconstruction across diverse applications.

Comments:
- 16 pages, 9 figures, 13 tables

---

## CtrlNeRF: The Generative Neural Radiation Fields for the Controllable  Synthesis of High-fidelity 3D-Aware Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Jian Liu, Zhen Yu | cs.CV | [PDF](http://arxiv.org/pdf/2412.00754v1){: .btn .btn-green } |

**Abstract**: The neural radiance field (NERF) advocates learning the continuous
representation of 3D geometry through a multilayer perceptron (MLP). By
integrating this into a generative model, the generative neural radiance field
(GRAF) is capable of producing images from random noise z without 3D
supervision. In practice, the shape and appearance are modeled by z_s and z_a,
respectively, to manipulate them separately during inference. However, it is
challenging to represent multiple scenes using a solitary MLP and precisely
control the generation of 3D geometry in terms of shape and appearance. In this
paper, we introduce a controllable generative model (i.e. \textbf{CtrlNeRF})
that uses a single MLP network to represent multiple scenes with shared
weights. Consequently, we manipulated the shape and appearance codes to realize
the controllable generation of high-fidelity images with 3D consistency.
Moreover, the model enables the synthesis of novel views that do not exist in
the training sets via camera pose alteration and feature interpolation.
Extensive experiments were conducted to demonstrate its superiority in 3D-aware
image generation compared to its counterparts.



---

## Ref-GS: Directional Factorization for 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Youjia Zhang, Anpei Chen, Yumin Wan, Zikai Song, Junqing Yu, Yawei Luo, Wei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2412.00905v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Ref-GS, a novel approach for directional light
factorization in 2D Gaussian splatting, which enables photorealistic
view-dependent appearance rendering and precise geometry recovery. Ref-GS
builds upon the deferred rendering of Gaussian splatting and applies
directional encoding to the deferred-rendered surface, effectively reducing the
ambiguity between orientation and viewing angle. Next, we introduce a spherical
Mip-grid to capture varying levels of surface roughness, enabling
roughness-aware Gaussian shading. Additionally, we propose a simple yet
efficient geometry-lighting factorization that connects geometry and lighting
via the vector outer product, significantly reducing renderer overhead when
integrating volumetric attributes. Our method achieves superior photorealistic
rendering for a range of open-world scenes while also accurately recovering
geometry.

Comments:
- Project page: https://ref-gs.github.io/

---

## DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.00851v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting have shown promising results.
Existing methods typically assume static scenes and/or multiple images with
prior poses. Dynamics, sparse views, and unknown poses significantly increase
the problem complexity due to insufficient geometric constraints. To overcome
this challenge, we propose a method that can use only two images without prior
poses to fit Gaussians in dynamic environments. To achieve this, we introduce
two technical contributions. First, we propose an object-level two-view bundle
adjustment. This strategy decomposes dynamic scenes into piece-wise rigid
components, and jointly estimates the camera pose and motions of dynamic
objects. Second, we design an SE(3) field-driven Gaussian training method. It
enables fine-grained motion modeling through learnable per-Gaussian
transformations. Our method leads to high-fidelity novel view synthesis of
dynamic scenes while accurately preserving temporal consistency and object
motion. Experiments on both synthetic and real-world datasets demonstrate that
our method significantly outperforms state-of-the-art approaches designed for
the cases of static environments, multiple images, and/or known poses. Our
project page is available at https://colin-de.github.io/DynSUP/.



---

## ChatSplat: 3D Conversational Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Hanlin Chen, Fangyin Wei, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2412.00734v1){: .btn .btn-green } |

**Abstract**: Humans naturally interact with their 3D surroundings using language, and
modeling 3D language fields for scene understanding and interaction has gained
growing interest. This paper introduces ChatSplat, a system that constructs a
3D language field, enabling rich chat-based interaction within 3D space. Unlike
existing methods that primarily use CLIP-derived language features focused
solely on segmentation, ChatSplat facilitates interaction on three levels:
objects, views, and the entire 3D scene. For view-level interaction, we
designed an encoder that encodes the rendered feature map of each view into
tokens, which are then processed by a large language model (LLM) for
conversation. At the scene level, ChatSplat combines multi-view tokens,
enabling interactions that consider the entire scene. For object-level
interaction, ChatSplat uses a patch-wise language embedding, unlike LangSplat's
pixel-wise language embedding that implicitly includes mask and embedding.
Here, we explicitly decouple the language embedding into separate mask and
feature map representations, allowing more flexible object-level interaction.
To address the challenge of learning 3D Gaussians posed by the complex and
diverse distribution of language embeddings used in the LLM, we introduce a
learnable normalization technique to standardize these embeddings, facilitating
effective learning. Extensive experimental results demonstrate that ChatSplat
supports multi-level interactions -- object, view, and scene -- within 3D
space, enhancing both understanding and engagement.



---

## SAGA: Surface-Aligned Gaussian Avatar

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Ronghan Chen, Yang Cong, Jiayue Liu | cs.CV | [PDF](http://arxiv.org/pdf/2412.00845v1){: .btn .btn-green } |

**Abstract**: This paper presents a Surface-Aligned Gaussian representation for creating
animatable human avatars from monocular videos,aiming at improving the novel
view and pose synthesis performance while ensuring fast training and real-time
rendering. Recently,3DGS has emerged as a more efficient and expressive
alternative to NeRF, and has been used for creating dynamic human avatars.
However,when applied to the severely ill-posed task of monocular dynamic
reconstruction, the Gaussians tend to overfit the constantly changing regions
such as clothes wrinkles or shadows since these regions cannot provide
consistent supervision, resulting in noisy geometry and abrupt deformation that
typically fail to generalize under novel views and poses.To address these
limitations, we present SAGA,i.e.,Surface-Aligned Gaussian Avatar,which aligns
the Gaussians with a mesh to enforce well-defined geometry and consistent
deformation, thereby improving generalization under novel views and poses.
Unlike existing strict alignment methods that suffer from limited expressive
power and low realism,SAGA employs a two-stage alignment strategy where the
Gaussians are first adhered on while then detached from the mesh, thus
facilitating both good geometry and high expressivity. In the Adhered Stage, we
improve the flexibility of Adhered-on-Mesh Gaussians by allowing them to flow
on the mesh, in contrast to existing methods that rigidly bind Gaussians to
fixed location. In the second Detached Stage, we introduce a Gaussian-Mesh
Alignment regularization, which allows us to unleash the expressivity by
detaching the Gaussians but maintain the geometric alignment by minimizing
their location and orientation offsets from the bound triangles. Finally, since
the Gaussians may drift outside the bound triangles during optimization, an
efficient Walking-on-Mesh strategy is proposed to dynamically update the bound
triangles.

Comments:
- Submitted to TPAMI. Major Revision. Project page:
  https://gostinshell.github.io/SAGA/
