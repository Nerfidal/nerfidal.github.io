---
layout: default
title: December 2024
parent: Papers
nav_order: 202412
---

<!---metadata--->


## Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:  Diverse-Resolution Training Outperforms Interpolation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-24 | Anselm Krainovic, Stefan Ruschke, Reinhard Heckel | cs.CV | [PDF](http://arxiv.org/pdf/2412.18584v1){: .btn .btn-green } |

**Abstract**: Deep learning-based 3D imaging, in particular magnetic resonance imaging
(MRI), is challenging because of limited availability of 3D training data.
Therefore, 2D diffusion models trained on 2D slices are starting to be
leveraged for 3D MRI reconstruction. However, as we show in this paper,
existing methods pertain to a fixed voxel size, and performance degrades when
the voxel size is varied, as it is often the case in clinical practice. In this
paper, we propose and study several approaches for resolution-robust 3D MRI
reconstruction with 2D diffusion priors. As a result of this investigation, we
obtain a simple resolution-robust variational 3D reconstruction approach based
on diffusion-guided regularization of randomly sampled 2D slices. This method
provides competitive reconstruction quality compared to posterior sampling
baselines. Towards resolving the sensitivity to resolution-shifts, we
investigate state-of-the-art model-based approaches including Gaussian
splatting, neural representations, and infinite-dimensional diffusion models,
as well as a simple data-centric approach of training the diffusion model on
several resolutions. Our experiments demonstrate that the model-based
approaches fail to close the performance gap in 3D MRI. In contrast, the
data-centric approach of training the diffusion model on various resolutions
effectively provides a resolution-robust method without compromising accuracy.



---

## RSGaussian:3D Gaussian Splatting with LiDAR for Aerial Remote Sensing  Novel View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-24 | Yiling Yao, Wenjuan Zhang, Bing Zhang, Bocheng Li, Yaning Wang, Bowen Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.18380v1){: .btn .btn-green } |

**Abstract**: This study presents RSGaussian, an innovative novel view synthesis (NVS)
method for aerial remote sensing scenes that incorporate LiDAR point cloud as
constraints into the 3D Gaussian Splatting method, which ensures that Gaussians
grow and split along geometric benchmarks, addressing the overgrowth and
floaters issues occurs. Additionally, the approach introduces coordinate
transformations with distortion parameters for camera models to achieve
pixel-level alignment between LiDAR point clouds and 2D images, facilitating
heterogeneous data fusion and achieving the high-precision geo-alignment
required in aerial remote sensing. Depth and plane consistency losses are
incorporated into the loss function to guide Gaussians towards real depth and
plane representations, significantly improving depth estimation accuracy.
Experimental results indicate that our approach has achieved novel view
synthesis that balances photo-realistic visual quality and high-precision
geometric estimation under aerial remote sensing datasets. Finally, we have
also established and open-sourced a dense LiDAR point cloud dataset along with
its corresponding aerial multi-view images, AIR-LONGYAN.



---

## LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-23 | Hao Li, Roy Qin, Zhengyu Zou, Diqi He, Bohan Li, Bingquan Dai, Dingewn Zhang, Junwei Han | cs.CV | [PDF](http://arxiv.org/pdf/2412.17635v2){: .btn .btn-green } |

**Abstract**: Applying Gaussian Splatting to perception tasks for 3D scene understanding is
becoming increasingly popular. Most existing works primarily focus on rendering
2D feature maps from novel viewpoints, which leads to an imprecise 3D language
field with outlier languages, ultimately failing to align objects in 3D space.
By utilizing masked images for feature extraction, these approaches also lack
essential contextual information, leading to inaccurate feature representation.
To this end, we propose a Language-Embedded Surface Field (LangSurf), which
accurately aligns the 3D language fields with the surface of objects,
facilitating precise 2D and 3D segmentation with text query, widely expanding
the downstream tasks such as removal and editing. The core of LangSurf is a
joint training strategy that flattens the language Gaussian on the object
surfaces using geometry supervision and contrastive losses to assign accurate
language features to the Gaussians of objects. In addition, we also introduce
the Hierarchical-Context Awareness Module to extract features at the image
level for contextual information then perform hierarchical mask pooling using
masks segmented by SAM to obtain fine-grained language features in different
hierarchies. Extensive experiments on open-vocabulary 2D and 3D semantic
segmentation demonstrate that LangSurf outperforms the previous
state-of-the-art method LangSplat by a large margin. As shown in Fig. 1, our
method is capable of segmenting objects in 3D space, thus boosting the
effectiveness of our approach in instance recognition, removal, and editing,
which is also supported by comprehensive experiments.
\url{https://langsurf.github.io}.

Comments:
- \url{https://langsurf.github.io}

---

## Exploring Dynamic Novel View Synthesis Technologies for Cinematography

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-23 | Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull | cs.CV | [PDF](http://arxiv.org/pdf/2412.17532v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) has shown significant promise for applications in
cinematographic production, particularly through the exploitation of Neural
Radiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D
scenes, enabling the creation of new shots that are challenging to capture in
the real world due to set topology or expensive equipment requirement. This
innovation also offers cinematographic advantages such as smooth camera
movements, virtual re-shoots, slow-motion effects, etc. This paper explores
dynamic NVS with the aim of facilitating the model selection process. We
showcase its potential through a short montage filmed using various NVS models.



---

## ActiveGS: Active Scene Reconstruction using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-23 | Liren Jin, Xingguang Zhong, Yue Pan, Jens Behley, Cyrill Stachniss, Marija PopoviÄ‡ | cs.RO | [PDF](http://arxiv.org/pdf/2412.17769v1){: .btn .btn-green } |

**Abstract**: Robotics applications often rely on scene reconstructions to enable
downstream tasks. In this work, we tackle the challenge of actively building an
accurate map of an unknown scene using an on-board RGB-D camera. We propose a
hybrid map representation that combines a Gaussian splatting map with a coarse
voxel map, leveraging the strengths of both representations: the high-fidelity
scene reconstruction capabilities of Gaussian splatting and the spatial
modelling strengths of the voxel map. The core of our framework is an effective
confidence modelling technique for the Gaussian splatting map to identify
under-reconstructed areas, while utilising spatial information from the voxel
map to target unexplored areas and assist in collision-free path planning. By
actively collecting scene information in under-reconstructed and unexplored
areas for map updates, our approach achieves superior Gaussian splatting
reconstruction results compared to state-of-the-art approaches. Additionally,
we demonstrate the applicability of our active scene reconstruction framework
in the real world using an unmanned aerial vehicle.



---

## Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained  Tiling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-23 | Hao Gui, Lin Hu, Rui Chen, Mingxiao Huang, Yuxin Yin, Jin Yang, Yong Wu | cs.CV | [PDF](http://arxiv.org/pdf/2412.17378v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is increasingly attracting attention in both
academia and industry owing to its superior visual quality and rendering speed.
However, training a 3DGS model remains a time-intensive task, especially in
load imbalance scenarios where workload diversity among pixels and Gaussian
spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS,
a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS
training process, perfectly solving load-imbalance issues. First, we
innovatively introduce the inter-block dynamic workload distribution technique
to map workloads to Streaming Multiprocessor(SM) resources within a single GPU
dynamically, which constitutes the foundation of load balancing. Second, we are
the first to propose the Gaussian-wise parallel rendering technique to
significantly reduce workload divergence inside a warp, which serves as a
critical component in addressing load imbalance. Based on the above two
methods, we further creatively put forward the fine-grained combined load
balancing technique to uniformly distribute workload across all SMs, which
boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we
present a self-adaptive render kernel selection strategy during the 3DGS
training process based on different load-balance situations, which effectively
improves training efficiency.



---

## GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal  Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-23 | Jingqiu Zhou, Lue Fan, Xuesong Chen, Linjiang Huang, Si Liu, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.17715v1){: .btn .btn-green } |

**Abstract**: In this paper, we present GaussianPainter, the first method to paint a point
cloud into 3D Gaussians given a reference image. GaussianPainter introduces an
innovative feed-forward approach to overcome the limitations of time-consuming
test-time optimization in 3D Gaussian splatting. Our method addresses a
critical challenge in the field: the non-uniqueness problem inherent in the
large parameter space of 3D Gaussian splatting. This space, encompassing
rotation, anisotropic scales, and spherical harmonic coefficients, introduces
the challenge of rendering similar images from substantially different Gaussian
fields. As a result, feed-forward networks face instability when attempting to
directly predict high-quality Gaussian fields, struggling to converge on
consistent parameters for a given output. To address this issue, we propose to
estimate a surface normal for each point to determine its Gaussian rotation.
This strategy enables the network to effectively predict the remaining Gaussian
parameters in the constrained space. We further enhance our approach with an
appearance injection module, incorporating reference image appearance into
Gaussian fields via a multiscale triplane representation. Our method
successfully balances efficiency and fidelity in 3D Gaussian generation,
achieving high-quality, diverse, and robust 3D content creation from point
clouds in a single forward pass.

Comments:
- To appear in AAAI 2025

---

## CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed  Learning for Large Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-23 | Yuanyuan Gao, Yalun Dai, Hao Li, Weicai Ye, Junyi Chen, Danpeng Chen, Dingwen Zhang, Tong He, Guofeng Zhang, Junwei Han | cs.CV | [PDF](http://arxiv.org/pdf/2412.17612v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene
reconstruction. However, most existing GS-based surface reconstruction methods
focus on 3D objects or limited scenes. Directly applying these methods to
large-scale scene reconstruction will pose challenges such as high memory
costs, excessive time consumption, and lack of geometric detail, which makes it
difficult to implement in practical applications. To address these issues, we
propose a multi-agent collaborative fast 3DGS surface reconstruction framework
based on distributed learning for large-scale surface reconstruction.
Specifically, we develop local model compression (LMC) and model aggregation
schemes (MAS) to achieve high-quality surface representation of large scenes
while reducing GPU memory consumption. Extensive experiments on Urban3d,
MegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast
and scalable high-fidelity surface reconstruction and photorealistic rendering.
Our project page is available at \url{https://gyy456.github.io/CoSurfGS}.

Comments:
- Our project page is available at
  \url{https://gyy456.github.io/CoSurfGS}

---

## Editing Implicit and Explicit Representations of Radiance Fields: A  Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-23 | Arthur Hubert, Gamal Elghazaly, Raphael Frank | cs.CV | [PDF](http://arxiv.org/pdf/2412.17628v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent
years by offering a new volumetric representation, which is compact and
provides high-quality image rendering. However, the methods to edit those
radiance fields developed slower than the many improvements to other aspects of
NeRF. With the recent development of alternative radiance field-based
representations inspired by NeRF as well as the worldwide rise in popularity of
text-to-image models, many new opportunities and strategies have emerged to
provide radiance field editing. In this paper, we deliver a comprehensive
survey of the different editing methods present in the literature for NeRF and
other similar radiance field representations. We propose a new taxonomy for
classifying existing works based on their editing methodologies, review
pioneering models, reflect on current and potential new applications of
radiance field editing, and compare state-of-the-art approaches in terms of
editing options and performance.



---

## FaceLift: Single Image to 3D Head with View Generation and GS-LRM


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-23 | Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu | cs.CV | [PDF](http://arxiv.org/pdf/2412.17812v1){: .btn .btn-green } |

**Abstract**: We present FaceLift, a feed-forward approach for rapid, high-quality,
360-degree head reconstruction from a single image. Our pipeline begins by
employing a multi-view latent diffusion model that generates consistent side
and back views of the head from a single facial input. These generated views
then serve as input to a GS-LRM reconstructor, which produces a comprehensive
3D representation using Gaussian splats. To train our system, we develop a
dataset of multi-view renderings using synthetic 3D human head as-sets. The
diffusion-based multi-view generator is trained exclusively on synthetic head
images, while the GS-LRM reconstructor undergoes initial training on Objaverse
followed by fine-tuning on synthetic head data. FaceLift excels at preserving
identity and maintaining view consistency across views. Despite being trained
solely on synthetic data, FaceLift demonstrates remarkable generalization to
real-world images. Through extensive qualitative and quantitative evaluations,
we show that FaceLift outperforms state-of-the-art methods in 3D head
reconstruction, highlighting its practical applicability and robust performance
on real-world images. In addition to single image reconstruction, FaceLift
supports video inputs for 4D novel view synthesis and seamlessly integrates
with 2D reanimation techniques to enable 3D facial animation. Project page:
https://weijielyu.github.io/FaceLift.

Comments:
- Project page: https://weijielyu.github.io/FaceLift

---

## GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality  Photorealistic 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-22 | Hanqing Jiang, Xiaojun Xiang, Han Sun, Hongjie Li, Liyang Zhou, Xiaoyu Zhang, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.16809v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently attracted wide attentions in
various areas such as 3D navigation, Virtual Reality (VR) and 3D simulation,
due to its photorealistic and efficient rendering performance. High-quality
reconstrution of 3DGS relies on sufficient splats and a reasonable distribution
of these splats to fit real geometric surface and texture details, which turns
out to be a challenging problem. We present GeoTexDensifier, a novel
geometry-texture-aware densification strategy to reconstruct high-quality
Gaussian splats which better comply with the geometric structure and texture
richness of the scene. Specifically, our GeoTexDensifier framework carries out
an auxiliary texture-aware densification method to produce a denser
distribution of splats in fully textured areas, while keeping sparsity in
low-texture regions to maintain the quality of Gaussian point cloud. Meanwhile,
a geometry-aware splitting strategy takes depth and normal priors to guide the
splitting sampling and filter out the noisy splats whose initial positions are
far from the actual geometric surfaces they aim to fit, under a Validation of
Depth Ratio Change checking. With the help of relative monocular depth prior,
such geometry-aware validation can effectively reduce the influence of
scattered Gaussians to the final rendering quality, especially in regions with
weak textures or without sufficient training views. The texture-aware
densification and geometry-aware splitting strategies are fully combined to
obtain a set of high-quality Gaussian splats. We experiment our GeoTexDensifier
framework on various datasets and compare our Novel View Synthesis results to
other state-of-the-art 3DGS approaches, with detailed quantitative and
qualitative evaluations to demonstrate the effectiveness of our method in
producing more photorealistic 3DGS models.

Comments:
- 12 pages, 8 figures, 1 table

---

## GSemSplat: Generalizable Semantic 3D Gaussian Splatting from  Uncalibrated Image Pairs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-22 | Xingrui Wang, Cuiling Lan, Hanxin Zhu, Zhibo Chen, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2412.16932v1){: .btn .btn-green } |

**Abstract**: Modeling and understanding the 3D world is crucial for various applications,
from augmented reality to robotic navigation. Recent advancements based on 3D
Gaussian Splatting have integrated semantic information from multi-view images
into Gaussian primitives. However, these methods typically require costly
per-scene optimization from dense calibrated images, limiting their
practicality. In this paper, we consider the new task of generalizable 3D
semantic field modeling from sparse, uncalibrated image pairs. Building upon
the Splatt3R architecture, we introduce GSemSplat, a framework that learns
open-vocabulary semantic representations linked to 3D Gaussians without the
need for per-scene optimization, dense image collections or calibration. To
ensure effective and reliable learning of semantic features in 3D space, we
employ a dual-feature approach that leverages both region-specific and
context-aware semantic features as supervision in the 2D space. This allows us
to capitalize on their complementary strengths. Experimental results on the
ScanNet++ dataset demonstrate the effectiveness and superiority of our approach
compared to the traditional scene-specific method. We hope our work will
inspire more research into generalizable 3D understanding.



---

## LUCES-MV: A Multi-View Dataset for Near-Field Point Light Source  Photometric Stereo

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-21 | Fotios Logothetis, Ignas Budvytis, Stephan Liwicki, Roberto Cipolla | cs.CV | [PDF](http://arxiv.org/pdf/2412.16737v1){: .btn .btn-green } |

**Abstract**: The biggest improvements in Photometric Stereo (PS) field has recently come
from adoption of differentiable volumetric rendering techniques such as NeRF or
Neural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV
benchmark. However, while there are sizeable datasets for environment lit
objects such as Digital Twin Catalogue (DTS), there are only several small
Photometric Stereo datasets which often lack challenging objects (simple,
smooth, untextured) and practical, small form factor (near-field) light setup.
  To address this, we propose LUCES-MV, the first real-world, multi-view
dataset designed for near-field point light source photometric stereo. Our
dataset includes 15 objects with diverse materials, each imaged under varying
light conditions from an array of 15 LEDs positioned 30 to 40 centimeters from
the camera center. To facilitate transparent end-to-end evaluation, our dataset
provides not only ground truth normals and ground truth object meshes and poses
but also light and camera calibration images.
  We evaluate state-of-the-art near-field photometric stereo algorithms,
highlighting their strengths and limitations across different material and
shape complexities. LUCES-MV dataset offers an important benchmark for
developing more robust, accurate and scalable real-world Photometric Stereo
based 3D reconstruction methods.



---

## Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for  Optimized Structural Integrity

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-21 | Tianqi Shen, Shaohua Liu, Jiaqi Feng, Ziye Ma, Ning An | cs.CV | [PDF](http://arxiv.org/pdf/2412.16619v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has emerged as a crucial technique for representing
discrete volumetric radiance fields. It leverages unique parametrization to
mitigate computational demands in scene optimization. This work introduces
Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key
limitations in current approaches: compromised pixel-level structural integrity
due to incomplete initial geometric coverage, and inadequate feature-level
integrity from insufficient topological constraints during optimization. To
overcome these limitations, Topology-GS incorporates a novel interpolation
strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused
regularization term based on persistent barcodes, named PersLoss. LPVI utilizes
persistent homology to guide adaptive interpolation, enhancing point coverage
in low-curvature areas while preserving topological structure. PersLoss aligns
the visual perceptual similarity of rendered images with ground truth by
constraining distances between their topological features. Comprehensive
experiments on three novel-view synthesis benchmarks demonstrate that
Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS
metrics, while maintaining efficient memory usage. This study pioneers the
integration of topology with 3D-GS, laying the groundwork for future research
in this area.



---

## OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional  Images with Editable Capabilities

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-21 | Suyoung Lee, Jaeyoung Chung, Kihoon Kim, Jaeyoo Huh, Gunhee Lee, Minsoo Lee, Kyoung Mu Lee | cs.CV | [PDF](http://arxiv.org/pdf/2412.16604v1){: .btn .btn-green } |

**Abstract**: Feed-forward 3D Gaussian Splatting (3DGS) models have gained significant
popularity due to their ability to generate scenes immediately without needing
per-scene optimization. Although omnidirectional images are getting more
popular since they reduce the computation for image stitching to composite a
holistic scene, existing feed-forward models are only designed for perspective
images. The unique optical properties of omnidirectional images make it
difficult for feature encoders to correctly understand the context of the image
and make the Gaussian non-uniform in space, which hinders the image quality
synthesized from novel views. We propose OmniSplat, a pioneering work for fast
feed-forward 3DGS generation from a few omnidirectional images. We introduce
Yin-Yang grid and decompose images based on it to reduce the domain gap between
omnidirectional and perspective images. The Yin-Yang grid can use the existing
CNN structure as it is, but its quasi-uniform characteristic allows the
decomposed image to be similar to a perspective image, so it can exploit the
strong prior knowledge of the learned feed-forward network. OmniSplat
demonstrates higher reconstruction accuracy than existing feed-forward networks
trained on perspective images. Furthermore, we enhance the segmentation
consistency between omnidirectional images by leveraging attention from the
encoder of OmniSplat, providing fast and clean 3DGS editing results.



---

## CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from  Defocused Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-20 | Jungho Lee, Suhwan Cho, Taeoh Kim, Ho-Deok Jang, Minhyeok Lee, Geonho Cha, Dongyoon Wee, Dogyoon Lee, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2412.16028v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.

Comments:
- Project Page: https://Jho-Yonsei.github.io/CoCoGaussian/

---

## IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-20 | Chun Gu, Xiaofei Wei, Zixuan Zeng, Yuxuan Yao, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.15867v1){: .btn .btn-green } |

**Abstract**: In inverse rendering, accurately modeling visibility and indirect radiance
for incident light is essential for capturing secondary effects. Due to the
absence of a powerful Gaussian ray tracer, previous 3DGS-based methods have
either adopted a simplified rendering equation or used learnable parameters to
approximate incident light, resulting in inaccurate material and lighting
estimations. To this end, we introduce inter-reflective Gaussian splatting
(IRGS) for inverse rendering. To capture inter-reflection, we apply the full
rendering equation without simplification and compute incident radiance on the
fly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we
present an efficient optimization scheme to handle the computational demands of
Monte Carlo sampling for rendering equation evaluation. Furthermore, we
introduce a novel strategy for querying the indirect radiance of incident light
when relighting the optimized scenes. Extensive experiments on multiple
standard benchmarks validate the effectiveness of IRGS, demonstrating its
capability to accurately model complex inter-reflection effects.

Comments:
- Project page: https://fudan-zvg.github.io/IRGS

---

## NeuroPump: Simultaneous Geometric and Color Rectification for Underwater  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-20 | Yue Guo, Haoxiang Liao, Haibin Ling, Bingyao Huang | cs.CV | [PDF](http://arxiv.org/pdf/2412.15890v1){: .btn .btn-green } |

**Abstract**: Underwater image restoration aims to remove geometric and color distortions
due to water refraction, absorption and scattering. Previous studies focus on
restoring either color or the geometry, but to our best knowledge, not both.
However, in practice it may be cumbersome to address the two rectifications
one-by-one. In this paper, we propose NeuroPump, a self-supervised method to
simultaneously optimize and rectify underwater geometry and color as if water
were pumped out. The key idea is to explicitly model refraction, absorption and
scattering in Neural Radiance Field (NeRF) pipeline, such that it not only
performs simultaneous geometric and color rectification, but also enables to
synthesize novel views and optical effects by controlling the decoupled
parameters. In addition, to address issue of lack of real paired ground truth
images, we propose an underwater 360 benchmark dataset that has real paired
(i.e., with and without water) images. Our method clearly outperforms other
baselines both quantitatively and qualitatively.



---

## AvatarPerfect: User-Assisted 3D Gaussian Splatting Avatar Refinement  with Automatic Pose Suggestion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-20 | Jotaro Sakamiya, I-Chao Shen, Jinsong Zhang, Mustafa Doga Dogan, Takeo Igarashi | cs.HC | [PDF](http://arxiv.org/pdf/2412.15609v1){: .btn .btn-green } |

**Abstract**: Creating high-quality 3D avatars using 3D Gaussian Splatting (3DGS) from a
monocular video benefits virtual reality and telecommunication applications.
However, existing automatic methods exhibit artifacts under novel poses due to
limited information in the input video. We propose AvatarPerfect, a novel
system that allows users to iteratively refine 3DGS avatars by manually editing
the rendered avatar images. In each iteration, our system suggests a new body
and camera pose to help users identify and correct artifacts. The edited images
are then used to update the current avatar, and our system suggests the next
body and camera pose for further refinement. To investigate the effectiveness
of AvatarPerfect, we conducted a user study comparing our method to an existing
3DGS editor SuperSplat, which allows direct manipulation of Gaussians without
automatic pose suggestions. The results indicate that our system enables users
to obtain higher quality refined 3DGS avatars than the existing 3DGS editor.

Comments:
- 13 pages

---

## EGSRAL: An Enhanced 3D Gaussian Splatting based Renderer with Automated  Labeling for Large-Scale Driving Scene

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-20 | Yixiong Huo, Guangfeng Jiang, Hongyang Wei, Ji Liu, Song Zhang, Han Liu, Xingliang Huang, Mingjie Lu, Jinzhang Peng, Dong Li, Lu Tian, Emad Barsoum | cs.CV | [PDF](http://arxiv.org/pdf/2412.15550v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D GS) has gained popularity due to its faster
rendering speed and high-quality novel view synthesis. Some researchers have
explored using 3D GS for reconstructing driving scenes. However, these methods
often rely on various data types, such as depth maps, 3D boxes, and
trajectories of moving objects. Additionally, the lack of annotations for
synthesized images limits their direct application in downstream tasks. To
address these issues, we propose EGSRAL, a 3D GS-based method that relies
solely on training images without extra annotations. EGSRAL enhances 3D GS's
capability to model both dynamic objects and static backgrounds and introduces
a novel adaptor for auto labeling, generating corresponding annotations based
on existing annotations. We also propose a grouping strategy for vanilla 3D GS
to address perspective issues in rendering large-scale, complex scenes. Our
method achieves state-of-the-art performance on multiple datasets without any
extra annotation. For example, the PSNR metric reaches 29.04 on the nuScenes
dataset. Moreover, our automated labeling can significantly improve the
performance of 2D/3D detection tasks. Code is available at
https://github.com/jiangxb98/EGSRAL.

Comments:
- AAAI2025

---

## Interactive Scene Authoring with Specialized Generative Primitives

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-20 | ClÃ©ment Jambon, Changwoon Choi, Dongsu Zhang, Olga Sorkine-Hornung, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2412.16253v1){: .btn .btn-green } |

**Abstract**: Generating high-quality 3D digital assets often requires expert knowledge of
complex design tools. We introduce Specialized Generative Primitives, a
generative framework that allows non-expert users to author high-quality 3D
scenes in a seamless, lightweight, and controllable manner. Each primitive is
an efficient generative model that captures the distribution of a single
exemplar from the real world. With our framework, users capture a video of an
environment, which we turn into a high-quality and explicit appearance model
thanks to 3D Gaussian Splatting. Users then select regions of interest guided
by semantically-aware features. To create a generative primitive, we adapt
Generative Cellular Automata to single-exemplar training and controllable
generation. We decouple the generative task from the appearance model by
operating on sparse voxels and we recover a high-quality output with a
subsequent sparse patch consistency step. Each primitive can be trained within
10 minutes and used to author new scenes interactively in a fully compositional
manner. We showcase interactive sessions where various primitives are extracted
from real-world scenes and controlled to create 3D assets and scenes in a few
minutes. We also demonstrate additional capabilities of our primitives:
handling various 3D representations to control generation, transferring
appearances, and editing geometries.



---

## SOUS VIDE: Cooking Visual Drone Navigation Policies in a Gaussian  Splatting Vacuum

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-20 | JunEn Low, Maximilian Adang, Javier Yu, Keiko Nagami, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2412.16346v1){: .btn .btn-green } |

**Abstract**: We propose a new simulator, training approach, and policy architecture,
collectively called SOUS VIDE, for end-to-end visual drone navigation. Our
trained policies exhibit zero-shot sim-to-real transfer with robust real-world
performance using only on-board perception and computation. Our simulator,
called FiGS, couples a computationally simple drone dynamics model with a high
visual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly
simulate drone flights producing photorealistic images at up to 130 fps. We use
FiGS to collect 100k-300k observation-action pairs from an expert MPC with
privileged state and dynamics information, randomized over dynamics parameters
and spatial disturbances. We then distill this expert MPC into an end-to-end
visuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net
processes color image, optical flow and IMU data streams into low-level body
rate and thrust commands at 20Hz onboard a drone. Crucially, SV-Net includes a
Rapid Motor Adaptation (RMA) module that adapts at runtime to variations in
drone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE
policies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in
ambient brightness, shifting or removing objects from the scene, and people
moving aggressively through the drone's visual field. Code, data, and
experiment videos can be found on our project page:
https://stanfordmsl.github.io/SousVide/.



---

## NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for  Vision of Autonomous Systems

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-20 | Laura Weihl, Bilal Wehbe, Andrzej WÄ…sowski | cs.CV | [PDF](http://arxiv.org/pdf/2412.16141v1){: .btn .btn-green } |

**Abstract**: Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.



---

## LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-19 | Pou-Chun Kung, Xianling Zhang, Katherine A. Skinner, Nikita Jaipuria | cs.CV | [PDF](http://arxiv.org/pdf/2412.15447v1){: .btn .btn-green } |

**Abstract**: Photorealistic 3D scene reconstruction plays an important role in autonomous
driving, enabling the generation of novel data from existing datasets to
simulate safety-critical scenarios and expand training data without additional
acquisition costs. Gaussian Splatting (GS) facilitates real-time,
photorealistic rendering with an explicit 3D Gaussian representation of the
scene, providing faster processing and more intuitive scene editing than the
implicit Neural Radiance Fields (NeRFs). While extensive GS research has
yielded promising advancements in autonomous driving applications, they
overlook two critical aspects: First, existing methods mainly focus on
low-speed and feature-rich urban scenes and ignore the fact that highway
scenarios play a significant role in autonomous driving. Second, while LiDARs
are commonplace in autonomous driving platforms, existing methods learn
primarily from images and use LiDAR only for initial estimates or without
precise sensor modeling, thus missing out on leveraging the rich depth
information LiDAR offers and limiting the ability to synthesize LiDAR data. In
this paper, we propose a novel GS method for dynamic scene synthesis and
editing with improved scene reconstruction through LiDAR supervision and
support for LiDAR rendering. Unlike prior works that are tested mostly on urban
datasets, to the best of our knowledge, we are the first to focus on the more
challenging and highly relevant highway scenes for autonomous driving, with
sparse sensor views and monotone backgrounds.



---

## SqueezeMe: Efficient Gaussian Avatars for VR

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-19 | Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon | cs.CV | [PDF](http://arxiv.org/pdf/2412.15171v2){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has enabled real-time 3D human avatars with unprecedented
levels of visual quality. While previous methods require a desktop GPU for
real-time inference of a single avatar, we aim to squeeze multiple Gaussian
avatars onto a portable virtual reality headset with real-time drivable
inference. We begin by training a previous work, Animatable Gaussians, on a
high quality dataset captured with 512 cameras. The Gaussians are animated by
controlling base set of Gaussians with linear blend skinning (LBS) motion and
then further adjusting the Gaussians with a neural network decoder to correct
their appearance. When deploying the model on a Meta Quest 3 VR headset, we
find two major computational bottlenecks: the decoder and the rendering. To
accelerate the decoder, we train the Gaussians in UV-space instead of
pixel-space, and we distill the decoder to a single neural network layer.
Further, we discover that neighborhoods of Gaussians can share a single
corrective from the decoder, which provides an additional speedup. To
accelerate the rendering, we develop a custom pipeline in Vulkan that runs on
the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently
at 72 FPS on a VR headset. Demo videos are at
https://forresti.github.io/squeezeme.

Comments:
- v2

---

## GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-19 | Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun | cs.CV | [PDF](http://arxiv.org/pdf/2412.14579v1){: .btn .btn-green } |

**Abstract**: 3D occupancy perception is gaining increasing attention due to its capability
to offer detailed and precise environment representations. Previous
weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU
varying by 5-10 points due to sampling count along camera rays. Recently,
real-time Gaussian splatting has gained widespread popularity in 3D
reconstruction, and the occupancy prediction task can also be viewed as a
reconstruction task. Consequently, we propose GSRender, which naturally employs
3D Gaussian Splatting for occupancy prediction, simplifying the sampling
process. In addition, the limitations of 2D supervision result in duplicate
predictions along the same camera ray. We implemented the Ray Compensation (RC)
module, which mitigates this issue by compensating for features from adjacent
frames. Finally, we redesigned the loss to eliminate the impact of dynamic
objects from adjacent frames. Extensive experiments demonstrate that our
approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while
narrowing the gap with 3D supervision methods. Our code will be released soon.



---

## Bright-NeRF:Brightening Neural Radiance Field with Color Restoration  from Low-light Raw Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-19 | Min Wang, Xin Huang, Guoqing Zhou, Qifeng Guo, Qing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.14547v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have demonstrated prominent performance in
novel view synthesis. However, their input heavily relies on image acquisition
under normal light conditions, making it challenging to learn accurate scene
representation in low-light environments where images typically exhibit
significant noise and severe color distortion. To address these challenges, we
propose a novel approach, Bright-NeRF, which learns enhanced and high-quality
radiance fields from multi-view low-light raw images in an unsupervised manner.
Our method simultaneously achieves color restoration, denoising, and enhanced
novel view synthesis. Specifically, we leverage a physically-inspired model of
the sensor's response to illumination and introduce a chromatic adaptation loss
to constrain the learning of response, enabling consistent color perception of
objects regardless of lighting conditions. We further utilize the raw data's
properties to expose the scene's intensity automatically. Additionally, we have
collected a multi-view low-light raw image dataset to advance research in this
field. Experimental results demonstrate that our proposed method significantly
outperforms existing 2D and 3D approaches. Our code and dataset will be made
publicly available.

Comments:
- Accepted by AAAI2025

---

## Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF  Separation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-19 | Yongsung Kim, Minjun Park, Jooyoung Choi, Sungroh Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2412.14568v1){: .btn .btn-green } |

**Abstract**: Recent learning-based Multi-View Stereo models have demonstrated
state-of-the-art performance in sparse-view 3D reconstruction. However,
directly applying 3D Gaussian Splatting (3DGS) as a refinement step following
these models presents challenges. We hypothesize that the excessive positional
degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting
color patterns at the cost of structural fidelity. To address this, we propose
reprojection-based DoF separation, a method distinguishing positional DoFs in
terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To
independently manage each DoF, we introduce a reprojection process along with
tailored constraints for each DoF. Through experiments across various datasets,
we confirm that separating the positional DoFs of Gaussians and applying
targeted constraints effectively suppresses geometric artifacts, producing
reconstruction results that are both visually and geometrically plausible.

Comments:
- 11 pages

---

## SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-19 | Zhuowen Shen, Yuan Liu, Zhang Chen, Zhong Li, Jiepeng Wang, Yongqing Liang, Zhengming Yu, Jingdong Zhang, Yi Xu, Scott Schaefer, Xin Li, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.15400v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has achieved impressive improvements for both novel-view
synthesis and surface reconstruction from multi-view images. However, current
methods still struggle to reconstruct high-quality surfaces from only sparse
view input images using Gaussian splatting. In this paper, we propose a novel
method called SolidGS to address this problem. We observed that the
reconstructed geometry can be severely inconsistent across multi-views, due to
the property of Gaussian function in geometry rendering. This motivates us to
consolidate all Gaussians by adopting a more solid kernel function, which
effectively improves the surface reconstruction quality. With the additional
help of geometrical regularization and monocular normal estimation, our method
achieves superior performance on the sparse view surface reconstruction than
all the Gaussian splatting methods and neural field methods on the widely used
DTU, Tanks-and-Temples, and LLFF datasets.

Comments:
- Project page: https://mickshen7558.github.io/projects/SolidGS/

---

## Dream to Manipulate: Compositional World Models Empowering Robot  Imitation Learning with Imagination

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-19 | Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves | cs.RO | [PDF](http://arxiv.org/pdf/2412.14957v1){: .btn .btn-green } |

**Abstract**: A world model provides an agent with a representation of its environment,
enabling it to predict the causal consequences of its actions. Current world
models typically cannot directly and explicitly imitate the actual environment
in front of a robot, often resulting in unrealistic behaviors and
hallucinations that make them unsuitable for real-world applications. In this
paper, we introduce a new paradigm for constructing world models that are
explicit representations of the real world and its dynamics. By integrating
cutting-edge advances in real-time photorealism with Gaussian Splatting and
physics simulators, we propose the first compositional manipulation world
model, which we call DreMa. DreMa replicates the observed world and its
dynamics, allowing it to imagine novel configurations of objects and predict
the future consequences of robot actions. We leverage this capability to
generate new data for imitation learning by applying equivariant
transformations to a small set of demonstrations. Our evaluations across
various settings demonstrate significant improvements in both accuracy and
robustness by incrementing actions and object distributions, reducing the data
needed to learn a policy and improving the generalization of the agents. As a
highlight, we show that a real Franka Emika Panda robot, powered by DreMa's
imagination, can successfully learn novel physical tasks from just a single
example per task variation (one-shot policy learning). Our project page and
source code can be found in https://leobarcellona.github.io/DreamToManipulate/



---

## DreaMark: Rooting Watermark in Score Distillation Sampling Generated  Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-18 | Xingyu Zhu, Xiapu Luo, Xuetao Wei | cs.GR | [PDF](http://arxiv.org/pdf/2412.15278v1){: .btn .btn-green } |

**Abstract**: Recent advancements in text-to-3D generation can generate neural radiance
fields (NeRFs) with score distillation sampling, enabling 3D asset creation
without real-world data capture. With the rapid advancement in NeRF generation
quality, protecting the copyright of the generated NeRF has become increasingly
important. While prior works can watermark NeRFs in a post-generation way, they
suffer from two vulnerabilities. First, a delay lies between NeRF generation
and watermarking because the secret message is embedded into the NeRF model
post-generation through fine-tuning. Second, generating a non-watermarked NeRF
as an intermediate creates a potential vulnerability for theft. To address both
issues, we propose Dreamark to embed a secret message by backdooring the NeRF
during NeRF generation. In detail, we first pre-train a watermark decoder.
Then, the Dreamark generates backdoored NeRFs in a way that the target secret
message can be verified by the pre-trained watermark decoder on an arbitrary
trigger viewport. We evaluate the generation quality and watermark robustness
against image- and model-level attacks. Extensive experiments show that the
watermarking process will not degrade the generation quality, and the watermark
achieves 90+% accuracy among both image-level attacks (e.g., Gaussian noise)
and model-level attacks (e.g., pruning attack).



---

## Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance  Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-18 | Tao Lu, Ankit Dhiman, R Srinath, Emre Arslan, Angela Xing, Yuanbo Xiangli, R Venkatesh Babu, Srinath Sridhar | cs.CV | [PDF](http://arxiv.org/pdf/2412.13547v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis is an important problem in computer vision with
applications in 3D reconstruction, mixed reality, and robotics. Recent methods
like 3D Gaussian Splatting (3DGS) have become the preferred method for this
task, providing high-quality novel views in real time. However, the training
time of a 3DGS model is slow, often taking 30 minutes for a scene with 200
views. In contrast, our goal is to reduce the optimization time by training for
fewer steps while maintaining high rendering quality. Specifically, we combine
the guidance from both the position error and the appearance error to achieve a
more effective densification. To balance the rate between adding new Gaussians
and fitting old Gaussians, we develop a convergence-aware budget control
mechanism. Moreover, to make the densification process more reliable, we
selectively add new Gaussians from mostly visited regions. With these designs,
we reduce the Gaussian optimization steps to one-third of the previous approach
while achieving a comparable or even better novel view rendering quality. To
further facilitate the rapid fitting of 4K resolution images, we introduce a
dilation-based rendering technique. Our method, Turbo-GS, speeds up
optimization for typical scenes and scales well to high-resolution (4K)
scenarios on standard datasets. Through extensive experiments, we show that our
method is significantly faster in optimization than other methods while
retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.

Comments:
- Project page: https://ivl.cs.brown.edu/research/turbo-gs

---

## Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data  Presentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-18 | Yunqi Guo, Kaiyuan Hou, Heming Fu, Hongkai Chen, Zhenyu Yan, Guoliang Xing, Xiaofan Jiang | cs.HC | [PDF](http://arxiv.org/pdf/2412.13509v1){: .btn .btn-green } |

**Abstract**: Understanding sensor data can be challenging for non-experts because of the
complexity and unique semantic meanings of sensor modalities. This calls for
intuitive and effective methods to present sensor information. However,
creating intuitive sensor data visualizations presents three key challenges:
the variability of sensor readings, gaps in domain comprehension, and the
dynamic nature of sensor data. To address these issues, we develop Vivar, a
novel AR system that integrates multi-modal sensor data and presents 3D
volumetric content for visualization. In particular, we introduce a cross-modal
embedding approach that maps sensor data into a pre-trained visual embedding
space through barycentric interpolation. This allows for accurate and
continuous integration of multi-modal sensor information. Vivar also
incorporates sensor-aware AR scene generation using foundation models and 3D
Gaussian Splatting (3DGS) without requiring domain expertise. In addition,
Vivar leverages latent reuse and caching strategies to accelerate 2D and AR
content generation. Our extensive experiments demonstrate that our system
achieves 11$\times$ latency reduction without compromising quality. A user
study involving over 485 participants, including domain experts, demonstrates
Vivar's effectiveness in accuracy, consistency, and real-world applicability,
paving the way for more intuitive sensor data visualization.



---

## GAGS: Granularity-Aware Feature Distillation for Language Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-18 | Yuning Peng, Haiping Wang, Yuan Liu, Chenglu Wen, Zhen Dong, Bisheng Yang | cs.CV | [PDF](http://arxiv.org/pdf/2412.13654v1){: .btn .btn-green } |

**Abstract**: 3D open-vocabulary scene understanding, which accurately perceives complex
semantic properties of objects in space, has gained significant attention in
recent years. In this paper, we propose GAGS, a framework that distills 2D CLIP
features into 3D Gaussian splatting, enabling open-vocabulary queries for
renderings on arbitrary viewpoints. The main challenge of distilling 2D
features for 3D fields lies in the multiview inconsistency of extracted 2D
features, which provides unstable supervision for the 3D feature field. GAGS
addresses this challenge with two novel strategies. First, GAGS associates the
prompt point density of SAM with the camera distances, which significantly
improves the multiview consistency of segmentation results. Second, GAGS
further decodes a granularity factor to guide the distillation process and this
granularity factor can be learned in a unsupervised manner to only select the
multiview consistent 2D features in the distillation process. Experimental
results on two datasets demonstrate significant performance and stability
improvements of GAGS in visual grounding and semantic segmentation, with an
inference speed 2$\times$ faster than baseline methods. The code and additional
results are available at https://pz0826.github.io/GAGS-Webpage/ .

Comments:
- Project page: https://pz0826.github.io/GAGS-Webpage/

---

## 4D Radar-Inertial Odometry based on Gaussian Modeling and  Multi-Hypothesis Scan Matching

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-18 | Fernando Amodeo, Luis Merino, Fernando Caballero | cs.RO | [PDF](http://arxiv.org/pdf/2412.13639v1){: .btn .btn-green } |

**Abstract**: 4D millimeter-wave (mmWave) radars are sensors that provide robustness
against adverse weather conditions (rain, snow, fog, etc.), and as such they
are increasingly being used for odometry and SLAM applications. However, the
noisy and sparse nature of the returned scan data proves to be a challenging
obstacle for existing point cloud matching based solutions, especially those
originally intended for more accurate sensors such as LiDAR. Inspired by visual
odometry research around 3D Gaussian Splatting, in this paper we propose using
freely positioned 3D Gaussians to create a summarized representation of a radar
point cloud tolerant to sensor noise, and subsequently leverage its inherent
probability distribution function for registration (similar to NDT). Moreover,
we propose simultaneously optimizing multiple scan matching hypotheses in order
to further increase the robustness of the system against local optima of the
function. Finally, we fuse our Gaussian modeling and scan matching algorithms
into an EKF radar-inertial odometry system designed after current best
practices. Experiments show that our Gaussian-based odometry is able to
outperform current baselines on a well-known 4D radar dataset used for
evaluation.

Comments:
- Our code and results can be publicly accessed at:
  https://github.com/robotics-upo/gaussian-rio

---

## GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-18 | Xiaobao Wei, Peng Chen, Ming Lu, Hui Chen, Feng Tian | cs.CV | [PDF](http://arxiv.org/pdf/2412.13983v1){: .btn .btn-green } |

**Abstract**: Rendering photorealistic head avatars from arbitrary viewpoints is crucial
for various applications like virtual reality. Although previous methods based
on Neural Radiance Fields (NeRF) can achieve impressive results, they lack
fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have
improved rendering quality and real-time performance but still require
significant storage overhead. In this paper, we introduce a method called
GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians
for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an
appearance GNN to generate the attributes of the 3D Gaussians from the tracked
mesh. Therefore, our method can store the GNN models instead of the 3D
Gaussians, significantly reducing the storage overhead to just 10MB. To reduce
the impact of face-tracking errors, we also present a novel graph-guided
optimization module to refine face-tracking parameters during training.
Finally, we introduce a 3D-aware enhancer for post-processing to enhance the
rendering quality. We conduct comprehensive experiments to demonstrate the
advantages of GraphAvatar, surpassing existing methods in visual fidelity and
storage consumption. The ablation study sheds light on the trade-offs between
rendering quality and model size. The code will be released at:
https://github.com/ucwxb/GraphAvatar

Comments:
- accepted by AAAI2025

---

## Real-time Free-view Human Rendering from Sparse-view RGB Videos using  Double Unprojected Textures


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt, Marc Habermann | cs.CV | [PDF](http://arxiv.org/pdf/2412.13183v1){: .btn .btn-green } |

**Abstract**: Real-time free-view human rendering from sparse-view RGB inputs is a
challenging task due to the sensor scarcity and the tight time budget. To
ensure efficiency, recent methods leverage 2D CNNs operating in texture space
to learn rendering primitives. However, they either jointly learn geometry and
appearance, or completely ignore sparse image information for geometry
estimation, significantly harming visual quality and robustness to unseen body
poses. To address these issues, we present Double Unprojected Textures, which
at the core disentangles coarse geometric deformation estimation from
appearance synthesis, enabling robust and photorealistic 4K rendering in
real-time. Specifically, we first introduce a novel image-conditioned template
deformation network, which estimates the coarse deformation of the human
template from a first unprojected texture. This updated geometry is then used
to apply a second and more accurate texture unprojection. The resulting texture
map has fewer artifacts and better alignment with input views, which benefits
our learning of finer-level geometry and appearance represented by Gaussian
splats. We validate the effectiveness and efficiency of the proposed method in
quantitative and qualitative experiments, which significantly surpasses other
state-of-the-art methods.

Comments:
- Project page: https://vcai.mpi-inf.mpg.de/projects/DUT/

---

## 4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel  Reconstruction from Sparse-View Dynamic DSA Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Zhentao Liu, Ruyi Zha, Huangxuan Zhao, Hongdong Li, Zhiming Cui | eess.IV | [PDF](http://arxiv.org/pdf/2412.12919v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D vessel structures from sparse-view dynamic digital
subtraction angiography (DSA) images enables accurate medical assessment while
reducing radiation exposure. Existing methods often produce suboptimal results
or require excessive computation time. In this work, we propose 4D radiative
Gaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently.
In detail, we represent the vessels with 4D radiative Gaussian kernels. Each
kernel has time-invariant geometry parameters, including position, rotation,
and scale, to model static vessel structures. The time-dependent central
attenuation of each kernel is predicted from a compact neural network to
capture the temporal varying response of contrast agent flow. We splat these
Gaussian kernels to synthesize DSA images via X-ray rasterization and optimize
the model with real captured ones. The final 3D vessel volume is voxelized from
the well-trained kernels. Moreover, we introduce accumulated attenuation
pruning and bounded scaling activation to improve reconstruction quality.
Extensive experiments on real-world patient data demonstrate that 4DRGS
achieves impressive results in 5 minutes training, which is 32x faster than the
state-of-the-art method. This underscores the potential of 4DRGS for real-world
clinics.

Comments:
- Zhentao Liu and Ruyi Zha made equal contributions

---

## CATSplat: Context-Aware Transformer with Spatial Guidance for  Generalizable 3D Gaussian Splatting from A Single-View Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Wonseok Roh, Hwanhee Jung, Jong Wook Kim, Seunggwan Lee, Innfarn Yoo, Andreas Lugmayr, Seunggeun Chi, Karthik Ramani, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2412.12906v1){: .btn .btn-green } |

**Abstract**: Recently, generalizable feed-forward methods based on 3D Gaussian Splatting
have gained significant attention for their potential to reconstruct 3D scenes
using finite resources. These approaches create a 3D radiance field,
parameterized by per-pixel 3D Gaussian primitives, from just a few images in a
single forward pass. However, unlike multi-view methods that benefit from
cross-view correspondences, 3D scene reconstruction with a single-view image
remains an underexplored area. In this work, we introduce CATSplat, a novel
generalizable transformer-based framework designed to break through the
inherent constraints in monocular settings. First, we propose leveraging
textual guidance from a visual-language model to complement insufficient
information from a single image. By incorporating scene-specific contextual
details from text embeddings through cross-attention, we pave the way for
context-aware 3D scene reconstruction beyond relying solely on visual cues.
Moreover, we advocate utilizing spatial guidance from 3D point features toward
comprehensive geometric understanding under single-view settings. With 3D
priors, image features can capture rich structural insights for predicting 3D
Gaussians without multi-view techniques. Extensive experiments on large-scale
datasets demonstrate the state-of-the-art performance of CATSplat in
single-view 3D scene reconstruction with high-quality novel view synthesis.



---

## EOGS: Gaussian Splatting for Earth Observation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Luca Savant Aira, Gabriele Facciolo, Thibaud Ehret | cs.CV | [PDF](http://arxiv.org/pdf/2412.13047v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian splatting has emerged as a strong alternative to NeRF,
demonstrating impressive 3D modeling capabilities while requiring only a
fraction of the training and rendering time. In this paper, we show how the
standard Gaussian splatting framework can be adapted for remote sensing,
retaining its high efficiency. This enables us to achieve state-of-the-art
performance in just a few minutes, compared to the day-long optimization
required by the best-performing NeRF-based Earth observation methods. The
proposed framework incorporates remote-sensing improvements from EO-NeRF, such
as radiometric correction and shadow modeling, while introducing novel
components, including sparsity, view consistency, and opacity regularizations.



---

## HyperGS: Hyperspectral 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Christopher Thirgood, Oscar Mendez, Erin Chao Ling, Jon Storey, Simon Hadfield | cs.CV | [PDF](http://arxiv.org/pdf/2412.12849v1){: .btn .btn-green } |

**Abstract**: We introduce HyperGS, a novel framework for Hyperspectral Novel View
Synthesis (HNVS), based on a new latent 3D Gaussian Splatting (3DGS) technique.
Our approach enables simultaneous spatial and spectral renderings by encoding
material properties from multi-view 3D hyperspectral datasets. HyperGS
reconstructs high-fidelity views from arbitrary perspectives with improved
accuracy and speed, outperforming currently existing methods. To address the
challenges of high-dimensional data, we perform view synthesis in a learned
latent space, incorporating a pixel-wise adaptive density function and a
pruning technique for increased training stability and efficiency.
Additionally, we introduce the first HNVS benchmark, implementing a number of
new baselines based on recent SOTA RGB-NVS techniques, alongside the small
number of prior works on HNVS. We demonstrate HyperGS's robustness through
extensive evaluation of real and simulated hyperspectral scenes with a 14db
accuracy improvement upon previously published models.



---

## 3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Qi Wu, Janick Martinez Esturo, Ashkan Mirzaei, Nicolas Moenne-Loccoz, Zan Gojcic | cs.GR | [PDF](http://arxiv.org/pdf/2412.12507v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown great potential for efficient
reconstruction and high-fidelity real-time rendering of complex scenes on
consumer hardware. However, due to its rasterization-based formulation, 3DGS is
constrained to ideal pinhole cameras and lacks support for secondary lighting
effects. Recent methods address these limitations by tracing volumetric
particles instead, however, this comes at the cost of significantly slower
rendering speeds. In this work, we propose 3D Gaussian Unscented Transform
(3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented
Transform that approximates the particles through sigma points, which can be
projected exactly under any nonlinear projection function. This modification
enables trivial support of distorted cameras with time dependent effects such
as rolling shutter, while retaining the efficiency of rasterization.
Additionally, we align our rendering formulation with that of tracing-based
methods, enabling secondary ray tracing required to represent phenomena such as
reflections and refraction within the same 3D representation.



---

## Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Leo Segre, Shai Avidan | cs.CV | [PDF](http://arxiv.org/pdf/2412.12772v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have advanced photorealistic novel view
synthesis, but their reliance on photometric reconstruction introduces
artifacts, commonly known as "floaters". These artifacts degrade novel view
quality, especially in areas unseen by the training cameras. We present a fast,
post-hoc NeRF cleanup method that eliminates such artifacts by enforcing our
Free Space Prior, effectively minimizing floaters without disrupting the NeRF's
representation of observed regions. Unlike existing approaches that rely on
either Maximum Likelihood (ML) estimation to fit the data or a complex, local
data-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,
selecting the optimal model parameters under a simple global prior assumption
that unseen regions should remain empty. This enables our method to clean
artifacts in both seen and unseen areas, enhancing novel view quality even in
challenging scene regions. Our method is comparable with existing NeRF cleanup
models while being 2.5x faster in inference time, requires no additional memory
beyond the original NeRF, and achieves cleanup training in less than 30
seconds. Our code will be made publically available.



---

## Towards a Training Free Approach for 3D Scene Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Vivek Madhavaram, Shivangana Rawat, Chaitanya Devaguptapu, Charu Sharma, Manohar Kaul | cs.CV | [PDF](http://arxiv.org/pdf/2412.12766v1){: .btn .btn-green } |

**Abstract**: Text driven diffusion models have shown remarkable capabilities in editing
images. However, when editing 3D scenes, existing works mostly rely on training
a NeRF for 3D editing. Recent NeRF editing methods leverages edit operations by
deploying 2D diffusion models and project these edits into 3D space. They
require strong positional priors alongside text prompt to identify the edit
location. These methods are operational on small 3D scenes and are more
generalized to particular scene. They require training for each specific edit
and cannot be exploited in real-time edits. To address these limitations, we
propose a novel method, FreeEdit, to make edits in training free manner using
mesh representations as a substitute for NeRF. Training-free methods are now a
possibility because of the advances in foundation model's space. We leverage
these models to bring a training-free alternative and introduce solutions for
insertion, replacement and deletion. We consider insertion, replacement and
deletion as basic blocks for performing intricate edits with certain
combinations of these operations. Given a text prompt and a 3D scene, our model
is capable of identifying what object should be inserted/replaced or deleted
and location where edit should be performed. We also introduce a novel
algorithm as part of FreeEdit to find the optimal location on grounding object
for placement. We evaluate our model by comparing it with baseline models on a
wide range of scenes using quantitative and qualitative metrics and showcase
the merits of our method with respect to others.



---

## Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-17 | Sebastian Weiss, Derek Bradley | cs.CV | [PDF](http://arxiv.org/pdf/2412.12734v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has recently emerged as the go-to representation for
reconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian
primitives has further improved multi-view consistency and surface
reconstruction accuracy. In this work we highlight the similarity between 2D
Gaussian Splatting (2DGS) and billboards from traditional computer graphics.
Both use flat semi-transparent 2D geometry that is positioned, oriented and
scaled in 3D space. However 2DGS uses a solid color per splat and an opacity
modulated by a Gaussian distribution, where billboards are more expressive,
modulating the color with a uv-parameterized texture. We propose to unify these
concepts by presenting Gaussian Billboards, a modification of 2DGS to add
spatially-varying color achieved using per-splat texture interpolation. The
result is a mixture of the two representations, which benefits from both the
robust scene optimization power of 2DGS and the expressiveness of texture
mapping. We show that our method can improve the sharpness and quality of the
scene representation in a wide range of qualitative and quantitative
evaluations compared to the original 2DGS implementation.



---

## SweepEvGS: Event-Based 3D Gaussian Splatting for Macro and Micro  Radiance Field Rendering from a Single Sweep

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-16 | Jingqian Wu, Shuo Zhu, Chutian Wang, Boxin Shi, Edmund Y. Lam | cs.CV | [PDF](http://arxiv.org/pdf/2412.11579v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3D-GS) have demonstrated the
potential of using 3D Gaussian primitives for high-speed, high-fidelity, and
cost-efficient novel view synthesis from continuously calibrated input views.
However, conventional methods require high-frame-rate dense and high-quality
sharp images, which are time-consuming and inefficient to capture, especially
in dynamic environments. Event cameras, with their high temporal resolution and
ability to capture asynchronous brightness changes, offer a promising
alternative for more reliable scene reconstruction without motion blur. In this
paper, we propose SweepEvGS, a novel hardware-integrated method that leverages
event cameras for robust and accurate novel view synthesis across various
imaging settings from a single sweep. SweepEvGS utilizes the initial static
frame with dense event streams captured during a single camera sweep to
effectively reconstruct detailed scene views. We also introduce different
real-world hardware imaging systems for real-world data collection and
evaluation for future research. We validate the robustness and efficiency of
SweepEvGS through experiments in three different imaging settings: synthetic
objects, real-world macro-level, and real-world micro-level view synthesis. Our
results demonstrate that SweepEvGS surpasses existing methods in visual
rendering quality, rendering speed, and computational efficiency, highlighting
its potential for dynamic practical applications.



---

## PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-16 | Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2412.12096v1){: .btn .btn-green } |

**Abstract**: With the advent of portable 360{\deg} cameras, panorama has gained
significant attention in applications like virtual reality (VR), virtual tours,
robotics, and autonomous driving. As a result, wide-baseline panorama view
synthesis has emerged as a vital task, where high resolution, fast inference,
and memory efficiency are essential. Nevertheless, existing methods are
typically constrained to lower resolutions (512 $\times$ 1024) due to demanding
memory and computational requirements. In this paper, we present PanSplat, a
generalizable, feed-forward approach that efficiently supports resolution up to
4K (2048 $\times$ 4096). Our approach features a tailored spherical 3D Gaussian
pyramid with a Fibonacci lattice arrangement, enhancing image quality while
reducing information redundancy. To accommodate the demands of high resolution,
we propose a pipeline that integrates a hierarchical spherical cost volume and
Gaussian heads with local operations, enabling two-step deferred
backpropagation for memory-efficient training on a single A100 GPU. Experiments
demonstrate that PanSplat achieves state-of-the-art results with superior
efficiency and image quality across both synthetic and real-world datasets.
Code will be available at \url{https://github.com/chengzhag/PanSplat}.

Comments:
- Project Page: https://chengzhag.github.io/publication/pansplat/ Code:
  https://github.com/chengzhag/PanSplat

---

## Wonderland: Navigating 3D Scenes from a Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-16 | Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren | cs.CV | [PDF](http://arxiv.org/pdf/2412.12091v1){: .btn .btn-green } |

**Abstract**: This paper addresses a challenging question: How can we efficiently create
high-quality, wide-scope 3D scenes from a single arbitrary image? Existing
methods face several constraints, such as requiring multi-view data,
time-consuming per-scene optimization, low visual quality in backgrounds, and
distorted reconstructions in unseen areas. We propose a novel pipeline to
overcome these limitations. Specifically, we introduce a large-scale
reconstruction model that uses latents from a video diffusion model to predict
3D Gaussian Splattings for the scenes in a feed-forward manner. The video
diffusion model is designed to create videos precisely following specified
camera trajectories, allowing it to generate compressed video latents that
contain multi-view information while maintaining 3D consistency. We train the
3D reconstruction model to operate on the video latent space with a progressive
training strategy, enabling the efficient generation of high-quality,
wide-scope, and generic 3D scenes. Extensive evaluations across various
datasets demonstrate that our model significantly outperforms existing methods
for single-view 3D scene generation, particularly with out-of-domain images.
For the first time, we demonstrate that a 3D reconstruction model can be
effectively built upon the latent space of a diffusion model to realize
efficient 3D scene generation.

Comments:
- Project page: https://snap-research.github.io/wonderland/

---

## Deformable Radial Kernel Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-16 | Yi-Hua Huang, Ming-Xian Lin, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi | cs.CV | [PDF](http://arxiv.org/pdf/2412.11752v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian splatting has emerged as a robust technique for
representing 3D scenes, enabling real-time rasterization and high-fidelity
rendering. However, Gaussians' inherent radial symmetry and smoothness
constraints limit their ability to represent complex shapes, often requiring
thousands of primitives to approximate detailed geometry. We introduce
Deformable Radial Kernel (DRK), which extends Gaussian splatting into a more
general and flexible framework. Through learnable radial bases with adjustable
angles and scales, DRK efficiently models diverse shape primitives while
enabling precise control over edge sharpness and boundary curvature. iven DRK's
planar nature, we further develop accurate ray-primitive intersection
computation for depth sorting and introduce efficient kernel culling strategies
for improved rasterization efficiency. Extensive experiments demonstrate that
DRK outperforms existing methods in both representation efficiency and
rendering quality, achieving state-of-the-art performance while dramatically
reducing primitive count.



---

## GS-ProCams: Gaussian Splatting-based Projector-Camera Systems

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-16 | Qingyue Deng, Jijiang Li, Haibin Ling, Bingyao Huang | cs.CV | [PDF](http://arxiv.org/pdf/2412.11762v1){: .btn .btn-green } |

**Abstract**: We present GS-ProCams, the first Gaussian Splatting-based framework for
projector-camera systems (ProCams). GS-ProCams significantly enhances the
efficiency of projection mapping (PM) that requires establishing geometric and
radiometric mappings between the projector and the camera. Previous CNN-based
ProCams are constrained to a specific viewpoint, limiting their applicability
to novel perspectives. In contrast, NeRF-based ProCams support view-agnostic
projection mapping, however, they require an additional colocated light source
and demand significant computational and memory resources. To address this
issue, we propose GS-ProCams that employs 2D Gaussian for scene
representations, and enables efficient view-agnostic ProCams applications. In
particular, we explicitly model the complex geometric and photometric mappings
of ProCams using projector responses, the target surface's geometry and
materials represented by Gaussians, and global illumination component. Then, we
employ differentiable physically-based rendering to jointly estimate them from
captured multi-view projections. Compared to state-of-the-art NeRF-based
methods, our GS-ProCams eliminates the need for additional devices, achieving
superior ProCams simulation quality. It is also 600 times faster and uses only
1/10 of the GPU memory.



---

## Sequence Matters: Harnessing Video Models in 3D Super-Resolution

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-16 | Hyun-kyu Ko, Dongheok Park, Youngin Park, Byeonghyeon Lee, Juhee Han, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2412.11525v2){: .btn .btn-green } |

**Abstract**: 3D super-resolution aims to reconstruct high-fidelity 3D models from
low-resolution (LR) multi-view images. Early studies primarily focused on
single-image super-resolution (SISR) models to upsample LR images into
high-resolution images. However, these methods often lack view consistency
because they operate independently on each image. Although various
post-processing techniques have been extensively explored to mitigate these
inconsistencies, they have yet to fully resolve the issues. In this paper, we
perform a comprehensive study of 3D super-resolution by leveraging video
super-resolution (VSR) models. By utilizing VSR models, we ensure a higher
degree of spatial consistency and can reference surrounding spatial
information, leading to more accurate and detailed reconstructions. Our
findings reveal that VSR models can perform remarkably well even on sequences
that lack precise spatial alignment. Given this observation, we propose a
simple yet practical approach to align LR images without involving fine-tuning
or generating 'smooth' trajectory from the trained 3D models over LR images.
The experimental results show that the surprisingly simple algorithms can
achieve the state-of-the-art results of 3D super-resolution tasks on standard
benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.
Project page: https://ko-lani.github.io/Sequence-Matters

Comments:
- Project page: https://ko-lani.github.io/Sequence-Matters

---

## VRVVC: Variable-Rate NeRF-Based Volumetric Video Compression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-16 | Qiang Hu, Houqiang Zhong, Zihan Zheng, Xiaoyun Zhang, Zhengxue Cheng, Li Song, Guangtao Zhai, Yanfeng Wang | eess.IV | [PDF](http://arxiv.org/pdf/2412.11362v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF)-based volumetric video has revolutionized visual
media by delivering photorealistic Free-Viewpoint Video (FVV) experiences that
provide audiences with unprecedented immersion and interactivity. However, the
substantial data volumes pose significant challenges for storage and
transmission. Existing solutions typically optimize NeRF representation and
compression independently or focus on a single fixed rate-distortion (RD)
tradeoff. In this paper, we propose VRVVC, a novel end-to-end joint
optimization variable-rate framework for volumetric video compression that
achieves variable bitrates using a single model while maintaining superior RD
performance. Specifically, VRVVC introduces a compact tri-plane implicit
residual representation for inter-frame modeling of long-duration dynamic
scenes, effectively reducing temporal redundancy. We further propose a
variable-rate residual representation compression scheme that leverages a
learnable quantization and a tiny MLP-based entropy model. This approach
enables variable bitrates through the utilization of predefined Lagrange
multipliers to manage the quantization error of all latent representations.
Finally, we present an end-to-end progressive training strategy combined with a
multi-rate-distortion loss function to optimize the entire framework. Extensive
experiments demonstrate that VRVVC achieves a wide range of variable bitrates
within a single model and surpasses the RD performance of existing methods
across various datasets.



---

## EditSplat: Multi-View Fusion and Attention-Guided Optimization for  View-Consistent 3D Scene Editing with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-16 | Dong In Lee, Hyeongcheol Park, Jiyoung Seo, Eunbyung Park, Hyunje Park, Ha Dam Baek, Shin Sangheon, Sangmin kim, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2412.11520v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D editing have highlighted the potential of
text-driven methods in real-time, user-friendly AR/VR applications. However,
current methods rely on 2D diffusion models without adequately considering
multi-view information, resulting in multi-view inconsistency. While 3D
Gaussian Splatting (3DGS) significantly improves rendering quality and speed,
its 3D editing process encounters difficulties with inefficient optimization,
as pre-trained Gaussians retain excessive source information, hindering
optimization. To address these limitations, we propose \textbf{EditSplat}, a
novel 3D editing framework that integrates Multi-view Fusion Guidance (MFG) and
Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by
incorporating essential multi-view information into the diffusion process,
leveraging classifier-free guidance from the text-to-image diffusion model and
the geometric properties of 3DGS. Additionally, our AGT leverages the explicit
representation of 3DGS to selectively prune and optimize 3D Gaussians,
enhancing optimization efficiency and enabling precise, semantically rich local
edits. Through extensive qualitative and quantitative evaluations, EditSplat
achieves superior multi-view consistency and editing quality over existing
methods, significantly enhancing overall efficiency.



---

## DCSEG: Decoupled 3D Open-Set Segmentation using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-14 | Luis Wiedmann, Luca Wiehe, David Rozenberszki | cs.CV | [PDF](http://arxiv.org/pdf/2412.10972v1){: .btn .btn-green } |

**Abstract**: Open-set 3D segmentation represents a major point of interest for multiple
downstream robotics and augmented/virtual reality applications. Recent advances
introduce 3D Gaussian Splatting as a computationally efficient representation
of the underlying scene. They enable the rendering of novel views while
achieving real-time display rates and matching the quality of computationally
far more expensive methods. We present a decoupled 3D segmentation pipeline to
ensure modularity and adaptability to novel 3D representations and semantic
segmentation foundation models. The pipeline proposes class-agnostic masks
based on a 3D reconstruction of the scene. Given the resulting class-agnostic
masks, we use a class-aware 2D foundation model to add class annotations to the
3D masks. We test this pipeline with 3D Gaussian Splatting and different 2D
segmentation models and achieve better performance than more tailored
approaches while also significantly increasing the modularity.



---

## Sharpening Your Density Fields: Spiking Neuron Aided Fast Geometry  Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Yi Gu, Zhaorui Wang, Dongjun Ye, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2412.09881v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have achieved remarkable progress in neural
rendering. Extracting geometry from NeRF typically relies on the Marching Cubes
algorithm, which uses a hand-crafted threshold to define the level set.
However, this threshold-based approach requires laborious and scenario-specific
tuning, limiting its practicality for real-world applications. In this work, we
seek to enhance the efficiency of this method during the training time. To this
end, we introduce a spiking neuron mechanism that dynamically adjusts the
threshold, eliminating the need for manual selection. Despite its promise,
directly training with the spiking neuron often results in model collapse and
noisy outputs. To overcome these challenges, we propose a round-robin strategy
that stabilizes the training process and enables the geometry network to
achieve a sharper and more precise density distribution with minimal
computational overhead. We validate our approach through extensive experiments
on both synthetic and real-world datasets. The results show that our method
significantly improves the performance of threshold-based techniques, offering
a more robust and efficient solution for NeRF geometry extraction.



---

## SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D  Gaussians from Monocular Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim | cs.CV | [PDF](http://arxiv.org/pdf/2412.09982v2){: .btn .btn-green } |

**Abstract**: Synthesizing novel views from in-the-wild monocular videos is challenging due
to scene dynamics and the lack of multi-view cues. To address this, we propose
SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for
high-quality reconstruction and fast rendering from monocular videos. At its
core is a novel Motion-Adaptive Spline (MAS) method, which represents
continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a
small number of control points. For MAS, we introduce a Motion-Adaptive Control
points Pruning (MACP) method to model the deformation of each dynamic 3D
Gaussian across varying motions, progressively pruning control points while
maintaining dynamic modeling integrity. Additionally, we present a joint
optimization strategy for camera parameter estimation and 3D Gaussian
attributes, leveraging photometric and geometric consistency. This eliminates
the need for Structure-from-Motion preprocessing and enhances SplineGS's
robustness in real-world conditions. Experiments show that SplineGS
significantly outperforms state-of-the-art methods in novel view synthesis
quality for dynamic scenes from monocular videos, achieving thousands times
faster rendering speed.

Comments:
- The first two authors contributed equally to this work (equal
  contribution). The last two authors advised equally to this work. Please
  visit our project page at this https://kaist-viclab.github.io/splinegs-site/

---

## SuperGSeg: Open-Vocabulary 3D Segmentation with Structured  Super-Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2412.10231v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently gained traction for its efficient training
and real-time rendering. While the vanilla Gaussian Splatting representation is
mainly designed for view synthesis, more recent works investigated how to
extend it with scene understanding and language features. However, existing
methods lack a detailed comprehension of scenes, limiting their ability to
segment and interpret complex structures. To this end, We introduce SuperGSeg,
a novel approach that fosters cohesive, context-aware scene representation by
disentangling segmentation and language field distillation. SuperGSeg first
employs neural Gaussians to learn instance and hierarchical segmentation
features from multi-view images with the aid of off-the-shelf 2D masks. These
features are then leveraged to create a sparse set of what we call
Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language
features into 3D space. Through Super-Gaussians, our method enables
high-dimensional language feature rendering without extreme increases in GPU
memory. Extensive experiments demonstrate that SuperGSeg outperforms prior
works on both open-vocabulary object localization and semantic segmentation
tasks.

Comments:
- 13 pages, 8 figures

---

## NeRF-Texture: Synthesizing Neural Radiance Field Textures

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Yi-Hua Huang, Yan-Pei Cao, Yu-Kun Lai, Ying Shan, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2412.10004v1){: .btn .btn-green } |

**Abstract**: Texture synthesis is a fundamental problem in computer graphics that would
benefit various applications. Existing methods are effective in handling 2D
image textures. In contrast, many real-world textures contain meso-structure in
the 3D geometry space, such as grass, leaves, and fabrics, which cannot be
effectively modeled using only 2D image textures. We propose a novel texture
synthesis method with Neural Radiance Fields (NeRF) to capture and synthesize
textures from given multi-view images. In the proposed NeRF texture
representation, a scene with fine geometric details is disentangled into the
meso-structure textures and the underlying base shape. This allows textures
with meso-structure to be effectively learned as latent features situated on
the base shape, which are fed into a NeRF decoder trained simultaneously to
represent the rich view-dependent appearance. Using this implicit
representation, we can synthesize NeRF-based textures through patch matching of
latent features. However, inconsistencies between the metrics of the
reconstructed content space and the latent feature space may compromise the
synthesis quality. To enhance matching performance, we further regularize the
distribution of latent features by incorporating a clustering constraint. In
addition to generating NeRF textures over a planar domain, our method can also
synthesize NeRF textures over curved surfaces, which are practically useful.
Experimental results and evaluations demonstrate the effectiveness of our
approach.



---

## GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Jiapeng Tang, Davide Davoli, Tobias Kirschstein, Liam Schoneveld, Matthias Niessner | cs.CV | [PDF](http://arxiv.org/pdf/2412.10209v1){: .btn .btn-green } |

**Abstract**: We propose a novel approach for reconstructing animatable 3D Gaussian avatars
from monocular videos captured by commodity devices like smartphones.
Photorealistic 3D head avatar reconstruction from such recordings is
challenging due to limited observations, which leaves unobserved regions
under-constrained and can lead to artifacts in novel views. To address this
problem, we introduce a multi-view head diffusion model, leveraging its priors
to fill in missing regions and ensure view consistency in Gaussian splatting
renderings. To enable precise viewpoint control, we use normal maps rendered
from FLAME-based head reconstruction, which provides pixel-aligned inductive
biases. We also condition the diffusion model on VAE features extracted from
the input image to preserve details of facial identity and appearance. For
Gaussian avatar reconstruction, we distill multi-view diffusion priors by using
iteratively denoised images as pseudo-ground truths, effectively mitigating
over-saturation issues. To further improve photorealism, we apply latent
upsampling to refine the denoised latent before decoding it into an image. We
evaluate our method on the NeRSemble dataset, showing that GAF outperforms the
previous state-of-the-art methods in novel view synthesis by a 5.34\% higher
SSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions
from monocular videos captured on commodity devices.

Comments:
- Paper Video: https://youtu.be/QuIYTljvhyg Project Page:
  https://tangjiapeng.github.io/projects/GAF

---

## RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Lizhi Bai, Chunqi Tian, Jun Yang, Siyu Zhang, Masanori Suganuma, Takayuki Okatani | cs.RO | [PDF](http://arxiv.org/pdf/2412.09868v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as a promising technique for high-quality
3D rendering, leading to increasing interest in integrating 3DGS into realism
SLAM systems. However, existing methods face challenges such as Gaussian
primitives redundancy, forgetting problem during continuous optimization, and
difficulty in initializing primitives in monocular case due to lack of depth
information. In order to achieve efficient and photorealistic mapping, we
propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular
and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian
primitives optimization and consists of three key components. Firstly, we
propose an efficient incremental mapping approach to achieve a compact and
accurate representation of the scene through adaptive sampling and Gaussian
primitives filtering. Secondly, a dynamic window optimization method is
proposed to mitigate the forgetting problem and improve map consistency.
Finally, for the monocular case, a monocular keyframe initialization method
based on sparse point cloud is proposed to improve the initialization accuracy
of Gaussian primitives, which provides a geometric basis for subsequent
optimization. The results of numerous experiments demonstrate that RP-SLAM
achieves state-of-the-art map rendering accuracy while ensuring real-time
performance and model compactness.



---

## TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting  from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-13 | Liang Zhao, Zehan Bao, Yi Xie, Hong Chen, Yaohui Chen, Weifu Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.10051v1){: .btn .btn-green } |

**Abstract**: Recent advances in Gaussian Splatting have significantly advanced the field,
achieving both panoptic and interactive segmentation of 3D scenes. However,
existing methodologies often overlook the critical need for reconstructing
specified targets with complex structures from sparse views. To address this
issue, we introduce TSGaussian, a novel framework that combines semantic
constraints with depth priors to avoid geometry degradation in challenging
novel view synthesis tasks. Our approach prioritizes computational resources on
designated targets while minimizing background allocation. Bounding boxes from
YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask
predictions, ensuring semantic accuracy and cost efficiency. TSGaussian
effectively clusters 3D gaussians by introducing a compact identity encoding
for each Gaussian ellipsoid and incorporating 3D spatial consistency
regularization. Leveraging these modules, we propose a pruning strategy to
effectively reduce redundancy in 3D gaussians. Extensive experiments
demonstrate that TSGaussian outperforms state-of-the-art methods on three
standard datasets and a new challenging dataset we collected, achieving
superior results in novel view synthesis of specific objects. Code is available
at: https://github.com/leon2000-ai/TSGaussian.



---

## FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Jiale Xu, Shenghua Gao, Ying Shan | cs.CV | [PDF](http://arxiv.org/pdf/2412.09573v1){: .btn .btn-green } |

**Abstract**: Existing sparse-view reconstruction models heavily rely on accurate known
camera poses. However, deriving camera extrinsics and intrinsics from
sparse-view images presents significant challenges. In this work, we present
FreeSplatter, a highly scalable, feed-forward reconstruction framework capable
of generating high-quality 3D Gaussians from uncalibrated sparse-view images
and recovering their camera parameters in mere seconds. FreeSplatter is built
upon a streamlined transformer architecture, comprising sequential
self-attention blocks that facilitate information exchange among multi-view
image tokens and decode them into pixel-wise 3D Gaussian primitives. The
predicted Gaussian primitives are situated in a unified reference frame,
allowing for high-fidelity 3D modeling and instant camera parameter estimation
using off-the-shelf solvers. To cater to both object-centric and scene-level
reconstruction, we train two model variants of FreeSplatter on extensive
datasets. In both scenarios, FreeSplatter outperforms state-of-the-art
baselines in terms of reconstruction quality and pose estimation accuracy.
Furthermore, we showcase FreeSplatter's potential in enhancing the productivity
of downstream applications, such as text/image-to-3D content creation.

Comments:
- Project page: https://bluestyle97.github.io/projects/freesplatter/

---

## PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Sean Wu, Shamik Basu, Tim Broedermann, Luc Van Gool, Christos Sakaridis | cs.CV | [PDF](http://arxiv.org/pdf/2412.09680v1){: .btn .btn-green } |

**Abstract**: We tackle the ill-posed inverse rendering problem in 3D reconstruction with a
Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR)
theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and
3D Gaussian Splatting approaches: they estimate view-dependent appearance
without modeling scene materials and illumination. To address this limitation,
we present an inverse rendering (IR) model capable of jointly estimating scene
geometry, materials, and illumination. Our model builds upon recent NeRF-based
IR approaches, but crucially introduces two novel physics-based priors that
better constrain the IR estimation. Our priors are rigorously formulated as
intuitive loss terms and achieve state-of-the-art material estimation without
compromising novel view synthesis quality. Our method is easily adaptable to
other inverse rendering and 3D reconstruction frameworks that require material
estimation. We demonstrate the importance of extending current neural rendering
approaches to fully model scene properties beyond geometry and view-dependent
appearance. Code is publicly available at https://github.com/s3anwu/pbrnerf

Comments:
- 16 pages, 7 figures. Code is publicly available at
  https://github.com/s3anwu/pbrnerf

---

## GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Dongyue Lu, Lingdong Kong, Tianxin Huang, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2412.09511v1){: .btn .btn-green } |

**Abstract**: Identifying affordance regions on 3D objects from semantic cues is essential
for robotics and human-machine interaction. However, existing 3D affordance
learning methods struggle with generalization and robustness due to limited
annotated data and a reliance on 3D backbones focused on geometric encoding,
which often lack resilience to real-world noise and data corruption. We propose
GEAL, a novel framework designed to enhance the generalization and robustness
of 3D affordance learning by leveraging large-scale pre-trained 2D models. We
employ a dual-branch architecture with Gaussian splatting to establish
consistent mappings between 3D point clouds and 2D representations, enabling
realistic 2D renderings from sparse point clouds. A granularity-adaptive fusion
module and a 2D-3D consistency alignment module further strengthen cross-modal
alignment and knowledge transfer, allowing the 3D branch to benefit from the
rich semantics and generalization capacity of 2D models. To holistically assess
the robustness, we introduce two new corruption-based benchmarks: PIAD-C and
LASO-C. Extensive experiments on public datasets and our benchmarks show that
GEAL consistently outperforms existing methods across seen and novel object
categories, as well as corrupted data, demonstrating robust and adaptable
affordance prediction under diverse conditions. Code and corruption datasets
have been made publicly available.

Comments:
- 22 pages, 8 figures, 12 tables; Project Page at
  https://dylanorange.github.io/projects/geal

---

## Feat2GS: Probing Visual Foundation Models with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, Yuliang Xiu | cs.CV | [PDF](http://arxiv.org/pdf/2412.09606v1){: .btn .btn-green } |

**Abstract**: Given that visual foundation models (VFMs) are trained on extensive datasets
but often limited to 2D images, a natural question arises: how well do they
understand the 3D world? With the differences in architecture and training
protocols (i.e., objectives, proxy tasks), a unified framework to fairly and
comprehensively probe their 3D awareness is urgently needed. Existing works on
3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or
two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately,
these tasks ignore texture awareness, and require 3D data as ground-truth,
which limits the scale and diversity of their evaluation set. To address these
issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM
features extracted from unposed images. This allows us to probe 3D awareness
for geometry and texture via novel view synthesis, without requiring 3D data.
Additionally, the disentanglement of 3DGS parameters - geometry
($\boldsymbol{x}, \alpha, \Sigma$) and texture ($\boldsymbol{c}$) - enables
separate analysis of texture and geometry awareness. Under Feat2GS, we conduct
extensive experiments to probe the 3D awareness of several VFMs, and
investigate the ingredients that lead to a 3D aware VFM. Building on these
findings, we develop several variants that achieve state-of-the-art across
diverse datasets. This makes Feat2GS useful for probing VFMs, and as a
simple-yet-effective baseline for novel-view synthesis. Code and data will be
made available at https://fanegg.github.io/Feat2GS/.

Comments:
- Project Page: https://fanegg.github.io/Feat2GS/

---

## LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Haotian Mao, Zhuoxiong Xu, Siyue Wei, Yule Quan, Nianchen Deng, Xubo Yang | cs.HC | [PDF](http://arxiv.org/pdf/2412.09176v1){: .btn .btn-green } |

**Abstract**: Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has
shown immense potential in VR content creation due to its high-quality
rendering and efficient production process. However, existing physics-based
interaction systems for 3DGS can only perform simple and non-realistic
simulations or demand extensive user input for complex scenes, primarily due to
the absence of scene understanding. In this paper, we propose LIVE-GS, a highly
realistic interactive VR system powered by LLM. After object-aware GS
reconstruction, we prompt GPT-4o to analyze the physical properties of objects
in the scene, which are used to guide physical simulations consistent with real
phenomena. We also design a GPT-assisted GS inpainting module to fill the
unseen area covered by manipulative objects. To perform a precise segmentation
of Gaussian kernels, we propose a feature-mask segmentation strategy. To enable
rich interaction, we further propose a computationally efficient physical
simulation framework through an PBD-based unified interpolation method,
supporting various physical forms such as rigid body, soft body, and granular
materials. Our experimental results show that with the help of LLM's
understanding and enhancement of scenes, our VR system can support complex and
realistic interactions without additional manual design and annotation.



---

## MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative  Ego-Motion and Photorealistic 3D Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Xiaohao Xu, Feng Xue, Shibo Zhao, Yike Pan, Sebastian Scherer, Xiaonan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2412.09723v1){: .btn .btn-green } |

**Abstract**: Real-time multi-agent collaboration for ego-motion estimation and
high-fidelity 3D reconstruction is vital for scalable spatial intelligence.
However, traditional methods produce sparse, low-detail maps, while recent
dense mapping approaches struggle with high latency. To overcome these
challenges, we present MAC-Ego3D, a novel framework for real-time collaborative
photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D
enables agents to independently construct, align, and iteratively refine local
maps using a unified Gaussian splat representation. Through Intra-Agent
Gaussian Consensus, it enforces spatial coherence among neighboring Gaussian
splats within an agent. For global alignment, parallelized Inter-Agent Gaussian
Consensus, which asynchronously aligns and optimizes local maps by regularizing
multi-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D
model. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D
rendering, enabling rapid inter-agent Gaussian association and alignment.
MAC-Ego3D bridges local precision and global coherence, delivering higher
efficiency, largely reducing localization error, and improving mapping
fidelity. It establishes a new SOTA on synthetic and real-world benchmarks,
achieving a 15x increase in inference speed, order-of-magnitude reductions in
ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10
dB. Our code will be made publicly available at
https://github.com/Xiaohao-Xu/MAC-Ego3D .

Comments:
- 27 pages, 25 figures

---

## LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video  Generation Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-12 | Yabo Chen, Chen Yang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Wei Shen, Wenrui Dai, Hongkai Xiong, Qi Tian | cs.CV | [PDF](http://arxiv.org/pdf/2412.09597v1){: .btn .btn-green } |

**Abstract**: Single-image 3D reconstruction remains a fundamental challenge in computer
vision due to inherent geometric ambiguities and limited viewpoint information.
Recent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D
priors learned from large-scale video data. However, leveraging these priors
effectively faces three key challenges: (1) degradation in quality across large
camera motions, (2) difficulties in achieving precise camera control, and (3)
geometric distortions inherent to the diffusion process that damage 3D
consistency. We address these challenges by proposing LiftImage3D, a framework
that effectively releases LVDMs' generative priors while ensuring 3D
consistency. Specifically, we design an articulated trajectory strategy to
generate video frames, which decomposes video sequences with large camera
motions into ones with controllable small motions. Then we use robust neural
matching models, i.e. MASt3R, to calibrate the camera poses of generated frames
and produce corresponding point clouds. Finally, we propose a distortion-aware
3D Gaussian splatting representation, which can learn independent distortions
between frames and output undistorted canonical Gaussians. Extensive
experiments demonstrate that LiftImage3D achieves state-of-the-art performance
on two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and
generalizes well to diverse in-the-wild images, from cartoon illustrations to
complex real-world scenes.

Comments:
- Project page: https://liftimage3d.github.io/

---

## SLGaussian: Fast Language Gaussian Splatting in Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Kangjie Chen, BingQuan Dai, Minghan Qin, Dongbin Zhang, Peihao Li, Yingshuang Zou, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.08331v1){: .btn .btn-green } |

**Abstract**: 3D semantic field learning is crucial for applications like autonomous
navigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from
limited viewpoints is essential. Existing methods struggle under sparse view
conditions, relying on inefficient per-scene multi-view optimizations, which
are impractical for many real-world tasks. To address this, we propose
SLGaussian, a feed-forward method for constructing 3D semantic fields from
sparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring
consistent SAM segmentations through video tracking and using low-dimensional
indexing for high-dimensional CLIP features, SLGaussian efficiently embeds
language information in 3D space, offering a robust solution for accurate 3D
scene understanding under sparse view conditions. In experiments on two-view
sparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,
SLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,
and mIoU. Moreover, our model achieves scene inference in under 30 seconds and
open-vocabulary querying in just 0.011 seconds per query.



---

## GN-FR:Generalizable Neural Radiance Fields for Flare Removal

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Gopi Raju Matta, Rahul Siddartha, Rongali Simhachala Venkata Girish, Sumit Sharma, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2412.08200v1){: .btn .btn-green } |

**Abstract**: Flare, an optical phenomenon resulting from unwanted scattering and
reflections within a lens system, presents a significant challenge in imaging.
The diverse patterns of flares, such as halos, streaks, color bleeding, and
haze, complicate the flare removal process. Existing traditional and
learning-based methods have exhibited limited efficacy due to their reliance on
single-image approaches, where flare removal is highly ill-posed. We address
this by framing flare removal as a multi-view image problem, taking advantage
of the view-dependent nature of flare artifacts. This approach leverages
information from neighboring views to recover details obscured by flare in
individual images. Our proposed framework, GN-FR (Generalizable Neural Radiance
Fields for Flare Removal), can render flare-free views from a sparse set of
input images affected by lens flare and generalizes across different scenes in
an unsupervised manner. GN-FR incorporates several modules within the
Generalizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation
(FMG), View Sampler (VS), and Point Sampler (PS). To overcome the
impracticality of capturing both flare-corrupted and flare-free data, we
introduce a masking loss function that utilizes mask information in an
unsupervised setting. Additionally, we present a 3D multi-view flare dataset,
comprising 17 real flare scenes with 782 images, 80 real flare patterns, and
their corresponding annotated flare-occupancy masks. To our knowledge, this is
the first work to address flare removal within a Neural Radiance Fields (NeRF)
framework.



---

## ProGDF: Progressive Gaussian Differential Field for Controllable and  Flexible 3D Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Yian Zhao, Wanshi Xu, Yang Wu, Weiheng Huang, Zhongqian Sun, Wei Yang | cs.GR | [PDF](http://arxiv.org/pdf/2412.08152v1){: .btn .btn-green } |

**Abstract**: 3D editing plays a crucial role in editing and reusing existing 3D assets,
thereby enhancing productivity. Recently, 3DGS-based methods have gained
increasing attention due to their efficient rendering and flexibility. However,
achieving desired 3D editing results often requires multiple adjustments in an
iterative loop, resulting in tens of minutes of training time cost for each
attempt and a cumbersome trial-and-error cycle for users. This in-the-loop
training paradigm results in a poor user experience. To address this issue, we
introduce the concept of process-oriented modelling for 3D editing and propose
the Progressive Gaussian Differential Field (ProGDF), an out-of-loop training
approach that requires only a single training session to provide users with
controllable editing capability and variable editing results through a
user-friendly interface in real-time. ProGDF consists of two key components:
Progressive Gaussian Splatting (PGS) and Gaussian Differential Field (GDF). PGS
introduces the progressive constraint to extract the diverse intermediate
results of the editing process and employs rendering quality regularization to
improve the quality of these results. Based on these intermediate results, GDF
leverages a lightweight neural network to model the editing process. Extensive
results on two novel applications, namely controllable 3D editing and flexible
fine-grained 3D manipulation, demonstrate the effectiveness, practicality and
flexibility of the proposed ProGDF.



---

## NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF  and Neural View Synthesis Methods

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Qiang Qu, Hanxue Liang, Xiaoming Chen, Yuk Ying Chung, Yiran Shen | cs.CV | [PDF](http://arxiv.org/pdf/2412.08029v1){: .btn .btn-green } |

**Abstract**: Neural View Synthesis (NVS) has demonstrated efficacy in generating
high-fidelity dense viewpoint videos using a image set with sparse views.
However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not
tailored for the scenes with dense viewpoints synthesized by NVS and NeRF
variants, thus, they often fall short in capturing the perceptual quality,
including spatial and angular aspects of NVS-synthesized scenes. Furthermore,
the lack of dense ground truth views makes the full reference quality
assessment on NVS-synthesized scenes challenging. For instance, datasets such
as LLFF provide only sparse images, insufficient for complete full-reference
assessments. To address the issues above, we propose NeRF-NQA, the first
no-reference quality assessment method for densely-observed scenes synthesized
from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment
strategy, integrating both viewwise and pointwise approaches, to evaluate the
quality of NVS-generated scenes. The viewwise approach assesses the spatial
quality of each individual synthesized view and the overall inter-views
consistency, while the pointwise approach focuses on the angular qualities of
scene surface points and their compound inter-point quality. Extensive
evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality
assessment methods (from fields of image, video, and light-field assessment).
The results demonstrate NeRF-NQA outperforms the existing assessment methods
significantly and it shows substantial superiority on assessing NVS-synthesized
scenes without references. An implementation of this paper are available at
https://github.com/VincentQQu/NeRF-NQA.



---

## DSplats: 3D Generation by Denoising Splats-Based Multiview Diffusion  Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-11 | Kevin Miao, Harsh Agrawal, Qihang Zhang, Federico Semeraro, Marco Cavallo, Jiatao Gu, Alexander Toshev | eess.IV | [PDF](http://arxiv.org/pdf/2412.09648v1){: .btn .btn-green } |

**Abstract**: Generating high-quality 3D content requires models capable of learning robust
distributions of complex scenes and the real-world objects within them. Recent
Gaussian-based 3D reconstruction techniques have achieved impressive results in
recovering high-fidelity 3D assets from sparse input images by predicting 3D
Gaussians in a feed-forward manner. However, these techniques often lack the
extensive priors and expressiveness offered by Diffusion Models. On the other
hand, 2D Diffusion Models, which have been successfully applied to denoise
multiview images, show potential for generating a wide range of photorealistic
3D outputs but still fall short on explicit 3D priors and consistency. In this
work, we aim to bridge these two approaches by introducing DSplats, a novel
method that directly denoises multiview images using Gaussian Splat-based
Reconstructors to produce a diverse array of realistic 3D assets. To harness
the extensive priors of 2D Diffusion Models, we incorporate a pretrained Latent
Diffusion Model into the reconstructor backbone to predict a set of 3D
Gaussians. Additionally, the explicit 3D representation embedded in the
denoising network provides a strong inductive bias, ensuring geometrically
consistent novel view generation. Our qualitative and quantitative experiments
demonstrate that DSplats not only produces high-quality, spatially consistent
outputs, but also sets a new standard in single-image to 3D reconstruction.
When evaluated on the Google Scanned Objects dataset, DSplats achieves a PSNR
of 20.38, an SSIM of 0.842, and an LPIPS of 0.109.



---

## GASP: Gaussian Avatars with Synthetic Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrusaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay P. Namboodiri, Benjamin E Lundell | cs.CV | [PDF](http://arxiv.org/pdf/2412.07739v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has changed the game for real-time photo-realistic
rendering. One of the most popular applications of Gaussian Splatting is to
create animatable avatars, known as Gaussian Avatars. Recent works have pushed
the boundaries of quality and rendering efficiency but suffer from two main
limitations. Either they require expensive multi-camera rigs to produce avatars
with free-view rendering, or they can be trained with a single camera but only
rendered at high quality from this fixed viewpoint. An ideal model would be
trained using a short monocular video or image from available hardware, such as
a webcam, and rendered from any view. To this end, we propose GASP: Gaussian
Avatars with Synthetic Priors. To overcome the limitations of existing
datasets, we exploit the pixel-perfect nature of synthetic data to train a
Gaussian Avatar prior. By fitting this prior model to a single photo or video
and fine-tuning it, we get a high-quality Gaussian Avatar, which supports
360$^\circ$ rendering. Our prior is only required for fitting, not inference,
enabling real-time application. Through our method, we obtain high-quality,
animatable Avatars from limited data which can be animated and rendered at
70fps on commercial hardware. See our project page
(https://microsoft.github.io/GASP/) for results.

Comments:
- Project page: https://microsoft.github.io/GASP/

---

## Diffusion-Based Attention Warping for Consistent 3D Scene Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Eyal Gomel, Lior Wolf | cs.CV | [PDF](http://arxiv.org/pdf/2412.07984v1){: .btn .btn-green } |

**Abstract**: We present a novel method for 3D scene editing using diffusion models,
designed to ensure view consistency and realism across perspectives. Our
approach leverages attention features extracted from a single reference image
to define the intended edits. These features are warped across multiple views
by aligning them with scene geometry derived from Gaussian splatting depth
estimates. Injecting these warped features into other viewpoints enables
coherent propagation of edits, achieving high fidelity and spatial alignment in
3D space. Extensive evaluations demonstrate the effectiveness of our method in
generating versatile edits of 3D scenes, significantly advancing the
capabilities of scene manipulation compared to the existing methods. Project
page: \url{https://attention-warp.github.io}



---

## EventSplat: 3D Gaussian Splatting from Moving Event Cameras for  Real-time Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Toshiya Yura, Ashkan Mirzaei, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2412.07293v1){: .btn .btn-green } |

**Abstract**: We introduce a method for using event camera data in novel view synthesis via
Gaussian Splatting. Event cameras offer exceptional temporal resolution and a
high dynamic range. Leveraging these capabilities allows us to effectively
address the novel view synthesis challenge in the presence of fast camera
motion. For initialization of the optimization process, our approach uses prior
knowledge encoded in an event-to-video model. We also use spline interpolation
for obtaining high quality poses along the event camera trajectory. This
enhances the reconstruction quality from fast-moving cameras while overcoming
the computational limitations traditionally associated with event-based Neural
Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that
our results achieve higher visual fidelity and better performance than existing
event-based NeRF approaches while being an order of magnitude faster to render.



---

## ResGS: Residual Densification of 3D Gaussian for Efficient Detail  Recovery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Yanzhe Lyu, Kai Cheng, Xin Kang, Xuejin Chen | cs.CV | [PDF](http://arxiv.org/pdf/2412.07494v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view
synthesis, achieving high fidelity and efficiency. However, it often struggles
to capture rich details and complete geometry. Our analysis highlights a key
limitation of 3D-GS caused by the fixed threshold in densification, which
balances geometry coverage against detail recovery as the threshold varies. To
address this, we introduce a novel densification method, residual split, which
adds a downscaled Gaussian as a residual. Our approach is capable of adaptively
retrieving details and complementing missing geometry while enabling
progressive refinement. To further support this method, we propose a pipeline
named ResGS. Specifically, we integrate a Gaussian image pyramid for
progressive supervision and implement a selection scheme that prioritizes the
densification of coarse Gaussians over time. Extensive experiments demonstrate
that our method achieves SOTA rendering quality. Consistent performance
improvements can be achieved by applying our residual split on various 3D-GS
variants, underscoring its versatility and potential for broader application in
3D-GS-based applications.



---

## Proc-GS: Procedural Building Generation for City Assembly with 3D  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Yixuan Li, Xingjian Ran, Linning Xu, Tao Lu, Mulin Yu, Zhenzhi Wang, Yuanbo Xiangli, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2412.07660v1){: .btn .btn-green } |

**Abstract**: Buildings are primary components of cities, often featuring repeated elements
such as windows and doors. Traditional 3D building asset creation is
labor-intensive and requires specialized skills to develop design rules. Recent
generative models for building creation often overlook these patterns, leading
to low visual fidelity and limited scalability. Drawing inspiration from
procedural modeling techniques used in the gaming and visual effects industry,
our method, Proc-GS, integrates procedural code into the 3D Gaussian Splatting
(3D-GS) framework, leveraging their advantages in high-fidelity rendering and
efficient asset management from both worlds. By manipulating procedural code,
we can streamline this process and generate an infinite variety of buildings.
This integration significantly reduces model size by utilizing shared
foundational assets, enabling scalable generation with precise control over
building assembly. We showcase the potential for expansive cityscape generation
while maintaining high rendering fidelity and precise control on both real and
synthetic cases.

Comments:
- Project page: https://city-super.github.io/procgs/

---

## Faster and Better 3D Splatting via Group Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-10 | Chengbo Wang, Guozheng Ma, Yifei Xue, Yizhen Lao | cs.CV | [PDF](http://arxiv.org/pdf/2412.07608v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel
view synthesis, demonstrating remarkable capability in high-fidelity scene
reconstruction through its Gaussian primitive representations. However, the
computational overhead induced by the massive number of primitives poses a
significant bottleneck to training efficiency. To overcome this challenge, we
propose Group Training, a simple yet effective strategy that organizes Gaussian
primitives into manageable groups, optimizing training efficiency and improving
rendering quality. This approach shows universal compatibility with existing
3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently
achieving accelerated training while maintaining superior synthesis quality.
Extensive experiments reveal that our straightforward Group Training strategy
achieves up to 30% faster convergence and improved rendering quality across
diverse scenarios.



---

## Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Renlong Wu, Zhilu Zhang, Mingyang Chen, Xiaopeng Fan, Zifei Yan, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2412.06424v1){: .btn .btn-green } |

**Abstract**: Recent 4D reconstruction methods have yielded impressive results but rely on
sharp videos as supervision. However, motion blur often occurs in videos due to
camera shake and object movement, while existing methods render blurry results
when using such videos for reconstructing 4D models. Although a few NeRF-based
approaches attempted to address the problem, they struggled to produce
high-quality results, due to the inaccuracy in estimating continuous dynamic
representations within the exposure time. Encouraged by recent works in 3D
motion trajectory modeling using 3D Gaussian Splatting (3DGS), we suggest
taking 3DGS as the scene representation manner, and propose the first 4D
Gaussian Splatting framework to reconstruct a high-quality 4D model from blurry
monocular video, named Deblur4DGS. Specifically, we transform continuous
dynamic representations estimation within an exposure time into the exposure
time estimation. Moreover, we introduce exposure regularization to avoid
trivial solutions, as well as multi-frame and multi-resolution consistency ones
to alleviate artifacts. Furthermore, to better represent objects with large
motion, we suggest blur-aware variable canonical Gaussians. Beyond novel-view
synthesis, Deblur4DGS can be applied to improve blurry video from multiple
perspectives, including deblurring, frame interpolation, and video
stabilization. Extensive experiments on the above four tasks show that
Deblur4DGS outperforms state-of-the-art 4D reconstruction methods. The codes
are available at https://github.com/ZcsrenlongZ/Deblur4DGS.

Comments:
- 17 pages

---

## Splatter-360: Generalizable 360$^{\circ}$ Gaussian Splatting for  Wide-baseline Panoramic Images

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Zheng Chen, Chenming Wu, Zhelun Shen, Chen Zhao, Weicai Ye, Haocheng Feng, Errui Ding, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.06250v1){: .btn .btn-green } |

**Abstract**: Wide-baseline panoramic images are frequently used in applications like VR
and simulations to minimize capturing labor costs and storage needs. However,
synthesizing novel views from these panoramic images in real time remains a
significant challenge, especially due to panoramic imagery's high resolution
and inherent distortions. Although existing 3D Gaussian splatting (3DGS)
methods can produce photo-realistic views under narrow baselines, they often
overfit the training views when dealing with wide-baseline panoramic images due
to the difficulty in learning precise geometry from sparse 360$^{\circ}$ views.
This paper presents \textit{Splatter-360}, a novel end-to-end generalizable
3DGS framework designed to handle wide-baseline panoramic images. Unlike
previous approaches, \textit{Splatter-360} performs multi-view matching
directly in the spherical domain by constructing a spherical cost volume
through a spherical sweep algorithm, enhancing the network's depth perception
and geometry estimation. Additionally, we introduce a 3D-aware bi-projection
encoder to mitigate the distortions inherent in panoramic images and integrate
cross-view attention to improve feature interactions across multiple
viewpoints. This enables robust 3D-aware feature representations and real-time
rendering capabilities. Experimental results on the HM3D~\cite{hm3d} and
Replica~\cite{replica} demonstrate that \textit{Splatter-360} significantly
outperforms state-of-the-art NeRF and 3DGS methods (e.g., PanoGRF, MVSplat,
DepthSplat, and HiSplat) in both synthesis quality and generalization
performance for wide-baseline panoramic images. Code and trained models are
available at \url{https://3d-aigc.github.io/Splatter-360/}.

Comments:
- Project page:https://3d-aigc.github.io/Splatter-360/. Code:
  https://github.com/thucz/splatter360

---

## 4D Gaussian Splatting with Scale-aware Residual Field and Adaptive  Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Jinbo Yan, Rui Peng, Luyang Tang, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.06299v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic scenes from video sequences is a highly promising task
in the multimedia domain. While previous methods have made progress, they often
struggle with slow rendering and managing temporal complexities such as
significant motion and object appearance/disappearance. In this paper, we
propose SaRO-GS as a novel dynamic scene representation capable of achieving
real-time rendering while effectively handling temporal complexities in dynamic
scenes. To address the issue of slow rendering speed, we adopt a Gaussian
primitive-based representation and optimize the Gaussians in 4D space, which
facilitates real-time rendering with the assistance of 3D Gaussian Splatting.
Additionally, to handle temporally complex dynamic scenes, we introduce a
Scale-aware Residual Field. This field considers the size information of each
Gaussian primitive while encoding its residual feature and aligns with the
self-splitting behavior of Gaussian primitives. Furthermore, we propose an
Adaptive Optimization Schedule, which assigns different optimization strategies
to Gaussian primitives based on their distinct temporal properties, thereby
expediting the reconstruction of dynamic regions. Through evaluations on
monocular and multi-view datasets, our method has demonstrated state-of-the-art
performance. Please see our project page at
https://yjb6.github.io/SaRO-GS.github.io.



---

## Diffusing Differentiable Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Yash Savani, Marc Finzi, J. Zico Kolter | cs.CV | [PDF](http://arxiv.org/pdf/2412.06981v1){: .btn .btn-green } |

**Abstract**: We introduce a novel, training-free method for sampling differentiable
representations (diffreps) using pretrained diffusion models. Rather than
merely mode-seeking, our method achieves sampling by "pulling back" the
dynamics of the reverse-time process--from the image space to the diffrep
parameter space--and updating the parameters according to this pulled-back
process. We identify an implicit constraint on the samples induced by the
diffrep and demonstrate that addressing this constraint significantly improves
the consistency and detail of the generated objects. Our method yields diffreps
with substantially improved quality and diversity for images, panoramas, and 3D
NeRFs compared to existing techniques. Our approach is a general-purpose method
for sampling diffreps, expanding the scope of problems that diffusion models
can tackle.

Comments:
- Published at NeurIPS 2024

---

## Advancing Extended Reality with 3D Gaussian Splatting: Innovations and  Prospects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng | cs.CV | [PDF](http://arxiv.org/pdf/2412.06257v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has attracted significant attention for its
potential to revolutionize 3D representation, rendering, and interaction.
Despite the rapid growth of 3DGS research, its direct application to Extended
Reality (XR) remains underexplored. Although many studies recognize the
potential of 3DGS for XR, few have explicitly focused on or demonstrated its
effectiveness within XR environments. In this paper, we aim to synthesize
innovations in 3DGS that show specific potential for advancing XR research and
development. We conduct a comprehensive review of publicly available 3DGS
papers, with a focus on those referencing XR-related concepts. Additionally, we
perform an in-depth analysis of innovations explicitly relevant to XR and
propose a taxonomy to highlight their significance. Building on these insights,
we propose several prospective XR research areas where 3DGS can make promising
contributions, yet remain rarely touched. By investigating the intersection of
3DGS and XR, this paper provides a roadmap to push the boundaries of XR using
cutting-edge 3DGS techniques.

Comments:
- IEEE AIxVR 2025

---

## Generative Densification: Learning to Densify Gaussians for  High-Fidelity Generalizable 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Seungtae Nam, Xiangyu Sun, Gyeongjin Kang, Younggeun Lee, Seungjun Oh, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2412.06234v2){: .btn .btn-green } |

**Abstract**: Generalized feed-forward Gaussian models have achieved significant progress
in sparse-view 3D reconstruction by leveraging prior knowledge from large
multi-view datasets. However, these models often struggle to represent
high-frequency details due to the limited number of Gaussians. While the
densification strategy used in per-scene 3D Gaussian splatting (3D-GS)
optimization can be adapted to the feed-forward models, it may not be ideally
suited for generalized scenarios. In this paper, we propose Generative
Densification, an efficient and generalizable method to densify Gaussians
generated by feed-forward models. Unlike the 3D-GS densification strategy,
which iteratively splits and clones raw Gaussian parameters, our method
up-samples feature representations from the feed-forward models and generates
their corresponding fine Gaussians in a single forward pass, leveraging the
embedded prior knowledge for enhanced generalization. Experimental results on
both object-level and scene-level reconstruction tasks demonstrate that our
method outperforms state-of-the-art approaches with comparable or smaller model
sizes, achieving notable improvements in representing fine details.

Comments:
- Project page: https://stnamjef.github.io/GenerativeDensification/

---

## Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view  Event Cameras

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Viktor Rudnev, Gereon Fox, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik | cs.CV | [PDF](http://arxiv.org/pdf/2412.06770v1){: .btn .btn-green } |

**Abstract**: Volumetric reconstruction of dynamic scenes is an important problem in
computer vision. It is especially challenging in poor lighting and with fast
motion. It is partly due to the limitations of RGB cameras: To capture fast
motion without much blur, the framerate must be increased, which in turn
requires more lighting. In contrast, event cameras, which record changes in
pixel brightness asynchronously, are much less dependent on lighting, making
them more suitable for recording fast motion. We hence propose the first method
to spatiotemporally reconstruct a scene from sparse multi-view event streams
and sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF
models, one per short recording segment. The individual segments are supervised
with a set of event- and RGB-based losses and sparse-view regularisation. We
assemble a real-world multi-view camera rig with six static event cameras
around the object and record a benchmark multi-view event stream dataset of
challenging motions. Our work outperforms RGB-based baselines, producing
state-of-the-art results, and opens up the topic of multi-view event-based
reconstruction as a new path for fast scene capture beyond RGB cameras. The
code and the data will be released soon at
https://4dqv.mpi-inf.mpg.de/DynEventNeRF/

Comments:
- 15 pages, 11 figures, 6 tables

---

## MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2  Seconds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-09 | Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, Zhicheng Yan | cs.CV | [PDF](http://arxiv.org/pdf/2412.06974v1){: .btn .btn-green } |

**Abstract**: Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R
no longer require camera calibration and camera pose estimation. However, they
only process a pair of views at a time to infer pixel-aligned pointmaps. When
dealing with more than two views, a combinatorial number of error prone
pairwise reconstructions are usually followed by an expensive global
optimization, which often fails to rectify the pairwise reconstruction errors.
To handle more views, reduce errors, and improve inference time, we propose the
fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view
decoder blocks which exchange information across any number of views while
considering one reference view. To make our method robust to reference view
selection, we further propose MV-DUSt3R+, which employs cross-reference-view
blocks to fuse information across different reference view choices. To further
enable novel view synthesis, we extend both by adding and jointly training
Gaussian splatting heads. Experiments on multi-view stereo reconstruction,
multi-view pose estimation, and novel view synthesis confirm that our methods
improve significantly upon prior art. Code will be released.



---

## Efficient Semantic Splatting for Remote Sensing Multi-view Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-08 | Zipeng Qi, Hao Chen, Haotian Zhang, Zhengxia Zou, Zhenwei Shi | cs.CV | [PDF](http://arxiv.org/pdf/2412.05969v2){: .btn .btn-green } |

**Abstract**: In this paper, we propose a novel semantic splatting approach based on
Gaussian Splatting to achieve efficient and low-latency. Our method projects
the RGB attributes and semantic features of point clouds onto the image plane,
simultaneously rendering RGB images and semantic segmentation results.
Leveraging the explicit structure of point clouds and a one-time rendering
strategy, our approach significantly enhances efficiency during optimization
and rendering. Additionally, we employ SAM2 to generate pseudo-labels for
boundary regions, which often lack sufficient supervision, and introduce
two-level aggregation losses at the 2D feature map and 3D spatial levels to
improve the view-consistent and spatial continuity.



---

## GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting  and Meshing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-08 | Jianing Zhang, Yuchao Zheng, Ziwei Li, Qionghai Dai, Xiaoyun Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2412.05908v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has gained attention for its efficient representation and
rendering of 3D scenes using continuous Gaussian primitives. However, it
struggles with sparse-view inputs due to limited geometric and photometric
information, causing ambiguities in depth, shape, and texture.
  we propose GBR: Generative Bundle Refinement, a method for high-fidelity
Gaussian splatting and meshing using only 4-6 input views. GBR integrates a
neural bundle adjustment module to enhance geometry accuracy and a generative
depth refinement module to improve geometry fidelity. More specifically, the
neural bundle adjustment module integrates a foundation network to produce
initial 3D point maps and point matches from unposed images, followed by bundle
adjustment optimization to improve multiview consistency and point cloud
accuracy. The generative depth refinement module employs a diffusion-based
strategy to enhance geometric details and fidelity while preserving the scale.
Finally, for Gaussian splatting optimization, we propose a multimodal loss
function incorporating depth and normal consistency, geometric regularization,
and pseudo-view supervision, providing robust guidance under sparse-view
conditions. Experiments on widely used datasets show that GBR significantly
outperforms existing methods under sparse-view inputs. Additionally, GBR
demonstrates the ability to reconstruct and render large-scale real-world
scenes, such as the Pavilion of Prince Teng and the Great Wall, with remarkable
details using only 6 views.



---

## Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Saqib Javed, Ahmad Jarrar Khan, Corentin Dumery, Chen Zhao, Mathieu Salzmann | cs.CV | [PDF](http://arxiv.org/pdf/2412.05700v1){: .btn .btn-green } |

**Abstract**: Recent advancements in high-fidelity dynamic scene reconstruction have
leveraged dynamic 3D Gaussians and 4D Gaussian Splatting for realistic scene
representation. However, to make these methods viable for real-time
applications such as AR/VR, gaming, and rendering on low-power devices,
substantial reductions in memory usage and improvements in rendering efficiency
are required. While many state-of-the-art methods prioritize lightweight
implementations, they struggle in handling scenes with complex motions or long
sequences. In this work, we introduce Temporally Compressed 3D Gaussian
Splatting (TC3DGS), a novel technique designed specifically to effectively
compress dynamic 3D Gaussian representations. TC3DGS selectively prunes
Gaussians based on their temporal relevance and employs gradient-aware
mixed-precision quantization to dynamically compress Gaussian parameters. It
additionally relies on a variation of the Ramer-Douglas-Peucker algorithm in a
post-processing step to further reduce storage by interpolating Gaussian
trajectories across frames. Our experiments across multiple datasets
demonstrate that TC3DGS achieves up to 67$\times$ compression with minimal or
no degradation in visual quality.

Comments:
- Code will be released soon

---

## Radiant: Large-scale 3D Gaussian Rendering based on Hierarchical  Framework


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Haosong Peng, Tianyu Qi, Yufeng Zhan, Hao Li, Yalun Dai, Yuanqing Xia | cs.CV | [PDF](http://arxiv.org/pdf/2412.05546v1){: .btn .btn-green } |

**Abstract**: With the advancement of computer vision, the recently emerged 3D Gaussian
Splatting (3DGS) has increasingly become a popular scene reconstruction
algorithm due to its outstanding performance. Distributed 3DGS can efficiently
utilize edge devices to directly train on the collected images, thereby
offloading computational demands and enhancing efficiency. However, traditional
distributed frameworks often overlook computational and communication
challenges in real-world environments, hindering large-scale deployment and
potentially posing privacy risks. In this paper, we propose Radiant, a
hierarchical 3DGS algorithm designed for large-scale scene reconstruction that
considers system heterogeneity, enhancing the model performance and training
efficiency. Via extensive empirical study, we find that it is crucial to
partition the regions for each edge appropriately and allocate varying camera
positions to each device for image collection and training. The core of Radiant
is partitioning regions based on heterogeneous environment information and
allocating workloads to each device accordingly. Furthermore, we provide a 3DGS
model aggregation algorithm that enhances the quality and ensures the
continuity of models' boundaries. Finally, we develop a testbed, and
experiments demonstrate that Radiant improved reconstruction quality by up to
25.7\% and reduced up to 79.6\% end-to-end latency.



---

## Template-free Articulated Gaussian Splatting for Real-time Reposable  Dynamic View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Diwen Wan, Yuxiang Wang, Ruijie Lu, Gang Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2412.05570v1){: .btn .btn-green } |

**Abstract**: While novel view synthesis for dynamic scenes has made significant progress,
capturing skeleton models of objects and re-posing them remains a challenging
task. To tackle this problem, in this paper, we propose a novel approach to
automatically discover the associated skeleton model for dynamic objects from
videos without the need for object-specific templates. Our approach utilizes 3D
Gaussian Splatting and superpoints to reconstruct dynamic objects. Treating
superpoints as rigid parts, we can discover the underlying skeleton model
through intuitive cues and optimize it using the kinematic model. Besides, an
adaptive control strategy is applied to avoid the emergence of redundant
superpoints. Extensive experiments demonstrate the effectiveness and efficiency
of our method in obtaining re-posable 3D objects. Not only can our approach
achieve excellent visual fidelity, but it also allows for the real-time
rendering of high-resolution images.

Comments:
- Accepted by NeurIPS 2024

---

## Text-to-3D Gaussian Splatting with Physics-Grounded Motion Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Wenqing Wang, Yun Fu | cs.CV | [PDF](http://arxiv.org/pdf/2412.05560v1){: .btn .btn-green } |

**Abstract**: Text-to-3D generation is a valuable technology in virtual reality and digital
content creation. While recent works have pushed the boundaries of text-to-3D
generation, producing high-fidelity 3D objects with inefficient prompts and
simulating their physics-grounded motion accurately still remain unsolved
challenges. To address these challenges, we present an innovative framework
that utilizes the Large Language Model (LLM)-refined prompts and diffusion
priors-guided Gaussian Splatting (GS) for generating 3D models with accurate
appearances and geometric structures. We also incorporate a continuum
mechanics-based deformation map and color regularization to synthesize vivid
physics-grounded motion for the generated 3D Gaussians, adhering to the
conservation of mass and momentum. By integrating text-to-3D generation with
physics-grounded motion synthesis, our framework renders photo-realistic 3D
objects that exhibit physics-aware motion, accurately reflecting the behaviors
of the objects under various forces and constraints across different materials.
Extensive experiments demonstrate that our approach achieves high-quality 3D
generations with realistic physics-grounded motion.



---

## WATER-GS: Toward Copyright Protection for 3D Gaussian Splatting via  Universal Watermarking

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-07 | Yuqi Tan, Xiang Liu, Shuzhao Xie, Bin Chen, Shu-Tao Xia, Zhi Wang | cs.CR | [PDF](http://arxiv.org/pdf/2412.05695v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for 3D scene
representation, providing rapid rendering speeds and high fidelity. As 3DGS
gains prominence, safeguarding its intellectual property becomes increasingly
crucial since 3DGS could be used to imitate unauthorized scene creations and
raise copyright issues. Existing watermarking methods for implicit NeRFs cannot
be directly applied to 3DGS due to its explicit representation and real-time
rendering process, leaving watermarking for 3DGS largely unexplored. In
response, we propose WATER-GS, a novel method designed to protect 3DGS
copyrights through a universal watermarking strategy. First, we introduce a
pre-trained watermark decoder, treating raw 3DGS generative modules as
potential watermark encoders to ensure imperceptibility. Additionally, we
implement novel 3D distortion layers to enhance the robustness of the embedded
watermark against common real-world distortions of point cloud data.
Comprehensive experiments and ablation studies demonstrate that WATER-GS
effectively embeds imperceptible and robust watermarks into 3DGS without
compromising rendering efficiency and quality. Our experiments indicate that
the 3D distortion layers can yield up to a 20% improvement in accuracy rate.
Notably, our method is adaptable to different 3DGS variants, including 3DGS
compression frameworks and 2D Gaussian splatting.



---

## Extrapolated Urban View Synthesis Benchmark

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Xiangyu Han, Zhen Jia, Boyi Li, Yan Wang, Boris Ivanovic, Yurong You, Lingjie Liu, Yue Wang, Marco Pavone, Chen Feng, Yiming Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.05256v2){: .btn .btn-green } |

**Abstract**: Photorealistic simulators are essential for the training and evaluation of
vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis
(NVS), a crucial capability that generates diverse unseen viewpoints to
accommodate the broad and continuous pose distribution of AVs. Recent advances
in radiance fields, such as 3D Gaussian Splatting, achieve photorealistic
rendering at real-time speeds and have been widely used in modeling large-scale
driving scenes. However, their performance is commonly evaluated using an
interpolated setup with highly correlated training and test views. In contrast,
extrapolation, where test views largely deviate from training views, remains
underexplored, limiting progress in generalizable simulation technology. To
address this gap, we leverage publicly available AV datasets with multiple
traversals, multiple vehicles, and multiple cameras to build the first
Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct
quantitative and qualitative evaluations of state-of-the-art Gaussian Splatting
methods across different difficulty levels. Our results show that Gaussian
Splatting is prone to overfitting to training views. Besides, incorporating
diffusion priors and improving geometry cannot fundamentally improve NVS under
large view changes, highlighting the need for more robust approaches and
large-scale training. We have released our data to help advance self-driving
and urban robotics simulation technology.

Comments:
- Project page: https://ai4ce.github.io/EUVS-Benchmark/

---

## MixedGaussianAvatar: Realistically and Geometrically Accurate Head  Avatar via Mixed 2D-3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu | cs.CV | [PDF](http://arxiv.org/pdf/2412.04955v2){: .btn .btn-green } |

**Abstract**: Reconstructing high-fidelity 3D head avatars is crucial in various
applications such as virtual reality. The pioneering methods reconstruct
realistic head avatars with Neural Radiance Fields (NeRF), which have been
limited by training and rendering speed. Recent methods based on 3D Gaussian
Splatting (3DGS) significantly improve the efficiency of training and
rendering. However, the surface inconsistency of 3DGS results in subpar
geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy
at the expense of rendering fidelity. To leverage the benefits of both 2DGS and
3DGS, we propose a novel method named MixedGaussianAvatar for realistically and
geometrically accurate head avatar reconstruction. Our main idea is to utilize
2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric
accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model
and connect additional 3D Gaussians to those 2D Gaussians where the rendering
quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation.
These 2D-3D Gaussians can then be animated using FLAME parameters. We further
introduce a progressive training strategy that first trains the 2D Gaussians
and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority
of MixedGaussianAvatar through comprehensive experiments. The code will be
released at: https://github.com/ChenVoid/MGA/.

Comments:
- Project: https://chenvoid.github.io/MGA/

---

## Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Jixuan Fan, Wanhua Li, Yifei Han, Yansong Tang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04887v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has demonstrated notable success in large-scale scene
reconstruction, but challenges persist due to high training memory consumption
and storage overhead. Hybrid representations that integrate implicit and
explicit features offer a way to mitigate these limitations. However, when
applied in parallelized block-wise training, two critical issues arise since
reconstruction accuracy deteriorates due to reduced data diversity when
training each block independently, and parallel training restricts the number
of divided blocks to the available number of GPUs. To address these issues, we
propose Momentum-GS, a novel approach that leverages momentum-based
self-distillation to promote consistency and accuracy across the blocks while
decoupling the number of blocks from the physical GPU count. Our method
maintains a teacher Gaussian decoder updated with momentum, ensuring a stable
reference during training. This teacher provides each block with global
guidance in a self-distillation manner, promoting spatial consistency in
reconstruction. To further ensure consistency across the blocks, we incorporate
block weighting, dynamically adjusting each block's weight according to its
reconstruction accuracy. Extensive experiments on large-scale scenes show that
our method consistently outperforms existing techniques, achieving a 12.8%
improvement in LPIPS over CityGaussian with much fewer divided blocks and
establishing a new state of the art. Project page:
https://jixuan-fan.github.io/Momentum-GS_Page/



---

## Pushing Rendering Boundaries: Hard Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Qingshan Xu, Jiequan Cui, Xuanyu Yi, Yuxuan Wang, Yuan Zhou, Yew-Soon Ong, Hanwang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04826v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive Novel View Synthesis
(NVS) results in a real-time rendering manner. During training, it relies
heavily on the average magnitude of view-space positional gradients to grow
Gaussians to reduce rendering loss. However, this average operation smooths the
positional gradients from different viewpoints and rendering errors from
different pixels, hindering the growth and optimization of many defective
Gaussians. This leads to strong spurious artifacts in some areas. To address
this problem, we propose Hard Gaussian Splatting, dubbed HGS, which considers
multi-view significant positional gradients and rendering errors to grow hard
Gaussians that fill the gaps of classical Gaussian Splatting on 3D scenes, thus
achieving superior NVS results. In detail, we present positional gradient
driven HGS, which leverages multi-view significant positional gradients to
uncover hard Gaussians. Moreover, we propose rendering error guided HGS, which
identifies noticeable pixel rendering errors and potentially over-large
Gaussians to jointly mine hard Gaussians. By growing and optimizing these hard
Gaussians, our method helps to resolve blurring and needle-like artifacts.
Experiments on various datasets demonstrate that our method achieves
state-of-the-art rendering quality while maintaining real-time efficiency.



---

## Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Susung Hong, Johanna Karras, Ricardo Martin-Brualla, Ira Kemelmacher-Shlizerman | cs.CV | [PDF](http://arxiv.org/pdf/2412.05279v1){: .btn .btn-green } |

**Abstract**: The fields of 3D reconstruction and text-based 3D editing have advanced
significantly with the evolution of text-based diffusion models. While existing
3D editing methods excel at modifying color, texture, and style, they struggle
with extensive geometric or appearance changes, thus limiting their
applications. We propose Perturb-and-Revise, which makes possible a variety of
NeRF editing. First, we perturb the NeRF parameters with random initializations
to create a versatile initialization. We automatically determine the
perturbation magnitude through analysis of the local loss landscape. Then, we
revise the edited NeRF via generative trajectories. Combined with the
generative process, we impose identity-preserving gradients to refine the
edited NeRF. Extensive experiments demonstrate that Perturb-and-Revise
facilitates flexible, effective, and consistent editing of color, appearance,
and geometry in 3D. For 360{\deg} results, please visit our project page:
https://susunghong.github.io/Perturb-and-Revise.

Comments:
- Project page: https://susunghong.github.io/Perturb-and-Revise

---

## WRF-GS: Wireless Radiation Field Reconstruction with 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-06 | Chaozheng Wen, Jingwen Tong, Yingdong Hu, Zehong Lin, Jun Zhang | cs.NI | [PDF](http://arxiv.org/pdf/2412.04832v1){: .btn .btn-green } |

**Abstract**: Wireless channel modeling plays a pivotal role in designing, analyzing, and
optimizing wireless communication systems. Nevertheless, developing an
effective channel modeling approach has been a longstanding challenge. This
issue has been escalated due to the denser network deployment, larger antenna
arrays, and wider bandwidth in 5G and beyond networks. To address this
challenge, we put forth WRF-GS, a novel framework for channel modeling based on
wireless radiation field (WRF) reconstruction using 3D Gaussian splatting.
WRF-GS employs 3D Gaussian primitives and neural networks to capture the
interactions between the environment and radio signals, enabling efficient WRF
reconstruction and visualization of the propagation characteristics. The
reconstructed WRF can then be used to synthesize the spatial spectrum for
comprehensive wireless channel characterization. Notably, with a small number
of measurements, WRF-GS can synthesize new spatial spectra within milliseconds
for a given scene, thereby enabling latency-sensitive applications.
Experimental results demonstrate that WRF-GS outperforms existing methods for
spatial spectrum synthesis, such as ray tracing and other deep-learning
approaches. Moreover, WRF-GS achieves superior performance in the channel state
information prediction task, surpassing existing methods by a significant
margin of more than 2.43 dB.

Comments:
- accepted to the IEEE International Conference on Computer
  Communications (INFOCOM 2025)

---

## Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04459v1){: .btn .btn-green } |

**Abstract**: We propose an efficient radiance field rendering algorithm that incorporates
a rasterization process on sparse voxels without neural networks or 3D
Gaussians. There are two key contributions coupled with the proposed system.
The first is to render sparse voxels in the correct depth order along pixel
rays by using dynamic Morton ordering. This avoids the well-known popping
artifact found in Gaussian splatting. Second, we adaptively fit sparse voxels
to different levels of detail within scenes, faithfully reproducing scene
details while achieving high rendering frame rates. Our method improves the
previous neural-free voxel grid representation by over 4db PSNR and more than
10x rendering FPS speedup, achieving state-of-the-art comparable novel-view
synthesis results. Additionally, our neural-free sparse voxels are seamlessly
compatible with grid-based 3D processing algorithms. We achieve promising mesh
reconstruction accuracy by integrating TSDF-Fusion and Marching Cubes into our
sparse grid system.

Comments:
- Code release in progress

---

## QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming  Free-viewpoint Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Sharath Girish, Tianye Li, Amrita Mazumdar, Abhinav Shrivastava, David Luebke, Shalini De Mello | cs.CV | [PDF](http://arxiv.org/pdf/2412.04469v1){: .btn .btn-green } |

**Abstract**: Online free-viewpoint video (FVV) streaming is a challenging problem, which
is relatively under-explored. It requires incremental on-the-fly updates to a
volumetric representation, fast training and rendering to satisfy real-time
constraints and a small memory footprint for efficient transmission. If
achieved, it can enhance user experience by enabling novel applications, e.g.,
3D video conferencing and live volumetric video broadcast, among others. In
this work, we propose a novel framework for QUantized and Efficient ENcoding
(QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly
learns Gaussian attribute residuals between consecutive frames at each
time-step without imposing any structural constraints on them, allowing for
high quality reconstruction and generalizability. To efficiently store the
residuals, we further propose a quantization-sparsity framework, which contains
a learned latent-decoder for effectively quantizing attribute residuals other
than Gaussian positions and a learned gating module to sparsify position
residuals. We propose to use the Gaussian viewspace gradient difference vector
as a signal to separate the static and dynamic content of the scene. It acts as
a guide for effective sparsity learning and speeds up training. On diverse FVV
benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all
metrics. Notably, for several highly dynamic scenes, it reduces the model size
to just 0.7 MB per frame while training in under 5 sec and rendering at 350
FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen

Comments:
- Accepted at NeurIPS 2024, Project website:
  https://research.nvidia.com/labs/amri/projects/queen

---

## HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, Jieping Ye | cs.CV | [PDF](http://arxiv.org/pdf/2412.03844v1){: .btn .btn-green } |

**Abstract**: Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS)
in scenes featuring transient objects is challenging. We propose a novel hybrid
representation, termed as HybridGS, using 2D Gaussians for transient objects
per image and maintaining traditional 3D Gaussians for the whole static scenes.
Note that, the 3DGS itself is better suited for modeling static scenes that
assume multi-view consistency, but the transient objects appear occasionally
and do not adhere to the assumption, thus we model them as planar objects from
a single view, represented with 2D Gaussians. Our novel representation
decomposes the scene from the perspective of fundamental viewpoint consistency,
making it more reasonable. Additionally, we present a novel multi-view
regulated supervision method for 3DGS that leverages information from
co-visible regions, further enhancing the distinctions between the transients
and statics. Then, we propose a straightforward yet effective multi-stage
training strategy to ensure robust training and high-quality view synthesis
across various settings. Experiments on benchmark datasets show our
state-of-the-art performance of novel view synthesis in both indoor and outdoor
scenes, even in the presence of distracting elements.

Comments:
- Project page: https://gujiaqivadin.github.io/hybridgs/

---

## DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for  Monocular Dynamic 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Xuesong Li, Jinguang Tong, Jie Hong, Vivien Rolland, Lars Petersson | cs.CV | [PDF](http://arxiv.org/pdf/2412.03910v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction from monocular video is critical for real-world
applications. This paper tackles the dual challenges of dynamic novel-view
synthesis and 3D geometry reconstruction by introducing a hybrid framework:
Deformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both
modules can leverage each other for both tasks. During training, depth maps
generated by the deformable Gaussian splatting module guide the ray sampling
for faster processing and provide depth supervision within the dynamic neural
surface module to improve geometry reconstruction. Simultaneously, the dynamic
neural surface directs the distribution of Gaussian primitives around the
surface, enhancing rendering quality. To further refine depth supervision, we
introduce a depth-filtering process on depth maps derived from Gaussian
rasterization. Extensive experiments on public datasets demonstrate that DGNS
achieves state-of-the-art performance in both novel-view synthesis and 3D
reconstruction.



---

## Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth  Motion Helps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Yiqing Liang, Mikhail Okunev, Mikaela Angelina Uy, Runfeng Li, Leonidas Guibas, James Tompkin, Adam W. Harley | cs.CV | [PDF](http://arxiv.org/pdf/2412.04457v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting methods are emerging as a popular approach for converting
multi-view image data into scene representations that allow view synthesis. In
particular, there is interest in enabling view synthesis for dynamic scenes
using only monocular input data -- an ill-posed and challenging problem. The
fast pace of work in this area has produced multiple simultaneous papers that
claim to work best, which cannot all be true. In this work, we organize,
benchmark, and analyze many Gaussian-splatting-based methods, providing
apples-to-apples comparisons that prior works have lacked. We use multiple
existing datasets and a new instructive synthetic dataset designed to isolate
factors that affect reconstruction quality. We systematically categorize
Gaussian splatting methods into specific motion representation types and
quantify how their differences impact performance. Empirically, we find that
their rank order is well-defined in synthetic data, but the complexity of
real-world data currently overwhelms the differences. Furthermore, the fast
rendering speed of all Gaussian-based methods comes at the cost of brittleness
in optimization. We summarize our experiments into a list of findings that can
help to further progress in this lively problem setting. Project Webpage:
https://lynl7130.github.io/MonoDyGauBench.github.io/

Comments:
- 37 pages, 39 figures, 9 tables

---

## PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human  Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Shota Sasaki, Jane Wu, Ko Nishino | cs.CV | [PDF](http://arxiv.org/pdf/2412.04433v2){: .btn .btn-green } |

**Abstract**: This paper introduces a novel clothed human model that can be learned from
multiview RGB videos, with a particular emphasis on recovering physically
accurate body and cloth movements. Our method, Position Based Dynamic Gaussians
(PBDyG), realizes ``movement-dependent'' cloth deformation via physical
simulation, rather than merely relying on ``pose-dependent'' rigid
transformations. We model the clothed human holistically but with two distinct
physical entities in contact: clothing modeled as 3D Gaussians, which are
attached to a skinned SMPL body that follows the movement of the person in the
input videos. The articulation of the SMPL body also drives physically-based
simulation of the clothes' Gaussians to transform the avatar to novel poses. In
order to run position based dynamics simulation, physical properties including
mass and material stiffness are estimated from the RGB videos through Dynamic
3D Gaussian Splatting. Experiments demonstrate that our method not only
accurately reproduces appearance but also enables the reconstruction of avatars
wearing highly deformable garments, such as skirts or coats, which have been
challenging to reconstruct using existing methods.



---

## Multi-View Pose-Agnostic Change Localization with Zero Labels

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Chamuditha Jayanga Galappaththige, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko Suenderhauf, Dimity Miller | cs.CV | [PDF](http://arxiv.org/pdf/2412.03911v1){: .btn .btn-green } |

**Abstract**: Autonomous agents often require accurate methods for detecting and localizing
changes in their environment, particularly when observations are captured from
unconstrained and inconsistent viewpoints. We propose a novel label-free,
pose-agnostic change detection method that integrates information from multiple
viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS)
representation of the scene. With as few as 5 images of the post-change scene,
our approach can learn additional change channels in a 3DGS and produce change
masks that outperform single-view techniques. Our change-aware 3D scene
representation additionally enables the generation of accurate change masks for
unseen viewpoints. Experimental results demonstrate state-of-the-art
performance in complex multi-object scenes, achieving a 1.7$\times$ and
1.6$\times$ improvement in Mean Intersection Over Union and F1 score
respectively over other baselines. We also contribute a new real-world dataset
to benchmark change detection in diverse challenging scenes in the presence of
lighting variations.



---

## Turbo3D: Ultra-fast Text-to-3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Hanzhe Hu, Tianwei Yin, Fujun Luan, Yiwei Hu, Hao Tan, Zexiang Xu, Sai Bi, Shubham Tulsiani, Kai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04470v1){: .btn .btn-green } |

**Abstract**: We present Turbo3D, an ultra-fast text-to-3D system capable of generating
high-quality Gaussian splatting assets in under one second. Turbo3D employs a
rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian
reconstructor, both operating in latent space. The 4-step, 4-view generator is
a student model distilled through a novel Dual-Teacher approach, which
encourages the student to learn view consistency from a multi-view teacher and
photo-realism from a single-view teacher. By shifting the Gaussian
reconstructor's inputs from pixel space to latent space, we eliminate the extra
image decoding time and halve the transformer sequence length for maximum
efficiency. Our method demonstrates superior 3D generation results compared to
previous baselines, while operating in a fraction of their runtime.

Comments:
- project page: https://turbo-3d.github.io/

---

## RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Yoonwoo Jeong, Junmyeong Lee, Hoseung Choi, Minsu Cho | cs.CV | [PDF](http://arxiv.org/pdf/2412.03077v1){: .btn .btn-green } |

**Abstract**: Dynamic view synthesis (DVS) has advanced remarkably in recent years,
achieving high-fidelity rendering while reducing computational costs. Despite
the progress, optimizing dynamic neural fields from casual videos remains
challenging, as these videos do not provide direct 3D information, such as
camera trajectories or the underlying scene geometry. In this work, we present
RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual
videos. It effectively learns motion and underlying geometry of scenes by
separating dynamic and static primitives, and ensures that the learned motion
and geometry are physically plausible by incorporating motion and geometric
regularization terms. We also introduce a comprehensive benchmark, Kubric-MRig,
that provides extensive camera and object motion along with simultaneous
multi-view captures, features that are absent in previous benchmarks.
Experimental results demonstrate that the proposed method significantly
outperforms previous pose-free dynamic neural fields and achieves competitive
rendering quality compared to existing pose-free static neural fields. The code
and data are publicly available at https://rodygs.github.io/.

Comments:
- Project Page: https://rodygs.github.io/

---

## NeRF and Gaussian Splatting SLAM in the Wild

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Fabian Schmidt, Markus Enzweiler, Abhinav Valada | cs.RO | [PDF](http://arxiv.org/pdf/2412.03263v1){: .btn .btn-green } |

**Abstract**: Navigating outdoor environments with visual Simultaneous Localization and
Mapping (SLAM) systems poses significant challenges due to dynamic scenes,
lighting variations, and seasonal changes, requiring robust solutions. While
traditional SLAM methods struggle with adaptability, deep learning-based
approaches and emerging neural radiance fields as well as Gaussian
Splatting-based SLAM methods, offer promising alternatives. However, these
methods have primarily been evaluated in controlled indoor environments with
stable conditions, leaving a gap in understanding their performance in
unstructured and variable outdoor settings. This study addresses this gap by
evaluating these methods in natural outdoor environments, focusing on camera
tracking accuracy, robustness to environmental factors, and computational
efficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate
that neural SLAM methods achieve superior robustness, particularly under
challenging conditions such as low light, but at a high computational cost. At
the same time, traditional methods perform the best across seasons but are
highly sensitive to variations in lighting conditions. The code of the
benchmark is publicly available at
https://github.com/iis-esslingen/nerf-3dgs-benchmark.

Comments:
- 5 pages, 2 figures, 4 tables

---

## Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2412.03526v1){: .btn .btn-green } |

**Abstract**: Recent advancements in static feed-forward scene reconstruction have
demonstrated significant progress in high-quality novel view synthesis.
However, these models often struggle with generalizability across diverse
environments and fail to effectively handle dynamic content. We present BTimer
(short for BulletTimer), the first motion-aware feed-forward model for
real-time reconstruction and novel view synthesis of dynamic scenes. Our
approach reconstructs the full scene in a 3D Gaussian Splatting representation
at a given target ('bullet') timestamp by aggregating information from all the
context frames. Such a formulation allows BTimer to gain scalability and
generalization by leveraging both static and dynamic scene datasets. Given a
casual monocular dynamic video, BTimer reconstructs a bullet-time scene within
150ms while reaching state-of-the-art performance on both static and dynamic
scene datasets, even compared with optimization-based approaches.

Comments:
- Project website:
  https://research.nvidia.com/labs/toronto-ai/bullet-timer/

---

## SGSST: Scaling Gaussian Splatting StyleTransfer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Bruno Galerne, Jianling Wang, Lara Raad, Jean-Michel Morel | cs.CV | [PDF](http://arxiv.org/pdf/2412.03371v1){: .btn .btn-green } |

**Abstract**: Applying style transfer to a full 3D environment is a challenging task that
has seen many developments since the advent of neural rendering. 3D Gaussian
splatting (3DGS) has recently pushed further many limits of neural rendering in
terms of training speed and reconstruction quality. This work introduces SGSST:
Scaling Gaussian Splatting Style Transfer, an optimization-based method to
apply style transfer to pretrained 3DGS scenes. We demonstrate that a new
multiscale loss based on global neural statistics, that we name SOS for
Simultaneously Optimized Scales, enables style transfer to ultra-high
resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such
high image resolutions, it also produces superior visual quality as assessed by
thorough qualitative, quantitative and perceptual comparisons.



---

## Splats in Splats: Embedding Invisible 3D Watermark within Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Yijia Guo, Wenkai Huang, Yang Li, Gaolei Li, Hang Zhang, Liwen Hu, Jianhua Li, Tiejun Huang, Lei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2412.03121v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction
performance with explicit scene representations. Given the widespread
application of 3DGS in 3D reconstruction and generation tasks, there is an
urgent need to protect the copyright of 3DGS assets. However, existing
copyright protection techniques for 3DGS overlook the usability of 3D assets,
posing challenges for practical deployment. Here we describe WaterGS, the first
3DGS watermarking framework that embeds 3D content in 3DGS itself without
modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep
insight into spherical harmonics (SH) and devise an importance-graded SH
coefficient encryption strategy to embed the hidden SH coefficients.
Furthermore, we employ a convolutional autoencoder to establish a mapping
between the original Gaussian primitives' opacity and the hidden Gaussian
primitives' opacity. Extensive experiments indicate that WaterGS significantly
outperforms existing 3D steganography techniques, with 5.31% higher scene
fidelity and 3X faster rendering speed, while ensuring security, robustness,
and user experience. Codes and data will be released at
https://water-gs.github.io.



---

## 2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains  for High-Fidelity Indoor Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Wanting Zhang, Haodong Xiang, Zhichao Liao, Xiansong Lai, Xinghui Li, Long Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2412.03428v1){: .btn .btn-green } |

**Abstract**: The reconstruction of indoor scenes remains challenging due to the inherent
complexity of spatial structures and the prevalence of textureless regions.
Recent advancements in 3D Gaussian Splatting have improved novel view synthesis
with accelerated processing but have yet to deliver comparable performance in
surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method
leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction.
Specifically, we employ a seed-guided mechanism to control the distribution of
2D Gaussians, with the density of seed points dynamically optimized through
adaptive growth and pruning mechanisms. To further improve geometric accuracy,
we incorporate monocular depth and normal priors to provide constraints for
details and textureless regions respectively. Additionally, multi-view
consistency constraints are employed to mitigate artifacts and further enhance
reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets
demonstrate that our method achieves state-of-the-art performance in indoor
scene reconstruction.



---

## Urban4D: Semantic-Guided 4D Gaussian Splatting for Urban Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Ziwen Li, Jiaxin Huang, Runnan Chen, Yunlong Che, Yandong Guo, Tongliang Liu, Fakhri Karray, Mingming Gong | cs.CV | [PDF](http://arxiv.org/pdf/2412.03473v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic urban scenes presents significant challenges due to
their intrinsic geometric structures and spatiotemporal dynamics. Existing
methods that attempt to model dynamic urban scenes without leveraging priors on
potentially moving regions often produce suboptimal results. Meanwhile,
approaches based on manual 3D annotations yield improved reconstruction quality
but are impractical due to labor-intensive labeling. In this paper, we revisit
the potential of 2D semantic maps for classifying dynamic and static Gaussians
and integrating spatial and temporal dimensions for urban scene representation.
We introduce Urban4D, a novel framework that employs a semantic-guided
decomposition strategy inspired by advances in deep 2D semantic map generation.
Our approach distinguishes potentially dynamic objects through reliable
semantic Gaussians. To explicitly model dynamic objects, we propose an
intuitive and effective 4D Gaussian splatting (4DGS) representation that
aggregates temporal information through learnable time embeddings for each
Gaussian, predicting their deformations at desired timestamps using a
multilayer perceptron (MLP). For more accurate static reconstruction, we also
design a k-nearest neighbor (KNN)-based consistency regularization to handle
the ground surface due to its low-texture characteristic. Extensive experiments
on real-world datasets demonstrate that Urban4D not only achieves comparable or
better quality than previous state-of-the-art methods but also effectively
captures dynamic objects while maintaining high visual fidelity for static
elements.



---

## Dense Scene Reconstruction from Light-Field Images Affected by Rolling  Shutter


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Hermes McGriff, Renato Martins, Nicolas Andreff, Cedric Demonceaux | cs.CV | [PDF](http://arxiv.org/pdf/2412.03518v1){: .btn .btn-green } |

**Abstract**: This paper presents a dense depth estimation approach from light-field (LF)
images that is able to compensate for strong rolling shutter (RS) effects. Our
method estimates RS compensated views and dense RS compensated disparity maps.
We present a two-stage method based on a 2D Gaussians Splatting that allows for
a ``render and compare" strategy with a point cloud formulation. In the first
stage, a subset of sub-aperture images is used to estimate an RS agnostic 3D
shape that is related to the scene target shape ``up to a motion". In the
second stage, the deformation of the 3D shape is computed by estimating an
admissible camera motion. We demonstrate the effectiveness and advantages of
this approach through several experiments conducted for different scenes and
types of motions. Due to lack of suitable datasets for evaluation, we also
present a new carefully designed synthetic dataset of RS LF images. The source
code, trained models and dataset will be made publicly available at:
https://github.com/ICB-Vision-AI/DenseRSLF



---

## Volumetrically Consistent 3D Gaussian Rasterization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Chinmay Talegaonkar, Yash Belhe, Ravi Ramamoorthi, Nicholas Antipa | cs.CV | [PDF](http://arxiv.org/pdf/2412.03378v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view
synthesis at high inference speeds. However, its splatting-based rendering
model makes several approximations to the rendering equation, reducing physical
accuracy. We show that splatting and its approximations are unnecessary, even
within a rasterizer; we instead volumetrically integrate 3D Gaussians directly
to compute the transmittance across them analytically. We use this analytic
transmittance to derive more physically-accurate alpha values than 3DGS, which
can directly be used within their framework. The result is a method that more
closely follows the volume rendering equation (similar to ray-tracing) while
enjoying the speed benefits of rasterization. Our method represents opaque
surfaces with higher accuracy and fewer points than 3DGS. This enables it to
outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being
volumetrically consistent also enables our method to work out of the box for
tomography. We match the state-of-the-art 3DGS-based tomography method with
fewer points. Being volumetrically consistent also enables our method to work
out of the box for tomography. We match the state-of-the-art 3DGS-based
tomography method with fewer points.



---

## Gaussian Object Carver: Object-Compositional Gaussian Splatting with  surfaces completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Liu Liu, Xinjie Wang, Jiaxiong Qiu, Tianwei Lin, Xiaolin Zhou, Zhizhong Su | cs.CV | [PDF](http://arxiv.org/pdf/2412.02075v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction is a foundational problem in computer vision. Despite
recent advancements in Neural Implicit Representations (NIR), existing methods
often lack editability and compositional flexibility, limiting their use in
scenarios requiring high interactivity and object-level manipulation. In this
paper, we introduce the Gaussian Object Carver (GOC), a novel, efficient, and
scalable framework for object-compositional 3D scene reconstruction. GOC
leverages 3D Gaussian Splatting (GS), enriched with monocular geometry priors
and multi-view geometry regularization, to achieve high-quality and flexible
reconstruction. Furthermore, we propose a zero-shot Object Surface Completion
(OSC) model, which uses 3D priors from 3d object data to reconstruct unobserved
surfaces, ensuring object completeness even in occluded areas. Experimental
results demonstrate that GOC improves reconstruction efficiency and geometric
fidelity. It holds promise for advancing the practical application of digital
twins in embodied AI, AR/VR, and interactive simulation environments.



---

## Multi-robot autonomous 3D reconstruction using Gaussian splatting with  Semantic guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Jing Zeng, Qi Ye, Tianle Liu, Yang Xu, Jin Li, Jinming Xu, Liang Li, Jiming Chen | cs.RO | [PDF](http://arxiv.org/pdf/2412.02249v1){: .btn .btn-green } |

**Abstract**: Implicit neural representations and 3D Gaussian splatting (3DGS) have shown
great potential for scene reconstruction. Recent studies have expanded their
applications in autonomous reconstruction through task assignment methods.
However, these methods are mainly limited to single robot, and rapid
reconstruction of large-scale scenes remains challenging. Additionally,
task-driven planning based on surface uncertainty is prone to being trapped in
local optima. To this end, we propose the first 3DGS-based centralized
multi-robot autonomous 3D reconstruction framework. To further reduce time cost
of task generation and improve reconstruction quality, we integrate online
open-vocabulary semantic segmentation with surface uncertainty of 3DGS,
focusing view sampling on regions with high instance uncertainty. Finally, we
develop a multi-robot collaboration strategy with mode and task assignments
improving reconstruction quality while ensuring planning efficiency. Our method
demonstrates the highest reconstruction quality among all planning methods and
superior planning efficiency compared to existing multi-robot methods. We
deploy our method on multiple robots, and results show that it can effectively
plan view paths and reconstruct scenes with high quality.



---

## AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent  Gaussian Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong | cs.CV | [PDF](http://arxiv.org/pdf/2412.02684v1){: .btn .btn-green } |

**Abstract**: Generating animatable human avatars from a single image is essential for
various digital human modeling applications. Existing 3D reconstruction methods
often struggle to capture fine details in animatable models, while generative
approaches for controllable animation, though avoiding explicit 3D modeling,
suffer from viewpoint inconsistencies in extreme poses and computational
inefficiencies. In this paper, we address these challenges by leveraging the
power of generative models to produce detailed multi-view canonical pose
images, which help resolve ambiguities in animatable human reconstruction. We
then propose a robust method for 3D reconstruction of inconsistent images,
enabling real-time rendering during inference. Specifically, we adapt a
transformer-based video generation model to generate multi-view canonical pose
images and normal maps, pretraining on a large-scale video dataset to improve
generalization. To handle view inconsistencies, we recast the reconstruction
problem as a 4D task and introduce an efficient 3D modeling approach using 4D
Gaussian Splatting. Experiments demonstrate that our method achieves
photorealistic, real-time animation of 3D human avatars from in-the-wild
images, showcasing its effectiveness and generalization capability.

Comments:
- Project Page: https://lingtengqiu.github.io/2024/AniGS/

---

## TimeWalker: Personalized Neural Space for Lifelong Head Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Dongwei Pan, Yang Li, Hongsheng Li, Kwan-Yee Lin | cs.CV | [PDF](http://arxiv.org/pdf/2412.02421v1){: .btn .btn-green } |

**Abstract**: We present TimeWalker, a novel framework that models realistic, full-scale 3D
head avatars of a person on lifelong scale. Unlike current human head avatar
pipelines that capture identity at the momentary level(e.g., instant
photography or short videos), TimeWalker constructs a person's comprehensive
identity from unstructured data collection over his/her various life stages,
offering a paradigm to achieve full reconstruction and animation of that person
at different moments of life. At the heart of TimeWalker's success is a novel
neural parametric model that learns personalized representation with the
disentanglement of shape, expression, and appearance across ages. Central to
our methodology are the concepts of two aspects: (1) We track back to the
principle of modeling a person's identity in an additive combination of average
head representation in the canonical space, and moment-specific head attribute
representations driven from a set of neural head basis. To learn the set of
head basis that could represent the comprehensive head variations in a compact
manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It
dynamically adjusts the number and blend weights of neural head bases,
according to both shared and specific traits of the target person over ages.
(2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian
splatting representation, to model head motion deformations like facial
expressions without losing the realism of rendering and reconstruction.
DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that
utilize the priors from parametric model, and move/rotate with the change of
expression. Through extensive experimental evaluations, we show TimeWalker's
ability to reconstruct and animate avatars across decoupled dimensions with
realistic rendering effects, demonstrating a way to achieve personalized 'time
traveling' in a breeze.

Comments:
- Project Page: https://timewalker2024.github.io/timewalker.github.io/
  , Video: https://www.youtube.com/watch?v=x8cpOVMY_ko

---

## RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex  Motions via Relay Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Qiankun Gao, Yanmin Wu, Chengxiang Wen, Jiarui Meng, Luyang Tang, Jie Chen, Ronggang Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.02493v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic scenes with large-scale and complex motions remains a
significant challenge. Recent techniques like Neural Radiance Fields and 3D
Gaussian Splatting (3DGS) have shown promise but still struggle with scenes
involving substantial movement. This paper proposes RelayGS, a novel method
based on 3DGS, specifically designed to represent and reconstruct highly
dynamic scenes. Our RelayGS learns a complete 4D representation with canonical
3D Gaussians and a compact motion field, consisting of three stages. First, we
learn a fundamental 3DGS from all frames, ignoring temporal scene variations,
and use a learnable mask to separate the highly dynamic foreground from the
minimally moving background. Second, we replicate multiple copies of the
decoupled foreground Gaussians from the first stage, each corresponding to a
temporal segment, and optimize them using pseudo-views constructed from
multiple frames within each segment. These Gaussians, termed Relay Gaussians,
act as explicit relay nodes, simplifying and breaking down large-scale motion
trajectories into smaller, manageable segments. Finally, we jointly learn the
scene's temporal motion and refine the canonical Gaussians learned from the
first two stages. We conduct thorough experiments on two dynamic scene datasets
featuring large and complex motions, where our RelayGS outperforms
state-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs
real-world basketball game scenes in a much more complete and coherent manner,
whereas previous methods usually struggle to capture the complex motion of
players. Code will be publicly available at https://github.com/gqk/RelayGS

Comments:
- Technical Report. GitHub: https://github.com/gqk/RelayGS

---

## SparseGrasp: Robotic Grasping via 3D Semantic Gaussian Splatting from  Sparse Multi-View RGB Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Junqiu Yu, Xinlin Ren, Yongchong Gu, Haitao Lin, Tianyu Wang, Yi Zhu, Hang Xu, Yu-Gang Jiang, Xiangyang Xue, Yanwei Fu | cs.RO | [PDF](http://arxiv.org/pdf/2412.02140v1){: .btn .btn-green } |

**Abstract**: Language-guided robotic grasping is a rapidly advancing field where robots
are instructed using human language to grasp specific objects. However,
existing methods often depend on dense camera views and struggle to quickly
update scenes, limiting their effectiveness in changeable environments.
  In contrast, we propose SparseGrasp, a novel open-vocabulary robotic grasping
system that operates efficiently with sparse-view RGB images and handles scene
updates fastly. Our system builds upon and significantly enhances existing
computer vision modules in robotic learning. Specifically, SparseGrasp utilizes
DUSt3R to generate a dense point cloud as the initialization for 3D Gaussian
Splatting (3DGS), maintaining high fidelity even under sparse supervision.
Importantly, SparseGrasp incorporates semantic awareness from recent vision
foundation models. To further improve processing efficiency, we repurpose
Principal Component Analysis (PCA) to compress features from 2D models.
Additionally, we introduce a novel render-and-compare strategy that ensures
rapid scene updates, enabling multi-turn grasping in changeable environments.
  Experimental results show that SparseGrasp significantly outperforms
state-of-the-art methods in terms of both speed and adaptability, providing a
robust solution for multi-turn grasping in changeable environment.



---

## Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D  Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Abdurrahman Zeybey, Mehmet Ergezer, Tommy Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2412.02803v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has advanced radiance field reconstruction, enabling
high-quality view synthesis and fast rendering in 3D modeling. While
adversarial attacks on object detection models are well-studied for 2D images,
their impact on 3D models remains underexplored. This work introduces the
Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate
adversarial noise targeting the CLIP vision-language model. M-IFGSM
specifically alters the object of interest by focusing perturbations on masked
regions, degrading the performance of CLIP's zero-shot object detection
capability when applied to 3D models. Using eight objects from the Common
Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces
the accuracy and confidence of the model, with adversarial noise being nearly
imperceptible to human observers. The top-1 accuracy in original model renders
drops from 95.4\% to 12.5\% for train images and from 91.2\% to 35.4\% for test
images, with confidence levels reflecting this shift from true classification
to misclassification, underscoring the risks of adversarial attacks on 3D
models in applications such as autonomous driving, robotics, and surveillance.
The significance of this research lies in its potential to expose
vulnerabilities in modern 3D vision models, including radiance fields,
prompting the development of more robust defenses and security measures in
critical real-world applications.

Comments:
- Accepted to Safe Generative AI Workshop @ NeurIPS 2024:
  https://neurips.cc/virtual/2024/workshop/84705

---

## How to Use Diffusion Priors under Sparse Views?

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Qisen Wang, Yifan Zhao, Jiawei Ma, Jia Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.02225v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis under sparse views has been a long-term important
challenge in 3D reconstruction. Existing works mainly rely on introducing
external semantic or depth priors to supervise the optimization of 3D
representations. However, the diffusion model, as an external prior that can
directly provide visual supervision, has always underperformed in sparse-view
3D reconstruction using Score Distillation Sampling (SDS) due to the low
information entropy of sparse views compared to text, leading to optimization
challenges caused by mode deviation. To this end, we present a thorough
analysis of SDS from the mode-seeking perspective and propose Inline Prior
Guided Score Matching (IPSM), which leverages visual inline priors provided by
pose relationships between viewpoints to rectify the rendered image
distribution and decomposes the original optimization objective of SDS, thereby
offering effective diffusion visual guidance without any fine-tuning or
pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts
3D Gaussian Splatting as the backbone and supplements depth and geometry
consistency regularization based on IPSM to further improve inline priors and
rectified distribution. Experimental results on different public datasets show
that our method achieves state-of-the-art reconstruction quality. The code is
released at https://github.com/iCVTEAM/IPSM.



---

## GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Zhiyuan Chen, Fan Lu, Guo Yu, Bin Li, Sanqing Qu, Yuan Huang, Changhong Fu, Guang Chen | cs.CV | [PDF](http://arxiv.org/pdf/2412.02267v1){: .btn .btn-green } |

**Abstract**: Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is
crucial for robotic manipulation. However, existing approaches typically rely
on accurate depth information, which is non-trivial to obtain in real-world
scenarios. Although depth estimation algorithms can be employed, geometric
inaccuracy can lead to failures in RGBD-based pose tracking methods. To address
this challenge, we introduce GSGTrack, a novel RGB-based pose tracking
framework that jointly optimizes geometry and pose. Specifically, we adopt 3D
Gaussian Splatting to create an optimizable 3D representation, which is learned
simultaneously with a graph-based geometry optimization to capture the object's
appearance features and refine its geometry. However, the joint optimization
process is susceptible to perturbations from noisy pose and geometry data.
Thus, we propose an object silhouette loss to address the issue of pixel-wise
loss being overly sensitive to pose noise during tracking. To mitigate the
geometric ambiguities caused by inaccurate depth information, we propose a
geometry-consistent image pair selection strategy, which filters out
low-confidence pairs and ensures robust geometric optimization. Extensive
experiments on the OnePose and HO3D datasets demonstrate the effectiveness of
GSGTrack in both 6DoF pose tracking and object reconstruction.



---

## SparseLGS: Sparse View Language Embedded Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Jun Hu, Zhang Chen, Zhong Li, Yi Xu, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.02245v2){: .btn .btn-green } |

**Abstract**: Recently, several studies have combined Gaussian Splatting to obtain scene
representations with language embeddings for open-vocabulary 3D scene
understanding. While these methods perform well, they essentially require very
dense multi-view inputs, limiting their applicability in real-world scenarios.
In this work, we propose SparseLGS to address the challenge of 3D scene
understanding with pose-free and sparse view input images. Our method leverages
a learning-based dense stereo model to handle pose-free and sparse inputs, and
a three-step region matching approach to address the multi-view semantic
inconsistency problem, which is especially important for sparse inputs.
Different from directly learning high-dimensional CLIP features, we extract
low-dimensional information and build bijections to avoid excessive learning
and storage costs. We introduce a reconstruction loss during semantic training
to improve Gaussian positions and shapes. To the best of our knowledge, we are
the first to address the 3D semantic field problem with sparse pose-free
inputs. Experimental results show that SparseLGS achieves comparable quality
when reconstructing semantic fields with fewer inputs (3-4 views) compared to
previous SOTA methods with dense input. Besides, when using the same sparse
input, SparseLGS leads significantly in quality and heavily improves the
computation speed (5$\times$speedup). Project page:
https://ustc3dv.github.io/SparseLGS

Comments:
- Project Page: https://ustc3dv.github.io/SparseLGS

---

## HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Hongyu Zhou, Longzhong Lin, Jiabao Wang, Yichong Lu, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01718v1){: .btn .btn-green } |

**Abstract**: In the past few decades, autonomous driving algorithms have made significant
progress in perception, planning, and control. However, evaluating individual
components does not fully reflect the performance of entire systems,
highlighting the need for more holistic assessment methods. This motivates the
development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator
for evaluating autonomous driving algorithms. We achieve this by lifting
captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving
the rendering quality for closed-loop scenarios, and building the closed-loop
environment. In terms of rendering, We tackle challenges of novel view
synthesis in closed-loop scenarios, including viewpoint extrapolation and
360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further
enables the full closed simulation loop, dynamically updating the ego and actor
states and observations based on control commands. Moreover, HUGSIM offers a
comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo,
nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair
and realistic evaluation platform for existing autonomous driving algorithms.
HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks
the potential for fine-tuning autonomous driving algorithms in a photorealistic
closed-loop setting.

Comments:
- Our project page is at https://xdimlab.github.io/HUGSIM

---

## 6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Yufeng Jin, Vignesh Prasad, Snehal Jauhri, Mathias Franzius, Georgia Chalvatzaki | cs.CV | [PDF](http://arxiv.org/pdf/2412.01543v1){: .btn .btn-green } |

**Abstract**: Efficient and accurate object pose estimation is an essential component for
modern vision systems in many applications such as Augmented Reality,
autonomous driving, and robotics. While research in model-based 6D object pose
estimation has delivered promising results, model-free methods are hindered by
the high computational load in rendering and inferring consistent poses of
arbitrary objects in a live RGB-D video stream. To address this issue, we
present 6DOPE-GS, a novel method for online 6D object pose estimation \&
tracking with a single RGB-D camera by effectively leveraging advances in
Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of
Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses
and 3D object reconstruction. To achieve the necessary efficiency and accuracy
for live tracking, our method uses incremental 2D Gaussian Splatting with an
intelligent dynamic keyframe selection procedure to achieve high spatial object
coverage and prevent erroneous pose updates. We also propose an opacity
statistic-based pruning mechanism for adaptive Gaussian density control, to
ensure training stability and efficiency. We evaluate our method on the HO3D
and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of
state-of-the-art baselines for model-free simultaneous 6D pose tracking and
reconstruction while providing a 5$\times$ speedup. We also demonstrate the
method's suitability for live, dynamic object tracking and reconstruction in a
real-world setting.



---

## Occam's LGS: A Simple Approach for Language Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Jiahuan Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2412.01807v1){: .btn .btn-green } |

**Abstract**: TL;DR: Gaussian Splatting is a widely adopted approach for 3D scene
representation that offers efficient, high-quality 3D reconstruction and
rendering. A major reason for the success of 3DGS is its simplicity of
representing a scene with a set of Gaussians, which makes it easy to interpret
and adapt. To enhance scene understanding beyond the visual representation,
approaches have been developed that extend 3D Gaussian Splatting with semantic
vision-language features, especially allowing for open-set tasks. In this
setting, the language features of 3D Gaussian Splatting are often aggregated
from multiple 2D views. Existing works address this aggregation problem using
cumbersome techniques that lead to high computational cost and training time.
  In this work, we show that the sophisticated techniques for language-grounded
3D Gaussian Splatting are simply unnecessary. Instead, we apply Occam's razor
to the task at hand and perform weighted multi-view feature aggregation using
the weights derived from the standard rendering process, followed by a simple
heuristic-based noisy Gaussian filtration. Doing so offers us state-of-the-art
results with a speed-up of two orders of magnitude. We showcase our results in
two commonly used benchmark datasets: LERF and 3D-OVS. Our simple approach
allows us to perform reasoning directly in the language features, without any
compression whatsoever. Such modeling in turn offers easy scene manipulation,
unlike the existing methods -- which we illustrate using an application of
object insertion in the scene. Furthermore, we provide a thorough discussion
regarding the significance of our contributions within the context of the
current literature. Project Page: https://insait-institute.github.io/OccamLGS/

Comments:
- Project Page: https://insait-institute.github.io/OccamLGS/

---

## Driving Scene Synthesis on Free-form Trajectories with Generative Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zeyu Yang, Zijie Pan, Yuankun Yang, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.01717v1){: .btn .btn-green } |

**Abstract**: Driving scene synthesis along free-form trajectories is essential for driving
simulations to enable closed-loop evaluation of end-to-end driving policies.
While existing methods excel at novel view synthesis on recorded trajectories,
they face challenges with novel trajectories due to limited views of driving
videos and the vastness of driving environments. To tackle this challenge, we
propose a novel free-form driving view synthesis approach, dubbed DriveX, by
leveraging video generative prior to optimize a 3D model across a variety of
trajectories. Concretely, we crafted an inverse problem that enables a video
diffusion model to be utilized as a prior for many-trajectory optimization of a
parametric 3D model (e.g., Gaussian splatting). To seamlessly use the
generative prior, we iteratively conduct this process during optimization. Our
resulting model can produce high-fidelity virtual driving environments outside
the recorded trajectory, enabling free-form trajectory driving simulation.
Beyond real driving scenes, DriveX can also be utilized to simulate virtual
driving worlds from AI-generated videos.



---

## GFreeDet: Exploiting Gaussian Splatting and Foundation Models for  Model-free Unseen Object Detection in the BOP Challenge 2024

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Xingyu Liu, Yingyue Li, Chengxi Li, Gu Wang, Chenyangguang Zhang, Ziqin Huang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2412.01552v2){: .btn .btn-green } |

**Abstract**: In this report, we provide the technical details of the submitted method
GFreeDet, which exploits Gaussian splatting and vision Foundation models for
the model-free unseen object Detection track in the BOP 2024 Challenge.



---

## Planar Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Farhad G. Zanjani, Hong Cai, Hanno Ackermann, Leila Mirvakhabova, Fatih Porikli | cs.CV | [PDF](http://arxiv.org/pdf/2412.01931v1){: .btn .btn-green } |

**Abstract**: This paper presents Planar Gaussian Splatting (PGS), a novel neural rendering
approach to learn the 3D geometry and parse the 3D planes of a scene, directly
from multiple RGB images. The PGS leverages Gaussian primitives to model the
scene and employ a hierarchical Gaussian mixture approach to group them.
Similar Gaussians are progressively merged probabilistically in the
tree-structured Gaussian mixtures to identify distinct 3D plane instances and
form the overall 3D scene geometry. In order to enable the grouping, the
Gaussian primitives contain additional parameters, such as plane descriptors
derived by lifting 2D masks from a general 2D segmentation model and surface
normals. Experiments show that the proposed PGS achieves state-of-the-art
performance in 3D planar reconstruction without requiring either 3D plane
labels or depth supervision. In contrast to existing supervised methods that
have limited generalizability and struggle under domain shift, PGS maintains
its performance across datasets thanks to its neural rendering and
scene-specific optimization mechanism, while also being significantly faster
than existing optimization-based approaches.

Comments:
- IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),
  2025

---

## Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale  Aerial-to-Ground Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Lihan Jiang, Kerui Ren, Mulin Yu, Linning Xu, Junting Dong, Tao Lu, Feng Zhao, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2412.01745v1){: .btn .btn-green } |

**Abstract**: Seamless integration of both aerial and street view images remains a
significant challenge in neural scene reconstruction and rendering. Existing
methods predominantly focus on single domain, limiting their applications in
immersive environments, which demand extensive free view exploration with large
view changes both horizontally and vertically. We introduce Horizon-GS, a novel
approach built upon Gaussian Splatting techniques, tackles the unified
reconstruction and rendering for aerial and street views. Our method addresses
the key challenges of combining these perspectives with a new training
strategy, overcoming viewpoint discrepancies to generate high-fidelity scenes.
We also curate a high-quality aerial-to-ground views dataset encompassing both
synthetic and real-world scene to advance further research. Experiments across
diverse urban scene datasets confirm the effectiveness of our method.



---

## HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Yunzhou Song, Heguang Lin, Jiahui Lei, Lingjie Liu, Kostas Daniilidis | cs.CV | [PDF](http://arxiv.org/pdf/2412.01823v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural rendering, particularly 2D Gaussian Splatting
(2DGS), have shown promising results for jointly reconstructing fine appearance
and geometry by leveraging 2D Gaussian surfels. However, current methods face
significant challenges when rendering at arbitrary viewpoints, such as
anti-aliasing for down-sampled rendering, and texture detail preservation for
high-resolution rendering. We proposed a novel method to align the 2D surfels
with texture maps and augment it with per-ray depth sorting and fisher-based
pruning for rendering consistency and efficiency. With correct order,
per-surfel texture maps significantly improve the capabilities to capture fine
details. Additionally, to render high-fidelity details in varying viewpoints,
we designed a frustum-based sampling method to mitigate the aliasing artifacts.
Experimental results on benchmarks and our custom texture-rich dataset
demonstrate that our method surpasses existing techniques, particularly in
detail preservation and anti-aliasing.

Comments:
- Project Page: https://timsong412.github.io/HDGS-ProjPage/

---

## CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Kai He, Chin-Hsuan Wu, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2412.01792v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D representations, such as Neural Radiance Fields and 3D
Gaussian Splatting, have greatly improved realistic scene modeling and
novel-view synthesis. However, achieving controllable and consistent editing in
dynamic 3D scenes remains a significant challenge. Previous work is largely
constrained by its editing backbones, resulting in inconsistent edits and
limited controllability. In our work, we introduce a novel framework that first
fine-tunes the InstructPix2Pix model, followed by a two-stage optimization of
the scene based on deformable 3D Gaussians. Our fine-tuning enables the model
to "learn" the editing ability from a single edited reference image,
transforming the complex task of dynamic scene editing into a simple 2D image
editing process. By directly learning editing regions and styles from the
reference, our approach enables consistent and precise local edits without the
need for tracking desired editing regions, effectively addressing key
challenges in dynamic scene editing. Then, our two-stage optimization
progressively edits the trained dynamic scene, using a designed edited image
buffer to accelerate convergence and improve temporal consistency. Compared to
state-of-the-art methods, our approach offers more flexible and controllable
local scene editing, achieving high-quality and consistent results.

Comments:
- Project page: https://ihe-kaii.github.io/CTRL-D/

---

## Diffusion Models with Anisotropic Gaussian Splatting for Image  Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Jacob Fein-Ashley, Benjamin Fein-Ashley | cs.CV | [PDF](http://arxiv.org/pdf/2412.01682v2){: .btn .btn-green } |

**Abstract**: Image inpainting is a fundamental task in computer vision, aiming to restore
missing or corrupted regions in images realistically. While recent deep
learning approaches have significantly advanced the state-of-the-art,
challenges remain in maintaining structural continuity and generating coherent
textures, particularly in large missing areas. Diffusion models have shown
promise in generating high-fidelity images but often lack the structural
guidance necessary for realistic inpainting. We propose a novel inpainting
method that combines diffusion models with anisotropic Gaussian splatting to
capture both local structures and global context effectively. By modeling
missing regions using anisotropic Gaussian functions that adapt to local image
gradients, our approach provides structural guidance to the diffusion-based
inpainting network. The Gaussian splat maps are integrated into the diffusion
process, enhancing the model's ability to generate high-fidelity and
structurally coherent inpainting results. Extensive experiments demonstrate
that our method outperforms state-of-the-art techniques, producing visually
plausible results with enhanced structural integrity and texture realism.



---

## 3DSceneEditor: Controllable 3D Scene Editing with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Ziyang Yan, Lei Li, Yihua Shao, Siyu Chen, Wuzong Kai, Jenq-Neng Hwang, Hao Zhao, Fabio Remondino | cs.CV | [PDF](http://arxiv.org/pdf/2412.01583v1){: .btn .btn-green } |

**Abstract**: The creation of 3D scenes has traditionally been both labor-intensive and
costly, requiring designers to meticulously configure 3D assets and
environments. Recent advancements in generative AI, including text-to-3D and
image-to-3D methods, have dramatically reduced the complexity and cost of this
process. However, current techniques for editing complex 3D scenes continue to
rely on generally interactive multi-step, 2D-to-3D projection methods and
diffusion-based techniques, which often lack precision in control and hamper
real-time performance. In this work, we propose 3DSceneEditor, a fully 3D-based
paradigm for real-time, precise editing of intricate 3D scenes using Gaussian
Splatting. Unlike conventional methods, 3DSceneEditor operates through a
streamlined 3D pipeline, enabling direct manipulation of Gaussians for
efficient, high-quality edits based on input prompts.The proposed framework (i)
integrates a pre-trained instance segmentation model for semantic labeling;
(ii) employs a zero-shot grounding approach with CLIP to align target objects
with user prompts; and (iii) applies scene modifications, such as object
addition, repositioning, recoloring, replacing, and deletion directly on
Gaussians. Extensive experimental results show that 3DSceneEditor achieves
superior editing precision and speed with respect to current SOTA 3D scene
editing approaches, establishing a new benchmark for efficient and interactive
3D scene customization.

Comments:
- Project Page: https://ziyangyan.github.io/3DSceneEditor

---

## RGBDS-SLAM: A RGB-D Semantic Dense SLAM Based on 3D Multi Level Pyramid  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zhenzhong Cao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01217v1){: .btn .btn-green } |

**Abstract**: High-quality reconstruction is crucial for dense SLAM. Recent popular
approaches utilize 3D Gaussian Splatting (3D GS) techniques for RGB, depth, and
semantic reconstruction of scenes. However, these methods often overlook issues
of detail and consistency in different parts of the scene. To address this, we
propose RGBDS-SLAM, a RGB-D semantic dense SLAM system based on 3D multi-level
pyramid gaussian splatting, which enables high-quality dense reconstruction of
scene RGB, depth, and semantics.In this system, we introduce a 3D multi-level
pyramid gaussian splatting method that restores scene details by extracting
multi-level image pyramids for gaussian splatting training, ensuring
consistency in RGB, depth, and semantic reconstructions. Additionally, we
design a tightly-coupled multi-features reconstruction optimization mechanism,
allowing the reconstruction accuracy of RGB, depth, and semantic maps to
mutually enhance each other during the rendering optimization process.
Extensive quantitative, qualitative, and ablation experiments on the Replica
and ScanNet public datasets demonstrate that our proposed method outperforms
current state-of-the-art methods. The open-source code will be available at:
https://github.com/zhenzhongcao/RGBDS-SLAM.



---

## SfM-Free 3D Gaussian Splatting via Hierarchical Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Bo Ji, Angela Yao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01553v1){: .btn .btn-green } |

**Abstract**: Standard 3D Gaussian Splatting (3DGS) relies on known or pre-computed camera
poses and a sparse point cloud, obtained from structure-from-motion (SfM)
preprocessing, to initialize and grow 3D Gaussians. We propose a novel SfM-Free
3DGS (SFGS) method for video input, eliminating the need for known camera poses
and SfM preprocessing. Our approach introduces a hierarchical training strategy
that trains and merges multiple 3D Gaussian representations -- each optimized
for specific scene regions -- into a single, unified 3DGS model representing
the entire scene. To compensate for large camera motions, we leverage video
frame interpolation models. Additionally, we incorporate multi-source
supervision to reduce overfitting and enhance representation. Experimental
results reveal that our approach significantly surpasses state-of-the-art
SfM-free novel view synthesis methods. On the Tanks and Temples dataset, we
improve PSNR by an average of 2.25dB, with a maximum gain of 3.72dB in the best
scene. On the CO3D-V2 dataset, we achieve an average PSNR boost of 1.74dB, with
a top gain of 3.90dB. The code is available at
https://github.com/jibo27/3DGS_Hierarchical_Training.



---

## ULSR-GS: Ultra Large-scale Surface Reconstruction Gaussian Splatting  with Multi-View Geometric Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zhuoxiao Li, Shanliang Yao, Qizhong Gao, Angel F. Garcia-Fernandez, Yong Yue, Xiaohui Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2412.01402v1){: .btn .btn-green } |

**Abstract**: While Gaussian Splatting (GS) demonstrates efficient and high-quality scene
rendering and small area surface extraction ability, it falls short in handling
large-scale aerial image surface extraction tasks. To overcome this, we present
ULSR-GS, a framework dedicated to high-fidelity surface extraction in
ultra-large-scale scenes, addressing the limitations of existing GS-based mesh
extraction methods. Specifically, we propose a point-to-photo partitioning
approach combined with a multi-view optimal view matching principle to select
the best training images for each sub-region. Additionally, during training,
ULSR-GS employs a densification strategy based on multi-view geometric
consistency to enhance surface extraction details. Experimental results
demonstrate that ULSR-GS outperforms other state-of-the-art GS-based works on
large-scale aerial photogrammetry benchmark datasets, significantly improving
surface extraction accuracy in complex urban environments. Project page:
https://ulsrgs.github.io.

Comments:
- Project page: https://ulsrgs.github.io

---

## Ref-GS: Directional Factorization for 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Youjia Zhang, Anpei Chen, Yumin Wan, Zikai Song, Junqing Yu, Yawei Luo, Wei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2412.00905v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Ref-GS, a novel approach for directional light
factorization in 2D Gaussian splatting, which enables photorealistic
view-dependent appearance rendering and precise geometry recovery. Ref-GS
builds upon the deferred rendering of Gaussian splatting and applies
directional encoding to the deferred-rendered surface, effectively reducing the
ambiguity between orientation and viewing angle. Next, we introduce a spherical
Mip-grid to capture varying levels of surface roughness, enabling
roughness-aware Gaussian shading. Additionally, we propose a simple yet
efficient geometry-lighting factorization that connects geometry and lighting
via the vector outer product, significantly reducing renderer overhead when
integrating volumetric attributes. Our method achieves superior photorealistic
rendering for a range of open-world scenes while also accurately recovering
geometry.

Comments:
- Project page: https://ref-gs.github.io/

---

## DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.00851v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting have shown promising results.
Existing methods typically assume static scenes and/or multiple images with
prior poses. Dynamics, sparse views, and unknown poses significantly increase
the problem complexity due to insufficient geometric constraints. To overcome
this challenge, we propose a method that can use only two images without prior
poses to fit Gaussians in dynamic environments. To achieve this, we introduce
two technical contributions. First, we propose an object-level two-view bundle
adjustment. This strategy decomposes dynamic scenes into piece-wise rigid
components, and jointly estimates the camera pose and motions of dynamic
objects. Second, we design an SE(3) field-driven Gaussian training method. It
enables fine-grained motion modeling through learnable per-Gaussian
transformations. Our method leads to high-fidelity novel view synthesis of
dynamic scenes while accurately preserving temporal consistency and object
motion. Experiments on both synthetic and real-world datasets demonstrate that
our method significantly outperforms state-of-the-art approaches designed for
the cases of static environments, multiple images, and/or known poses. Our
project page is available at https://colin-de.github.io/DynSUP/.



---

## ChatSplat: 3D Conversational Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Hanlin Chen, Fangyin Wei, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2412.00734v1){: .btn .btn-green } |

**Abstract**: Humans naturally interact with their 3D surroundings using language, and
modeling 3D language fields for scene understanding and interaction has gained
growing interest. This paper introduces ChatSplat, a system that constructs a
3D language field, enabling rich chat-based interaction within 3D space. Unlike
existing methods that primarily use CLIP-derived language features focused
solely on segmentation, ChatSplat facilitates interaction on three levels:
objects, views, and the entire 3D scene. For view-level interaction, we
designed an encoder that encodes the rendered feature map of each view into
tokens, which are then processed by a large language model (LLM) for
conversation. At the scene level, ChatSplat combines multi-view tokens,
enabling interactions that consider the entire scene. For object-level
interaction, ChatSplat uses a patch-wise language embedding, unlike LangSplat's
pixel-wise language embedding that implicitly includes mask and embedding.
Here, we explicitly decouple the language embedding into separate mask and
feature map representations, allowing more flexible object-level interaction.
To address the challenge of learning 3D Gaussians posed by the complex and
diverse distribution of language embeddings used in the LLM, we introduce a
learnable normalization technique to standardize these embeddings, facilitating
effective learning. Extensive experimental results demonstrate that ChatSplat
supports multi-level interactions -- object, view, and scene -- within 3D
space, enhancing both understanding and engagement.



---

## FlashSLAM: Accelerated RGB-D SLAM for Real-Time 3D Scene Reconstruction  with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Phu Pham, Damon Conover, Aniket Bera | cs.CV | [PDF](http://arxiv.org/pdf/2412.00682v1){: .btn .btn-green } |

**Abstract**: We present FlashSLAM, a novel SLAM approach that leverages 3D Gaussian
Splatting for efficient and robust 3D scene reconstruction. Existing 3DGS-based
SLAM methods often fall short in sparse view settings and during large camera
movements due to their reliance on gradient descent-based optimization, which
is both slow and inaccurate. FlashSLAM addresses these limitations by combining
3DGS with a fast vision-based camera tracking technique, utilizing a pretrained
feature matching model and point cloud registration for precise pose estimation
in under 80 ms - a 90% reduction in tracking time compared to SplaTAM - without
costly iterative rendering. In sparse settings, our method achieves up to a 92%
improvement in average tracking accuracy over previous methods. Additionally,
it accounts for noise in depth sensors, enhancing robustness when using
unspecialized devices such as smartphones. Extensive experiments show that
FlashSLAM performs reliably across both sparse and dense settings, in synthetic
and real-world environments. Evaluations on benchmark datasets highlight its
superior accuracy and efficiency, establishing FlashSLAM as a versatile and
high-performance solution for SLAM, advancing the state-of-the-art in 3D
reconstruction across diverse applications.

Comments:
- 16 pages, 9 figures, 13 tables

---

## SAGA: Surface-Aligned Gaussian Avatar

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Ronghan Chen, Yang Cong, Jiayue Liu | cs.CV | [PDF](http://arxiv.org/pdf/2412.00845v1){: .btn .btn-green } |

**Abstract**: This paper presents a Surface-Aligned Gaussian representation for creating
animatable human avatars from monocular videos,aiming at improving the novel
view and pose synthesis performance while ensuring fast training and real-time
rendering. Recently,3DGS has emerged as a more efficient and expressive
alternative to NeRF, and has been used for creating dynamic human avatars.
However,when applied to the severely ill-posed task of monocular dynamic
reconstruction, the Gaussians tend to overfit the constantly changing regions
such as clothes wrinkles or shadows since these regions cannot provide
consistent supervision, resulting in noisy geometry and abrupt deformation that
typically fail to generalize under novel views and poses.To address these
limitations, we present SAGA,i.e.,Surface-Aligned Gaussian Avatar,which aligns
the Gaussians with a mesh to enforce well-defined geometry and consistent
deformation, thereby improving generalization under novel views and poses.
Unlike existing strict alignment methods that suffer from limited expressive
power and low realism,SAGA employs a two-stage alignment strategy where the
Gaussians are first adhered on while then detached from the mesh, thus
facilitating both good geometry and high expressivity. In the Adhered Stage, we
improve the flexibility of Adhered-on-Mesh Gaussians by allowing them to flow
on the mesh, in contrast to existing methods that rigidly bind Gaussians to
fixed location. In the second Detached Stage, we introduce a Gaussian-Mesh
Alignment regularization, which allows us to unleash the expressivity by
detaching the Gaussians but maintain the geometric alignment by minimizing
their location and orientation offsets from the bound triangles. Finally, since
the Gaussians may drift outside the bound triangles during optimization, an
efficient Walking-on-Mesh strategy is proposed to dynamically update the bound
triangles.

Comments:
- Submitted to TPAMI. Major Revision. Project page:
  https://gostinshell.github.io/SAGA/

---

## CtrlNeRF: The Generative Neural Radiation Fields for the Controllable  Synthesis of High-fidelity 3D-Aware Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Jian Liu, Zhen Yu | cs.CV | [PDF](http://arxiv.org/pdf/2412.00754v1){: .btn .btn-green } |

**Abstract**: The neural radiance field (NERF) advocates learning the continuous
representation of 3D geometry through a multilayer perceptron (MLP). By
integrating this into a generative model, the generative neural radiance field
(GRAF) is capable of producing images from random noise z without 3D
supervision. In practice, the shape and appearance are modeled by z_s and z_a,
respectively, to manipulate them separately during inference. However, it is
challenging to represent multiple scenes using a solitary MLP and precisely
control the generation of 3D geometry in terms of shape and appearance. In this
paper, we introduce a controllable generative model (i.e. \textbf{CtrlNeRF})
that uses a single MLP network to represent multiple scenes with shared
weights. Consequently, we manipulated the shape and appearance codes to realize
the controllable generation of high-fidelity images with 3D consistency.
Moreover, the model enables the synthesis of novel views that do not exist in
the training sets via camera pose alteration and feature interpolation.
Extensive experiments were conducted to demonstrate its superiority in 3D-aware
image generation compared to its counterparts.



---

## VR-Doh: Hands-on 3D Modeling in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Zhaofeng Luo, Zhitong Cui, Shijian Luo, Mengyu Chu, Minchen Li | cs.GR | [PDF](http://arxiv.org/pdf/2412.00814v1){: .btn .btn-green } |

**Abstract**: We present VR-Doh, a hands-on 3D modeling system designed for creating and
manipulating elastoplastic objects in virtual reality (VR). The system employs
the Material Point Method (MPM) for simulating realistic large deformations and
incorporates optimized Gaussian Splatting for seamless rendering. With direct,
hand-based interactions, users can naturally sculpt, deform, and edit objects
interactively. To achieve real-time performance, we developed localized
simulation techniques, optimized collision handling, and separated appearance
and physical representations, ensuring smooth and responsive user interaction.
The system supports both freeform creation and precise adjustments, catering to
diverse modeling tasks. A user study involving novice and experienced users
highlights the system's intuitive design, immersive feedback, and creative
potential. Compared to traditional geometry-based modeling tools, our approach
offers improved accessibility and natural interaction in specific contexts.


