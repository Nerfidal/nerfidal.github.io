---
layout: default
title: December 2024
parent: Papers
nav_order: 202412
---

<!---metadata--->


## Turbo3D: Ultra-fast Text-to-3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Hanzhe Hu, Tianwei Yin, Fujun Luan, Yiwei Hu, Hao Tan, Zexiang Xu, Sai Bi, Shubham Tulsiani, Kai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04470v1){: .btn .btn-green } |

**Abstract**: We present Turbo3D, an ultra-fast text-to-3D system capable of generating
high-quality Gaussian splatting assets in under one second. Turbo3D employs a
rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian
reconstructor, both operating in latent space. The 4-step, 4-view generator is
a student model distilled through a novel Dual-Teacher approach, which
encourages the student to learn view consistency from a multi-view teacher and
photo-realism from a single-view teacher. By shifting the Gaussian
reconstructor's inputs from pixel space to latent space, we eliminate the extra
image decoding time and halve the transformer sequence length for maximum
efficiency. Our method demonstrates superior 3D generation results compared to
previous baselines, while operating in a fraction of their runtime.

Comments:
- project page: https://turbo-3d.github.io/

---

## DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for  Monocular Dynamic 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Xuesong Li, Jinguang Tong, Jie Hong, Vivien Rolland, Lars Petersson | cs.CV | [PDF](http://arxiv.org/pdf/2412.03910v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction from monocular video is critical for real-world
applications. This paper tackles the dual challenges of dynamic novel-view
synthesis and 3D geometry reconstruction by introducing a hybrid framework:
Deformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both
modules can leverage each other for both tasks. During training, depth maps
generated by the deformable Gaussian splatting module guide the ray sampling
for faster processing and provide depth supervision within the dynamic neural
surface module to improve geometry reconstruction. Simultaneously, the dynamic
neural surface directs the distribution of Gaussian primitives around the
surface, enhancing rendering quality. To further refine depth supervision, we
introduce a depth-filtering process on depth maps derived from Gaussian
rasterization. Extensive experiments on public datasets demonstrate that DGNS
achieves state-of-the-art performance in both novel-view synthesis and 3D
reconstruction.



---

## QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming  Free-viewpoint Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Sharath Girish, Tianye Li, Amrita Mazumdar, Abhinav Shrivastava, David Luebke, Shalini De Mello | cs.CV | [PDF](http://arxiv.org/pdf/2412.04469v1){: .btn .btn-green } |

**Abstract**: Online free-viewpoint video (FVV) streaming is a challenging problem, which
is relatively under-explored. It requires incremental on-the-fly updates to a
volumetric representation, fast training and rendering to satisfy real-time
constraints and a small memory footprint for efficient transmission. If
achieved, it can enhance user experience by enabling novel applications, e.g.,
3D video conferencing and live volumetric video broadcast, among others. In
this work, we propose a novel framework for QUantized and Efficient ENcoding
(QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly
learns Gaussian attribute residuals between consecutive frames at each
time-step without imposing any structural constraints on them, allowing for
high quality reconstruction and generalizability. To efficiently store the
residuals, we further propose a quantization-sparsity framework, which contains
a learned latent-decoder for effectively quantizing attribute residuals other
than Gaussian positions and a learned gating module to sparsify position
residuals. We propose to use the Gaussian viewspace gradient difference vector
as a signal to separate the static and dynamic content of the scene. It acts as
a guide for effective sparsity learning and speeds up training. On diverse FVV
benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all
metrics. Notably, for several highly dynamic scenes, it reduces the model size
to just 0.7 MB per frame while training in under 5 sec and rendering at 350
FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen

Comments:
- Accepted at NeurIPS 2024, Project website:
  https://research.nvidia.com/labs/amri/projects/queen

---

## HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, Jieping Ye | cs.CV | [PDF](http://arxiv.org/pdf/2412.03844v1){: .btn .btn-green } |

**Abstract**: Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS)
in scenes featuring transient objects is challenging. We propose a novel hybrid
representation, termed as HybridGS, using 2D Gaussians for transient objects
per image and maintaining traditional 3D Gaussians for the whole static scenes.
Note that, the 3DGS itself is better suited for modeling static scenes that
assume multi-view consistency, but the transient objects appear occasionally
and do not adhere to the assumption, thus we model them as planar objects from
a single view, represented with 2D Gaussians. Our novel representation
decomposes the scene from the perspective of fundamental viewpoint consistency,
making it more reasonable. Additionally, we present a novel multi-view
regulated supervision method for 3DGS that leverages information from
co-visible regions, further enhancing the distinctions between the transients
and statics. Then, we propose a straightforward yet effective multi-stage
training strategy to ensure robust training and high-quality view synthesis
across various settings. Experiments on benchmark datasets show our
state-of-the-art performance of novel view synthesis in both indoor and outdoor
scenes, even in the presence of distracting elements.

Comments:
- Project page: https://gujiaqivadin.github.io/hybridgs/

---

## Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2412.04459v1){: .btn .btn-green } |

**Abstract**: We propose an efficient radiance field rendering algorithm that incorporates
a rasterization process on sparse voxels without neural networks or 3D
Gaussians. There are two key contributions coupled with the proposed system.
The first is to render sparse voxels in the correct depth order along pixel
rays by using dynamic Morton ordering. This avoids the well-known popping
artifact found in Gaussian splatting. Second, we adaptively fit sparse voxels
to different levels of detail within scenes, faithfully reproducing scene
details while achieving high rendering frame rates. Our method improves the
previous neural-free voxel grid representation by over 4db PSNR and more than
10x rendering FPS speedup, achieving state-of-the-art comparable novel-view
synthesis results. Additionally, our neural-free sparse voxels are seamlessly
compatible with grid-based 3D processing algorithms. We achieve promising mesh
reconstruction accuracy by integrating TSDF-Fusion and Marching Cubes into our
sparse grid system.

Comments:
- Code release in progress

---

## Multi-View Pose-Agnostic Change Localization with Zero Labels

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Chamuditha Jayanga Galappaththige, Jason Lai, Lloyd Windrim, Donald Dansereau, Niko Suenderhauf, Dimity Miller | cs.CV | [PDF](http://arxiv.org/pdf/2412.03911v1){: .btn .btn-green } |

**Abstract**: Autonomous agents often require accurate methods for detecting and localizing
changes in their environment, particularly when observations are captured from
unconstrained and inconsistent viewpoints. We propose a novel label-free,
pose-agnostic change detection method that integrates information from multiple
viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS)
representation of the scene. With as few as 5 images of the post-change scene,
our approach can learn additional change channels in a 3DGS and produce change
masks that outperform single-view techniques. Our change-aware 3D scene
representation additionally enables the generation of accurate change masks for
unseen viewpoints. Experimental results demonstrate state-of-the-art
performance in complex multi-object scenes, achieving a 1.7$\times$ and
1.6$\times$ improvement in Mean Intersection Over Union and F1 score
respectively over other baselines. We also contribute a new real-world dataset
to benchmark change detection in diverse challenging scenes in the presence of
lighting variations.



---

## Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth  Motion Helps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Yiqing Liang, Mikhail Okunev, Mikaela Angelina Uy, Runfeng Li, Leonidas Guibas, James Tompkin, Adam W. Harley | cs.CV | [PDF](http://arxiv.org/pdf/2412.04457v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting methods are emerging as a popular approach for converting
multi-view image data into scene representations that allow view synthesis. In
particular, there is interest in enabling view synthesis for dynamic scenes
using only monocular input data -- an ill-posed and challenging problem. The
fast pace of work in this area has produced multiple simultaneous papers that
claim to work best, which cannot all be true. In this work, we organize,
benchmark, and analyze many Gaussian-splatting-based methods, providing
apples-to-apples comparisons that prior works have lacked. We use multiple
existing datasets and a new instructive synthetic dataset designed to isolate
factors that affect reconstruction quality. We systematically categorize
Gaussian splatting methods into specific motion representation types and
quantify how their differences impact performance. Empirically, we find that
their rank order is well-defined in synthetic data, but the complexity of
real-world data currently overwhelms the differences. Furthermore, the fast
rendering speed of all Gaussian-based methods comes at the cost of brittleness
in optimization. We summarize our experiments into a list of findings that can
help to further progress in this lively problem setting. Project Webpage:
https://lynl7130.github.io/MonoDyGauBench.github.io/

Comments:
- 37 pages, 39 figures, 9 tables

---

## PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human  Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-05 | Shota Sasaki, Jane Wu, Ko Nishino | cs.CV | [PDF](http://arxiv.org/pdf/2412.04433v1){: .btn .btn-green } |

**Abstract**: This paper introduces a novel clothed human model that can be learned from
multiview RGB videos, with a particular emphasis on recovering physically
accurate body and cloth movements. Our method, Position Based Dynamic Gaussians
(PBDyG), realizes ``movement-dependent'' cloth deformation via physical
simulation, rather than merely relying on ``pose-dependent'' rigid
transformations. We model the clothed human holistically but with two distinct
physical entities in contact: clothing modeled as 3D Gaussians, which are
attached to a skinned SMPL body that follows the movement of the person in the
input videos. The articulation of the SMPL body also drives physically-based
simulation of the clothes' Gaussians to transform the avatar to novel poses. In
order to run position based dynamics simulation, physical properties including
mass and material stiffness are estimated from the RGB videos through Dynamic
3D Gaussian Splatting. Experiments demonstrate that our method not only
accurately reproduces appearance but also enables the reconstruction of avatars
wearing highly deformable garments, such as skirts or coats, which have been
challenging to reconstruct using existing methods.



---

## Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2412.03526v1){: .btn .btn-green } |

**Abstract**: Recent advancements in static feed-forward scene reconstruction have
demonstrated significant progress in high-quality novel view synthesis.
However, these models often struggle with generalizability across diverse
environments and fail to effectively handle dynamic content. We present BTimer
(short for BulletTimer), the first motion-aware feed-forward model for
real-time reconstruction and novel view synthesis of dynamic scenes. Our
approach reconstructs the full scene in a 3D Gaussian Splatting representation
at a given target ('bullet') timestamp by aggregating information from all the
context frames. Such a formulation allows BTimer to gain scalability and
generalization by leveraging both static and dynamic scene datasets. Given a
casual monocular dynamic video, BTimer reconstructs a bullet-time scene within
150ms while reaching state-of-the-art performance on both static and dynamic
scene datasets, even compared with optimization-based approaches.

Comments:
- Project website:
  https://research.nvidia.com/labs/toronto-ai/bullet-timer/

---

## NeRF and Gaussian Splatting SLAM in the Wild

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Fabian Schmidt, Markus Enzweiler, Abhinav Valada | cs.RO | [PDF](http://arxiv.org/pdf/2412.03263v1){: .btn .btn-green } |

**Abstract**: Navigating outdoor environments with visual Simultaneous Localization and
Mapping (SLAM) systems poses significant challenges due to dynamic scenes,
lighting variations, and seasonal changes, requiring robust solutions. While
traditional SLAM methods struggle with adaptability, deep learning-based
approaches and emerging neural radiance fields as well as Gaussian
Splatting-based SLAM methods, offer promising alternatives. However, these
methods have primarily been evaluated in controlled indoor environments with
stable conditions, leaving a gap in understanding their performance in
unstructured and variable outdoor settings. This study addresses this gap by
evaluating these methods in natural outdoor environments, focusing on camera
tracking accuracy, robustness to environmental factors, and computational
efficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate
that neural SLAM methods achieve superior robustness, particularly under
challenging conditions such as low light, but at a high computational cost. At
the same time, traditional methods perform the best across seasons but are
highly sensitive to variations in lighting conditions. The code of the
benchmark is publicly available at
https://github.com/iis-esslingen/nerf-3dgs-benchmark.

Comments:
- 5 pages, 2 figures, 4 tables

---

## Dense Scene Reconstruction from Light-Field Images Affected by Rolling  Shutter


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Hermes McGriff, Renato Martins, Nicolas Andreff, Cedric Demonceaux | cs.CV | [PDF](http://arxiv.org/pdf/2412.03518v1){: .btn .btn-green } |

**Abstract**: This paper presents a dense depth estimation approach from light-field (LF)
images that is able to compensate for strong rolling shutter (RS) effects. Our
method estimates RS compensated views and dense RS compensated disparity maps.
We present a two-stage method based on a 2D Gaussians Splatting that allows for
a ``render and compare" strategy with a point cloud formulation. In the first
stage, a subset of sub-aperture images is used to estimate an RS agnostic 3D
shape that is related to the scene target shape ``up to a motion". In the
second stage, the deformation of the 3D shape is computed by estimating an
admissible camera motion. We demonstrate the effectiveness and advantages of
this approach through several experiments conducted for different scenes and
types of motions. Due to lack of suitable datasets for evaluation, we also
present a new carefully designed synthetic dataset of RS LF images. The source
code, trained models and dataset will be made publicly available at:
https://github.com/ICB-Vision-AI/DenseRSLF



---

## RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Yoonwoo Jeong, Junmyeong Lee, Hoseung Choi, Minsu Cho | cs.CV | [PDF](http://arxiv.org/pdf/2412.03077v1){: .btn .btn-green } |

**Abstract**: Dynamic view synthesis (DVS) has advanced remarkably in recent years,
achieving high-fidelity rendering while reducing computational costs. Despite
the progress, optimizing dynamic neural fields from casual videos remains
challenging, as these videos do not provide direct 3D information, such as
camera trajectories or the underlying scene geometry. In this work, we present
RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual
videos. It effectively learns motion and underlying geometry of scenes by
separating dynamic and static primitives, and ensures that the learned motion
and geometry are physically plausible by incorporating motion and geometric
regularization terms. We also introduce a comprehensive benchmark, Kubric-MRig,
that provides extensive camera and object motion along with simultaneous
multi-view captures, features that are absent in previous benchmarks.
Experimental results demonstrate that the proposed method significantly
outperforms previous pose-free dynamic neural fields and achieves competitive
rendering quality compared to existing pose-free static neural fields. The code
and data are publicly available at https://rodygs.github.io/.

Comments:
- Project Page: https://rodygs.github.io/

---

## Splats in Splats: Embedding Invisible 3D Watermark within Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Yijia Guo, Wenkai Huang, Yang Li, Gaolei Li, Hang Zhang, Liwen Hu, Jianhua Li, Tiejun Huang, Lei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2412.03121v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction
performance with explicit scene representations. Given the widespread
application of 3DGS in 3D reconstruction and generation tasks, there is an
urgent need to protect the copyright of 3DGS assets. However, existing
copyright protection techniques for 3DGS overlook the usability of 3D assets,
posing challenges for practical deployment. Here we describe WaterGS, the first
3DGS watermarking framework that embeds 3D content in 3DGS itself without
modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep
insight into spherical harmonics (SH) and devise an importance-graded SH
coefficient encryption strategy to embed the hidden SH coefficients.
Furthermore, we employ a convolutional autoencoder to establish a mapping
between the original Gaussian primitives' opacity and the hidden Gaussian
primitives' opacity. Extensive experiments indicate that WaterGS significantly
outperforms existing 3D steganography techniques, with 5.31% higher scene
fidelity and 3X faster rendering speed, while ensuring security, robustness,
and user experience. Codes and data will be released at
https://water-gs.github.io.



---

## Urban4D: Semantic-Guided 4D Gaussian Splatting for Urban Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Ziwen Li, Jiaxin Huang, Runnan Chen, Yunlong Che, Yandong Guo, Tongliang Liu, Fakhri Karray, Mingming Gong | cs.CV | [PDF](http://arxiv.org/pdf/2412.03473v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic urban scenes presents significant challenges due to
their intrinsic geometric structures and spatiotemporal dynamics. Existing
methods that attempt to model dynamic urban scenes without leveraging priors on
potentially moving regions often produce suboptimal results. Meanwhile,
approaches based on manual 3D annotations yield improved reconstruction quality
but are impractical due to labor-intensive labeling. In this paper, we revisit
the potential of 2D semantic maps for classifying dynamic and static Gaussians
and integrating spatial and temporal dimensions for urban scene representation.
We introduce Urban4D, a novel framework that employs a semantic-guided
decomposition strategy inspired by advances in deep 2D semantic map generation.
Our approach distinguishes potentially dynamic objects through reliable
semantic Gaussians. To explicitly model dynamic objects, we propose an
intuitive and effective 4D Gaussian splatting (4DGS) representation that
aggregates temporal information through learnable time embeddings for each
Gaussian, predicting their deformations at desired timestamps using a
multilayer perceptron (MLP). For more accurate static reconstruction, we also
design a k-nearest neighbor (KNN)-based consistency regularization to handle
the ground surface due to its low-texture characteristic. Extensive experiments
on real-world datasets demonstrate that Urban4D not only achieves comparable or
better quality than previous state-of-the-art methods but also effectively
captures dynamic objects while maintaining high visual fidelity for static
elements.



---

## SGSST: Scaling Gaussian Splatting StyleTransfer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Bruno Galerne, Jianling Wang, Lara Raad, Jean-Michel Morel | cs.CV | [PDF](http://arxiv.org/pdf/2412.03371v1){: .btn .btn-green } |

**Abstract**: Applying style transfer to a full 3D environment is a challenging task that
has seen many developments since the advent of neural rendering. 3D Gaussian
splatting (3DGS) has recently pushed further many limits of neural rendering in
terms of training speed and reconstruction quality. This work introduces SGSST:
Scaling Gaussian Splatting Style Transfer, an optimization-based method to
apply style transfer to pretrained 3DGS scenes. We demonstrate that a new
multiscale loss based on global neural statistics, that we name SOS for
Simultaneously Optimized Scales, enables style transfer to ultra-high
resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such
high image resolutions, it also produces superior visual quality as assessed by
thorough qualitative, quantitative and perceptual comparisons.



---

## 2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains  for High-Fidelity Indoor Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Wanting Zhang, Haodong Xiang, Zhichao Liao, Xiansong Lai, Xinghui Li, Long Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2412.03428v1){: .btn .btn-green } |

**Abstract**: The reconstruction of indoor scenes remains challenging due to the inherent
complexity of spatial structures and the prevalence of textureless regions.
Recent advancements in 3D Gaussian Splatting have improved novel view synthesis
with accelerated processing but have yet to deliver comparable performance in
surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method
leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction.
Specifically, we employ a seed-guided mechanism to control the distribution of
2D Gaussians, with the density of seed points dynamically optimized through
adaptive growth and pruning mechanisms. To further improve geometric accuracy,
we incorporate monocular depth and normal priors to provide constraints for
details and textureless regions respectively. Additionally, multi-view
consistency constraints are employed to mitigate artifacts and further enhance
reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets
demonstrate that our method achieves state-of-the-art performance in indoor
scene reconstruction.



---

## Volumetrically Consistent 3D Gaussian Rasterization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-04 | Chinmay Talegaonkar, Yash Belhe, Ravi Ramamoorthi, Nicholas Antipa | cs.CV | [PDF](http://arxiv.org/pdf/2412.03378v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view
synthesis at high inference speeds. However, its splatting-based rendering
model makes several approximations to the rendering equation, reducing physical
accuracy. We show that splatting and its approximations are unnecessary, even
within a rasterizer; we instead volumetrically integrate 3D Gaussians directly
to compute the transmittance across them analytically. We use this analytic
transmittance to derive more physically-accurate alpha values than 3DGS, which
can directly be used within their framework. The result is a method that more
closely follows the volume rendering equation (similar to ray-tracing) while
enjoying the speed benefits of rasterization. Our method represents opaque
surfaces with higher accuracy and fewer points than 3DGS. This enables it to
outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being
volumetrically consistent also enables our method to work out of the box for
tomography. We match the state-of-the-art 3DGS-based tomography method with
fewer points. Being volumetrically consistent also enables our method to work
out of the box for tomography. We match the state-of-the-art 3DGS-based
tomography method with fewer points.



---

## SparseGrasp: Robotic Grasping via 3D Semantic Gaussian Splatting from  Sparse Multi-View RGB Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Junqiu Yu, Xinlin Ren, Yongchong Gu, Haitao Lin, Tianyu Wang, Yi Zhu, Hang Xu, Yu-Gang Jiang, Xiangyang Xue, Yanwei Fu | cs.RO | [PDF](http://arxiv.org/pdf/2412.02140v1){: .btn .btn-green } |

**Abstract**: Language-guided robotic grasping is a rapidly advancing field where robots
are instructed using human language to grasp specific objects. However,
existing methods often depend on dense camera views and struggle to quickly
update scenes, limiting their effectiveness in changeable environments.
  In contrast, we propose SparseGrasp, a novel open-vocabulary robotic grasping
system that operates efficiently with sparse-view RGB images and handles scene
updates fastly. Our system builds upon and significantly enhances existing
computer vision modules in robotic learning. Specifically, SparseGrasp utilizes
DUSt3R to generate a dense point cloud as the initialization for 3D Gaussian
Splatting (3DGS), maintaining high fidelity even under sparse supervision.
Importantly, SparseGrasp incorporates semantic awareness from recent vision
foundation models. To further improve processing efficiency, we repurpose
Principal Component Analysis (PCA) to compress features from 2D models.
Additionally, we introduce a novel render-and-compare strategy that ensures
rapid scene updates, enabling multi-turn grasping in changeable environments.
  Experimental results show that SparseGrasp significantly outperforms
state-of-the-art methods in terms of both speed and adaptability, providing a
robust solution for multi-turn grasping in changeable environment.



---

## Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D  Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Abdurrahman Zeybey, Mehmet Ergezer, Tommy Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2412.02803v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has advanced radiance field reconstruction, enabling
high-quality view synthesis and fast rendering in 3D modeling. While
adversarial attacks on object detection models are well-studied for 2D images,
their impact on 3D models remains underexplored. This work introduces the
Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate
adversarial noise targeting the CLIP vision-language model. M-IFGSM
specifically alters the object of interest by focusing perturbations on masked
regions, degrading the performance of CLIP's zero-shot object detection
capability when applied to 3D models. Using eight objects from the Common
Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces
the accuracy and confidence of the model, with adversarial noise being nearly
imperceptible to human observers. The top-1 accuracy in original model renders
drops from 95.4\% to 12.5\% for train images and from 91.2\% to 35.4\% for test
images, with confidence levels reflecting this shift from true classification
to misclassification, underscoring the risks of adversarial attacks on 3D
models in applications such as autonomous driving, robotics, and surveillance.
The significance of this research lies in its potential to expose
vulnerabilities in modern 3D vision models, including radiance fields,
prompting the development of more robust defenses and security measures in
critical real-world applications.

Comments:
- Accepted to Safe Generative AI Workshop @ NeurIPS 2024:
  https://neurips.cc/virtual/2024/workshop/84705

---

## Multi-robot autonomous 3D reconstruction using Gaussian splatting with  Semantic guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Jing Zeng, Qi Ye, Tianle Liu, Yang Xu, Jin Li, Jinming Xu, Liang Li, Jiming Chen | cs.RO | [PDF](http://arxiv.org/pdf/2412.02249v1){: .btn .btn-green } |

**Abstract**: Implicit neural representations and 3D Gaussian splatting (3DGS) have shown
great potential for scene reconstruction. Recent studies have expanded their
applications in autonomous reconstruction through task assignment methods.
However, these methods are mainly limited to single robot, and rapid
reconstruction of large-scale scenes remains challenging. Additionally,
task-driven planning based on surface uncertainty is prone to being trapped in
local optima. To this end, we propose the first 3DGS-based centralized
multi-robot autonomous 3D reconstruction framework. To further reduce time cost
of task generation and improve reconstruction quality, we integrate online
open-vocabulary semantic segmentation with surface uncertainty of 3DGS,
focusing view sampling on regions with high instance uncertainty. Finally, we
develop a multi-robot collaboration strategy with mode and task assignments
improving reconstruction quality while ensuring planning efficiency. Our method
demonstrates the highest reconstruction quality among all planning methods and
superior planning efficiency compared to existing multi-robot methods. We
deploy our method on multiple robots, and results show that it can effectively
plan view paths and reconstruct scenes with high quality.



---

## TimeWalker: Personalized Neural Space for Lifelong Head Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Dongwei Pan, Yang Li, Hongsheng Li, Kwan-Yee Lin | cs.CV | [PDF](http://arxiv.org/pdf/2412.02421v1){: .btn .btn-green } |

**Abstract**: We present TimeWalker, a novel framework that models realistic, full-scale 3D
head avatars of a person on lifelong scale. Unlike current human head avatar
pipelines that capture identity at the momentary level(e.g., instant
photography or short videos), TimeWalker constructs a person's comprehensive
identity from unstructured data collection over his/her various life stages,
offering a paradigm to achieve full reconstruction and animation of that person
at different moments of life. At the heart of TimeWalker's success is a novel
neural parametric model that learns personalized representation with the
disentanglement of shape, expression, and appearance across ages. Central to
our methodology are the concepts of two aspects: (1) We track back to the
principle of modeling a person's identity in an additive combination of average
head representation in the canonical space, and moment-specific head attribute
representations driven from a set of neural head basis. To learn the set of
head basis that could represent the comprehensive head variations in a compact
manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It
dynamically adjusts the number and blend weights of neural head bases,
according to both shared and specific traits of the target person over ages.
(2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian
splatting representation, to model head motion deformations like facial
expressions without losing the realism of rendering and reconstruction.
DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that
utilize the priors from parametric model, and move/rotate with the change of
expression. Through extensive experimental evaluations, we show TimeWalker's
ability to reconstruct and animate avatars across decoupled dimensions with
realistic rendering effects, demonstrating a way to achieve personalized 'time
traveling' in a breeze.

Comments:
- Project Page: https://timewalker2024.github.io/timewalker.github.io/
  , Video: https://www.youtube.com/watch?v=x8cpOVMY_ko

---

## GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Zhiyuan Chen, Fan Lu, Guo Yu, Bin Li, Sanqing Qu, Yuan Huang, Changhong Fu, Guang Chen | cs.CV | [PDF](http://arxiv.org/pdf/2412.02267v1){: .btn .btn-green } |

**Abstract**: Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is
crucial for robotic manipulation. However, existing approaches typically rely
on accurate depth information, which is non-trivial to obtain in real-world
scenarios. Although depth estimation algorithms can be employed, geometric
inaccuracy can lead to failures in RGBD-based pose tracking methods. To address
this challenge, we introduce GSGTrack, a novel RGB-based pose tracking
framework that jointly optimizes geometry and pose. Specifically, we adopt 3D
Gaussian Splatting to create an optimizable 3D representation, which is learned
simultaneously with a graph-based geometry optimization to capture the object's
appearance features and refine its geometry. However, the joint optimization
process is susceptible to perturbations from noisy pose and geometry data.
Thus, we propose an object silhouette loss to address the issue of pixel-wise
loss being overly sensitive to pose noise during tracking. To mitigate the
geometric ambiguities caused by inaccurate depth information, we propose a
geometry-consistent image pair selection strategy, which filters out
low-confidence pairs and ensures robust geometric optimization. Extensive
experiments on the OnePose and HO3D datasets demonstrate the effectiveness of
GSGTrack in both 6DoF pose tracking and object reconstruction.



---

## Gaussian Object Carver: Object-Compositional Gaussian Splatting with  surfaces completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Liu Liu, Xinjie Wang, Jiaxiong Qiu, Tianwei Lin, Xiaolin Zhou, Zhizhong Su | cs.CV | [PDF](http://arxiv.org/pdf/2412.02075v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction is a foundational problem in computer vision. Despite
recent advancements in Neural Implicit Representations (NIR), existing methods
often lack editability and compositional flexibility, limiting their use in
scenarios requiring high interactivity and object-level manipulation. In this
paper, we introduce the Gaussian Object Carver (GOC), a novel, efficient, and
scalable framework for object-compositional 3D scene reconstruction. GOC
leverages 3D Gaussian Splatting (GS), enriched with monocular geometry priors
and multi-view geometry regularization, to achieve high-quality and flexible
reconstruction. Furthermore, we propose a zero-shot Object Surface Completion
(OSC) model, which uses 3D priors from 3d object data to reconstruct unobserved
surfaces, ensuring object completeness even in occluded areas. Experimental
results demonstrate that GOC improves reconstruction efficiency and geometric
fidelity. It holds promise for advancing the practical application of digital
twins in embodied AI, AR/VR, and interactive simulation environments.



---

## SparseLGS: Sparse View Language Embedded Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Jun Hu, Zhang Chen, Zhong Li, Yi Xu, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.02245v2){: .btn .btn-green } |

**Abstract**: Recently, several studies have combined Gaussian Splatting to obtain scene
representations with language embeddings for open-vocabulary 3D scene
understanding. While these methods perform well, they essentially require very
dense multi-view inputs, limiting their applicability in real-world scenarios.
In this work, we propose SparseLGS to address the challenge of 3D scene
understanding with pose-free and sparse view input images. Our method leverages
a learning-based dense stereo model to handle pose-free and sparse inputs, and
a three-step region matching approach to address the multi-view semantic
inconsistency problem, which is especially important for sparse inputs.
Different from directly learning high-dimensional CLIP features, we extract
low-dimensional information and build bijections to avoid excessive learning
and storage costs. We introduce a reconstruction loss during semantic training
to improve Gaussian positions and shapes. To the best of our knowledge, we are
the first to address the 3D semantic field problem with sparse pose-free
inputs. Experimental results show that SparseLGS achieves comparable quality
when reconstructing semantic fields with fewer inputs (3-4 views) compared to
previous SOTA methods with dense input. Besides, when using the same sparse
input, SparseLGS leads significantly in quality and heavily improves the
computation speed (5$\times$speedup). Project page:
https://ustc3dv.github.io/SparseLGS

Comments:
- Project Page: https://ustc3dv.github.io/SparseLGS

---

## How to Use Diffusion Priors under Sparse Views?

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Qisen Wang, Yifan Zhao, Jiawei Ma, Jia Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.02225v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis under sparse views has been a long-term important
challenge in 3D reconstruction. Existing works mainly rely on introducing
external semantic or depth priors to supervise the optimization of 3D
representations. However, the diffusion model, as an external prior that can
directly provide visual supervision, has always underperformed in sparse-view
3D reconstruction using Score Distillation Sampling (SDS) due to the low
information entropy of sparse views compared to text, leading to optimization
challenges caused by mode deviation. To this end, we present a thorough
analysis of SDS from the mode-seeking perspective and propose Inline Prior
Guided Score Matching (IPSM), which leverages visual inline priors provided by
pose relationships between viewpoints to rectify the rendered image
distribution and decomposes the original optimization objective of SDS, thereby
offering effective diffusion visual guidance without any fine-tuning or
pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts
3D Gaussian Splatting as the backbone and supplements depth and geometry
consistency regularization based on IPSM to further improve inline priors and
rectified distribution. Experimental results on different public datasets show
that our method achieves state-of-the-art reconstruction quality. The code is
released at https://github.com/iCVTEAM/IPSM.



---

## AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent  Gaussian Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong | cs.CV | [PDF](http://arxiv.org/pdf/2412.02684v1){: .btn .btn-green } |

**Abstract**: Generating animatable human avatars from a single image is essential for
various digital human modeling applications. Existing 3D reconstruction methods
often struggle to capture fine details in animatable models, while generative
approaches for controllable animation, though avoiding explicit 3D modeling,
suffer from viewpoint inconsistencies in extreme poses and computational
inefficiencies. In this paper, we address these challenges by leveraging the
power of generative models to produce detailed multi-view canonical pose
images, which help resolve ambiguities in animatable human reconstruction. We
then propose a robust method for 3D reconstruction of inconsistent images,
enabling real-time rendering during inference. Specifically, we adapt a
transformer-based video generation model to generate multi-view canonical pose
images and normal maps, pretraining on a large-scale video dataset to improve
generalization. To handle view inconsistencies, we recast the reconstruction
problem as a 4D task and introduce an efficient 3D modeling approach using 4D
Gaussian Splatting. Experiments demonstrate that our method achieves
photorealistic, real-time animation of 3D human avatars from in-the-wild
images, showcasing its effectiveness and generalization capability.

Comments:
- Project Page: https://lingtengqiu.github.io/2024/AniGS/

---

## RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex  Motions via Relay Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Qiankun Gao, Yanmin Wu, Chengxiang Wen, Jiarui Meng, Luyang Tang, Jie Chen, Ronggang Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.02493v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic scenes with large-scale and complex motions remains a
significant challenge. Recent techniques like Neural Radiance Fields and 3D
Gaussian Splatting (3DGS) have shown promise but still struggle with scenes
involving substantial movement. This paper proposes RelayGS, a novel method
based on 3DGS, specifically designed to represent and reconstruct highly
dynamic scenes. Our RelayGS learns a complete 4D representation with canonical
3D Gaussians and a compact motion field, consisting of three stages. First, we
learn a fundamental 3DGS from all frames, ignoring temporal scene variations,
and use a learnable mask to separate the highly dynamic foreground from the
minimally moving background. Second, we replicate multiple copies of the
decoupled foreground Gaussians from the first stage, each corresponding to a
temporal segment, and optimize them using pseudo-views constructed from
multiple frames within each segment. These Gaussians, termed Relay Gaussians,
act as explicit relay nodes, simplifying and breaking down large-scale motion
trajectories into smaller, manageable segments. Finally, we jointly learn the
scene's temporal motion and refine the canonical Gaussians learned from the
first two stages. We conduct thorough experiments on two dynamic scene datasets
featuring large and complex motions, where our RelayGS outperforms
state-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs
real-world basketball game scenes in a much more complete and coherent manner,
whereas previous methods usually struggle to capture the complex motion of
players. Code will be publicly available at https://github.com/gqk/RelayGS

Comments:
- Technical Report. GitHub: https://github.com/gqk/RelayGS

---

## Driving Scene Synthesis on Free-form Trajectories with Generative Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zeyu Yang, Zijie Pan, Yuankun Yang, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.01717v1){: .btn .btn-green } |

**Abstract**: Driving scene synthesis along free-form trajectories is essential for driving
simulations to enable closed-loop evaluation of end-to-end driving policies.
While existing methods excel at novel view synthesis on recorded trajectories,
they face challenges with novel trajectories due to limited views of driving
videos and the vastness of driving environments. To tackle this challenge, we
propose a novel free-form driving view synthesis approach, dubbed DriveX, by
leveraging video generative prior to optimize a 3D model across a variety of
trajectories. Concretely, we crafted an inverse problem that enables a video
diffusion model to be utilized as a prior for many-trajectory optimization of a
parametric 3D model (e.g., Gaussian splatting). To seamlessly use the
generative prior, we iteratively conduct this process during optimization. Our
resulting model can produce high-fidelity virtual driving environments outside
the recorded trajectory, enabling free-form trajectory driving simulation.
Beyond real driving scenes, DriveX can also be utilized to simulate virtual
driving worlds from AI-generated videos.



---

## HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Hongyu Zhou, Longzhong Lin, Jiabao Wang, Yichong Lu, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01718v1){: .btn .btn-green } |

**Abstract**: In the past few decades, autonomous driving algorithms have made significant
progress in perception, planning, and control. However, evaluating individual
components does not fully reflect the performance of entire systems,
highlighting the need for more holistic assessment methods. This motivates the
development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator
for evaluating autonomous driving algorithms. We achieve this by lifting
captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving
the rendering quality for closed-loop scenarios, and building the closed-loop
environment. In terms of rendering, We tackle challenges of novel view
synthesis in closed-loop scenarios, including viewpoint extrapolation and
360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further
enables the full closed simulation loop, dynamically updating the ego and actor
states and observations based on control commands. Moreover, HUGSIM offers a
comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo,
nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair
and realistic evaluation platform for existing autonomous driving algorithms.
HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks
the potential for fine-tuning autonomous driving algorithms in a photorealistic
closed-loop setting.

Comments:
- Our project page is at https://xdimlab.github.io/HUGSIM

---

## Planar Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Farhad G. Zanjani, Hong Cai, Hanno Ackermann, Leila Mirvakhabova, Fatih Porikli | cs.CV | [PDF](http://arxiv.org/pdf/2412.01931v1){: .btn .btn-green } |

**Abstract**: This paper presents Planar Gaussian Splatting (PGS), a novel neural rendering
approach to learn the 3D geometry and parse the 3D planes of a scene, directly
from multiple RGB images. The PGS leverages Gaussian primitives to model the
scene and employ a hierarchical Gaussian mixture approach to group them.
Similar Gaussians are progressively merged probabilistically in the
tree-structured Gaussian mixtures to identify distinct 3D plane instances and
form the overall 3D scene geometry. In order to enable the grouping, the
Gaussian primitives contain additional parameters, such as plane descriptors
derived by lifting 2D masks from a general 2D segmentation model and surface
normals. Experiments show that the proposed PGS achieves state-of-the-art
performance in 3D planar reconstruction without requiring either 3D plane
labels or depth supervision. In contrast to existing supervised methods that
have limited generalizability and struggle under domain shift, PGS maintains
its performance across datasets thanks to its neural rendering and
scene-specific optimization mechanism, while also being significantly faster
than existing optimization-based approaches.

Comments:
- IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),
  2025

---

## HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Yunzhou Song, Heguang Lin, Jiahui Lei, Lingjie Liu, Kostas Daniilidis | cs.CV | [PDF](http://arxiv.org/pdf/2412.01823v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural rendering, particularly 2D Gaussian Splatting
(2DGS), have shown promising results for jointly reconstructing fine appearance
and geometry by leveraging 2D Gaussian surfels. However, current methods face
significant challenges when rendering at arbitrary viewpoints, such as
anti-aliasing for down-sampled rendering, and texture detail preservation for
high-resolution rendering. We proposed a novel method to align the 2D surfels
with texture maps and augment it with per-ray depth sorting and fisher-based
pruning for rendering consistency and efficiency. With correct order,
per-surfel texture maps significantly improve the capabilities to capture fine
details. Additionally, to render high-fidelity details in varying viewpoints,
we designed a frustum-based sampling method to mitigate the aliasing artifacts.
Experimental results on benchmarks and our custom texture-rich dataset
demonstrate that our method surpasses existing techniques, particularly in
detail preservation and anti-aliasing.

Comments:
- Project Page: https://timsong412.github.io/HDGS-ProjPage/

---

## Occam's LGS: A Simple Approach for Language Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Jiahuan Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2412.01807v1){: .btn .btn-green } |

**Abstract**: TL;DR: Gaussian Splatting is a widely adopted approach for 3D scene
representation that offers efficient, high-quality 3D reconstruction and
rendering. A major reason for the success of 3DGS is its simplicity of
representing a scene with a set of Gaussians, which makes it easy to interpret
and adapt. To enhance scene understanding beyond the visual representation,
approaches have been developed that extend 3D Gaussian Splatting with semantic
vision-language features, especially allowing for open-set tasks. In this
setting, the language features of 3D Gaussian Splatting are often aggregated
from multiple 2D views. Existing works address this aggregation problem using
cumbersome techniques that lead to high computational cost and training time.
  In this work, we show that the sophisticated techniques for language-grounded
3D Gaussian Splatting are simply unnecessary. Instead, we apply Occam's razor
to the task at hand and perform weighted multi-view feature aggregation using
the weights derived from the standard rendering process, followed by a simple
heuristic-based noisy Gaussian filtration. Doing so offers us state-of-the-art
results with a speed-up of two orders of magnitude. We showcase our results in
two commonly used benchmark datasets: LERF and 3D-OVS. Our simple approach
allows us to perform reasoning directly in the language features, without any
compression whatsoever. Such modeling in turn offers easy scene manipulation,
unlike the existing methods -- which we illustrate using an application of
object insertion in the scene. Furthermore, we provide a thorough discussion
regarding the significance of our contributions within the context of the
current literature. Project Page: https://insait-institute.github.io/OccamLGS/

Comments:
- Project Page: https://insait-institute.github.io/OccamLGS/

---

## CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Kai He, Chin-Hsuan Wu, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2412.01792v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D representations, such as Neural Radiance Fields and 3D
Gaussian Splatting, have greatly improved realistic scene modeling and
novel-view synthesis. However, achieving controllable and consistent editing in
dynamic 3D scenes remains a significant challenge. Previous work is largely
constrained by its editing backbones, resulting in inconsistent edits and
limited controllability. In our work, we introduce a novel framework that first
fine-tunes the InstructPix2Pix model, followed by a two-stage optimization of
the scene based on deformable 3D Gaussians. Our fine-tuning enables the model
to "learn" the editing ability from a single edited reference image,
transforming the complex task of dynamic scene editing into a simple 2D image
editing process. By directly learning editing regions and styles from the
reference, our approach enables consistent and precise local edits without the
need for tracking desired editing regions, effectively addressing key
challenges in dynamic scene editing. Then, our two-stage optimization
progressively edits the trained dynamic scene, using a designed edited image
buffer to accelerate convergence and improve temporal consistency. Compared to
state-of-the-art methods, our approach offers more flexible and controllable
local scene editing, achieving high-quality and consistent results.

Comments:
- Project page: https://ihe-kaii.github.io/CTRL-D/

---

## Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale  Aerial-to-Ground Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Lihan Jiang, Kerui Ren, Mulin Yu, Linning Xu, Junting Dong, Tao Lu, Feng Zhao, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2412.01745v1){: .btn .btn-green } |

**Abstract**: Seamless integration of both aerial and street view images remains a
significant challenge in neural scene reconstruction and rendering. Existing
methods predominantly focus on single domain, limiting their applications in
immersive environments, which demand extensive free view exploration with large
view changes both horizontally and vertically. We introduce Horizon-GS, a novel
approach built upon Gaussian Splatting techniques, tackles the unified
reconstruction and rendering for aerial and street views. Our method addresses
the key challenges of combining these perspectives with a new training
strategy, overcoming viewpoint discrepancies to generate high-fidelity scenes.
We also curate a high-quality aerial-to-ground views dataset encompassing both
synthetic and real-world scene to advance further research. Experiments across
diverse urban scene datasets confirm the effectiveness of our method.



---

## SfM-Free 3D Gaussian Splatting via Hierarchical Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Bo Ji, Angela Yao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01553v1){: .btn .btn-green } |

**Abstract**: Standard 3D Gaussian Splatting (3DGS) relies on known or pre-computed camera
poses and a sparse point cloud, obtained from structure-from-motion (SfM)
preprocessing, to initialize and grow 3D Gaussians. We propose a novel SfM-Free
3DGS (SFGS) method for video input, eliminating the need for known camera poses
and SfM preprocessing. Our approach introduces a hierarchical training strategy
that trains and merges multiple 3D Gaussian representations -- each optimized
for specific scene regions -- into a single, unified 3DGS model representing
the entire scene. To compensate for large camera motions, we leverage video
frame interpolation models. Additionally, we incorporate multi-source
supervision to reduce overfitting and enhance representation. Experimental
results reveal that our approach significantly surpasses state-of-the-art
SfM-free novel view synthesis methods. On the Tanks and Temples dataset, we
improve PSNR by an average of 2.25dB, with a maximum gain of 3.72dB in the best
scene. On the CO3D-V2 dataset, we achieve an average PSNR boost of 1.74dB, with
a top gain of 3.90dB. The code is available at
https://github.com/jibo27/3DGS_Hierarchical_Training.



---

## GFreeDet: Exploiting Gaussian Splatting and Foundation Models for  Model-free Unseen Object Detection in the BOP Challenge 2024

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Xingyu Liu, Yingyue Li, Chengxi Li, Gu Wang, Chenyangguang Zhang, Ziqin Huang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2412.01552v2){: .btn .btn-green } |

**Abstract**: In this report, we provide the technical details of the submitted method
GFreeDet, which exploits Gaussian splatting and vision Foundation models for
the model-free unseen object Detection track in the BOP 2024 Challenge.



---

## 6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Yufeng Jin, Vignesh Prasad, Snehal Jauhri, Mathias Franzius, Georgia Chalvatzaki | cs.CV | [PDF](http://arxiv.org/pdf/2412.01543v1){: .btn .btn-green } |

**Abstract**: Efficient and accurate object pose estimation is an essential component for
modern vision systems in many applications such as Augmented Reality,
autonomous driving, and robotics. While research in model-based 6D object pose
estimation has delivered promising results, model-free methods are hindered by
the high computational load in rendering and inferring consistent poses of
arbitrary objects in a live RGB-D video stream. To address this issue, we
present 6DOPE-GS, a novel method for online 6D object pose estimation \&
tracking with a single RGB-D camera by effectively leveraging advances in
Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of
Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses
and 3D object reconstruction. To achieve the necessary efficiency and accuracy
for live tracking, our method uses incremental 2D Gaussian Splatting with an
intelligent dynamic keyframe selection procedure to achieve high spatial object
coverage and prevent erroneous pose updates. We also propose an opacity
statistic-based pruning mechanism for adaptive Gaussian density control, to
ensure training stability and efficiency. We evaluate our method on the HO3D
and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of
state-of-the-art baselines for model-free simultaneous 6D pose tracking and
reconstruction while providing a 5$\times$ speedup. We also demonstrate the
method's suitability for live, dynamic object tracking and reconstruction in a
real-world setting.



---

## ULSR-GS: Ultra Large-scale Surface Reconstruction Gaussian Splatting  with Multi-View Geometric Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zhuoxiao Li, Shanliang Yao, Qizhong Gao, Angel F. Garcia-Fernandez, Yong Yue, Xiaohui Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2412.01402v1){: .btn .btn-green } |

**Abstract**: While Gaussian Splatting (GS) demonstrates efficient and high-quality scene
rendering and small area surface extraction ability, it falls short in handling
large-scale aerial image surface extraction tasks. To overcome this, we present
ULSR-GS, a framework dedicated to high-fidelity surface extraction in
ultra-large-scale scenes, addressing the limitations of existing GS-based mesh
extraction methods. Specifically, we propose a point-to-photo partitioning
approach combined with a multi-view optimal view matching principle to select
the best training images for each sub-region. Additionally, during training,
ULSR-GS employs a densification strategy based on multi-view geometric
consistency to enhance surface extraction details. Experimental results
demonstrate that ULSR-GS outperforms other state-of-the-art GS-based works on
large-scale aerial photogrammetry benchmark datasets, significantly improving
surface extraction accuracy in complex urban environments. Project page:
https://ulsrgs.github.io.

Comments:
- Project page: https://ulsrgs.github.io

---

## RGBDS-SLAM: A RGB-D Semantic Dense SLAM Based on 3D Multi Level Pyramid  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zhenzhong Cao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01217v1){: .btn .btn-green } |

**Abstract**: High-quality reconstruction is crucial for dense SLAM. Recent popular
approaches utilize 3D Gaussian Splatting (3D GS) techniques for RGB, depth, and
semantic reconstruction of scenes. However, these methods often overlook issues
of detail and consistency in different parts of the scene. To address this, we
propose RGBDS-SLAM, a RGB-D semantic dense SLAM system based on 3D multi-level
pyramid gaussian splatting, which enables high-quality dense reconstruction of
scene RGB, depth, and semantics.In this system, we introduce a 3D multi-level
pyramid gaussian splatting method that restores scene details by extracting
multi-level image pyramids for gaussian splatting training, ensuring
consistency in RGB, depth, and semantic reconstructions. Additionally, we
design a tightly-coupled multi-features reconstruction optimization mechanism,
allowing the reconstruction accuracy of RGB, depth, and semantic maps to
mutually enhance each other during the rendering optimization process.
Extensive quantitative, qualitative, and ablation experiments on the Replica
and ScanNet public datasets demonstrate that our proposed method outperforms
current state-of-the-art methods. The open-source code will be available at:
https://github.com/zhenzhongcao/RGBDS-SLAM.



---

## Diffusion Models with Anisotropic Gaussian Splatting for Image  Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Jacob Fein-Ashley, Benjamin Fein-Ashley | cs.CV | [PDF](http://arxiv.org/pdf/2412.01682v2){: .btn .btn-green } |

**Abstract**: Image inpainting is a fundamental task in computer vision, aiming to restore
missing or corrupted regions in images realistically. While recent deep
learning approaches have significantly advanced the state-of-the-art,
challenges remain in maintaining structural continuity and generating coherent
textures, particularly in large missing areas. Diffusion models have shown
promise in generating high-fidelity images but often lack the structural
guidance necessary for realistic inpainting. We propose a novel inpainting
method that combines diffusion models with anisotropic Gaussian splatting to
capture both local structures and global context effectively. By modeling
missing regions using anisotropic Gaussian functions that adapt to local image
gradients, our approach provides structural guidance to the diffusion-based
inpainting network. The Gaussian splat maps are integrated into the diffusion
process, enhancing the model's ability to generate high-fidelity and
structurally coherent inpainting results. Extensive experiments demonstrate
that our method outperforms state-of-the-art techniques, producing visually
plausible results with enhanced structural integrity and texture realism.



---

## 3DSceneEditor: Controllable 3D Scene Editing with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Ziyang Yan, Lei Li, Yihua Shao, Siyu Chen, Wuzong Kai, Jenq-Neng Hwang, Hao Zhao, Fabio Remondino | cs.CV | [PDF](http://arxiv.org/pdf/2412.01583v1){: .btn .btn-green } |

**Abstract**: The creation of 3D scenes has traditionally been both labor-intensive and
costly, requiring designers to meticulously configure 3D assets and
environments. Recent advancements in generative AI, including text-to-3D and
image-to-3D methods, have dramatically reduced the complexity and cost of this
process. However, current techniques for editing complex 3D scenes continue to
rely on generally interactive multi-step, 2D-to-3D projection methods and
diffusion-based techniques, which often lack precision in control and hamper
real-time performance. In this work, we propose 3DSceneEditor, a fully 3D-based
paradigm for real-time, precise editing of intricate 3D scenes using Gaussian
Splatting. Unlike conventional methods, 3DSceneEditor operates through a
streamlined 3D pipeline, enabling direct manipulation of Gaussians for
efficient, high-quality edits based on input prompts.The proposed framework (i)
integrates a pre-trained instance segmentation model for semantic labeling;
(ii) employs a zero-shot grounding approach with CLIP to align target objects
with user prompts; and (iii) applies scene modifications, such as object
addition, repositioning, recoloring, replacing, and deletion directly on
Gaussians. Extensive experimental results show that 3DSceneEditor achieves
superior editing precision and speed with respect to current SOTA 3D scene
editing approaches, establishing a new benchmark for efficient and interactive
3D scene customization.

Comments:
- Project Page: https://ziyangyan.github.io/3DSceneEditor

---

## SAGA: Surface-Aligned Gaussian Avatar

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Ronghan Chen, Yang Cong, Jiayue Liu | cs.CV | [PDF](http://arxiv.org/pdf/2412.00845v1){: .btn .btn-green } |

**Abstract**: This paper presents a Surface-Aligned Gaussian representation for creating
animatable human avatars from monocular videos,aiming at improving the novel
view and pose synthesis performance while ensuring fast training and real-time
rendering. Recently,3DGS has emerged as a more efficient and expressive
alternative to NeRF, and has been used for creating dynamic human avatars.
However,when applied to the severely ill-posed task of monocular dynamic
reconstruction, the Gaussians tend to overfit the constantly changing regions
such as clothes wrinkles or shadows since these regions cannot provide
consistent supervision, resulting in noisy geometry and abrupt deformation that
typically fail to generalize under novel views and poses.To address these
limitations, we present SAGA,i.e.,Surface-Aligned Gaussian Avatar,which aligns
the Gaussians with a mesh to enforce well-defined geometry and consistent
deformation, thereby improving generalization under novel views and poses.
Unlike existing strict alignment methods that suffer from limited expressive
power and low realism,SAGA employs a two-stage alignment strategy where the
Gaussians are first adhered on while then detached from the mesh, thus
facilitating both good geometry and high expressivity. In the Adhered Stage, we
improve the flexibility of Adhered-on-Mesh Gaussians by allowing them to flow
on the mesh, in contrast to existing methods that rigidly bind Gaussians to
fixed location. In the second Detached Stage, we introduce a Gaussian-Mesh
Alignment regularization, which allows us to unleash the expressivity by
detaching the Gaussians but maintain the geometric alignment by minimizing
their location and orientation offsets from the bound triangles. Finally, since
the Gaussians may drift outside the bound triangles during optimization, an
efficient Walking-on-Mesh strategy is proposed to dynamically update the bound
triangles.

Comments:
- Submitted to TPAMI. Major Revision. Project page:
  https://gostinshell.github.io/SAGA/

---

## CtrlNeRF: The Generative Neural Radiation Fields for the Controllable  Synthesis of High-fidelity 3D-Aware Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Jian Liu, Zhen Yu | cs.CV | [PDF](http://arxiv.org/pdf/2412.00754v1){: .btn .btn-green } |

**Abstract**: The neural radiance field (NERF) advocates learning the continuous
representation of 3D geometry through a multilayer perceptron (MLP). By
integrating this into a generative model, the generative neural radiance field
(GRAF) is capable of producing images from random noise z without 3D
supervision. In practice, the shape and appearance are modeled by z_s and z_a,
respectively, to manipulate them separately during inference. However, it is
challenging to represent multiple scenes using a solitary MLP and precisely
control the generation of 3D geometry in terms of shape and appearance. In this
paper, we introduce a controllable generative model (i.e. \textbf{CtrlNeRF})
that uses a single MLP network to represent multiple scenes with shared
weights. Consequently, we manipulated the shape and appearance codes to realize
the controllable generation of high-fidelity images with 3D consistency.
Moreover, the model enables the synthesis of novel views that do not exist in
the training sets via camera pose alteration and feature interpolation.
Extensive experiments were conducted to demonstrate its superiority in 3D-aware
image generation compared to its counterparts.



---

## FlashSLAM: Accelerated RGB-D SLAM for Real-Time 3D Scene Reconstruction  with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Phu Pham, Damon Conover, Aniket Bera | cs.CV | [PDF](http://arxiv.org/pdf/2412.00682v1){: .btn .btn-green } |

**Abstract**: We present FlashSLAM, a novel SLAM approach that leverages 3D Gaussian
Splatting for efficient and robust 3D scene reconstruction. Existing 3DGS-based
SLAM methods often fall short in sparse view settings and during large camera
movements due to their reliance on gradient descent-based optimization, which
is both slow and inaccurate. FlashSLAM addresses these limitations by combining
3DGS with a fast vision-based camera tracking technique, utilizing a pretrained
feature matching model and point cloud registration for precise pose estimation
in under 80 ms - a 90% reduction in tracking time compared to SplaTAM - without
costly iterative rendering. In sparse settings, our method achieves up to a 92%
improvement in average tracking accuracy over previous methods. Additionally,
it accounts for noise in depth sensors, enhancing robustness when using
unspecialized devices such as smartphones. Extensive experiments show that
FlashSLAM performs reliably across both sparse and dense settings, in synthetic
and real-world environments. Evaluations on benchmark datasets highlight its
superior accuracy and efficiency, establishing FlashSLAM as a versatile and
high-performance solution for SLAM, advancing the state-of-the-art in 3D
reconstruction across diverse applications.

Comments:
- 16 pages, 9 figures, 13 tables

---

## Ref-GS: Directional Factorization for 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Youjia Zhang, Anpei Chen, Yumin Wan, Zikai Song, Junqing Yu, Yawei Luo, Wei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2412.00905v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Ref-GS, a novel approach for directional light
factorization in 2D Gaussian splatting, which enables photorealistic
view-dependent appearance rendering and precise geometry recovery. Ref-GS
builds upon the deferred rendering of Gaussian splatting and applies
directional encoding to the deferred-rendered surface, effectively reducing the
ambiguity between orientation and viewing angle. Next, we introduce a spherical
Mip-grid to capture varying levels of surface roughness, enabling
roughness-aware Gaussian shading. Additionally, we propose a simple yet
efficient geometry-lighting factorization that connects geometry and lighting
via the vector outer product, significantly reducing renderer overhead when
integrating volumetric attributes. Our method achieves superior photorealistic
rendering for a range of open-world scenes while also accurately recovering
geometry.

Comments:
- Project page: https://ref-gs.github.io/

---

## ChatSplat: 3D Conversational Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Hanlin Chen, Fangyin Wei, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2412.00734v1){: .btn .btn-green } |

**Abstract**: Humans naturally interact with their 3D surroundings using language, and
modeling 3D language fields for scene understanding and interaction has gained
growing interest. This paper introduces ChatSplat, a system that constructs a
3D language field, enabling rich chat-based interaction within 3D space. Unlike
existing methods that primarily use CLIP-derived language features focused
solely on segmentation, ChatSplat facilitates interaction on three levels:
objects, views, and the entire 3D scene. For view-level interaction, we
designed an encoder that encodes the rendered feature map of each view into
tokens, which are then processed by a large language model (LLM) for
conversation. At the scene level, ChatSplat combines multi-view tokens,
enabling interactions that consider the entire scene. For object-level
interaction, ChatSplat uses a patch-wise language embedding, unlike LangSplat's
pixel-wise language embedding that implicitly includes mask and embedding.
Here, we explicitly decouple the language embedding into separate mask and
feature map representations, allowing more flexible object-level interaction.
To address the challenge of learning 3D Gaussians posed by the complex and
diverse distribution of language embeddings used in the LLM, we introduce a
learnable normalization technique to standardize these embeddings, facilitating
effective learning. Extensive experimental results demonstrate that ChatSplat
supports multi-level interactions -- object, view, and scene -- within 3D
space, enhancing both understanding and engagement.



---

## DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.00851v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting have shown promising results.
Existing methods typically assume static scenes and/or multiple images with
prior poses. Dynamics, sparse views, and unknown poses significantly increase
the problem complexity due to insufficient geometric constraints. To overcome
this challenge, we propose a method that can use only two images without prior
poses to fit Gaussians in dynamic environments. To achieve this, we introduce
two technical contributions. First, we propose an object-level two-view bundle
adjustment. This strategy decomposes dynamic scenes into piece-wise rigid
components, and jointly estimates the camera pose and motions of dynamic
objects. Second, we design an SE(3) field-driven Gaussian training method. It
enables fine-grained motion modeling through learnable per-Gaussian
transformations. Our method leads to high-fidelity novel view synthesis of
dynamic scenes while accurately preserving temporal consistency and object
motion. Experiments on both synthetic and real-world datasets demonstrate that
our method significantly outperforms state-of-the-art approaches designed for
the cases of static environments, multiple images, and/or known poses. Our
project page is available at https://colin-de.github.io/DynSUP/.



---

## VR-Doh: Hands-on 3D Modeling in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Zhaofeng Luo, Zhitong Cui, Shijian Luo, Mengyu Chu, Minchen Li | cs.GR | [PDF](http://arxiv.org/pdf/2412.00814v1){: .btn .btn-green } |

**Abstract**: We present VR-Doh, a hands-on 3D modeling system designed for creating and
manipulating elastoplastic objects in virtual reality (VR). The system employs
the Material Point Method (MPM) for simulating realistic large deformations and
incorporates optimized Gaussian Splatting for seamless rendering. With direct,
hand-based interactions, users can naturally sculpt, deform, and edit objects
interactively. To achieve real-time performance, we developed localized
simulation techniques, optimized collision handling, and separated appearance
and physical representations, ensuring smooth and responsive user interaction.
The system supports both freeform creation and precise adjustments, catering to
diverse modeling tasks. A user study involving novice and experienced users
highlights the system's intuitive design, immersive feedback, and creative
potential. Compared to traditional geometry-based modeling tools, our approach
offers improved accessibility and natural interaction in specific contexts.


