---
layout: default
title: December 2024
parent: Papers
nav_order: 202412
---

<!---metadata--->


## AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent  Gaussian Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong | cs.CV | [PDF](http://arxiv.org/pdf/2412.02684v1){: .btn .btn-green } |

**Abstract**: Generating animatable human avatars from a single image is essential for
various digital human modeling applications. Existing 3D reconstruction methods
often struggle to capture fine details in animatable models, while generative
approaches for controllable animation, though avoiding explicit 3D modeling,
suffer from viewpoint inconsistencies in extreme poses and computational
inefficiencies. In this paper, we address these challenges by leveraging the
power of generative models to produce detailed multi-view canonical pose
images, which help resolve ambiguities in animatable human reconstruction. We
then propose a robust method for 3D reconstruction of inconsistent images,
enabling real-time rendering during inference. Specifically, we adapt a
transformer-based video generation model to generate multi-view canonical pose
images and normal maps, pretraining on a large-scale video dataset to improve
generalization. To handle view inconsistencies, we recast the reconstruction
problem as a 4D task and introduce an efficient 3D modeling approach using 4D
Gaussian Splatting. Experiments demonstrate that our method achieves
photorealistic, real-time animation of 3D human avatars from in-the-wild
images, showcasing its effectiveness and generalization capability.

Comments:
- Project Page: https://lingtengqiu.github.io/2024/AniGS/

---

## How to Use Diffusion Priors under Sparse Views?

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Qisen Wang, Yifan Zhao, Jiawei Ma, Jia Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.02225v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis under sparse views has been a long-term important
challenge in 3D reconstruction. Existing works mainly rely on introducing
external semantic or depth priors to supervise the optimization of 3D
representations. However, the diffusion model, as an external prior that can
directly provide visual supervision, has always underperformed in sparse-view
3D reconstruction using Score Distillation Sampling (SDS) due to the low
information entropy of sparse views compared to text, leading to optimization
challenges caused by mode deviation. To this end, we present a thorough
analysis of SDS from the mode-seeking perspective and propose Inline Prior
Guided Score Matching (IPSM), which leverages visual inline priors provided by
pose relationships between viewpoints to rectify the rendered image
distribution and decomposes the original optimization objective of SDS, thereby
offering effective diffusion visual guidance without any fine-tuning or
pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts
3D Gaussian Splatting as the backbone and supplements depth and geometry
consistency regularization based on IPSM to further improve inline priors and
rectified distribution. Experimental results on different public datasets show
that our method achieves state-of-the-art reconstruction quality. The code is
released at https://github.com/iCVTEAM/IPSM.



---

## RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex  Motions via Relay Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Qiankun Gao, Yanmin Wu, Chengxiang Wen, Jiarui Meng, Luyang Tang, Jie Chen, Ronggang Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.02493v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic scenes with large-scale and complex motions remains a
significant challenge. Recent techniques like Neural Radiance Fields and 3D
Gaussian Splatting (3DGS) have shown promise but still struggle with scenes
involving substantial movement. This paper proposes RelayGS, a novel method
based on 3DGS, specifically designed to represent and reconstruct highly
dynamic scenes. Our RelayGS learns a complete 4D representation with canonical
3D Gaussians and a compact motion field, consisting of three stages. First, we
learn a fundamental 3DGS from all frames, ignoring temporal scene variations,
and use a learnable mask to separate the highly dynamic foreground from the
minimally moving background. Second, we replicate multiple copies of the
decoupled foreground Gaussians from the first stage, each corresponding to a
temporal segment, and optimize them using pseudo-views constructed from
multiple frames within each segment. These Gaussians, termed Relay Gaussians,
act as explicit relay nodes, simplifying and breaking down large-scale motion
trajectories into smaller, manageable segments. Finally, we jointly learn the
scene's temporal motion and refine the canonical Gaussians learned from the
first two stages. We conduct thorough experiments on two dynamic scene datasets
featuring large and complex motions, where our RelayGS outperforms
state-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs
real-world basketball game scenes in a much more complete and coherent manner,
whereas previous methods usually struggle to capture the complex motion of
players. Code will be publicly available at https://github.com/gqk/RelayGS

Comments:
- Technical Report. GitHub: https://github.com/gqk/RelayGS

---

## Gaussian Object Carver: Object-Compositional Gaussian Splatting with  surfaces completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Liu Liu, Xinjie Wang, Jiaxiong Qiu, Tianwei Lin, Xiaolin Zhou, Zhizhong Su | cs.CV | [PDF](http://arxiv.org/pdf/2412.02075v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction is a foundational problem in computer vision. Despite
recent advancements in Neural Implicit Representations (NIR), existing methods
often lack editability and compositional flexibility, limiting their use in
scenarios requiring high interactivity and object-level manipulation. In this
paper, we introduce the Gaussian Object Carver (GOC), a novel, efficient, and
scalable framework for object-compositional 3D scene reconstruction. GOC
leverages 3D Gaussian Splatting (GS), enriched with monocular geometry priors
and multi-view geometry regularization, to achieve high-quality and flexible
reconstruction. Furthermore, we propose a zero-shot Object Surface Completion
(OSC) model, which uses 3D priors from 3d object data to reconstruct unobserved
surfaces, ensuring object completeness even in occluded areas. Experimental
results demonstrate that GOC improves reconstruction efficiency and geometric
fidelity. It holds promise for advancing the practical application of digital
twins in embodied AI, AR/VR, and interactive simulation environments.



---

## SparseGrasp: Robotic Grasping via 3D Semantic Gaussian Splatting from  Sparse Multi-View RGB Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Junqiu Yu, Xinlin Ren, Yongchong Gu, Haitao Lin, Tianyu Wang, Yi Zhu, Hang Xu, Yu-Gang Jiang, Xiangyang Xue, Yanwei Fu | cs.RO | [PDF](http://arxiv.org/pdf/2412.02140v1){: .btn .btn-green } |

**Abstract**: Language-guided robotic grasping is a rapidly advancing field where robots
are instructed using human language to grasp specific objects. However,
existing methods often depend on dense camera views and struggle to quickly
update scenes, limiting their effectiveness in changeable environments.
  In contrast, we propose SparseGrasp, a novel open-vocabulary robotic grasping
system that operates efficiently with sparse-view RGB images and handles scene
updates fastly. Our system builds upon and significantly enhances existing
computer vision modules in robotic learning. Specifically, SparseGrasp utilizes
DUSt3R to generate a dense point cloud as the initialization for 3D Gaussian
Splatting (3DGS), maintaining high fidelity even under sparse supervision.
Importantly, SparseGrasp incorporates semantic awareness from recent vision
foundation models. To further improve processing efficiency, we repurpose
Principal Component Analysis (PCA) to compress features from 2D models.
Additionally, we introduce a novel render-and-compare strategy that ensures
rapid scene updates, enabling multi-turn grasping in changeable environments.
  Experimental results show that SparseGrasp significantly outperforms
state-of-the-art methods in terms of both speed and adaptability, providing a
robust solution for multi-turn grasping in changeable environment.



---

## TimeWalker: Personalized Neural Space for Lifelong Head Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Dongwei Pan, Yang Li, Hongsheng Li, Kwan-Yee Lin | cs.CV | [PDF](http://arxiv.org/pdf/2412.02421v1){: .btn .btn-green } |

**Abstract**: We present TimeWalker, a novel framework that models realistic, full-scale 3D
head avatars of a person on lifelong scale. Unlike current human head avatar
pipelines that capture identity at the momentary level(e.g., instant
photography or short videos), TimeWalker constructs a person's comprehensive
identity from unstructured data collection over his/her various life stages,
offering a paradigm to achieve full reconstruction and animation of that person
at different moments of life. At the heart of TimeWalker's success is a novel
neural parametric model that learns personalized representation with the
disentanglement of shape, expression, and appearance across ages. Central to
our methodology are the concepts of two aspects: (1) We track back to the
principle of modeling a person's identity in an additive combination of average
head representation in the canonical space, and moment-specific head attribute
representations driven from a set of neural head basis. To learn the set of
head basis that could represent the comprehensive head variations in a compact
manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It
dynamically adjusts the number and blend weights of neural head bases,
according to both shared and specific traits of the target person over ages.
(2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian
splatting representation, to model head motion deformations like facial
expressions without losing the realism of rendering and reconstruction.
DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that
utilize the priors from parametric model, and move/rotate with the change of
expression. Through extensive experimental evaluations, we show TimeWalker's
ability to reconstruct and animate avatars across decoupled dimensions with
realistic rendering effects, demonstrating a way to achieve personalized 'time
traveling' in a breeze.

Comments:
- Project Page: https://timewalker2024.github.io/timewalker.github.io/
  , Video: https://www.youtube.com/watch?v=x8cpOVMY_ko

---

## SparseLGS: Sparse View Language Embedded Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Jun Hu, Zhang Chen, Zhong Li, Yi Xu, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.02245v1){: .btn .btn-green } |

**Abstract**: Recently, several studies have combined Gaussian Splatting to obtain scene
representations with language embeddings for open-vocabulary 3D scene
understanding. While these methods perform well, they essentially require very
dense multi-view inputs, limiting their applicability in real-world scenarios.
In this work, we propose SparseLGS to address the challenge of 3D scene
understanding with pose-free and sparse view input images. Our method leverages
a learning-based dense stereo model to handle pose-free and sparse inputs, and
a three-step region matching approach to address the multi-view semantic
inconsistency problem, which is especially important for sparse inputs.
Different from directly learning high-dimensional CLIP features, we extract
low-dimensional information and build bijections to avoid excessive learning
and storage costs. We introduce a reconstruction loss during semantic training
to improve Gaussian positions and shapes. To the best of our knowledge, we are
the first to address the 3D semantic field problem with sparse pose-free
inputs. Experimental results show that SparseLGS achieves comparable quality
when reconstructing semantic fields with fewer inputs (3-4 views) compared to
previous SOTA methods with dense input. Besides, when using the same sparse
input, SparseLGS leads significantly in quality and heavily improves the
computation speed (5$\times$ speedup). Project page: {\tt\small
\url{https://ustc3dv.github.io/SparseLGS}}

Comments:
- Project Page: https://ustc3dv.github.io/SparseLGS

---

## GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Zhiyuan Chen, Fan Lu, Guo Yu, Bin Li, Sanqing Qu, Yuan Huang, Changhong Fu, Guang Chen | cs.CV | [PDF](http://arxiv.org/pdf/2412.02267v1){: .btn .btn-green } |

**Abstract**: Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is
crucial for robotic manipulation. However, existing approaches typically rely
on accurate depth information, which is non-trivial to obtain in real-world
scenarios. Although depth estimation algorithms can be employed, geometric
inaccuracy can lead to failures in RGBD-based pose tracking methods. To address
this challenge, we introduce GSGTrack, a novel RGB-based pose tracking
framework that jointly optimizes geometry and pose. Specifically, we adopt 3D
Gaussian Splatting to create an optimizable 3D representation, which is learned
simultaneously with a graph-based geometry optimization to capture the object's
appearance features and refine its geometry. However, the joint optimization
process is susceptible to perturbations from noisy pose and geometry data.
Thus, we propose an object silhouette loss to address the issue of pixel-wise
loss being overly sensitive to pose noise during tracking. To mitigate the
geometric ambiguities caused by inaccurate depth information, we propose a
geometry-consistent image pair selection strategy, which filters out
low-confidence pairs and ensures robust geometric optimization. Extensive
experiments on the OnePose and HO3D datasets demonstrate the effectiveness of
GSGTrack in both 6DoF pose tracking and object reconstruction.



---

## Multi-robot autonomous 3D reconstruction using Gaussian splatting with  Semantic guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-03 | Jing Zeng, Qi Ye, Tianle Liu, Yang Xu, Jin Li, Jinming Xu, Liang Li, Jiming Chen | cs.RO | [PDF](http://arxiv.org/pdf/2412.02249v1){: .btn .btn-green } |

**Abstract**: Implicit neural representations and 3D Gaussian splatting (3DGS) have shown
great potential for scene reconstruction. Recent studies have expanded their
applications in autonomous reconstruction through task assignment methods.
However, these methods are mainly limited to single robot, and rapid
reconstruction of large-scale scenes remains challenging. Additionally,
task-driven planning based on surface uncertainty is prone to being trapped in
local optima. To this end, we propose the first 3DGS-based centralized
multi-robot autonomous 3D reconstruction framework. To further reduce time cost
of task generation and improve reconstruction quality, we integrate online
open-vocabulary semantic segmentation with surface uncertainty of 3DGS,
focusing view sampling on regions with high instance uncertainty. Finally, we
develop a multi-robot collaboration strategy with mode and task assignments
improving reconstruction quality while ensuring planning efficiency. Our method
demonstrates the highest reconstruction quality among all planning methods and
superior planning efficiency compared to existing multi-robot methods. We
deploy our method on multiple robots, and results show that it can effectively
plan view paths and reconstruct scenes with high quality.



---

## HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Hongyu Zhou, Longzhong Lin, Jiabao Wang, Yichong Lu, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01718v1){: .btn .btn-green } |

**Abstract**: In the past few decades, autonomous driving algorithms have made significant
progress in perception, planning, and control. However, evaluating individual
components does not fully reflect the performance of entire systems,
highlighting the need for more holistic assessment methods. This motivates the
development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator
for evaluating autonomous driving algorithms. We achieve this by lifting
captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving
the rendering quality for closed-loop scenarios, and building the closed-loop
environment. In terms of rendering, We tackle challenges of novel view
synthesis in closed-loop scenarios, including viewpoint extrapolation and
360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further
enables the full closed simulation loop, dynamically updating the ego and actor
states and observations based on control commands. Moreover, HUGSIM offers a
comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo,
nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair
and realistic evaluation platform for existing autonomous driving algorithms.
HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks
the potential for fine-tuning autonomous driving algorithms in a photorealistic
closed-loop setting.

Comments:
- Our project page is at https://xdimlab.github.io/HUGSIM

---

## Planar Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Farhad G. Zanjani, Hong Cai, Hanno Ackermann, Leila Mirvakhabova, Fatih Porikli | cs.CV | [PDF](http://arxiv.org/pdf/2412.01931v1){: .btn .btn-green } |

**Abstract**: This paper presents Planar Gaussian Splatting (PGS), a novel neural rendering
approach to learn the 3D geometry and parse the 3D planes of a scene, directly
from multiple RGB images. The PGS leverages Gaussian primitives to model the
scene and employ a hierarchical Gaussian mixture approach to group them.
Similar Gaussians are progressively merged probabilistically in the
tree-structured Gaussian mixtures to identify distinct 3D plane instances and
form the overall 3D scene geometry. In order to enable the grouping, the
Gaussian primitives contain additional parameters, such as plane descriptors
derived by lifting 2D masks from a general 2D segmentation model and surface
normals. Experiments show that the proposed PGS achieves state-of-the-art
performance in 3D planar reconstruction without requiring either 3D plane
labels or depth supervision. In contrast to existing supervised methods that
have limited generalizability and struggle under domain shift, PGS maintains
its performance across datasets thanks to its neural rendering and
scene-specific optimization mechanism, while also being significantly faster
than existing optimization-based approaches.

Comments:
- IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),
  2025

---

## HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Yunzhou Song, Heguang Lin, Jiahui Lei, Lingjie Liu, Kostas Daniilidis | cs.CV | [PDF](http://arxiv.org/pdf/2412.01823v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural rendering, particularly 2D Gaussian Splatting
(2DGS), have shown promising results for jointly reconstructing fine appearance
and geometry by leveraging 2D Gaussian surfels. However, current methods face
significant challenges when rendering at arbitrary viewpoints, such as
anti-aliasing for down-sampled rendering, and texture detail preservation for
high-resolution rendering. We proposed a novel method to align the 2D surfels
with texture maps and augment it with per-ray depth sorting and fisher-based
pruning for rendering consistency and efficiency. With correct order,
per-surfel texture maps significantly improve the capabilities to capture fine
details. Additionally, to render high-fidelity details in varying viewpoints,
we designed a frustum-based sampling method to mitigate the aliasing artifacts.
Experimental results on benchmarks and our custom texture-rich dataset
demonstrate that our method surpasses existing techniques, particularly in
detail preservation and anti-aliasing.

Comments:
- Project Page: https://timsong412.github.io/HDGS-ProjPage/

---

## Occam's LGS: A Simple Approach for Language Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Jiahuan Cheng, Jan-Nico Zaech, Luc Van Gool, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2412.01807v1){: .btn .btn-green } |

**Abstract**: TL;DR: Gaussian Splatting is a widely adopted approach for 3D scene
representation that offers efficient, high-quality 3D reconstruction and
rendering. A major reason for the success of 3DGS is its simplicity of
representing a scene with a set of Gaussians, which makes it easy to interpret
and adapt. To enhance scene understanding beyond the visual representation,
approaches have been developed that extend 3D Gaussian Splatting with semantic
vision-language features, especially allowing for open-set tasks. In this
setting, the language features of 3D Gaussian Splatting are often aggregated
from multiple 2D views. Existing works address this aggregation problem using
cumbersome techniques that lead to high computational cost and training time.
  In this work, we show that the sophisticated techniques for language-grounded
3D Gaussian Splatting are simply unnecessary. Instead, we apply Occam's razor
to the task at hand and perform weighted multi-view feature aggregation using
the weights derived from the standard rendering process, followed by a simple
heuristic-based noisy Gaussian filtration. Doing so offers us state-of-the-art
results with a speed-up of two orders of magnitude. We showcase our results in
two commonly used benchmark datasets: LERF and 3D-OVS. Our simple approach
allows us to perform reasoning directly in the language features, without any
compression whatsoever. Such modeling in turn offers easy scene manipulation,
unlike the existing methods -- which we illustrate using an application of
object insertion in the scene. Furthermore, we provide a thorough discussion
regarding the significance of our contributions within the context of the
current literature. Project Page: https://insait-institute.github.io/OccamLGS/

Comments:
- Project Page: https://insait-institute.github.io/OccamLGS/

---

## CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Kai He, Chin-Hsuan Wu, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2412.01792v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D representations, such as Neural Radiance Fields and 3D
Gaussian Splatting, have greatly improved realistic scene modeling and
novel-view synthesis. However, achieving controllable and consistent editing in
dynamic 3D scenes remains a significant challenge. Previous work is largely
constrained by its editing backbones, resulting in inconsistent edits and
limited controllability. In our work, we introduce a novel framework that first
fine-tunes the InstructPix2Pix model, followed by a two-stage optimization of
the scene based on deformable 3D Gaussians. Our fine-tuning enables the model
to "learn" the editing ability from a single edited reference image,
transforming the complex task of dynamic scene editing into a simple 2D image
editing process. By directly learning editing regions and styles from the
reference, our approach enables consistent and precise local edits without the
need for tracking desired editing regions, effectively addressing key
challenges in dynamic scene editing. Then, our two-stage optimization
progressively edits the trained dynamic scene, using a designed edited image
buffer to accelerate convergence and improve temporal consistency. Compared to
state-of-the-art methods, our approach offers more flexible and controllable
local scene editing, achieving high-quality and consistent results.

Comments:
- Project page: https://ihe-kaii.github.io/CTRL-D/

---

## Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale  Aerial-to-Ground Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Lihan Jiang, Kerui Ren, Mulin Yu, Linning Xu, Junting Dong, Tao Lu, Feng Zhao, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2412.01745v1){: .btn .btn-green } |

**Abstract**: Seamless integration of both aerial and street view images remains a
significant challenge in neural scene reconstruction and rendering. Existing
methods predominantly focus on single domain, limiting their applications in
immersive environments, which demand extensive free view exploration with large
view changes both horizontally and vertically. We introduce Horizon-GS, a novel
approach built upon Gaussian Splatting techniques, tackles the unified
reconstruction and rendering for aerial and street views. Our method addresses
the key challenges of combining these perspectives with a new training
strategy, overcoming viewpoint discrepancies to generate high-fidelity scenes.
We also curate a high-quality aerial-to-ground views dataset encompassing both
synthetic and real-world scene to advance further research. Experiments across
diverse urban scene datasets confirm the effectiveness of our method.



---

## Driving Scene Synthesis on Free-form Trajectories with Generative Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zeyu Yang, Zijie Pan, Yuankun Yang, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2412.01717v1){: .btn .btn-green } |

**Abstract**: Driving scene synthesis along free-form trajectories is essential for driving
simulations to enable closed-loop evaluation of end-to-end driving policies.
While existing methods excel at novel view synthesis on recorded trajectories,
they face challenges with novel trajectories due to limited views of driving
videos and the vastness of driving environments. To tackle this challenge, we
propose a novel free-form driving view synthesis approach, dubbed DriveX, by
leveraging video generative prior to optimize a 3D model across a variety of
trajectories. Concretely, we crafted an inverse problem that enables a video
diffusion model to be utilized as a prior for many-trajectory optimization of a
parametric 3D model (e.g., Gaussian splatting). To seamlessly use the
generative prior, we iteratively conduct this process during optimization. Our
resulting model can produce high-fidelity virtual driving environments outside
the recorded trajectory, enabling free-form trajectory driving simulation.
Beyond real driving scenes, DriveX can also be utilized to simulate virtual
driving worlds from AI-generated videos.



---

## 3DSceneEditor: Controllable 3D Scene Editing with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Ziyang Yan, Lei Li, Yihua Shao, Siyu Chen, Wuzong Kai, Jenq-Neng Hwang, Hao Zhao, Fabio Remondino | cs.CV | [PDF](http://arxiv.org/pdf/2412.01583v1){: .btn .btn-green } |

**Abstract**: The creation of 3D scenes has traditionally been both labor-intensive and
costly, requiring designers to meticulously configure 3D assets and
environments. Recent advancements in generative AI, including text-to-3D and
image-to-3D methods, have dramatically reduced the complexity and cost of this
process. However, current techniques for editing complex 3D scenes continue to
rely on generally interactive multi-step, 2D-to-3D projection methods and
diffusion-based techniques, which often lack precision in control and hamper
real-time performance. In this work, we propose 3DSceneEditor, a fully 3D-based
paradigm for real-time, precise editing of intricate 3D scenes using Gaussian
Splatting. Unlike conventional methods, 3DSceneEditor operates through a
streamlined 3D pipeline, enabling direct manipulation of Gaussians for
efficient, high-quality edits based on input prompts.The proposed framework (i)
integrates a pre-trained instance segmentation model for semantic labeling;
(ii) employs a zero-shot grounding approach with CLIP to align target objects
with user prompts; and (iii) applies scene modifications, such as object
addition, repositioning, recoloring, replacing, and deletion directly on
Gaussians. Extensive experimental results show that 3DSceneEditor achieves
superior editing precision and speed with respect to current SOTA 3D scene
editing approaches, establishing a new benchmark for efficient and interactive
3D scene customization.

Comments:
- Project Page: https://ziyangyan.github.io/3DSceneEditor

---

## SfM-Free 3D Gaussian Splatting via Hierarchical Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Bo Ji, Angela Yao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01553v1){: .btn .btn-green } |

**Abstract**: Standard 3D Gaussian Splatting (3DGS) relies on known or pre-computed camera
poses and a sparse point cloud, obtained from structure-from-motion (SfM)
preprocessing, to initialize and grow 3D Gaussians. We propose a novel SfM-Free
3DGS (SFGS) method for video input, eliminating the need for known camera poses
and SfM preprocessing. Our approach introduces a hierarchical training strategy
that trains and merges multiple 3D Gaussian representations -- each optimized
for specific scene regions -- into a single, unified 3DGS model representing
the entire scene. To compensate for large camera motions, we leverage video
frame interpolation models. Additionally, we incorporate multi-source
supervision to reduce overfitting and enhance representation. Experimental
results reveal that our approach significantly surpasses state-of-the-art
SfM-free novel view synthesis methods. On the Tanks and Temples dataset, we
improve PSNR by an average of 2.25dB, with a maximum gain of 3.72dB in the best
scene. On the CO3D-V2 dataset, we achieve an average PSNR boost of 1.74dB, with
a top gain of 3.90dB. The code is available at
https://github.com/jibo27/3DGS_Hierarchical_Training.



---

## GFreeDet: Exploiting Gaussian Splatting and Foundation Models for  Model-free Unseen Object Detection in the BOP Challenge 2024

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Xingyu Liu, Yingyue Li, Chengxi Li, Gu Wang, Chenyangguang Zhang, Ziqin Huang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2412.01552v2){: .btn .btn-green } |

**Abstract**: In this report, we provide the technical details of the submitted method
GFreeDet, which exploits Gaussian splatting and vision Foundation models for
the model-free unseen object Detection track in the BOP 2024 Challenge.



---

## 6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Yufeng Jin, Vignesh Prasad, Snehal Jauhri, Mathias Franzius, Georgia Chalvatzaki | cs.CV | [PDF](http://arxiv.org/pdf/2412.01543v1){: .btn .btn-green } |

**Abstract**: Efficient and accurate object pose estimation is an essential component for
modern vision systems in many applications such as Augmented Reality,
autonomous driving, and robotics. While research in model-based 6D object pose
estimation has delivered promising results, model-free methods are hindered by
the high computational load in rendering and inferring consistent poses of
arbitrary objects in a live RGB-D video stream. To address this issue, we
present 6DOPE-GS, a novel method for online 6D object pose estimation \&
tracking with a single RGB-D camera by effectively leveraging advances in
Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of
Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses
and 3D object reconstruction. To achieve the necessary efficiency and accuracy
for live tracking, our method uses incremental 2D Gaussian Splatting with an
intelligent dynamic keyframe selection procedure to achieve high spatial object
coverage and prevent erroneous pose updates. We also propose an opacity
statistic-based pruning mechanism for adaptive Gaussian density control, to
ensure training stability and efficiency. We evaluate our method on the HO3D
and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of
state-of-the-art baselines for model-free simultaneous 6D pose tracking and
reconstruction while providing a 5$\times$ speedup. We also demonstrate the
method's suitability for live, dynamic object tracking and reconstruction in a
real-world setting.



---

## ULSR-GS: Ultra Large-scale Surface Reconstruction Gaussian Splatting  with Multi-View Geometric Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zhuoxiao Li, Shanliang Yao, Qizhong Gao, Angel F. Garcia-Fernandez, Yong Yue, Xiaohui Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2412.01402v1){: .btn .btn-green } |

**Abstract**: While Gaussian Splatting (GS) demonstrates efficient and high-quality scene
rendering and small area surface extraction ability, it falls short in handling
large-scale aerial image surface extraction tasks. To overcome this, we present
ULSR-GS, a framework dedicated to high-fidelity surface extraction in
ultra-large-scale scenes, addressing the limitations of existing GS-based mesh
extraction methods. Specifically, we propose a point-to-photo partitioning
approach combined with a multi-view optimal view matching principle to select
the best training images for each sub-region. Additionally, during training,
ULSR-GS employs a densification strategy based on multi-view geometric
consistency to enhance surface extraction details. Experimental results
demonstrate that ULSR-GS outperforms other state-of-the-art GS-based works on
large-scale aerial photogrammetry benchmark datasets, significantly improving
surface extraction accuracy in complex urban environments. Project page:
https://ulsrgs.github.io.

Comments:
- Project page: https://ulsrgs.github.io

---

## RGBDS-SLAM: A RGB-D Semantic Dense SLAM Based on 3D Multi Level Pyramid  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Zhenzhong Cao | cs.CV | [PDF](http://arxiv.org/pdf/2412.01217v1){: .btn .btn-green } |

**Abstract**: High-quality reconstruction is crucial for dense SLAM. Recent popular
approaches utilize 3D Gaussian Splatting (3D GS) techniques for RGB, depth, and
semantic reconstruction of scenes. However, these methods often overlook issues
of detail and consistency in different parts of the scene. To address this, we
propose RGBDS-SLAM, a RGB-D semantic dense SLAM system based on 3D multi-level
pyramid gaussian splatting, which enables high-quality dense reconstruction of
scene RGB, depth, and semantics.In this system, we introduce a 3D multi-level
pyramid gaussian splatting method that restores scene details by extracting
multi-level image pyramids for gaussian splatting training, ensuring
consistency in RGB, depth, and semantic reconstructions. Additionally, we
design a tightly-coupled multi-features reconstruction optimization mechanism,
allowing the reconstruction accuracy of RGB, depth, and semantic maps to
mutually enhance each other during the rendering optimization process.
Extensive quantitative, qualitative, and ablation experiments on the Replica
and ScanNet public datasets demonstrate that our proposed method outperforms
current state-of-the-art methods. The open-source code will be available at:
https://github.com/zhenzhongcao/RGBDS-SLAM.



---

## Diffusion Models with Anisotropic Gaussian Splatting for Image  Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-02 | Jacob Fein-Ashley, Benjamin Fein-Ashley | cs.CV | [PDF](http://arxiv.org/pdf/2412.01682v2){: .btn .btn-green } |

**Abstract**: Image inpainting is a fundamental task in computer vision, aiming to restore
missing or corrupted regions in images realistically. While recent deep
learning approaches have significantly advanced the state-of-the-art,
challenges remain in maintaining structural continuity and generating coherent
textures, particularly in large missing areas. Diffusion models have shown
promise in generating high-fidelity images but often lack the structural
guidance necessary for realistic inpainting. We propose a novel inpainting
method that combines diffusion models with anisotropic Gaussian splatting to
capture both local structures and global context effectively. By modeling
missing regions using anisotropic Gaussian functions that adapt to local image
gradients, our approach provides structural guidance to the diffusion-based
inpainting network. The Gaussian splat maps are integrated into the diffusion
process, enhancing the model's ability to generate high-fidelity and
structurally coherent inpainting results. Extensive experiments demonstrate
that our method outperforms state-of-the-art techniques, producing visually
plausible results with enhanced structural integrity and texture realism.



---

## CtrlNeRF: The Generative Neural Radiation Fields for the Controllable  Synthesis of High-fidelity 3D-Aware Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Jian Liu, Zhen Yu | cs.CV | [PDF](http://arxiv.org/pdf/2412.00754v1){: .btn .btn-green } |

**Abstract**: The neural radiance field (NERF) advocates learning the continuous
representation of 3D geometry through a multilayer perceptron (MLP). By
integrating this into a generative model, the generative neural radiance field
(GRAF) is capable of producing images from random noise z without 3D
supervision. In practice, the shape and appearance are modeled by z_s and z_a,
respectively, to manipulate them separately during inference. However, it is
challenging to represent multiple scenes using a solitary MLP and precisely
control the generation of 3D geometry in terms of shape and appearance. In this
paper, we introduce a controllable generative model (i.e. \textbf{CtrlNeRF})
that uses a single MLP network to represent multiple scenes with shared
weights. Consequently, we manipulated the shape and appearance codes to realize
the controllable generation of high-fidelity images with 3D consistency.
Moreover, the model enables the synthesis of novel views that do not exist in
the training sets via camera pose alteration and feature interpolation.
Extensive experiments were conducted to demonstrate its superiority in 3D-aware
image generation compared to its counterparts.



---

## FlashSLAM: Accelerated RGB-D SLAM for Real-Time 3D Scene Reconstruction  with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Phu Pham, Damon Conover, Aniket Bera | cs.CV | [PDF](http://arxiv.org/pdf/2412.00682v1){: .btn .btn-green } |

**Abstract**: We present FlashSLAM, a novel SLAM approach that leverages 3D Gaussian
Splatting for efficient and robust 3D scene reconstruction. Existing 3DGS-based
SLAM methods often fall short in sparse view settings and during large camera
movements due to their reliance on gradient descent-based optimization, which
is both slow and inaccurate. FlashSLAM addresses these limitations by combining
3DGS with a fast vision-based camera tracking technique, utilizing a pretrained
feature matching model and point cloud registration for precise pose estimation
in under 80 ms - a 90% reduction in tracking time compared to SplaTAM - without
costly iterative rendering. In sparse settings, our method achieves up to a 92%
improvement in average tracking accuracy over previous methods. Additionally,
it accounts for noise in depth sensors, enhancing robustness when using
unspecialized devices such as smartphones. Extensive experiments show that
FlashSLAM performs reliably across both sparse and dense settings, in synthetic
and real-world environments. Evaluations on benchmark datasets highlight its
superior accuracy and efficiency, establishing FlashSLAM as a versatile and
high-performance solution for SLAM, advancing the state-of-the-art in 3D
reconstruction across diverse applications.

Comments:
- 16 pages, 9 figures, 13 tables

---

## ChatSplat: 3D Conversational Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Hanlin Chen, Fangyin Wei, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2412.00734v1){: .btn .btn-green } |

**Abstract**: Humans naturally interact with their 3D surroundings using language, and
modeling 3D language fields for scene understanding and interaction has gained
growing interest. This paper introduces ChatSplat, a system that constructs a
3D language field, enabling rich chat-based interaction within 3D space. Unlike
existing methods that primarily use CLIP-derived language features focused
solely on segmentation, ChatSplat facilitates interaction on three levels:
objects, views, and the entire 3D scene. For view-level interaction, we
designed an encoder that encodes the rendered feature map of each view into
tokens, which are then processed by a large language model (LLM) for
conversation. At the scene level, ChatSplat combines multi-view tokens,
enabling interactions that consider the entire scene. For object-level
interaction, ChatSplat uses a patch-wise language embedding, unlike LangSplat's
pixel-wise language embedding that implicitly includes mask and embedding.
Here, we explicitly decouple the language embedding into separate mask and
feature map representations, allowing more flexible object-level interaction.
To address the challenge of learning 3D Gaussians posed by the complex and
diverse distribution of language embeddings used in the LLM, we introduce a
learnable normalization technique to standardize these embeddings, facilitating
effective learning. Extensive experimental results demonstrate that ChatSplat
supports multi-level interactions -- object, view, and scene -- within 3D
space, enhancing both understanding and engagement.



---

## SAGA: Surface-Aligned Gaussian Avatar

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Ronghan Chen, Yang Cong, Jiayue Liu | cs.CV | [PDF](http://arxiv.org/pdf/2412.00845v1){: .btn .btn-green } |

**Abstract**: This paper presents a Surface-Aligned Gaussian representation for creating
animatable human avatars from monocular videos,aiming at improving the novel
view and pose synthesis performance while ensuring fast training and real-time
rendering. Recently,3DGS has emerged as a more efficient and expressive
alternative to NeRF, and has been used for creating dynamic human avatars.
However,when applied to the severely ill-posed task of monocular dynamic
reconstruction, the Gaussians tend to overfit the constantly changing regions
such as clothes wrinkles or shadows since these regions cannot provide
consistent supervision, resulting in noisy geometry and abrupt deformation that
typically fail to generalize under novel views and poses.To address these
limitations, we present SAGA,i.e.,Surface-Aligned Gaussian Avatar,which aligns
the Gaussians with a mesh to enforce well-defined geometry and consistent
deformation, thereby improving generalization under novel views and poses.
Unlike existing strict alignment methods that suffer from limited expressive
power and low realism,SAGA employs a two-stage alignment strategy where the
Gaussians are first adhered on while then detached from the mesh, thus
facilitating both good geometry and high expressivity. In the Adhered Stage, we
improve the flexibility of Adhered-on-Mesh Gaussians by allowing them to flow
on the mesh, in contrast to existing methods that rigidly bind Gaussians to
fixed location. In the second Detached Stage, we introduce a Gaussian-Mesh
Alignment regularization, which allows us to unleash the expressivity by
detaching the Gaussians but maintain the geometric alignment by minimizing
their location and orientation offsets from the bound triangles. Finally, since
the Gaussians may drift outside the bound triangles during optimization, an
efficient Walking-on-Mesh strategy is proposed to dynamically update the bound
triangles.

Comments:
- Submitted to TPAMI. Major Revision. Project page:
  https://gostinshell.github.io/SAGA/

---

## VR-Doh: Hands-on 3D Modeling in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Zhaofeng Luo, Zhitong Cui, Shijian Luo, Mengyu Chu, Minchen Li | cs.GR | [PDF](http://arxiv.org/pdf/2412.00814v1){: .btn .btn-green } |

**Abstract**: We present VR-Doh, a hands-on 3D modeling system designed for creating and
manipulating elastoplastic objects in virtual reality (VR). The system employs
the Material Point Method (MPM) for simulating realistic large deformations and
incorporates optimized Gaussian Splatting for seamless rendering. With direct,
hand-based interactions, users can naturally sculpt, deform, and edit objects
interactively. To achieve real-time performance, we developed localized
simulation techniques, optimized collision handling, and separated appearance
and physical representations, ensuring smooth and responsive user interaction.
The system supports both freeform creation and precise adjustments, catering to
diverse modeling tasks. A user study involving novice and experienced users
highlights the system's intuitive design, immersive feedback, and creative
potential. Compared to traditional geometry-based modeling tools, our approach
offers improved accessibility and natural interaction in specific contexts.



---

## DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li | cs.CV | [PDF](http://arxiv.org/pdf/2412.00851v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting have shown promising results.
Existing methods typically assume static scenes and/or multiple images with
prior poses. Dynamics, sparse views, and unknown poses significantly increase
the problem complexity due to insufficient geometric constraints. To overcome
this challenge, we propose a method that can use only two images without prior
poses to fit Gaussians in dynamic environments. To achieve this, we introduce
two technical contributions. First, we propose an object-level two-view bundle
adjustment. This strategy decomposes dynamic scenes into piece-wise rigid
components, and jointly estimates the camera pose and motions of dynamic
objects. Second, we design an SE(3) field-driven Gaussian training method. It
enables fine-grained motion modeling through learnable per-Gaussian
transformations. Our method leads to high-fidelity novel view synthesis of
dynamic scenes while accurately preserving temporal consistency and object
motion. Experiments on both synthetic and real-world datasets demonstrate that
our method significantly outperforms state-of-the-art approaches designed for
the cases of static environments, multiple images, and/or known poses. Our
project page is available at https://colin-de.github.io/DynSUP/.



---

## Ref-GS: Directional Factorization for 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-12-01 | Youjia Zhang, Anpei Chen, Yumin Wan, Zikai Song, Junqing Yu, Yawei Luo, Wei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2412.00905v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Ref-GS, a novel approach for directional light
factorization in 2D Gaussian splatting, which enables photorealistic
view-dependent appearance rendering and precise geometry recovery. Ref-GS
builds upon the deferred rendering of Gaussian splatting and applies
directional encoding to the deferred-rendered surface, effectively reducing the
ambiguity between orientation and viewing angle. Next, we introduce a spherical
Mip-grid to capture varying levels of surface roughness, enabling
roughness-aware Gaussian shading. Additionally, we propose a simple yet
efficient geometry-lighting factorization that connects geometry and lighting
via the vector outer product, significantly reducing renderer overhead when
integrating volumetric attributes. Our method achieves superior photorealistic
rendering for a range of open-world scenes while also accurately recovering
geometry.

Comments:
- Project page: https://ref-gs.github.io/
