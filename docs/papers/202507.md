---
layout: default
title: July 2025
parent: Papers
nav_order: 202507
---

<!---metadata--->


## SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with  Segmented Consistency Trajectory Distillation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-07 | Jiahao Zhu, Zixuan Chen, Guangcong Wang, Xiaohua Xie, Yi Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2507.05256v1){: .btn .btn-green } |

**Abstract**: Recent advancements in text-to-3D generation improve the visual quality of
Score Distillation Sampling (SDS) and its variants by directly connecting
Consistency Distillation (CD) to score distillation. However, due to the
imbalance between self-consistency and cross-consistency, these CD-based
methods inherently suffer from improper conditional guidance, leading to
sub-optimal generation results. To address this issue, we present
SegmentDreamer, a novel framework designed to fully unleash the potential of
consistency models for high-fidelity text-to-3D generation. Specifically, we
reformulate SDS through the proposed Segmented Consistency Trajectory
Distillation (SCTD), effectively mitigating the imbalance issues by explicitly
defining the relationship between self- and cross-consistency. Moreover, SCTD
partitions the Probability Flow Ordinary Differential Equation (PF-ODE)
trajectory into multiple sub-trajectories and ensures consistency within each
segment, which can theoretically provide a significantly tighter upper bound on
distillation error. Additionally, we propose a distillation pipeline for a more
swift and stable generation. Extensive experiments demonstrate that our
SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling
high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).

Comments:
- Accepted by ICCV 2025, project page: https://zjhjojo.github.io/

---

## InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D  Geometry-Consistent Attention Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-07 | Minghao Wen, Shengjie Wu, Kangkan Wang, Dong Liang | cs.CV | [PDF](http://arxiv.org/pdf/2507.04961v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting based 3D editing has demonstrated impressive
performance in recent years. However, the multi-view editing often exhibits
significant local inconsistency, especially in areas of non-rigid deformation,
which lead to local artifacts, texture blurring, or semantic variations in
edited 3D scenes. We also found that the existing editing methods, which rely
entirely on text prompts make the editing process a "one-shot deal", making it
difficult for users to control the editing degree flexibly. In response to
these challenges, we present InterGSEdit, a novel framework for high-quality
3DGS editing via interactively selecting key views with users' preferences. We
propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to
adaptively screen a group of semantically consistent reference views for each
user-selected key view. Then, the cross-attention maps derived from the
reference views are used in a weighted Gaussian Splatting unprojection to
construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project
$GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D
cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive
attention strategy that prioritizes 3D-constrained attention for geometric
consistency during early inference, and gradually prioritizes 2D
cross-attention maps in diffusion for fine-grained features during the later
inference. Extensive experiments demonstrate that InterGSEdit achieves
state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing
with improved user experience.



---

## A View-consistent Sampling Method for Regularized Training of Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-06 | Aoxiang Fan, Corentin Dumery, Nicolas Talabot, Pascal Fua | cs.CV | [PDF](http://arxiv.org/pdf/2507.04408v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene
representation and 3D recovery. To improve its performance on real-world data,
depth regularizations have proven to be the most effective ones. However, depth
estimation models not only require expensive 3D supervision in training, but
also suffer from generalization issues. As a result, the depth estimations can
be erroneous in practice, especially for outdoor unbounded scenes. In this
paper, we propose to employ view-consistent distributions instead of fixed
depth value estimations to regularize NeRF training. Specifically, the
distribution is computed by utilizing both low-level color features and
high-level distilled features from foundation models at the projected 2D
pixel-locations from per-ray sampled 3D points. By sampling from the
view-consistency distributions, an implicit regularization is imposed on the
training of NeRF. We also utilize a depth-pushing loss that works in
conjunction with the sampling technique to jointly provide effective
regularizations for eliminating the failure modes. Extensive experiments
conducted on various scenes from public datasets demonstrate that our proposed
method can generate significantly better novel view synthesis results than
state-of-the-art NeRF variants as well as different depth regularization
methods.

Comments:
- ICCV 2025 accepted

---

## A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated  Rendering in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Shuo Xin, Haiyu Wang, Sai Qian Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2507.04147v1){: .btn .btn-green } |

**Abstract**: Virtual reality (VR) significantly transforms immersive digital interfaces,
greatly enhancing education, professional practices, and entertainment by
increasing user engagement and opening up new possibilities in various
industries. Among its numerous applications, image rendering is crucial.
Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high
computational demands, driven predominantly by user expectations for superior
visual quality. This results in notable processing delays for real-time image
rendering, which greatly affects the user experience. Additionally, VR devices
such as head-mounted displays (HMDs) are intricately linked to human visual
behavior, leveraging knowledge from perception and cognition to improve user
experience. These insights have spurred the development of foveated rendering,
a technique that dynamically adjusts rendering resolution based on the user's
gaze direction. The resultant solution, known as gaze-tracked foveated
rendering, significantly reduces the computational burden of the rendering
process.
  Although gaze-tracked foveated rendering can reduce rendering costs, the
computational overhead of the gaze tracking process itself can sometimes
outweigh the rendering savings, leading to increased processing latency. To
address this issue, we propose an efficient rendering framework
called~\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated
rendering via the parallelization of gaze tracking and foveated rendering
processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a
state-of-the-art neural rendering technique. Evaluation results demonstrate
that A3FR can reduce end-to-end rendering latency by up to $2\times$ while
maintaining visual quality.

Comments:
- ACM International Conference on Supercomputing 2025

---

## Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Xiaolei Lang, Jiajun Lv, Kai Tang, Laijian Li, Jianxin Huang, Lina Liu, Yong Liu, Xingxing Zuo | cs.RO | [PDF](http://arxiv.org/pdf/2507.04004v1){: .btn .btn-green } |

**Abstract**: This paper proposes an innovative LiDAR-Inertial-Camera SLAM system with 3D
Gaussian Splatting, which is the first to jointly consider visual quality,
geometric accuracy, and real-time performance. It robustly and accurately
estimates poses while building a photo-realistic 3D Gaussian map in real time
that enables high-quality novel view RGB and depth rendering. To effectively
address under-reconstruction in regions not covered by the LiDAR, we employ a
lightweight zero-shot depth model that synergistically combines RGB appearance
cues with sparse LiDAR measurements to generate dense depth maps. The depth
completion enables reliable Gaussian initialization in LiDAR-blind areas,
significantly improving system applicability for sparse LiDAR sensors. To
enhance geometric accuracy, we use sparse but precise LiDAR depths to supervise
Gaussian map optimization and accelerate it with carefully designed
CUDA-accelerated strategies. Furthermore, we explore how the incrementally
reconstructed Gaussian map can improve the robustness of odometry. By tightly
incorporating photometric constraints from the Gaussian map into the
continuous-time factor graph optimization, we demonstrate improved pose
estimation under LiDAR degradation scenarios. We also showcase downstream
applications via extending our elaborate system, including video frame
interpolation and fast 3D mesh extraction. To support rigorous evaluation, we
construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth
poses, depth maps, and extrapolated trajectories for assessing out-of-sequence
novel view synthesis. Extensive experiments on both public and self-collected
datasets demonstrate the superiority and versatility of our system across LiDAR
sensors with varying sampling densities. Both the dataset and code will be made
publicly available on project page https://xingxingzuo.github.io/gaussian_lic2.



---

## ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic  Urban Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Guile Wu, Dongfeng Bai, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2507.03886v1){: .btn .btn-green } |

**Abstract**: This work focuses on modeling dynamic urban environments for autonomous
driving simulation. Contemporary data-driven methods using neural radiance
fields have achieved photorealistic driving scene modeling, but they suffer
from low rendering efficacy. Recently, some approaches have explored 3D
Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity
reconstruction and real-time rendering. However, these approaches often neglect
to model fine-grained variations between frames and camera viewpoints, leading
to suboptimal results. In this work, we propose a new approach named ArmGS that
exploits composite driving Gaussian splatting with multi-granularity appearance
refinement for autonomous driving scene modeling. The core idea of our approach
is devising a multi-level appearance modeling scheme to optimize a set of
transformation parameters for composite Gaussian refinement from multiple
granularities, ranging from local Gaussian level to global image level and
dynamic actor level. This not only models global scene appearance variations
between frames and camera viewpoints, but also models local fine-grained
changes of background and objects. Extensive experiments on multiple
challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and
VKITTI2, demonstrate the superiority of our approach over the state-of-the-art
methods.

Comments:
- Technical report

---

## Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian  Pointmaps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-04 | Chong Cheng, Sicheng Yu, Zijian Wang, Yifan Zhou, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.03737v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its
high-fidelity and real-time novel view synthesis performance. However, some
previous 3DGS SLAM methods employ a differentiable rendering pipeline for
tracking, \textbf{lack geometric priors} in outdoor scenes. Other approaches
introduce separate tracking modules, but they accumulate errors with
significant camera movement, leading to \textbf{scale drift}. To address these
challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS.
Technically, we establish a self-consistent tracking module anchored in the
3DGS pointmap, which avoids cumulative scale drift and achieves more precise
and robust tracking with fewer iterations. Additionally, we design a
patch-based pointmap dynamic mapping module, which introduces geometric priors
while avoiding scale ambiguity. This significantly enhances tracking accuracy
and the quality of scene reconstruction, making it particularly suitable for
complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV
datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel
view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy.
Project page: https://3dagentworld.github.io/S3PO-GS/.

Comments:
- Accepted by ICCV2025

---

## Gbake: Baking 3D Gaussian Splats into Reflection Probes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Stephen Pasch, Joel K. Salzman, Changxi Zheng | cs.GR | [PDF](http://arxiv.org/pdf/2507.02257v1){: .btn .btn-green } |

**Abstract**: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

Comments:
- SIGGRAPH 2025 Posters

---

## HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity  Animatable Face Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Gent Serifi, Marcel C. Bühler | cs.CV | [PDF](http://arxiv.org/pdf/2507.02803v1){: .btn .btn-green } |

**Abstract**: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

Comments:
- Project page: https://gserifi.github.io/HyperGaussians

---

## ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and  Manipulation of Articulated Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Qiaojun Yu, Xibin Yuan, Yu jiang, Junting Chen, Dongzhe Zheng, Ce Hao, Yang You, Yixing Chen, Yao Mu, Liu Liu, Cewu Lu | cs.RO | [PDF](http://arxiv.org/pdf/2507.02600v1){: .btn .btn-green } |

**Abstract**: Articulated object manipulation remains a critical challenge in robotics due
to the complex kinematic constraints and the limited physical reasoning of
existing methods. In this work, we introduce ArtGS, a novel framework that
extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling
for articulated object understanding and interaction. ArtGS begins with
multi-view RGB-D reconstruction, followed by reasoning with a vision-language
model (VLM) to extract semantic and structural information, particularly the
articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS
optimizes the parameters of the articulated bones, ensuring physically
consistent motion constraints and enhancing the manipulation policy. By
leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and
closed-loop optimization, ArtGS establishes a new framework for efficient,
scalable, and generalizable articulated object modeling and manipulation.
Experiments conducted in both simulation and real-world environments
demonstrate that ArtGS significantly outperforms previous methods in joint
estimation accuracy and manipulation success rates across a variety of
articulated objects. Additional images and videos are available on the project
website: https://sites.google.com/view/artgs/home

Comments:
- Accepted by IROS 2025

---

## LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local  Implicit Feature Decoupling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Jiahao Wu, Rui Peng, Jianbo Jiao, Jiayu Yang, Luyang Tang, Kaiqiang Xiong, Jie Liang, Jinbo Yan, Runling Liu, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.02363v1){: .btn .btn-green } |

**Abstract**: Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamlined local spaces defined by seeds, enabling
global modeling by capturing motion within each local space. 2) We decouple
static and dynamic features for local space motion modeling. A static feature
shared across time steps captures static information, while a dynamic residual
field provides time-specific features. These are combined and decoded to
generate Temporal Gaussians, modeling motion within each local space. As a
result, we propose a novel dynamic scene reconstruction framework to model
highly dynamic real-world scenes more realistically. Our method not only
demonstrates competitive performance on various fine-scale datasets compared to
state-of-the-art (SOTA) methods, but also represents the first attempt to model
larger and more complex highly dynamic scenes. Project page:
https://wujh2001.github.io/LocalDyGS/.

Comments:
- Accepted by ICCV 2025

---

## Tile and Slide : A New Framework for Scaling NeRF from Local to Global  3D Earth Observation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-02 | Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet | cs.CV | [PDF](http://arxiv.org/pdf/2507.01631v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

Comments:
- Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D
  Vision Across Altitudes). Version before camera ready. Our code will be made
  public after the conference

---

## 3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial  Camouflage Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-02 | Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2507.01367v1){: .btn .btn-green } |

**Abstract**: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

Comments:
- Accepted by ICCV 2025

---

## A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale  Reconstruction with External Memory

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Felix Windisch, Lukas Radl, Thomas Köhler, Michael Steiner, Dieter Schmalstieg, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2507.01110v2){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.



---

## Masks make discriminative models great again!


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Tianshi Cao, Marie-Julie Rakotosaona, Ben Poole, Federico Tombari, Michael Niemeyer | cs.CV | [PDF](http://arxiv.org/pdf/2507.00916v1){: .btn .btn-green } |

**Abstract**: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.



---

## VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online  Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Keiko Nagami, Timothy Chen, Javier Yu, Ola Shorinwa, Maximilian Adang, Carlyn Dougherty, Eric Cristofalo, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2507.01125v1){: .btn .btn-green } |

**Abstract**: We present VISTA (Viewpoint-based Image selection with Semantic Task
Awareness), an active exploration method for robots to plan informative
trajectories that improve 3D map quality in areas most relevant for task
completion. Given an open-vocabulary search instruction (e.g., "find a
person"), VISTA enables a robot to explore its environment to search for the
object of interest, while simultaneously building a real-time semantic 3D
Gaussian Splatting reconstruction of the scene. The robot navigates its
environment by planning receding-horizon trajectories that prioritize semantic
similarity to the query and exploration of unseen regions of the environment.
To evaluate trajectories, VISTA introduces a novel, efficient
viewpoint-semantic coverage metric that quantifies both the geometric view
diversity and task relevance in the 3D scene. On static datasets, our coverage
metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in
computation speed and reconstruction quality. In quadrotor hardware
experiments, VISTA achieves 6x higher success rates in challenging maps,
compared to baseline methods, while matching baseline performance in less
challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying
it on a quadrotor drone and a Spot quadruped robot. Open-source code will be
released upon acceptance of the paper.

Comments:
- 9 pages, 4 figures

---

## Surgical Neural Radiance Fields from One Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Alberto Neri, Maximilan Fehrentz, Veronica Penza, Leonardo S. Mattos, Nazim Haouchine | cs.CV | [PDF](http://arxiv.org/pdf/2507.00969v1){: .btn .btn-green } |

**Abstract**: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.



---

## GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And  Dynamic Density Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Xingjun Wang, Lianlei Shan | cs.CV | [PDF](http://arxiv.org/pdf/2507.00363v1){: .btn .btn-green } |

**Abstract**: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.



---

## PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance  point cloud reconstruction via joint-channel NeRF with multi-view image  instance matching

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Xin Yang, Ruiming Du, Hanyang Huang, Jiayang Xie, Pengyao Xie, Leisen Fang, Ziyue Guo, Nanjun Jiang, Yu Jiang, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2507.00371v1){: .btn .btn-green } |

**Abstract**: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.



---

## GaussianVLM: Scene-centric 3D Vision-Language Models using  Language-aligned Gaussian Splats for Embodied Reasoning and Beyond

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang, Danda Pani Paudel, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2507.00886v1){: .btn .btn-green } |

**Abstract**: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.



---

## LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail  Conserved Anti-Aliasing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Zhenya Yang, Bingchen Gong, Kai Chen | cs.CV | [PDF](http://arxiv.org/pdf/2507.00554v2){: .btn .btn-green } |

**Abstract**: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.


