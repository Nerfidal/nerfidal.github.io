---
layout: default
title: July 2025
parent: Papers
nav_order: 202507
---

<!---metadata--->


## Tile and Slide : A New Framework for Scaling NeRF from Local to Global  3D Earth Observation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-02 | Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet | cs.CV | [PDF](http://arxiv.org/pdf/2507.01631v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

Comments:
- Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D
  Vision Across Altitudes). Version before camera ready. Our code will be made
  public after the conference

---

## 3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial  Camouflage Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-02 | Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2507.01367v1){: .btn .btn-green } |

**Abstract**: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

Comments:
- Accepted by ICCV 2025

---

## GaussianVLM: Scene-centric 3D Vision-Language Models using  Language-aligned Gaussian Splats for Embodied Reasoning and Beyond

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang, Danda Pani Paudel, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2507.00886v1){: .btn .btn-green } |

**Abstract**: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.



---

## LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail  Conserved Anti-Aliasing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Zhenya Yang, Bingchen Gong, Kai Chen, Qi Dou | cs.CV | [PDF](http://arxiv.org/pdf/2507.00554v1){: .btn .btn-green } |

**Abstract**: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.



---

## PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance  point cloud reconstruction via joint-channel NeRF with multi-view image  instance matching

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Xin Yang, Ruiming Du, Hanyang Huang, Jiayang Xie, Pengyao Xie, Leisen Fang, Ziyue Guo, Nanjun Jiang, Yu Jiang, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2507.00371v1){: .btn .btn-green } |

**Abstract**: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.



---

## Masks make discriminative models great again!


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Tianshi Cao, Marie-Julie Rakotosaona, Ben Poole, Federico Tombari, Michael Niemeyer | cs.CV | [PDF](http://arxiv.org/pdf/2507.00916v1){: .btn .btn-green } |

**Abstract**: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.



---

## Surgical Neural Radiance Fields from One Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Alberto Neri, Maximilan Fehrentz, Veronica Penza, Leonardo S. Mattos, Nazim Haouchine | cs.CV | [PDF](http://arxiv.org/pdf/2507.00969v1){: .btn .btn-green } |

**Abstract**: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.



---

## A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale  Reconstruction with External Memory

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Felix Windisch, Lukas Radl, Thomas KÃ¶hler, Michael Steiner, Dieter Schmalstieg, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2507.01110v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.



---

## VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online  Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Keiko Nagami, Timothy Chen, Javier Yu, Ola Shorinwa, Maximilian Adang, Carlyn Dougherty, Eric Cristofalo, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2507.01125v1){: .btn .btn-green } |

**Abstract**: We present VISTA (Viewpoint-based Image selection with Semantic Task
Awareness), an active exploration method for robots to plan informative
trajectories that improve 3D map quality in areas most relevant for task
completion. Given an open-vocabulary search instruction (e.g., "find a
person"), VISTA enables a robot to explore its environment to search for the
object of interest, while simultaneously building a real-time semantic 3D
Gaussian Splatting reconstruction of the scene. The robot navigates its
environment by planning receding-horizon trajectories that prioritize semantic
similarity to the query and exploration of unseen regions of the environment.
To evaluate trajectories, VISTA introduces a novel, efficient
viewpoint-semantic coverage metric that quantifies both the geometric view
diversity and task relevance in the 3D scene. On static datasets, our coverage
metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in
computation speed and reconstruction quality. In quadrotor hardware
experiments, VISTA achieves 6x higher success rates in challenging maps,
compared to baseline methods, while matching baseline performance in less
challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying
it on a quadrotor drone and a Spot quadruped robot. Open-source code will be
released upon acceptance of the paper.

Comments:
- 9 pages, 4 figures

---

## GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And  Dynamic Density Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Xingjun Wang, Lianlei Shan | cs.CV | [PDF](http://arxiv.org/pdf/2507.00363v1){: .btn .btn-green } |

**Abstract**: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.


