---
layout: default
title: July 2025
parent: Papers
nav_order: 202507
---

<!---metadata--->


## Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Wijayathunga W. M. R. D. B | cs.GR | [PDF](http://arxiv.org/pdf/2507.07000v1){: .btn .btn-green } |

**Abstract**: We propose a novel framework that enhances non-rigid 3D model deformations by
bridging mesh representations with 3D Gaussian splatting. While traditional
Gaussian splatting delivers fast, real-time radiance-field rendering, its
post-editing capabilities and support for large-scale, non-rigid deformations
remain limited. Our method addresses these challenges by embedding Gaussian
kernels directly onto explicit mesh surfaces. This allows the mesh's inherent
topological and geometric priors to guide intuitive editing operations -- such
as moving, scaling, and rotating individual 3D components -- and enables
complex deformations like bending and stretching. This work paves the way for
more flexible 3D content-creation workflows in applications spanning virtual
reality, character animation, and interactive design.



---

## Photometric Stereo using Gaussian Splatting and inverse rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Matéo Ducastel, David Tschumperlé, Yvain Quéau | eess.IV | [PDF](http://arxiv.org/pdf/2507.06684v1){: .btn .btn-green } |

**Abstract**: Recent state-of-the-art algorithms in photometric stereo rely on neural
networks and operate either through prior learning or inverse rendering
optimization. Here, we revisit the problem of calibrated photometric stereo by
leveraging recent advances in 3D inverse rendering using the Gaussian Splatting
formalism. This allows us to parameterize the 3D scene to be reconstructed and
optimize it in a more interpretable manner. Our approach incorporates a
simplified model for light representation and demonstrates the potential of the
Gaussian Splatting rendering engine for the photometric stereo problem.

Comments:
- in French language. GRETSI 2025, Association GRETSI, Aug 2025,
  Strasbourg, France

---

## FlexGaussian: Flexible and Cost-Effective Training-Free Compression for  3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Boyuan Tian, Qizhe Gao, Siran Xianyu, Xiaotong Cui, Minjia Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2507.06671v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

Comments:
- To appear at ACM MM 2025

---

## ClipGS: Clippable Gaussian Splatting for Interactive Cinematic  Visualization of Volumetric Medical Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Chengkun Li, Yuqi Tong, Kai Chen, Zhenya Yang, Ruiyang Li, Shi Qiu, Jason Ying-Kuen Chan, Pheng-Ann Heng, Qi Dou | cs.CV | [PDF](http://arxiv.org/pdf/2507.06647v1){: .btn .btn-green } |

**Abstract**: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

Comments:
- Early accepted by MICCAI 2025. Project is available at:
  https://med-air.github.io/ClipGS

---

## A Probabilistic Approach to Uncertainty Quantification Leveraging 3D  Geometry

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Rushil Desai, Frederik Warburg, Trevor Darrell, Marissa Ramirez de Chanlatte | cs.CV | [PDF](http://arxiv.org/pdf/2507.06269v1){: .btn .btn-green } |

**Abstract**: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and awareness of fidelity surface
geometric uncertainty are essential. Unlike radiance-based models such as NeRF
or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define
continuous and differentiable geometry, making them better suited for physical
modeling and analysis. BayesSDF leverages a Laplace approximation to quantify
local surface instability via Hessian-based metrics, enabling computationally
efficient, surface-aware uncertainty estimation. Our method shows that
uncertainty predictions correspond closely with poorly reconstructed geometry,
providing actionable confidence measures for downstream use. Extensive
evaluations on synthetic and real-world datasets demonstrate that BayesSDF
outperforms existing methods in both calibration and geometric consistency,
establishing a strong foundation for uncertainty-aware 3D scene reconstruction,
simulation, and robotic decision-making.

Comments:
- ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)

---

## LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for  Panorama-Style Mobile Captures

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Seungoh Han, Jaehoon Jang, Hyunsu Kim, Jaeheung Surh, Junhyung Kwak, Hyowon Ha, Kyungdon Joo | cs.GR | [PDF](http://arxiv.org/pdf/2507.06109v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel
view synthesis (NVS) with impressive quality in indoor scenes. However,
achieving high-fidelity rendering requires meticulously captured images
covering the entire scene, limiting accessibility for general users. We aim to
develop a practical 3DGS-based NVS framework using simple panorama-style motion
with a handheld camera (e.g., mobile device). While convenient, this
rotation-dominant motion and narrow baseline make accurate camera pose and 3D
point estimation challenging, especially in textureless indoor scenes. To
address these challenges, we propose LighthouseGS, a novel framework inspired
by the lighthouse-like sweeping motion of panoramic views. LighthouseGS
leverages rough geometric priors, such as mobile device camera poses and
monocular depth estimation, and utilizes the planar structures often found in
indoor environments. We present a new initialization method called plane
scaffold assembly to generate consistent 3D points on these structures,
followed by a stable pruning strategy to enhance geometry and optimization
stability. Additionally, we introduce geometric and photometric corrections to
resolve inconsistencies from motion drift and auto-exposure in mobile devices.
Tested on collected real and synthetic indoor scenes, LighthouseGS delivers
photorealistic rendering, surpassing state-of-the-art methods and demonstrating
the potential for panoramic view synthesis and object placement.

Comments:
- Preprint

---

## 3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Haitao Lu, Haijier Chen, Haoze Liu, Shoujian Zhang, Bo Xu, Ziao Liu | cs.RO | [PDF](http://arxiv.org/pdf/2507.05661v1){: .btn .btn-green } |

**Abstract**: In autonomous robotic systems, precise localization is a prerequisite for
safe navigation. However, in complex urban environments, GNSS positioning often
suffers from signal occlusion and multipath effects, leading to unreliable
absolute positioning. Traditional mapping approaches are constrained by storage
requirements and computational inefficiency, limiting their applicability to
resource-constrained robotic platforms. To address these challenges, we propose
3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian
Splatting (3DGS), enabling centimeter-level positioning using only a single
monocular RGB image on the client side. We combine multi-sensor data to
construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side
localization requires just a standard camera input. Using SuperPoint and
SuperGlue for feature extraction and matching, our core innovation is an
iterative optimization strategy that refines localization results through
step-by-step rendering, making it suitable for real-time autonomous navigation.
Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves
average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads,
boulevard roads, and traffic-dense highways respectively, significantly
outperforming other representative methods while requiring only monocular RGB
input. This approach provides autonomous robots with reliable localization
capabilities even in challenging urban environments where GNSS fails.

Comments:
- 13 pages,7 figures,4 tables

---

## Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D  Gaussian Splatting for Photorealistic Scenes Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Jiayi Song, Zihan Ye, Qingyuan Zhou, Weidong Yang, Ben Fei, Jingyi Xu, Ying He, Wanli Ouyang | cs.CV | [PDF](http://arxiv.org/pdf/2507.06103v1){: .btn .btn-green } |

**Abstract**: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.



---

## VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Alexandre Symeonidis-Herzig, Özge Mercanoğlu Sincan, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2507.06060v1){: .btn .btn-green } |

**Abstract**: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.



---

## DreamArt: Generating Interactable Articulated Objects from a Single  Image

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Ruijie Lu, Yu Liu, Jiaxiang Tang, Junfeng Ni, Yuxiang Wang, Diwen Wan, Gang Zeng, Yixin Chen, Siyuan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2507.05763v1){: .btn .btn-green } |

**Abstract**: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

Comments:
- Technical Report

---

## D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for  Free-Viewpoint Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Wenkang Zhang, Yan Zhao, Qiang Wang, Li Song, Zhengxue Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2507.05859v1){: .btn .btn-green } |

**Abstract**: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

Comments:
- 12 pages, 9 figures, 8 tables

---

## Mastering Regional 3DGS: Locating, Initializing, and Editing with  Diverse 2D Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-07 | Lanqing Guo, Yufei Wang, Hezhen Hu, Yan Zheng, Yeying Jin, Siyu Huang, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.05426v1){: .btn .btn-green } |

**Abstract**: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.



---

## SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with  Segmented Consistency Trajectory Distillation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-07 | Jiahao Zhu, Zixuan Chen, Guangcong Wang, Xiaohua Xie, Yi Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2507.05256v1){: .btn .btn-green } |

**Abstract**: Recent advancements in text-to-3D generation improve the visual quality of
Score Distillation Sampling (SDS) and its variants by directly connecting
Consistency Distillation (CD) to score distillation. However, due to the
imbalance between self-consistency and cross-consistency, these CD-based
methods inherently suffer from improper conditional guidance, leading to
sub-optimal generation results. To address this issue, we present
SegmentDreamer, a novel framework designed to fully unleash the potential of
consistency models for high-fidelity text-to-3D generation. Specifically, we
reformulate SDS through the proposed Segmented Consistency Trajectory
Distillation (SCTD), effectively mitigating the imbalance issues by explicitly
defining the relationship between self- and cross-consistency. Moreover, SCTD
partitions the Probability Flow Ordinary Differential Equation (PF-ODE)
trajectory into multiple sub-trajectories and ensures consistency within each
segment, which can theoretically provide a significantly tighter upper bound on
distillation error. Additionally, we propose a distillation pipeline for a more
swift and stable generation. Extensive experiments demonstrate that our
SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling
high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).

Comments:
- Accepted by ICCV 2025, project page: https://zjhjojo.github.io/

---

## InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D  Geometry-Consistent Attention Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-07 | Minghao Wen, Shengjie Wu, Kangkan Wang, Dong Liang | cs.CV | [PDF](http://arxiv.org/pdf/2507.04961v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting based 3D editing has demonstrated impressive
performance in recent years. However, the multi-view editing often exhibits
significant local inconsistency, especially in areas of non-rigid deformation,
which lead to local artifacts, texture blurring, or semantic variations in
edited 3D scenes. We also found that the existing editing methods, which rely
entirely on text prompts make the editing process a "one-shot deal", making it
difficult for users to control the editing degree flexibly. In response to
these challenges, we present InterGSEdit, a novel framework for high-quality
3DGS editing via interactively selecting key views with users' preferences. We
propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to
adaptively screen a group of semantically consistent reference views for each
user-selected key view. Then, the cross-attention maps derived from the
reference views are used in a weighted Gaussian Splatting unprojection to
construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project
$GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D
cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive
attention strategy that prioritizes 3D-constrained attention for geometric
consistency during early inference, and gradually prioritizes 2D
cross-attention maps in diffusion for fine-grained features during the later
inference. Extensive experiments demonstrate that InterGSEdit achieves
state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing
with improved user experience.



---

## A View-consistent Sampling Method for Regularized Training of Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-06 | Aoxiang Fan, Corentin Dumery, Nicolas Talabot, Pascal Fua | cs.CV | [PDF](http://arxiv.org/pdf/2507.04408v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene
representation and 3D recovery. To improve its performance on real-world data,
depth regularizations have proven to be the most effective ones. However, depth
estimation models not only require expensive 3D supervision in training, but
also suffer from generalization issues. As a result, the depth estimations can
be erroneous in practice, especially for outdoor unbounded scenes. In this
paper, we propose to employ view-consistent distributions instead of fixed
depth value estimations to regularize NeRF training. Specifically, the
distribution is computed by utilizing both low-level color features and
high-level distilled features from foundation models at the projected 2D
pixel-locations from per-ray sampled 3D points. By sampling from the
view-consistency distributions, an implicit regularization is imposed on the
training of NeRF. We also utilize a depth-pushing loss that works in
conjunction with the sampling technique to jointly provide effective
regularizations for eliminating the failure modes. Extensive experiments
conducted on various scenes from public datasets demonstrate that our proposed
method can generate significantly better novel view synthesis results than
state-of-the-art NeRF variants as well as different depth regularization
methods.

Comments:
- ICCV 2025 accepted

---

## ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic  Urban Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Guile Wu, Dongfeng Bai, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2507.03886v1){: .btn .btn-green } |

**Abstract**: This work focuses on modeling dynamic urban environments for autonomous
driving simulation. Contemporary data-driven methods using neural radiance
fields have achieved photorealistic driving scene modeling, but they suffer
from low rendering efficacy. Recently, some approaches have explored 3D
Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity
reconstruction and real-time rendering. However, these approaches often neglect
to model fine-grained variations between frames and camera viewpoints, leading
to suboptimal results. In this work, we propose a new approach named ArmGS that
exploits composite driving Gaussian splatting with multi-granularity appearance
refinement for autonomous driving scene modeling. The core idea of our approach
is devising a multi-level appearance modeling scheme to optimize a set of
transformation parameters for composite Gaussian refinement from multiple
granularities, ranging from local Gaussian level to global image level and
dynamic actor level. This not only models global scene appearance variations
between frames and camera viewpoints, but also models local fine-grained
changes of background and objects. Extensive experiments on multiple
challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and
VKITTI2, demonstrate the superiority of our approach over the state-of-the-art
methods.

Comments:
- Technical report

---

## A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated  Rendering in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Shuo Xin, Haiyu Wang, Sai Qian Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2507.04147v1){: .btn .btn-green } |

**Abstract**: Virtual reality (VR) significantly transforms immersive digital interfaces,
greatly enhancing education, professional practices, and entertainment by
increasing user engagement and opening up new possibilities in various
industries. Among its numerous applications, image rendering is crucial.
Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high
computational demands, driven predominantly by user expectations for superior
visual quality. This results in notable processing delays for real-time image
rendering, which greatly affects the user experience. Additionally, VR devices
such as head-mounted displays (HMDs) are intricately linked to human visual
behavior, leveraging knowledge from perception and cognition to improve user
experience. These insights have spurred the development of foveated rendering,
a technique that dynamically adjusts rendering resolution based on the user's
gaze direction. The resultant solution, known as gaze-tracked foveated
rendering, significantly reduces the computational burden of the rendering
process.
  Although gaze-tracked foveated rendering can reduce rendering costs, the
computational overhead of the gaze tracking process itself can sometimes
outweigh the rendering savings, leading to increased processing latency. To
address this issue, we propose an efficient rendering framework
called~\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated
rendering via the parallelization of gaze tracking and foveated rendering
processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a
state-of-the-art neural rendering technique. Evaluation results demonstrate
that A3FR can reduce end-to-end rendering latency by up to $2\times$ while
maintaining visual quality.

Comments:
- ACM International Conference on Supercomputing 2025

---

## Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Xiaolei Lang, Jiajun Lv, Kai Tang, Laijian Li, Jianxin Huang, Lina Liu, Yong Liu, Xingxing Zuo | cs.RO | [PDF](http://arxiv.org/pdf/2507.04004v2){: .btn .btn-green } |

**Abstract**: This paper presents the first photo-realistic LiDAR-Inertial-Camera Gaussian
Splatting SLAM system that simultaneously addresses visual quality, geometric
accuracy, and real-time performance. The proposed method performs robust and
accurate pose estimation within a continuous-time trajectory optimization
framework, while incrementally reconstructing a 3D Gaussian map using camera
and LiDAR data, all in real time. The resulting map enables high-quality,
real-time novel view rendering of both RGB images and depth maps. To
effectively address under-reconstruction in regions not covered by the LiDAR,
we employ a lightweight zero-shot depth model that synergistically combines RGB
appearance cues with sparse LiDAR measurements to generate dense depth maps.
The depth completion enables reliable Gaussian initialization in LiDAR-blind
areas, significantly improving system applicability for sparse LiDAR sensors.
To enhance geometric accuracy, we use sparse but precise LiDAR depths to
supervise Gaussian map optimization and accelerate it with carefully designed
CUDA-accelerated strategies. Furthermore, we explore how the incrementally
reconstructed Gaussian map can improve the robustness of odometry. By tightly
incorporating photometric constraints from the Gaussian map into the
continuous-time factor graph optimization, we demonstrate improved pose
estimation under LiDAR degradation scenarios. We also showcase downstream
applications via extending our elaborate system, including video frame
interpolation and fast 3D mesh extraction. To support rigorous evaluation, we
construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth
poses, depth maps, and extrapolated trajectories for assessing out-of-sequence
novel view synthesis. Both the dataset and code will be made publicly available
on project page https://xingxingzuo.github.io/gaussian_lic2.



---

## Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian  Pointmaps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-04 | Chong Cheng, Sicheng Yu, Zijian Wang, Yifan Zhou, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.03737v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its
high-fidelity and real-time novel view synthesis performance. However, some
previous 3DGS SLAM methods employ a differentiable rendering pipeline for
tracking, \textbf{lack geometric priors} in outdoor scenes. Other approaches
introduce separate tracking modules, but they accumulate errors with
significant camera movement, leading to \textbf{scale drift}. To address these
challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS.
Technically, we establish a self-consistent tracking module anchored in the
3DGS pointmap, which avoids cumulative scale drift and achieves more precise
and robust tracking with fewer iterations. Additionally, we design a
patch-based pointmap dynamic mapping module, which introduces geometric priors
while avoiding scale ambiguity. This significantly enhances tracking accuracy
and the quality of scene reconstruction, making it particularly suitable for
complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV
datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel
view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy.
Project page: https://3dagentworld.github.io/S3PO-GS/.

Comments:
- Accepted by ICCV2025

---

## ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and  Manipulation of Articulated Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Qiaojun Yu, Xibin Yuan, Yu jiang, Junting Chen, Dongzhe Zheng, Ce Hao, Yang You, Yixing Chen, Yao Mu, Liu Liu, Cewu Lu | cs.RO | [PDF](http://arxiv.org/pdf/2507.02600v1){: .btn .btn-green } |

**Abstract**: Articulated object manipulation remains a critical challenge in robotics due
to the complex kinematic constraints and the limited physical reasoning of
existing methods. In this work, we introduce ArtGS, a novel framework that
extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling
for articulated object understanding and interaction. ArtGS begins with
multi-view RGB-D reconstruction, followed by reasoning with a vision-language
model (VLM) to extract semantic and structural information, particularly the
articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS
optimizes the parameters of the articulated bones, ensuring physically
consistent motion constraints and enhancing the manipulation policy. By
leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and
closed-loop optimization, ArtGS establishes a new framework for efficient,
scalable, and generalizable articulated object modeling and manipulation.
Experiments conducted in both simulation and real-world environments
demonstrate that ArtGS significantly outperforms previous methods in joint
estimation accuracy and manipulation success rates across a variety of
articulated objects. Additional images and videos are available on the project
website: https://sites.google.com/view/artgs/home

Comments:
- Accepted by IROS 2025

---

## LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local  Implicit Feature Decoupling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Jiahao Wu, Rui Peng, Jianbo Jiao, Jiayu Yang, Luyang Tang, Kaiqiang Xiong, Jie Liang, Jinbo Yan, Runling Liu, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.02363v1){: .btn .btn-green } |

**Abstract**: Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamlined local spaces defined by seeds, enabling
global modeling by capturing motion within each local space. 2) We decouple
static and dynamic features for local space motion modeling. A static feature
shared across time steps captures static information, while a dynamic residual
field provides time-specific features. These are combined and decoded to
generate Temporal Gaussians, modeling motion within each local space. As a
result, we propose a novel dynamic scene reconstruction framework to model
highly dynamic real-world scenes more realistically. Our method not only
demonstrates competitive performance on various fine-scale datasets compared to
state-of-the-art (SOTA) methods, but also represents the first attempt to model
larger and more complex highly dynamic scenes. Project page:
https://wujh2001.github.io/LocalDyGS/.

Comments:
- Accepted by ICCV 2025

---

## Gbake: Baking 3D Gaussian Splats into Reflection Probes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Stephen Pasch, Joel K. Salzman, Changxi Zheng | cs.GR | [PDF](http://arxiv.org/pdf/2507.02257v1){: .btn .btn-green } |

**Abstract**: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

Comments:
- SIGGRAPH 2025 Posters

---

## HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity  Animatable Face Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Gent Serifi, Marcel C. Bühler | cs.CV | [PDF](http://arxiv.org/pdf/2507.02803v2){: .btn .btn-green } |

**Abstract**: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

Comments:
- Project page: https://gserifi.github.io/HyperGaussians, Code:
  https://github.com/gserifi/HyperGaussians

---

## Tile and Slide : A New Framework for Scaling NeRF from Local to Global  3D Earth Observation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-02 | Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet | cs.CV | [PDF](http://arxiv.org/pdf/2507.01631v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

Comments:
- Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D
  Vision Across Altitudes). Version before camera ready. Our code will be made
  public after the conference

---

## 3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial  Camouflage Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-02 | Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2507.01367v1){: .btn .btn-green } |

**Abstract**: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

Comments:
- Accepted by ICCV 2025

---

## GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And  Dynamic Density Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Xingjun Wang, Lianlei Shan | cs.CV | [PDF](http://arxiv.org/pdf/2507.00363v1){: .btn .btn-green } |

**Abstract**: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.



---

## LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail  Conserved Anti-Aliasing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Zhenya Yang, Bingchen Gong, Kai Chen | cs.CV | [PDF](http://arxiv.org/pdf/2507.00554v2){: .btn .btn-green } |

**Abstract**: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.



---

## GaussianVLM: Scene-centric 3D Vision-Language Models using  Language-aligned Gaussian Splats for Embodied Reasoning and Beyond

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang, Danda Pani Paudel, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2507.00886v1){: .btn .btn-green } |

**Abstract**: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.



---

## PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance  point cloud reconstruction via joint-channel NeRF with multi-view image  instance matching

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Xin Yang, Ruiming Du, Hanyang Huang, Jiayang Xie, Pengyao Xie, Leisen Fang, Ziyue Guo, Nanjun Jiang, Yu Jiang, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2507.00371v1){: .btn .btn-green } |

**Abstract**: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.



---

## Surgical Neural Radiance Fields from One Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Alberto Neri, Maximilan Fehrentz, Veronica Penza, Leonardo S. Mattos, Nazim Haouchine | cs.CV | [PDF](http://arxiv.org/pdf/2507.00969v1){: .btn .btn-green } |

**Abstract**: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.



---

## A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale  Reconstruction with External Memory

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Felix Windisch, Lukas Radl, Thomas Köhler, Michael Steiner, Dieter Schmalstieg, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2507.01110v2){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.



---

## Masks make discriminative models great again!


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Tianshi Cao, Marie-Julie Rakotosaona, Ben Poole, Federico Tombari, Michael Niemeyer | cs.CV | [PDF](http://arxiv.org/pdf/2507.00916v1){: .btn .btn-green } |

**Abstract**: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.



---

## VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online  Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Keiko Nagami, Timothy Chen, Javier Yu, Ola Shorinwa, Maximilian Adang, Carlyn Dougherty, Eric Cristofalo, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2507.01125v1){: .btn .btn-green } |

**Abstract**: We present VISTA (Viewpoint-based Image selection with Semantic Task
Awareness), an active exploration method for robots to plan informative
trajectories that improve 3D map quality in areas most relevant for task
completion. Given an open-vocabulary search instruction (e.g., "find a
person"), VISTA enables a robot to explore its environment to search for the
object of interest, while simultaneously building a real-time semantic 3D
Gaussian Splatting reconstruction of the scene. The robot navigates its
environment by planning receding-horizon trajectories that prioritize semantic
similarity to the query and exploration of unseen regions of the environment.
To evaluate trajectories, VISTA introduces a novel, efficient
viewpoint-semantic coverage metric that quantifies both the geometric view
diversity and task relevance in the 3D scene. On static datasets, our coverage
metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in
computation speed and reconstruction quality. In quadrotor hardware
experiments, VISTA achieves 6x higher success rates in challenging maps,
compared to baseline methods, while matching baseline performance in less
challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying
it on a quadrotor drone and a Spot quadruped robot. Open-source code will be
released upon acceptance of the paper.

Comments:
- 9 pages, 4 figures
