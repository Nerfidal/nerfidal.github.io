---
layout: default
title: July 2025
parent: Papers
nav_order: 202507
---

<!---metadata--->


## Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-24 | Chong Cheng, Zijian Wang, Sicheng Yu, Yu Hu, Nanjie Yao, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.18541v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D
representation. Its effectiveness largely depends on precise camera poses and
accurate point cloud initialization, which are often derived from pretrained
Multi-View Stereo (MVS) models. However, in unposed reconstruction task from
hundreds of outdoor images, existing MVS models may struggle with memory limits
and lose accuracy as the number of input images grows. To address this
limitation, we propose a novel unposed 3DGS reconstruction framework that
integrates pretrained MVS priors with the probabilistic Procrustes mapping
strategy. The method partitions input images into subsets, maps submaps into a
global space, and jointly optimizes geometry and poses with 3DGS. Technically,
we formulate the mapping of tens of millions of point clouds as a probabilistic
Procrustes problem and solve a closed-form alignment. By employing
probabilistic coupling along with a soft dustbin mechanism to reject uncertain
correspondences, our method globally aligns point clouds and poses within
minutes across hundreds of images. Moreover, we propose a joint optimization
framework for 3DGS and camera poses. It constructs Gaussians from
confidence-aware anchor points and integrates 3DGS differentiable rendering
with an analytical Jacobian to jointly refine scene and poses, enabling
accurate reconstruction and pose estimation. Experiments on Waymo and KITTI
datasets show that our method achieves accurate reconstruction from unposed
image sequences, setting a new state of the art for unposed 3DGS
reconstruction.



---

## GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-24 | SeungJun Moon, Hah Min Lew, Seungeun Lee, Ji-Su Kang, Gyeong-Moon Park | cs.GR | [PDF](http://arxiv.org/pdf/2507.18155v1){: .btn .btn-green } |

**Abstract**: Despite recent progress in 3D head avatar generation, balancing identity
preservation, i.e., reconstruction, with novel poses and expressions, i.e.,
animation, remains a challenge. Existing methods struggle to adapt Gaussians to
varying geometrical deviations across facial regions, resulting in suboptimal
quality. To address this, we propose GeoAvatar, a framework for adaptive
geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation
Stage (APS), an unsupervised method that segments Gaussians into rigid and
flexible sets for adaptive offset regularization. Then, based on mouth anatomy
and dynamics, we introduce a novel mouth structure and the part-wise
deformation strategy to enhance the animation fidelity of the mouth. Finally,
we propose a regularization loss for precise rigging between Gaussians and 3DMM
faces. Moreover, we release DynamicFace, a video dataset with highly expressive
facial motions. Extensive experiments show the superiority of GeoAvatar
compared to state-of-the-art methods in reconstruction and novel animation
scenarios.

Comments:
- ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/

---

## CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-24 | Haoran Xu, Saining Zhang, Peishuo Li, Baijun Ye, Xiaoxue Chen, Huan-ang Gao, Jv Zheng, Xiaowei Song, Ziqiao Peng, Run Miao, Jinrang Jia, Yifeng Shi, Guangqi Yi, Hang Zhao, Hao Tang, Hongyang Li, Kaicheng Yu, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2507.18473v1){: .btn .btn-green } |

**Abstract**: Vehicle-to-everything (V2X) communication plays a crucial role in autonomous
driving, enabling cooperation between vehicles and infrastructure. While
simulation has significantly contributed to various autonomous driving tasks,
its potential for data generation and augmentation in V2X scenarios remains
underexplored. In this paper, we introduce CRUISE, a comprehensive
reconstruction-and-synthesis framework designed for V2X driving environments.
CRUISE employs decomposed Gaussian Splatting to accurately reconstruct
real-world scenes while supporting flexible editing. By decomposing dynamic
traffic participants into editable Gaussian representations, CRUISE allows for
seamless modification and augmentation of driving scenes. Furthermore, the
framework renders images from both ego-vehicle and infrastructure views,
enabling large-scale V2X dataset augmentation for training and evaluation. Our
experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X
driving scenes with high fidelity; 2) using CRUISE improves 3D detection across
ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D
tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates
challenging corner cases.

Comments:
- IROS 2025, Code: https://github.com/SainingZhang/CRUISE

---

## High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency  and photorealistic details

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-24 | Jun Zhou, Dinghao Li, Nannan Li, Mingjie Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.18023v1){: .btn .btn-green } |

**Abstract**: Recent advancements in multi-view 3D reconstruction and novel-view synthesis,
particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS), have greatly enhanced the fidelity and efficiency of 3D content
creation. However, inpainting 3D scenes remains a challenging task due to the
inherent irregularity of 3D structures and the critical need for maintaining
multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting
framework that reconstructs complete 3D scenes by leveraging sparse inpainted
views. Our framework incorporates an automatic Mask Refinement Process and
region-wise Uncertainty-guided Optimization. Specifically, we refine the
inpainting mask using a series of operations, including Gaussian scene
filtering and back-projection, enabling more accurate localization of occluded
regions and realistic boundary restoration. Furthermore, our Uncertainty-guided
Fine-grained Optimization strategy, which estimates the importance of each
region across multi-view images during training, alleviates multi-view
inconsistencies and enhances the fidelity of fine details in the inpainted
results. Comprehensive experiments conducted on diverse datasets demonstrate
that our approach outperforms existing state-of-the-art methods in both visual
quality and view consistency.



---

## MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D  Content Creation from a Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-24 | Xiaotian Chen, DongFu Yin, Fei Richard Yu, Xuanchen Li, Xinhao Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2507.18371v1){: .btn .btn-green } |

**Abstract**: Advances in generative modeling have significantly enhanced digital content
creation, extending from 2D images to complex 3D and 4D scenes. Despite
substantial progress, producing high-fidelity and temporally consistent dynamic
4D content remains a challenge. In this paper, we propose MVG4D, a novel
framework that generates dynamic 4D content from a single still image by
combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,
MVG4D employs an image matrix module that synthesizes temporally coherent and
spatially diverse multi-view images, providing rich supervisory signals for
downstream 3D and 4D reconstruction. These multi-view images are used to
optimize a 3D Gaussian point cloud, which is further extended into the temporal
domain via a lightweight deformation network. Our method effectively enhances
temporal consistency, geometric fidelity, and visual realism, addressing key
challenges in motion discontinuity and background degradation that affect prior
4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate
that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and
time efficiency. Notably, it reduces flickering artifacts and sharpens
structural details across views and time, enabling more immersive AR/VR
experiences. MVG4D sets a new direction for efficient and controllable 4D
generation from minimal inputs.



---

## PS-GS: Gaussian Splatting for Multi-View Photometric Stereo

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-24 | Yixiao Chen, Bin Liang, Hanzhi Guo, Yongqing Cheng, Jiayi Zhao, Dongdong Weng | cs.GR | [PDF](http://arxiv.org/pdf/2507.18231v1){: .btn .btn-green } |

**Abstract**: Integrating inverse rendering with multi-view photometric stereo (MVPS)
yields more accurate 3D reconstructions than the inverse rendering approaches
that rely on fixed environment illumination. However, efficient inverse
rendering with MVPS remains challenging. To fill this gap, we introduce the
Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently
and jointly estimates the geometry, materials, and lighting of the object that
is illuminated by diverse directional lights (multi-light). Our method first
reconstructs a standard 2D Gaussian splatting model as the initial geometry.
Based on the initialization model, it then proceeds with the deferred inverse
rendering by the full rendering equation containing a lighting-computing
multi-layer perceptron. During the whole optimization, we regularize the
rendered normal maps by the uncalibrated photometric stereo estimated normals.
We also propose the 2D Gaussian ray-tracing for single directional light to
refine the incident lighting. The regularizations and the use of multi-view and
multi-light images mitigate the ill-posed problem of inverse rendering. After
optimization, the reconstructed object can be used for novel-view synthesis,
relighting, and material and shape editing. Experiments on both synthetic and
real datasets demonstrate that our method outperforms prior works in terms of
reconstruction accuracy and computational efficiency.



---

## G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-24 | Gyuhyeon Pak, Hae Min Cho, Euntai Kim | cs.RO | [PDF](http://arxiv.org/pdf/2507.18344v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting
SLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D
reconstruction and robust camera pose tracking in real-time by representing
each scene element using a Gaussian distribution constrained to the local
tangent plane. This effectively models the local surface as a 2D Gaussian disk
aligned with the underlying geometry, leading to more consistent depth
interpretation across multiple viewpoints compared to conventional 3D
ellipsoid-based representations with isotropic uncertainty. To integrate this
representation into the SLAM pipeline, we embed the surface-aligned Gaussian
disks into a Generalized ICP framework by introducing anisotropic covariance
prior without altering the underlying registration formulation. Furthermore we
propose a geometry-aware loss that supervises photometric, depth, and normal
consistency. Our system achieves real-time operation while preserving both
visual and geometric fidelity. Extensive experiments on the Replica and
TUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems
in terms of localization accuracy, reconstruction completeness, while
maintaining the rendering quality.

Comments:
- 8 pages, 6 figures

---

## Exploring Active Learning for Label-Efficient Training of Semantic  Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-23 | Yuzhe Zhu, Lile Cai, Kangkang Lu, Fayao Liu, Xulei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2507.17351v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) models are implicit neural scene representation
methods that offer unprecedented capabilities in novel view synthesis.
Semantically-aware NeRFs not only capture the shape and radiance of a scene,
but also encode semantic information of the scene. The training of
semantically-aware NeRFs typically requires pixel-level class labels, which can
be prohibitively expensive to collect. In this work, we explore active learning
as a potential solution to alleviate the annotation burden. We investigate
various design choices for active learning of semantically-aware NeRF,
including selection granularity and selection strategies. We further propose a
novel active learning strategy that takes into account 3D geometric constraints
in sample selection. Our experiments demonstrate that active learning can
effectively reduce the annotation cost of training semantically-aware NeRF,
achieving more than 2X reduction in annotation cost compared to random
sampling.

Comments:
- Accepted to ICME 2025

---

## Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-23 | Hyeongmin Lee, Kyungjune Baek | cs.GR | [PDF](http://arxiv.org/pdf/2507.17336v1){: .btn .btn-green } |

**Abstract**: Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed
rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric
videos. However, the large number of Gaussians, substantial temporal
redundancies, and especially the absence of an entropy-aware compression
framework result in large storage requirements. Consequently, this poses
significant challenges for practical deployment, efficient edge-device
processing, and data transmission. In this paper, we introduce a novel
end-to-end RD-optimized compression framework tailored for 4DGS, aiming to
enable flexible, high-fidelity rendering across varied computational platforms.
Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the
state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS
compression methods for compatibility while effectively addressing additional
challenges introduced by the temporal axis. In particular, instead of storing
motion trajectories independently per point, we employ a wavelet transform to
reflect the real-world smoothness prior, significantly enhancing storage
efficiency. This approach yields significantly improved compression ratios and
provides a user-controlled balance between compression efficiency and rendering
quality. Extensive experiments demonstrate the effectiveness of our method,
achieving up to 91x compression compared to the original Ex4DGS model while
maintaining high visual fidelity. These results highlight the applicability of
our framework for real-time dynamic scene rendering in diverse scenarios, from
resource-constrained edge devices to high-performance environments.

Comments:
- 21 pages, 10 figures

---

## StreamME: Simplify 3D Gaussian Avatar within Live Stream

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-22 | Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu | cs.GR | [PDF](http://arxiv.org/pdf/2507.17029v1){: .btn .btn-green } |

**Abstract**: We propose StreamME, a method focuses on fast 3D avatar reconstruction. The
StreamME synchronously records and reconstructs a head avatar from live video
streams without any pre-cached data, enabling seamless integration of the
reconstructed appearance into downstream applications. This exceptionally fast
training strategy, which we refer to as on-the-fly training, is central to our
approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating
the reliance on MLPs in deformable 3DGS and relying solely on geometry, which
significantly improves the adaptation speed to facial expression. To further
ensure high efficiency in on-the-fly training, we introduced a simplification
strategy based on primary points, which distributes the point clouds more
sparsely across the facial surface, optimizing points number while maintaining
rendering quality. Leveraging the on-the-fly training capabilities, our method
protects the facial privacy and reduces communication bandwidth in VR system or
online conference. Additionally, it can be directly applied to downstream
application such as animation, toonify, and relighting. Please refer to our
project page for more details: https://songluchuan.github.io/StreamME/.

Comments:
- 12 pages, 15 Figures

---

## LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence  Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-22 | Guichen Huang, Ruoyu Wang, Xiangjun Gao, Che Sun, Yuwei Wu, Shenghua Gao, Yunde Jia | cs.CV | [PDF](http://arxiv.org/pdf/2507.16144v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its
application to online long-sequence scenarios is still limited. Existing
methods either rely on slow per-scene optimization or fail to provide efficient
incremental updates, hindering continuous performance. In this paper, we
propose LongSplat, an online real-time 3D Gaussian reconstruction framework
designed for long-sequence image input. The core idea is a streaming update
mechanism that incrementally integrates current-view observations while
selectively compressing redundant historical Gaussians. Crucial to this
mechanism is our Gaussian-Image Representation (GIR), a representation that
encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR
simultaneously enables efficient fusion of current-view and historical
Gaussians and identity-aware redundancy compression. These functions enable
online reconstruction and adapt the model to long sequences without
overwhelming memory or computational costs. Furthermore, we leverage an
existing image compression method to guide the generation of more compact and
higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat
achieves state-of-the-art efficiency-quality trade-offs in real-time novel view
synthesis, delivering real-time reconstruction while reducing Gaussian counts
by 44\% compared to existing per-pixel Gaussian prediction methods.



---

## EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent  Diffusion


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-22 | Shang Liu, Chenjie Cao, Chaohui Yu, Wen Qian, Jing Wang, Fan Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.16535v2){: .btn .btn-green } |

**Abstract**: Despite the remarkable developments achieved by recent 3D generation works,
scaling these methods to geographic extents, such as modeling thousands of
square kilometers of Earth's surface, remains an open challenge. We address
this through a dual innovation in data infrastructure and model architecture.
First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,
consisting of 50k curated scenes (each measuring 600m x 600m) captured across
the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene
provides pose-annotated multi-view images, depth maps, normals, semantic
segmentation, and camera poses, with explicit quality control to ensure terrain
diversity. Building on this foundation, we propose EarthCrafter, a tailored
framework for large-scale 3D Earth generation via sparse-decoupled latent
diffusion. Our architecture separates structural and textural generation: 1)
Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D
Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the
costly computation suffering from vast geographic scales while preserving
critical information. 2) We propose condition-aware flow matching models
trained on mixed inputs (semantics, images, or neither) to flexibly model
latent geometry and texture features independently. Extensive experiments
demonstrate that EarthCrafter performs substantially better in extremely
large-scale generation. The framework further supports versatile applications,
from semantic-guided urban layout generation to unconditional terrain
synthesis, while maintaining geographic plausibility through our rich data
priors from Aerial-Earth3D. Our project page is available at
https://whiteinblue.github.io/earthcrafter/

Comments:
- Models and codes will be released at this https URL:
  https://github.com/whiteinblue/EarthCrafter

---

## Sparse-View 3D Reconstruction: Recent Advances and Open Challenges

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-22 | Tanveer Younis, Zhanglin Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2507.16406v1){: .btn .btn-green } |

**Abstract**: Sparse-view 3D reconstruction is essential for applications in which dense
image acquisition is impractical, such as robotics, augmented/virtual reality
(AR/VR), and autonomous systems. In these settings, minimal image overlap
prevents reliable correspondence matching, causing traditional methods, such as
structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey
reviews the latest advances in neural implicit models (e.g., NeRF and its
regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian
Splatting), and hybrid frameworks that leverage priors from diffusion and
vision foundation models (VFMs).We analyze how geometric regularization,
explicit shape modeling, and generative inference are used to mitigate
artifacts such as floaters and pose ambiguities in sparse-view settings.
Comparative results on standard benchmarks reveal key trade-offs between the
reconstruction accuracy, efficiency, and generalization. Unlike previous
reviews, our survey provides a unified perspective on geometry-based, neural
implicit, and generative (diffusion-based) methods. We highlight the persistent
challenges in domain generalization and pose-free reconstruction and outline
future directions for developing 3D-native generative priors and achieving
real-time, unconstrained sparse-view reconstruction.

Comments:
- 30 pages, 6 figures

---

## Appearance Harmonization via Bilateral Grid Prediction with Transformers  for 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-21 | Jisu Shin, Richard Shaw, Seunghyun Shin, Anton Pelykh, Zhensong Zhang, Hae-Gon Jeon, Eduardo Perez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2507.15748v1){: .btn .btn-green } |

**Abstract**: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

Comments:
- 10 pages, 3 figures, NeurIPS 2025 under review

---

## SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-21 | Zihui Gao, Jia-Wang Bian, Guosheng Lin, Hao Chen, Chunhua Shen | cs.CV | [PDF](http://arxiv.org/pdf/2507.15602v1){: .btn .btn-green } |

**Abstract**: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.



---

## ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-21 | Ruijie Zhu, Mulin Yu, Linning Xu, Lihan Jiang, Yixuan Li, Tianzhu Zhang, Jiangmiao Pang, Bo Dai | cs.GR | [PDF](http://arxiv.org/pdf/2507.15454v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

Comments:
- Accepted by ICCV 2025

---

## GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage  Conditional Processing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-21 | Minnan Pei, Gang Li, Junwen Si, Zeyu Zhu, Zitao Mo, Peisong Wang, Zhuoran Song, Xiaoyao Liang, Jian Cheng | cs.AR | [PDF](http://arxiv.org/pdf/2507.15300v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering
technique for high-fidelity view synthesis, prompting the development of
dedicated 3DGS accelerators for mobile applications. Through in-depth analysis,
we identify two major limitations in the conventional decoupled
preprocessing-rendering dataflow adopted by existing accelerators: 1) a
significant portion of preprocessed Gaussians are not used in rendering, and 2)
the same Gaussian gets repeatedly loaded across different tile renderings,
resulting in substantial computational and data movement overhead. To address
these issues, we propose GCC, a novel accelerator designed for fast and
energy-efficient 3DGS inference. At the dataflow level, GCC introduces: 1)
cross-stage conditional processing, which interleaves preprocessing and
rendering to dynamically skip unnecessary Gaussian preprocessing; and 2)
Gaussian-wise rendering, ensuring that all rendering operations for a given
Gaussian are completed before moving to the next, thereby eliminating
duplicated Gaussian loading. We also propose an alpha-based boundary
identification method to derive compact and accurate Gaussian regions, thereby
reducing rendering costs. We implement our GCC accelerator in 28nm technology.
Extensive experiments demonstrate that GCC significantly outperforms the
state-of-the-art 3DGS inference accelerator, GSCore, in both performance and
energy efficiency.



---

## Gaussian Splatting with Discretized SDF for Relightable Assets

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-21 | Zuo-Liang Zhu, Jian Yang, Beibei Wang | cs.GR | [PDF](http://arxiv.org/pdf/2507.15629v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.



---

## DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-21 | Hung Nguyen, Runfa Li, An Le, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2507.15690v1){: .btn .btn-green } |

**Abstract**: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

Comments:
- 6 pages, 4 figures

---

## Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization  for Remote Sensing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-21 | Boni Hu, Zhenyu Xia, Lin Chen, Pengcheng Han, Shuhui Bu | cs.CV | [PDF](http://arxiv.org/pdf/2507.15683v1){: .btn .btn-green } |

**Abstract**: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

Comments:
- 17 pages, 11 figures

---

## Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian  Splatting Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-20 | Xiufeng Huang, Ka Chun Cheung, Runmin Cong, Simon See, Renjie Wan | cs.CV | [PDF](http://arxiv.org/pdf/2507.14921v1){: .btn .btn-green } |

**Abstract**: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

Comments:
- ACMMM2025. Non-camera-ready version

---

## Adaptive 3D Gaussian Splatting Video Streaming

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-19 | Han Gong, Qiyue Li, Zhi Liu, Hao Zhou, Peng Yuan Zhou, Zhu Li, Jie Li | cs.CV | [PDF](http://arxiv.org/pdf/2507.14432v1){: .btn .btn-green } |

**Abstract**: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.



---

## Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-19 | Jiahui Zhang, Yuelei Li, Anpei Chen, Muyu Xu, Kunhao Liu, Jianyuan Wang, Xiao-Xiao Long, Hanxue Liang, Zexiang Xu, Hao Su, Christian Theobalt, Christian Rupprecht, Andrea Vedaldi, Hanspeter Pfister, Shijian Lu, Fangneng Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2507.14501v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

Comments:
- A project page associated with this survey is available at
  https://fnzhan.com/projects/Feed-Forward-3D

---

## DCHM: Depth-Consistent Human Modeling for Multiview Detection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-19 | Jiahao Ma, Tianyu Wang, Miaomiao Liu, David Ahmedt-Aristizabal, Chuong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2507.14505v1){: .btn .btn-green } |

**Abstract**: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

Comments:
- multi-view detection, sparse-view reconstruction

---

## DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary  queries in NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-19 | Doriand Petit, Steve Bourgeois, Vincent Gay-Bellile, Florian Chabot, Lo√Øc Barthe | cs.CV | [PDF](http://arxiv.org/pdf/2507.14596v1){: .btn .btn-green } |

**Abstract**: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

Comments:
- Published at ICCV'25

---

## Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware  Tiling and Meta-Learning-Based Bitrate Adaptation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-19 | Han Gong, Qiyue Li, Jie Li, Zhi Liu | cs.CV | [PDF](http://arxiv.org/pdf/2507.14454v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.



---

## Neural-GASh: A CGA-based neural radiance prediction pipeline for  real-time shading


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-18 | Efstratios Geronikolakis, Manos Kamarianakis, Antonis Protopsaltis, George Papagiannakis | cs.GR | [PDF](http://arxiv.org/pdf/2507.13917v1){: .btn .btn-green } |

**Abstract**: This paper presents Neural-GASh, a novel real-time shading pipeline for 3D
meshes, that leverages a neural radiance field architecture to perform
image-based rendering (IBR) using Conformal Geometric Algebra (CGA)-encoded
vertex information as input. Unlike traditional Precomputed Radiance Transfer
(PRT) methods, that require expensive offline precomputations, our learned
model directly consumes CGA-based representations of vertex positions and
normals, enabling dynamic scene shading without precomputation. Integrated
seamlessly into the Unity engine, Neural-GASh facilitates accurate shading of
animated and deformed 3D meshes - capabilities essential for dynamic,
interactive environments. The shading of the scene is implemented within Unity,
where rotation of scene lights in terms of Spherical Harmonics is also
performed optimally using CGA. This neural field approach is designed to
deliver fast and efficient light transport simulation across diverse platforms,
including mobile and VR, while preserving high rendering quality. Additionally,
we evaluate our method on scenes generated via 3D Gaussian splats, further
demonstrating the flexibility and robustness of Neural-GASh in diverse
scenarios. Performance is evaluated in comparison to conventional PRT,
demonstrating competitive rendering speeds even with complex geometries.

Comments:
- 11 pages, 10 figures

---

## EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D  Avatar Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-18 | Seungjun Moon, Sangjoon Yu, Gyeong-Moon Park | cs.CV | [PDF](http://arxiv.org/pdf/2507.13648v1){: .btn .btn-green } |

**Abstract**: The rapid advancement of neural radiance fields (NeRF) has paved the way to
generate animatable human avatars from a monocular video. However, the sole
usage of NeRF suffers from a lack of details, which results in the emergence of
hybrid representation that utilizes SMPL-based mesh together with NeRF
representation. While hybrid-based models show photo-realistic human avatar
generation qualities, they suffer from extremely slow inference due to their
deformation scheme: to be aligned with the mesh, hybrid-based models use the
deformation based on SMPL skinning weights, which needs high computational
costs on each sampled point. We observe that since most of the sampled points
are located in empty space, they do not affect the generation quality but
result in inference latency with deformation. In light of this observation, we
propose EPSilon, a hybrid-based 3D avatar generation scheme with novel
efficient point sampling strategies that boost both training and inference. In
EPSilon, we propose two methods to omit empty points at rendering; empty ray
omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that
progress through the empty space. Then, EIO narrows down the sampling interval
on the ray, which wipes out the region not occupied by either clothes or mesh.
The delicate sampling scheme of EPSilon enables not only great computational
cost reduction during deformation but also the designation of the important
regions to be sampled, which enables a single-stage NeRF structure without
hierarchical sampling. Compared to existing methods, EPSilon maintains the
generation quality while using only 3.9% of sampled points and achieves around
20 times faster inference, together with 4 times faster training convergence.
We provide video results on https://github.com/seungjun-moon/epsilon.



---

## TexGS-VolVis: Expressive Scene Editing for Volume Visualization via  Textured Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-18 | Kaiyuan Tang, Kuangshi Ai, Jun Han, Chaoli Wang | cs.GR | [PDF](http://arxiv.org/pdf/2507.13586v1){: .btn .btn-green } |

**Abstract**: Advancements in volume visualization (VolVis) focus on extracting insights
from 3D volumetric data by generating visually compelling renderings that
reveal complex internal structures. Existing VolVis approaches have explored
non-photorealistic rendering techniques to enhance the clarity, expressiveness,
and informativeness of visual communication. While effective, these methods
often rely on complex predefined rules and are limited to transferring a single
style, restricting their flexibility. To overcome these limitations, we
advocate the representation of VolVis scenes using differentiable Gaussian
primitives combined with pretrained large models to enable arbitrary style
transfer and real-time rendering. However, conventional 3D Gaussian primitives
tightly couple geometry and appearance, leading to suboptimal stylization
results. To address this, we introduce TexGS-VolVis, a textured Gaussian
splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives,
extending each Gaussian with additional texture and shading attributes,
resulting in higher-quality, geometry-consistent stylization and enhanced
lighting control during inference. Despite these improvements, achieving
flexible and controllable scene editing remains challenging. To further enhance
stylization, we develop image- and text-driven non-photorealistic scene editing
tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing
with fine-grained control. We evaluate TexGS-VolVis both qualitatively and
quantitatively across various volume rendering scenes, demonstrating its
superiority over existing methods in terms of efficiency, visual quality, and
editing flexibility.

Comments:
- Accepted by IEEE VIS 2025

---

## PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-18 | Yu Wei, Jiahui Zhang, Xiaoqin Zhang, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2507.13891v2){: .btn .btn-green } |

**Abstract**: COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing
attention due to its remarkable performance in reconstructing high-quality 3D
scenes from unposed images or videos. However, it often struggles to handle
scenes with complex camera trajectories as featured by drastic rotation and
translation across adjacent camera views, leading to degraded estimation of
camera poses and further local minima in joint optimization of camera poses and
3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that
achieves superior 3D scene modeling and camera pose estimation via camera pose
co-regularization. PCR-GS achieves regularization from two perspectives. The
first is feature reprojection regularization which extracts view-robust DINO
features from adjacent camera views and aligns their semantic information for
camera pose regularization. The second is wavelet-based frequency
regularization which exploits discrepancy in high-frequency details to further
optimize the rotation matrix in camera poses. Extensive experiments over
multiple real-world scenes show that the proposed PCR-GS achieves superior
pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.



---

## TimeNeRF: Building Generalizable Neural Radiance Fields across Time from  Few-Shot Input Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-18 | Hsiang-Hui Hung, Huu-Phu Do, Yung-Hui Li, Ching-Chun Huang | cs.CV | [PDF](http://arxiv.org/pdf/2507.13929v1){: .btn .btn-green } |

**Abstract**: We present TimeNeRF, a generalizable neural rendering approach for rendering
novel views at arbitrary viewpoints and at arbitrary times, even with few input
views. For real-world applications, it is expensive to collect multiple views
and inefficient to re-optimize for unseen scenes. Moreover, as the digital
realm, particularly the metaverse, strives for increasingly immersive
experiences, the ability to model 3D environments that naturally transition
between day and night becomes paramount. While current techniques based on
Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing
novel views, the exploration of NeRF's potential for temporal 3D scene modeling
remains limited, with no dedicated datasets available for this purpose. To this
end, our approach harnesses the strengths of multi-view stereo, neural radiance
fields, and disentanglement strategies across diverse datasets. This equips our
model with the capability for generalizability in a few-shot setting, allows us
to construct an implicit content radiance field for scene representation, and
further enables the building of neural radiance fields at any arbitrary time.
Finally, we synthesize novel views of that time via volume rendering.
Experiments show that TimeNeRF can render novel views in a few-shot setting
without per-scene optimization. Most notably, it excels in creating realistic
novel views that transition smoothly across different times, adeptly capturing
intricate natural scene changes from dawn to dusk.

Comments:
- Accepted by MM 2024

---

## HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D  Reconstruction and Controllable Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Tielong Wang, Yuxuan Xiong, Jinfan Liu, Zhifan Zhang, Ye Chen, Yue Shi, Bingbing Ni | cs.GR | [PDF](http://arxiv.org/pdf/2507.11971v1){: .btn .btn-green } |

**Abstract**: Current 3D representations like meshes, voxels, point clouds, and NeRF-based
neural implicit fields exhibit significant limitations: they are often
task-specific, lacking universal applicability across reconstruction,
generation, editing, and driving. While meshes offer high precision, their
dense vertex data complicates editing; NeRFs deliver excellent rendering but
suffer from structural ambiguity, hindering animation and manipulation; all
representations inherently struggle with the trade-off between data complexity
and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical
Proxy Node representation. Its core innovation lies in representing an object's
shape and texture via a sparse set of hierarchically organized
(tree-structured) proxy nodes distributed on its surface and interior. Each
node stores local shape and texture information (implicitly encoded by a small
MLP) within its neighborhood. Querying any 3D coordinate's properties involves
efficient neural interpolation and lightweight decoding from relevant nearby
and parent nodes. This framework yields a highly compact representation where
nodes align with local semantics, enabling direct drag-and-edit manipulation,
and offers scalable quality-complexity control. Extensive experiments across 3D
reconstruction and editing demonstrate our method's expressive efficiency,
high-fidelity rendering quality, and superior editability.



---

## BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Davide Di Nucci, Matteo Tomei, Guido Borghi, Luca Ciuffreda, Roberto Vezzani, Rita Cucchiara | cs.CV | [PDF](http://arxiv.org/pdf/2507.12095v1){: .btn .btn-green } |

**Abstract**: Accurate 3D reconstruction of vehicles is vital for applications such as
vehicle inspection, predictive maintenance, and urban planning. Existing
methods like Neural Radiance Fields and Gaussian Splatting have shown
impressive results but remain limited by their reliance on dense input views,
which hinders real-world applicability. This paper addresses the challenge of
reconstructing vehicles from sparse-view inputs, leveraging depth maps and a
robust pose estimation architecture to synthesize novel views and augment
training data. Specifically, we enhance Gaussian Splatting by integrating a
selective photometric loss, applied only to high-confidence pixels, and
replacing standard Structure-from-Motion pipelines with the DUSt3R architecture
to improve camera pose estimation. Furthermore, we present a novel dataset
featuring both synthetic and real-world public transportation vehicles,
enabling extensive evaluation of our approach. Experimental results demonstrate
state-of-the-art performance across multiple benchmarks, showcasing the
method's ability to achieve high-quality reconstructions even under constrained
input conditions.



---

## DoRF: Doppler Radiance Fields for Robust Human Activity Recognition  Using Wi-Fi

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Navid Hasanzadeh, Shahrokh Valaee | eess.SP | [PDF](http://arxiv.org/pdf/2507.12132v1){: .btn .btn-green } |

**Abstract**: Wi-Fi Channel State Information (CSI) has gained increasing interest for
remote sensing applications. Recent studies show that Doppler velocity
projections extracted from CSI can enable human activity recognition (HAR) that
is robust to environmental changes and generalizes to new users. However,
despite these advances, generalizability still remains insufficient for
practical deployment. Inspired by neural radiance fields (NeRF), which learn a
volumetric representation of a 3D scene from 2D images, this work proposes a
novel approach to reconstruct an informative 3D latent motion representation
from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The
resulting latent representation is then used to construct a uniform Doppler
radiance field (DoRF) of the motion, providing a comprehensive view of the
performed activity and improving the robustness to environmental variability.
The results show that the proposed approach noticeably enhances the
generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential
of DoRFs for practical sensing applications.



---

## SGLoc: Semantic Localization System for Camera Pose Estimation from 3D  Gaussian Splatting Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Beining Xu, Siting Zhu, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.12027v1){: .btn .btn-green } |

**Abstract**: We propose SGLoc, a novel localization system that directly regresses camera
poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic
information. Our method utilizes the semantic relationship between 2D image and
3D scene representation to estimate the 6DoF pose without prior pose
information. In this system, we introduce a multi-level pose regression
strategy that progressively estimates and refines the pose of query image from
the global 3DGS map, without requiring initial pose priors. Moreover, we
introduce a semantic-based global retrieval algorithm that establishes
correspondences between 2D (image) and 3D (3DGS map). By matching the extracted
scene semantic descriptors of 2D query image and 3DGS semantic representation,
we align the image with the local region of the global 3DGS map, thereby
obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by
iteratively optimizing the difference between the query image and the rendered
image from 3DGS. Our SGLoc demonstrates superior performance over baselines on
12scenes and 7scenes datasets, showing excellent capabilities in global
localization without initial pose prior. Code will be available at
https://github.com/IRMVLab/SGLoc.

Comments:
- 8 pages, 2 figures, IROS 2025

---

## NLI4VolVis: Natural Language Interaction for Volume Visualization via  LLM Multi-Agents and Editable 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Kuangshi Ai, Kaiyuan Tang, Chaoli Wang | cs.HC | [PDF](http://arxiv.org/pdf/2507.12621v1){: .btn .btn-green } |

**Abstract**: Traditional volume visualization (VolVis) methods, like direct volume
rendering, suffer from rigid transfer function designs and high computational
costs. Although novel view synthesis approaches enhance rendering efficiency,
they require additional learning effort for non-experts and lack support for
semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an
interactive system that enables users to explore, query, and edit volumetric
scenes using natural language. NLI4VolVis integrates multi-view semantic
segmentation and vision-language models to extract and understand semantic
components in a scene. We introduce a multi-agent large language model
architecture equipped with extensive function-calling tools to interpret user
intents and execute visualization tasks. The agents leverage external tools and
declarative VolVis commands to interact with the VolVis engine powered by 3D
editable Gaussians, enabling open-vocabulary object querying, real-time scene
editing, best-view selection, and 2D stylization. We validate our system
through case studies and a user study, highlighting its improved accessibility
and usability in volumetric data exploration. We strongly recommend readers
check our case studies, demo video, and source code at
https://nli4volvis.github.io/.

Comments:
- IEEE VIS 2025. Project Page: https://nli4volvis.github.io/

---

## VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via  Deformable 3D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Siyuan Yao, Chaoli Wang | cs.GR | [PDF](http://arxiv.org/pdf/2507.12667v1){: .btn .btn-green } |

**Abstract**: Visualization of large-scale time-dependent simulation data is crucial for
domain scientists to analyze complex phenomena, but it demands significant I/O
bandwidth, storage, and computational resources. To enable effective
visualization on local, low-end machines, recent advances in view synthesis
techniques, such as neural radiance fields, utilize neural networks to generate
novel visualizations for volumetric scenes. However, these methods focus on
reconstruction quality rather than facilitating interactive visualization
exploration, such as feature extraction and tracking. We introduce VolSegGS, a
novel Gaussian splatting framework that supports interactive segmentation and
tracking in dynamic volumetric scenes for exploratory visualization and
analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic
volumetric scene, allowing for real-time novel view synthesis. For accurate
segmentation, we leverage the view-independent colors of Gaussians for
coarse-level segmentation and refine the results with an affinity field network
for fine-level segmentation. Additionally, by embedding segmentation results
within the Gaussians, we ensure that their deformation enables continuous
tracking of segmented regions over time. We demonstrate the effectiveness of
VolSegGS with several time-varying datasets and compare our solutions against
state-of-the-art methods. With the ability to interact with a dynamic scene in
real time and provide flexible segmentation and tracking capabilities, VolSegGS
offers a powerful solution under low computational demands. This framework
unlocks exciting new possibilities for time-varying volumetric data analysis
and visualization.



---

## AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Jiawei Xu, Kai Deng, Zexin Fan, Shenlong Wang, Jin Xie, Jian Yang | cs.CV | [PDF](http://arxiv.org/pdf/2507.12137v2){: .btn .btn-green } |

**Abstract**: Modeling and rendering dynamic urban driving scenes is crucial for
self-driving simulation. Current high-quality methods typically rely on costly
manual object tracklet annotations, while self-supervised approaches fail to
capture dynamic object motions accurately and decompose scenes properly,
resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised
framework for high-quality free-viewpoint rendering of driving scenes from a
single log. At its core is a novel learnable motion model that integrates
locality-aware B-spline curves with global-aware trigonometric functions,
enabling flexible yet precise dynamic object modeling. Rather than requiring
comprehensive semantic labeling, AD-GS automatically segments scenes into
objects and background with the simplified pseudo 2D segmentation, representing
objects using dynamic Gaussians and bidirectional temporal visibility masks.
Further, our model incorporates visibility reasoning and physically rigid
regularization to enhance robustness. Extensive evaluations demonstrate that
our annotation-free model significantly outperforms current state-of-the-art
annotation-free methods and is competitive with annotation-dependent
approaches.

Comments:
- Accepted by ICCV 2025

---

## Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Jingqian Wu, Peiqi Duan, Zongqiang Wang, Changwei Wang, Boxin Shi, Edmund Y. Lam | cs.CV | [PDF](http://arxiv.org/pdf/2507.11931v1){: .btn .btn-green } |

**Abstract**: In low-light environments, conventional cameras often struggle to capture
clear multi-view images of objects due to dynamic range limitations and motion
blur caused by long exposure. Event cameras, with their high-dynamic range and
high-speed properties, have the potential to mitigate these issues.
Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,
facilitating bright frame synthesis from multiple viewpoints in low-light
conditions. However, naively using an event-assisted 3D GS approach still faced
challenges because, in low light, events are noisy, frames lack quality, and
the color tone may be inconsistent. To address these issues, we propose
Dark-EvGS, the first event-assisted 3D GS framework that enables the
reconstruction of bright frames from arbitrary viewpoints along the camera
trajectory. Triplet-level supervision is proposed to gain holistic knowledge,
granular details, and sharp scene rendering. The color tone matching block is
proposed to guarantee the color consistency of the rendered frames.
Furthermore, we introduce the first real-captured dataset for the event-guided
bright frame synthesis task via 3D GS-based radiance field reconstruction.
Experiments demonstrate that our method achieves better results than existing
methods, conquering radiance field reconstruction under challenging low-light
conditions. The code and sample data are included in the supplementary
material.



---

## Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-16 | Beizhen Zhao, Yifan Zhou, Sicheng Yu, Zijian Wang, Hao Wang | cs.GR | [PDF](http://arxiv.org/pdf/2507.12498v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction,
which effectively balances rendering quality, efficiency, and speed. However,
existing 3DGS approaches usually generate plausible outputs and face
significant challenges in complex scene reconstruction, manifesting as
incomplete holistic structural outlines and unclear local lighting effects. To
address these issues simultaneously, we propose a novel decoupled optimization
framework, which integrates wavelet decomposition into 3D Gaussian Splatting
and 2D sampling. Technically, through 3D wavelet decomposition, our approach
divides point clouds into high-frequency and low-frequency components, enabling
targeted optimization for each. The low-frequency component captures global
structural outlines and manages the distribution of Gaussians through
voxelization. In contrast, the high-frequency component restores intricate
geometric and textural details while incorporating a relight module to mitigate
lighting artifacts and enhance photorealistic rendering. Additionally, a 2D
wavelet decomposition is applied to the training images, simulating radiance
variations. This provides critical guidance for high-frequency detail
reconstruction, ensuring seamless integration of details with the global
structure. Extensive experiments on challenging datasets demonstrate our method
achieves state-of-the-art performance across various metrics, surpassing
existing approaches and advancing the field of 3D scene reconstruction.

Comments:
- 9 pages

---

## Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with  Regularized Score Distillation Sampling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-15 | Hayeon Kim, Ji Ha Jang, Se Young Chun | cs.CV | [PDF](http://arxiv.org/pdf/2507.11061v2){: .btn .btn-green } |

**Abstract**: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing. Code is available at
https://janeyeon.github.io/romap.



---

## A Mixed-Primitive-based Gaussian Splatting Method for Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-15 | Haoxuan Qu, Yujun Cai, Hossein Rahmani, Ajay Kumar, Junsong Yuan, Jun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2507.11321v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.



---

## TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth  Reconstruction via Physics Simulation for Scene Update

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-15 | Jeongyun Kim, Seunghoon Jeong, Giseop Kim, Myung-Hwan Jeon, Eunji Jun, Ayoung Kim | cs.RO | [PDF](http://arxiv.org/pdf/2507.11069v2){: .btn .btn-green } |

**Abstract**: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.



---

## 3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-14 | Yixun Zhang, Lizhi Wang, Junjun Zhao, Wending Zhao, Feng Zhou, Yonghao Dang, Jianqin Yin | cs.CV | [PDF](http://arxiv.org/pdf/2507.09993v1){: .btn .btn-green } |

**Abstract**: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

Comments:
- Submitted to WACV 2026

---

## VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-14 | Zihang Zeng, Shu Sun, Meixia Tao, Yin Xu, Xianghao Yu | eess.SP | [PDF](http://arxiv.org/pdf/2507.09987v1){: .btn .btn-green } |

**Abstract**: Wireless channel modeling in complex environments is crucial for wireless
communication system design and deployment. Traditional channel modeling
approaches face challenges in balancing accuracy, efficiency, and scalability,
while recent neural approaches such as neural radiance field (NeRF) suffer from
long training and slow inference. To tackle these challenges, we propose
voxelized radiance field (VoxelRF), a novel neural representation for wireless
channel modeling that enables fast and accurate synthesis of spatial spectra.
VoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based
methods with trilinear interpolation of voxel grid-based representation, and
two shallow MLPs to model both propagation and transmitter-dependent effects.
To further accelerate training and improve generalization, we introduce
progressive learning, empty space skipping, and an additional background
entropy loss function. Experimental results demonstrate that VoxelRF achieves
competitive accuracy with significantly reduced computation and limited
training data, making it more practical for real-time and resource-constrained
wireless applications.



---

## ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-14 | Shivangi Aneja, Sebastian Weiss, Irene Baeza, Prashanth Chandran, Gaspard Zoss, Matthias Nie√üner, Derek Bradley | cs.GR | [PDF](http://arxiv.org/pdf/2507.10542v1){: .btn .btn-green } |

**Abstract**: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

Comments:
- (SIGGRAPH 2025) Paper Video: https://youtu.be/VyWkgsGdbkk Project
  Page: https://shivangi-aneja.github.io/projects/scaffoldavatar/

---

## Stable Score Distillation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-12 | Haiming Zhu, Yangyang Xu, Chenshu Xu, Tingrui Shen, Wenxi Liu, Yong Du, Jun Yu, Shengfeng He | cs.CV | [PDF](http://arxiv.org/pdf/2507.09168v1){: .btn .btn-green } |

**Abstract**: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.



---

## From images to properties: a NeRF-driven framework for granular material  parameter inversion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-11 | Cheng-Hsi Hsiao, Krishna Kumar | cs.CV | [PDF](http://arxiv.org/pdf/2507.09005v1){: .btn .btn-green } |

**Abstract**: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.



---

## Learning human-to-robot handovers through 3D scene reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-11 | Yuekun Wu, Yik Lung Pang, Andrea Cavallaro, Changjae Oh | cs.RO | [PDF](http://arxiv.org/pdf/2507.08726v1){: .btn .btn-green } |

**Abstract**: Learning robot manipulation policies from raw, real-world image data requires
a large number of robot-action trials in the physical environment. Although
training using simulations offers a cost-effective alternative, the visual
domain gap between simulation and robot workspace remains a major limitation.
Gaussian Splatting visual reconstruction methods have recently provided new
directions for robot manipulation by generating realistic environments. In this
paper, we propose the first method for learning supervised-based robot
handovers solely from RGB images without the need of real-robot training or
real-robot data collection. The proposed policy learner, Human-to-Robot
Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view
Gaussian Splatting reconstruction of human-to-robot handover scenes to generate
robot demonstrations containing image-action pairs captured with a camera
mounted on the robot gripper. As a result, the simulated camera pose changes in
the reconstructed scene can be directly translated into gripper pose changes.
We train a robot policy on demonstrations collected with 16 household objects
and {\em directly} deploy this policy in the real environment. Experiments in
both Gaussian Splatting reconstructed scene and real-world human-to-robot
handover experiments demonstrate that H2RH-SGS serves as a new and effective
representation for the human-to-robot handover task.

Comments:
- 8 pages, 6 figures, 2 table

---

## RePaintGS: Reference-Guided Gaussian Splatting for Realistic and  View-Consistent 3D Scene Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-11 | Ji Hyun Seo, Byounhyun Yoo, Gerard Jounghyun Kim | cs.CV | [PDF](http://arxiv.org/pdf/2507.08434v1){: .btn .btn-green } |

**Abstract**: Radiance field methods, such as Neural Radiance Field or 3D Gaussian
Splatting, have emerged as seminal 3D representations for synthesizing
realistic novel views. For practical applications, there is ongoing research on
flexible scene editing techniques, among which object removal is a
representative task. However, removing objects exposes occluded regions, often
leading to unnatural appearances. Thus, studies have employed image inpainting
techniques to replace such regions with plausible content - a task referred to
as 3D scene inpainting. However, image inpainting methods produce one of many
plausible completions for each view, leading to inconsistencies between
viewpoints. A widely adopted approach leverages perceptual cues to blend
inpainted views smoothly. However, it is prone to detail loss and can fail when
there are perceptual inconsistencies across views. In this paper, we propose a
novel 3D scene inpainting method that reliably produces realistic and
perceptually consistent results even for complex scenes by leveraging a
reference view. Given the inpainted reference view, we estimate the inpainting
similarity of the other views to adjust their contribution in constructing an
accurate geometry tailored to the reference. This geometry is then used to warp
the reference inpainting to other views as pseudo-ground truth, guiding the
optimization to match the reference appearance. Comparative evaluation studies
have shown that our approach improves both the geometric fidelity and
appearance consistency of inpainted scenes.



---

## MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A  Benchmark for 3D Segmentation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-10 | Bangning Wei, Joshua Maraval, Meriem Outtas, Kidiyo Kpalma, Nicolas Ramin, Lu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2507.07519v1){: .btn .btn-green } |

**Abstract**: The application of methods based on Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3D GS) have steadily gained popularity in the field of 3D
object segmentation in static scenes. These approaches demonstrate efficacy in
a range of 3D scene understanding and editing tasks. Nevertheless, the 4D
object segmentation of dynamic scenes remains an underexplored field due to the
absence of a sufficiently extensive and accurately labelled multi-view video
dataset. In this paper, we present MUVOD, a new multi-view video dataset for
training and evaluating object segmentation in reconstructed real-world
scenarios. The 17 selected scenes, describing various indoor or outdoor
activities, are collected from different sources of datasets originating from
various types of camera rigs. Each scene contains a minimum of 9 views and a
maximum of 46 views. We provide 7830 RGB images (30 frames per video) with
their corresponding segmentation mask in 4D motion, meaning that any object of
interest in the scene could be tracked across temporal frames of a given view
or across different views belonging to the same camera rig. This dataset, which
contains 459 instances of 73 categories, is intended as a basic benchmark for
the evaluation of multi-view video segmentation methods. We also present an
evaluation metric and a baseline segmentation approach to encourage and
evaluate progress in this evolving field. Additionally, we propose a new
benchmark for 3D object segmentation task with a subset of annotated multi-view
images selected from our MUVOD dataset. This subset contains 50 objects of
different conditions in different scenarios, providing a more comprehensive
analysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD
dataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.



---

## RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance  Transfer and Reflection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-10 | Yongyang Zhou, Fang-Lue Zhang, Zichen Wang, Lei Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2507.07733v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in
novel view synthesis. However, rendering reflective objects remains a
significant challenge, particularly in inverse rendering and relighting. We
introduce RTR-GS, a novel inverse rendering framework capable of robustly
rendering objects with arbitrary reflectance properties, decomposing BRDF and
lighting, and delivering credible relighting results. Given a collection of
multi-view images, our method effectively recovers geometric structure through
a hybrid rendering model that combines forward rendering for radiance transfer
with deferred rendering for reflections. This approach successfully separates
high-frequency and low-frequency appearances, mitigating floating artifacts
caused by spherical harmonic overfitting when handling high-frequency details.
We further refine BRDF and lighting decomposition using an additional
physically-based deferred rendering branch. Experimental results show that our
method enhances novel view synthesis, normal estimation, decomposition, and
relighting while maintaining efficient training inference process.

Comments:
- 16 pages

---

## Temporally Consistent Amodal Completion for 3D Human-Object Interaction  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-10 | Hyungjun Doh, Dong In Lee, Seunggeun Chi, Pin-Hao Huang, Kwonjoon Lee, Sangpil Kim, Karthik Ramani | cs.CV | [PDF](http://arxiv.org/pdf/2507.08137v1){: .btn .btn-green } |

**Abstract**: We introduce a novel framework for reconstructing dynamic human-object
interactions from monocular video that overcomes challenges associated with
occlusions and temporal inconsistencies. Traditional 3D reconstruction methods
typically assume static objects or full visibility of dynamic subjects, leading
to degraded performance when these assumptions are violated-particularly in
scenarios where mutual occlusions occur. To address this, our framework
leverages amodal completion to infer the complete structure of partially
obscured regions. Unlike conventional approaches that operate on individual
frames, our method integrates temporal context, enforcing coherence across
video sequences to incrementally refine and stabilize reconstructions. This
template-free strategy adapts to varying conditions without relying on
predefined models, significantly enhancing the recovery of intricate details in
dynamic scenes. We validate our approach using 3D Gaussian Splatting on
challenging monocular videos, demonstrating superior precision in handling
occlusions and maintaining temporal stability compared to existing techniques.



---

## RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-10 | Chong Cheng, Yu Hu, Sicheng Yu, Beizhen Zhao, Zijian Wang, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.08136v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing
scenes from unposed images. However, optimization-based 3DGS methods struggle
with sparse views due to limited prior knowledge. Meanwhile, feed-forward
Gaussian approaches are constrained by input formats, making it challenging to
incorporate more input views. To address these challenges, we propose RegGS, a
3D Gaussian registration-based framework for reconstructing unposed sparse
views. RegGS aligns local 3D Gaussians generated by a feed-forward network into
a globally consistent 3D Gaussian representation. Technically, we implement an
entropy-regularized Sinkhorn algorithm to efficiently solve the optimal
transport Mixture 2-Wasserstein $(\text{MW}_2)$ distance, which serves as an
alignment metric for Gaussian mixture models (GMMs) in $\mathrm{Sim}(3)$ space.
Furthermore, we design a joint 3DGS registration module that integrates the
$\text{MW}_2$ distance, photometric consistency, and depth geometry. This
enables a coarse-to-fine registration process while accurately estimating
camera poses and aligning the scene. Experiments on the RE10K and ACID datasets
demonstrate that RegGS effectively registers local Gaussians with high
fidelity, achieving precise pose estimation and high-quality novel-view
synthesis. Project page: https://3dagentworld.github.io/reggs/.

Comments:
- Accepted to ICCV 2025

---

## SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene  Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-10 | Wei Yao, Shuzhao Xie, Letian Li, Weixiang Zhang, Zhixin Lai, Shiqi Dai, Ke Zhang, Zhi Wang | cs.GR | [PDF](http://arxiv.org/pdf/2507.07465v1){: .btn .btn-green } |

**Abstract**: Current 4D Gaussian frameworks for dynamic scene reconstruction deliver
impressive visual fidelity and rendering speed, however, the inherent trade-off
between storage costs and the ability to characterize complex physical motions
significantly limits the practical application of these methods. To tackle
these problems, we propose SD-GS, a compact and efficient dynamic Gaussian
splatting framework for complex dynamic scene reconstruction, featuring two key
contributions. First, we introduce a deformable anchor grid, a hierarchical and
memory-efficient scene representation where each anchor point derives multiple
3D Gaussians in its local spatiotemporal region and serves as the geometric
backbone of the 3D scene. Second, to enhance modeling capability for complex
motions, we present a deformation-aware densification strategy that adaptively
grows anchors in under-reconstructed high-dynamic regions while reducing
redundancy in static areas, achieving superior visual quality with fewer
anchors. Experimental results demonstrate that, compared to state-of-the-art
methods, SD-GS achieves an average of 60\% reduction in model size and an
average of 100\% improvement in FPS, significantly enhancing computational
efficiency while maintaining or even surpassing visual quality.



---

## Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for  Unconstrained Image Collections

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-10 | Yongtang Bao, Chengjie Tang, Yuze Wang, Haojie Li | cs.CV | [PDF](http://arxiv.org/pdf/2507.07395v1){: .btn .btn-green } |

**Abstract**: Reconstructing and segmenting scenes from unconstrained photo collections
obtained from the Internet is a novel but challenging task. Unconstrained photo
collections are easier to get than well-captured photo collections. These
unconstrained images suffer from inconsistent lighting and transient
occlusions, which makes segmentation challenging. Previous segmentation methods
cannot address transient occlusions or accurately restore the scene's lighting
conditions. Therefore, we propose Seg-Wild, an interactive segmentation method
based on 3D Gaussian Splatting for unconstrained image collections, suitable
for in-the-wild scenes. We integrate multi-dimensional feature embeddings for
each 3D Gaussian and calculate the feature similarity between the feature
embeddings and the segmentation target to achieve interactive segmentation in
the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to
smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and
calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We
also designed a benchmark to evaluate segmentation quality in in-the-wild
scenes. Experimental results demonstrate that compared to previous methods,
Seg-Wild achieves better segmentation results and reconstruction quality. Our
code will be available at https://github.com/Sugar0725/Seg-Wild.



---

## FlexGaussian: Flexible and Cost-Effective Training-Free Compression for  3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Boyuan Tian, Qizhe Gao, Siran Xianyu, Xiaotong Cui, Minjia Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2507.06671v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

Comments:
- To appear at ACM MM 2025

---

## LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+  FPS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Wanhua Li, Yujie Zhao, Minghan Qin, Yang Liu, Yuanhao Cai, Chuang Gan, Hanspeter Pfister | cs.GR | [PDF](http://arxiv.org/pdf/2507.07136v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce LangSplatV2, which achieves high-dimensional
feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6
FPS for high-resolution images, providing a 42 $\times$ speedup and a 47
$\times$ boost over LangSplat respectively, along with improved query accuracy.
LangSplat employs Gaussian Splatting to embed 2D CLIP language features into
3D, significantly enhancing speed and learning a precise 3D language field with
SAM semantics. Such advancements in 3D language fields are crucial for
applications that require language interaction within complex scenes. However,
LangSplat does not yet achieve real-time inference performance (8.2 FPS), even
with advanced A100 GPUs, severely limiting its broader application. In this
paper, we first conduct a detailed time analysis of LangSplat, identifying the
heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2
assumes that each Gaussian acts as a sparse code within a global dictionary,
leading to the learning of a 3D sparse coefficient field that entirely
eliminates the need for a heavyweight decoder. By leveraging this sparsity, we
further propose an efficient sparse coefficient splatting method with CUDA
optimization, rendering high-dimensional feature maps at high quality while
incurring only the time cost of splatting an ultra-low-dimensional feature. Our
experimental results demonstrate that LangSplatV2 not only achieves better or
competitive query accuracy but is also significantly faster. Codes and demos
are available at our project page: https://langsplat-v2.github.io.

Comments:
- Project Page: https://langsplat-v2.github.io

---

## Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Wijayathunga W. M. R. D. B | cs.GR | [PDF](http://arxiv.org/pdf/2507.07000v1){: .btn .btn-green } |

**Abstract**: We propose a novel framework that enhances non-rigid 3D model deformations by
bridging mesh representations with 3D Gaussian splatting. While traditional
Gaussian splatting delivers fast, real-time radiance-field rendering, its
post-editing capabilities and support for large-scale, non-rigid deformations
remain limited. Our method addresses these challenges by embedding Gaussian
kernels directly onto explicit mesh surfaces. This allows the mesh's inherent
topological and geometric priors to guide intuitive editing operations -- such
as moving, scaling, and rotating individual 3D components -- and enables
complex deformations like bending and stretching. This work paves the way for
more flexible 3D content-creation workflows in applications spanning virtual
reality, character animation, and interactive design.



---

## ClipGS: Clippable Gaussian Splatting for Interactive Cinematic  Visualization of Volumetric Medical Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Chengkun Li, Yuqi Tong, Kai Chen, Zhenya Yang, Ruiyang Li, Shi Qiu, Jason Ying-Kuen Chan, Pheng-Ann Heng, Qi Dou | cs.CV | [PDF](http://arxiv.org/pdf/2507.06647v1){: .btn .btn-green } |

**Abstract**: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

Comments:
- Early accepted by MICCAI 2025. Project is available at:
  https://med-air.github.io/ClipGS

---

## Photometric Stereo using Gaussian Splatting and inverse rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-09 | Mat√©o Ducastel, David Tschumperl√©, Yvain Qu√©au | eess.IV | [PDF](http://arxiv.org/pdf/2507.06684v1){: .btn .btn-green } |

**Abstract**: Recent state-of-the-art algorithms in photometric stereo rely on neural
networks and operate either through prior learning or inverse rendering
optimization. Here, we revisit the problem of calibrated photometric stereo by
leveraging recent advances in 3D inverse rendering using the Gaussian Splatting
formalism. This allows us to parameterize the 3D scene to be reconstructed and
optimize it in a more interpretable manner. Our approach incorporates a
simplified model for light representation and demonstrates the potential of the
Gaussian Splatting rendering engine for the photometric stereo problem.

Comments:
- in French language. GRETSI 2025, Association GRETSI, Aug 2025,
  Strasbourg, France

---

## LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for  Panorama-Style Mobile Captures

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Seungoh Han, Jaehoon Jang, Hyunsu Kim, Jaeheung Surh, Junhyung Kwak, Hyowon Ha, Kyungdon Joo | cs.GR | [PDF](http://arxiv.org/pdf/2507.06109v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel
view synthesis (NVS) with impressive quality in indoor scenes. However,
achieving high-fidelity rendering requires meticulously captured images
covering the entire scene, limiting accessibility for general users. We aim to
develop a practical 3DGS-based NVS framework using simple panorama-style motion
with a handheld camera (e.g., mobile device). While convenient, this
rotation-dominant motion and narrow baseline make accurate camera pose and 3D
point estimation challenging, especially in textureless indoor scenes. To
address these challenges, we propose LighthouseGS, a novel framework inspired
by the lighthouse-like sweeping motion of panoramic views. LighthouseGS
leverages rough geometric priors, such as mobile device camera poses and
monocular depth estimation, and utilizes the planar structures often found in
indoor environments. We present a new initialization method called plane
scaffold assembly to generate consistent 3D points on these structures,
followed by a stable pruning strategy to enhance geometry and optimization
stability. Additionally, we introduce geometric and photometric corrections to
resolve inconsistencies from motion drift and auto-exposure in mobile devices.
Tested on collected real and synthetic indoor scenes, LighthouseGS delivers
photorealistic rendering, surpassing state-of-the-art methods and demonstrating
the potential for panoramic view synthesis and object placement.

Comments:
- Preprint

---

## 3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Haitao Lu, Haijier Chen, Haoze Liu, Shoujian Zhang, Bo Xu, Ziao Liu | cs.RO | [PDF](http://arxiv.org/pdf/2507.05661v1){: .btn .btn-green } |

**Abstract**: In autonomous robotic systems, precise localization is a prerequisite for
safe navigation. However, in complex urban environments, GNSS positioning often
suffers from signal occlusion and multipath effects, leading to unreliable
absolute positioning. Traditional mapping approaches are constrained by storage
requirements and computational inefficiency, limiting their applicability to
resource-constrained robotic platforms. To address these challenges, we propose
3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian
Splatting (3DGS), enabling centimeter-level positioning using only a single
monocular RGB image on the client side. We combine multi-sensor data to
construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side
localization requires just a standard camera input. Using SuperPoint and
SuperGlue for feature extraction and matching, our core innovation is an
iterative optimization strategy that refines localization results through
step-by-step rendering, making it suitable for real-time autonomous navigation.
Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves
average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads,
boulevard roads, and traffic-dense highways respectively, significantly
outperforming other representative methods while requiring only monocular RGB
input. This approach provides autonomous robots with reliable localization
capabilities even in challenging urban environments where GNSS fails.

Comments:
- 13 pages,7 figures,4 tables

---

## BayesSDF: Surface-Based Laplacian Uncertainty Estimation for 3D Geometry  with Neural Signed Distance Fields

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Rushil Desai | cs.CV | [PDF](http://arxiv.org/pdf/2507.06269v2){: .btn .btn-green } |

**Abstract**: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and reliable uncertainty estimates are
essential. Unlike radiance-based models such as Neural Radiance Fields (NeRF)
or 3D Gaussian splatting, which lack explicit surface formulations, Signed
Distance Functions (SDFs) define continuous and differentiable geometry, making
them better suited for physical modeling and analysis. BayesSDF leverages a
Laplace approximation to quantify local surface instability using Hessian-based
metrics, enabling efficient, surfaceaware uncertainty estimation. Our method
shows that uncertainty predictions correspond closely with poorly reconstructed
geometry, providing actionable confidence measures for downstream use.
Extensive evaluations on synthetic and real-world datasets demonstrate that
BayesSDF outperforms existing methods in both calibration and geometric
consistency, establishing a strong foundation for uncertainty-aware 3D scene
reconstruction, simulation, and robotic decision-making.

Comments:
- ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)

---

## VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Alexandre Symeonidis-Herzig, √ñzge Mercanoƒülu Sincan, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2507.06060v1){: .btn .btn-green } |

**Abstract**: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.



---

## Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D  Gaussian Splatting for Photorealistic Scenes Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Jiayi Song, Zihan Ye, Qingyuan Zhou, Weidong Yang, Ben Fei, Jingyi Xu, Ying He, Wanli Ouyang | cs.CV | [PDF](http://arxiv.org/pdf/2507.06103v1){: .btn .btn-green } |

**Abstract**: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.



---

## D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for  Free-Viewpoint Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Wenkang Zhang, Yan Zhao, Qiang Wang, Li Song, Zhengxue Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2507.05859v1){: .btn .btn-green } |

**Abstract**: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

Comments:
- 12 pages, 9 figures, 8 tables

---

## DreamArt: Generating Interactable Articulated Objects from a Single  Image

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-08 | Ruijie Lu, Yu Liu, Jiaxiang Tang, Junfeng Ni, Yuxiang Wang, Diwen Wan, Gang Zeng, Yixin Chen, Siyuan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2507.05763v1){: .btn .btn-green } |

**Abstract**: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

Comments:
- Technical Report

---

## InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D  Geometry-Consistent Attention Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-07 | Minghao Wen, Shengjie Wu, Kangkan Wang, Dong Liang | cs.CV | [PDF](http://arxiv.org/pdf/2507.04961v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting based 3D editing has demonstrated impressive
performance in recent years. However, the multi-view editing often exhibits
significant local inconsistency, especially in areas of non-rigid deformation,
which lead to local artifacts, texture blurring, or semantic variations in
edited 3D scenes. We also found that the existing editing methods, which rely
entirely on text prompts make the editing process a "one-shot deal", making it
difficult for users to control the editing degree flexibly. In response to
these challenges, we present InterGSEdit, a novel framework for high-quality
3DGS editing via interactively selecting key views with users' preferences. We
propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to
adaptively screen a group of semantically consistent reference views for each
user-selected key view. Then, the cross-attention maps derived from the
reference views are used in a weighted Gaussian Splatting unprojection to
construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project
$GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D
cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive
attention strategy that prioritizes 3D-constrained attention for geometric
consistency during early inference, and gradually prioritizes 2D
cross-attention maps in diffusion for fine-grained features during the later
inference. Extensive experiments demonstrate that InterGSEdit achieves
state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing
with improved user experience.



---

## SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with  Segmented Consistency Trajectory Distillation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-07 | Jiahao Zhu, Zixuan Chen, Guangcong Wang, Xiaohua Xie, Yi Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2507.05256v1){: .btn .btn-green } |

**Abstract**: Recent advancements in text-to-3D generation improve the visual quality of
Score Distillation Sampling (SDS) and its variants by directly connecting
Consistency Distillation (CD) to score distillation. However, due to the
imbalance between self-consistency and cross-consistency, these CD-based
methods inherently suffer from improper conditional guidance, leading to
sub-optimal generation results. To address this issue, we present
SegmentDreamer, a novel framework designed to fully unleash the potential of
consistency models for high-fidelity text-to-3D generation. Specifically, we
reformulate SDS through the proposed Segmented Consistency Trajectory
Distillation (SCTD), effectively mitigating the imbalance issues by explicitly
defining the relationship between self- and cross-consistency. Moreover, SCTD
partitions the Probability Flow Ordinary Differential Equation (PF-ODE)
trajectory into multiple sub-trajectories and ensures consistency within each
segment, which can theoretically provide a significantly tighter upper bound on
distillation error. Additionally, we propose a distillation pipeline for a more
swift and stable generation. Extensive experiments demonstrate that our
SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling
high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).

Comments:
- Accepted by ICCV 2025, project page: https://zjhjojo.github.io/

---

## Mastering Regional 3DGS: Locating, Initializing, and Editing with  Diverse 2D Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-07 | Lanqing Guo, Yufei Wang, Hezhen Hu, Yan Zheng, Yeying Jin, Siyu Huang, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.05426v1){: .btn .btn-green } |

**Abstract**: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.



---

## A View-consistent Sampling Method for Regularized Training of Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-06 | Aoxiang Fan, Corentin Dumery, Nicolas Talabot, Pascal Fua | cs.CV | [PDF](http://arxiv.org/pdf/2507.04408v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene
representation and 3D recovery. To improve its performance on real-world data,
depth regularizations have proven to be the most effective ones. However, depth
estimation models not only require expensive 3D supervision in training, but
also suffer from generalization issues. As a result, the depth estimations can
be erroneous in practice, especially for outdoor unbounded scenes. In this
paper, we propose to employ view-consistent distributions instead of fixed
depth value estimations to regularize NeRF training. Specifically, the
distribution is computed by utilizing both low-level color features and
high-level distilled features from foundation models at the projected 2D
pixel-locations from per-ray sampled 3D points. By sampling from the
view-consistency distributions, an implicit regularization is imposed on the
training of NeRF. We also utilize a depth-pushing loss that works in
conjunction with the sampling technique to jointly provide effective
regularizations for eliminating the failure modes. Extensive experiments
conducted on various scenes from public datasets demonstrate that our proposed
method can generate significantly better novel view synthesis results than
state-of-the-art NeRF variants as well as different depth regularization
methods.

Comments:
- ICCV 2025 accepted

---

## ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic  Urban Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Guile Wu, Dongfeng Bai, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2507.03886v1){: .btn .btn-green } |

**Abstract**: This work focuses on modeling dynamic urban environments for autonomous
driving simulation. Contemporary data-driven methods using neural radiance
fields have achieved photorealistic driving scene modeling, but they suffer
from low rendering efficacy. Recently, some approaches have explored 3D
Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity
reconstruction and real-time rendering. However, these approaches often neglect
to model fine-grained variations between frames and camera viewpoints, leading
to suboptimal results. In this work, we propose a new approach named ArmGS that
exploits composite driving Gaussian splatting with multi-granularity appearance
refinement for autonomous driving scene modeling. The core idea of our approach
is devising a multi-level appearance modeling scheme to optimize a set of
transformation parameters for composite Gaussian refinement from multiple
granularities, ranging from local Gaussian level to global image level and
dynamic actor level. This not only models global scene appearance variations
between frames and camera viewpoints, but also models local fine-grained
changes of background and objects. Extensive experiments on multiple
challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and
VKITTI2, demonstrate the superiority of our approach over the state-of-the-art
methods.

Comments:
- Technical report

---

## Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Xiaolei Lang, Jiajun Lv, Kai Tang, Laijian Li, Jianxin Huang, Lina Liu, Yong Liu, Xingxing Zuo | cs.RO | [PDF](http://arxiv.org/pdf/2507.04004v2){: .btn .btn-green } |

**Abstract**: This paper presents the first photo-realistic LiDAR-Inertial-Camera Gaussian
Splatting SLAM system that simultaneously addresses visual quality, geometric
accuracy, and real-time performance. The proposed method performs robust and
accurate pose estimation within a continuous-time trajectory optimization
framework, while incrementally reconstructing a 3D Gaussian map using camera
and LiDAR data, all in real time. The resulting map enables high-quality,
real-time novel view rendering of both RGB images and depth maps. To
effectively address under-reconstruction in regions not covered by the LiDAR,
we employ a lightweight zero-shot depth model that synergistically combines RGB
appearance cues with sparse LiDAR measurements to generate dense depth maps.
The depth completion enables reliable Gaussian initialization in LiDAR-blind
areas, significantly improving system applicability for sparse LiDAR sensors.
To enhance geometric accuracy, we use sparse but precise LiDAR depths to
supervise Gaussian map optimization and accelerate it with carefully designed
CUDA-accelerated strategies. Furthermore, we explore how the incrementally
reconstructed Gaussian map can improve the robustness of odometry. By tightly
incorporating photometric constraints from the Gaussian map into the
continuous-time factor graph optimization, we demonstrate improved pose
estimation under LiDAR degradation scenarios. We also showcase downstream
applications via extending our elaborate system, including video frame
interpolation and fast 3D mesh extraction. To support rigorous evaluation, we
construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth
poses, depth maps, and extrapolated trajectories for assessing out-of-sequence
novel view synthesis. Both the dataset and code will be made publicly available
on project page https://xingxingzuo.github.io/gaussian_lic2.



---

## A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated  Rendering in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-05 | Shuo Xin, Haiyu Wang, Sai Qian Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2507.04147v1){: .btn .btn-green } |

**Abstract**: Virtual reality (VR) significantly transforms immersive digital interfaces,
greatly enhancing education, professional practices, and entertainment by
increasing user engagement and opening up new possibilities in various
industries. Among its numerous applications, image rendering is crucial.
Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high
computational demands, driven predominantly by user expectations for superior
visual quality. This results in notable processing delays for real-time image
rendering, which greatly affects the user experience. Additionally, VR devices
such as head-mounted displays (HMDs) are intricately linked to human visual
behavior, leveraging knowledge from perception and cognition to improve user
experience. These insights have spurred the development of foveated rendering,
a technique that dynamically adjusts rendering resolution based on the user's
gaze direction. The resultant solution, known as gaze-tracked foveated
rendering, significantly reduces the computational burden of the rendering
process.
  Although gaze-tracked foveated rendering can reduce rendering costs, the
computational overhead of the gaze tracking process itself can sometimes
outweigh the rendering savings, leading to increased processing latency. To
address this issue, we propose an efficient rendering framework
called~\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated
rendering via the parallelization of gaze tracking and foveated rendering
processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a
state-of-the-art neural rendering technique. Evaluation results demonstrate
that A3FR can reduce end-to-end rendering latency by up to $2\times$ while
maintaining visual quality.

Comments:
- ACM International Conference on Supercomputing 2025

---

## Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian  Pointmaps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-04 | Chong Cheng, Sicheng Yu, Zijian Wang, Yifan Zhou, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.03737v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its
high-fidelity and real-time novel view synthesis performance. However, some
previous 3DGS SLAM methods employ a differentiable rendering pipeline for
tracking, \textbf{lack geometric priors} in outdoor scenes. Other approaches
introduce separate tracking modules, but they accumulate errors with
significant camera movement, leading to \textbf{scale drift}. To address these
challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS.
Technically, we establish a self-consistent tracking module anchored in the
3DGS pointmap, which avoids cumulative scale drift and achieves more precise
and robust tracking with fewer iterations. Additionally, we design a
patch-based pointmap dynamic mapping module, which introduces geometric priors
while avoiding scale ambiguity. This significantly enhances tracking accuracy
and the quality of scene reconstruction, making it particularly suitable for
complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV
datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel
view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy.
Project page: https://3dagentworld.github.io/S3PO-GS/.

Comments:
- Accepted by ICCV2025

---

## ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and  Manipulation of Articulated Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Qiaojun Yu, Xibin Yuan, Yu jiang, Junting Chen, Dongzhe Zheng, Ce Hao, Yang You, Yixing Chen, Yao Mu, Liu Liu, Cewu Lu | cs.RO | [PDF](http://arxiv.org/pdf/2507.02600v1){: .btn .btn-green } |

**Abstract**: Articulated object manipulation remains a critical challenge in robotics due
to the complex kinematic constraints and the limited physical reasoning of
existing methods. In this work, we introduce ArtGS, a novel framework that
extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling
for articulated object understanding and interaction. ArtGS begins with
multi-view RGB-D reconstruction, followed by reasoning with a vision-language
model (VLM) to extract semantic and structural information, particularly the
articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS
optimizes the parameters of the articulated bones, ensuring physically
consistent motion constraints and enhancing the manipulation policy. By
leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and
closed-loop optimization, ArtGS establishes a new framework for efficient,
scalable, and generalizable articulated object modeling and manipulation.
Experiments conducted in both simulation and real-world environments
demonstrate that ArtGS significantly outperforms previous methods in joint
estimation accuracy and manipulation success rates across a variety of
articulated objects. Additional images and videos are available on the project
website: https://sites.google.com/view/artgs/home

Comments:
- Accepted by IROS 2025

---

## LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local  Implicit Feature Decoupling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Jiahao Wu, Rui Peng, Jianbo Jiao, Jiayu Yang, Luyang Tang, Kaiqiang Xiong, Jie Liang, Jinbo Yan, Runling Liu, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2507.02363v1){: .btn .btn-green } |

**Abstract**: Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamlined local spaces defined by seeds, enabling
global modeling by capturing motion within each local space. 2) We decouple
static and dynamic features for local space motion modeling. A static feature
shared across time steps captures static information, while a dynamic residual
field provides time-specific features. These are combined and decoded to
generate Temporal Gaussians, modeling motion within each local space. As a
result, we propose a novel dynamic scene reconstruction framework to model
highly dynamic real-world scenes more realistically. Our method not only
demonstrates competitive performance on various fine-scale datasets compared to
state-of-the-art (SOTA) methods, but also represents the first attempt to model
larger and more complex highly dynamic scenes. Project page:
https://wujh2001.github.io/LocalDyGS/.

Comments:
- Accepted by ICCV 2025

---

## Gbake: Baking 3D Gaussian Splats into Reflection Probes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Stephen Pasch, Joel K. Salzman, Changxi Zheng | cs.GR | [PDF](http://arxiv.org/pdf/2507.02257v1){: .btn .btn-green } |

**Abstract**: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

Comments:
- SIGGRAPH 2025 Posters

---

## HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity  Animatable Face Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-03 | Gent Serifi, Marcel C. B√ºhler | cs.CV | [PDF](http://arxiv.org/pdf/2507.02803v2){: .btn .btn-green } |

**Abstract**: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

Comments:
- Project page: https://gserifi.github.io/HyperGaussians, Code:
  https://github.com/gserifi/HyperGaussians

---

## 3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial  Camouflage Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-02 | Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2507.01367v1){: .btn .btn-green } |

**Abstract**: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

Comments:
- Accepted by ICCV 2025

---

## Tile and Slide : A New Framework for Scaling NeRF from Local to Global  3D Earth Observation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-02 | Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet | cs.CV | [PDF](http://arxiv.org/pdf/2507.01631v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

Comments:
- Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D
  Vision Across Altitudes). Version before camera ready. Our code will be made
  public after the conference

---

## Masks make discriminative models great again!


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Tianshi Cao, Marie-Julie Rakotosaona, Ben Poole, Federico Tombari, Michael Niemeyer | cs.CV | [PDF](http://arxiv.org/pdf/2507.00916v1){: .btn .btn-green } |

**Abstract**: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.



---

## GaussianVLM: Scene-centric 3D Vision-Language Models using  Language-aligned Gaussian Splats for Embodied Reasoning and Beyond

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang, Danda Pani Paudel, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2507.00886v1){: .btn .btn-green } |

**Abstract**: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.



---

## LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail  Conserved Anti-Aliasing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Zhenya Yang, Bingchen Gong, Kai Chen | cs.CV | [PDF](http://arxiv.org/pdf/2507.00554v2){: .btn .btn-green } |

**Abstract**: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.



---

## Surgical Neural Radiance Fields from One Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Alberto Neri, Maximilan Fehrentz, Veronica Penza, Leonardo S. Mattos, Nazim Haouchine | cs.CV | [PDF](http://arxiv.org/pdf/2507.00969v1){: .btn .btn-green } |

**Abstract**: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.



---

## GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And  Dynamic Density Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Xingjun Wang, Lianlei Shan | cs.CV | [PDF](http://arxiv.org/pdf/2507.00363v1){: .btn .btn-green } |

**Abstract**: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.



---

## PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance  point cloud reconstruction via joint-channel NeRF with multi-view image  instance matching

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Xin Yang, Ruiming Du, Hanyang Huang, Jiayang Xie, Pengyao Xie, Leisen Fang, Ziyue Guo, Nanjun Jiang, Yu Jiang, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2507.00371v1){: .btn .btn-green } |

**Abstract**: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.



---

## A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale  Reconstruction with External Memory

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Felix Windisch, Lukas Radl, Thomas K√∂hler, Michael Steiner, Dieter Schmalstieg, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2507.01110v2){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.



---

## VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online  Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-07-01 | Keiko Nagami, Timothy Chen, Javier Yu, Ola Shorinwa, Maximilian Adang, Carlyn Dougherty, Eric Cristofalo, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2507.01125v1){: .btn .btn-green } |

**Abstract**: We present VISTA (Viewpoint-based Image selection with Semantic Task
Awareness), an active exploration method for robots to plan informative
trajectories that improve 3D map quality in areas most relevant for task
completion. Given an open-vocabulary search instruction (e.g., "find a
person"), VISTA enables a robot to explore its environment to search for the
object of interest, while simultaneously building a real-time semantic 3D
Gaussian Splatting reconstruction of the scene. The robot navigates its
environment by planning receding-horizon trajectories that prioritize semantic
similarity to the query and exploration of unseen regions of the environment.
To evaluate trajectories, VISTA introduces a novel, efficient
viewpoint-semantic coverage metric that quantifies both the geometric view
diversity and task relevance in the 3D scene. On static datasets, our coverage
metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in
computation speed and reconstruction quality. In quadrotor hardware
experiments, VISTA achieves 6x higher success rates in challenging maps,
compared to baseline methods, while matching baseline performance in less
challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying
it on a quadrotor drone and a Spot quadruped robot. Open-source code will be
released upon acceptance of the paper.

Comments:
- 9 pages, 4 figures
