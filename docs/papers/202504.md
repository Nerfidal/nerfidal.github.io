---
layout: default
title: April 2025
parent: Papers
nav_order: 202504
---

<!---metadata--->


## InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars  with Deformable Gaussians


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-10 | Kefan Chen, Sergiu Oprea, Justin Theiss, Sreyas Mohan, Srinath Sridhar, Aayush Prakash | cs.CV | [PDF](http://arxiv.org/pdf/2504.07949v1){: .btn .btn-green } |

**Abstract**: With the rising interest from the community in digital avatars coupled with
the importance of expressions and gestures in communication, modeling natural
avatar behavior remains an important challenge across many industries such as
teleconferencing, gaming, and AR/VR. Human hands are the primary tool for
interacting with the environment and essential for realistic human behavior
modeling, yet existing 3D hand and head avatar models often overlook the
crucial aspect of hand-body interactions, such as between hand and face. We
present InteracttAvatar, the first model to faithfully capture the
photorealistic appearance of dynamic hand and non-rigid hand-face interactions.
Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian
Splatting as well as a dynamic refinement module, captures pose-dependent
change, e.g. the fine wrinkles and complex shadows that occur during
articulation. Importantly, our hand-face interaction module models the subtle
geometry and appearance dynamics that underlie common gestures. Through
experiments of novel view synthesis, self reenactment and cross-identity
reenactment, we demonstrate that InteracttAvatar can reconstruct hand and
hand-face interactions from monocular or multiview videos with high-fidelity
details and be animated with novel poses.



---

## View-Dependent Uncertainty Estimation of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-10 | Chenyu Han, Corentin Dumery | cs.CV | [PDF](http://arxiv.org/pdf/2504.07370v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene
reconstruction for its high visual accuracy. However, uncertainty estimation of
3DGS scenes remains underexplored and is crucial to downstream tasks such as
asset extraction and scene completion. Since the appearance of 3D gaussians is
view-dependent, the color of a gaussian can thus be certain from an angle and
uncertain from another. We thus propose to model uncertainty in 3DGS as an
additional view-dependent per-gaussian feature that can be modeled with
spherical harmonics. This simple yet effective modeling is easily interpretable
and can be integrated into the traditional 3DGS pipeline. It is also
significantly faster than ensemble methods while maintaining high accuracy, as
demonstrated in our experiments.



---

## Stochastic Ray Tracing of 3D Transparent Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Xin Sun, Iliyan Georgiev, Yun Fei, Miloš Hašan | cs.GR | [PDF](http://arxiv.org/pdf/2504.06598v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has recently been widely adopted as a 3D representation
for novel-view synthesis, relighting, and text-to-3D generation tasks, offering
realistic and detailed results through a collection of explicit 3D Gaussians
carrying opacities and view-dependent colors. However, efficient rendering of
many transparent primitives remains a significant challenge. Existing
approaches either rasterize the 3D Gaussians with approximate sorting per view
or rely on high-end RTX GPUs to exhaustively process all ray-Gaussian
intersections (bounding Gaussians by meshes). This paper proposes a stochastic
ray tracing method to render 3D clouds of transparent primitives. Instead of
processing all ray-Gaussian intersections in sequential order, each ray
traverses the acceleration structure only once, randomly accepting and shading
a single intersection (or N intersections, using a simple extension). This
approach minimizes shading time and avoids sorting the Gaussians along the ray
while minimizing the register usage and maximizing parallelism even on low-end
GPUs. The cost of rays through the Gaussian asset is comparable to that of
standard mesh-intersection rays. While our method introduces noise, the shading
is unbiased, and the variance is slight, as stochastic acceptance is
importance-sampled based on accumulated opacity. The alignment with the Monte
Carlo philosophy simplifies implementation and easily integrates our method
into a conventional path-tracing framework.

Comments:
- 10 pages, 6 figures, 5 tables

---

## IAAO: Interactive Affordance Learning for Articulated Objects in 3D  Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Can Zhang, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2504.06827v1){: .btn .btn-green } |

**Abstract**: This work presents IAAO, a novel framework that builds an explicit 3D model
for intelligent agents to gain understanding of articulated objects in their
environment through interaction. Unlike prior methods that rely on
task-specific networks and assumptions about movable parts, our IAAO leverages
large foundation models to estimate interactive affordances and part
articulations in three stages. We first build hierarchical features and label
fields for each object state using 3D Gaussian Splatting (3DGS) by distilling
mask features and view-consistent labels from multi-view images. We then
perform object- and part-level queries on the 3D Gaussian primitives to
identify static and articulated elements, estimating global transformations and
local articulation parameters along with affordances. Finally, scenes from
different states are merged and refined based on the estimated transformations,
enabling robust affordance-based interaction and manipulation of objects.
Experimental results demonstrate the effectiveness of our method.



---

## SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Hanxiao Sun, YuPeng Gao, Jin Xie, Jian Yang, Beibei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.06815v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D assets from images, known as inverse rendering (IR),
remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting
(3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS)
tasks. Methods apply it to relighting by separating radiance into BRDF
parameters and lighting, yet produce inferior relighting quality with artifacts
and unnatural indirect illumination due to the limited capability of each
Gaussian, which has constant material parameters and normal, alongside the
absence of physical constraints for indirect lighting. In this paper, we
present a novel framework called Spatially-vayring Gaussian Inverse Rendering
(SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we
propose a new representation-Spatially-varying Gaussian (SVG)-that allows
per-Gaussian spatially varying parameters. This enhanced representation is
complemented by a SVG splatting scheme akin to vertex/fragment shading in
traditional graphics pipelines. Furthermore, we integrate a physically-based
indirect lighting model, enabling more realistic relighting. The proposed
SVG-IR framework significantly improves rendering quality, outperforming
state-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio
(PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in
relighting tasks, all while maintaining a real-time rendering speed.



---

## GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Anil Armagan, Albert Saà-Garriga, Bruno Manganelli, Kyuwon Kim, M. Kerim Yucel | cs.CV | [PDF](http://arxiv.org/pdf/2504.06716v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly
due to its ability to converge reasonably fast, faithfully represent the scene
and render (novel) views in a fast fashion. However, it suffers from large
storage and memory requirements, and its training speed still lags behind the
hash-grid based radiance field approaches (e.g. Instant-NGP), which makes it
especially difficult to deploy them in robotics scenarios, where 3D
reconstruction is crucial for accurate operation. In this paper, we propose
GSta that dynamically identifies Gaussians that have converged well during
training, based on their positional and color gradient norms. By forcing such
Gaussians into a siesta and stopping their updates (freezing) during training,
we improve training speed with competitive accuracy compared to state of the
art. We also propose an early stopping mechanism based on the PSNR values
computed on a subset of training images. Combined with other improvements, such
as integrating a learning rate scheduler, GSta achieves an improved Pareto
front in convergence speed, memory and storage requirements, while preserving
quality. We also show that GSta can improve other methods and complement
orthogonal approaches in efficiency improvement; once combined with Trick-GS,
GSta achieves up to 5x faster training, 16x smaller disk size compared to
vanilla GS, while having comparable accuracy and consuming only half the peak
memory. More visualisations are available at
https://anilarmagan.github.io/SRUK-GSta.

Comments:
- 9 pages. In submission to an IEEE conference

---

## Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and  Phenotyping of Wheat Heads with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Daiwei Zhang, Joaquin Gajardo, Tomislav Medic, Isinsu Katircioglu, Mike Boss, Norbert Kirchgessner, Achim Walter, Lukas Roth | cs.CV | [PDF](http://arxiv.org/pdf/2504.06978v1){: .btn .btn-green } |

**Abstract**: Automated extraction of plant morphological traits is crucial for supporting
crop breeding and agricultural management through high-throughput field
phenotyping (HTFP). Solutions based on multi-view RGB images are attractive due
to their scalability and affordability, enabling volumetric measurements that
2D approaches cannot directly capture. While advanced methods like Neural
Radiance Fields (NeRFs) have shown promise, their application has been limited
to counting or extracting traits from only a few plants or organs. Furthermore,
accurately measuring complex structures like individual wheat heads-essential
for studying crop yields-remains particularly challenging due to occlusions and
the dense arrangement of crop canopies in field conditions. The recent
development of 3D Gaussian Splatting (3DGS) offers a promising alternative for
HTFP due to its high-quality reconstructions and explicit point-based
representation. In this paper, we present Wheat3DGS, a novel approach that
leverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance
segmentation and morphological measurement of hundreds of wheat heads
automatically, representing the first application of 3DGS to HTFP. We validate
the accuracy of wheat head extraction against high-resolution laser scan data,
obtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and
40.2% for length, width, and volume. We provide additional comparisons to
NeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating
superior results. Our approach enables rapid, non-destructive measurements of
key yield-related traits at scale, with significant implications for
accelerating crop breeding and improving our understanding of wheat
development.

Comments:
- Copyright 2025 IEEE. This is the author's version of the work. It is
  posted here for your personal use. Not for redistribution. The definitive
  version is published in the 2025 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops (CVPRW)

---

## S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in  Remote Sensing Applications

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Masquil Elías, Marí Roger, Ehret Thibaud, Meinhardt-Llopis Enric, Musé Pablo, Facciolo Gabriele | cs.CV | [PDF](http://arxiv.org/pdf/2504.06920v1){: .btn .btn-green } |

**Abstract**: We introduce the S-EO dataset: a large-scale, high-resolution dataset,
designed to advance geometry-aware shadow detection. Collected from diverse
public-domain sources, including challenge datasets and government providers
such as USGS, our dataset comprises 702 georeferenced tiles across the USA,
each covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3
pansharpened RGB images, panchromatic images, and a ground-truth DSM of the
area obtained from LiDAR scans. For each image, we provide a shadow mask
derived from geometry and sun position, a vegetation mask based on the NDVI
index, and a bundle-adjusted RPC model. With approximately 20,000 images, the
S-EO dataset establishes a new public resource for shadow detection in remote
sensing imagery and its applications to 3D reconstruction. To demonstrate the
dataset's impact, we train and evaluate a shadow detector, showcasing its
ability to generalize, even to aerial images. Finally, we extend EO-NeRF - a
state-of-the-art NeRF approach for satellite imagery - to leverage our shadow
predictions for improved 3D reconstructions.

Comments:
- Accepted at Earthvision 2025 (CVPR Workshop)

---

## Collision avoidance from monocular vision trained with novel view  synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Valentin Tordjman--Levavasseur, Stéphane Caron | cs.RO | [PDF](http://arxiv.org/pdf/2504.06651v1){: .btn .btn-green } |

**Abstract**: Collision avoidance can be checked in explicit environment models such as
elevation maps or occupancy grids, yet integrating such models with a
locomotion policy requires accurate state estimation. In this work, we consider
the question of collision avoidance from an implicit environment model. We use
monocular RGB images as inputs and train a collisionavoidance policy from
photorealistic images generated by 2D Gaussian splatting. We evaluate the
resulting pipeline in realworld experiments under velocity commands that bring
the robot on an intercept course with obstacles. Our results suggest that RGB
images can be enough to make collision-avoidance decisions, both in the room
where training data was collected and in out-of-distribution environments.



---

## Meta-Continual Learning of Neural Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Seungyoon Woo, Junhyeog Yun, Gunhee Kim | cs.AI | [PDF](http://arxiv.org/pdf/2504.05806v1){: .btn .btn-green } |

**Abstract**: Neural Fields (NF) have gained prominence as a versatile framework for
complex data representation. This work unveils a new problem setting termed
\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel
strategy that employs a modular architecture combined with optimization-based
meta-learning. Focused on overcoming the limitations of existing methods for
continual learning of neural fields, such as catastrophic forgetting and slow
convergence, our strategy achieves high-quality reconstruction with
significantly improved learning speed. We further introduce Fisher Information
Maximization loss for neural radiance fields (FIM-NeRF), which maximizes
information gains at the sample level to enhance learning generalization, with
proved convergence guarantee and generalization bound. We perform extensive
evaluations across image, audio, video reconstruction, and view synthesis tasks
on six diverse datasets, demonstrating our method's superiority in
reconstruction quality and speed over existing MCL and CL-NF approaches.
Notably, our approach attains rapid adaptation of neural fields for city-scale
NeRF rendering with reduced parameter requirement.



---

## SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve  Phoneme-Viseme Alignment Ambiguity

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Yihuan Huang, Jiajun Liu, Yanzhen Ren, Wuyang Liu, Juhua Tang | cs.GR | [PDF](http://arxiv.org/pdf/2504.05803v1){: .btn .btn-green } |

**Abstract**: Speech-driven talking head synthesis tasks commonly use general acoustic
features (such as HuBERT and DeepSpeech) as guided speech features. However, we
discovered that these features suffer from phoneme-viseme alignment ambiguity,
which refers to the uncertainty and imprecision in matching phonemes (speech)
with visemes (lip). To address this issue, we propose the Speech Encoder for
Lip (SE4Lip) to encode lip features from speech directly, aligning speech and
lip features in the joint embedding space by a cross-modal alignment framework.
The STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve
the fine-grained speech features. Experimental results show that SE4Lip
achieves state-of-the-art performance in both NeRF and 3DGS rendering models.
Its lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline
and produces results close to the ground truth videos.



---

## InvNeRF-Seg: Fine-Tuning a Pre-Trained NeRF for 3D Object Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Jiangsan Zhao, Jakob Geipel, Krzysztof Kusnierek, Xuean Cui | cs.CV | [PDF](http://arxiv.org/pdf/2504.05751v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have been widely adopted for reconstructing
high quality 3D point clouds from 2D RGB images. However, the segmentation of
these reconstructed 3D scenes is more essential for downstream tasks such as
object counting, size estimation, and scene understanding. While segmentation
on raw 3D point clouds using deep learning requires labor intensive and
time-consuming manual annotation, directly training NeRF on binary masks also
fails due to the absence of color and shading cues essential for geometry
learning. We propose Invariant NeRF for Segmentation (InvNeRFSeg), a two step,
zero change fine tuning strategy for 3D segmentation. We first train a standard
NeRF on RGB images and then fine tune it using 2D segmentation masks without
altering either the model architecture or loss function. This approach produces
higher quality, cleaner segmented point clouds directly from the refined
radiance field with minimal computational overhead or complexity. Field density
analysis reveals consistent semantic refinement: densities of object regions
increase while background densities are suppressed, ensuring clean and
interpretable segmentations. We demonstrate InvNeRFSegs superior performance
over both SA3D and FruitNeRF on both synthetic fruit and real world soybean
datasets. This approach effectively extends 2D segmentation to high quality 3D
segmentation.



---

## Micro-splatting: Maximizing Isotropic Constraints for Refined  Optimization in 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Jee Won Lee, Hansol Lim, Sooyeun Yang, Jongseong Choi | cs.GR | [PDF](http://arxiv.org/pdf/2504.05740v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have achieved impressive
scalability and real-time rendering for large-scale scenes but often fall short
in capturing fine-grained details. Conventional approaches that rely on
relatively large covariance parameters tend to produce blurred representations,
while directly reducing covariance sizes leads to sparsity. In this work, we
introduce Micro-splatting (Maximizing Isotropic Constraints for Refined
Optimization in 3D Gaussian Splatting), a novel framework designed to overcome
these limitations. Our approach leverages a covariance regularization term to
penalize excessively large Gaussians to ensure each splat remains compact and
isotropic. This work implements an adaptive densification strategy that
dynamically refines regions with high image gradients by lowering the splitting
threshold, followed by loss function enhancement. This strategy results in a
denser and more detailed gaussian means where needed, without sacrificing
rendering efficiency. Quantitative evaluations using metrics such as L1, L2,
PSNR, SSIM, and LPIPS, alongside qualitative comparisons demonstrate that our
method significantly enhances fine-details in 3D reconstructions.



---

## Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Zhicong Sun, Jacqueline Lo, Jinxing Hu | cs.RO | [PDF](http://arxiv.org/pdf/2504.04844v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping (SLAM) technology now has
photorealistic mapping capabilities thanks to the real-time high-fidelity
rendering capability of 3D Gaussian splatting (3DGS). However, due to the
static representation of scenes, current 3DGS-based SLAM encounters issues with
pose drift and failure to reconstruct accurate maps in dynamic environments. To
address this problem, we present D4DGS-SLAM, the first SLAM method based on
4DGS map representation for dynamic environments. By incorporating the temporal
dimension into scene representation, D4DGS-SLAM enables high-quality
reconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we
can obtain the dynamics, visibility, and reliability of scene points, and
filter stable static points for tracking accordingly. When optimizing Gaussian
points, we apply different isotropic regularization terms to Gaussians with
varying dynamic characteristics. Experimental results on real-world dynamic
scene datasets demonstrate that our method outperforms state-of-the-art
approaches in both camera pose tracking and map quality.

Comments:
- This paper is currently under reviewed for IROS 2025

---

## PanoDreamer: Consistent Text to 360-Degree Scene Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Zhexiao Xiong, Zhang Chen, Zhong Li, Yi Xu, Nathan Jacobs | cs.CV | [PDF](http://arxiv.org/pdf/2504.05152v1){: .btn .btn-green } |

**Abstract**: Automatically generating a complete 3D scene from a text description, a
reference image, or both has significant applications in fields like virtual
reality and gaming. However, current methods often generate low-quality
textures and inconsistent 3D structures. This is especially true when
extrapolating significantly beyond the field of view of the reference image. To
address these challenges, we propose PanoDreamer, a novel framework for
consistent, 3D scene generation with flexible text and image control. Our
approach employs a large language model and a warp-refine pipeline, first
generating an initial set of images and then compositing them into a 360-degree
panorama. This panorama is then lifted into 3D to form an initial point cloud.
We then use several approaches to generate additional images, from different
viewpoints, that are consistent with the initial point cloud and expand/refine
the initial point cloud. Given the resulting set of images, we utilize 3D
Gaussian Splatting to create the final 3D scene, which can then be rendered
from different viewpoints. Experiments demonstrate the effectiveness of
PanoDreamer in generating high-quality, geometrically consistent 3D scenes.

Comments:
- Accepted by CVPR 2025 Workshop on Computer Vision for Metaverse

---

## DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Wanzhou Liu, Zhexiao Xiong, Xinyu Li, Nathan Jacobs | cs.CV | [PDF](http://arxiv.org/pdf/2504.04679v1){: .btn .btn-green } |

**Abstract**: Recent novel view synthesis (NVS) techniques, including Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene
reconstruction with high-quality rendering and realistic detail recovery.
Effectively removing occlusions while preserving scene details can further
enhance the robustness and applicability of these techniques. However, existing
approaches for object and occlusion removal predominantly rely on generative
priors, which, despite filling the resulting holes, introduce new artifacts and
blurriness. Moreover, existing benchmark datasets for evaluating occlusion
removal methods lack realistic complexity and viewpoint variations. To address
these issues, we introduce DeclutterSet, a novel dataset featuring diverse
scenes with pronounced occlusions distributed across foreground, midground, and
background, exhibiting substantial relative motion across viewpoints. We
further introduce DeclutterNeRF, an occlusion removal method free from
generative priors. DeclutterNeRF introduces joint multi-view optimization of
learnable camera parameters, occlusion annealing regularization, and employs an
explainable stochastic structural similarity loss, ensuring high-quality,
artifact-free reconstructions from incomplete images. Experiments demonstrate
that DeclutterNeRF significantly outperforms state-of-the-art methods on our
proposed DeclutterSet, establishing a strong baseline for future research.

Comments:
- Accepted by CVPR 2025 4th CV4Metaverse Workshop. 15 pages, 10
  figures. Code and data at: https://github.com/wanzhouliu/declutter-nerf

---

## 3D Gaussian Particle Approximation of VDB Datasets: A Study for  Scientific Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Isha Sharma, Dieter Schmalstieg | cs.GR | [PDF](http://arxiv.org/pdf/2504.04857v1){: .btn .btn-green } |

**Abstract**: The complexity and scale of Volumetric and Simulation datasets for Scientific
Visualization(SciVis) continue to grow. And the approaches and advantages of
memory-efficient data formats and storage techniques for such datasets vary.
OpenVDB library and its VDB data format excels in memory efficiency through its
hierarchical and dynamic tree structure, with active and inactive sub-trees for
data storage. It is heavily used in current production renderers for both
animation and rendering stages in VFX pipelines and photorealistic rendering of
volumes and fluids. However, it still remains to be fully leveraged in SciVis
where domains dealing with sparse scalar fields like porous media, time varying
volumes such as tornado and weather simulation or high resolution simulation of
Computational Fluid Dynamics present ample number of large challenging data
sets.Goal of this paper is not only to explore the use of OpenVDB in SciVis but
also to explore a level of detail(LOD) technique using 3D Gaussian particles
approximating voxel regions. For rendering, we utilize NVIDIA OptiX library for
ray marching through the Gaussians particles. Data modeling using 3D Gaussians
has been very popular lately due to success in stereoscopic image to 3D scene
conversion using Gaussian Splatting and Gaussian approximation and mixture
models aren't entirely new in SciVis as well. Our work explores the integration
with rendering software libraries like OpenVDB and OptiX to take advantage of
their built-in memory compaction and hardware acceleration features, while also
leveraging the performance capabilities of modern GPUs. Thus, we present a
SciVis rendering approach that uses 3D Gaussians at varying LOD in a lossy
scheme derived from VDB datasets, rather than focusing on photorealistic volume
rendering.



---

## L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Yi-Zhen Tsai, Xuechen Zhang, Zheng Li, Jiasi Chen | cs.GR | [PDF](http://arxiv.org/pdf/2504.05517v1){: .btn .btn-green } |

**Abstract**: Traditional 3D content representations include dense point clouds that
consume large amounts of data and hence network bandwidth, while newer
representations such as neural radiance fields suffer from poor frame rates due
to their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS)
can be seen as a generalization of point clouds that meet the best of both
worlds, with high visual quality and efficient rendering for real-time frame
rates. However, delivering 3DGS scenes from a hosting server to client devices
is still challenging due to high network data consumption (e.g., 1.5 GB for a
single scene). The goal of this work is to create an efficient 3D content
delivery framework that allows users to view high quality 3D scenes with 3DGS
as the underlying data representation. The main contributions of the paper are:
(1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling
algorithms to choose what splats to download at what time, and (3) Trace-driven
experiments from users wearing virtual reality headsets to evaluate the visual
quality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS
demonstrates high visual quality, achieving 16.9% higher average SSIM compared
to baselines, and also works with other compressed 3DGS representations.



---

## Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather  Effects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Gal Fiebelman, Hadar Averbuch-Elor, Sagie Benaim | cs.GR | [PDF](http://arxiv.org/pdf/2504.05296v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently enabled fast and photorealistic
reconstruction of static 3D scenes. However, introducing dynamic elements that
interact naturally with such static scenes remains challenging. Accordingly, we
present a novel hybrid framework that combines Gaussian-particle
representations for incorporating physically-based global weather effects into
static 3D Gaussian Splatting scenes, correctly handling the interactions of
dynamic elements with the static scene. We follow a three-stage process: we
first map static 3D Gaussians to a particle-based representation. We then
introduce dynamic particles and simulate their motion using the Material Point
Method (MPM). Finally, we map the simulated particles back to the Gaussian
domain while introducing appearance parameters tailored for specific effects.
To correctly handle the interactions of dynamic elements with the static scene,
we introduce specialized collision handling techniques. Our approach supports a
variety of weather effects, including snowfall, rainfall, fog, and sandstorms,
and can also support falling objects, all with physically plausible motion and
appearance. Experiments demonstrate that our method significantly outperforms
existing approaches in both visual quality and physical realism.

Comments:
- Project webpage: https://galfiebelman.github.io/let-it-snow/

---

## View-Dependent Deformation Fields for 2D Editing of 3D Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Martin El Mqirmi, Noam Aigerman | cs.GR | [PDF](http://arxiv.org/pdf/2504.05544v1){: .btn .btn-green } |

**Abstract**: We propose a method for authoring non-realistic 3D objects (represented as
either 3D Gaussian Splats or meshes), that comply with 2D edits from specific
viewpoints. Namely, given a 3D object, a user chooses different viewpoints and
interactively deforms the object in the 2D image plane of each view. The method
then produces a "deformation field" - an interpolation between those 2D
deformations in a smooth manner as the viewpoint changes. Our core observation
is that the 2D deformations do not need to be tied to an underlying object, nor
share the same deformation space. We use this observation to devise a method
for authoring view-dependent deformations, holding several technical
contributions: first, a novel way to compositionality-blend between the 2D
deformations after lifting them to 3D - this enables the user to "stack" the
deformations similarly to layers in an editing software, each deformation
operating on the results of the previous; second, a novel method to apply the
3D deformation to 3D Gaussian Splats; third, an approach to author the 2D
deformations, by deforming a 2D mesh encapsulating a rendered image of the
object. We show the versatility and efficacy of our method by adding cartoonish
effects to objects, providing means to modify human characters, fitting 3D
models to given 2D sketches and caricatures, resolving occlusions, and
recreating classic non-realistic paintings as 3D models.



---

## Tool-as-Interface: Learning Robot Policies from Human Tool Usage through  Imitation Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-06 | Haonan Chen, Cheng Zhu, Yunzhu Li, Katherine Driggs-Campbell | cs.RO | [PDF](http://arxiv.org/pdf/2504.04612v1){: .btn .btn-green } |

**Abstract**: Tool use is critical for enabling robots to perform complex real-world tasks,
and leveraging human tool-use data can be instrumental for teaching robots.
However, existing data collection methods like teleoperation are slow, prone to
control delays, and unsuitable for dynamic tasks. In contrast, human natural
data, where humans directly perform tasks with tools, offers natural,
unstructured interactions that are both efficient and easy to collect. Building
on the insight that humans and robots can share the same tools, we propose a
framework to transfer tool-use knowledge from human data to robots. Using two
RGB cameras, our method generates 3D reconstruction, applies Gaussian splatting
for novel view augmentation, employs segmentation models to extract
embodiment-agnostic observations, and leverages task-space tool-action
representations to train visuomotor policies. We validate our approach on
diverse real-world tasks, including meatball scooping, pan flipping, wine
bottle balancing, and other complex tasks. Our method achieves a 71\% higher
average success rate compared to diffusion policies trained with teleoperation
data and reduces data collection time by 77\%, with some tasks solvable only by
our framework. Compared to hand-held gripper, our method cuts data collection
time by 41\%. Additionally, our method bridges the embodiment gap, improves
robustness to variations in camera viewpoints and robot configurations, and
generalizes effectively across objects and spatial setups.

Comments:
- Project Page: https://tool-as-interface.github.io. 17 pages, 14
  figures

---

## Thermoxels: a voxel-based method to generate simulation-ready 3D thermal  models

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-06 | Etienne Chassaing, Florent Forest, Olga Fink, Malcolm Mielle | cs.CV | [PDF](http://arxiv.org/pdf/2504.04448v1){: .btn .btn-green } |

**Abstract**: In the European Union, buildings account for 42% of energy use and 35% of
greenhouse gas emissions. Since most existing buildings will still be in use by
2050, retrofitting is crucial for emissions reduction. However, current
building assessment methods rely mainly on qualitative thermal imaging, which
limits data-driven decisions for energy savings. On the other hand,
quantitative assessments using finite element analysis (FEA) offer precise
insights but require manual CAD design, which is tedious and error-prone.
Recent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and
Gaussian Splatting, enable precise 3D modeling from sparse images but lack
clearly defined volumes and the interfaces between them needed for FEA. We
propose Thermoxels, a novel voxel-based method able to generate FEA-compatible
models, including both geometry and temperature, from a sparse set of RGB and
thermal images. Using pairs of RGB and thermal images as input, Thermoxels
represents a scene's geometry as a set of voxels comprising color and
temperature information. After optimization, a simple process is used to
transform Thermoxels' models into tetrahedral meshes compatible with FEA. We
demonstrate Thermoxels' capability to generate RGB+Thermal meshes of 3D scenes,
surpassing other state-of-the-art methods. To showcase the practical
applications of Thermoxels' models, we conduct a simple heat conduction
simulation using FEA, achieving convergence from an initial state defined by
Thermoxels' thermal reconstruction. Additionally, we compare Thermoxels' image
synthesis abilities with current state-of-the-art methods, showing competitive
results, and discuss the limitations of existing metrics in assessing mesh
quality.

Comments:
- 7 pages, 2 figures

---

## 3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-05 | Zhisheng Huang, Peng Wang, Jingdong Zhang, Yuan Liu, Xin Li, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.04294v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its
efficiency and quality, but like many novel view synthesis methods, it heavily
depends on accurate camera poses from Structure-from-Motion (SfM) systems.
Although recent SfM pipelines have made impressive progress, questions remain
about how to further improve both their robust performance in challenging
conditions (e.g., textureless scenes) and the precision of camera parameter
estimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework
that bridges this gap by jointly optimizing 3D Gaussians and camera parameters
from large reconstruction priors MASt3R-SfM. We note that naively performing
joint 3D Gaussian and camera optimization faces two challenges: the sensitivity
to the quality of SfM initialization, and its limited capacity for global
optimization, leading to suboptimal reconstruction results. Our 3R-GS,
overcomes these issues by incorporating optimized practices, enabling robust
scene reconstruction even with imperfect camera registration. Extensive
experiments demonstrate that 3R-GS delivers high-quality novel view synthesis
and precise camera pose estimation while remaining computationally efficient.
Project page: https://zsh523.github.io/3R-GS/



---

## Interpretable Single-View 3D Gaussian Splatting using Unsupervised  Hierarchical Disentangled Representation Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-05 | Yuyang Zhang, Baao Xie, Hu Zhu, Qi Wang, Huanting Guo, Xin Jin, Wenjun Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2504.04190v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has recently marked a significant advancement in 3D
reconstruction, delivering both rapid rendering and high-quality results.
However, existing 3DGS methods pose challenges in understanding underlying 3D
semantics, which hinders model controllability and interpretability. To address
it, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to
discover both coarse- and fine-grained 3D semantics via hierarchical
disentangled representation learning (DRL). Specifically, the model employs a
dual-branch architecture, consisting of a point cloud initialization branch and
a triplane-Gaussian generation branch, to achieve coarse-grained
disentanglement by separating 3D geometry and visual appearance features.
Subsequently, fine-grained semantic representations within each modality are
further discovered through DRL-based encoder-adapters. To our knowledge, this
is the first work to achieve unsupervised interpretable 3DGS. Evaluations
indicate that our model achieves 3D disentanglement while preserving
high-quality and rapid reconstruction.



---

## NeRFlex: Resource-aware Real-time High-quality Rendering of Complex  Scenes on Mobile Devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Zhe Wang, Yifei Zhu | cs.GR | [PDF](http://arxiv.org/pdf/2504.03415v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) is a cutting-edge neural network-based
technique for novel view synthesis in 3D reconstruction. However, its
significant computational demands pose challenges for deployment on mobile
devices. While mesh-based NeRF solutions have shown potential in achieving
real-time rendering on mobile platforms, they often fail to deliver
high-quality reconstructions when rendering practical complex scenes.
Additionally, the non-negligible memory overhead caused by pre-computed
intermediate results complicates their practical application. To overcome these
challenges, we present NeRFlex, a resource-aware, high-resolution, real-time
rendering framework for complex scenes on mobile devices. NeRFlex integrates
mobile NeRF rendering with multi-NeRF representations that decompose a scene
into multiple sub-scenes, each represented by an individual NeRF network.
Crucially, NeRFlex considers both memory and computation constraints as
first-class citizens and redesigns the reconstruction process accordingly.
NeRFlex first designs a detail-oriented segmentation module to identify
sub-scenes with high-frequency details. For each NeRF network, a lightweight
profiler, built on domain knowledge, is used to accurately map configurations
to visual quality and memory usage. Based on these insights and the resource
constraints on mobile devices, NeRFlex presents a dynamic programming algorithm
to efficiently determine configurations for all NeRF representations, despite
the NP-hardness of the original decision problem. Extensive experiments on
real-world datasets and mobile devices demonstrate that NeRFlex achieves
real-time, high-quality rendering on commercial mobile devices.

Comments:
- This paper is accepted by 45th IEEE International Conference on
  Distributed Computing Systems (ICDCS 2025)

---

## HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction  via Gaussian Restoration


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Boyuan Wang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Guan Huang, Lihong Liu, Xingang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.03536v1){: .btn .btn-green } |

**Abstract**: Single-image human reconstruction is vital for digital human modeling
applications but remains an extremely challenging task. Current approaches rely
on generative models to synthesize multi-view images for subsequent 3D
reconstruction and animation. However, directly generating multiple views from
a single human image suffers from geometric inconsistencies, resulting in
issues like fragmented or blurred limbs in the reconstructed models. To tackle
these limitations, we introduce \textbf{HumanDreamer-X}, a novel framework that
integrates multi-view human generation and reconstruction into a unified
pipeline, which significantly enhances the geometric consistency and visual
fidelity of the reconstructed 3D models. In this framework, 3D Gaussian
Splatting serves as an explicit 3D representation to provide initial geometry
and appearance priority. Building upon this foundation, \textbf{HumanFixer} is
trained to restore 3DGS renderings, which guarantee photorealistic results.
Furthermore, we delve into the inherent challenges associated with attention
mechanisms in multi-view human generation, and propose an attention modulation
strategy that effectively enhances geometric details identity consistency
across multi-view. Experimental results demonstrate that our approach markedly
improves generation and reconstruction PSNR quality metrics by 16.45% and
12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing
generalization capabilities on in-the-wild data and applicability to various
human reconstruction backbone models.

Comments:
- Project Page: https://humandreamer-x.github.io/

---

## WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Jianhao Zheng, Zihan Zhu, Valentin Bieri, Marc Pollefeys, Songyou Peng, Iro Armeni | cs.CV | [PDF](http://arxiv.org/pdf/2504.03886v1){: .btn .btn-green } |

**Abstract**: We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system
designed to handle dynamic environments by leveraging uncertainty-aware
geometric mapping. Unlike traditional SLAM systems, which assume static scenes,
our approach integrates depth and uncertainty information to enhance tracking,
mapping, and rendering performance in the presence of moving objects. We
introduce an uncertainty map, predicted by a shallow multi-layer perceptron and
DINOv2 features, to guide dynamic object removal during both tracking and
mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map
optimization, improving reconstruction accuracy. Our system is evaluated on
multiple datasets and demonstrates artifact-free view synthesis. Results
showcase WildGS-SLAM's superior performance in dynamic environments compared to
state-of-the-art methods.



---

## MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Yash Kulthe, Andrew Gilbert, John Collomosse | cs.CV | [PDF](http://arxiv.org/pdf/2504.02517v1){: .btn .btn-green } |

**Abstract**: We present MultiNeRF, a 3D watermarking method that embeds multiple uniquely
keyed watermarks within images rendered by a single Neural Radiance Field
(NeRF) model, whilst maintaining high visual quality. Our approach extends the
TensoRF NeRF model by incorporating a dedicated watermark grid alongside the
existing geometry and appearance grids. This extension ensures higher watermark
capacity without entangling watermark signals with scene content. We propose a
FiLM-based conditional modulation mechanism that dynamically activates
watermarks based on input identifiers, allowing multiple independent watermarks
to be embedded and extracted without requiring model retraining. MultiNeRF is
validated on the NeRF-Synthetic and LLFF datasets, with statistically
significant improvements in robust capacity without compromising rendering
quality. By generalizing single-watermark NeRF methods into a flexible
multi-watermarking framework, MultiNeRF provides a scalable solution for 3D
content. attribution.



---

## MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Renwu Li, Wenjing Ke, Dong Li, Lu Tian, Emad Barsoum | cs.CV | [PDF](http://arxiv.org/pdf/2504.02437v1){: .btn .btn-green } |

**Abstract**: We present MonoGS++, a novel fast and accurate Simultaneous Localization and
Mapping (SLAM) method that leverages 3D Gaussian representations and operates
solely on RGB inputs. While previous 3D Gaussian Splatting (GS)-based methods
largely depended on depth sensors, our approach reduces the hardware dependency
and only requires RGB input, leveraging online visual odometry (VO) to generate
sparse point clouds in real-time. To reduce redundancy and enhance the quality
of 3D scene reconstruction, we implemented a series of methodological
enhancements in 3D Gaussian mapping. Firstly, we introduced dynamic 3D Gaussian
insertion to avoid adding redundant Gaussians in previously well-reconstructed
areas. Secondly, we introduced clarity-enhancing Gaussian densification module
and planar regularization to handle texture-less areas and flat surfaces
better. We achieved precise camera tracking results both on the synthetic
Replica and real-world TUM-RGBD datasets, comparable to those of the
state-of-the-art. Additionally, our method realized a significant 5.57x
improvement in frames per second (fps) over the previous state-of-the-art,
MonoGS.



---

## LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Ming-Jia Yang, Yu-Xiao Guo, Yang Liu, Bin Zhou, Xin Tong | cs.CV | [PDF](http://arxiv.org/pdf/2504.02337v1){: .btn .btn-green } |

**Abstract**: Generating realistic, room-level indoor scenes with semantically plausible
and detailed appearances from in-the-wild images is crucial for various
applications in VR, AR, and robotics. The success of NeRF-based generative
methods indicates a promising direction to address this challenge. However,
unlike their success at the object level, existing scene-level generative
methods require additional information, such as multiple views, depth images,
or semantic guidance, rather than relying solely on RGB images. This is because
NeRF-based methods necessitate prior knowledge of camera poses, which is
challenging to approximate for indoor scenes due to the complexity of defining
alignment and the difficulty of globally estimating poses from a single image,
given the unseen parts behind the camera. To address this challenge, we
redefine global poses within the framework of Local-Pose-Alignment (LPA) -- an
anchor-based multi-local-coordinate system that uses a selected number of
anchors as the roots of these coordinates. Building on this foundation, we
introduce LPA-GAN, a novel NeRF-based generative approach that incorporates
specific modifications to estimate the priors of camera poses under LPA. It
also co-optimizes the pose predictor and scene generation processes. Our
ablation study and comparisons with straightforward extensions of NeRF-based
object generative methods demonstrate the effectiveness of our approach.
Furthermore, visual comparisons with other techniques reveal that our method
achieves superior view-to-view consistency and semantic normality.



---

## ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Yuan Zhou, Shilong Jin, Litao Hua, Wanjun Lv, Haoran Duan, Jungong Han | cs.CV | [PDF](http://arxiv.org/pdf/2504.02316v1){: .btn .btn-green } |

**Abstract**: Recent advances in zero-shot text-to-3D generation have revolutionized 3D
content creation by enabling direct synthesis from textual descriptions. While
state-of-the-art methods leverage 3D Gaussian Splatting with score distillation
to enhance multi-view rendering through pre-trained text-to-image (T2I) models,
they suffer from inherent view biases in T2I priors. These biases lead to
inconsistent 3D generation, particularly manifesting as the multi-face Janus
problem, where objects exhibit conflicting features across views. To address
this fundamental challenge, we propose ConsDreamer, a novel framework that
mitigates view bias by refining both the conditional and unconditional terms in
the score distillation process: (1) a View Disentanglement Module (VDM) that
eliminates viewpoint biases in conditional prompts by decoupling irrelevant
view components and injecting precise camera parameters; and (2) a
similarity-based partial order loss that enforces geometric consistency in the
unconditional term by aligning cosine similarities with azimuth relationships.
Extensive experiments demonstrate that ConsDreamer effectively mitigates the
multi-face Janus problem in text-to-3D generation, outperforming existing
methods in both visual quality and consistency.

Comments:
- 13 pages, 11 figures, 3 tables

---

## Compressing 3D Gaussian Splatting by Noise-Substituted Vector  Quantization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Haishan Wang, Mohammad Hassan Vali, Arno Solin | cs.CV | [PDF](http://arxiv.org/pdf/2504.03059v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D
reconstruction, achieving high-quality results with real-time radiance field
rendering. However, a key challenge is the substantial storage cost:
reconstructing a single scene typically requires millions of Gaussian splats,
each represented by 59 floating-point parameters, resulting in approximately 1
GB of memory. To address this challenge, we propose a compression method by
building separate attribute codebooks and storing only discrete code indices.
Specifically, we employ noise-substituted vector quantization technique to
jointly train the codebooks and model features, ensuring consistency between
gradient descent optimization and parameter discretization. Our method reduces
the memory consumption efficiently (around $45\times$) while maintaining
competitive reconstruction quality on standard 3D benchmark scenes. Experiments
on different codebook sizes show the trade-off between compression ratio and
image quality. Furthermore, the trained compressed model remains fully
compatible with popular 3DGS viewers and enables faster rendering speed, making
it well-suited for practical applications.

Comments:
- Appearing in Scandinavian Conference on Image Analysis (SCIA) 2025

---

## Digital-twin imaging based on descattering Gaussian splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Suguru Shimomura, Kazuki Yamanouchi, Jun Tanida | physics.optics | [PDF](http://arxiv.org/pdf/2504.02278v1){: .btn .btn-green } |

**Abstract**: Three-dimensional imaging through scattering media is important in medical
science and astronomy. We propose a digital-twin imaging method based on
Gaussian splatting to observe an object behind a scattering medium. A digital
twin model built through data assimilation, emulates the behavior of objects
and environmental changes in a virtual space. By constructing a digital twin
using point clouds composed of Gaussians and simulating the scattering process
through the convolution of a point spread function, three-dimensional objects
behind a scattering medium can be reproduced as a digital twin. In this study,
a high-contrast digital twin reproducing a three-dimensional object was
successfully constructed from degraded images, assuming that data were acquired
from wavefronts disturbed by a scattering medium. This technique reproduces
objects by integrating data processing with image measurements.



---

## Toward Real-world BEV Perception: Depth Uncertainty Estimation via  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen | cs.CV | [PDF](http://arxiv.org/pdf/2504.01957v2){: .btn .btn-green } |

**Abstract**: Bird's-eye view (BEV) perception has gained significant attention because it
provides a unified representation to fuse multiple view images and enables a
wide range of down-stream autonomous driving tasks, such as forecasting and
planning. Recent state-of-the-art models utilize projection-based methods which
formulate BEV perception as query learning to bypass explicit depth estimation.
While we observe promising advancements in this paradigm, they still fall short
of real-world applications because of the lack of uncertainty modeling and
expensive computational requirement. In this work, we introduce GaussianLSS, a
novel uncertainty-aware BEV perception framework that revisits
unprojection-based methods, specifically the Lift-Splat-Shoot (LSS) paradigm,
and enhances them with depth un-certainty modeling. GaussianLSS represents
spatial dispersion by learning a soft depth mean and computing the variance of
the depth distribution, which implicitly captures object extents. We then
transform the depth distribution into 3D Gaussians and rasterize them to
construct uncertainty-aware BEV features. We evaluate GaussianLSS on the
nuScenes dataset, achieving state-of-the-art performance compared to
unprojection-based methods. In particular, it provides significant advantages
in speed, running 2.5x faster, and in memory efficiency, using 0.3x less memory
compared to projection-based methods, while achieving competitive performance
with only a 0.4% IoU difference.

Comments:
- Accepted to CVPR'25. https://hcis-lab.github.io/GaussianLSS/

---

## FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D  Scene Reconstruction and Benchmarking

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Ulas Gunes, Matias Turkulainen, Xuqian Ren, Arno Solin, Juho Kannala, Esa Rahtu | cs.CV | [PDF](http://arxiv.org/pdf/2504.01732v1){: .btn .btn-green } |

**Abstract**: The development of large-scale 3D scene reconstruction and novel view
synthesis methods mostly rely on datasets comprising perspective images with
narrow fields of view (FoV). While effective for small-scale scenes, these
datasets require large image sets and extensive structure-from-motion (SfM)
processing, limiting scalability. To address this, we introduce a fisheye image
dataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye
lenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor
scenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense
point clouds that can be used as geometric ground-truth, enabling robust
benchmarking under challenging conditions such as occlusions and reflections.
While the baseline experiments focus on vanilla Gaussian Splatting and NeRF
based Nerfacto methods, the dataset supports diverse approaches for scene
reconstruction, novel view synthesis, and image-based rendering.

Comments:
- SCIA 2025

---

## 3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Hao Wu, Hao Wang, Ruochong Li, Xuran Ma, Hui Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2504.01619v1){: .btn .btn-green } |

**Abstract**: Recent advancements in text-to-3D generation have shown remarkable results by
leveraging 3D priors in combination with 2D diffusion. However, previous
methods utilize 3D priors that lack detailed and complex structural
information, limiting them to generating simple objects and presenting
challenges for creating intricate structures such as bonsai. In this paper, we
propose 3DBonsai, a novel text-to-3D framework for generating 3D bonsai with
complex structures. Technically, we first design a trainable 3D space
colonization algorithm to produce bonsai structures, which are then enhanced
through random sampling and point cloud augmentation to serve as the 3D
Gaussian priors. We introduce two bonsai generation pipelines with distinct
structural levels: fine structure conditioned generation, which initializes 3D
Gaussians using a 3D structure prior to produce detailed and complex bonsai,
and coarse structure conditioned generation, which employs a multi-view
structure consistency module to align 2D and 3D structures. Moreover, we have
compiled a unified 2D and 3D Chinese-style bonsai dataset. Our experimental
results demonstrate that 3DBonsai significantly outperforms existing methods,
providing a new benchmark for structure-aware 3D bonsai generation.

Comments:
- Accepted by ICME 2025

---

## WorldPrompter: Traversable Text-to-Scene Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Zhaoyang Zhang, Yannick Hold-Geoffroy, Miloš Hašan, Chen Ziwen, Fujun Luan, Julie Dorsey, Yiwei Hu | cs.GR | [PDF](http://arxiv.org/pdf/2504.02045v1){: .btn .btn-green } |

**Abstract**: Scene-level 3D generation is a challenging research topic, with most existing
methods generating only partial scenes and offering limited navigational
freedom. We introduce WorldPrompter, a novel generative pipeline for
synthesizing traversable 3D scenes from text prompts. We leverage panoramic
videos as an intermediate representation to model the 360{\deg} details of a
scene. WorldPrompter incorporates a conditional 360{\deg} panoramic video
generator, capable of producing a 128-frame video that simulates a person
walking through and capturing a virtual environment. The resulting video is
then reconstructed as Gaussian splats by a fast feedforward 3D reconstructor,
enabling a true walkable experience within the 3D scene. Experiments
demonstrate that our panoramic video generation model achieves convincing view
consistency across frames, enabling high-quality panoramic Gaussian splat
reconstruction and facilitating traversal over an area of the scene.
Qualitative and quantitative results also show it outperforms the
state-of-the-art 360{\deg} video generators and 3D scene generation models.



---

## UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Jaehoon Choi, Dongki Jung, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2504.02158v1){: .btn .btn-green } |

**Abstract**: We present UAVTwin, a method for creating digital twins from real-world
environments and facilitating data augmentation for training downstream models
embedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses
on synthesizing foreground components, such as various human instances in
motion within complex scene backgrounds, from UAV perspectives. This is
achieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing
backgrounds along with controllable synthetic human models that display diverse
appearances and actions in multiple poses. To the best of our knowledge,
UAVTwin is the first approach for UAV-based perception that is capable of
generating high-fidelity digital twins based on 3DGS. The proposed work
significantly enhances downstream models through data augmentation for
real-world environments with multiple dynamic objects and significant
appearance variations-both of which typically introduce artifacts in 3DGS-based
modeling. To tackle these challenges, we propose a novel appearance modeling
strategy and a mask refinement module to enhance the training of 3D Gaussian
Splatting. We demonstrate the high quality of neural rendering by achieving a
1.23 dB improvement in PSNR compared to recent methods. Furthermore, we
validate the effectiveness of data augmentation by showing a 2.5% to 13.7%
improvement in mAP for the human detection task.



---

## Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D  Reconstruction and Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar | cs.CV | [PDF](http://arxiv.org/pdf/2504.01960v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance
Fields (NeRF) have achieved impressive results in real-time 3D reconstruction
and novel view synthesis. However, these methods struggle in large-scale,
unconstrained environments where sparse and uneven input coverage, transient
occlusions, appearance variability, and inconsistent camera settings lead to
degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a
multi-view diffusion model to address these limitations. By generating
pseudo-observations conditioned on multi-view inputs, our method transforms
under-constrained 3D reconstruction problems into well-posed ones, enabling
robust optimization even with sparse data. GS-Diff further integrates several
enhancements, including appearance embedding, monocular depth priors, dynamic
object modeling, anisotropy regularization, and advanced rasterization
techniques, to tackle geometric and photometric challenges in real-world
settings. Experiments on four benchmarks demonstrate that GS-Diff consistently
outperforms state-of-the-art baselines by significant margins.

Comments:
- WACV ULTRRA Workshop 2025

---

## 3D Gaussian Inverse Rendering with Approximated Global Illumination

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Zirui Wu, Jianteng Chen, Laijian Li, Shaoteng Wu, Zhikai Zhu, Kang Xu, Martin R. Oswald, Jie Song | cs.GR | [PDF](http://arxiv.org/pdf/2504.01358v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting shows great potential in reconstructing photo-realistic
3D scenes. However, these methods typically bake illumination into their
representations, limiting their use for physically-based rendering and scene
editing. Although recent inverse rendering approaches aim to decompose scenes
into material and lighting components, they often rely on simplifying
assumptions that fail when editing. We present a novel approach that enables
efficient global illumination for 3D Gaussians Splatting through screen-space
ray tracing. Our key insight is that a substantial amount of indirect light can
be traced back to surfaces visible within the current view frustum. Leveraging
this observation, we augment the direct shading computed by 3D Gaussians with
Monte-Carlo screen-space ray-tracing to capture one-bounce indirect
illumination. In this way, our method enables realistic global illumination
without sacrificing the computational efficiency and editability benefits of 3D
Gaussians. Through experiments, we show that the screen-space approximation we
utilize allows for indirect illumination and supports real-time rendering and
editing. Code, data, and models will be made available at our project page:
https://wuzirui.github.io/gs-ssr.



---

## Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting  Conditions with View-Adaptive Curve Adjustment

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Ziteng Cui, Xuangeng Chu, Tatsuya Harada | cs.CV | [PDF](http://arxiv.org/pdf/2504.01503v1){: .btn .btn-green } |

**Abstract**: Capturing high-quality photographs under diverse real-world lighting
conditions is challenging, as both natural lighting (e.g., low-light) and
camera exposure settings (e.g., exposure time) significantly impact image
quality. This challenge becomes more pronounced in multi-view scenarios, where
variations in lighting and image signal processor (ISP) settings across
viewpoints introduce photometric inconsistencies. Such lighting degradations
and view-dependent variations pose substantial challenges to novel view
synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel
approach to achieving high-quality novel view synthesis results under diverse
challenging lighting conditions using 3DGS. By adopting per-view color matrix
mapping and view-adaptive curve adjustments, Luminance-GS achieves
state-of-the-art (SOTA) results across various lighting conditions -- including
low-light, overexposure, and varying exposure -- while not altering the
original 3DGS explicit representation. Compared to previous NeRF- and
3DGS-based baselines, Luminance-GS provides real-time rendering speed with
improved reconstruction quality.

Comments:
- CVPR 2025, project page:
  https://cuiziteng.github.io/Luminance_GS_web/

---

## FlowR: Flowing from Sparse to Dense 3D Reconstructions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Tobias Fischer, Samuel Rota Bulò, Yung-Hsu Yang, Nikhil Varma Keetha, Lorenzo Porzi, Norman Müller, Katja Schwarz, Jonathon Luiten, Marc Pollefeys, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2504.01647v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting enables high-quality novel view synthesis (NVS) at
real-time frame rates. However, its quality drops sharply as we depart from the
training views. Thus, dense captures are needed to match the high-quality
expectations of some applications, e.g. Virtual Reality (VR). However, such
dense captures are very laborious and expensive to obtain. Existing works have
explored using 2D generative models to alleviate this requirement by
distillation or generating additional training views. These methods are often
conditioned only on a handful of reference input views and thus do not fully
exploit the available 3D information, leading to inconsistent generation
results and reconstruction artifacts. To tackle this problem, we propose a
multi-view, flow matching model that learns a flow to connect novel view
renderings from possibly sparse reconstructions to renderings that we expect
from dense reconstructions. This enables augmenting scene captures with novel,
generated views to improve reconstruction quality. Our model is trained on a
novel dataset of 3.6M image pairs and can process up to 45 views at 540x960
resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline
consistently improves NVS in sparse- and dense-view scenarios, leading to
higher-quality reconstructions than prior works across multiple, widely-used
NVS benchmarks.

Comments:
- Project page is available at https://tobiasfshr.github.io/pub/flowr

---

## RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable  3D Gaussian Avatars

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Yahui Li, Zhi Zeng, Liming Pang, Guixuan Zhang, Shuwu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.01559v1){: .btn .btn-green } |

**Abstract**: Modeling animatable human avatars from monocular or multi-view videos has
been widely studied, with recent approaches leveraging neural radiance fields
(NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in
novel-view and novel-pose synthesis. However, existing methods often struggle
to accurately capture the dynamics of loose clothing, as they primarily rely on
global pose conditioning or static per-frame representations, leading to
oversmoothing and temporal inconsistencies in non-rigid regions. To address
this, We propose RealityAvatar, an efficient framework for high-fidelity
digital human modeling, specifically targeting loosely dressed avatars. Our
method leverages 3D Gaussian Splatting to capture complex clothing deformations
and motion dynamics while ensuring geometric consistency. By incorporating a
motion trend module and a latentbone encoder, we explicitly model
pose-dependent deformations and temporal variations in clothing behavior.
Extensive experiments on benchmark datasets demonstrate the effectiveness of
our approach in capturing fine-grained clothing deformations and motion-driven
shape variations. Our method significantly enhances structural fidelity and
perceptual quality in dynamic human reconstruction, particularly in non-rigid
regions, while achieving better consistency across temporal frames.



---

## High-fidelity 3D Object Generation from Single Image with RGBN-Volume  Gaussian Reconstruction Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao | cs.CV | [PDF](http://arxiv.org/pdf/2504.01512v1){: .btn .btn-green } |

**Abstract**: Recently single-view 3D generation via Gaussian splatting has emerged and
developed quickly. They learn 3D Gaussians from 2D RGB images generated from
pre-trained multi-view diffusion (MVD) models, and have shown a promising
avenue for 3D generation through a single image. Despite the current progress,
these methods still suffer from the inconsistency jointly caused by the
geometric ambiguity in the 2D images, and the lack of structure of 3D
Gaussians, leading to distorted and blurry 3D object generation. In this paper,
we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian
Reconstruction Model designed to generate high-fidelity 3D objects from
single-view images. Our key insight is a structured 3D representation can
simultaneously mitigate the afore-mentioned two issues. To this end, we propose
a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation
contains explicit 3D geometric information, eliminating the geometric ambiguity
from 2D images. It also structures Gaussians during learning so that the
optimization tends to find better local optima. Our 3D voxel representation is
obtained by a fusion module that aligns RGB features and surface normal
features, both of which can be estimated from 2D images. Extensive experiments
demonstrate the superiority of our methods over prior works in terms of
high-quality reconstruction results, robust generalization, and good
efficiency.

Comments:
- 12 pages

---

## BOGausS: Better Optimized Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Stéphane Pateux, Matthieu Gendrin, Luce Morin, Théo Ladune, Xiaoran Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2504.01844v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view
synthesis. Its framework provides fast and high-fidelity rendering. Although
less complex than other solutions such as Neural Radiance Fields (NeRF), there
are still some challenges building smaller models without sacrificing quality.
In this study, we perform a careful analysis of 3DGS training process and
propose a new optimization methodology. Our Better Optimized Gaussian Splatting
(BOGausS) solution is able to generate models up to ten times lighter than the
original 3DGS with no quality degradation, thus significantly boosting the
performance of Gaussian Splatting compared to the state of the art.



---

## UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene  Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Yunxuan Mao, Rong Xiong, Yue Wang, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2504.00763v1){: .btn .btn-green } |

**Abstract**: Reconstructing and decomposing dynamic urban scenes is crucial for autonomous
driving, urban planning, and scene editing. However, existing methods fail to
perform instance-aware decomposition without manual annotations, which is
crucial for instance-level scene editing.We propose UnIRe, a 3D Gaussian
Splatting (3DGS) based approach that decomposes a scene into a static
background and individual dynamic instances using only RGB images and LiDAR
point clouds. At its core, we introduce 4D superpoints, a novel representation
that clusters multi-frame LiDAR points in 4D space, enabling unsupervised
instance separation based on spatiotemporal correlations. These 4D superpoints
serve as the foundation for our decomposed 4D initialization, i.e., providing
spatial and temporal initialization to train a dynamic 3DGS for arbitrary
dynamic classes without requiring bounding boxes or object
templates.Furthermore, we introduce a smoothness regularization strategy in
both 2D and 3D space, further improving the temporal stability.Experiments on
benchmark datasets show that our method outperforms existing methods in
decomposed dynamic scene reconstruction while enabling accurate and flexible
instance-level editing, making it a practical solution for real-world
applications.



---

## Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Tianqi Ding, Dawei Xiang, Pablo Rivas, Liang Dong | cs.CV | [PDF](http://arxiv.org/pdf/2504.00950v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have become a popular 3D reconstruction
approach in recent years. While they produce high-quality results, they also
demand lengthy training times, often spanning days. This paper studies neural
pruning as a strategy to address these concerns. We compare pruning approaches,
including uniform sampling, importance-based methods, and coreset-based
techniques, to reduce the model size and speed up training. Our findings show
that coreset-driven pruning can achieve a 50% reduction in model size and a 35%
speedup in training, with only a slight decrease in accuracy. These results
suggest that pruning can be an effective method for improving the efficiency of
NeRF models in resource-limited settings.

Comments:
- 12 pages, 4 figures, accepted by International Conference on the AI
  Revolution: Research, Ethics, and Society (AIR-RES 2025)

---

## Distilling Multi-view Diffusion Models into 3D Generators

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2504.00457v3){: .btn .btn-green } |

**Abstract**: We introduce DD3G, a formulation that Distills a multi-view Diffusion model
(MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and
integrates extensive visual and spatial geometric knowledge from the MV-DM by
simulating its ordinary differential equation (ODE) trajectory, ensuring the
distilled generator generalizes better than those trained solely on 3D data.
Unlike previous amortized optimization approaches, we align the MV-DM and 3D
generator representation spaces to transfer the teacher's probabilistic flow to
the student, thus avoiding inconsistencies in optimization objectives caused by
probabilistic sampling. The introduction of probabilistic flow and the coupling
of various attributes in 3D Gaussians introduce challenges in the generation
process. To tackle this, we propose PEPD, a generator consisting of Pattern
Extraction and Progressive Decoding phases, which enables efficient fusion of
probabilistic flow and converts a single image into 3D Gaussians within 0.06
seconds. Furthermore, to reduce knowledge loss and overcome sparse-view
supervision, we design a joint optimization objective that ensures the quality
of generated samples through explicit supervision and implicit verification.
Leveraging existing 2D generation models, we compile 120k high-quality RGBA
images for distillation. Experiments on synthetic and public datasets
demonstrate the effectiveness of our method. Our project is available at:
https://qinbaigao.github.io/DD3G_project/



---

## OccludeNeRF: Geometric-aware 3D Scene Inpainting with Collaborative  Score Distillation in NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Jingyu Shi, Achleshwar Luthra, Jiazhi Li, Xiang Gao, Xiyun Song, Zongfang Lin, David Gu, Heather Yu | eess.IV | [PDF](http://arxiv.org/pdf/2504.02007v1){: .btn .btn-green } |

**Abstract**: With Neural Radiance Fields (NeRFs) arising as a powerful 3D representation,
research has investigated its various downstream tasks, including inpainting
NeRFs with 2D images. Despite successful efforts addressing the view
consistency and geometry quality, prior methods yet suffer from occlusion in
NeRF inpainting tasks, where 2D prior is severely limited in forming a faithful
reconstruction of the scene to inpaint.
  To address this, we propose a novel approach that enables cross-view
information sharing during knowledge distillation from a diffusion model,
effectively propagating occluded information across limited views.
Additionally, to align the distillation direction across multiple sampled
views, we apply a grid-based denoising strategy and incorporate additional
rendered views to enhance cross-view consistency. To assess our approach's
capability of handling occlusion cases, we construct a dataset consisting of
challenging scenes with severe occlusion, in addition to existing datasets.
Compared with baseline methods, our method demonstrates better performance in
cross-view consistency and faithfulness in reconstruction, while preserving
high rendering quality and fidelity.

Comments:
- CVPR 2025 CV4Metaverse

---

## NeuRadar: Neural Radiance Fields for Automotive Radar Point Clouds

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Mahan Rafidashti, Ji Lan, Maryam Fatemi, Junsheng Fu, Lars Hammarstrand, Lennart Svensson | cs.CV | [PDF](http://arxiv.org/pdf/2504.00859v1){: .btn .btn-green } |

**Abstract**: Radar is an important sensor for autonomous driving (AD) systems due to its
robustness to adverse weather and different lighting conditions. Novel view
synthesis using neural radiance fields (NeRFs) has recently received
considerable attention in AD due to its potential to enable efficient testing
and validation but remains unexplored for radar point clouds. In this paper, we
present NeuRadar, a NeRF-based model that jointly generates radar point clouds,
camera images, and lidar point clouds. We explore set-based object detection
methods such as DETR, and propose an encoder-based solution grounded in the
NeRF geometry for improved generalizability. We propose both a deterministic
and a probabilistic point cloud representation to accurately model the radar
behavior, with the latter being able to capture radar's stochastic behavior. We
achieve realistic reconstruction results for two automotive datasets,
establishing a baseline for NeRF-based radar point cloud simulation models. In
addition, we release radar data for ZOD's Sequences and Drives to enable
further research in this field. To encourage further development of radar
NeRFs, we release the source code for NeuRadar.



---

## DropGaussian: Structural Regularization for Sparse-view Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Hyunwoo Park, Gun Ryu, Wonjun Kim | cs.CV | [PDF](http://arxiv.org/pdf/2504.00773v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in
the field of novel view synthesis due to its fast performance while yielding
the excellent image quality. However, 3DGS in sparse-view settings (e.g.,
three-view inputs) often faces with the problem of overfitting to training
views, which significantly drops the visual quality of novel view images. Many
existing approaches have tackled this issue by using strong priors, such as 2D
generative contextual information and external depth signals. In contrast, this
paper introduces a prior-free method, so-called DropGaussian, with simple
changes in 3D Gaussian splatting. Specifically, we randomly remove Gaussians
during the training process in a similar way of dropout, which allows
non-excluded Gaussians to have larger gradients while improving their
visibility. This makes the remaining Gaussians to contribute more to the
optimization process for rendering with sparse input views. Such simple
operation effectively alleviates the overfitting problem and enhances the
quality of novel view synthesis. By simply applying DropGaussian to the
original 3DGS framework, we can achieve the competitive performance with
existing prior-based 3DGS methods in sparse-view settings of benchmark datasets
without any additional complexity. The code and model are publicly available
at: https://github.com/DCVL-3D/DropGaussian release.

Comments:
- Accepted by CVPR 2025

---

## Robust LiDAR-Camera Calibration with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Shuyi Zhou, Shuxiang Xie, Ryoichi Ishikawa, Takeshi Oishi | cs.RO | [PDF](http://arxiv.org/pdf/2504.00525v1){: .btn .btn-green } |

**Abstract**: LiDAR-camera systems have become increasingly popular in robotics recently. A
critical and initial step in integrating the LiDAR and camera data is the
calibration of the LiDAR-camera system. Most existing calibration methods rely
on auxiliary target objects, which often involve complex manual operations,
whereas targetless methods have yet to achieve practical effectiveness.
Recognizing that 2D Gaussian Splatting (2DGS) can reconstruct geometric
information from camera image sequences, we propose a calibration method that
estimates LiDAR-camera extrinsic parameters using geometric constraints. The
proposed method begins by reconstructing colorless 2DGS using LiDAR point
clouds. Subsequently, we update the colors of the Gaussian splats by minimizing
the photometric loss. The extrinsic parameters are optimized during this
process. Additionally, we address the limitations of the photometric loss by
incorporating the reprojection and triangulation losses, thereby enhancing the
calibration robustness and accuracy.

Comments:
- Accepted in IEEE Robotics and Automation Letters. Code available at:
  https://github.com/ShuyiZhou495/RobustCalibration

---

## ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with  Multi-modal Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Qi Song, Chenghong Li, Haotong Lin, Sida Peng, Rui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2504.00437v1){: .btn .btn-green } |

**Abstract**: We present a novel approach, termed ADGaussian, for generalizable street
scene reconstruction. The proposed method enables high-quality rendering from
single-view input. Unlike prior Gaussian Splatting methods that primarily focus
on geometry refinement, we emphasize the importance of joint optimization of
image and depth features for accurate Gaussian prediction. To this end, we
first incorporate sparse LiDAR depth as an additional input modality,
formulating the Gaussian prediction process as a joint learning framework of
visual information and geometric clue. Furthermore, we propose a multi-modal
feature matching strategy coupled with a multi-scale Gaussian decoding model to
enhance the joint refinement of multi-modal features, thereby enabling
efficient multi-modal Gaussian learning. Extensive experiments on two
large-scale autonomous driving datasets, Waymo and KITTI, demonstrate that our
ADGaussian achieves state-of-the-art performance and exhibits superior
zero-shot generalization capabilities in novel-view shifting.

Comments:
- The project page can be found at
  https://maggiesong7.github.io/research/ADGaussian/

---

## Monocular and Generalizable Gaussian Talking Head Animation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Shengjie Gong, Haojie Li, Jiapeng Tang, Dongming Hu, Shuangping Huang, Hao Chen, Tianshui Chen, Zhuoman Liu | cs.CV | [PDF](http://arxiv.org/pdf/2504.00665v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce Monocular and Generalizable Gaussian Talking Head
Animation (MGGTalk), which requires monocular datasets and generalizes to
unseen identities without personalized re-training. Compared with previous 3D
Gaussian Splatting (3DGS) methods that requires elusive multi-view datasets or
tedious personalized learning/inference, MGGtalk enables more practical and
broader applications. However, in the absence of multi-view and personalized
training data, the incompleteness of geometric and appearance information poses
a significant challenge. To address these challenges, MGGTalk explores depth
information to enhance geometric and facial symmetry characteristics to
supplement both geometric and appearance features. Initially, based on the
pixel-wise geometric information obtained from depth estimation, we incorporate
symmetry operations and point cloud filtering techniques to ensure a complete
and precise position parameter for 3DGS. Subsequently, we adopt a two-stage
strategy with symmetric priors for predicting the remaining 3DGS parameters. We
begin by predicting Gaussian parameters for the visible facial regions of the
source image. These parameters are subsequently utilized to improve the
prediction of Gaussian parameters for the non-visible regions. Extensive
experiments demonstrate that MGGTalk surpasses previous state-of-the-art
methods, achieving superior performance across various metrics.

Comments:
- Accepted by CVPR 2025

---

## Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single  Panoramic Image for Your Immerse Exploration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Zilong Huang, Jun He, Junyan Ye, Lihan Jiang, Weijia Li, Yiping Chen, Ting Han | cs.CV | [PDF](http://arxiv.org/pdf/2504.00387v1){: .btn .btn-green } |

**Abstract**: The reconstruction of immersive and realistic 3D scenes holds significant
practical importance in various fields of computer vision and computer
graphics. Typically, immersive and realistic scenes should be free from
obstructions by dynamic objects, maintain global texture consistency, and allow
for unrestricted exploration. The current mainstream methods for image-driven
scene construction involves iteratively refining the initial image using a
moving virtual camera to generate the scene. However, previous methods struggle
with visual discontinuities due to global texture inconsistencies under varying
camera poses, and they frequently exhibit scene voids caused by
foreground-background occlusions. To this end, we propose a novel layered 3D
scene reconstruction framework from panoramic image, named Scene4U.
Specifically, Scene4U integrates an open-vocabulary segmentation model with a
large language model to decompose a real panorama into multiple layers. Then,
we employs a layered repair module based on diffusion model to restore occluded
regions using visual cues and depth information, generating a hierarchical
representation of the scene. The multi-layer panorama is then initialized as a
3D Gaussian Splatting representation, followed by layered optimization, which
ultimately produces an immersive 3D scene with semantic and structural
consistency that supports free exploration. Scene4U outperforms
state-of-the-art method, improving by 24.24% in LPIPS and 24.40% in BRISQUE,
while also achieving the fastest training speed. Additionally, to demonstrate
the robustness of Scene4U and allow users to experience immersive scenes from
various landmarks, we build WorldVista3D dataset for 3D scene reconstruction,
which contains panoramic images of globally renowned sites. The implementation
code and dataset will be released at https://github.com/LongHZ140516/Scene4U .

Comments:
- CVPR 2025, 11 pages, 7 figures
