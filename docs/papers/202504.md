---
layout: default
title: April 2025
parent: Papers
nav_order: 202504
---

<!---metadata--->


## SLAM&Render: A Benchmark for the Intersection Between Neural Rendering,  Gaussian Splatting and SLAM

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-18 | Samuel Cerezo, Gaetano Meli, Tomás Berriel Martins, Kirill Safronov, Javier Civera | cs.RO | [PDF](http://arxiv.org/pdf/2504.13713v1){: .btn .btn-green } |

**Abstract**: Models and methods originally developed for novel view synthesis and scene
rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are
increasingly being adopted as representations in Simultaneous Localization and
Mapping (SLAM). However, existing datasets fail to include the specific
challenges of both fields, such as multimodality and sequentiality in SLAM or
generalization across viewpoints and illumination conditions in neural
rendering. To bridge this gap, we introduce SLAM&Render, a novel dataset
designed to benchmark methods in the intersection between SLAM and novel view
rendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot
kinematic data, and ground-truth pose streams. By releasing robot kinematic
data, the dataset also enables the assessment of novel SLAM strategies when
applied to robot manipulators. The dataset sequences span five different setups
featuring consumer and industrial objects under four different lighting
conditions, with separate training and test trajectories per scene, as well as
object rearrangements. Our experimental results, obtained with several
baselines from the literature, validate SLAM&Render as a relevant benchmark for
this emerging research area.



---

## Green Robotic Mixed Reality with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-18 | Chenxuan Liu, He Li, Zongze Li, Shuai Wang, Wei Xu, Kejiang Ye, Derrick Wing Kwan Ng, Chengzhong Xu | cs.RO | [PDF](http://arxiv.org/pdf/2504.13697v1){: .btn .btn-green } |

**Abstract**: Realizing green communication in robotic mixed reality (RoboMR) systems
presents a challenge, due to the necessity of uploading high-resolution images
at high frequencies through wireless channels. This paper proposes Gaussian
splatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and
makes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS
model which enables the simulator to opportunistically render a photo-realistic
view from the robot's pose, thereby reducing the need for excessive image
uploads. Since the GS model may involve discrepancies compared to the actual
environments, a GS cross-layer optimization (GSCLO) framework is further
proposed, which jointly optimizes content switching (i.e., deciding whether to
upload image or not) and power allocation across different frames. The GSCLO
problem is solved by an accelerated penalty optimization (APO) algorithm.
Experiments demonstrate that the proposed GSRMR reduces the communication
energy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with
APO outperforms extensive baseline schemes, in terms of peak signal-to-noise
ratio (PSNR) and structural similarity index measure (SSIM).

Comments:
- 6 pages, 5 figures, accepted by IEEE INFOCOM 2025 Workshop on
  Networked Robotics and Communication Systems

---

## EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-18 | Beizhen Zhao, Yifan Zhou, Zijian Wang, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.13540v1){: .btn .btn-green } |

**Abstract**: In this paper, we explore an open research problem concerning the
reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian
Splatting (3DGS) to produce 3D scenes due to its efficient training process.
However, these methodologies may generate incomplete 3D scenes or blurred
multiviews. This is because of (1) inaccurate 3DGS point initialization and (2)
the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To
address these issues, we propose a novel framework EG-Gaussian, which utilizes
epipolar geometry and graph networks for 3D scene reconstruction. Initially, we
integrate epipolar geometry into the 3DGS initialization phase to enhance
initial 3DGS point construction. Then, we specifically design a graph learning
module to refine 3DGS spatial features, in which we incorporate both spatial
coordinates and angular relationships among neighboring points. Experiments on
indoor and outdoor benchmark datasets demonstrate that our approach
significantly improves reconstruction accuracy compared to 3DGS-based methods.



---

## Volume Encoding Gaussians: Transfer Function-Agnostic 3D Gaussians for  Volume Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Landon Dyken, Andres Sewell, Will Usher, Steve Petruzza, Sidharth Kumar | cs.GR | [PDF](http://arxiv.org/pdf/2504.13339v1){: .btn .btn-green } |

**Abstract**: While HPC resources are increasingly being used to produce adaptively refined
or unstructured volume datasets, current research in applying machine
learning-based representation to visualization has largely ignored this type of
data. To address this, we introduce Volume Encoding Gaussians (VEG), a novel 3D
Gaussian-based representation for scientific volume visualization focused on
unstructured volumes. Unlike prior 3D Gaussian Splatting (3DGS) methods that
store view-dependent color and opacity for each Gaussian, VEG decouple the
visual appearance from the data representation by encoding only scalar values,
enabling transfer-function-agnostic rendering of 3DGS models for interactive
scientific visualization. VEG are directly initialized from volume datasets,
eliminating the need for structure-from-motion pipelines like COLMAP. To ensure
complete scalar field coverage, we introduce an opacity-guided training
strategy, using differentiable rendering with multiple transfer functions to
optimize our data representation. This allows VEG to preserve fine features
across the full scalar range of a dataset while remaining independent of any
specific transfer function. Each Gaussian is scaled and rotated to adapt to
local geometry, allowing for efficient representation of unstructured meshes
without storing mesh connectivity and while using far fewer primitives. Across
a diverse set of data, VEG achieve high reconstruction quality, compress large
volume datasets by up to 3600x, and support lightning-fast rendering on
commodity GPUs, enabling interactive visualization of large-scale structured
and unstructured volumes.



---

## TSGS: Improving Gaussian Splatting for Transparent Surface  Reconstruction via Normal and De-lighting Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Mingwei Li, Pu Pang, Hehe Fan, Hua Huang, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2504.12799v1){: .btn .btn-green } |

**Abstract**: Reconstructing transparent surfaces is essential for tasks such as robotic
manipulation in labs, yet it poses a significant challenge for 3D
reconstruction techniques like 3D Gaussian Splatting (3DGS). These methods
often encounter a transparency-depth dilemma, where the pursuit of
photorealistic rendering through standard $\alpha$-blending undermines
geometric precision, resulting in considerable depth estimation errors for
transparent materials. To address this issue, we introduce Transparent Surface
Gaussian Splatting (TSGS), a new framework that separates geometry learning
from appearance refinement. In the geometry learning stage, TSGS focuses on
geometry by using specular-suppressed inputs to accurately represent surfaces.
In the second stage, TSGS improves visual fidelity through anisotropic specular
modeling, crucially maintaining the established opacity to ensure geometric
accuracy. To enhance depth inference, TSGS employs a first-surface depth
extraction method. This technique uses a sliding window over $\alpha$-blending
weights to pinpoint the most likely surface location and calculates a robust
weighted average depth. To evaluate the transparent surface reconstruction task
under realistic conditions, we collect a TransLab dataset that includes complex
transparent laboratory glassware. Extensive experiments on TransLab show that
TSGS achieves accurate geometric reconstruction and realistic rendering of
transparent objects simultaneously within the efficient 3DGS framework.
Specifically, TSGS significantly surpasses current leading methods, achieving a
37.3% reduction in chamfer distance and an 8.0% improvement in F1 score
compared to the top baseline. The code and dataset will be released at
https://longxiang-ai.github.io/TSGS/.

Comments:
- Project page: https://longxiang-ai.github.io/TSGS/

---

## Training-Free Hierarchical Scene Understanding for Gaussian Splatting  with Superpoint Graphs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Shaohui Dai, Yansong Qu, Zheyan Li, Xinyang Li, Shengchuan Zhang, Liujuan Cao | cs.CV | [PDF](http://arxiv.org/pdf/2504.13153v1){: .btn .btn-green } |

**Abstract**: Bridging natural language and 3D geometry is a crucial step toward flexible,
language-driven scene understanding. While recent advances in 3D Gaussian
Splatting (3DGS) have enabled fast and high-quality scene reconstruction,
research has also explored incorporating open-vocabulary understanding into
3DGS. However, most existing methods require iterative optimization over
per-view 2D semantic feature maps, which not only results in inefficiencies but
also leads to inconsistent 3D semantics across views. To address these
limitations, we introduce a training-free framework that constructs a
superpoint graph directly from Gaussian primitives. The superpoint graph
partitions the scene into spatially compact and semantically coherent regions,
forming view-consistent 3D entities and providing a structured foundation for
open-vocabulary understanding. Based on the graph structure, we design an
efficient reprojection strategy that lifts 2D semantic features onto the
superpoints, avoiding costly multi-view iterative training. The resulting
representation ensures strong 3D semantic coherence and naturally supports
hierarchical understanding, enabling both coarse- and fine-grained
open-vocabulary perception within a unified semantic field. Extensive
experiments demonstrate that our method achieves state-of-the-art
open-vocabulary segmentation performance, with semantic field reconstruction
completed over $30\times$ faster. Our code will be available at
https://github.com/Atrovast/THGS.



---

## CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene  Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Xiangrui Liu, Xinju Wu, Shiqi Wang, Zhu Li, Sam Kwong | cs.GR | [PDF](http://arxiv.org/pdf/2504.13022v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers
from substantial data volume due to inherent primitive redundancy. To enable
future photorealistic 3D immersive visual communication applications,
significant compression is essential for transmission over the existing
Internet infrastructure. Hence, we propose Compressed Gaussian Splatting
(CompGS++), a novel framework that leverages compact Gaussian primitives to
achieve accurate 3D modeling with substantial size reduction for both static
and dynamic scenes. Our design is based on the principle of eliminating
redundancy both between and within primitives. Specifically, we develop a
comprehensive prediction paradigm to address inter-primitive redundancy through
spatial and temporal primitive prediction modules. The spatial primitive
prediction module establishes predictive relationships for scene primitives and
enables most primitives to be encoded as compact residuals, substantially
reducing the spatial redundancy. We further devise a temporal primitive
prediction module to handle dynamic scenes, which exploits primitive
correlations across timestamps to effectively reduce temporal redundancy.
Moreover, we devise a rate-constrained optimization module that jointly
minimizes reconstruction error and rate consumption. This module effectively
eliminates parameter redundancy within primitives and enhances the overall
compactness of scene representations. Comprehensive evaluations across multiple
benchmark datasets demonstrate that CompGS++ significantly outperforms existing
methods, achieving superior compression performance while preserving accurate
scene modeling. Our implementation will be made publicly available on GitHub to
facilitate further research.

Comments:
- Submitted to a journal

---

## Digital Twin Generation from Visual Data: A Survey

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Andrew Melnik, Benjamin Alt, Giang Nguyen, Artur Wilkowski, Maciej Stefańczyk, Qirui Wu, Sinan Harms, Helge Rhodin, Manolis Savva, Michael Beetz | cs.CV | [PDF](http://arxiv.org/pdf/2504.13159v1){: .btn .btn-green } |

**Abstract**: This survey explores recent developments in generating digital twins from
videos. Such digital twins can be used for robotics application, media content
creation, or design and construction works. We analyze various approaches,
including 3D Gaussian Splatting, generative in-painting, semantic segmentation,
and foundation models highlighting their advantages and limitations.
Additionally, we discuss challenges such as occlusions, lighting variations,
and scalability, as well as potential future research directions. This survey
aims to provide a comprehensive overview of state-of-the-art methodologies and
their implications for real-world applications. Awesome list:
https://github.com/ndrwmlnk/awesome-digital-twins



---

## GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation  with Unity Integration

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Rendong Zhang, Alexandra Watkins, Nilanjan Sarkar | cs.GR | [PDF](http://arxiv.org/pdf/2504.12999v1){: .btn .btn-green } |

**Abstract**: Photorealistic avatars have become essential for immersive applications in
virtual reality (VR) and augmented reality (AR), enabling lifelike interactions
in areas such as training simulations, telemedicine, and virtual collaboration.
These avatars bridge the gap between the physical and digital worlds, improving
the user experience through realistic human representation. However, existing
avatar creation techniques face significant challenges, including high costs,
long creation times, and limited utility in virtual applications. Manual
methods, such as MetaHuman, require extensive time and expertise, while
automatic approaches, such as NeRF-based pipelines often lack efficiency,
detailed facial expression fidelity, and are unable to be rendered at a speed
sufficent for real-time applications. By involving several cutting-edge modern
techniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar
creation pipeline that leverages monocular video input to create a scalable and
efficient photorealistic avatar directly compatible with the Unity game engine.
Our pipeline incorporates a novel Gaussian splatting technique with customized
preprocessing that enables the user of "in the wild" monocular video capture,
detailed facial expression reconstruction and embedding within a fully rigged
avatar model. Additionally, we present a Unity-integrated Gaussian Splatting
Avatar Editor, offering a user-friendly environment for VR/AR application
development. Experimental results validate the effectiveness of our
preprocessing pipeline in standardizing custom data for 3DGS training and
demonstrate the versatility of Gaussian avatars in Unity, highlighting the
scalability and practicality of our approach.



---

## AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Michael Steiner, Thomas Köhler, Lukas Radl, Felix Windisch, Dieter Schmalstieg, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2504.12811v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,
it still faces challenges such as aliasing, projection artifacts, and view
inconsistencies, primarily due to the simplification of treating splats as 2D
entities. We argue that incorporating full 3D evaluation of Gaussians
throughout the 3DGS pipeline can effectively address these issues while
preserving rasterization efficiency. Specifically, we introduce an adaptive 3D
smoothing filter to mitigate aliasing and present a stable view-space bounding
method that eliminates popping artifacts when Gaussians extend beyond the view
frustum. Furthermore, we promote tile-based culling to 3D with screen-space
planes, accelerating rendering and reducing sorting costs for hierarchical
rasterization. Our method achieves state-of-the-art quality on in-distribution
evaluation sets and significantly outperforms other approaches for
out-of-distribution views. Our qualitative evaluations further demonstrate the
effective removal of aliasing, distortions, and popping artifacts, ensuring
real-time, artifact-free rendering.



---

## CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Yifei Tong, Runze Tian, Xiao Han, Dingyao Liu, Fenggen Yu, Yan Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2504.12800v1){: .btn .btn-green } |

**Abstract**: As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of
real scenes, enabling user-friendly deformation to create novel scenes while
preserving fine details from the original 3DGS has attracted significant
research attention. We introduce CAGE-GS, a cage-based 3DGS deformation method
that seamlessly aligns a source 3DGS scene with a user-defined target shape.
Our approach learns a deformation cage from the target, which guides the
geometric transformation of the source scene. While the cages effectively
control structural alignment, preserving the textural appearance of 3DGS
remains challenging due to the complexity of covariance parameters. To address
this, we employ a Jacobian matrix-based strategy to update the covariance
parameters of each Gaussian, ensuring texture fidelity post-deformation. Our
method is highly flexible, accommodating various target shape representations,
including texts, images, point clouds, meshes and 3DGS models. Extensive
experiments and ablation studies on both public datasets and newly proposed
scenes demonstrate that our method significantly outperforms existing
techniques in both efficiency and deformation quality.



---

## ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from  Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Zetong Zhang, Manuel Kaufmann, Lixin Xue, Jie Song, Martin R. Oswald | cs.CV | [PDF](http://arxiv.org/pdf/2504.13167v2){: .btn .btn-green } |

**Abstract**: Creating a photorealistic scene and human reconstruction from a single
monocular in-the-wild video figures prominently in the perception of a
human-centric 3D world. Recent neural rendering advances have enabled holistic
human-scene reconstruction but require pre-calibrated camera and human poses,
and days of training time. In this work, we introduce a novel unified framework
that simultaneously performs camera tracking, human pose estimation and
human-scene reconstruction in an online fashion. 3D Gaussian Splatting is
utilized to learn Gaussian primitives for humans and scenes efficiently, and
reconstruction-based camera tracking and human pose estimation modules are
designed to enable holistic understanding and effective disentanglement of pose
and appearance. Specifically, we design a human deformation module to
reconstruct the details and enhance generalizability to out-of-distribution
poses faithfully. Aiming to learn the spatial correlation between human and
scene accurately, we introduce occlusion-aware human silhouette rendering and
monocular geometric priors, which further improve reconstruction quality.
Experiments on the EMDB and NeuMan datasets demonstrate superior or on-par
performance with existing methods in camera tracking, human pose estimation,
novel view synthesis and runtime. Our project page is at
https://eth-ait.github.io/ODHSR.

Comments:
- Accepted at CVPR 2025

---

## ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing  with Diffusion Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Xiao Han, Runze Tian, Yifei Tong, Fenggen Yu, Dingyao Liu, Yan Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2504.12788v1){: .btn .btn-green } |

**Abstract**: Drag-driven editing has become popular among designers for its ability to
modify complex geometric structures through simple and intuitive manipulation,
allowing users to adjust and reshape content with minimal technical skill. This
drag operation has been incorporated into numerous methods to facilitate the
editing of 2D images and 3D meshes in design. However, few studies have
explored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS)
representation, as deforming 3DGS while preserving shape coherence and visual
continuity remains challenging. In this paper, we introduce ARAP-GS, a
drag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP)
deformation. Unlike previous 3DGS editing methods, we are the first to apply
ARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven
geometric transformations. To preserve scene appearance after deformation, we
incorporate an advanced diffusion prior for image super-resolution within our
iterative optimization process. This approach enhances visual quality while
maintaining multi-view consistency in the edited results. Experiments show that
ARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its
effectiveness and superiority for drag-driven 3DGS editing. Additionally, our
method is highly efficient, requiring only 10 to 20 minutes to edit a scene on
a single RTX 3090 GPU.



---

## Novel Demonstration Generation with Gaussian Splatting Enables Robust  One-Shot Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Sizhe Yang, Wenye Yu, Jia Zeng, Jun Lv, Kerui Ren, Cewu Lu, Dahua Lin, Jiangmiao Pang | cs.RO | [PDF](http://arxiv.org/pdf/2504.13175v1){: .btn .btn-green } |

**Abstract**: Visuomotor policies learned from teleoperated demonstrations face challenges
such as lengthy data collection, high costs, and limited data diversity.
Existing approaches address these issues by augmenting image observations in
RGB space or employing Real-to-Sim-to-Real pipelines based on physical
simulators. However, the former is constrained to 2D data augmentation, while
the latter suffers from imprecise physical simulation caused by inaccurate
geometric reconstruction. This paper introduces RoboSplat, a novel method that
generates diverse, visually realistic demonstrations by directly manipulating
3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian
Splatting (3DGS), directly edit the reconstructed scene, and augment data
across six types of generalization with five techniques: 3D Gaussian
replacement for varying object types, scene appearance, and robot embodiments;
equivariant transformations for different object poses; visual attribute
editing for various lighting conditions; novel view synthesis for new camera
perspectives; and 3D content generation for diverse object types. Comprehensive
real-world experiments demonstrate that RoboSplat significantly enhances the
generalization of visuomotor policies under diverse disturbances. Notably,
while policies trained on hundreds of real-world demonstrations with additional
2D data augmentation achieve an average success rate of 57.2%, RoboSplat
attains 87.8% in one-shot settings across six types of generalization in the
real world.

Comments:
- Published at Robotics: Science and Systems (RSS) 2025

---

## Second-order Optimization of Gaussian Splats with Importance Sampling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Hamza Pehlivan, Andrea Boscolo Camiletto, Lin Geng Foo, Marc Habermann, Christian Theobalt | cs.CV | [PDF](http://arxiv.org/pdf/2504.12905v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to
its high rendering quality and fast inference time. However, 3DGS predominantly
relies on first-order optimizers such as Adam, which leads to long training
times. To address this limitation, we propose a novel second-order optimization
strategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which
we specifically tailor towards Gaussian Splatting. Our key insight is that the
Jacobian in 3DGS exhibits significant sparsity since each Gaussian affects only
a limited number of pixels. We exploit this sparsity by proposing a matrix-free
and GPU-parallelized LM optimization. To further improve its efficiency, we
propose sampling strategies for both the camera views and loss function and,
consequently, the normal equation, significantly reducing the computational
complexity. In addition, we increase the convergence rate of the second-order
approximation by introducing an effective heuristic to determine the learning
rate that avoids the expensive computation cost of line search methods. As a
result, our method achieves a $3\times$ speedup over standard LM and
outperforms Adam by $~6\times$ when the Gaussian count is low while remaining
competitive for moderate counts. Project Page:
https://vcai.mpi-inf.mpg.de/projects/LM-IS



---

## BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-16 | Wenhua Wu, Tong Zhao, Chensheng Peng, Lei Yang, Yintao Wei, Zhe Liu, Hesheng Wang | cs.GR | [PDF](http://arxiv.org/pdf/2504.13207v1){: .btn .btn-green } |

**Abstract**: Road surface is the sole contact medium for wheels or robot feet.
Reconstructing road surface is crucial for unmanned vehicles and mobile robots.
Recent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS)
have achieved remarkable results in scene reconstruction. However, they
typically rely on multi-view image inputs and require prolonged optimization
times. In this paper, we propose BEV-GS, a real-time single-frame road surface
reconstruction method based on feed-forward Gaussian splatting. BEV-GS consists
of a prediction module and a rendering module. The prediction module introduces
separate geometry and texture networks following Bird's-Eye-View paradigm.
Geometric and texture parameters are directly estimated from a single frame,
avoiding per-scene optimization. In the rendering module, we utilize grid
Gaussian for road surface representation and novel view synthesis, which better
aligns with road surface characteristics. Our method achieves state-of-the-art
performance on the real-world dataset RSRD. The road elevation error reduces to
1.73 cm, and the PSNR of novel view synthesis reaches 28.36 dB. The prediction
and rendering FPS is 26, and 2061, respectively, enabling high-accuracy and
real-time applications. The code will be available at:
\href{https://github.com/cat-wwh/BEV-GS}{\texttt{https://github.com/cat-wwh/BEV-GS}}



---

## CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-16 | Wei Sun, Yanzhao Zhou, Jianbin Jiao, Yuan Li | cs.CV | [PDF](http://arxiv.org/pdf/2504.11893v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D scene understanding is crucial for applications requiring
natural language-driven spatial interpretation, such as robotics and augmented
reality. While 3D Gaussian Splatting (3DGS) offers a powerful representation
for scene reconstruction, integrating it with open-vocabulary frameworks
reveals a key challenge: cross-view granularity inconsistency. This issue,
stemming from 2D segmentation methods like SAM, results in inconsistent object
segmentations across views (e.g., a "coffee set" segmented as a single entity
in one view but as "cup + coffee + spoon" in another). Existing 3DGS-based
methods often rely on isolated per-Gaussian feature learning, neglecting the
spatial context needed for cohesive object reasoning, leading to fragmented
representations. We propose Context-Aware Gaussian Splatting (CAGS), a novel
framework that addresses this challenge by incorporating spatial context into
3DGS. CAGS constructs local graphs to propagate contextual features across
Gaussians, reducing noise from inconsistent granularity, employs mask-centric
contrastive learning to smooth SAM-derived features across views, and leverages
a precomputation strategy to reduce computational cost by precomputing
neighborhood relationships, enabling efficient training in large-scale scenes.
By integrating spatial context, CAGS significantly improves 3D instance
segmentation and reduces fragmentation errors on datasets like LERF-OVS and
ScanNet, enabling robust language-guided 3D scene understanding.



---

## R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh  Reconstruction with Diffusion Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-16 | Haoyang Wang, Liming Liu, Peiheng Wang, Junlin Hao, Jiangkai Wu, Xinggong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.11946v1){: .btn .btn-green } |

**Abstract**: Mesh reconstruction from multi-view images is a fundamental problem in
computer vision, but its performance degrades significantly under sparse-view
conditions, especially in unseen regions where no ground-truth observations are
available. While recent advances in diffusion models have demonstrated strong
capabilities in synthesizing novel views from limited inputs, their outputs
often suffer from visual artifacts and lack 3D consistency, posing challenges
for reliable mesh optimization. In this paper, we propose a novel framework
that leverages diffusion models to enhance sparse-view mesh reconstruction in a
principled and reliable manner. To address the instability of diffusion
outputs, we propose a Consensus Diffusion Module that filters unreliable
generations via interquartile range (IQR) analysis and performs variance-aware
image fusion to produce robust pseudo-supervision. Building on this, we design
an online reinforcement learning strategy based on the Upper Confidence Bound
(UCB) to adaptively select the most informative viewpoints for enhancement,
guided by diffusion loss. Finally, the fused images are used to jointly
supervise a NeRF-based model alongside sparse-view ground truth, ensuring
consistency across both geometry and appearance. Extensive experiments
demonstrate that our method achieves significant improvements in both geometric
quality and rendering quality.



---

## GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-François Lalonde | cs.CV | [PDF](http://arxiv.org/pdf/2504.10809v2){: .btn .btn-green } |

**Abstract**: We present GaSLight, a method that generates spatially-varying lighting from
regular images. Our method proposes using HDR Gaussian Splats as light source
representation, marking the first time regular images can serve as light
sources in a 3D renderer. Our two-stage process first enhances the dynamic
range of images plausibly and accurately by leveraging the priors embedded in
diffusion models. Next, we employ Gaussian Splats to model 3D lighting,
achieving spatially variant lighting. Our approach yields state-of-the-art
results on HDR estimations and their applications in illuminating virtual
objects and scenes. To facilitate the benchmarking of images as light sources,
we introduce a novel dataset of calibrated and unsaturated HDR to evaluate
images as light sources. We assess our method using a combination of this novel
dataset and an existing dataset from the literature. Project page:
https://lvsn.github.io/gaslight/



---

## Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Andrea Simonelli, Norman Müller, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2504.11024v1){: .btn .btn-green } |

**Abstract**: The increasing availability of digital 3D environments, whether through
image-based 3D reconstruction, generation, or scans obtained by robots, is
driving innovation across various applications. These come with a significant
demand for 3D interaction, such as 3D Interactive Segmentation, which is useful
for tasks like object selection and manipulation. Additionally, there is a
persistent need for solutions that are efficient, precise, and performing well
across diverse settings, particularly in unseen environments and with
unfamiliar objects. In this work, we introduce a 3D interactive segmentation
method that consistently surpasses previous state-of-the-art techniques on both
in-domain and out-of-domain datasets. Our simple approach integrates a
voxel-based sparse encoder with a lightweight transformer-based decoder that
implements implicit click fusion, achieving superior performance and maximizing
efficiency. Our method demonstrates substantial improvements on benchmark
datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on
unseen geometric distributions such as the ones obtained by Gaussian Splatting.
The project web-page is available at https://simonelli-andrea.github.io/easy3d.



---

## EDGS: Eliminating Densification for Efficient Convergence of 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Dmytro Kotovenko, Olga Grebenkova, Björn Ommer | cs.GR | [PDF](http://arxiv.org/pdf/2504.13204v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting reconstructs scenes by starting from a sparse
Structure-from-Motion initialization and iteratively refining
under-reconstructed regions. This process is inherently slow, as it requires
multiple densification steps where Gaussians are repeatedly split and adjusted,
following a lengthy optimization path. Moreover, this incremental approach
often leads to suboptimal renderings, particularly in high-frequency regions
where detail is critical.
  We propose a fundamentally different approach: we eliminate densification
process with a one-step approximation of scene geometry using triangulated
pixels from dense image correspondences. This dense initialization allows us to
estimate rough geometry of the scene while preserving rich details from input
RGB images, providing each Gaussian with well-informed colors, scales, and
positions. As a result, we dramatically shorten the optimization path and
remove the need for densification. Unlike traditional methods that rely on
sparse keypoints, our dense initialization ensures uniform detail across the
scene, even in high-frequency regions where 3DGS and other methods struggle.
Moreover, since all splats are initialized in parallel at the start of
optimization, we eliminate the need to wait for densification to adjust new
Gaussians.
  Our method not only outperforms speed-optimized models in training efficiency
but also achieves higher rendering quality than state-of-the-art approaches,
all while using only half the splats of standard 3DGS. It is fully compatible
with other 3DGS acceleration techniques, making it a versatile and efficient
solution that can be integrated with existing approaches.



---

## 3D Gabor Splatting: Reconstruction of High-frequency Surface Texture  using Gabor Noise

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Haato Watanabe, Kenji Tojo, Nobuyuki Umetani | cs.GR | [PDF](http://arxiv.org/pdf/2504.11003v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has experienced explosive popularity in the past few
years in the field of novel view synthesis. The lightweight and differentiable
representation of the radiance field using the Gaussian enables rapid and
high-quality reconstruction and fast rendering. However, reconstructing objects
with high-frequency surface textures (e.g., fine stripes) requires many skinny
Gaussian kernels because each Gaussian represents only one color if viewed from
one direction. Thus, reconstructing the stripes pattern, for example, requires
Gaussians for at least the number of stripes. We present 3D Gabor splatting,
which augments the Gaussian kernel to represent spatially high-frequency
signals using Gabor noise. The Gabor kernel is a combination of a Gaussian term
and spatially fluctuating wave functions, making it suitable for representing
spatial high-frequency texture. We demonstrate that our 3D Gabor splatting can
reconstruct various high-frequency textures on the objects.

Comments:
- 4 pages, 5 figures, Eurographics 2025 Short Paper

---

## 3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin | cs.CV | [PDF](http://arxiv.org/pdf/2504.11218v2){: .btn .btn-green } |

**Abstract**: 3D affordance reasoning is essential in associating human instructions with
the functional regions of 3D objects, facilitating precise, task-oriented
manipulations in embodied AI. However, current methods, which predominantly
depend on sparse 3D point clouds, exhibit limited generalizability and
robustness due to their sensitivity to coordinate variations and the inherent
sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers
high-fidelity, real-time rendering with minimal computational overhead by
representing scenes as dense, continuous distributions. This positions 3DGS as
a highly effective approach for capturing fine-grained affordance details and
improving recognition accuracy. Nevertheless, its full potential remains
largely untapped due to the absence of large-scale, 3DGS-specific affordance
datasets. To overcome these limitations, we present 3DAffordSplat, the first
large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.
This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,
and 6,631 manually annotated affordance labels, encompassing 21 object
categories and 18 affordance types. Building upon this dataset, we introduce
AffordSplatNet, a novel model specifically designed for affordance reasoning
using 3DGS representations. AffordSplatNet features an innovative cross-modal
structure alignment module that exploits structural consistency priors to align
3D point cloud and 3DGS representations, resulting in enhanced affordance
recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat
dataset significantly advances affordance learning within the 3DGS domain,
while AffordSplatNet consistently outperforms existing methods across both seen
and unseen settings, highlighting its robust generalization capabilities.

Comments:
- The first large-scale 3D Gaussians Affordance Reasoning Benchmark

---

## EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Yufei Deng, Yuanjian Wang, Rong Xiao, Chenwei Tang, Jizhe Zhou, Jiahao Fan, Deng Xiong, Jiancheng Lv, Huajin Tang | cs.CV | [PDF](http://arxiv.org/pdf/2504.10012v1){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view
synthesis, its performance degrades with motion blur. In scenarios with rapid
motion or low-light conditions, existing RGB-based deblurring methods struggle
to model camera pose and radiance changes during exposure, reducing
reconstruction accuracy. Event cameras, capturing continuous brightness changes
during exposure, can effectively assist in modeling motion blur and improving
reconstruction quality. Therefore, we propose Event-driven Bundle Adjusted
Deblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D
Gaussians from event streams and severely blurred images. This method jointly
learns the parameters of these Gaussians while recovering camera motion
trajectories during exposure time. Specifically, we first construct a blur loss
function by synthesizing multiple latent sharp images during the exposure time,
minimizing the difference between real and synthesized blurred images. Then we
use event stream to supervise the light intensity changes between latent sharp
images at any time within the exposure period, supplementing the light
intensity dynamic changes lost in RGB images. Furthermore, we optimize the
latent sharp images at intermediate exposure times based on the event-based
double integral (EDI) prior, applying consistency constraints to enhance the
details and texture information of the reconstructed images. Extensive
experiments on synthetic and real-world datasets show that EBAD-Gaussian can
achieve high-quality 3D scene reconstruction under the condition of blurred
images and event stream inputs.



---

## ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content  Generation with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Huiqi Wu, Jianbo Mei, Yingjie Huang, Yining Xu, Jingjiao You, Yilong Liu, Li Yao | cs.CV | [PDF](http://arxiv.org/pdf/2504.10316v1){: .btn .btn-green } |

**Abstract**: In recent years, significant advancements have been made in text-driven 3D
content generation. However, several challenges remain. In practical
applications, users often provide extremely simple text inputs while expecting
high-quality 3D content. Generating optimal results from such minimal text is a
difficult task due to the strong dependency of text-to-3D models on the quality
of input prompts. Moreover, the generation process exhibits high variability,
making it difficult to control. Consequently, multiple iterations are typically
required to produce content that meets user expectations, reducing generation
efficiency. To address this issue, we propose GPT-4V for self-optimization,
which significantly enhances the efficiency of generating satisfactory content
in a single attempt. Furthermore, the controllability of text-to-3D generation
methods has not been fully explored. Our approach enables users to not only
provide textual descriptions but also specify additional conditions, such as
style, edges, scribbles, poses, or combinations of multiple conditions,
allowing for more precise control over the generated 3D content. Additionally,
during training, we effectively integrate multi-view information, including
multi-view depth, masks, features, and images, to address the common Janus
problem in 3D content generation. Extensive experiments demonstrate that our
method achieves robust generalization, facilitating the efficient and
controllable generation of high-quality 3D content.



---

## DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar  Relighting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Zeren Jiang, Shaofei Wang, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2504.10486v1){: .btn .btn-green } |

**Abstract**: Creating relightable and animatable human avatars from monocular videos is a
rising research topic with a range of applications, e.g. virtual reality,
sports, and video games. Previous works utilize neural fields together with
physically based rendering (PBR), to estimate geometry and disentangle
appearance properties of human avatars. However, one drawback of these methods
is the slow rendering speed due to the expensive Monte Carlo ray tracing. To
tackle this problem, we proposed to distill the knowledge from implicit neural
fields (teacher) to explicit 2D Gaussian splatting (student) representation to
take advantage of the fast rasterization property of Gaussian splatting. To
avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We
also propose novel part-wise ambient occlusion probes for shadow computation.
Shadow prediction is achieved by querying these probes only once per pixel,
which paves the way for real-time relighting of avatars. These techniques
combined give high-quality relighting results with realistic shadow effects.
Our experiments demonstrate that the proposed student model achieves comparable
or even better relighting results with our teacher model while being 370 times
faster at inference time, achieving a 67 FPS rendering speed.

Comments:
- 16 pages, 8 figures, Project pages:
  https://jzr99.github.io/DNF-Avatar/

---

## MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based  Dynamic-Resolution Ray Sampling

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Yunpeng Tan, Junlin Hao, Jiangkai Wu, Liming Liu, Qingyang Li, Xinggong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.09878v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) is widely known for high-fidelity novel view
synthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting,
requires minutes for training, far from the real-time performance required by
multimedia scenarios like telemedicine. One of the obstacles is its inefficient
sampling, which is only partially addressed by existing works. Existing
point-sampling algorithms uniformly sample simple-texture regions (easy to fit)
and complex-texture regions (hard to fit), while existing ray-sampling
algorithms sample these regions all in the finest granularity (i.e. the pixel
level), both wasting GPU training resources. Actually, regions with different
texture intensities require different sampling granularities. To this end, we
propose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which
employs Monte Carlo Tree Search (MCTS) to partition each training image into
pixel blocks with different sizes for active block-wise training. Specifically,
the trees are initialized according to the texture of training images to boost
the initialization speed, and an expansion/pruning module dynamically optimizes
the block partition. MCBlock is implemented in Nerfstudio, an open-source
toolset, and achieves a training acceleration of up to 2.33x, surpassing other
ray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF
model and contribute to the multimedia community.



---

## NeRF-Based Transparent Object Grasping Enhanced by Shape Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Yi Han, Zixin Lin, Dongjie Li, Lvping Chen, Yongliang Shi, Gan Ma | cs.RO | [PDF](http://arxiv.org/pdf/2504.09868v1){: .btn .btn-green } |

**Abstract**: Transparent object grasping remains a persistent challenge in robotics,
largely due to the difficulty of acquiring precise 3D information. Conventional
optical 3D sensors struggle to capture transparent objects, and machine
learning methods are often hindered by their reliance on high-quality datasets.
Leveraging NeRF's capability for continuous spatial opacity modeling, our
proposed architecture integrates a NeRF-based approach for reconstructing the
3D information of transparent objects. Despite this, certain portions of the
reconstructed 3D information may remain incomplete. To address these
deficiencies, we introduce a shape-prior-driven completion mechanism, further
refined by a geometric pose estimation method we have developed. This allows us
to obtain a complete and reliable 3D information of transparent objects.
Utilizing this refined data, we perform scene-level grasp prediction and deploy
the results in real-world robotic systems. Experimental validation demonstrates
the efficacy of our architecture, showcasing its capability to reliably capture
3D information of various transparent objects in cluttered scenes, and
correspondingly, achieve high-quality, stables, and executable grasp
predictions.



---

## LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian  Splatting for Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Hao Sun, Fenggen Yu, Huiyao Xu, Tao Zhang, Changqing Zou | cs.CV | [PDF](http://arxiv.org/pdf/2504.10331v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) in low-light scenes remains a significant
challenge due to degraded inputs characterized by severe noise, low dynamic
range (LDR) and unreliable initialization. While recent NeRF-based approaches
have shown promising results, most suffer from high computational costs, and
some rely on carefully captured or pre-processed data--such as RAW sensor
inputs or multi-exposure sequences--which severely limits their practicality.
In contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with
competitive visual fidelity; however, existing 3DGS-based methods struggle with
low-light sRGB inputs, resulting in unstable Gaussian initialization and
ineffective noise suppression. To address these challenges, we propose
LL-Gaussian, a novel framework for 3D reconstruction and enhancement from
low-light sRGB images, enabling pseudo normal-light novel view synthesis. Our
method introduces three key innovations: 1) an end-to-end Low-Light Gaussian
Initialization Module (LLGIM) that leverages dense priors from learning-based
MVS approach to generate high-quality initial point clouds; 2) a dual-branch
Gaussian decomposition model that disentangles intrinsic scene properties
(reflectance and illumination) from transient interference, enabling stable and
interpretable optimization; 3) an unsupervised optimization strategy guided by
both physical constrains and diffusion prior to jointly steer decomposition and
enhancement. Additionally, we contribute a challenging dataset collected in
extreme low-light environments and demonstrate the effectiveness of
LL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian
achieves up to 2,000 times faster inference and reduces training time to just
2%, while delivering superior reconstruction and rendering quality.



---

## GaussVideoDreamer: 3D Scene Generation with Video Diffusion and  Inconsistency-Aware Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Junlin Hao, Peiheng Wang, Haoyang Wang, Xinggong Zhang, Zongming Guo | cs.CV | [PDF](http://arxiv.org/pdf/2504.10001v3){: .btn .btn-green } |

**Abstract**: Single-image 3D scene reconstruction presents significant challenges due to
its inherently ill-posed nature and limited input constraints. Recent advances
have explored two promising directions: multiview generative models that train
on 3D consistent datasets but struggle with out-of-distribution generalization,
and 3D scene inpainting and completion frameworks that suffer from cross-view
inconsistency and suboptimal error handling, as they depend exclusively on
depth data or 3D smoothness, which ultimately degrades output quality and
computational performance. Building upon these approaches, we present
GaussVideoDreamer, which advances generative multimedia approaches by bridging
the gap between image, video, and 3D generation, integrating their strengths
through two key innovations: (1) A progressive video inpainting strategy that
harnesses temporal coherence for improved multiview consistency and faster
convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video
diffusion with 3D consistent multiview evidence. Our pipeline combines three
core components: a geometry-aware initialization protocol, Inconsistency-Aware
Gaussian Splatting, and a progressive video inpainting strategy. Experimental
results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and
at least 2x speedup compared to existing methods while maintaining robust
performance across diverse scenes.



---

## DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-13 | Yexing Xu, Longguang Wang, Minglin Chen, Sheng Ao, Li Li, Yulan Guo | cs.CV | [PDF](http://arxiv.org/pdf/2504.09491v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting (3DGS) has demonstrated promising results in
novel view synthesis, its performance degrades dramatically with sparse inputs
and generates undesirable artifacts. As the number of training views decreases,
the novel view synthesis task degrades to a highly under-determined problem
such that existing methods suffer from the notorious overfitting issue.
Interestingly, we observe that models with fewer Gaussian primitives exhibit
less overfitting under sparse inputs. Inspired by this observation, we propose
a Random Dropout Regularization (RDR) to exploit the advantages of
low-complexity models to alleviate overfitting. In addition, to remedy the lack
of high-frequency details for these models, an Edge-guided Splitting Strategy
(ESS) is developed. With these two techniques, our method (termed DropoutGS)
provides a simple yet effective plug-in approach to improve the generalization
performance of existing 3DGS methods. Extensive experiments show that our
DropoutGS produces state-of-the-art performance under sparse views on benchmark
datasets including Blender, LLFF, and DTU. The project page is at:
https://xuyx55.github.io/DropoutGS/.

Comments:
- Accepted by CVPR 2025

---

## TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-13 | Zhicong Wu, Hongbin Xu, Gang Xu, Ping Nie, Zhixin Yan, Jinkai Zheng, Liangqiong Qu, Ming Li, Liqiang Nie | cs.CV | [PDF](http://arxiv.org/pdf/2504.09588v1){: .btn .btn-green } |

**Abstract**: Recent advancements in Generalizable Gaussian Splatting have enabled robust
3D reconstruction from sparse input views by utilizing feed-forward Gaussian
Splatting models, achieving superior cross-scene generalization. However, while
many methods focus on geometric consistency, they often neglect the potential
of text-driven guidance to enhance semantic understanding, which is crucial for
accurately reconstructing fine-grained details in complex scenes. To address
this limitation, we propose TextSplat--the first text-driven Generalizable
Gaussian Splatting framework. By employing a text-guided fusion of diverse
semantic cues, our framework learns robust cross-modal feature representations
that improve the alignment of geometric and semantic information, producing
high-fidelity 3D reconstructions. Specifically, our framework employs three
parallel modules to obtain complementary representations: the Diffusion Prior
Depth Estimator for accurate depth information, the Semantic Aware Segmentation
Network for detailed semantic information, and the Multi-View Interaction
Network for refined cross-view features. Then, in the Text-Guided Semantic
Fusion Module, these representations are integrated via the text-guided and
attention-based feature aggregation mechanism, resulting in enhanced 3D
Gaussian parameters enriched with detailed semantic cues. Experimental results
on various benchmark datasets demonstrate improved performance compared to
existing methods across multiple evaluation metrics, validating the
effectiveness of our framework. The code will be publicly available.



---

## You Need a Transition Plane: Bridging Continuous Panoramic 3D  Reconstruction with Perspective Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-12 | Zhijie Shen, Chunyu Lin, Shujuan Huang, Lang Nie, Kang Liao, Yao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2504.09062v1){: .btn .btn-green } |

**Abstract**: Recently, reconstructing scenes from a single panoramic image using advanced
3D Gaussian Splatting (3DGS) techniques has attracted growing interest.
Panoramic images offer a 360$\times$ 180 field of view (FoV), capturing the
entire scene in a single shot. However, panoramic images introduce severe
distortion, making it challenging to render 3D Gaussians into 2D distorted
equirectangular space directly. Converting equirectangular images to cubemap
projections partially alleviates this problem but introduces new challenges,
such as projection distortion and discontinuities across cube-face boundaries.
To address these limitations, we present a novel framework, named TPGS, to
bridge continuous panoramic 3D scene reconstruction with perspective Gaussian
splatting. Firstly, we introduce a Transition Plane between adjacent cube faces
to enable smoother transitions in splatting directions and mitigate
optimization ambiguity in the boundary region. Moreover, an intra-to-inter face
optimization strategy is proposed to enhance local details and restore visual
consistency across cube-face boundaries. Specifically, we optimize 3D Gaussians
within individual cube faces and then fine-tune them in the stitched panoramic
space. Additionally, we introduce a spherical sampling technique to eliminate
visible stitching seams. Extensive experiments on indoor and outdoor,
egocentric, and roaming benchmark datasets demonstrate that our approach
outperforms existing state-of-the-art methods. Code and models will be
available at https://github.com/zhijieshen-bjtu/TPGS.



---

## BIGS: Bimanual Category-agnostic Interaction Reconstruction from  Monocular Videos via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-12 | Jeongwan On, Kyeonghwan Gwak, Gunyoung Kang, Junuk Cha, Soohyun Hwang, Hyein Hwang, Seungryul Baek | cs.CV | [PDF](http://arxiv.org/pdf/2504.09097v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem
that can find numerous applications. Despite recent advances, there is no
comprehensive pipeline yet for bimanual class-agnostic interaction
reconstruction from a monocular RGB video, where two hands and an unknown
object are interacting with each other. Previous works tackled the limited
hand-object interaction case, where object templates are pre-known or only one
hand is involved in the interaction. The bimanual interaction reconstruction
exhibits severe occlusions introduced by complex interactions between two hands
and an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D
Gaussian Splatting), a method that reconstructs 3D Gaussians of hands and an
unknown object from a monocular video. To robustly obtain object Gaussians
avoiding severe occlusions, we leverage prior knowledge of pre-trained
diffusion model with score distillation sampling (SDS) loss, to reconstruct
unseen object parts. For hand Gaussians, we exploit the 3D priors of hand model
(i.e., MANO) and share a single Gaussian for two hands to effectively
accumulate hand 3D information, given limited views. To further consider the 3D
alignment between hands and objects, we include the interacting-subjects
optimization step during Gaussian optimization. Our method achieves the
state-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose
estimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering
quality (PSNR, SSIM, LPIPS), respectively.

Comments:
- Accepted to CVPR 2025

---

## A Constrained Optimization Approach for Gaussian Splatting from  Coarsely-posed Images and Noisy Lidar Point Clouds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-12 | Jizong Peng, Tze Ho Elden Tse, Kai Xu, Wenchao Gao, Angela Yao | cs.CV | [PDF](http://arxiv.org/pdf/2504.09129v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it
needs to be initialized from accurate camera poses and high-fidelity point
clouds. Typically, the initialization is taken from Structure-from-Motion (SfM)
algorithms; however, SfM is time-consuming and restricts the application of
3DGS in real-world scenarios and large-scale scene reconstruction. We introduce
a constrained optimization method for simultaneous camera pose estimation and
3D reconstruction that does not require SfM support. Core to our approach is
decomposing a camera pose into a sequence of camera-to-(device-)center and
(device-)center-to-world optimizations. To facilitate, we propose two
optimization constraints conditioned to the sensitivity of each parameter group
and restricts each parameter's search space. In addition, as we learn the scene
geometry directly from the noisy point clouds, we propose geometric constraints
to improve the reconstruction quality. Experiments demonstrate that the
proposed method significantly outperforms the existing (multi-modal) 3DGS
baseline and methods supplemented by COLMAP on both our collected dataset and
two public benchmarks.



---

## BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via  Adaptive Block-Based Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-12 | Yongchang Wu, Zipeng Qi, Zhenwei Shi, Zhengxia Zou | cs.CV | [PDF](http://arxiv.org/pdf/2504.09048v2){: .btn .btn-green } |

**Abstract**: The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated
remarkable potential in novel view synthesis tasks. The divide-and-conquer
paradigm has enabled large-scale scene reconstruction, but significant
challenges remain in scene partitioning, optimization, and merging processes.
This paper introduces BlockGaussian, a novel framework incorporating a
content-aware scene partition strategy and visibility-aware block optimization
to achieve efficient and high-quality large-scale scene reconstruction.
Specifically, our approach considers the content-complexity variation across
different regions and balances computational load during scene partitioning,
enabling efficient scene reconstruction. To tackle the supervision mismatch
issue during independent block optimization, we introduce auxiliary points
during individual block optimization to align the ground-truth supervision,
which enhances the reconstruction quality. Furthermore, we propose a
pseudo-view geometry constraint that effectively mitigates rendering
degradation caused by airspace floaters during block merging. Extensive
experiments on large-scale scenes demonstrate that our approach achieves
state-of-the-art performance in both reconstruction efficiency and rendering
quality, with a 5x speedup in optimization and an average PSNR improvement of
1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces
computational requirements, enabling large-scale scene reconstruction on a
single 24GB VRAM device. The project page is available at
https://github.com/SunshineWYC/BlockGaussian

Comments:
- https://github.com/SunshineWYC/BlockGaussian

---

## HAL-NeRF: High Accuracy Localization Leveraging Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-11 | Asterios Reppas, Grigorios-Aris Cheimariotis, Panos K. Papadopoulos, Panagiotis Frasiolas, Dimitrios Zarpalas | cs.CV | [PDF](http://arxiv.org/pdf/2504.08901v1){: .btn .btn-green } |

**Abstract**: Precise camera localization is a critical task in XR applications and
robotics. Using only the camera captures as input to a system is an inexpensive
option that enables localization in large indoor and outdoor environments, but
it presents challenges in achieving high accuracy. Specifically, camera
relocalization methods, such as Absolute Pose Regression (APR), can localize
cameras with a median translation error of more than $0.5m$ in outdoor scenes.
This paper presents HAL-NeRF, a high-accuracy localization method that combines
a CNN pose regressor with a refinement module based on a Monte Carlo particle
filter. The Nerfacto model, an implementation of Neural Radiance Fields
(NeRFs), is used to augment the data for training the pose regressor and to
measure photometric loss in the particle filter refinement module. HAL-NeRF
leverages Nerfacto's ability to synthesize high-quality novel views,
significantly improving the performance of the localization pipeline. HAL-NeRF
achieves state-of-the-art results that are conventionally measured as the
average of the median per scene errors. The translation error was $0.025m$ and
the rotation error was $0.59$ degrees and 0.04m and 0.58 degrees on the
7-Scenes dataset and Cambridge Landmarks datasets respectively, with the
trade-off of increased computational time. This work highlights the potential
of combining APR with NeRF-based refinement techniques to advance monocular
camera relocalization accuracy.

Comments:
- 8 pages, 4 figures

---

## FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level  Interactive Agents


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-11 | Xin Tan, Yuzhou Ji, He Zhu, Yuan Xie | cs.CV | [PDF](http://arxiv.org/pdf/2504.08581v1){: .btn .btn-green } |

**Abstract**: The semantically interactive radiance field has long been a promising
backbone for 3D real-world applications, such as embodied AI to achieve scene
understanding and manipulation. However, multi-granularity interaction remains
a challenging task due to the ambiguity of language and degraded quality when
it comes to queries upon object components. In this work, we present FMLGS, an
approach that supports part-level open-vocabulary query within 3D Gaussian
Splatting (3DGS). We propose an efficient pipeline for building and querying
consistent object- and part-level semantics based on Segment Anything Model 2
(SAM2). We designed a semantic deviation strategy to solve the problem of
language ambiguity among object parts, which interpolates the semantic features
of fine-grained targets for enriched information. Once trained, we can query
both objects and their describable parts using natural language. Comparisons
with other state-of-the-art methods prove that our method can not only better
locate specified part-level targets, but also achieve first-place performance
concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x
faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further
integrate FMLGS as a virtual agent that can interactively navigate through 3D
scenes, locate targets, and respond to user demands through a chat interface,
which demonstrates the potential of our work to be further expanded and applied
in the future.



---

## In-2-4D: Inbetweening from Two Single-View Images to 4D Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-11 | Sauradip Nag, Daniel Cohen-Or, Hao Zhang, Ali Mahdavi-Amiri | cs.GR | [PDF](http://arxiv.org/pdf/2504.08366v1){: .btn .btn-green } |

**Abstract**: We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)
inbetweening from a minimalistic input setting: two single-view images
capturing an object in two distinct motion states. Given two images
representing the start and end states of an object in motion, our goal is to
generate and reconstruct the motion in 4D. We utilize a video interpolation
model to predict the motion, but large frame-to-frame motions can lead to
ambiguous interpretations. To overcome this, we employ a hierarchical approach
to identify keyframes that are visually close to the input states and show
significant motion, then generate smooth fragments between them. For each
fragment, we construct the 3D representation of the keyframe using Gaussian
Splatting. The temporal frames within the fragment guide the motion, enabling
their transformation into dynamic Gaussians through a deformation field. To
improve temporal consistency and refine 3D motion, we expand the self-attention
of multi-view diffusion across timesteps and apply rigid transformation
regularization. Finally, we merge the independently generated 3D motion
segments by interpolating boundary deformation fields and optimizing them to
align with the guiding video, ensuring smooth and flicker-free transitions.
Through extensive qualitative and quantitiave experiments as well as a user
study, we show the effectiveness of our method and its components. The project
page is available at https://in-2-4d.github.io/

Comments:
- Technical Report

---

## Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-11 | Bram Vanherle, Brent Zoomers, Jeroen Put, Frank Van Reeth, Nick Michiels | cs.CV | [PDF](http://arxiv.org/pdf/2504.08473v1){: .btn .btn-green } |

**Abstract**: Generating synthetic images is a useful method for cheaply obtaining labeled
data for training computer vision models. However, obtaining accurate 3D models
of relevant objects is necessary, and the resulting images often have a gap in
realism due to challenges in simulating lighting effects and camera artifacts.
We propose using the novel view synthesis method called Gaussian Splatting to
address these challenges. We have developed a synthetic data pipeline for
generating high-quality context-aware instance segmentation training data for
specific objects. This process is fully automated, requiring only a video of
the target object. We train a Gaussian Splatting model of the target object and
automatically extract the object from the video. Leveraging Gaussian Splatting,
we then render the object on a random background image, and monocular depth
estimation is employed to place the object in a believable pose. We introduce a
novel dataset to validate our approach and show superior performance over other
data generation approaches, such as Cut-and-Paste and Diffusion model-based
generation.

Comments:
- Accepted at the International Conference on Robotics, Computer Vision
  and Intelligent Systems 2025 (ROBOVIS)

---

## InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars  with Deformable Gaussians


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-10 | Kefan Chen, Sergiu Oprea, Justin Theiss, Sreyas Mohan, Srinath Sridhar, Aayush Prakash | cs.CV | [PDF](http://arxiv.org/pdf/2504.07949v1){: .btn .btn-green } |

**Abstract**: With the rising interest from the community in digital avatars coupled with
the importance of expressions and gestures in communication, modeling natural
avatar behavior remains an important challenge across many industries such as
teleconferencing, gaming, and AR/VR. Human hands are the primary tool for
interacting with the environment and essential for realistic human behavior
modeling, yet existing 3D hand and head avatar models often overlook the
crucial aspect of hand-body interactions, such as between hand and face. We
present InteracttAvatar, the first model to faithfully capture the
photorealistic appearance of dynamic hand and non-rigid hand-face interactions.
Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian
Splatting as well as a dynamic refinement module, captures pose-dependent
change, e.g. the fine wrinkles and complex shadows that occur during
articulation. Importantly, our hand-face interaction module models the subtle
geometry and appearance dynamics that underlie common gestures. Through
experiments of novel view synthesis, self reenactment and cross-identity
reenactment, we demonstrate that InteracttAvatar can reconstruct hand and
hand-face interactions from monocular or multiview videos with high-fidelity
details and be animated with novel poses.



---

## View-Dependent Uncertainty Estimation of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-10 | Chenyu Han, Corentin Dumery | cs.CV | [PDF](http://arxiv.org/pdf/2504.07370v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene
reconstruction for its high visual accuracy. However, uncertainty estimation of
3DGS scenes remains underexplored and is crucial to downstream tasks such as
asset extraction and scene completion. Since the appearance of 3D gaussians is
view-dependent, the color of a gaussian can thus be certain from an angle and
uncertain from another. We thus propose to model uncertainty in 3DGS as an
additional view-dependent per-gaussian feature that can be modeled with
spherical harmonics. This simple yet effective modeling is easily interpretable
and can be integrated into the traditional 3DGS pipeline. It is also
significantly faster than ensemble methods while maintaining high accuracy, as
demonstrated in our experiments.



---

## ContrastiveGaussian: High-Fidelity 3D Generation with Contrastive  Learning and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-10 | Junbang Liu, Enpei Huang, Dongxing Mao, Hui Zhang, Xinyuan Song, Yongxin Ni | cs.CV | [PDF](http://arxiv.org/pdf/2504.08100v1){: .btn .btn-green } |

**Abstract**: Creating 3D content from single-view images is a challenging problem that has
attracted considerable attention in recent years. Current approaches typically
utilize score distillation sampling (SDS) from pre-trained 2D diffusion models
to generate multi-view 3D representations. Although some methods have made
notable progress by balancing generation speed and model quality, their
performance is often limited by the visual inconsistencies of the diffusion
model outputs. In this work, we propose ContrastiveGaussian, which integrates
contrastive learning into the generative process. By using a perceptual loss,
we effectively differentiate between positive and negative samples, leveraging
the visual inconsistencies to improve 3D generation quality. To further enhance
sample differentiation and improve contrastive learning, we incorporate a
super-resolution model and introduce another Quantity-Aware Triplet Loss to
address varying sample distributions during training. Our experiments
demonstrate that our approach achieves superior texture fidelity and improved
geometric consistency.

Comments:
- Code will be available at
  https://github.com/YaNLlan-ljb/ContrastiveGaussian

---

## Stochastic Ray Tracing of 3D Transparent Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Xin Sun, Iliyan Georgiev, Yun Fei, Miloš Hašan | cs.GR | [PDF](http://arxiv.org/pdf/2504.06598v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has recently been widely adopted as a 3D representation
for novel-view synthesis, relighting, and text-to-3D generation tasks, offering
realistic and detailed results through a collection of explicit 3D Gaussians
carrying opacities and view-dependent colors. However, efficient rendering of
many transparent primitives remains a significant challenge. Existing
approaches either rasterize the 3D Gaussians with approximate sorting per view
or rely on high-end RTX GPUs to exhaustively process all ray-Gaussian
intersections (bounding Gaussians by meshes). This paper proposes a stochastic
ray tracing method to render 3D clouds of transparent primitives. Instead of
processing all ray-Gaussian intersections in sequential order, each ray
traverses the acceleration structure only once, randomly accepting and shading
a single intersection (or N intersections, using a simple extension). This
approach minimizes shading time and avoids sorting the Gaussians along the ray
while minimizing the register usage and maximizing parallelism even on low-end
GPUs. The cost of rays through the Gaussian asset is comparable to that of
standard mesh-intersection rays. While our method introduces noise, the shading
is unbiased, and the variance is slight, as stochastic acceptance is
importance-sampled based on accumulated opacity. The alignment with the Monte
Carlo philosophy simplifies implementation and easily integrates our method
into a conventional path-tracing framework.

Comments:
- 10 pages, 6 figures, 5 tables

---

## Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and  Phenotyping of Wheat Heads with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Daiwei Zhang, Joaquin Gajardo, Tomislav Medic, Isinsu Katircioglu, Mike Boss, Norbert Kirchgessner, Achim Walter, Lukas Roth | cs.CV | [PDF](http://arxiv.org/pdf/2504.06978v1){: .btn .btn-green } |

**Abstract**: Automated extraction of plant morphological traits is crucial for supporting
crop breeding and agricultural management through high-throughput field
phenotyping (HTFP). Solutions based on multi-view RGB images are attractive due
to their scalability and affordability, enabling volumetric measurements that
2D approaches cannot directly capture. While advanced methods like Neural
Radiance Fields (NeRFs) have shown promise, their application has been limited
to counting or extracting traits from only a few plants or organs. Furthermore,
accurately measuring complex structures like individual wheat heads-essential
for studying crop yields-remains particularly challenging due to occlusions and
the dense arrangement of crop canopies in field conditions. The recent
development of 3D Gaussian Splatting (3DGS) offers a promising alternative for
HTFP due to its high-quality reconstructions and explicit point-based
representation. In this paper, we present Wheat3DGS, a novel approach that
leverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance
segmentation and morphological measurement of hundreds of wheat heads
automatically, representing the first application of 3DGS to HTFP. We validate
the accuracy of wheat head extraction against high-resolution laser scan data,
obtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and
40.2% for length, width, and volume. We provide additional comparisons to
NeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating
superior results. Our approach enables rapid, non-destructive measurements of
key yield-related traits at scale, with significant implications for
accelerating crop breeding and improving our understanding of wheat
development.

Comments:
- Copyright 2025 IEEE. This is the author's version of the work. It is
  posted here for your personal use. Not for redistribution. The definitive
  version is published in the 2025 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops (CVPRW)

---

## SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Hanxiao Sun, YuPeng Gao, Jin Xie, Jian Yang, Beibei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.06815v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D assets from images, known as inverse rendering (IR),
remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting
(3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS)
tasks. Methods apply it to relighting by separating radiance into BRDF
parameters and lighting, yet produce inferior relighting quality with artifacts
and unnatural indirect illumination due to the limited capability of each
Gaussian, which has constant material parameters and normal, alongside the
absence of physical constraints for indirect lighting. In this paper, we
present a novel framework called Spatially-vayring Gaussian Inverse Rendering
(SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we
propose a new representation-Spatially-varying Gaussian (SVG)-that allows
per-Gaussian spatially varying parameters. This enhanced representation is
complemented by a SVG splatting scheme akin to vertex/fragment shading in
traditional graphics pipelines. Furthermore, we integrate a physically-based
indirect lighting model, enabling more realistic relighting. The proposed
SVG-IR framework significantly improves rendering quality, outperforming
state-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio
(PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in
relighting tasks, all while maintaining a real-time rendering speed.



---

## IAAO: Interactive Affordance Learning for Articulated Objects in 3D  Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Can Zhang, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2504.06827v1){: .btn .btn-green } |

**Abstract**: This work presents IAAO, a novel framework that builds an explicit 3D model
for intelligent agents to gain understanding of articulated objects in their
environment through interaction. Unlike prior methods that rely on
task-specific networks and assumptions about movable parts, our IAAO leverages
large foundation models to estimate interactive affordances and part
articulations in three stages. We first build hierarchical features and label
fields for each object state using 3D Gaussian Splatting (3DGS) by distilling
mask features and view-consistent labels from multi-view images. We then
perform object- and part-level queries on the 3D Gaussian primitives to
identify static and articulated elements, estimating global transformations and
local articulation parameters along with affordances. Finally, scenes from
different states are merged and refined based on the estimated transformations,
enabling robust affordance-based interaction and manipulation of objects.
Experimental results demonstrate the effectiveness of our method.



---

## GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Anil Armagan, Albert Saà-Garriga, Bruno Manganelli, Kyuwon Kim, M. Kerim Yucel | cs.CV | [PDF](http://arxiv.org/pdf/2504.06716v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly
due to its ability to converge reasonably fast, faithfully represent the scene
and render (novel) views in a fast fashion. However, it suffers from large
storage and memory requirements, and its training speed still lags behind the
hash-grid based radiance field approaches (e.g. Instant-NGP), which makes it
especially difficult to deploy them in robotics scenarios, where 3D
reconstruction is crucial for accurate operation. In this paper, we propose
GSta that dynamically identifies Gaussians that have converged well during
training, based on their positional and color gradient norms. By forcing such
Gaussians into a siesta and stopping their updates (freezing) during training,
we improve training speed with competitive accuracy compared to state of the
art. We also propose an early stopping mechanism based on the PSNR values
computed on a subset of training images. Combined with other improvements, such
as integrating a learning rate scheduler, GSta achieves an improved Pareto
front in convergence speed, memory and storage requirements, while preserving
quality. We also show that GSta can improve other methods and complement
orthogonal approaches in efficiency improvement; once combined with Trick-GS,
GSta achieves up to 5x faster training, 16x smaller disk size compared to
vanilla GS, while having comparable accuracy and consuming only half the peak
memory. More visualisations are available at
https://anilarmagan.github.io/SRUK-GSta.

Comments:
- 9 pages. In submission to an IEEE conference

---

## S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in  Remote Sensing Applications

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Masquil Elías, Marí Roger, Ehret Thibaud, Meinhardt-Llopis Enric, Musé Pablo, Facciolo Gabriele | cs.CV | [PDF](http://arxiv.org/pdf/2504.06920v1){: .btn .btn-green } |

**Abstract**: We introduce the S-EO dataset: a large-scale, high-resolution dataset,
designed to advance geometry-aware shadow detection. Collected from diverse
public-domain sources, including challenge datasets and government providers
such as USGS, our dataset comprises 702 georeferenced tiles across the USA,
each covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3
pansharpened RGB images, panchromatic images, and a ground-truth DSM of the
area obtained from LiDAR scans. For each image, we provide a shadow mask
derived from geometry and sun position, a vegetation mask based on the NDVI
index, and a bundle-adjusted RPC model. With approximately 20,000 images, the
S-EO dataset establishes a new public resource for shadow detection in remote
sensing imagery and its applications to 3D reconstruction. To demonstrate the
dataset's impact, we train and evaluate a shadow detector, showcasing its
ability to generalize, even to aerial images. Finally, we extend EO-NeRF - a
state-of-the-art NeRF approach for satellite imagery - to leverage our shadow
predictions for improved 3D reconstructions.

Comments:
- Accepted at Earthvision 2025 (CVPR Workshop)

---

## Collision avoidance from monocular vision trained with novel view  synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Valentin Tordjman--Levavasseur, Stéphane Caron | cs.RO | [PDF](http://arxiv.org/pdf/2504.06651v1){: .btn .btn-green } |

**Abstract**: Collision avoidance can be checked in explicit environment models such as
elevation maps or occupancy grids, yet integrating such models with a
locomotion policy requires accurate state estimation. In this work, we consider
the question of collision avoidance from an implicit environment model. We use
monocular RGB images as inputs and train a collisionavoidance policy from
photorealistic images generated by 2D Gaussian splatting. We evaluate the
resulting pipeline in realworld experiments under velocity commands that bring
the robot on an intercept course with obstacles. Our results suggest that RGB
images can be enough to make collision-avoidance decisions, both in the room
where training data was collected and in out-of-distribution environments.



---

## Meta-Continual Learning of Neural Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Seungyoon Woo, Junhyeog Yun, Gunhee Kim | cs.AI | [PDF](http://arxiv.org/pdf/2504.05806v1){: .btn .btn-green } |

**Abstract**: Neural Fields (NF) have gained prominence as a versatile framework for
complex data representation. This work unveils a new problem setting termed
\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel
strategy that employs a modular architecture combined with optimization-based
meta-learning. Focused on overcoming the limitations of existing methods for
continual learning of neural fields, such as catastrophic forgetting and slow
convergence, our strategy achieves high-quality reconstruction with
significantly improved learning speed. We further introduce Fisher Information
Maximization loss for neural radiance fields (FIM-NeRF), which maximizes
information gains at the sample level to enhance learning generalization, with
proved convergence guarantee and generalization bound. We perform extensive
evaluations across image, audio, video reconstruction, and view synthesis tasks
on six diverse datasets, demonstrating our method's superiority in
reconstruction quality and speed over existing MCL and CL-NF approaches.
Notably, our approach attains rapid adaptation of neural fields for city-scale
NeRF rendering with reduced parameter requirement.



---

## SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve  Phoneme-Viseme Alignment Ambiguity

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Yihuan Huang, Jiajun Liu, Yanzhen Ren, Wuyang Liu, Juhua Tang | cs.GR | [PDF](http://arxiv.org/pdf/2504.05803v1){: .btn .btn-green } |

**Abstract**: Speech-driven talking head synthesis tasks commonly use general acoustic
features (such as HuBERT and DeepSpeech) as guided speech features. However, we
discovered that these features suffer from phoneme-viseme alignment ambiguity,
which refers to the uncertainty and imprecision in matching phonemes (speech)
with visemes (lip). To address this issue, we propose the Speech Encoder for
Lip (SE4Lip) to encode lip features from speech directly, aligning speech and
lip features in the joint embedding space by a cross-modal alignment framework.
The STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve
the fine-grained speech features. Experimental results show that SE4Lip
achieves state-of-the-art performance in both NeRF and 3DGS rendering models.
Its lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline
and produces results close to the ground truth videos.



---

## Micro-splatting: Maximizing Isotropic Constraints for Refined  Optimization in 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Jee Won Lee, Hansol Lim, Sooyeun Yang, Jongseong Choi | cs.GR | [PDF](http://arxiv.org/pdf/2504.05740v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have achieved impressive
scalability and real-time rendering for large-scale scenes but often fall short
in capturing fine-grained details. Conventional approaches that rely on
relatively large covariance parameters tend to produce blurred representations,
while directly reducing covariance sizes leads to sparsity. In this work, we
introduce Micro-splatting (Maximizing Isotropic Constraints for Refined
Optimization in 3D Gaussian Splatting), a novel framework designed to overcome
these limitations. Our approach leverages a covariance regularization term to
penalize excessively large Gaussians to ensure each splat remains compact and
isotropic. This work implements an adaptive densification strategy that
dynamically refines regions with high image gradients by lowering the splitting
threshold, followed by loss function enhancement. This strategy results in a
denser and more detailed gaussian means where needed, without sacrificing
rendering efficiency. Quantitative evaluations using metrics such as L1, L2,
PSNR, SSIM, and LPIPS, alongside qualitative comparisons demonstrate that our
method significantly enhances fine-details in 3D reconstructions.



---

## InvNeRF-Seg: Fine-Tuning a Pre-Trained NeRF for 3D Object Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Jiangsan Zhao, Jakob Geipel, Krzysztof Kusnierek, Xuean Cui | cs.CV | [PDF](http://arxiv.org/pdf/2504.05751v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have been widely adopted for reconstructing
high quality 3D point clouds from 2D RGB images. However, the segmentation of
these reconstructed 3D scenes is more essential for downstream tasks such as
object counting, size estimation, and scene understanding. While segmentation
on raw 3D point clouds using deep learning requires labor intensive and
time-consuming manual annotation, directly training NeRF on binary masks also
fails due to the absence of color and shading cues essential for geometry
learning. We propose Invariant NeRF for Segmentation (InvNeRFSeg), a two step,
zero change fine tuning strategy for 3D segmentation. We first train a standard
NeRF on RGB images and then fine tune it using 2D segmentation masks without
altering either the model architecture or loss function. This approach produces
higher quality, cleaner segmented point clouds directly from the refined
radiance field with minimal computational overhead or complexity. Field density
analysis reveals consistent semantic refinement: densities of object regions
increase while background densities are suppressed, ensuring clean and
interpretable segmentations. We demonstrate InvNeRFSegs superior performance
over both SA3D and FruitNeRF on both synthetic fruit and real world soybean
datasets. This approach effectively extends 2D segmentation to high quality 3D
segmentation.



---

## DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Wanzhou Liu, Zhexiao Xiong, Xinyu Li, Nathan Jacobs | cs.CV | [PDF](http://arxiv.org/pdf/2504.04679v1){: .btn .btn-green } |

**Abstract**: Recent novel view synthesis (NVS) techniques, including Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene
reconstruction with high-quality rendering and realistic detail recovery.
Effectively removing occlusions while preserving scene details can further
enhance the robustness and applicability of these techniques. However, existing
approaches for object and occlusion removal predominantly rely on generative
priors, which, despite filling the resulting holes, introduce new artifacts and
blurriness. Moreover, existing benchmark datasets for evaluating occlusion
removal methods lack realistic complexity and viewpoint variations. To address
these issues, we introduce DeclutterSet, a novel dataset featuring diverse
scenes with pronounced occlusions distributed across foreground, midground, and
background, exhibiting substantial relative motion across viewpoints. We
further introduce DeclutterNeRF, an occlusion removal method free from
generative priors. DeclutterNeRF introduces joint multi-view optimization of
learnable camera parameters, occlusion annealing regularization, and employs an
explainable stochastic structural similarity loss, ensuring high-quality,
artifact-free reconstructions from incomplete images. Experiments demonstrate
that DeclutterNeRF significantly outperforms state-of-the-art methods on our
proposed DeclutterSet, establishing a strong baseline for future research.

Comments:
- Accepted by CVPR 2025 4th CV4Metaverse Workshop. 15 pages, 10
  figures. Code and data at: https://github.com/wanzhouliu/declutter-nerf

---

## View-Dependent Deformation Fields for 2D Editing of 3D Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Martin El Mqirmi, Noam Aigerman | cs.GR | [PDF](http://arxiv.org/pdf/2504.05544v1){: .btn .btn-green } |

**Abstract**: We propose a method for authoring non-realistic 3D objects (represented as
either 3D Gaussian Splats or meshes), that comply with 2D edits from specific
viewpoints. Namely, given a 3D object, a user chooses different viewpoints and
interactively deforms the object in the 2D image plane of each view. The method
then produces a "deformation field" - an interpolation between those 2D
deformations in a smooth manner as the viewpoint changes. Our core observation
is that the 2D deformations do not need to be tied to an underlying object, nor
share the same deformation space. We use this observation to devise a method
for authoring view-dependent deformations, holding several technical
contributions: first, a novel way to compositionality-blend between the 2D
deformations after lifting them to 3D - this enables the user to "stack" the
deformations similarly to layers in an editing software, each deformation
operating on the results of the previous; second, a novel method to apply the
3D deformation to 3D Gaussian Splats; third, an approach to author the 2D
deformations, by deforming a 2D mesh encapsulating a rendered image of the
object. We show the versatility and efficacy of our method by adding cartoonish
effects to objects, providing means to modify human characters, fitting 3D
models to given 2D sketches and caricatures, resolving occlusions, and
recreating classic non-realistic paintings as 3D models.



---

## 3D Gaussian Particle Approximation of VDB Datasets: A Study for  Scientific Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Isha Sharma, Dieter Schmalstieg | cs.GR | [PDF](http://arxiv.org/pdf/2504.04857v1){: .btn .btn-green } |

**Abstract**: The complexity and scale of Volumetric and Simulation datasets for Scientific
Visualization(SciVis) continue to grow. And the approaches and advantages of
memory-efficient data formats and storage techniques for such datasets vary.
OpenVDB library and its VDB data format excels in memory efficiency through its
hierarchical and dynamic tree structure, with active and inactive sub-trees for
data storage. It is heavily used in current production renderers for both
animation and rendering stages in VFX pipelines and photorealistic rendering of
volumes and fluids. However, it still remains to be fully leveraged in SciVis
where domains dealing with sparse scalar fields like porous media, time varying
volumes such as tornado and weather simulation or high resolution simulation of
Computational Fluid Dynamics present ample number of large challenging data
sets.Goal of this paper is not only to explore the use of OpenVDB in SciVis but
also to explore a level of detail(LOD) technique using 3D Gaussian particles
approximating voxel regions. For rendering, we utilize NVIDIA OptiX library for
ray marching through the Gaussians particles. Data modeling using 3D Gaussians
has been very popular lately due to success in stereoscopic image to 3D scene
conversion using Gaussian Splatting and Gaussian approximation and mixture
models aren't entirely new in SciVis as well. Our work explores the integration
with rendering software libraries like OpenVDB and OptiX to take advantage of
their built-in memory compaction and hardware acceleration features, while also
leveraging the performance capabilities of modern GPUs. Thus, we present a
SciVis rendering approach that uses 3D Gaussians at varying LOD in a lossy
scheme derived from VDB datasets, rather than focusing on photorealistic volume
rendering.



---

## Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather  Effects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Gal Fiebelman, Hadar Averbuch-Elor, Sagie Benaim | cs.GR | [PDF](http://arxiv.org/pdf/2504.05296v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently enabled fast and photorealistic
reconstruction of static 3D scenes. However, introducing dynamic elements that
interact naturally with such static scenes remains challenging. Accordingly, we
present a novel hybrid framework that combines Gaussian-particle
representations for incorporating physically-based global weather effects into
static 3D Gaussian Splatting scenes, correctly handling the interactions of
dynamic elements with the static scene. We follow a three-stage process: we
first map static 3D Gaussians to a particle-based representation. We then
introduce dynamic particles and simulate their motion using the Material Point
Method (MPM). Finally, we map the simulated particles back to the Gaussian
domain while introducing appearance parameters tailored for specific effects.
To correctly handle the interactions of dynamic elements with the static scene,
we introduce specialized collision handling techniques. Our approach supports a
variety of weather effects, including snowfall, rainfall, fog, and sandstorms,
and can also support falling objects, all with physically plausible motion and
appearance. Experiments demonstrate that our method significantly outperforms
existing approaches in both visual quality and physical realism.

Comments:
- Project webpage: https://galfiebelman.github.io/let-it-snow/

---

## PanoDreamer: Consistent Text to 360-Degree Scene Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Zhexiao Xiong, Zhang Chen, Zhong Li, Yi Xu, Nathan Jacobs | cs.CV | [PDF](http://arxiv.org/pdf/2504.05152v1){: .btn .btn-green } |

**Abstract**: Automatically generating a complete 3D scene from a text description, a
reference image, or both has significant applications in fields like virtual
reality and gaming. However, current methods often generate low-quality
textures and inconsistent 3D structures. This is especially true when
extrapolating significantly beyond the field of view of the reference image. To
address these challenges, we propose PanoDreamer, a novel framework for
consistent, 3D scene generation with flexible text and image control. Our
approach employs a large language model and a warp-refine pipeline, first
generating an initial set of images and then compositing them into a 360-degree
panorama. This panorama is then lifted into 3D to form an initial point cloud.
We then use several approaches to generate additional images, from different
viewpoints, that are consistent with the initial point cloud and expand/refine
the initial point cloud. Given the resulting set of images, we utilize 3D
Gaussian Splatting to create the final 3D scene, which can then be rendered
from different viewpoints. Experiments demonstrate the effectiveness of
PanoDreamer in generating high-quality, geometrically consistent 3D scenes.

Comments:
- Accepted by CVPR 2025 Workshop on Computer Vision for Metaverse

---

## L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Yi-Zhen Tsai, Xuechen Zhang, Zheng Li, Jiasi Chen | cs.GR | [PDF](http://arxiv.org/pdf/2504.05517v1){: .btn .btn-green } |

**Abstract**: Traditional 3D content representations include dense point clouds that
consume large amounts of data and hence network bandwidth, while newer
representations such as neural radiance fields suffer from poor frame rates due
to their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS)
can be seen as a generalization of point clouds that meet the best of both
worlds, with high visual quality and efficient rendering for real-time frame
rates. However, delivering 3DGS scenes from a hosting server to client devices
is still challenging due to high network data consumption (e.g., 1.5 GB for a
single scene). The goal of this work is to create an efficient 3D content
delivery framework that allows users to view high quality 3D scenes with 3DGS
as the underlying data representation. The main contributions of the paper are:
(1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling
algorithms to choose what splats to download at what time, and (3) Trace-driven
experiments from users wearing virtual reality headsets to evaluate the visual
quality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS
demonstrates high visual quality, achieving 16.9% higher average SSIM compared
to baselines, and also works with other compressed 3DGS representations.



---

## Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Zhicong Sun, Jacqueline Lo, Jinxing Hu | cs.RO | [PDF](http://arxiv.org/pdf/2504.04844v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping (SLAM) technology now has
photorealistic mapping capabilities thanks to the real-time high-fidelity
rendering capability of 3D Gaussian splatting (3DGS). However, due to the
static representation of scenes, current 3DGS-based SLAM encounters issues with
pose drift and failure to reconstruct accurate maps in dynamic environments. To
address this problem, we present D4DGS-SLAM, the first SLAM method based on
4DGS map representation for dynamic environments. By incorporating the temporal
dimension into scene representation, D4DGS-SLAM enables high-quality
reconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we
can obtain the dynamics, visibility, and reliability of scene points, and
filter stable static points for tracking accordingly. When optimizing Gaussian
points, we apply different isotropic regularization terms to Gaussians with
varying dynamic characteristics. Experimental results on real-world dynamic
scene datasets demonstrate that our method outperforms state-of-the-art
approaches in both camera pose tracking and map quality.

Comments:
- This paper is currently under reviewed for IROS 2025

---

## Tool-as-Interface: Learning Robot Policies from Human Tool Usage through  Imitation Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-06 | Haonan Chen, Cheng Zhu, Yunzhu Li, Katherine Driggs-Campbell | cs.RO | [PDF](http://arxiv.org/pdf/2504.04612v1){: .btn .btn-green } |

**Abstract**: Tool use is critical for enabling robots to perform complex real-world tasks,
and leveraging human tool-use data can be instrumental for teaching robots.
However, existing data collection methods like teleoperation are slow, prone to
control delays, and unsuitable for dynamic tasks. In contrast, human natural
data, where humans directly perform tasks with tools, offers natural,
unstructured interactions that are both efficient and easy to collect. Building
on the insight that humans and robots can share the same tools, we propose a
framework to transfer tool-use knowledge from human data to robots. Using two
RGB cameras, our method generates 3D reconstruction, applies Gaussian splatting
for novel view augmentation, employs segmentation models to extract
embodiment-agnostic observations, and leverages task-space tool-action
representations to train visuomotor policies. We validate our approach on
diverse real-world tasks, including meatball scooping, pan flipping, wine
bottle balancing, and other complex tasks. Our method achieves a 71\% higher
average success rate compared to diffusion policies trained with teleoperation
data and reduces data collection time by 77\%, with some tasks solvable only by
our framework. Compared to hand-held gripper, our method cuts data collection
time by 41\%. Additionally, our method bridges the embodiment gap, improves
robustness to variations in camera viewpoints and robot configurations, and
generalizes effectively across objects and spatial setups.

Comments:
- Project Page: https://tool-as-interface.github.io. 17 pages, 14
  figures

---

## Thermoxels: a voxel-based method to generate simulation-ready 3D thermal  models

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-06 | Etienne Chassaing, Florent Forest, Olga Fink, Malcolm Mielle | cs.CV | [PDF](http://arxiv.org/pdf/2504.04448v1){: .btn .btn-green } |

**Abstract**: In the European Union, buildings account for 42% of energy use and 35% of
greenhouse gas emissions. Since most existing buildings will still be in use by
2050, retrofitting is crucial for emissions reduction. However, current
building assessment methods rely mainly on qualitative thermal imaging, which
limits data-driven decisions for energy savings. On the other hand,
quantitative assessments using finite element analysis (FEA) offer precise
insights but require manual CAD design, which is tedious and error-prone.
Recent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and
Gaussian Splatting, enable precise 3D modeling from sparse images but lack
clearly defined volumes and the interfaces between them needed for FEA. We
propose Thermoxels, a novel voxel-based method able to generate FEA-compatible
models, including both geometry and temperature, from a sparse set of RGB and
thermal images. Using pairs of RGB and thermal images as input, Thermoxels
represents a scene's geometry as a set of voxels comprising color and
temperature information. After optimization, a simple process is used to
transform Thermoxels' models into tetrahedral meshes compatible with FEA. We
demonstrate Thermoxels' capability to generate RGB+Thermal meshes of 3D scenes,
surpassing other state-of-the-art methods. To showcase the practical
applications of Thermoxels' models, we conduct a simple heat conduction
simulation using FEA, achieving convergence from an initial state defined by
Thermoxels' thermal reconstruction. Additionally, we compare Thermoxels' image
synthesis abilities with current state-of-the-art methods, showing competitive
results, and discuss the limitations of existing metrics in assessing mesh
quality.

Comments:
- 7 pages, 2 figures

---

## Interpretable Single-View 3D Gaussian Splatting using Unsupervised  Hierarchical Disentangled Representation Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-05 | Yuyang Zhang, Baao Xie, Hu Zhu, Qi Wang, Huanting Guo, Xin Jin, Wenjun Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2504.04190v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has recently marked a significant advancement in 3D
reconstruction, delivering both rapid rendering and high-quality results.
However, existing 3DGS methods pose challenges in understanding underlying 3D
semantics, which hinders model controllability and interpretability. To address
it, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to
discover both coarse- and fine-grained 3D semantics via hierarchical
disentangled representation learning (DRL). Specifically, the model employs a
dual-branch architecture, consisting of a point cloud initialization branch and
a triplane-Gaussian generation branch, to achieve coarse-grained
disentanglement by separating 3D geometry and visual appearance features.
Subsequently, fine-grained semantic representations within each modality are
further discovered through DRL-based encoder-adapters. To our knowledge, this
is the first work to achieve unsupervised interpretable 3DGS. Evaluations
indicate that our model achieves 3D disentanglement while preserving
high-quality and rapid reconstruction.



---

## 3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-05 | Zhisheng Huang, Peng Wang, Jingdong Zhang, Yuan Liu, Xin Li, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.04294v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its
efficiency and quality, but like many novel view synthesis methods, it heavily
depends on accurate camera poses from Structure-from-Motion (SfM) systems.
Although recent SfM pipelines have made impressive progress, questions remain
about how to further improve both their robust performance in challenging
conditions (e.g., textureless scenes) and the precision of camera parameter
estimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework
that bridges this gap by jointly optimizing 3D Gaussians and camera parameters
from large reconstruction priors MASt3R-SfM. We note that naively performing
joint 3D Gaussian and camera optimization faces two challenges: the sensitivity
to the quality of SfM initialization, and its limited capacity for global
optimization, leading to suboptimal reconstruction results. Our 3R-GS,
overcomes these issues by incorporating optimized practices, enabling robust
scene reconstruction even with imperfect camera registration. Extensive
experiments demonstrate that 3R-GS delivers high-quality novel view synthesis
and precise camera pose estimation while remaining computationally efficient.
Project page: https://zsh523.github.io/3R-GS/



---

## NeRFlex: Resource-aware Real-time High-quality Rendering of Complex  Scenes on Mobile Devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Zhe Wang, Yifei Zhu | cs.GR | [PDF](http://arxiv.org/pdf/2504.03415v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) is a cutting-edge neural network-based
technique for novel view synthesis in 3D reconstruction. However, its
significant computational demands pose challenges for deployment on mobile
devices. While mesh-based NeRF solutions have shown potential in achieving
real-time rendering on mobile platforms, they often fail to deliver
high-quality reconstructions when rendering practical complex scenes.
Additionally, the non-negligible memory overhead caused by pre-computed
intermediate results complicates their practical application. To overcome these
challenges, we present NeRFlex, a resource-aware, high-resolution, real-time
rendering framework for complex scenes on mobile devices. NeRFlex integrates
mobile NeRF rendering with multi-NeRF representations that decompose a scene
into multiple sub-scenes, each represented by an individual NeRF network.
Crucially, NeRFlex considers both memory and computation constraints as
first-class citizens and redesigns the reconstruction process accordingly.
NeRFlex first designs a detail-oriented segmentation module to identify
sub-scenes with high-frequency details. For each NeRF network, a lightweight
profiler, built on domain knowledge, is used to accurately map configurations
to visual quality and memory usage. Based on these insights and the resource
constraints on mobile devices, NeRFlex presents a dynamic programming algorithm
to efficiently determine configurations for all NeRF representations, despite
the NP-hardness of the original decision problem. Extensive experiments on
real-world datasets and mobile devices demonstrate that NeRFlex achieves
real-time, high-quality rendering on commercial mobile devices.

Comments:
- This paper is accepted by 45th IEEE International Conference on
  Distributed Computing Systems (ICDCS 2025)

---

## HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction  via Gaussian Restoration


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Boyuan Wang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Guan Huang, Lihong Liu, Xingang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.03536v1){: .btn .btn-green } |

**Abstract**: Single-image human reconstruction is vital for digital human modeling
applications but remains an extremely challenging task. Current approaches rely
on generative models to synthesize multi-view images for subsequent 3D
reconstruction and animation. However, directly generating multiple views from
a single human image suffers from geometric inconsistencies, resulting in
issues like fragmented or blurred limbs in the reconstructed models. To tackle
these limitations, we introduce \textbf{HumanDreamer-X}, a novel framework that
integrates multi-view human generation and reconstruction into a unified
pipeline, which significantly enhances the geometric consistency and visual
fidelity of the reconstructed 3D models. In this framework, 3D Gaussian
Splatting serves as an explicit 3D representation to provide initial geometry
and appearance priority. Building upon this foundation, \textbf{HumanFixer} is
trained to restore 3DGS renderings, which guarantee photorealistic results.
Furthermore, we delve into the inherent challenges associated with attention
mechanisms in multi-view human generation, and propose an attention modulation
strategy that effectively enhances geometric details identity consistency
across multi-view. Experimental results demonstrate that our approach markedly
improves generation and reconstruction PSNR quality metrics by 16.45% and
12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing
generalization capabilities on in-the-wild data and applicability to various
human reconstruction backbone models.

Comments:
- Project Page: https://humandreamer-x.github.io/

---

## WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Jianhao Zheng, Zihan Zhu, Valentin Bieri, Marc Pollefeys, Songyou Peng, Iro Armeni | cs.CV | [PDF](http://arxiv.org/pdf/2504.03886v1){: .btn .btn-green } |

**Abstract**: We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system
designed to handle dynamic environments by leveraging uncertainty-aware
geometric mapping. Unlike traditional SLAM systems, which assume static scenes,
our approach integrates depth and uncertainty information to enhance tracking,
mapping, and rendering performance in the presence of moving objects. We
introduce an uncertainty map, predicted by a shallow multi-layer perceptron and
DINOv2 features, to guide dynamic object removal during both tracking and
mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map
optimization, improving reconstruction accuracy. Our system is evaluated on
multiple datasets and demonstrates artifact-free view synthesis. Results
showcase WildGS-SLAM's superior performance in dynamic environments compared to
state-of-the-art methods.



---

## LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Ming-Jia Yang, Yu-Xiao Guo, Yang Liu, Bin Zhou, Xin Tong | cs.CV | [PDF](http://arxiv.org/pdf/2504.02337v1){: .btn .btn-green } |

**Abstract**: Generating realistic, room-level indoor scenes with semantically plausible
and detailed appearances from in-the-wild images is crucial for various
applications in VR, AR, and robotics. The success of NeRF-based generative
methods indicates a promising direction to address this challenge. However,
unlike their success at the object level, existing scene-level generative
methods require additional information, such as multiple views, depth images,
or semantic guidance, rather than relying solely on RGB images. This is because
NeRF-based methods necessitate prior knowledge of camera poses, which is
challenging to approximate for indoor scenes due to the complexity of defining
alignment and the difficulty of globally estimating poses from a single image,
given the unseen parts behind the camera. To address this challenge, we
redefine global poses within the framework of Local-Pose-Alignment (LPA) -- an
anchor-based multi-local-coordinate system that uses a selected number of
anchors as the roots of these coordinates. Building on this foundation, we
introduce LPA-GAN, a novel NeRF-based generative approach that incorporates
specific modifications to estimate the priors of camera poses under LPA. It
also co-optimizes the pose predictor and scene generation processes. Our
ablation study and comparisons with straightforward extensions of NeRF-based
object generative methods demonstrate the effectiveness of our approach.
Furthermore, visual comparisons with other techniques reveal that our method
achieves superior view-to-view consistency and semantic normality.



---

## MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Renwu Li, Wenjing Ke, Dong Li, Lu Tian, Emad Barsoum | cs.CV | [PDF](http://arxiv.org/pdf/2504.02437v1){: .btn .btn-green } |

**Abstract**: We present MonoGS++, a novel fast and accurate Simultaneous Localization and
Mapping (SLAM) method that leverages 3D Gaussian representations and operates
solely on RGB inputs. While previous 3D Gaussian Splatting (GS)-based methods
largely depended on depth sensors, our approach reduces the hardware dependency
and only requires RGB input, leveraging online visual odometry (VO) to generate
sparse point clouds in real-time. To reduce redundancy and enhance the quality
of 3D scene reconstruction, we implemented a series of methodological
enhancements in 3D Gaussian mapping. Firstly, we introduced dynamic 3D Gaussian
insertion to avoid adding redundant Gaussians in previously well-reconstructed
areas. Secondly, we introduced clarity-enhancing Gaussian densification module
and planar regularization to handle texture-less areas and flat surfaces
better. We achieved precise camera tracking results both on the synthetic
Replica and real-world TUM-RGBD datasets, comparable to those of the
state-of-the-art. Additionally, our method realized a significant 5.57x
improvement in frames per second (fps) over the previous state-of-the-art,
MonoGS.



---

## ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Yuan Zhou, Shilong Jin, Litao Hua, Wanjun Lv, Haoran Duan, Jungong Han | cs.CV | [PDF](http://arxiv.org/pdf/2504.02316v1){: .btn .btn-green } |

**Abstract**: Recent advances in zero-shot text-to-3D generation have revolutionized 3D
content creation by enabling direct synthesis from textual descriptions. While
state-of-the-art methods leverage 3D Gaussian Splatting with score distillation
to enhance multi-view rendering through pre-trained text-to-image (T2I) models,
they suffer from inherent view biases in T2I priors. These biases lead to
inconsistent 3D generation, particularly manifesting as the multi-face Janus
problem, where objects exhibit conflicting features across views. To address
this fundamental challenge, we propose ConsDreamer, a novel framework that
mitigates view bias by refining both the conditional and unconditional terms in
the score distillation process: (1) a View Disentanglement Module (VDM) that
eliminates viewpoint biases in conditional prompts by decoupling irrelevant
view components and injecting precise camera parameters; and (2) a
similarity-based partial order loss that enforces geometric consistency in the
unconditional term by aligning cosine similarities with azimuth relationships.
Extensive experiments demonstrate that ConsDreamer effectively mitigates the
multi-face Janus problem in text-to-3D generation, outperforming existing
methods in both visual quality and consistency.

Comments:
- 13 pages, 11 figures, 3 tables

---

## Compressing 3D Gaussian Splatting by Noise-Substituted Vector  Quantization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Haishan Wang, Mohammad Hassan Vali, Arno Solin | cs.CV | [PDF](http://arxiv.org/pdf/2504.03059v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D
reconstruction, achieving high-quality results with real-time radiance field
rendering. However, a key challenge is the substantial storage cost:
reconstructing a single scene typically requires millions of Gaussian splats,
each represented by 59 floating-point parameters, resulting in approximately 1
GB of memory. To address this challenge, we propose a compression method by
building separate attribute codebooks and storing only discrete code indices.
Specifically, we employ noise-substituted vector quantization technique to
jointly train the codebooks and model features, ensuring consistency between
gradient descent optimization and parameter discretization. Our method reduces
the memory consumption efficiently (around $45\times$) while maintaining
competitive reconstruction quality on standard 3D benchmark scenes. Experiments
on different codebook sizes show the trade-off between compression ratio and
image quality. Furthermore, the trained compressed model remains fully
compatible with popular 3DGS viewers and enables faster rendering speed, making
it well-suited for practical applications.

Comments:
- Appearing in Scandinavian Conference on Image Analysis (SCIA) 2025

---

## Digital-twin imaging based on descattering Gaussian splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Suguru Shimomura, Kazuki Yamanouchi, Jun Tanida | physics.optics | [PDF](http://arxiv.org/pdf/2504.02278v1){: .btn .btn-green } |

**Abstract**: Three-dimensional imaging through scattering media is important in medical
science and astronomy. We propose a digital-twin imaging method based on
Gaussian splatting to observe an object behind a scattering medium. A digital
twin model built through data assimilation, emulates the behavior of objects
and environmental changes in a virtual space. By constructing a digital twin
using point clouds composed of Gaussians and simulating the scattering process
through the convolution of a point spread function, three-dimensional objects
behind a scattering medium can be reproduced as a digital twin. In this study,
a high-contrast digital twin reproducing a three-dimensional object was
successfully constructed from degraded images, assuming that data were acquired
from wavefronts disturbed by a scattering medium. This technique reproduces
objects by integrating data processing with image measurements.



---

## MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Yash Kulthe, Andrew Gilbert, John Collomosse | cs.CV | [PDF](http://arxiv.org/pdf/2504.02517v1){: .btn .btn-green } |

**Abstract**: We present MultiNeRF, a 3D watermarking method that embeds multiple uniquely
keyed watermarks within images rendered by a single Neural Radiance Field
(NeRF) model, whilst maintaining high visual quality. Our approach extends the
TensoRF NeRF model by incorporating a dedicated watermark grid alongside the
existing geometry and appearance grids. This extension ensures higher watermark
capacity without entangling watermark signals with scene content. We propose a
FiLM-based conditional modulation mechanism that dynamically activates
watermarks based on input identifiers, allowing multiple independent watermarks
to be embedded and extracted without requiring model retraining. MultiNeRF is
validated on the NeRF-Synthetic and LLFF datasets, with statistically
significant improvements in robust capacity without compromising rendering
quality. By generalizing single-watermark NeRF methods into a flexible
multi-watermarking framework, MultiNeRF provides a scalable solution for 3D
content. attribution.



---

## WorldPrompter: Traversable Text-to-Scene Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Zhaoyang Zhang, Yannick Hold-Geoffroy, Miloš Hašan, Chen Ziwen, Fujun Luan, Julie Dorsey, Yiwei Hu | cs.GR | [PDF](http://arxiv.org/pdf/2504.02045v1){: .btn .btn-green } |

**Abstract**: Scene-level 3D generation is a challenging research topic, with most existing
methods generating only partial scenes and offering limited navigational
freedom. We introduce WorldPrompter, a novel generative pipeline for
synthesizing traversable 3D scenes from text prompts. We leverage panoramic
videos as an intermediate representation to model the 360{\deg} details of a
scene. WorldPrompter incorporates a conditional 360{\deg} panoramic video
generator, capable of producing a 128-frame video that simulates a person
walking through and capturing a virtual environment. The resulting video is
then reconstructed as Gaussian splats by a fast feedforward 3D reconstructor,
enabling a true walkable experience within the 3D scene. Experiments
demonstrate that our panoramic video generation model achieves convincing view
consistency across frames, enabling high-quality panoramic Gaussian splat
reconstruction and facilitating traversal over an area of the scene.
Qualitative and quantitative results also show it outperforms the
state-of-the-art 360{\deg} video generators and 3D scene generation models.



---

## UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Jaehoon Choi, Dongki Jung, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2504.02158v1){: .btn .btn-green } |

**Abstract**: We present UAVTwin, a method for creating digital twins from real-world
environments and facilitating data augmentation for training downstream models
embedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses
on synthesizing foreground components, such as various human instances in
motion within complex scene backgrounds, from UAV perspectives. This is
achieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing
backgrounds along with controllable synthetic human models that display diverse
appearances and actions in multiple poses. To the best of our knowledge,
UAVTwin is the first approach for UAV-based perception that is capable of
generating high-fidelity digital twins based on 3DGS. The proposed work
significantly enhances downstream models through data augmentation for
real-world environments with multiple dynamic objects and significant
appearance variations-both of which typically introduce artifacts in 3DGS-based
modeling. To tackle these challenges, we propose a novel appearance modeling
strategy and a mask refinement module to enhance the training of 3D Gaussian
Splatting. We demonstrate the high quality of neural rendering by achieving a
1.23 dB improvement in PSNR compared to recent methods. Furthermore, we
validate the effectiveness of data augmentation by showing a 2.5% to 13.7%
improvement in mAP for the human detection task.



---

## 3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Hao Wu, Hao Wang, Ruochong Li, Xuran Ma, Hui Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2504.01619v1){: .btn .btn-green } |

**Abstract**: Recent advancements in text-to-3D generation have shown remarkable results by
leveraging 3D priors in combination with 2D diffusion. However, previous
methods utilize 3D priors that lack detailed and complex structural
information, limiting them to generating simple objects and presenting
challenges for creating intricate structures such as bonsai. In this paper, we
propose 3DBonsai, a novel text-to-3D framework for generating 3D bonsai with
complex structures. Technically, we first design a trainable 3D space
colonization algorithm to produce bonsai structures, which are then enhanced
through random sampling and point cloud augmentation to serve as the 3D
Gaussian priors. We introduce two bonsai generation pipelines with distinct
structural levels: fine structure conditioned generation, which initializes 3D
Gaussians using a 3D structure prior to produce detailed and complex bonsai,
and coarse structure conditioned generation, which employs a multi-view
structure consistency module to align 2D and 3D structures. Moreover, we have
compiled a unified 2D and 3D Chinese-style bonsai dataset. Our experimental
results demonstrate that 3DBonsai significantly outperforms existing methods,
providing a new benchmark for structure-aware 3D bonsai generation.

Comments:
- Accepted by ICME 2025

---

## Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D  Reconstruction and Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar | cs.CV | [PDF](http://arxiv.org/pdf/2504.01960v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance
Fields (NeRF) have achieved impressive results in real-time 3D reconstruction
and novel view synthesis. However, these methods struggle in large-scale,
unconstrained environments where sparse and uneven input coverage, transient
occlusions, appearance variability, and inconsistent camera settings lead to
degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a
multi-view diffusion model to address these limitations. By generating
pseudo-observations conditioned on multi-view inputs, our method transforms
under-constrained 3D reconstruction problems into well-posed ones, enabling
robust optimization even with sparse data. GS-Diff further integrates several
enhancements, including appearance embedding, monocular depth priors, dynamic
object modeling, anisotropy regularization, and advanced rasterization
techniques, to tackle geometric and photometric challenges in real-world
settings. Experiments on four benchmarks demonstrate that GS-Diff consistently
outperforms state-of-the-art baselines by significant margins.

Comments:
- WACV ULTRRA Workshop 2025

---

## Toward Real-world BEV Perception: Depth Uncertainty Estimation via  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen | cs.CV | [PDF](http://arxiv.org/pdf/2504.01957v2){: .btn .btn-green } |

**Abstract**: Bird's-eye view (BEV) perception has gained significant attention because it
provides a unified representation to fuse multiple view images and enables a
wide range of down-stream autonomous driving tasks, such as forecasting and
planning. Recent state-of-the-art models utilize projection-based methods which
formulate BEV perception as query learning to bypass explicit depth estimation.
While we observe promising advancements in this paradigm, they still fall short
of real-world applications because of the lack of uncertainty modeling and
expensive computational requirement. In this work, we introduce GaussianLSS, a
novel uncertainty-aware BEV perception framework that revisits
unprojection-based methods, specifically the Lift-Splat-Shoot (LSS) paradigm,
and enhances them with depth un-certainty modeling. GaussianLSS represents
spatial dispersion by learning a soft depth mean and computing the variance of
the depth distribution, which implicitly captures object extents. We then
transform the depth distribution into 3D Gaussians and rasterize them to
construct uncertainty-aware BEV features. We evaluate GaussianLSS on the
nuScenes dataset, achieving state-of-the-art performance compared to
unprojection-based methods. In particular, it provides significant advantages
in speed, running 2.5x faster, and in memory efficiency, using 0.3x less memory
compared to projection-based methods, while achieving competitive performance
with only a 0.4% IoU difference.

Comments:
- Accepted to CVPR'25. https://hcis-lab.github.io/GaussianLSS/

---

## FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D  Scene Reconstruction and Benchmarking

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Ulas Gunes, Matias Turkulainen, Xuqian Ren, Arno Solin, Juho Kannala, Esa Rahtu | cs.CV | [PDF](http://arxiv.org/pdf/2504.01732v1){: .btn .btn-green } |

**Abstract**: The development of large-scale 3D scene reconstruction and novel view
synthesis methods mostly rely on datasets comprising perspective images with
narrow fields of view (FoV). While effective for small-scale scenes, these
datasets require large image sets and extensive structure-from-motion (SfM)
processing, limiting scalability. To address this, we introduce a fisheye image
dataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye
lenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor
scenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense
point clouds that can be used as geometric ground-truth, enabling robust
benchmarking under challenging conditions such as occlusions and reflections.
While the baseline experiments focus on vanilla Gaussian Splatting and NeRF
based Nerfacto methods, the dataset supports diverse approaches for scene
reconstruction, novel view synthesis, and image-based rendering.

Comments:
- SCIA 2025

---

## Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting  Conditions with View-Adaptive Curve Adjustment

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Ziteng Cui, Xuangeng Chu, Tatsuya Harada | cs.CV | [PDF](http://arxiv.org/pdf/2504.01503v1){: .btn .btn-green } |

**Abstract**: Capturing high-quality photographs under diverse real-world lighting
conditions is challenging, as both natural lighting (e.g., low-light) and
camera exposure settings (e.g., exposure time) significantly impact image
quality. This challenge becomes more pronounced in multi-view scenarios, where
variations in lighting and image signal processor (ISP) settings across
viewpoints introduce photometric inconsistencies. Such lighting degradations
and view-dependent variations pose substantial challenges to novel view
synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel
approach to achieving high-quality novel view synthesis results under diverse
challenging lighting conditions using 3DGS. By adopting per-view color matrix
mapping and view-adaptive curve adjustments, Luminance-GS achieves
state-of-the-art (SOTA) results across various lighting conditions -- including
low-light, overexposure, and varying exposure -- while not altering the
original 3DGS explicit representation. Compared to previous NeRF- and
3DGS-based baselines, Luminance-GS provides real-time rendering speed with
improved reconstruction quality.

Comments:
- CVPR 2025, project page:
  https://cuiziteng.github.io/Luminance_GS_web/

---

## High-fidelity 3D Object Generation from Single Image with RGBN-Volume  Gaussian Reconstruction Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao | cs.CV | [PDF](http://arxiv.org/pdf/2504.01512v1){: .btn .btn-green } |

**Abstract**: Recently single-view 3D generation via Gaussian splatting has emerged and
developed quickly. They learn 3D Gaussians from 2D RGB images generated from
pre-trained multi-view diffusion (MVD) models, and have shown a promising
avenue for 3D generation through a single image. Despite the current progress,
these methods still suffer from the inconsistency jointly caused by the
geometric ambiguity in the 2D images, and the lack of structure of 3D
Gaussians, leading to distorted and blurry 3D object generation. In this paper,
we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian
Reconstruction Model designed to generate high-fidelity 3D objects from
single-view images. Our key insight is a structured 3D representation can
simultaneously mitigate the afore-mentioned two issues. To this end, we propose
a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation
contains explicit 3D geometric information, eliminating the geometric ambiguity
from 2D images. It also structures Gaussians during learning so that the
optimization tends to find better local optima. Our 3D voxel representation is
obtained by a fusion module that aligns RGB features and surface normal
features, both of which can be estimated from 2D images. Extensive experiments
demonstrate the superiority of our methods over prior works in terms of
high-quality reconstruction results, robust generalization, and good
efficiency.

Comments:
- 12 pages

---

## BOGausS: Better Optimized Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Stéphane Pateux, Matthieu Gendrin, Luce Morin, Théo Ladune, Xiaoran Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2504.01844v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view
synthesis. Its framework provides fast and high-fidelity rendering. Although
less complex than other solutions such as Neural Radiance Fields (NeRF), there
are still some challenges building smaller models without sacrificing quality.
In this study, we perform a careful analysis of 3DGS training process and
propose a new optimization methodology. Our Better Optimized Gaussian Splatting
(BOGausS) solution is able to generate models up to ten times lighter than the
original 3DGS with no quality degradation, thus significantly boosting the
performance of Gaussian Splatting compared to the state of the art.



---

## 3D Gaussian Inverse Rendering with Approximated Global Illumination

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Zirui Wu, Jianteng Chen, Laijian Li, Shaoteng Wu, Zhikai Zhu, Kang Xu, Martin R. Oswald, Jie Song | cs.GR | [PDF](http://arxiv.org/pdf/2504.01358v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting shows great potential in reconstructing photo-realistic
3D scenes. However, these methods typically bake illumination into their
representations, limiting their use for physically-based rendering and scene
editing. Although recent inverse rendering approaches aim to decompose scenes
into material and lighting components, they often rely on simplifying
assumptions that fail when editing. We present a novel approach that enables
efficient global illumination for 3D Gaussians Splatting through screen-space
ray tracing. Our key insight is that a substantial amount of indirect light can
be traced back to surfaces visible within the current view frustum. Leveraging
this observation, we augment the direct shading computed by 3D Gaussians with
Monte-Carlo screen-space ray-tracing to capture one-bounce indirect
illumination. In this way, our method enables realistic global illumination
without sacrificing the computational efficiency and editability benefits of 3D
Gaussians. Through experiments, we show that the screen-space approximation we
utilize allows for indirect illumination and supports real-time rendering and
editing. Code, data, and models will be made available at our project page:
https://wuzirui.github.io/gs-ssr.



---

## FlowR: Flowing from Sparse to Dense 3D Reconstructions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Tobias Fischer, Samuel Rota Bulò, Yung-Hsu Yang, Nikhil Varma Keetha, Lorenzo Porzi, Norman Müller, Katja Schwarz, Jonathon Luiten, Marc Pollefeys, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2504.01647v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting enables high-quality novel view synthesis (NVS) at
real-time frame rates. However, its quality drops sharply as we depart from the
training views. Thus, dense captures are needed to match the high-quality
expectations of some applications, e.g. Virtual Reality (VR). However, such
dense captures are very laborious and expensive to obtain. Existing works have
explored using 2D generative models to alleviate this requirement by
distillation or generating additional training views. These methods are often
conditioned only on a handful of reference input views and thus do not fully
exploit the available 3D information, leading to inconsistent generation
results and reconstruction artifacts. To tackle this problem, we propose a
multi-view, flow matching model that learns a flow to connect novel view
renderings from possibly sparse reconstructions to renderings that we expect
from dense reconstructions. This enables augmenting scene captures with novel,
generated views to improve reconstruction quality. Our model is trained on a
novel dataset of 3.6M image pairs and can process up to 45 views at 540x960
resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline
consistently improves NVS in sparse- and dense-view scenarios, leading to
higher-quality reconstructions than prior works across multiple, widely-used
NVS benchmarks.

Comments:
- Project page is available at https://tobiasfshr.github.io/pub/flowr

---

## RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable  3D Gaussian Avatars

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Yahui Li, Zhi Zeng, Liming Pang, Guixuan Zhang, Shuwu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.01559v1){: .btn .btn-green } |

**Abstract**: Modeling animatable human avatars from monocular or multi-view videos has
been widely studied, with recent approaches leveraging neural radiance fields
(NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in
novel-view and novel-pose synthesis. However, existing methods often struggle
to accurately capture the dynamics of loose clothing, as they primarily rely on
global pose conditioning or static per-frame representations, leading to
oversmoothing and temporal inconsistencies in non-rigid regions. To address
this, We propose RealityAvatar, an efficient framework for high-fidelity
digital human modeling, specifically targeting loosely dressed avatars. Our
method leverages 3D Gaussian Splatting to capture complex clothing deformations
and motion dynamics while ensuring geometric consistency. By incorporating a
motion trend module and a latentbone encoder, we explicitly model
pose-dependent deformations and temporal variations in clothing behavior.
Extensive experiments on benchmark datasets demonstrate the effectiveness of
our approach in capturing fine-grained clothing deformations and motion-driven
shape variations. Our method significantly enhances structural fidelity and
perceptual quality in dynamic human reconstruction, particularly in non-rigid
regions, while achieving better consistency across temporal frames.



---

## UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene  Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Yunxuan Mao, Rong Xiong, Yue Wang, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2504.00763v1){: .btn .btn-green } |

**Abstract**: Reconstructing and decomposing dynamic urban scenes is crucial for autonomous
driving, urban planning, and scene editing. However, existing methods fail to
perform instance-aware decomposition without manual annotations, which is
crucial for instance-level scene editing.We propose UnIRe, a 3D Gaussian
Splatting (3DGS) based approach that decomposes a scene into a static
background and individual dynamic instances using only RGB images and LiDAR
point clouds. At its core, we introduce 4D superpoints, a novel representation
that clusters multi-frame LiDAR points in 4D space, enabling unsupervised
instance separation based on spatiotemporal correlations. These 4D superpoints
serve as the foundation for our decomposed 4D initialization, i.e., providing
spatial and temporal initialization to train a dynamic 3DGS for arbitrary
dynamic classes without requiring bounding boxes or object
templates.Furthermore, we introduce a smoothness regularization strategy in
both 2D and 3D space, further improving the temporal stability.Experiments on
benchmark datasets show that our method outperforms existing methods in
decomposed dynamic scene reconstruction while enabling accurate and flexible
instance-level editing, making it a practical solution for real-world
applications.



---

## Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Tianqi Ding, Dawei Xiang, Pablo Rivas, Liang Dong | cs.CV | [PDF](http://arxiv.org/pdf/2504.00950v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have become a popular 3D reconstruction
approach in recent years. While they produce high-quality results, they also
demand lengthy training times, often spanning days. This paper studies neural
pruning as a strategy to address these concerns. We compare pruning approaches,
including uniform sampling, importance-based methods, and coreset-based
techniques, to reduce the model size and speed up training. Our findings show
that coreset-driven pruning can achieve a 50% reduction in model size and a 35%
speedup in training, with only a slight decrease in accuracy. These results
suggest that pruning can be an effective method for improving the efficiency of
NeRF models in resource-limited settings.

Comments:
- 12 pages, 4 figures, accepted by International Conference on the AI
  Revolution: Research, Ethics, and Society (AIR-RES 2025)

---

## NeuRadar: Neural Radiance Fields for Automotive Radar Point Clouds

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Mahan Rafidashti, Ji Lan, Maryam Fatemi, Junsheng Fu, Lars Hammarstrand, Lennart Svensson | cs.CV | [PDF](http://arxiv.org/pdf/2504.00859v1){: .btn .btn-green } |

**Abstract**: Radar is an important sensor for autonomous driving (AD) systems due to its
robustness to adverse weather and different lighting conditions. Novel view
synthesis using neural radiance fields (NeRFs) has recently received
considerable attention in AD due to its potential to enable efficient testing
and validation but remains unexplored for radar point clouds. In this paper, we
present NeuRadar, a NeRF-based model that jointly generates radar point clouds,
camera images, and lidar point clouds. We explore set-based object detection
methods such as DETR, and propose an encoder-based solution grounded in the
NeRF geometry for improved generalizability. We propose both a deterministic
and a probabilistic point cloud representation to accurately model the radar
behavior, with the latter being able to capture radar's stochastic behavior. We
achieve realistic reconstruction results for two automotive datasets,
establishing a baseline for NeRF-based radar point cloud simulation models. In
addition, we release radar data for ZOD's Sequences and Drives to enable
further research in this field. To encourage further development of radar
NeRFs, we release the source code for NeuRadar.



---

## Distilling Multi-view Diffusion Models into 3D Generators

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2504.00457v3){: .btn .btn-green } |

**Abstract**: We introduce DD3G, a formulation that Distills a multi-view Diffusion model
(MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and
integrates extensive visual and spatial geometric knowledge from the MV-DM by
simulating its ordinary differential equation (ODE) trajectory, ensuring the
distilled generator generalizes better than those trained solely on 3D data.
Unlike previous amortized optimization approaches, we align the MV-DM and 3D
generator representation spaces to transfer the teacher's probabilistic flow to
the student, thus avoiding inconsistencies in optimization objectives caused by
probabilistic sampling. The introduction of probabilistic flow and the coupling
of various attributes in 3D Gaussians introduce challenges in the generation
process. To tackle this, we propose PEPD, a generator consisting of Pattern
Extraction and Progressive Decoding phases, which enables efficient fusion of
probabilistic flow and converts a single image into 3D Gaussians within 0.06
seconds. Furthermore, to reduce knowledge loss and overcome sparse-view
supervision, we design a joint optimization objective that ensures the quality
of generated samples through explicit supervision and implicit verification.
Leveraging existing 2D generation models, we compile 120k high-quality RGBA
images for distillation. Experiments on synthetic and public datasets
demonstrate the effectiveness of our method. Our project is available at:
https://qinbaigao.github.io/DD3G_project/



---

## OccludeNeRF: Geometric-aware 3D Scene Inpainting with Collaborative  Score Distillation in NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Jingyu Shi, Achleshwar Luthra, Jiazhi Li, Xiang Gao, Xiyun Song, Zongfang Lin, David Gu, Heather Yu | eess.IV | [PDF](http://arxiv.org/pdf/2504.02007v1){: .btn .btn-green } |

**Abstract**: With Neural Radiance Fields (NeRFs) arising as a powerful 3D representation,
research has investigated its various downstream tasks, including inpainting
NeRFs with 2D images. Despite successful efforts addressing the view
consistency and geometry quality, prior methods yet suffer from occlusion in
NeRF inpainting tasks, where 2D prior is severely limited in forming a faithful
reconstruction of the scene to inpaint.
  To address this, we propose a novel approach that enables cross-view
information sharing during knowledge distillation from a diffusion model,
effectively propagating occluded information across limited views.
Additionally, to align the distillation direction across multiple sampled
views, we apply a grid-based denoising strategy and incorporate additional
rendered views to enhance cross-view consistency. To assess our approach's
capability of handling occlusion cases, we construct a dataset consisting of
challenging scenes with severe occlusion, in addition to existing datasets.
Compared with baseline methods, our method demonstrates better performance in
cross-view consistency and faithfulness in reconstruction, while preserving
high rendering quality and fidelity.

Comments:
- CVPR 2025 CV4Metaverse

---

## DropGaussian: Structural Regularization for Sparse-view Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Hyunwoo Park, Gun Ryu, Wonjun Kim | cs.CV | [PDF](http://arxiv.org/pdf/2504.00773v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in
the field of novel view synthesis due to its fast performance while yielding
the excellent image quality. However, 3DGS in sparse-view settings (e.g.,
three-view inputs) often faces with the problem of overfitting to training
views, which significantly drops the visual quality of novel view images. Many
existing approaches have tackled this issue by using strong priors, such as 2D
generative contextual information and external depth signals. In contrast, this
paper introduces a prior-free method, so-called DropGaussian, with simple
changes in 3D Gaussian splatting. Specifically, we randomly remove Gaussians
during the training process in a similar way of dropout, which allows
non-excluded Gaussians to have larger gradients while improving their
visibility. This makes the remaining Gaussians to contribute more to the
optimization process for rendering with sparse input views. Such simple
operation effectively alleviates the overfitting problem and enhances the
quality of novel view synthesis. By simply applying DropGaussian to the
original 3DGS framework, we can achieve the competitive performance with
existing prior-based 3DGS methods in sparse-view settings of benchmark datasets
without any additional complexity. The code and model are publicly available
at: https://github.com/DCVL-3D/DropGaussian release.

Comments:
- Accepted by CVPR 2025

---

## Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single  Panoramic Image for Your Immerse Exploration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Zilong Huang, Jun He, Junyan Ye, Lihan Jiang, Weijia Li, Yiping Chen, Ting Han | cs.CV | [PDF](http://arxiv.org/pdf/2504.00387v1){: .btn .btn-green } |

**Abstract**: The reconstruction of immersive and realistic 3D scenes holds significant
practical importance in various fields of computer vision and computer
graphics. Typically, immersive and realistic scenes should be free from
obstructions by dynamic objects, maintain global texture consistency, and allow
for unrestricted exploration. The current mainstream methods for image-driven
scene construction involves iteratively refining the initial image using a
moving virtual camera to generate the scene. However, previous methods struggle
with visual discontinuities due to global texture inconsistencies under varying
camera poses, and they frequently exhibit scene voids caused by
foreground-background occlusions. To this end, we propose a novel layered 3D
scene reconstruction framework from panoramic image, named Scene4U.
Specifically, Scene4U integrates an open-vocabulary segmentation model with a
large language model to decompose a real panorama into multiple layers. Then,
we employs a layered repair module based on diffusion model to restore occluded
regions using visual cues and depth information, generating a hierarchical
representation of the scene. The multi-layer panorama is then initialized as a
3D Gaussian Splatting representation, followed by layered optimization, which
ultimately produces an immersive 3D scene with semantic and structural
consistency that supports free exploration. Scene4U outperforms
state-of-the-art method, improving by 24.24% in LPIPS and 24.40% in BRISQUE,
while also achieving the fastest training speed. Additionally, to demonstrate
the robustness of Scene4U and allow users to experience immersive scenes from
various landmarks, we build WorldVista3D dataset for 3D scene reconstruction,
which contains panoramic images of globally renowned sites. The implementation
code and dataset will be released at https://github.com/LongHZ140516/Scene4U .

Comments:
- CVPR 2025, 11 pages, 7 figures

---

## ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with  Multi-modal Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Qi Song, Chenghong Li, Haotong Lin, Sida Peng, Rui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2504.00437v1){: .btn .btn-green } |

**Abstract**: We present a novel approach, termed ADGaussian, for generalizable street
scene reconstruction. The proposed method enables high-quality rendering from
single-view input. Unlike prior Gaussian Splatting methods that primarily focus
on geometry refinement, we emphasize the importance of joint optimization of
image and depth features for accurate Gaussian prediction. To this end, we
first incorporate sparse LiDAR depth as an additional input modality,
formulating the Gaussian prediction process as a joint learning framework of
visual information and geometric clue. Furthermore, we propose a multi-modal
feature matching strategy coupled with a multi-scale Gaussian decoding model to
enhance the joint refinement of multi-modal features, thereby enabling
efficient multi-modal Gaussian learning. Extensive experiments on two
large-scale autonomous driving datasets, Waymo and KITTI, demonstrate that our
ADGaussian achieves state-of-the-art performance and exhibits superior
zero-shot generalization capabilities in novel-view shifting.

Comments:
- The project page can be found at
  https://maggiesong7.github.io/research/ADGaussian/

---

## Monocular and Generalizable Gaussian Talking Head Animation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Shengjie Gong, Haojie Li, Jiapeng Tang, Dongming Hu, Shuangping Huang, Hao Chen, Tianshui Chen, Zhuoman Liu | cs.CV | [PDF](http://arxiv.org/pdf/2504.00665v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce Monocular and Generalizable Gaussian Talking Head
Animation (MGGTalk), which requires monocular datasets and generalizes to
unseen identities without personalized re-training. Compared with previous 3D
Gaussian Splatting (3DGS) methods that requires elusive multi-view datasets or
tedious personalized learning/inference, MGGtalk enables more practical and
broader applications. However, in the absence of multi-view and personalized
training data, the incompleteness of geometric and appearance information poses
a significant challenge. To address these challenges, MGGTalk explores depth
information to enhance geometric and facial symmetry characteristics to
supplement both geometric and appearance features. Initially, based on the
pixel-wise geometric information obtained from depth estimation, we incorporate
symmetry operations and point cloud filtering techniques to ensure a complete
and precise position parameter for 3DGS. Subsequently, we adopt a two-stage
strategy with symmetric priors for predicting the remaining 3DGS parameters. We
begin by predicting Gaussian parameters for the visible facial regions of the
source image. These parameters are subsequently utilized to improve the
prediction of Gaussian parameters for the non-visible regions. Extensive
experiments demonstrate that MGGTalk surpasses previous state-of-the-art
methods, achieving superior performance across various metrics.

Comments:
- Accepted by CVPR 2025

---

## Robust LiDAR-Camera Calibration with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Shuyi Zhou, Shuxiang Xie, Ryoichi Ishikawa, Takeshi Oishi | cs.RO | [PDF](http://arxiv.org/pdf/2504.00525v1){: .btn .btn-green } |

**Abstract**: LiDAR-camera systems have become increasingly popular in robotics recently. A
critical and initial step in integrating the LiDAR and camera data is the
calibration of the LiDAR-camera system. Most existing calibration methods rely
on auxiliary target objects, which often involve complex manual operations,
whereas targetless methods have yet to achieve practical effectiveness.
Recognizing that 2D Gaussian Splatting (2DGS) can reconstruct geometric
information from camera image sequences, we propose a calibration method that
estimates LiDAR-camera extrinsic parameters using geometric constraints. The
proposed method begins by reconstructing colorless 2DGS using LiDAR point
clouds. Subsequently, we update the colors of the Gaussian splats by minimizing
the photometric loss. The extrinsic parameters are optimized during this
process. Additionally, we address the limitations of the photometric loss by
incorporating the reprojection and triangulation losses, thereby enhancing the
calibration robustness and accuracy.

Comments:
- Accepted in IEEE Robotics and Automation Letters. Code available at:
  https://github.com/ShuyiZhou495/RobustCalibration
