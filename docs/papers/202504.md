---
layout: default
title: April 2025
parent: Papers
nav_order: 202504
---

<!---metadata--->


## HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-30 | Haiyang Zhou, Wangbo Yu, Jiawen Guan, Xinhua Cheng, Yonghong Tian, Li Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2504.21650v1){: .btn .btn-green } |

**Abstract**: The rapid advancement of diffusion models holds the promise of
revolutionizing the application of VR and AR technologies, which typically
require scene-level 4D assets for user experience. Nonetheless, existing
diffusion models predominantly concentrate on modeling static 3D scenes or
object-level dynamics, constraining their capacity to provide truly immersive
experiences. To address this issue, we propose HoloTime, a framework that
integrates video diffusion models to generate panoramic videos from a single
prompt or reference image, along with a 360-degree 4D scene reconstruction
method that seamlessly transforms the generated panoramic video into 4D assets,
enabling a fully immersive 4D experience for users. Specifically, to tame video
diffusion models for generating high-fidelity panoramic videos, we introduce
the 360World dataset, the first comprehensive collection of panoramic videos
suitable for downstream 4D scene reconstruction tasks. With this curated
dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion
model that can convert panoramic images into high-quality panoramic videos.
Following this, we present Panoramic Space-Time Reconstruction, which leverages
a space-time depth estimation method to transform the generated panoramic
videos into 4D point clouds, enabling the optimization of a holistic 4D
Gaussian Splatting representation to reconstruct spatially and temporally
consistent 4D scenes. To validate the efficacy of our method, we conducted a
comparative analysis with existing approaches, revealing its superiority in
both panoramic video generation and 4D scene reconstruction. This demonstrates
our method's capability to create more engaging and realistic immersive
environments, thereby enhancing user experiences in VR and AR applications.

Comments:
- Project homepage: https://zhouhyocean.github.io/holotime/

---

## EfficientHuman: Efficient Training and Reconstruction of Moving Human  using Articulated 2D Gaussian

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-29 | Hao Tian, Rui Liu, Wen Shen, Yilong Hu, Zhihao Zheng, Xiaolin Qin | cs.CV | [PDF](http://arxiv.org/pdf/2504.20607v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in
scene reconstruction and novel view synthesis. Recent work on reconstructing
the 3D human body using 3DGS attempts to leverage prior information on human
pose to enhance rendering quality and improve training speed. However, it
struggles to effectively fit dynamic surface planes due to multi-view
inconsistency and redundant Gaussians. This inconsistency arises because
Gaussian ellipsoids cannot accurately represent the surfaces of dynamic
objects, which hinders the rapid reconstruction of the dynamic human body.
Meanwhile, the prevalence of redundant Gaussians means that the training time
of these works is still not ideal for quickly fitting a dynamic human body. To
address these, we propose EfficientHuman, a model that quickly accomplishes the
dynamic reconstruction of the human body using Articulated 2D Gaussian while
ensuring high rendering quality. The key innovation involves encoding Gaussian
splats as Articulated 2D Gaussian surfels in canonical space and then
transforming them to pose space via Linear Blend Skinning (LBS) to achieve
efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian
surfels can quickly conform to the dynamic human body while ensuring
view-consistent geometries. Additionally, we introduce a pose calibration
module and an LBS optimization module to achieve precise fitting of dynamic
human poses, enhancing the model's performance. Extensive experiments on the
ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic
human reconstruction in less than a minute on average, which is 20 seconds
faster than the current state-of-the-art method, while also reducing the number
of redundant Gaussians.

Comments:
- 11 pages, 3 figures

---

## Creating Your Editable 3D Photorealistic Avatar with  Tetrahedron-constrained Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-29 | Hanxi Liu, Yifang Men, Zhouhui Lian | cs.GR | [PDF](http://arxiv.org/pdf/2504.20403v1){: .btn .btn-green } |

**Abstract**: Personalized 3D avatar editing holds significant promise due to its
user-friendliness and availability to applications such as AR/VR and virtual
try-ons. Previous studies have explored the feasibility of 3D editing, but
often struggle to generate visually pleasing results, possibly due to the
unstable representation learning under mixed optimization of geometry and
texture in complicated reconstructed scenarios. In this paper, we aim to
provide an accessible solution for ordinary users to create their editable 3D
avatars with precise region localization, geometric adaptability, and
photorealistic renderings. To tackle this challenge, we introduce a
meticulously designed framework that decouples the editing process into local
spatial adaptation and realistic appearance learning, utilizing a hybrid
Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying
representation. TetGS combines the controllable explicit structure of
tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian
Splatting and is optimized in a progressive manner comprising three stages: 3D
avatar instantiation from real-world monocular videos to provide accurate
priors for TetGS initialization; localized spatial adaptation with explicitly
partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and
geometry-based appearance generation with a coarse-to-fine activation strategy.
Both qualitative and quantitative experiments demonstrate the effectiveness and
superiority of our approach in generating photorealistic 3D editable avatars.



---

## GSFeatLoc: Visual Localization Using Feature Correspondence on 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-29 | Jongwon Lee, Timothy Bretl | cs.CV | [PDF](http://arxiv.org/pdf/2504.20379v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a method for localizing a query image with respect
to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the
method uses 3DGS to render a synthetic RGBD image at some initial pose
estimate. Second, it establishes 2D-2D correspondences between the query image
and this synthetic image. Third, it uses the depth map to lift the 2D-2D
correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP)
problem to produce a final pose estimate. Results from evaluation across three
existing datasets with 38 scenes and over 2,700 test images show that our
method significantly reduces both inference time (by over two orders of
magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation
error compared to baseline methods that use photometric loss minimization.
Results also show that our method tolerates large errors in the initial pose
estimate of up to 55{\deg} in rotation and 1.1 units in translation (normalized
by scene scale), achieving final pose errors of less than 5{\deg} in rotation
and 0.05 units in translation on 90% of images from the Synthetic NeRF and
Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and
Temples dataset.



---

## Large-scale visual SLAM for in-the-wild videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-29 | Shuo Sun, Torsten Sattler, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson | cs.CV | [PDF](http://arxiv.org/pdf/2504.20496v1){: .btn .btn-green } |

**Abstract**: Accurate and robust 3D scene reconstruction from casual, in-the-wild videos
can significantly simplify robot deployment to new environments. However,
reliable camera pose estimation and scene reconstruction from such
unconstrained videos remains an open challenge. Existing visual-only SLAM
methods perform well on benchmark datasets but struggle with real-world footage
which often exhibits uncontrolled motion including rapid rotations and pure
forward movements, textureless regions, and dynamic objects. We analyze the
limitations of current methods and introduce a robust pipeline designed to
improve 3D reconstruction from casual videos. We build upon recent deep visual
odometry methods but increase robustness in several ways. Camera intrinsics are
automatically recovered from the first few frames using structure-from-motion.
Dynamic objects and less-constrained areas are masked with a predictive model.
Additionally, we leverage monocular depth estimates to regularize bundle
adjustment, mitigating errors in low-parallax situations. Finally, we integrate
place recognition and loop closure to reduce long-term drift and refine both
intrinsics and pose estimates through global bundle adjustment. We demonstrate
large-scale contiguous 3D models from several online videos in various
environments. In contrast, baseline methods typically produce locally
inconsistent results at several points, producing separate segments or
distorted maps. In lieu of ground-truth pose data, we evaluate map consistency,
execution time and visual accuracy of re-rendered NeRF models. Our proposed
system establishes a new baseline for visual reconstruction from casual
uncontrolled videos found online, demonstrating more consistent reconstructions
over longer sequences of in-the-wild videos than previously achieved.

Comments:
- fix the overview figure

---

## GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-29 | Yuhan Xie, Yixi Cai, Yinqiang Zhang, Lei Yang, Jia Pan | cs.GR | [PDF](http://arxiv.org/pdf/2504.21067v1){: .btn .btn-green } |

**Abstract**: This research tackles the challenge of real-time active view selection and
uncertainty quantification on visual quality for active 3D reconstruction.
Visual quality is a critical aspect of 3D reconstruction. Recent advancements
such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have
notably enhanced the image rendering quality of reconstruction models.
Nonetheless, the efficient and effective acquisition of input images for
reconstruction-specifically, the selection of the most informative
viewpoint-remains an open challenge, which is crucial for active
reconstruction. Existing studies have primarily focused on evaluating geometric
completeness and exploring unobserved or unknown regions, without direct
evaluation of the visual uncertainty within the reconstruction model. To
address this gap, this paper introduces a probabilistic model that quantifies
visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we
formulate a criterion, Gaussian Splatting Shannon Mutual Information
(GauSS-MI), for real-time assessment of visual mutual information from novel
viewpoints, facilitating the selection of next best view. GauSS-MI is
implemented within an active reconstruction system integrated with a view and
motion planner. Extensive experiments across various simulated and real-world
scenes showcase the superior visual quality and reconstruction efficiency
performance of the proposed system.



---

## Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface  Reconstruction from Sparse Views

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-29 | Jiang Wu, Rui Li, Yu Zhu, Rong Guo, Jinqiu Sun, Yanning Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.20378v1){: .btn .btn-green } |

**Abstract**: We present a Gaussian Splatting method for surface reconstruction using
sparse input views. Previous methods relying on dense views struggle with
extremely sparse Structure-from-Motion points for initialization. While
learning-based Multi-view Stereo (MVS) provides dense 3D points, directly
combining it with Gaussian Splatting leads to suboptimal results due to the
ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS,
an MVS-initialized Gaussian Splatting pipeline for complete and accurate
reconstruction. Our key insight is to incorporate the geometric-prioritized
enhancement schemes, allowing for direct and robust geometric learning under
ill-posed conditions. Sparse2DGS outperforms existing methods by notable
margins while being ${2}\times$ faster than the NeRF-based fine-tuning
approach.

Comments:
- CVPR 2025

---

## GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for  Targeted Scene Confusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-29 | Jiaxin Hong, Sixu Chen, Shuoyang Sun, Hongyao Yu, Hao Fang, Yuqi Tan, Bin Chen, Shuhan Qi, Jiawei Li | cs.CV | [PDF](http://arxiv.org/pdf/2504.20829v1){: .btn .btn-green } |

**Abstract**: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene
representation and novel view synthesis, its rapid adoption in safety-critical
domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of
potential security vulnerabilities. This paper presents the first systematic
study of backdoor threats in 3DGS pipelines. We identify that adversaries may
implant backdoor views to induce malicious scene confusion during inference,
potentially leading to environmental misperception in autonomous navigation or
spatial distortion in immersive environments. To uncover this risk, we propose
GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap
injects malicious views at specific attack viewpoints while preserving
high-quality rendering in non-target views, ensuring minimal detectability and
maximizing potential harm. Specifically, the proposed method consists of a
three-stage pipeline (attack, stabilization, and normal training) to implant
stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing
attack efficacy and perceptual realism to expose security risks in 3D
rendering. Extensive experiments on both synthetic and real-world datasets
demonstrate that GuassTrap can effectively embed imperceptible yet harmful
backdoor views while maintaining high-quality rendering in normal views,
validating its robustness, adaptability, and practical applicability.



---

## Mesh-Learner: Texturing Mesh with Spherical Harmonics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-28 | Yunfei Wan, Jianheng Liu, Jiarong Lin, Fu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.19938v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a 3D reconstruction and rendering framework termed
Mesh-Learner that is natively compatible with traditional rasterization
pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e.,
texture filled with SH coefficients) into the learning process to learn each
mesh s view-dependent radiance end-to-end. Images are rendered by interpolating
surrounding SH Texels at each pixel s sampling point using a novel
interpolation method. Conversely, gradients from each pixel are back-propagated
to the related SH Texels in SH textures. Mesh-Learner exploits graphic features
of rasterization pipeline (texture sampling, deferred rendering) to render,
which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and
tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for
robotics) that are based on rasterization pipelines. Our system can train vast,
unlimited scenes because we transfer only the SH textures within the frustum to
the GPU for training. At other times, the SH textures are stored in CPU RAM,
which results in moderate GPU memory usage. The rendering results on
interpolation and extrapolation sequences in the Replica and FAST-LIVO2
datasets achieve state-of-the-art performance compared to existing
state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To
benefit the society, the code will be available at
https://github.com/hku-mars/Mesh-Learner.



---

## Joint Optimization of Neural Radiance Fields and Continuous Camera  Motion from a Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-28 | Hoang Chuong Nguyen, Wei Mao, Jose M. Alvarez, Miaomiao Liu | cs.CV | [PDF](http://arxiv.org/pdf/2504.19819v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) has demonstrated its superior capability to
represent 3D geometry but require accurately precomputed camera poses during
training. To mitigate this requirement, existing methods jointly optimize
camera poses and NeRF often relying on good pose initialisation or depth
priors. However, these approaches struggle in challenging scenarios, such as
large rotations, as they map each camera to a world coordinate system. We
propose a novel method that eliminates prior dependencies by modeling
continuous camera motions as time-dependent angular velocity and velocity.
Relative motions between cameras are learned first via velocity integration,
while camera poses can be obtained by aggregating such relative motions up to a
world coordinate system defined at a single time step within the video.
Specifically, accurate continuous camera movements are learned through a
time-dependent NeRF, which captures local scene geometry and motion by training
from neighboring frames for each time step. The learned motions enable
fine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D
and Scannet show our approach achieves superior camera pose and depth
estimation and comparable novel-view synthesis performance compared to
state-of-the-art methods. Our code is available at
https://github.com/HoangChuongNguyen/cope-nerf.



---

## CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel  View Synthesis in Autonomous Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-28 | Mohammad Altillawi, Fengyi Shen, Liudi Yang, Sai Manoj Prakhya, Ziyuan Liu | cs.CV | [PDF](http://arxiv.org/pdf/2504.19557v1){: .btn .btn-green } |

**Abstract**: Current point-based approaches encounter limitations in scalability and
rendering quality when using large 3D point cloud maps because using them
directly for novel view synthesis (NVS) leads to degraded visualizations. We
identify the primary issue behind these low-quality renderings as a visibility
mismatch between geometry and appearance, stemming from using these two
modalities together. To address this problem, we present CE-NPBG, a new
approach for novel view synthesis (NVS) in large-scale autonomous driving
scenes. Our method is a neural point-based technique that leverages two
modalities: posed images (cameras) and synchronized raw 3D point clouds
(LiDAR). We first employ a connectivity relationship graph between appearance
and geometry, which retrieves points from a large 3D point cloud map observed
from the current camera perspective and uses them for rendering. By leveraging
this connectivity, our method significantly improves rendering quality and
enhances run-time and scalability by using only a small subset of points from
the large 3D point cloud map. Our approach associates neural descriptors with
the points and uses them to synthesize views. To enhance the encoding of these
descriptors and elevate rendering quality, we propose a joint adversarial and
point rasterization training. During training, we pair an image-synthesizer
network with a multi-resolution discriminator. At inference, we decouple them
and use the image-synthesizer to generate novel views. We also integrate our
proposal into the recent 3D Gaussian Splatting work to highlight its benefits
for improved rendering and scalability.

Comments:
- Accepted in 2025 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)

---

## GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-28 | Zuxing Lu, Xin Yuan, Shaowen Yang, Jingyu Liu, Jiawei Wang, Changyin Sun | cs.RO | [PDF](http://arxiv.org/pdf/2504.19409v1){: .btn .btn-green } |

**Abstract**: Semantic-aware 3D scene reconstruction is essential for autonomous robots to
perform complex interactions. Semantic SLAM, an online approach, integrates
pose tracking, geometric reconstruction, and semantic mapping into a unified
framework, shows significant potential. However, existing systems, which rely
on 2D ground truth priors for supervision, are often limited by the sparsity
and noise of these signals in real-world environments. To address this
challenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D
Gaussian Splatting that leverages feature fields to achieve joint rendering of
appearance, geometry, and N-dimensional semantic features. By independently
optimizing feature gradients, our method supports semantic reconstruction using
various forms of 2D priors, particularly sparse and noisy signals. Experimental
results demonstrate that our approach outperforms previous methods in both
tracking accuracy and photorealistic rendering quality. When utilizing 2D
ground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation
performance with 95.03\% mIoU, while achieving up to 2.9$\times$ speedup with
only marginal performance degradation.



---

## Beyond Physical Reach: Comparing Head- and Cane-Mounted Cameras for  Last-Mile Navigation by Blind Users

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-27 | Apurv Varshney, Lucas Nadolskis, Tobias Höllerer, Michael Beyeler | cs.HC | [PDF](http://arxiv.org/pdf/2504.19345v1){: .btn .btn-green } |

**Abstract**: Blind individuals face persistent challenges in last-mile navigation,
including locating entrances, identifying obstacles, and navigating complex or
cluttered spaces. Although wearable cameras are increasingly used in assistive
systems, there has been no systematic, vantage-focused comparison to guide
their design. This paper addresses that gap through a two-part investigation.
First, we surveyed ten experienced blind cane users, uncovering navigation
strategies, pain points, and technology preferences. Participants stressed the
importance of multi-sensory integration, destination-focused travel, and
assistive tools that complement (rather than replace) the cane's tactile
utility. Second, we conducted controlled data collection with a blind
participant navigating five real-world environments using synchronized head-
and cane-mounted cameras, isolating vantage placement as the primary variable.
To assess how each vantage supports spatial perception, we evaluated SLAM
performance (for localization and mapping) and NeRF-based 3D reconstruction
(for downstream scene understanding). Head-mounted sensors delivered superior
localization accuracy, while cane-mounted views offered broader ground-level
coverage and richer environmental reconstructions. A combined (head+cane)
configuration consistently outperformed both. These results highlight the
complementary strengths of different sensor placements and offer actionable
guidance for developing hybrid navigation aids that are perceptive, robust, and
user-aligned.



---

## Rendering Anywhere You See: Renderability Field-guided Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-27 | Xiaofeng Jin, Yan Fang, Matteo Frosi, Jianfei Ge, Jiangjian Xiao, Matteo Matteucci | cs.CV | [PDF](http://arxiv.org/pdf/2504.19261v1){: .btn .btn-green } |

**Abstract**: Scene view synthesis, which generates novel views from limited perspectives,
is increasingly vital for applications like virtual reality, augmented reality,
and robotics. Unlike object-based tasks, such as generating 360{\deg} views of
a car, scene view synthesis handles entire environments where non-uniform
observations pose unique challenges for stable rendering quality. To address
this issue, we propose a novel approach: renderability field-guided gaussian
splatting (RF-GS). This method quantifies input inhomogeneity through a
renderability field, guiding pseudo-view sampling to enhanced visual
consistency. To ensure the quality of wide-baseline pseudo-views, we train an
image restoration model to map point projections to visible-light styles.
Additionally, our validated hybrid data optimization strategy effectively fuses
information of pseudo-view angles and source view textures. Comparative
experiments on simulated and real-world data show that our method outperforms
existing approaches in rendering stability.

Comments:
- 8 pages,8 figures

---

## IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic  Talking Heads from Monocular Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-27 | Yuan Li, Ziqian Bai, Feitong Tan, Zhaopeng Cui, Sean Fanello, Yinda Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.19165v2){: .btn .btn-green } |

**Abstract**: We propose a novel 3D-aware diffusion-based method for generating
photorealistic talking head videos directly from a single identity image and
explicit control signals (e.g., expressions). Our method generates Multiplane
Images (MPIs) that ensure geometric consistency, making them ideal for
immersive viewing experiences like binocular videos for VR headsets. Unlike
existing methods that often require a separate stage or joint optimization to
reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach
directly generates the final output through a single denoising process,
eliminating the need for post-processing steps to render novel views
efficiently. To effectively learn from monocular videos, we introduce a
training mechanism that reconstructs the output MPI randomly in either the
target or the reference camera space. This approach enables the model to
simultaneously learn sharp image details and underlying 3D information.
Extensive experiments demonstrate the effectiveness of our method, which
achieves competitive avatar quality and novel-view rendering capabilities, even
without explicit 3D reconstruction or high-quality multi-view training data.

Comments:
- CVPR2025; project page:
  https://y-u-a-n-l-i.github.io/projects/IM-Portrait/

---

## TransparentGS: Fast Inverse Rendering of Transparent Objects with  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-26 | Letian Huang, Dongwei Ye, Jialin Dan, Chengzhi Tao, Huiwen Liu, Kun Zhou, Bo Ren, Yuanqi Li, Yanwen Guo, Jie Guo | cs.GR | [PDF](http://arxiv.org/pdf/2504.18768v1){: .btn .btn-green } |

**Abstract**: The emergence of neural and Gaussian-based radiance field methods has led to
considerable advancements in novel view synthesis and 3D object reconstruction.
Nonetheless, specular reflection and refraction continue to pose significant
challenges due to the instability and incorrect overfitting of radiance fields
to high-frequency light variations. Currently, even 3D Gaussian Splatting
(3D-GS), as a powerful and efficient tool, falls short in recovering
transparent objects with nearby contents due to the existence of apparent
secondary ray effects. To address this issue, we propose TransparentGS, a fast
inverse rendering pipeline for transparent objects based on 3D-GS. The main
contributions are three-fold. Firstly, an efficient representation of
transparent objects, transparent Gaussian primitives, is designed to enable
specular refraction through a deferred refraction strategy. Secondly, we
leverage Gaussian light field probes (GaussProbe) to encode both ambient light
and nearby contents in a unified framework. Thirdly, a depth-based iterative
probes query (IterQuery) algorithm is proposed to reduce the parallax errors in
our probe-based framework. Experiments demonstrate the speed and accuracy of
our approach in recovering transparent objects from complex environments, as
well as several applications in computer graphics and vision.

Comments:
- accepted by SIGGRAPH 2025;
  https://letianhuang.github.io/transparentgs/

---

## 4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data  Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-26 | Zicong Chen, Zhenghao Chen, Wei Jiang, Wei Wang, Lei Liu, Dong Xu | cs.CE | [PDF](http://arxiv.org/pdf/2504.18925v2){: .btn .btn-green } |

**Abstract**: Storage is a significant challenge in reconstructing dynamic scenes with 4D
Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a
contextual coding framework that compresses 4DGS data to meet specific storage
constraints. Building upon the established deformable 3D Gaussian Splatting
(3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a
canonical 3DGS component, which are then compressed using Neural Voxel
Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC),
respectively. Specifically, we first decompose the 4D neural voxels into
distinct quantized features by separating the temporal and spatial dimensions.
To losslessly compress each quantized feature, we leverage the previously
compressed features from the temporal and spatial dimensions as priors and
apply NVCC to generate the spatiotemporal context for contextual coding. Next,
we employ a codebook to store spherical harmonics information from canonical
3DGS as quantized vectors, which are then losslessly compressed by using VQCC
with the auxiliary learned hyperpriors for contextual coding, thereby reducing
redundancy within the codebook. By integrating NVCC and VQCC, our contextual
coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to
specific storage requirements. Extensive experiments on three 4DGS data
compression benchmarks demonstrate that our method achieves an average storage
reduction of approximately 12 times while maintaining rendering fidelity
compared to our baseline 4DGS approach.



---

## RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny  Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-25 | Georgios Kouros, Minye Wu, Tinne Tuytelaars | cs.CV | [PDF](http://arxiv.org/pdf/2504.18468v2){: .btn .btn-green } |

**Abstract**: We introduce RGS-DR, a novel inverse rendering method for reconstructing and
rendering glossy and reflective objects with support for flexible relighting
and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian
Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D
Gaussian surfel representation to accurately estimate geometry and surface
normals, an essential property for high-quality inverse rendering. Our approach
explicitly models geometric and material properties through learnable
primitives rasterized into a deferred shading pipeline, effectively reducing
rendering artifacts and preserving sharp reflections. By employing a
multi-level cube mipmap, RGS-DR accurately approximates environment lighting
integrals, facilitating high-quality reconstruction and relighting. A residual
pass with spherical-mipmap-based directional encoding further refines the
appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality
reconstruction and rendering quality for shiny objects, often outperforming
reconstruction-exclusive state-of-the-art methods incapable of relighting.



---

## PerfCam: Digital Twinning for Production Lines Using 3D Gaussian  Splatting and Vision Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-25 | Michel Gokan Khan, Renan Guarese, Fabian Johnson, Xi Vincent Wang, Anders Bergman, Benjamin Edvinsson, Mario Romero, Jérémy Vachier, Jan Kronqvist | cs.CV | [PDF](http://arxiv.org/pdf/2504.18165v1){: .btn .btn-green } |

**Abstract**: We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning
framework that combines camera and sensory data with 3D Gaussian Splatting and
computer vision models for digital twinning, object tracking, and Key
Performance Indicators (KPIs) extraction in industrial production lines. By
utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam
offers a semi-automated approach to object tracking and spatial mapping,
enabling digital twins that capture real-time KPIs such as availability,
performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts
in the production line. We validate the effectiveness of PerfCam through a
practical deployment within realistic test production lines in the
pharmaceutical industry and contribute an openly published dataset to support
further research and development in the field. The results demonstrate
PerfCam's ability to deliver actionable insights through its precise digital
twin capabilities, underscoring its value as an effective tool for developing
usable digital twins in smart manufacturing environments and extracting
operational analytics.



---

## STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-25 | Yunze Deng, Haijun Xiong, Bin Feng, Xinggang Wang, Wenyu Liu | cs.CV | [PDF](http://arxiv.org/pdf/2504.18318v1){: .btn .btn-green } |

**Abstract**: Text-to-4D generation is rapidly developing and widely applied in various
scenarios. However, existing methods often fail to incorporate adequate
spatio-temporal modeling and prompt alignment within a unified framework,
resulting in temporal inconsistencies, geometric distortions, or low-quality 4D
content that deviates from the provided texts. Therefore, we propose STP4D, a
novel approach that aims to integrate comprehensive spatio-temporal-prompt
consistency modeling for high-quality text-to-4D generation. Specifically,
STP4D employs three carefully designed modules: Time-varying Prompt Embedding,
Geometric Information Enhancement, and Temporal Extension Deformation, which
collaborate to accomplish this goal. Furthermore, STP4D is among the first
methods to exploit the Diffusion model to generate 4D Gaussians, combining the
fine-grained modeling capabilities and the real-time rendering process of 4DGS
with the rapid inference speed of the Diffusion model. Extensive experiments
demonstrate that STP4D excels in generating high-fidelity 4D content with
exceptional efficiency (approximately 4.6s per asset), surpassing existing
methods in both quality and speed.



---

## iVR-GS: Inverse Volume Rendering for Explorable Visualization via  Editable 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-24 | Kaiyuan Tang, Siyuan Yao, Chaoli Wang | cs.GR | [PDF](http://arxiv.org/pdf/2504.17954v1){: .btn .btn-green } |

**Abstract**: In volume visualization, users can interactively explore the
three-dimensional data by specifying color and opacity mappings in the transfer
function (TF) or adjusting lighting parameters, facilitating meaningful
interpretation of the underlying structure. However, rendering large-scale
volumes demands powerful GPUs and high-speed memory access for real-time
performance. While existing novel view synthesis (NVS) methods offer faster
rendering speeds with lower hardware requirements, the visible parts of a
reconstructed scene are fixed and constrained by preset TF settings,
significantly limiting user exploration. This paper introduces inverse volume
rendering via Gaussian splatting (iVR-GS), an innovative NVS method that
reduces the rendering cost while enabling scene editing for interactive volume
exploration. Specifically, we compose multiple iVR-GS models associated with
basic TFs covering disjoint visible parts to make the entire volumetric scene
visible. Each basic model contains a collection of 3D editable Gaussians, where
each Gaussian is a 3D spatial point that supports real-time scene rendering and
editing. We demonstrate the superior reconstruction quality and composability
of iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on
various volume datasets. The code is available at
https://github.com/TouKaienn/iVR-GS.

Comments:
- Accepted by IEEE Transactions on Visualization and Computer Graphics
  (TVCG)

---

## CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from  Casually Captured Videos

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-24 | Shucheng Gong, Lingzhe Zhao, Wenpu Li, Hong Xie, Yin Zhang, Shiyu Zhao, Peidong Liu | cs.GR | [PDF](http://arxiv.org/pdf/2504.17728v1){: .btn .btn-green } |

**Abstract**: Recently, photo-realistic novel view synthesis from multi-view images, such
as neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered
widespread attention due to their superior performance. However, most works
rely on low dynamic range (LDR) images, which limits the capturing of richer
scene details. Some prior works have focused on high dynamic range (HDR) scene
reconstruction, typically require capturing of multi-view sharp images with
different exposure times at fixed camera positions during exposure times, which
is time-consuming and challenging in practice. For a more flexible data
acquisition, we propose a one-stage method: \textbf{CasualHDRSplat} to easily
and robustly reconstruct the 3D HDR scene from casually captured videos with
auto-exposure enabled, even in the presence of severe motion blur and varying
unknown exposure time. \textbf{CasualHDRSplat} contains a unified
differentiable physical imaging model which first applies continuous-time
trajectory constraint to imaging process so that we can jointly optimize
exposure time, camera response function (CRF), camera poses, and sharp 3D HDR
scene. Extensive experiments demonstrate that our approach outperforms existing
methods in terms of robustness and rendering quality. Our source code will be
available at https://github.com/WU-CVGL/CasualHDRSplat

Comments:
- Source Code: https://github.com/WU-CVGL/CasualHDRSplat

---

## PIN-WM: Learning Physics-INformed World Models for Non-Prehensile  Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-23 | Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu | cs.LG | [PDF](http://arxiv.org/pdf/2504.16693v1){: .btn .btn-green } |

**Abstract**: While non-prehensile manipulation (e.g., controlled pushing/poking)
constitutes a foundational robotic skill, its learning remains challenging due
to the high sensitivity to complex physical interactions involving friction and
restitution. To achieve robust policy learning and generalization, we opt to
learn a world model of the 3D rigid body dynamics involved in non-prehensile
manipulations and use it for model-based reinforcement learning. We propose
PIN-WM, a Physics-INformed World Model that enables efficient end-to-end
identification of a 3D rigid body dynamical system from visual observations.
Adopting differentiable physics simulation, PIN-WM can be learned with only
few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM
is learned with observational loss induced by Gaussian Splatting without
needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM
into a group of Digital Cousins via physics-aware randomizations which perturb
physics and rendering parameters to generate diverse and meaningful variations
of the PIN-WM. Extensive evaluations on both simulation and real-world tests
demonstrate that PIN-WM, enhanced with physics-aware digital cousins,
facilitates learning robust non-prehensile manipulation skills with Sim2Real
transfer, surpassing the Real2Sim2Real state-of-the-arts.



---

## ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and  Multi-Frame Integration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-23 | Andrea Conti, Matteo Poggi, Valerio Cambareri, Martin R. Oswald, Stefano Mattoccia | cs.CV | [PDF](http://arxiv.org/pdf/2504.16545v1){: .btn .btn-green } |

**Abstract**: Time-of-Flight (ToF) sensors provide efficient active depth sensing at
relatively low power budgets; among such designs, only very sparse measurements
from low-resolution sensors are considered to meet the increasingly limited
power constraints of mobile and AR/VR devices. However, such extreme sparsity
levels limit the seamless usage of ToF depth in SLAM. In this work, we propose
ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for
using effectively very sparse ToF input data. Our approach improves upon the
state of the art by introducing a multi-frame integration module, which
produces dense depth maps by merging cues from extremely sparse ToF depth,
monocular color, and multi-view geometry. Extensive experiments on both
synthetic and real sparse ToF datasets demonstrate the viability of our
approach, as it achieves state-of-the-art tracking and mapping performances on
reference datasets.



---

## Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene  Conceptional Learning

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-23 | Mingxuan Cui, Qing Guo, Yuyi Wang, Hongkai Yu, Di Lin, Qin Zou, Ming-Ming Cheng, Xi Li | cs.CV | [PDF](http://arxiv.org/pdf/2504.17815v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D
representation for novel view synthesis. This paper extends 3DGS capabilities
to inpainting, where masked objects in a scene are replaced with new contents
that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D
Gaussian inpainting (3DGI) is challenging in effectively leveraging
complementary visual and semantic cues from multiple input views, as occluded
areas in one view may be visible in others. To address this, we propose a
method that measures the visibility uncertainties of 3D points across different
input views and uses them to guide 3DGI in utilizing complementary visual cues.
We also employ uncertainties to learn a semantic concept of scene without the
masked object and use a diffusion model to fill masked objects in input images
based on the learned concept. Finally, we build a novel 3DGI framework, VISTA,
by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl
learning. VISTA generates high-quality 3DGS models capable of synthesizing
artifact-free and naturally inpainted novel views. Furthermore, our approach
extends to handling dynamic distractors arising from temporal object changes,
enhancing its versatility in diverse scene reconstruction scenarios. We
demonstrate the superior performance of our method over state-of-the-art
techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10
diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset
derived from UTB180, including fast-moving fish as inpainting targets.

Comments:
- 14 pages, 12 figures, ICCV

---

## SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-23 | Yuanjian Wang, Yufei Deng, Rong Xiao, Jiahao Fan, Chenwei Tang, Deng Xiong, Jiancheng Lv | cs.CV | [PDF](http://arxiv.org/pdf/2504.16389v1){: .btn .btn-green } |

**Abstract**: Event cameras are neuromorphic vision sensors that asynchronously capture
changes in logarithmic brightness changes, offering significant advantages such
as low latency, low power consumption, low bandwidth, and high dynamic range.
While these characteristics make them ideal for high-speed scenarios,
reconstructing geometrically consistent and photometrically accurate 3D
representations from event data remains fundamentally challenging. Current
event-based Neural Radiance Fields (NeRF) methods partially address these
challenges but suffer from persistent artifacts caused by aggressive network
learning in early stages and the inherent noise of event cameras. To overcome
these limitations, we present SaENeRF, a novel self-supervised framework that
effectively suppresses artifacts and enables 3D-consistent, dense, and
photorealistic NeRF reconstruction of static scenes solely from event streams.
Our approach normalizes predicted radiance variations based on accumulated
event polarities, facilitating progressive and rapid learning for scene
representation construction. Additionally, we introduce regularization losses
specifically designed to suppress artifacts in regions where photometric
changes fall below the event threshold and simultaneously enhance the light
intensity difference of non-zero events, thereby improving the visual fidelity
of the reconstructed scene. Extensive qualitative and quantitative experiments
demonstrate that our method significantly reduces artifacts and achieves
superior reconstruction quality compared to existing methods. The code is
available at https://github.com/Mr-firework/SaENeRF.

Comments:
- Accepted by IJCNN 2025

---

## HUG: Hierarchical Urban Gaussian Splatting with Block-Based  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-23 | Zhongtao Wang, Mai Su, Huishan Au, Yilong Li, Xizhe Cao, Chengwei Pan, Yisong Chen, Guoping Wang | cs.GR | [PDF](http://arxiv.org/pdf/2504.16606v1){: .btn .btn-green } |

**Abstract**: As urban 3D scenes become increasingly complex and the demand for
high-quality rendering grows, efficient scene reconstruction and rendering
techniques become crucial. We present HUG, a novel approach to address
inefficiencies in handling large-scale urban environments and intricate details
based on 3D Gaussian splatting. Our method optimizes data partitioning and the
reconstruction pipeline by incorporating a hierarchical neural Gaussian
representation. We employ an enhanced block-based reconstruction pipeline
focusing on improving reconstruction quality within each block and reducing the
need for redundant training regions around block boundaries. By integrating
neural Gaussian representation with a hierarchical architecture, we achieve
high-quality scene rendering at a low computational cost. This is demonstrated
by our state-of-the-art results on public benchmarks, which prove the
effectiveness and advantages in large-scale urban scene representation.



---

## Dual-Camera All-in-Focus Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-23 | Xianrui Luo, Zijin Wu, Juewen Peng, Huiqiang Sun, Zhiguo Cao, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2504.16636v1){: .btn .btn-green } |

**Abstract**: We present the first framework capable of synthesizing the all-in-focus
neural radiance field (NeRF) from inputs without manual refocusing. Without
refocusing, the camera will automatically focus on the fixed object for all
views, and current NeRF methods typically using one camera fail due to the
consistent defocus blur and a lack of sharp reference. To restore the
all-in-focus NeRF, we introduce the dual-camera from smartphones, where the
ultra-wide camera has a wider depth-of-field (DoF) and the main camera
possesses a higher resolution. The dual camera pair saves the high-fidelity
details from the main camera and uses the ultra-wide camera's deep DoF as
reference for all-in-focus restoration. To this end, we first implement spatial
warping and color matching to align the dual camera, followed by a
defocus-aware fusion module with learnable defocus parameters to predict a
defocus map and fuse the aligned camera pair. We also build a multi-view
dataset that includes image pairs of the main and ultra-wide cameras in a
smartphone. Extensive experiments on this dataset verify that our solution,
termed DC-NeRF, can produce high-quality all-in-focus novel views and compares
favorably against strong baselines quantitatively and qualitatively. We further
show DoF applications of DC-NeRF with adjustable blur intensity and focal
plane, including refocusing and split diopter.

Comments:
- Published by IEEE TPAMI 2025

---

## Gaussian Splatting is an Effective Data Generator for 3D Object  Detection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-23 | Farhad G. Zanjani, Davide Abati, Auke Wiggers, Dimitris Kalatzis, Jens Petersen, Hong Cai, Amirhossein Habibian | cs.CV | [PDF](http://arxiv.org/pdf/2504.16740v1){: .btn .btn-green } |

**Abstract**: We investigate data augmentation for 3D object detection in autonomous
driving. We utilize recent advancements in 3D reconstruction based on Gaussian
Splatting for 3D object placement in driving scenes. Unlike existing
diffusion-based methods that synthesize images conditioned on BEV layouts, our
approach places 3D objects directly in the reconstructed 3D space with
explicitly imposed geometric transformations. This ensures both the physical
plausibility of object placement and highly accurate 3D pose and position
annotations.
  Our experiments demonstrate that even by integrating a limited number of
external 3D objects into real scenes, the augmented data significantly enhances
3D object detection performance and outperforms existing diffusion-based 3D
augmentation for object detection. Extensive testing on the nuScenes dataset
reveals that imposing high geometric diversity in object placement has a
greater impact compared to the appearance diversity of objects. Additionally,
we show that generating hard examples, either by maximizing detection loss or
imposing high visual occlusion in camera images, does not lead to more
efficient 3D data augmentation for camera-based 3D object detection in
autonomous driving.



---

## Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D  Vision Tasks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-23 | Murat Bilgehan Ertan, Ronak Sahu, Phuong Ha Nguyen, Kaleel Mahmood, Marten van Dijk | cs.CV | [PDF](http://arxiv.org/pdf/2504.16557v1){: .btn .btn-green } |

**Abstract**: We introduce ROAR (Robust Object Removal and Re-annotation), a scalable
framework for privacy-preserving dataset obfuscation that eliminates sensitive
objects instead of modifying them. Our method integrates instance segmentation
with generative inpainting to remove identifiable entities while preserving
scene integrity. Extensive evaluations on 2D COCO-based object detection show
that ROAR achieves 87.5% of the baseline detection average precision (AP),
whereas image dropping achieves only 74.2% of the baseline AP, highlighting the
advantage of scrubbing in preserving dataset utility. The degradation is even
more severe for small objects due to occlusion and loss of fine-grained
details. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR
loss of at most 1.66 dB while maintaining SSIM and improving LPIPS,
demonstrating superior perceptual quality. Our findings establish object
removal as an effective privacy framework, achieving strong privacy guarantees
with minimal performance trade-offs. The results highlight key challenges in
generative inpainting, occlusion-robust segmentation, and task-specific
scrubbing, setting the foundation for future advancements in privacy-preserving
vision systems.

Comments:
- Submitted to ICCV 2025

---

## SmallGS: Gaussian Splatting-based Camera Pose Estimation for  Small-Baseline Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-22 | Yuxin Yao, Yan Zhang, Zhening Huang, Joan Lasenby | cs.CV | [PDF](http://arxiv.org/pdf/2504.17810v1){: .btn .btn-green } |

**Abstract**: Dynamic videos with small baseline motions are ubiquitous in daily life,
especially on social media. However, these videos present a challenge to
existing pose estimation frameworks due to ambiguous features, drift
accumulation, and insufficient triangulation constraints. Gaussian splatting,
which maintains an explicit representation for scenes, provides a reliable
novel view rasterization when the viewpoint change is small. Inspired by this,
we propose SmallGS, a camera pose estimation framework that is specifically
designed for small-baseline videos. SmallGS optimizes sequential camera poses
using Gaussian splatting, which reconstructs the scene from the first frame in
each video segment to provide a stable reference for the rest. The temporal
consistency of Gaussian splatting within limited viewpoint differences reduced
the requirement of sufficient depth variations in traditional camera pose
estimation. We further incorporate pretrained robust visual features, e.g.
DINOv2, into Gaussian splatting, where high-dimensional feature map rendering
enhances the robustness of camera pose estimation. By freezing the Gaussian
splatting and optimizing camera viewpoints based on rasterized features,
SmallGS effectively learns camera poses without requiring explicit feature
correspondences or strong parallax motion. We verify the effectiveness of
SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves
impressive accuracy in camera pose estimation compared to MonST3R and
DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at:
https://yuxinyao620.github.io/SmallGS

Comments:
- 10 pages, 4 figures, Accepted by CVPR workshop

---

## Pose Optimization for Autonomous Driving Datasets using Neural Rendering  Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-22 | Quentin Herau, Nathan Piasco, Moussab Bennehar, Luis Rolado, Dzmitry Tsishkou, Bingbing Liu, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux | cs.CV | [PDF](http://arxiv.org/pdf/2504.15776v1){: .btn .btn-green } |

**Abstract**: Autonomous driving systems rely on accurate perception and localization of
the ego car to ensure safety and reliability in challenging real-world driving
scenarios. Public datasets play a vital role in benchmarking and guiding
advancement in research by providing standardized resources for model
development and evaluation. However, potential inaccuracies in sensor
calibration and vehicle poses within these datasets can lead to erroneous
evaluations of downstream tasks, adversely impacting the reliability and
performance of the autonomous systems. To address this challenge, we propose a
robust optimization method based on Neural Radiance Fields (NeRF) to refine
sensor poses and calibration parameters, enhancing the integrity of dataset
benchmarks. To validate improvement in accuracy of our optimized poses without
ground truth, we present a thorough evaluation process, relying on reprojection
metrics, Novel View Synthesis rendering quality, and geometric alignment. We
demonstrate that our method achieves significant improvements in sensor pose
accuracy. By optimizing these critical parameters, our approach not only
improves the utility of existing datasets but also paves the way for more
reliable autonomous driving models. To foster continued progress in this field,
we make the optimized sensor poses publicly available, providing a valuable
resource for the research community.

Comments:
- under review

---

## Immersive Teleoperation Framework for Locomanipulation Tasks

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-21 | Takuya Boehringer, Jonathan Embley-Riches, Karim Hammoud, Valerio Modugno, Dimitrios Kanoulas | cs.RO | [PDF](http://arxiv.org/pdf/2504.15229v1){: .btn .btn-green } |

**Abstract**: Recent advancements in robotic loco-manipulation have leveraged Virtual
Reality (VR) to enhance the precision and immersiveness of teleoperation
systems, significantly outperforming traditional methods reliant on 2D camera
feeds and joystick controls. Despite these advancements, challenges remain,
particularly concerning user experience across different setups. This paper
introduces a novel VR-based teleoperation framework designed for a robotic
manipulator integrated onto a mobile platform. Central to our approach is the
application of Gaussian splatting, a technique that abstracts the manipulable
scene into a VR environment, thereby enabling more intuitive and immersive
interactions. Users can navigate and manipulate within the virtual scene as if
interacting with a real robot, enhancing both the engagement and efficacy of
teleoperation tasks. An extensive user study validates our approach,
demonstrating significant usability and efficiency improvements. Two-thirds
(66%) of participants completed tasks faster, achieving an average time
reduction of 43%. Additionally, 93% preferred the Gaussian Splat interface
overall, with unanimous (100%) recommendations for future use, highlighting
improvements in precision, responsiveness, and situational awareness. Finally,
we demonstrate the effectiveness of our framework through real-world
experiments in two distinct application scenarios, showcasing the practical
capabilities and versatility of the Splat-based VR interface.

Comments:
- CASE2025, 8 pages, 9 figures

---

## MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry  Monocular Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-21 | Minh-Quan Viet Bui, Jongmin Park, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim | cs.CV | [PDF](http://arxiv.org/pdf/2504.15122v1){: .btn .btn-green } |

**Abstract**: We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)
framework capable of reconstructing sharp and high-quality novel
spatio-temporal views from blurry monocular videos in an end-to-end manner.
Existing dynamic novel view synthesis (NVS) methods are highly sensitive to
motion blur in casually captured videos, resulting in significant degradation
of rendering quality. While recent approaches address motion-blurred inputs for
NVS, they primarily focus on static scene reconstruction and lack dedicated
motion modeling for dynamic objects. To overcome these limitations, our MoBGS
introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for
effective latent camera trajectory estimation, improving global camera motion
deblurring. In addition, we propose a physically-inspired Latent Camera-induced
Exposure Estimation (LCEE) method to ensure consistent deblurring of both
global camera and local object motion. Our MoBGS framework ensures the temporal
consistency of unseen latent timestamps and robust motion decomposition of
static and dynamic regions. Extensive experiments on the Stereo Blur dataset
and real-world blurry videos show that our MoBGS significantly outperforms the
very recent advanced methods (DyBluRF and Deblur4DGS), achieving
state-of-the-art performance for dynamic NVS under motion blur.

Comments:
- The first two authors contributed equally to this work (equal
  contribution). The last two authors advised equally to this work

---

## StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on  3D Gaussians

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-21 | Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng, Jiacheng Bao, Shengqi Liu, Yiying Yang, Xianfang Zeng, Gang Yu, Ming Li | cs.CV | [PDF](http://arxiv.org/pdf/2504.15281v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction
but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented
textures, semantic misalignment, and limited adaptability to abstract
aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer
that integrates multi-modal style conditioning, multi-level semantic alignment,
and perceptual quality enhancement. Our key insights include: (1) optimizing
only RGB attributes preserves geometric integrity during stylization; (2)
disentangling low-, medium-, and high-level semantics is critical for coherent
style transfer; (3) scalability across isolated objects and complex scenes is
essential for practical deployment. StyleMe3D introduces four novel components:
Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent
space for semantic alignment; Contrastive Style Descriptor (CSD) for localized,
content-aware texture transfer; Simultaneously Optimized Scale (SOS) to
decouple style details and structural coherence; and 3D Gaussian Quality
Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated
data to suppress artifacts and enhance visual harmony. Evaluated on NeRF
synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D
outperforms state-of-the-art methods in preserving geometric details (e.g.,
carvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,
coherent lighting in landscapes), while maintaining real-time rendering. This
work bridges photorealistic 3D GS and artistic stylization, unlocking
applications in gaming, virtual worlds, and digital art.

Comments:
- 16 pages; Project page: https://styleme3d.github.io/

---

## IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed  Real X-rays

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-20 | Sascha Jecklin, Aidana Massalimova, Ruyi Zha, Lilian Calvet, Christoph J. Laux, Mazda Farshad, Philipp Fürnstahl | cs.CV | [PDF](http://arxiv.org/pdf/2504.14699v1){: .btn .btn-green } |

**Abstract**: Spine surgery is a high-risk intervention demanding precise execution, often
supported by image-based navigation systems. Recently, supervised learning
approaches have gained attention for reconstructing 3D spinal anatomy from
sparse fluoroscopic data, significantly reducing reliance on
radiation-intensive 3D imaging systems. However, these methods typically
require large amounts of annotated training data and may struggle to generalize
across varying patient anatomies or imaging conditions. Instance-learning
approaches like Gaussian splatting could offer an alternative by avoiding
extensive annotation requirements. While Gaussian splatting has shown promise
for novel view synthesis, its application to sparse, arbitrarily posed real
intraoperative X-rays has remained largely unexplored. This work addresses this
limitation by extending the $R^2$-Gaussian splatting framework to reconstruct
anatomically consistent 3D volumes under these challenging conditions. We
introduce an anatomy-guided radiographic standardization step using style
transfer, improving visual consistency across views, and enhancing
reconstruction quality. Notably, our framework requires no pretraining, making
it inherently adaptable to new patients and anatomies. We evaluated our
approach using an ex-vivo dataset. Expert surgical evaluation confirmed the
clinical utility of the 3D reconstructions for navigation, especially when
using 20 to 30 views, and highlighted the standardization's benefit for
anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)
confirmed performance trade-offs compared to idealized settings, but also
validated the improvement gained from standardization over raw inputs. This
work demonstrates the feasibility of instance-based volumetric reconstruction
from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for
surgical navigation.



---

## Metamon-GS: Enhancing Representability with Variance-Guided  Densification and Light Encoding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-20 | Junyan Su, Baozhu Zhao, Xiaohan Zhang, Qi Liu | cs.CV | [PDF](http://arxiv.org/pdf/2504.14460v1){: .btn .btn-green } |

**Abstract**: The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view
synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point
features with anchor embeddings has significantly enhanced the performance of
newer 3DGS variants. While significant advances have been made, it is still
challenging to boost rendering performance. Feature embeddings have difficulty
accurately representing colors from different perspectives under varying
lighting conditions, which leads to a washed-out appearance. Another reason is
the lack of a proper densification strategy that prevents Gaussian point growth
in thinly initialized areas, resulting in blurriness and needle-shaped
artifacts. To address them, we propose Metamon-GS, from innovative viewpoints
of variance-guided densification strategy and multi-level hash grid. The
densification strategy guided by variance specifically targets Gaussians with
high gradient variance in pixels and compensates for the importance of regions
with extra Gaussians to improve reconstruction. The latter studies implicit
global lighting conditions and accurately interprets color from different
perspectives and feature embeddings. Our thorough experiments on publicly
available datasets show that Metamon-GS surpasses its baseline model and
previous versions, delivering superior quality in rendering novel views.



---

## VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided  Gaussian Number Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-20 | Lifeng Lin, Rongfeng Lu, Quan Chen, Haofan Ren, Ming Lu, Yaoqi Sun, Chenggang Yan, Anke Xue | cs.CV | [PDF](http://arxiv.org/pdf/2504.14548v1){: .btn .btn-green } |

**Abstract**: Sparse-view 3D reconstruction is a fundamental yet challenging task in
practical 3D reconstruction applications. Recently, many methods based on the
3D Gaussian Splatting (3DGS) framework have been proposed to address
sparse-view 3D reconstruction. Although these methods have made considerable
advancements, they still show significant issues with overfitting. To reduce
the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number
Control (VGNC) approach based on generative novel view synthesis (NVS) models.
To the best of our knowledge, this is the first attempt to alleviate the
overfitting issue of sparse-view 3DGS with generative validation images.
Specifically, we first introduce a validation image generation method based on
a generative NVS model. We then propose a Gaussian number control strategy that
utilizes generated validation images to determine the optimal Gaussian numbers,
thereby reducing the issue of overfitting. We conducted detailed experiments on
various sparse-view 3DGS baselines and datasets to evaluate the effectiveness
of VGNC. Extensive experiments show that our approach not only reduces
overfitting but also improves rendering quality on the test set while
decreasing the number of Gaussian points. This reduction lowers storage demands
and accelerates both training and rendering. The code will be released.

Comments:
- 10 pages,8 figures

---

## NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D  Open Vocabulary Instance Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-20 | Junyuan Fang, Zihan Wang, Yejun Zhang, Shuzhe Wang, Iaroslav Melekhov, Juho Kannala | cs.CV | [PDF](http://arxiv.org/pdf/2504.14638v1){: .btn .btn-green } |

**Abstract**: Vision-language models (VLMs) have demonstrated impressive zero-shot transfer
capabilities in image-level visual perception tasks. However, they fall short
in 3D instance-level segmentation tasks that require accurate localization and
recognition of individual objects. To bridge this gap, we introduce a novel 3D
Gaussian Splatting based hard visual prompting approach that leverages camera
interpolation to generate diverse viewpoints around target objects without any
2D-3D optimization or fine-tuning. Our method simulates realistic 3D
perspectives, effectively augmenting existing hard visual prompts by enforcing
geometric consistency across viewpoints. This training-free strategy seamlessly
integrates with prior hard visual prompts, enriching object-descriptive
features and enabling VLMs to achieve more robust and accurate 3D instance
segmentation in diverse 3D scenes.

Comments:
- 15 pages, 4 figures, Scandinavian Conference on Image Analysis 2025

---

## SEGA: Drivable 3D Gaussian Head Avatar from a Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-19 | Chen Guo, Zhuo Su, Jian Wang, Shuang Li, Xu Chang, Zhaohu Li, Yang Zhao, Guidong Wang, Ruqi Huang | cs.GR | [PDF](http://arxiv.org/pdf/2504.14373v2){: .btn .btn-green } |

**Abstract**: Creating photorealistic 3D head avatars from limited input has become
increasingly important for applications in virtual reality, telepresence, and
digital entertainment. While recent advances like neural rendering and 3D
Gaussian splatting have enabled high-quality digital human avatar creation and
animation, most methods rely on multiple images or multi-view inputs, limiting
their practicality for real-world use. In this paper, we propose SEGA, a novel
approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that
combines generalized prior models with a new hierarchical UV-space Gaussian
Splatting framework. SEGA seamlessly combines priors derived from large-scale
2D datasets with 3D priors learned from multi-view, multi-expression, and
multi-ID data, achieving robust generalization to unseen identities while
ensuring 3D consistency across novel viewpoints and expressions. We further
present a hierarchical UV-space Gaussian Splatting framework that leverages
FLAME-based structural priors and employs a dual-branch architecture to
disentangle dynamic and static facial components effectively. The dynamic
branch encodes expression-driven fine details, while the static branch focuses
on expression-invariant regions, enabling efficient parameter inference and
precomputation. This design maximizes the utility of limited 3D data and
achieves real-time performance for animation and rendering. Additionally, SEGA
performs person-specific fine-tuning to further enhance the fidelity and
realism of the generated avatars. Experiments show our method outperforms
state-of-the-art approaches in generalization ability, identity preservation,
and expression realism, advancing one-shot avatar creation for practical
applications.



---

## SLAM&Render: A Benchmark for the Intersection Between Neural Rendering,  Gaussian Splatting and SLAM

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-18 | Samuel Cerezo, Gaetano Meli, Tomás Berriel Martins, Kirill Safronov, Javier Civera | cs.RO | [PDF](http://arxiv.org/pdf/2504.13713v2){: .btn .btn-green } |

**Abstract**: Models and methods originally developed for novel view synthesis and scene
rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are
increasingly being adopted as representations in Simultaneous Localization and
Mapping (SLAM). However, existing datasets fail to include the specific
challenges of both fields, such as multimodality and sequentiality in SLAM or
generalization across viewpoints and illumination conditions in neural
rendering. To bridge this gap, we introduce SLAM&Render, a novel dataset
designed to benchmark methods in the intersection between SLAM and novel view
rendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot
kinematic data, and ground-truth pose streams. By releasing robot kinematic
data, the dataset also enables the assessment of novel SLAM strategies when
applied to robot manipulators. The dataset sequences span five different setups
featuring consumer and industrial objects under four different lighting
conditions, with separate training and test trajectories per scene, as well as
object rearrangements. Our experimental results, obtained with several
baselines from the literature, validate SLAM&Render as a relevant benchmark for
this emerging research area.

Comments:
- 8 pages, 8 figures, RA-L submission

---

## Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale  Training

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-18 | Andrea Amaduzzi, Pierluigi Zama Ramirez, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano | cs.CV | [PDF](http://arxiv.org/pdf/2504.13995v1){: .btn .btn-green } |

**Abstract**: Recent advances in Multimodal Large Language Models (MLLMs) have shown
remarkable capabilities in understanding both images and 3D data, yet these
modalities face inherent limitations in comprehensively representing object
geometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a
promising alternative, encoding both geometric and photorealistic properties
within the weights of a simple Multi-Layer Perceptron (MLP). This work
investigates the feasibility and effectiveness of ingesting NeRFs into an MLLM.
We introduce LLaNA, the first MLLM able to perform new tasks such as NeRF
captioning and Q\&A, by directly processing the weights of a NeRF's MLP.
Notably, LLaNA is able to extract information about the represented objects
without the need to render images or materialize 3D data structures. In
addition, we build the first large-scale NeRF-language dataset, composed by
more than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual
annotations that enable various NeRF-language tasks. Based on this dataset, we
develop a benchmark to evaluate the NeRF understanding capability of our
method. Results show that directly processing NeRF weights leads to better
performance on NeRF-Language tasks compared to approaches that rely on either
2D or 3D representations derived from NeRFs.

Comments:
- Under submission. Project page at
  https://andreamaduzzi.github.io/llana/

---

## Green Robotic Mixed Reality with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-18 | Chenxuan Liu, He Li, Zongze Li, Shuai Wang, Wei Xu, Kejiang Ye, Derrick Wing Kwan Ng, Chengzhong Xu | cs.RO | [PDF](http://arxiv.org/pdf/2504.13697v1){: .btn .btn-green } |

**Abstract**: Realizing green communication in robotic mixed reality (RoboMR) systems
presents a challenge, due to the necessity of uploading high-resolution images
at high frequencies through wireless channels. This paper proposes Gaussian
splatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and
makes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS
model which enables the simulator to opportunistically render a photo-realistic
view from the robot's pose, thereby reducing the need for excessive image
uploads. Since the GS model may involve discrepancies compared to the actual
environments, a GS cross-layer optimization (GSCLO) framework is further
proposed, which jointly optimizes content switching (i.e., deciding whether to
upload image or not) and power allocation across different frames. The GSCLO
problem is solved by an accelerated penalty optimization (APO) algorithm.
Experiments demonstrate that the proposed GSRMR reduces the communication
energy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with
APO outperforms extensive baseline schemes, in terms of peak signal-to-noise
ratio (PSNR) and structural similarity index measure (SSIM).

Comments:
- 6 pages, 5 figures, accepted by IEEE INFOCOM 2025 Workshop on
  Networked Robotics and Communication Systems

---

## EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-18 | Beizhen Zhao, Yifan Zhou, Zijian Wang, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.13540v1){: .btn .btn-green } |

**Abstract**: In this paper, we explore an open research problem concerning the
reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian
Splatting (3DGS) to produce 3D scenes due to its efficient training process.
However, these methodologies may generate incomplete 3D scenes or blurred
multiviews. This is because of (1) inaccurate 3DGS point initialization and (2)
the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To
address these issues, we propose a novel framework EG-Gaussian, which utilizes
epipolar geometry and graph networks for 3D scene reconstruction. Initially, we
integrate epipolar geometry into the 3DGS initialization phase to enhance
initial 3DGS point construction. Then, we specifically design a graph learning
module to refine 3DGS spatial features, in which we incorporate both spatial
coordinates and angular relationships among neighboring points. Experiments on
indoor and outdoor benchmark datasets demonstrate that our approach
significantly improves reconstruction accuracy compared to 3DGS-based methods.



---

## Second-order Optimization of Gaussian Splats with Importance Sampling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Hamza Pehlivan, Andrea Boscolo Camiletto, Lin Geng Foo, Marc Habermann, Christian Theobalt | cs.CV | [PDF](http://arxiv.org/pdf/2504.12905v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to
its high rendering quality and fast inference time. However, 3DGS predominantly
relies on first-order optimizers such as Adam, which leads to long training
times. To address this limitation, we propose a novel second-order optimization
strategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which
we specifically tailor towards Gaussian Splatting. Our key insight is that the
Jacobian in 3DGS exhibits significant sparsity since each Gaussian affects only
a limited number of pixels. We exploit this sparsity by proposing a matrix-free
and GPU-parallelized LM optimization. To further improve its efficiency, we
propose sampling strategies for both the camera views and loss function and,
consequently, the normal equation, significantly reducing the computational
complexity. In addition, we increase the convergence rate of the second-order
approximation by introducing an effective heuristic to determine the learning
rate that avoids the expensive computation cost of line search methods. As a
result, our method achieves a $3\times$ speedup over standard LM and
outperforms Adam by $~6\times$ when the Gaussian count is low while remaining
competitive for moderate counts. Project Page:
https://vcai.mpi-inf.mpg.de/projects/LM-IS



---

## Training-Free Hierarchical Scene Understanding for Gaussian Splatting  with Superpoint Graphs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Shaohui Dai, Yansong Qu, Zheyan Li, Xinyang Li, Shengchuan Zhang, Liujuan Cao | cs.CV | [PDF](http://arxiv.org/pdf/2504.13153v1){: .btn .btn-green } |

**Abstract**: Bridging natural language and 3D geometry is a crucial step toward flexible,
language-driven scene understanding. While recent advances in 3D Gaussian
Splatting (3DGS) have enabled fast and high-quality scene reconstruction,
research has also explored incorporating open-vocabulary understanding into
3DGS. However, most existing methods require iterative optimization over
per-view 2D semantic feature maps, which not only results in inefficiencies but
also leads to inconsistent 3D semantics across views. To address these
limitations, we introduce a training-free framework that constructs a
superpoint graph directly from Gaussian primitives. The superpoint graph
partitions the scene into spatially compact and semantically coherent regions,
forming view-consistent 3D entities and providing a structured foundation for
open-vocabulary understanding. Based on the graph structure, we design an
efficient reprojection strategy that lifts 2D semantic features onto the
superpoints, avoiding costly multi-view iterative training. The resulting
representation ensures strong 3D semantic coherence and naturally supports
hierarchical understanding, enabling both coarse- and fine-grained
open-vocabulary perception within a unified semantic field. Extensive
experiments demonstrate that our method achieves state-of-the-art
open-vocabulary segmentation performance, with semantic field reconstruction
completed over $30\times$ faster. Our code will be available at
https://github.com/Atrovast/THGS.



---

## Digital Twin Generation from Visual Data: A Survey

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Andrew Melnik, Benjamin Alt, Giang Nguyen, Artur Wilkowski, Maciej Stefańczyk, Qirui Wu, Sinan Harms, Helge Rhodin, Manolis Savva, Michael Beetz | cs.CV | [PDF](http://arxiv.org/pdf/2504.13159v1){: .btn .btn-green } |

**Abstract**: This survey explores recent developments in generating digital twins from
videos. Such digital twins can be used for robotics application, media content
creation, or design and construction works. We analyze various approaches,
including 3D Gaussian Splatting, generative in-painting, semantic segmentation,
and foundation models highlighting their advantages and limitations.
Additionally, we discuss challenges such as occlusions, lighting variations,
and scalability, as well as potential future research directions. This survey
aims to provide a comprehensive overview of state-of-the-art methodologies and
their implications for real-world applications. Awesome list:
https://github.com/ndrwmlnk/awesome-digital-twins



---

## Volume Encoding Gaussians: Transfer Function-Agnostic 3D Gaussians for  Volume Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Landon Dyken, Andres Sewell, Will Usher, Steve Petruzza, Sidharth Kumar | cs.GR | [PDF](http://arxiv.org/pdf/2504.13339v1){: .btn .btn-green } |

**Abstract**: While HPC resources are increasingly being used to produce adaptively refined
or unstructured volume datasets, current research in applying machine
learning-based representation to visualization has largely ignored this type of
data. To address this, we introduce Volume Encoding Gaussians (VEG), a novel 3D
Gaussian-based representation for scientific volume visualization focused on
unstructured volumes. Unlike prior 3D Gaussian Splatting (3DGS) methods that
store view-dependent color and opacity for each Gaussian, VEG decouple the
visual appearance from the data representation by encoding only scalar values,
enabling transfer-function-agnostic rendering of 3DGS models for interactive
scientific visualization. VEG are directly initialized from volume datasets,
eliminating the need for structure-from-motion pipelines like COLMAP. To ensure
complete scalar field coverage, we introduce an opacity-guided training
strategy, using differentiable rendering with multiple transfer functions to
optimize our data representation. This allows VEG to preserve fine features
across the full scalar range of a dataset while remaining independent of any
specific transfer function. Each Gaussian is scaled and rotated to adapt to
local geometry, allowing for efficient representation of unstructured meshes
without storing mesh connectivity and while using far fewer primitives. Across
a diverse set of data, VEG achieve high reconstruction quality, compress large
volume datasets by up to 3600x, and support lightning-fast rendering on
commodity GPUs, enabling interactive visualization of large-scale structured
and unstructured volumes.



---

## TSGS: Improving Gaussian Splatting for Transparent Surface  Reconstruction via Normal and De-lighting Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Mingwei Li, Pu Pang, Hehe Fan, Hua Huang, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2504.12799v1){: .btn .btn-green } |

**Abstract**: Reconstructing transparent surfaces is essential for tasks such as robotic
manipulation in labs, yet it poses a significant challenge for 3D
reconstruction techniques like 3D Gaussian Splatting (3DGS). These methods
often encounter a transparency-depth dilemma, where the pursuit of
photorealistic rendering through standard $\alpha$-blending undermines
geometric precision, resulting in considerable depth estimation errors for
transparent materials. To address this issue, we introduce Transparent Surface
Gaussian Splatting (TSGS), a new framework that separates geometry learning
from appearance refinement. In the geometry learning stage, TSGS focuses on
geometry by using specular-suppressed inputs to accurately represent surfaces.
In the second stage, TSGS improves visual fidelity through anisotropic specular
modeling, crucially maintaining the established opacity to ensure geometric
accuracy. To enhance depth inference, TSGS employs a first-surface depth
extraction method. This technique uses a sliding window over $\alpha$-blending
weights to pinpoint the most likely surface location and calculates a robust
weighted average depth. To evaluate the transparent surface reconstruction task
under realistic conditions, we collect a TransLab dataset that includes complex
transparent laboratory glassware. Extensive experiments on TransLab show that
TSGS achieves accurate geometric reconstruction and realistic rendering of
transparent objects simultaneously within the efficient 3DGS framework.
Specifically, TSGS significantly surpasses current leading methods, achieving a
37.3% reduction in chamfer distance and an 8.0% improvement in F1 score
compared to the top baseline. The code and dataset will be released at
https://longxiang-ai.github.io/TSGS/.

Comments:
- Project page: https://longxiang-ai.github.io/TSGS/

---

## Novel Demonstration Generation with Gaussian Splatting Enables Robust  One-Shot Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Sizhe Yang, Wenye Yu, Jia Zeng, Jun Lv, Kerui Ren, Cewu Lu, Dahua Lin, Jiangmiao Pang | cs.RO | [PDF](http://arxiv.org/pdf/2504.13175v1){: .btn .btn-green } |

**Abstract**: Visuomotor policies learned from teleoperated demonstrations face challenges
such as lengthy data collection, high costs, and limited data diversity.
Existing approaches address these issues by augmenting image observations in
RGB space or employing Real-to-Sim-to-Real pipelines based on physical
simulators. However, the former is constrained to 2D data augmentation, while
the latter suffers from imprecise physical simulation caused by inaccurate
geometric reconstruction. This paper introduces RoboSplat, a novel method that
generates diverse, visually realistic demonstrations by directly manipulating
3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian
Splatting (3DGS), directly edit the reconstructed scene, and augment data
across six types of generalization with five techniques: 3D Gaussian
replacement for varying object types, scene appearance, and robot embodiments;
equivariant transformations for different object poses; visual attribute
editing for various lighting conditions; novel view synthesis for new camera
perspectives; and 3D content generation for diverse object types. Comprehensive
real-world experiments demonstrate that RoboSplat significantly enhances the
generalization of visuomotor policies under diverse disturbances. Notably,
while policies trained on hundreds of real-world demonstrations with additional
2D data augmentation achieve an average success rate of 57.2%, RoboSplat
attains 87.8% in one-shot settings across six types of generalization in the
real world.

Comments:
- Published at Robotics: Science and Systems (RSS) 2025

---

## CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Yifei Tong, Runze Tian, Xiao Han, Dingyao Liu, Fenggen Yu, Yan Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2504.12800v1){: .btn .btn-green } |

**Abstract**: As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of
real scenes, enabling user-friendly deformation to create novel scenes while
preserving fine details from the original 3DGS has attracted significant
research attention. We introduce CAGE-GS, a cage-based 3DGS deformation method
that seamlessly aligns a source 3DGS scene with a user-defined target shape.
Our approach learns a deformation cage from the target, which guides the
geometric transformation of the source scene. While the cages effectively
control structural alignment, preserving the textural appearance of 3DGS
remains challenging due to the complexity of covariance parameters. To address
this, we employ a Jacobian matrix-based strategy to update the covariance
parameters of each Gaussian, ensuring texture fidelity post-deformation. Our
method is highly flexible, accommodating various target shape representations,
including texts, images, point clouds, meshes and 3DGS models. Extensive
experiments and ablation studies on both public datasets and newly proposed
scenes demonstrate that our method significantly outperforms existing
techniques in both efficiency and deformation quality.



---

## GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation  with Unity Integration

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Rendong Zhang, Alexandra Watkins, Nilanjan Sarkar | cs.GR | [PDF](http://arxiv.org/pdf/2504.12999v1){: .btn .btn-green } |

**Abstract**: Photorealistic avatars have become essential for immersive applications in
virtual reality (VR) and augmented reality (AR), enabling lifelike interactions
in areas such as training simulations, telemedicine, and virtual collaboration.
These avatars bridge the gap between the physical and digital worlds, improving
the user experience through realistic human representation. However, existing
avatar creation techniques face significant challenges, including high costs,
long creation times, and limited utility in virtual applications. Manual
methods, such as MetaHuman, require extensive time and expertise, while
automatic approaches, such as NeRF-based pipelines often lack efficiency,
detailed facial expression fidelity, and are unable to be rendered at a speed
sufficent for real-time applications. By involving several cutting-edge modern
techniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar
creation pipeline that leverages monocular video input to create a scalable and
efficient photorealistic avatar directly compatible with the Unity game engine.
Our pipeline incorporates a novel Gaussian splatting technique with customized
preprocessing that enables the user of "in the wild" monocular video capture,
detailed facial expression reconstruction and embedding within a fully rigged
avatar model. Additionally, we present a Unity-integrated Gaussian Splatting
Avatar Editor, offering a user-friendly environment for VR/AR application
development. Experimental results validate the effectiveness of our
preprocessing pipeline in standardizing custom data for 3DGS training and
demonstrate the versatility of Gaussian avatars in Unity, highlighting the
scalability and practicality of our approach.



---

## CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene  Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Xiangrui Liu, Xinju Wu, Shiqi Wang, Zhu Li, Sam Kwong | cs.GR | [PDF](http://arxiv.org/pdf/2504.13022v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers
from substantial data volume due to inherent primitive redundancy. To enable
future photorealistic 3D immersive visual communication applications,
significant compression is essential for transmission over the existing
Internet infrastructure. Hence, we propose Compressed Gaussian Splatting
(CompGS++), a novel framework that leverages compact Gaussian primitives to
achieve accurate 3D modeling with substantial size reduction for both static
and dynamic scenes. Our design is based on the principle of eliminating
redundancy both between and within primitives. Specifically, we develop a
comprehensive prediction paradigm to address inter-primitive redundancy through
spatial and temporal primitive prediction modules. The spatial primitive
prediction module establishes predictive relationships for scene primitives and
enables most primitives to be encoded as compact residuals, substantially
reducing the spatial redundancy. We further devise a temporal primitive
prediction module to handle dynamic scenes, which exploits primitive
correlations across timestamps to effectively reduce temporal redundancy.
Moreover, we devise a rate-constrained optimization module that jointly
minimizes reconstruction error and rate consumption. This module effectively
eliminates parameter redundancy within primitives and enhances the overall
compactness of scene representations. Comprehensive evaluations across multiple
benchmark datasets demonstrate that CompGS++ significantly outperforms existing
methods, achieving superior compression performance while preserving accurate
scene modeling. Our implementation will be made publicly available on GitHub to
facilitate further research.

Comments:
- Submitted to a journal

---

## ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing  with Diffusion Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Xiao Han, Runze Tian, Yifei Tong, Fenggen Yu, Dingyao Liu, Yan Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2504.12788v1){: .btn .btn-green } |

**Abstract**: Drag-driven editing has become popular among designers for its ability to
modify complex geometric structures through simple and intuitive manipulation,
allowing users to adjust and reshape content with minimal technical skill. This
drag operation has been incorporated into numerous methods to facilitate the
editing of 2D images and 3D meshes in design. However, few studies have
explored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS)
representation, as deforming 3DGS while preserving shape coherence and visual
continuity remains challenging. In this paper, we introduce ARAP-GS, a
drag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP)
deformation. Unlike previous 3DGS editing methods, we are the first to apply
ARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven
geometric transformations. To preserve scene appearance after deformation, we
incorporate an advanced diffusion prior for image super-resolution within our
iterative optimization process. This approach enhances visual quality while
maintaining multi-view consistency in the edited results. Experiments show that
ARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its
effectiveness and superiority for drag-driven 3DGS editing. Additionally, our
method is highly efficient, requiring only 10 to 20 minutes to edit a scene on
a single RTX 3090 GPU.



---

## ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from  Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Zetong Zhang, Manuel Kaufmann, Lixin Xue, Jie Song, Martin R. Oswald | cs.CV | [PDF](http://arxiv.org/pdf/2504.13167v2){: .btn .btn-green } |

**Abstract**: Creating a photorealistic scene and human reconstruction from a single
monocular in-the-wild video figures prominently in the perception of a
human-centric 3D world. Recent neural rendering advances have enabled holistic
human-scene reconstruction but require pre-calibrated camera and human poses,
and days of training time. In this work, we introduce a novel unified framework
that simultaneously performs camera tracking, human pose estimation and
human-scene reconstruction in an online fashion. 3D Gaussian Splatting is
utilized to learn Gaussian primitives for humans and scenes efficiently, and
reconstruction-based camera tracking and human pose estimation modules are
designed to enable holistic understanding and effective disentanglement of pose
and appearance. Specifically, we design a human deformation module to
reconstruct the details and enhance generalizability to out-of-distribution
poses faithfully. Aiming to learn the spatial correlation between human and
scene accurately, we introduce occlusion-aware human silhouette rendering and
monocular geometric priors, which further improve reconstruction quality.
Experiments on the EMDB and NeuMan datasets demonstrate superior or on-par
performance with existing methods in camera tracking, human pose estimation,
novel view synthesis and runtime. Our project page is at
https://eth-ait.github.io/ODHSR.

Comments:
- Accepted at CVPR 2025

---

## AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-17 | Michael Steiner, Thomas Köhler, Lukas Radl, Felix Windisch, Dieter Schmalstieg, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2504.12811v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,
it still faces challenges such as aliasing, projection artifacts, and view
inconsistencies, primarily due to the simplification of treating splats as 2D
entities. We argue that incorporating full 3D evaluation of Gaussians
throughout the 3DGS pipeline can effectively address these issues while
preserving rasterization efficiency. Specifically, we introduce an adaptive 3D
smoothing filter to mitigate aliasing and present a stable view-space bounding
method that eliminates popping artifacts when Gaussians extend beyond the view
frustum. Furthermore, we promote tile-based culling to 3D with screen-space
planes, accelerating rendering and reducing sorting costs for hierarchical
rasterization. Our method achieves state-of-the-art quality on in-distribution
evaluation sets and significantly outperforms other approaches for
out-of-distribution views. Our qualitative evaluations further demonstrate the
effective removal of aliasing, distortions, and popping artifacts, ensuring
real-time, artifact-free rendering.



---

## BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-16 | Wenhua Wu, Tong Zhao, Chensheng Peng, Lei Yang, Yintao Wei, Zhe Liu, Hesheng Wang | cs.GR | [PDF](http://arxiv.org/pdf/2504.13207v1){: .btn .btn-green } |

**Abstract**: Road surface is the sole contact medium for wheels or robot feet.
Reconstructing road surface is crucial for unmanned vehicles and mobile robots.
Recent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS)
have achieved remarkable results in scene reconstruction. However, they
typically rely on multi-view image inputs and require prolonged optimization
times. In this paper, we propose BEV-GS, a real-time single-frame road surface
reconstruction method based on feed-forward Gaussian splatting. BEV-GS consists
of a prediction module and a rendering module. The prediction module introduces
separate geometry and texture networks following Bird's-Eye-View paradigm.
Geometric and texture parameters are directly estimated from a single frame,
avoiding per-scene optimization. In the rendering module, we utilize grid
Gaussian for road surface representation and novel view synthesis, which better
aligns with road surface characteristics. Our method achieves state-of-the-art
performance on the real-world dataset RSRD. The road elevation error reduces to
1.73 cm, and the PSNR of novel view synthesis reaches 28.36 dB. The prediction
and rendering FPS is 26, and 2061, respectively, enabling high-accuracy and
real-time applications. The code will be available at:
\href{https://github.com/cat-wwh/BEV-GS}{\texttt{https://github.com/cat-wwh/BEV-GS}}



---

## R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh  Reconstruction with Diffusion Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-16 | Haoyang Wang, Liming Liu, Peiheng Wang, Junlin Hao, Jiangkai Wu, Xinggong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.11946v1){: .btn .btn-green } |

**Abstract**: Mesh reconstruction from multi-view images is a fundamental problem in
computer vision, but its performance degrades significantly under sparse-view
conditions, especially in unseen regions where no ground-truth observations are
available. While recent advances in diffusion models have demonstrated strong
capabilities in synthesizing novel views from limited inputs, their outputs
often suffer from visual artifacts and lack 3D consistency, posing challenges
for reliable mesh optimization. In this paper, we propose a novel framework
that leverages diffusion models to enhance sparse-view mesh reconstruction in a
principled and reliable manner. To address the instability of diffusion
outputs, we propose a Consensus Diffusion Module that filters unreliable
generations via interquartile range (IQR) analysis and performs variance-aware
image fusion to produce robust pseudo-supervision. Building on this, we design
an online reinforcement learning strategy based on the Upper Confidence Bound
(UCB) to adaptively select the most informative viewpoints for enhancement,
guided by diffusion loss. Finally, the fused images are used to jointly
supervise a NeRF-based model alongside sparse-view ground truth, ensuring
consistency across both geometry and appearance. Extensive experiments
demonstrate that our method achieves significant improvements in both geometric
quality and rendering quality.



---

## CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-16 | Wei Sun, Yanzhao Zhou, Jianbin Jiao, Yuan Li | cs.CV | [PDF](http://arxiv.org/pdf/2504.11893v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D scene understanding is crucial for applications requiring
natural language-driven spatial interpretation, such as robotics and augmented
reality. While 3D Gaussian Splatting (3DGS) offers a powerful representation
for scene reconstruction, integrating it with open-vocabulary frameworks
reveals a key challenge: cross-view granularity inconsistency. This issue,
stemming from 2D segmentation methods like SAM, results in inconsistent object
segmentations across views (e.g., a "coffee set" segmented as a single entity
in one view but as "cup + coffee + spoon" in another). Existing 3DGS-based
methods often rely on isolated per-Gaussian feature learning, neglecting the
spatial context needed for cohesive object reasoning, leading to fragmented
representations. We propose Context-Aware Gaussian Splatting (CAGS), a novel
framework that addresses this challenge by incorporating spatial context into
3DGS. CAGS constructs local graphs to propagate contextual features across
Gaussians, reducing noise from inconsistent granularity, employs mask-centric
contrastive learning to smooth SAM-derived features across views, and leverages
a precomputation strategy to reduce computational cost by precomputing
neighborhood relationships, enabling efficient training in large-scale scenes.
By integrating spatial context, CAGS significantly improves 3D instance
segmentation and reduces fragmentation errors on datasets like LERF-OVS and
ScanNet, enabling robust language-guided 3D scene understanding.



---

## GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-François Lalonde | cs.CV | [PDF](http://arxiv.org/pdf/2504.10809v2){: .btn .btn-green } |

**Abstract**: We present GaSLight, a method that generates spatially-varying lighting from
regular images. Our method proposes using HDR Gaussian Splats as light source
representation, marking the first time regular images can serve as light
sources in a 3D renderer. Our two-stage process first enhances the dynamic
range of images plausibly and accurately by leveraging the priors embedded in
diffusion models. Next, we employ Gaussian Splats to model 3D lighting,
achieving spatially variant lighting. Our approach yields state-of-the-art
results on HDR estimations and their applications in illuminating virtual
objects and scenes. To facilitate the benchmarking of images as light sources,
we introduce a novel dataset of calibrated and unsaturated HDR to evaluate
images as light sources. We assess our method using a combination of this novel
dataset and an existing dataset from the literature. Project page:
https://lvsn.github.io/gaslight/



---

## 3D Gabor Splatting: Reconstruction of High-frequency Surface Texture  using Gabor Noise

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Haato Watanabe, Kenji Tojo, Nobuyuki Umetani | cs.GR | [PDF](http://arxiv.org/pdf/2504.11003v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has experienced explosive popularity in the past few
years in the field of novel view synthesis. The lightweight and differentiable
representation of the radiance field using the Gaussian enables rapid and
high-quality reconstruction and fast rendering. However, reconstructing objects
with high-frequency surface textures (e.g., fine stripes) requires many skinny
Gaussian kernels because each Gaussian represents only one color if viewed from
one direction. Thus, reconstructing the stripes pattern, for example, requires
Gaussians for at least the number of stripes. We present 3D Gabor splatting,
which augments the Gaussian kernel to represent spatially high-frequency
signals using Gabor noise. The Gabor kernel is a combination of a Gaussian term
and spatially fluctuating wave functions, making it suitable for representing
spatial high-frequency texture. We demonstrate that our 3D Gabor splatting can
reconstruct various high-frequency textures on the objects.

Comments:
- 4 pages, 5 figures, Eurographics 2025 Short Paper

---

## Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Andrea Simonelli, Norman Müller, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2504.11024v1){: .btn .btn-green } |

**Abstract**: The increasing availability of digital 3D environments, whether through
image-based 3D reconstruction, generation, or scans obtained by robots, is
driving innovation across various applications. These come with a significant
demand for 3D interaction, such as 3D Interactive Segmentation, which is useful
for tasks like object selection and manipulation. Additionally, there is a
persistent need for solutions that are efficient, precise, and performing well
across diverse settings, particularly in unseen environments and with
unfamiliar objects. In this work, we introduce a 3D interactive segmentation
method that consistently surpasses previous state-of-the-art techniques on both
in-domain and out-of-domain datasets. Our simple approach integrates a
voxel-based sparse encoder with a lightweight transformer-based decoder that
implements implicit click fusion, achieving superior performance and maximizing
efficiency. Our method demonstrates substantial improvements on benchmark
datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on
unseen geometric distributions such as the ones obtained by Gaussian Splatting.
The project web-page is available at https://simonelli-andrea.github.io/easy3d.



---

## 3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin | cs.CV | [PDF](http://arxiv.org/pdf/2504.11218v2){: .btn .btn-green } |

**Abstract**: 3D affordance reasoning is essential in associating human instructions with
the functional regions of 3D objects, facilitating precise, task-oriented
manipulations in embodied AI. However, current methods, which predominantly
depend on sparse 3D point clouds, exhibit limited generalizability and
robustness due to their sensitivity to coordinate variations and the inherent
sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers
high-fidelity, real-time rendering with minimal computational overhead by
representing scenes as dense, continuous distributions. This positions 3DGS as
a highly effective approach for capturing fine-grained affordance details and
improving recognition accuracy. Nevertheless, its full potential remains
largely untapped due to the absence of large-scale, 3DGS-specific affordance
datasets. To overcome these limitations, we present 3DAffordSplat, the first
large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.
This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,
and 6,631 manually annotated affordance labels, encompassing 21 object
categories and 18 affordance types. Building upon this dataset, we introduce
AffordSplatNet, a novel model specifically designed for affordance reasoning
using 3DGS representations. AffordSplatNet features an innovative cross-modal
structure alignment module that exploits structural consistency priors to align
3D point cloud and 3DGS representations, resulting in enhanced affordance
recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat
dataset significantly advances affordance learning within the 3DGS domain,
while AffordSplatNet consistently outperforms existing methods across both seen
and unseen settings, highlighting its robust generalization capabilities.

Comments:
- The first large-scale 3D Gaussians Affordance Reasoning Benchmark

---

## EDGS: Eliminating Densification for Efficient Convergence of 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-15 | Dmytro Kotovenko, Olga Grebenkova, Björn Ommer | cs.GR | [PDF](http://arxiv.org/pdf/2504.13204v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting reconstructs scenes by starting from a sparse
Structure-from-Motion initialization and iteratively refining
under-reconstructed regions. This process is inherently slow, as it requires
multiple densification steps where Gaussians are repeatedly split and adjusted,
following a lengthy optimization path. Moreover, this incremental approach
often leads to suboptimal renderings, particularly in high-frequency regions
where detail is critical.
  We propose a fundamentally different approach: we eliminate densification
process with a one-step approximation of scene geometry using triangulated
pixels from dense image correspondences. This dense initialization allows us to
estimate rough geometry of the scene while preserving rich details from input
RGB images, providing each Gaussian with well-informed colors, scales, and
positions. As a result, we dramatically shorten the optimization path and
remove the need for densification. Unlike traditional methods that rely on
sparse keypoints, our dense initialization ensures uniform detail across the
scene, even in high-frequency regions where 3DGS and other methods struggle.
Moreover, since all splats are initialized in parallel at the start of
optimization, we eliminate the need to wait for densification to adjust new
Gaussians.
  Our method not only outperforms speed-optimized models in training efficiency
but also achieves higher rendering quality than state-of-the-art approaches,
all while using only half the splats of standard 3DGS. It is fully compatible
with other 3DGS acceleration techniques, making it a versatile and efficient
solution that can be integrated with existing approaches.



---

## DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar  Relighting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Zeren Jiang, Shaofei Wang, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2504.10486v1){: .btn .btn-green } |

**Abstract**: Creating relightable and animatable human avatars from monocular videos is a
rising research topic with a range of applications, e.g. virtual reality,
sports, and video games. Previous works utilize neural fields together with
physically based rendering (PBR), to estimate geometry and disentangle
appearance properties of human avatars. However, one drawback of these methods
is the slow rendering speed due to the expensive Monte Carlo ray tracing. To
tackle this problem, we proposed to distill the knowledge from implicit neural
fields (teacher) to explicit 2D Gaussian splatting (student) representation to
take advantage of the fast rasterization property of Gaussian splatting. To
avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We
also propose novel part-wise ambient occlusion probes for shadow computation.
Shadow prediction is achieved by querying these probes only once per pixel,
which paves the way for real-time relighting of avatars. These techniques
combined give high-quality relighting results with realistic shadow effects.
Our experiments demonstrate that the proposed student model achieves comparable
or even better relighting results with our teacher model while being 370 times
faster at inference time, achieving a 67 FPS rendering speed.

Comments:
- 16 pages, 8 figures, Project pages:
  https://jzr99.github.io/DNF-Avatar/

---

## ESCT3D: Efficient and Selectively Controllable Text-Driven 3D Content  Generation with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Huiqi Wu, Jianbo Mei, Yingjie Huang, Yining Xu, Jingjiao You, Yilong Liu, Li Yao | cs.CV | [PDF](http://arxiv.org/pdf/2504.10316v1){: .btn .btn-green } |

**Abstract**: In recent years, significant advancements have been made in text-driven 3D
content generation. However, several challenges remain. In practical
applications, users often provide extremely simple text inputs while expecting
high-quality 3D content. Generating optimal results from such minimal text is a
difficult task due to the strong dependency of text-to-3D models on the quality
of input prompts. Moreover, the generation process exhibits high variability,
making it difficult to control. Consequently, multiple iterations are typically
required to produce content that meets user expectations, reducing generation
efficiency. To address this issue, we propose GPT-4V for self-optimization,
which significantly enhances the efficiency of generating satisfactory content
in a single attempt. Furthermore, the controllability of text-to-3D generation
methods has not been fully explored. Our approach enables users to not only
provide textual descriptions but also specify additional conditions, such as
style, edges, scribbles, poses, or combinations of multiple conditions,
allowing for more precise control over the generated 3D content. Additionally,
during training, we effectively integrate multi-view information, including
multi-view depth, masks, features, and images, to address the common Janus
problem in 3D content generation. Extensive experiments demonstrate that our
method achieves robust generalization, facilitating the efficient and
controllable generation of high-quality 3D content.



---

## EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Yufei Deng, Yuanjian Wang, Rong Xiao, Chenwei Tang, Jizhe Zhou, Jiahao Fan, Deng Xiong, Jiancheng Lv, Huajin Tang | cs.CV | [PDF](http://arxiv.org/pdf/2504.10012v1){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting (3D-GS) achieves photorealistic novel view
synthesis, its performance degrades with motion blur. In scenarios with rapid
motion or low-light conditions, existing RGB-based deblurring methods struggle
to model camera pose and radiance changes during exposure, reducing
reconstruction accuracy. Event cameras, capturing continuous brightness changes
during exposure, can effectively assist in modeling motion blur and improving
reconstruction quality. Therefore, we propose Event-driven Bundle Adjusted
Deblur Gaussian Splatting (EBAD-Gaussian), which reconstructs sharp 3D
Gaussians from event streams and severely blurred images. This method jointly
learns the parameters of these Gaussians while recovering camera motion
trajectories during exposure time. Specifically, we first construct a blur loss
function by synthesizing multiple latent sharp images during the exposure time,
minimizing the difference between real and synthesized blurred images. Then we
use event stream to supervise the light intensity changes between latent sharp
images at any time within the exposure period, supplementing the light
intensity dynamic changes lost in RGB images. Furthermore, we optimize the
latent sharp images at intermediate exposure times based on the event-based
double integral (EDI) prior, applying consistency constraints to enhance the
details and texture information of the reconstructed images. Extensive
experiments on synthetic and real-world datasets show that EBAD-Gaussian can
achieve high-quality 3D scene reconstruction under the condition of blurred
images and event stream inputs.



---

## MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based  Dynamic-Resolution Ray Sampling

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Yunpeng Tan, Junlin Hao, Jiangkai Wu, Liming Liu, Qingyang Li, Xinggong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.09878v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) is widely known for high-fidelity novel view
synthesis. However, even the state-of-the-art NeRF model, Gaussian Splatting,
requires minutes for training, far from the real-time performance required by
multimedia scenarios like telemedicine. One of the obstacles is its inefficient
sampling, which is only partially addressed by existing works. Existing
point-sampling algorithms uniformly sample simple-texture regions (easy to fit)
and complex-texture regions (hard to fit), while existing ray-sampling
algorithms sample these regions all in the finest granularity (i.e. the pixel
level), both wasting GPU training resources. Actually, regions with different
texture intensities require different sampling granularities. To this end, we
propose a novel dynamic-resolution ray-sampling algorithm, MCBlock, which
employs Monte Carlo Tree Search (MCTS) to partition each training image into
pixel blocks with different sizes for active block-wise training. Specifically,
the trees are initialized according to the texture of training images to boost
the initialization speed, and an expansion/pruning module dynamically optimizes
the block partition. MCBlock is implemented in Nerfstudio, an open-source
toolset, and achieves a training acceleration of up to 2.33x, surpassing other
ray-sampling algorithms. We believe MCBlock can apply to any cone-tracing NeRF
model and contribute to the multimedia community.



---

## GaussVideoDreamer: 3D Scene Generation with Video Diffusion and  Inconsistency-Aware Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Junlin Hao, Peiheng Wang, Haoyang Wang, Xinggong Zhang, Zongming Guo | cs.CV | [PDF](http://arxiv.org/pdf/2504.10001v3){: .btn .btn-green } |

**Abstract**: Single-image 3D scene reconstruction presents significant challenges due to
its inherently ill-posed nature and limited input constraints. Recent advances
have explored two promising directions: multiview generative models that train
on 3D consistent datasets but struggle with out-of-distribution generalization,
and 3D scene inpainting and completion frameworks that suffer from cross-view
inconsistency and suboptimal error handling, as they depend exclusively on
depth data or 3D smoothness, which ultimately degrades output quality and
computational performance. Building upon these approaches, we present
GaussVideoDreamer, which advances generative multimedia approaches by bridging
the gap between image, video, and 3D generation, integrating their strengths
through two key innovations: (1) A progressive video inpainting strategy that
harnesses temporal coherence for improved multiview consistency and faster
convergence. (2) A 3D Gaussian Splatting consistency mask to guide the video
diffusion with 3D consistent multiview evidence. Our pipeline combines three
core components: a geometry-aware initialization protocol, Inconsistency-Aware
Gaussian Splatting, and a progressive video inpainting strategy. Experimental
results demonstrate that our approach achieves 32% higher LLaVA-IQA scores and
at least 2x speedup compared to existing methods while maintaining robust
performance across diverse scenes.



---

## NeRF-Based Transparent Object Grasping Enhanced by Shape Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Yi Han, Zixin Lin, Dongjie Li, Lvping Chen, Yongliang Shi, Gan Ma | cs.RO | [PDF](http://arxiv.org/pdf/2504.09868v1){: .btn .btn-green } |

**Abstract**: Transparent object grasping remains a persistent challenge in robotics,
largely due to the difficulty of acquiring precise 3D information. Conventional
optical 3D sensors struggle to capture transparent objects, and machine
learning methods are often hindered by their reliance on high-quality datasets.
Leveraging NeRF's capability for continuous spatial opacity modeling, our
proposed architecture integrates a NeRF-based approach for reconstructing the
3D information of transparent objects. Despite this, certain portions of the
reconstructed 3D information may remain incomplete. To address these
deficiencies, we introduce a shape-prior-driven completion mechanism, further
refined by a geometric pose estimation method we have developed. This allows us
to obtain a complete and reliable 3D information of transparent objects.
Utilizing this refined data, we perform scene-level grasp prediction and deploy
the results in real-world robotic systems. Experimental validation demonstrates
the efficacy of our architecture, showcasing its capability to reliably capture
3D information of various transparent objects in cluttered scenes, and
correspondingly, achieve high-quality, stables, and executable grasp
predictions.



---

## LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian  Splatting for Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-14 | Hao Sun, Fenggen Yu, Huiyao Xu, Tao Zhang, Changqing Zou | cs.CV | [PDF](http://arxiv.org/pdf/2504.10331v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) in low-light scenes remains a significant
challenge due to degraded inputs characterized by severe noise, low dynamic
range (LDR) and unreliable initialization. While recent NeRF-based approaches
have shown promising results, most suffer from high computational costs, and
some rely on carefully captured or pre-processed data--such as RAW sensor
inputs or multi-exposure sequences--which severely limits their practicality.
In contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with
competitive visual fidelity; however, existing 3DGS-based methods struggle with
low-light sRGB inputs, resulting in unstable Gaussian initialization and
ineffective noise suppression. To address these challenges, we propose
LL-Gaussian, a novel framework for 3D reconstruction and enhancement from
low-light sRGB images, enabling pseudo normal-light novel view synthesis. Our
method introduces three key innovations: 1) an end-to-end Low-Light Gaussian
Initialization Module (LLGIM) that leverages dense priors from learning-based
MVS approach to generate high-quality initial point clouds; 2) a dual-branch
Gaussian decomposition model that disentangles intrinsic scene properties
(reflectance and illumination) from transient interference, enabling stable and
interpretable optimization; 3) an unsupervised optimization strategy guided by
both physical constrains and diffusion prior to jointly steer decomposition and
enhancement. Additionally, we contribute a challenging dataset collected in
extreme low-light environments and demonstrate the effectiveness of
LL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian
achieves up to 2,000 times faster inference and reduces training time to just
2%, while delivering superior reconstruction and rendering quality.



---

## TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-13 | Zhicong Wu, Hongbin Xu, Gang Xu, Ping Nie, Zhixin Yan, Jinkai Zheng, Liangqiong Qu, Ming Li, Liqiang Nie | cs.CV | [PDF](http://arxiv.org/pdf/2504.09588v1){: .btn .btn-green } |

**Abstract**: Recent advancements in Generalizable Gaussian Splatting have enabled robust
3D reconstruction from sparse input views by utilizing feed-forward Gaussian
Splatting models, achieving superior cross-scene generalization. However, while
many methods focus on geometric consistency, they often neglect the potential
of text-driven guidance to enhance semantic understanding, which is crucial for
accurately reconstructing fine-grained details in complex scenes. To address
this limitation, we propose TextSplat--the first text-driven Generalizable
Gaussian Splatting framework. By employing a text-guided fusion of diverse
semantic cues, our framework learns robust cross-modal feature representations
that improve the alignment of geometric and semantic information, producing
high-fidelity 3D reconstructions. Specifically, our framework employs three
parallel modules to obtain complementary representations: the Diffusion Prior
Depth Estimator for accurate depth information, the Semantic Aware Segmentation
Network for detailed semantic information, and the Multi-View Interaction
Network for refined cross-view features. Then, in the Text-Guided Semantic
Fusion Module, these representations are integrated via the text-guided and
attention-based feature aggregation mechanism, resulting in enhanced 3D
Gaussian parameters enriched with detailed semantic cues. Experimental results
on various benchmark datasets demonstrate improved performance compared to
existing methods across multiple evaluation metrics, validating the
effectiveness of our framework. The code will be publicly available.



---

## DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-13 | Yexing Xu, Longguang Wang, Minglin Chen, Sheng Ao, Li Li, Yulan Guo | cs.CV | [PDF](http://arxiv.org/pdf/2504.09491v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting (3DGS) has demonstrated promising results in
novel view synthesis, its performance degrades dramatically with sparse inputs
and generates undesirable artifacts. As the number of training views decreases,
the novel view synthesis task degrades to a highly under-determined problem
such that existing methods suffer from the notorious overfitting issue.
Interestingly, we observe that models with fewer Gaussian primitives exhibit
less overfitting under sparse inputs. Inspired by this observation, we propose
a Random Dropout Regularization (RDR) to exploit the advantages of
low-complexity models to alleviate overfitting. In addition, to remedy the lack
of high-frequency details for these models, an Edge-guided Splitting Strategy
(ESS) is developed. With these two techniques, our method (termed DropoutGS)
provides a simple yet effective plug-in approach to improve the generalization
performance of existing 3DGS methods. Extensive experiments show that our
DropoutGS produces state-of-the-art performance under sparse views on benchmark
datasets including Blender, LLFF, and DTU. The project page is at:
https://xuyx55.github.io/DropoutGS/.

Comments:
- Accepted by CVPR 2025

---

## BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via  Adaptive Block-Based Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-12 | Yongchang Wu, Zipeng Qi, Zhenwei Shi, Zhengxia Zou | cs.CV | [PDF](http://arxiv.org/pdf/2504.09048v2){: .btn .btn-green } |

**Abstract**: The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated
remarkable potential in novel view synthesis tasks. The divide-and-conquer
paradigm has enabled large-scale scene reconstruction, but significant
challenges remain in scene partitioning, optimization, and merging processes.
This paper introduces BlockGaussian, a novel framework incorporating a
content-aware scene partition strategy and visibility-aware block optimization
to achieve efficient and high-quality large-scale scene reconstruction.
Specifically, our approach considers the content-complexity variation across
different regions and balances computational load during scene partitioning,
enabling efficient scene reconstruction. To tackle the supervision mismatch
issue during independent block optimization, we introduce auxiliary points
during individual block optimization to align the ground-truth supervision,
which enhances the reconstruction quality. Furthermore, we propose a
pseudo-view geometry constraint that effectively mitigates rendering
degradation caused by airspace floaters during block merging. Extensive
experiments on large-scale scenes demonstrate that our approach achieves
state-of-the-art performance in both reconstruction efficiency and rendering
quality, with a 5x speedup in optimization and an average PSNR improvement of
1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces
computational requirements, enabling large-scale scene reconstruction on a
single 24GB VRAM device. The project page is available at
https://github.com/SunshineWYC/BlockGaussian

Comments:
- https://github.com/SunshineWYC/BlockGaussian

---

## BIGS: Bimanual Category-agnostic Interaction Reconstruction from  Monocular Videos via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-12 | Jeongwan On, Kyeonghwan Gwak, Gunyoung Kang, Junuk Cha, Soohyun Hwang, Hyein Hwang, Seungryul Baek | cs.CV | [PDF](http://arxiv.org/pdf/2504.09097v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem
that can find numerous applications. Despite recent advances, there is no
comprehensive pipeline yet for bimanual class-agnostic interaction
reconstruction from a monocular RGB video, where two hands and an unknown
object are interacting with each other. Previous works tackled the limited
hand-object interaction case, where object templates are pre-known or only one
hand is involved in the interaction. The bimanual interaction reconstruction
exhibits severe occlusions introduced by complex interactions between two hands
and an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D
Gaussian Splatting), a method that reconstructs 3D Gaussians of hands and an
unknown object from a monocular video. To robustly obtain object Gaussians
avoiding severe occlusions, we leverage prior knowledge of pre-trained
diffusion model with score distillation sampling (SDS) loss, to reconstruct
unseen object parts. For hand Gaussians, we exploit the 3D priors of hand model
(i.e., MANO) and share a single Gaussian for two hands to effectively
accumulate hand 3D information, given limited views. To further consider the 3D
alignment between hands and objects, we include the interacting-subjects
optimization step during Gaussian optimization. Our method achieves the
state-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose
estimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering
quality (PSNR, SSIM, LPIPS), respectively.

Comments:
- Accepted to CVPR 2025

---

## You Need a Transition Plane: Bridging Continuous Panoramic 3D  Reconstruction with Perspective Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-12 | Zhijie Shen, Chunyu Lin, Shujuan Huang, Lang Nie, Kang Liao, Yao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2504.09062v1){: .btn .btn-green } |

**Abstract**: Recently, reconstructing scenes from a single panoramic image using advanced
3D Gaussian Splatting (3DGS) techniques has attracted growing interest.
Panoramic images offer a 360$\times$ 180 field of view (FoV), capturing the
entire scene in a single shot. However, panoramic images introduce severe
distortion, making it challenging to render 3D Gaussians into 2D distorted
equirectangular space directly. Converting equirectangular images to cubemap
projections partially alleviates this problem but introduces new challenges,
such as projection distortion and discontinuities across cube-face boundaries.
To address these limitations, we present a novel framework, named TPGS, to
bridge continuous panoramic 3D scene reconstruction with perspective Gaussian
splatting. Firstly, we introduce a Transition Plane between adjacent cube faces
to enable smoother transitions in splatting directions and mitigate
optimization ambiguity in the boundary region. Moreover, an intra-to-inter face
optimization strategy is proposed to enhance local details and restore visual
consistency across cube-face boundaries. Specifically, we optimize 3D Gaussians
within individual cube faces and then fine-tune them in the stitched panoramic
space. Additionally, we introduce a spherical sampling technique to eliminate
visible stitching seams. Extensive experiments on indoor and outdoor,
egocentric, and roaming benchmark datasets demonstrate that our approach
outperforms existing state-of-the-art methods. Code and models will be
available at https://github.com/zhijieshen-bjtu/TPGS.



---

## A Constrained Optimization Approach for Gaussian Splatting from  Coarsely-posed Images and Noisy Lidar Point Clouds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-12 | Jizong Peng, Tze Ho Elden Tse, Kai Xu, Wenchao Gao, Angela Yao | cs.CV | [PDF](http://arxiv.org/pdf/2504.09129v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a powerful reconstruction technique, but it
needs to be initialized from accurate camera poses and high-fidelity point
clouds. Typically, the initialization is taken from Structure-from-Motion (SfM)
algorithms; however, SfM is time-consuming and restricts the application of
3DGS in real-world scenarios and large-scale scene reconstruction. We introduce
a constrained optimization method for simultaneous camera pose estimation and
3D reconstruction that does not require SfM support. Core to our approach is
decomposing a camera pose into a sequence of camera-to-(device-)center and
(device-)center-to-world optimizations. To facilitate, we propose two
optimization constraints conditioned to the sensitivity of each parameter group
and restricts each parameter's search space. In addition, as we learn the scene
geometry directly from the noisy point clouds, we propose geometric constraints
to improve the reconstruction quality. Experiments demonstrate that the
proposed method significantly outperforms the existing (multi-modal) 3DGS
baseline and methods supplemented by COLMAP on both our collected dataset and
two public benchmarks.



---

## HAL-NeRF: High Accuracy Localization Leveraging Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-11 | Asterios Reppas, Grigorios-Aris Cheimariotis, Panos K. Papadopoulos, Panagiotis Frasiolas, Dimitrios Zarpalas | cs.CV | [PDF](http://arxiv.org/pdf/2504.08901v1){: .btn .btn-green } |

**Abstract**: Precise camera localization is a critical task in XR applications and
robotics. Using only the camera captures as input to a system is an inexpensive
option that enables localization in large indoor and outdoor environments, but
it presents challenges in achieving high accuracy. Specifically, camera
relocalization methods, such as Absolute Pose Regression (APR), can localize
cameras with a median translation error of more than $0.5m$ in outdoor scenes.
This paper presents HAL-NeRF, a high-accuracy localization method that combines
a CNN pose regressor with a refinement module based on a Monte Carlo particle
filter. The Nerfacto model, an implementation of Neural Radiance Fields
(NeRFs), is used to augment the data for training the pose regressor and to
measure photometric loss in the particle filter refinement module. HAL-NeRF
leverages Nerfacto's ability to synthesize high-quality novel views,
significantly improving the performance of the localization pipeline. HAL-NeRF
achieves state-of-the-art results that are conventionally measured as the
average of the median per scene errors. The translation error was $0.025m$ and
the rotation error was $0.59$ degrees and 0.04m and 0.58 degrees on the
7-Scenes dataset and Cambridge Landmarks datasets respectively, with the
trade-off of increased computational time. This work highlights the potential
of combining APR with NeRF-based refinement techniques to advance monocular
camera relocalization accuracy.

Comments:
- 8 pages, 4 figures

---

## FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level  Interactive Agents


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-11 | Xin Tan, Yuzhou Ji, He Zhu, Yuan Xie | cs.CV | [PDF](http://arxiv.org/pdf/2504.08581v1){: .btn .btn-green } |

**Abstract**: The semantically interactive radiance field has long been a promising
backbone for 3D real-world applications, such as embodied AI to achieve scene
understanding and manipulation. However, multi-granularity interaction remains
a challenging task due to the ambiguity of language and degraded quality when
it comes to queries upon object components. In this work, we present FMLGS, an
approach that supports part-level open-vocabulary query within 3D Gaussian
Splatting (3DGS). We propose an efficient pipeline for building and querying
consistent object- and part-level semantics based on Segment Anything Model 2
(SAM2). We designed a semantic deviation strategy to solve the problem of
language ambiguity among object parts, which interpolates the semantic features
of fine-grained targets for enriched information. Once trained, we can query
both objects and their describable parts using natural language. Comparisons
with other state-of-the-art methods prove that our method can not only better
locate specified part-level targets, but also achieve first-place performance
concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x
faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further
integrate FMLGS as a virtual agent that can interactively navigate through 3D
scenes, locate targets, and respond to user demands through a chat interface,
which demonstrates the potential of our work to be further expanded and applied
in the future.



---

## In-2-4D: Inbetweening from Two Single-View Images to 4D Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-11 | Sauradip Nag, Daniel Cohen-Or, Hao Zhang, Ali Mahdavi-Amiri | cs.GR | [PDF](http://arxiv.org/pdf/2504.08366v1){: .btn .btn-green } |

**Abstract**: We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)
inbetweening from a minimalistic input setting: two single-view images
capturing an object in two distinct motion states. Given two images
representing the start and end states of an object in motion, our goal is to
generate and reconstruct the motion in 4D. We utilize a video interpolation
model to predict the motion, but large frame-to-frame motions can lead to
ambiguous interpretations. To overcome this, we employ a hierarchical approach
to identify keyframes that are visually close to the input states and show
significant motion, then generate smooth fragments between them. For each
fragment, we construct the 3D representation of the keyframe using Gaussian
Splatting. The temporal frames within the fragment guide the motion, enabling
their transformation into dynamic Gaussians through a deformation field. To
improve temporal consistency and refine 3D motion, we expand the self-attention
of multi-view diffusion across timesteps and apply rigid transformation
regularization. Finally, we merge the independently generated 3D motion
segments by interpolating boundary deformation fields and optimizing them to
align with the guiding video, ensuring smooth and flicker-free transitions.
Through extensive qualitative and quantitiave experiments as well as a user
study, we show the effectiveness of our method and its components. The project
page is available at https://in-2-4d.github.io/

Comments:
- Technical Report

---

## Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-11 | Bram Vanherle, Brent Zoomers, Jeroen Put, Frank Van Reeth, Nick Michiels | cs.CV | [PDF](http://arxiv.org/pdf/2504.08473v1){: .btn .btn-green } |

**Abstract**: Generating synthetic images is a useful method for cheaply obtaining labeled
data for training computer vision models. However, obtaining accurate 3D models
of relevant objects is necessary, and the resulting images often have a gap in
realism due to challenges in simulating lighting effects and camera artifacts.
We propose using the novel view synthesis method called Gaussian Splatting to
address these challenges. We have developed a synthetic data pipeline for
generating high-quality context-aware instance segmentation training data for
specific objects. This process is fully automated, requiring only a video of
the target object. We train a Gaussian Splatting model of the target object and
automatically extract the object from the video. Leveraging Gaussian Splatting,
we then render the object on a random background image, and monocular depth
estimation is employed to place the object in a believable pose. We introduce a
novel dataset to validate our approach and show superior performance over other
data generation approaches, such as Cut-and-Paste and Diffusion model-based
generation.

Comments:
- Accepted at the International Conference on Robotics, Computer Vision
  and Intelligent Systems 2025 (ROBOVIS)

---

## InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars  with Deformable Gaussians


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-10 | Kefan Chen, Sergiu Oprea, Justin Theiss, Sreyas Mohan, Srinath Sridhar, Aayush Prakash | cs.CV | [PDF](http://arxiv.org/pdf/2504.07949v1){: .btn .btn-green } |

**Abstract**: With the rising interest from the community in digital avatars coupled with
the importance of expressions and gestures in communication, modeling natural
avatar behavior remains an important challenge across many industries such as
teleconferencing, gaming, and AR/VR. Human hands are the primary tool for
interacting with the environment and essential for realistic human behavior
modeling, yet existing 3D hand and head avatar models often overlook the
crucial aspect of hand-body interactions, such as between hand and face. We
present InteracttAvatar, the first model to faithfully capture the
photorealistic appearance of dynamic hand and non-rigid hand-face interactions.
Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian
Splatting as well as a dynamic refinement module, captures pose-dependent
change, e.g. the fine wrinkles and complex shadows that occur during
articulation. Importantly, our hand-face interaction module models the subtle
geometry and appearance dynamics that underlie common gestures. Through
experiments of novel view synthesis, self reenactment and cross-identity
reenactment, we demonstrate that InteracttAvatar can reconstruct hand and
hand-face interactions from monocular or multiview videos with high-fidelity
details and be animated with novel poses.



---

## View-Dependent Uncertainty Estimation of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-10 | Chenyu Han, Corentin Dumery | cs.CV | [PDF](http://arxiv.org/pdf/2504.07370v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene
reconstruction for its high visual accuracy. However, uncertainty estimation of
3DGS scenes remains underexplored and is crucial to downstream tasks such as
asset extraction and scene completion. Since the appearance of 3D gaussians is
view-dependent, the color of a gaussian can thus be certain from an angle and
uncertain from another. We thus propose to model uncertainty in 3DGS as an
additional view-dependent per-gaussian feature that can be modeled with
spherical harmonics. This simple yet effective modeling is easily interpretable
and can be integrated into the traditional 3DGS pipeline. It is also
significantly faster than ensemble methods while maintaining high accuracy, as
demonstrated in our experiments.



---

## ContrastiveGaussian: High-Fidelity 3D Generation with Contrastive  Learning and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-10 | Junbang Liu, Enpei Huang, Dongxing Mao, Hui Zhang, Xinyuan Song, Yongxin Ni | cs.CV | [PDF](http://arxiv.org/pdf/2504.08100v1){: .btn .btn-green } |

**Abstract**: Creating 3D content from single-view images is a challenging problem that has
attracted considerable attention in recent years. Current approaches typically
utilize score distillation sampling (SDS) from pre-trained 2D diffusion models
to generate multi-view 3D representations. Although some methods have made
notable progress by balancing generation speed and model quality, their
performance is often limited by the visual inconsistencies of the diffusion
model outputs. In this work, we propose ContrastiveGaussian, which integrates
contrastive learning into the generative process. By using a perceptual loss,
we effectively differentiate between positive and negative samples, leveraging
the visual inconsistencies to improve 3D generation quality. To further enhance
sample differentiation and improve contrastive learning, we incorporate a
super-resolution model and introduce another Quantity-Aware Triplet Loss to
address varying sample distributions during training. Our experiments
demonstrate that our approach achieves superior texture fidelity and improved
geometric consistency.

Comments:
- Code will be available at
  https://github.com/YaNLlan-ljb/ContrastiveGaussian

---

## SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Hanxiao Sun, YuPeng Gao, Jin Xie, Jian Yang, Beibei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.06815v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D assets from images, known as inverse rendering (IR),
remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting
(3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS)
tasks. Methods apply it to relighting by separating radiance into BRDF
parameters and lighting, yet produce inferior relighting quality with artifacts
and unnatural indirect illumination due to the limited capability of each
Gaussian, which has constant material parameters and normal, alongside the
absence of physical constraints for indirect lighting. In this paper, we
present a novel framework called Spatially-vayring Gaussian Inverse Rendering
(SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we
propose a new representation-Spatially-varying Gaussian (SVG)-that allows
per-Gaussian spatially varying parameters. This enhanced representation is
complemented by a SVG splatting scheme akin to vertex/fragment shading in
traditional graphics pipelines. Furthermore, we integrate a physically-based
indirect lighting model, enabling more realistic relighting. The proposed
SVG-IR framework significantly improves rendering quality, outperforming
state-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio
(PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in
relighting tasks, all while maintaining a real-time rendering speed.



---

## S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in  Remote Sensing Applications

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Masquil Elías, Marí Roger, Ehret Thibaud, Meinhardt-Llopis Enric, Musé Pablo, Facciolo Gabriele | cs.CV | [PDF](http://arxiv.org/pdf/2504.06920v1){: .btn .btn-green } |

**Abstract**: We introduce the S-EO dataset: a large-scale, high-resolution dataset,
designed to advance geometry-aware shadow detection. Collected from diverse
public-domain sources, including challenge datasets and government providers
such as USGS, our dataset comprises 702 georeferenced tiles across the USA,
each covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3
pansharpened RGB images, panchromatic images, and a ground-truth DSM of the
area obtained from LiDAR scans. For each image, we provide a shadow mask
derived from geometry and sun position, a vegetation mask based on the NDVI
index, and a bundle-adjusted RPC model. With approximately 20,000 images, the
S-EO dataset establishes a new public resource for shadow detection in remote
sensing imagery and its applications to 3D reconstruction. To demonstrate the
dataset's impact, we train and evaluate a shadow detector, showcasing its
ability to generalize, even to aerial images. Finally, we extend EO-NeRF - a
state-of-the-art NeRF approach for satellite imagery - to leverage our shadow
predictions for improved 3D reconstructions.

Comments:
- Accepted at Earthvision 2025 (CVPR Workshop)

---

## Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and  Phenotyping of Wheat Heads with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Daiwei Zhang, Joaquin Gajardo, Tomislav Medic, Isinsu Katircioglu, Mike Boss, Norbert Kirchgessner, Achim Walter, Lukas Roth | cs.CV | [PDF](http://arxiv.org/pdf/2504.06978v1){: .btn .btn-green } |

**Abstract**: Automated extraction of plant morphological traits is crucial for supporting
crop breeding and agricultural management through high-throughput field
phenotyping (HTFP). Solutions based on multi-view RGB images are attractive due
to their scalability and affordability, enabling volumetric measurements that
2D approaches cannot directly capture. While advanced methods like Neural
Radiance Fields (NeRFs) have shown promise, their application has been limited
to counting or extracting traits from only a few plants or organs. Furthermore,
accurately measuring complex structures like individual wheat heads-essential
for studying crop yields-remains particularly challenging due to occlusions and
the dense arrangement of crop canopies in field conditions. The recent
development of 3D Gaussian Splatting (3DGS) offers a promising alternative for
HTFP due to its high-quality reconstructions and explicit point-based
representation. In this paper, we present Wheat3DGS, a novel approach that
leverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance
segmentation and morphological measurement of hundreds of wheat heads
automatically, representing the first application of 3DGS to HTFP. We validate
the accuracy of wheat head extraction against high-resolution laser scan data,
obtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and
40.2% for length, width, and volume. We provide additional comparisons to
NeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating
superior results. Our approach enables rapid, non-destructive measurements of
key yield-related traits at scale, with significant implications for
accelerating crop breeding and improving our understanding of wheat
development.

Comments:
- Copyright 2025 IEEE. This is the author's version of the work. It is
  posted here for your personal use. Not for redistribution. The definitive
  version is published in the 2025 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops (CVPRW)

---

## IAAO: Interactive Affordance Learning for Articulated Objects in 3D  Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Can Zhang, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2504.06827v1){: .btn .btn-green } |

**Abstract**: This work presents IAAO, a novel framework that builds an explicit 3D model
for intelligent agents to gain understanding of articulated objects in their
environment through interaction. Unlike prior methods that rely on
task-specific networks and assumptions about movable parts, our IAAO leverages
large foundation models to estimate interactive affordances and part
articulations in three stages. We first build hierarchical features and label
fields for each object state using 3D Gaussian Splatting (3DGS) by distilling
mask features and view-consistent labels from multi-view images. We then
perform object- and part-level queries on the 3D Gaussian primitives to
identify static and articulated elements, estimating global transformations and
local articulation parameters along with affordances. Finally, scenes from
different states are merged and refined based on the estimated transformations,
enabling robust affordance-based interaction and manipulation of objects.
Experimental results demonstrate the effectiveness of our method.



---

## Stochastic Ray Tracing of 3D Transparent Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Xin Sun, Iliyan Georgiev, Yun Fei, Miloš Hašan | cs.GR | [PDF](http://arxiv.org/pdf/2504.06598v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has recently been widely adopted as a 3D representation
for novel-view synthesis, relighting, and text-to-3D generation tasks, offering
realistic and detailed results through a collection of explicit 3D Gaussians
carrying opacities and view-dependent colors. However, efficient rendering of
many transparent primitives remains a significant challenge. Existing
approaches either rasterize the 3D Gaussians with approximate sorting per view
or rely on high-end RTX GPUs to exhaustively process all ray-Gaussian
intersections (bounding Gaussians by meshes). This paper proposes a stochastic
ray tracing method to render 3D clouds of transparent primitives. Instead of
processing all ray-Gaussian intersections in sequential order, each ray
traverses the acceleration structure only once, randomly accepting and shading
a single intersection (or N intersections, using a simple extension). This
approach minimizes shading time and avoids sorting the Gaussians along the ray
while minimizing the register usage and maximizing parallelism even on low-end
GPUs. The cost of rays through the Gaussian asset is comparable to that of
standard mesh-intersection rays. While our method introduces noise, the shading
is unbiased, and the variance is slight, as stochastic acceptance is
importance-sampled based on accumulated opacity. The alignment with the Monte
Carlo philosophy simplifies implementation and easily integrates our method
into a conventional path-tracing framework.

Comments:
- 10 pages, 6 figures, 5 tables

---

## GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Anil Armagan, Albert Saà-Garriga, Bruno Manganelli, Kyuwon Kim, M. Kerim Yucel | cs.CV | [PDF](http://arxiv.org/pdf/2504.06716v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly
due to its ability to converge reasonably fast, faithfully represent the scene
and render (novel) views in a fast fashion. However, it suffers from large
storage and memory requirements, and its training speed still lags behind the
hash-grid based radiance field approaches (e.g. Instant-NGP), which makes it
especially difficult to deploy them in robotics scenarios, where 3D
reconstruction is crucial for accurate operation. In this paper, we propose
GSta that dynamically identifies Gaussians that have converged well during
training, based on their positional and color gradient norms. By forcing such
Gaussians into a siesta and stopping their updates (freezing) during training,
we improve training speed with competitive accuracy compared to state of the
art. We also propose an early stopping mechanism based on the PSNR values
computed on a subset of training images. Combined with other improvements, such
as integrating a learning rate scheduler, GSta achieves an improved Pareto
front in convergence speed, memory and storage requirements, while preserving
quality. We also show that GSta can improve other methods and complement
orthogonal approaches in efficiency improvement; once combined with Trick-GS,
GSta achieves up to 5x faster training, 16x smaller disk size compared to
vanilla GS, while having comparable accuracy and consuming only half the peak
memory. More visualisations are available at
https://anilarmagan.github.io/SRUK-GSta.

Comments:
- 9 pages. In submission to an IEEE conference

---

## Collision avoidance from monocular vision trained with novel view  synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-09 | Valentin Tordjman--Levavasseur, Stéphane Caron | cs.RO | [PDF](http://arxiv.org/pdf/2504.06651v1){: .btn .btn-green } |

**Abstract**: Collision avoidance can be checked in explicit environment models such as
elevation maps or occupancy grids, yet integrating such models with a
locomotion policy requires accurate state estimation. In this work, we consider
the question of collision avoidance from an implicit environment model. We use
monocular RGB images as inputs and train a collisionavoidance policy from
photorealistic images generated by 2D Gaussian splatting. We evaluate the
resulting pipeline in realworld experiments under velocity commands that bring
the robot on an intercept course with obstacles. Our results suggest that RGB
images can be enough to make collision-avoidance decisions, both in the room
where training data was collected and in out-of-distribution environments.



---

## Micro-splatting: Maximizing Isotropic Constraints for Refined  Optimization in 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Jee Won Lee, Hansol Lim, Sooyeun Yang, Jongseong Choi | cs.GR | [PDF](http://arxiv.org/pdf/2504.05740v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have achieved impressive
scalability and real-time rendering for large-scale scenes but often fall short
in capturing fine-grained details. Conventional approaches that rely on
relatively large covariance parameters tend to produce blurred representations,
while directly reducing covariance sizes leads to sparsity. In this work, we
introduce Micro-splatting (Maximizing Isotropic Constraints for Refined
Optimization in 3D Gaussian Splatting), a novel framework designed to overcome
these limitations. Our approach leverages a covariance regularization term to
penalize excessively large Gaussians to ensure each splat remains compact and
isotropic. This work implements an adaptive densification strategy that
dynamically refines regions with high image gradients by lowering the splitting
threshold, followed by loss function enhancement. This strategy results in a
denser and more detailed gaussian means where needed, without sacrificing
rendering efficiency. Quantitative evaluations using metrics such as L1, L2,
PSNR, SSIM, and LPIPS, alongside qualitative comparisons demonstrate that our
method significantly enhances fine-details in 3D reconstructions.



---

## InvNeRF-Seg: Fine-Tuning a Pre-Trained NeRF for 3D Object Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Jiangsan Zhao, Jakob Geipel, Krzysztof Kusnierek, Xuean Cui | cs.CV | [PDF](http://arxiv.org/pdf/2504.05751v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have been widely adopted for reconstructing
high quality 3D point clouds from 2D RGB images. However, the segmentation of
these reconstructed 3D scenes is more essential for downstream tasks such as
object counting, size estimation, and scene understanding. While segmentation
on raw 3D point clouds using deep learning requires labor intensive and
time-consuming manual annotation, directly training NeRF on binary masks also
fails due to the absence of color and shading cues essential for geometry
learning. We propose Invariant NeRF for Segmentation (InvNeRFSeg), a two step,
zero change fine tuning strategy for 3D segmentation. We first train a standard
NeRF on RGB images and then fine tune it using 2D segmentation masks without
altering either the model architecture or loss function. This approach produces
higher quality, cleaner segmented point clouds directly from the refined
radiance field with minimal computational overhead or complexity. Field density
analysis reveals consistent semantic refinement: densities of object regions
increase while background densities are suppressed, ensuring clean and
interpretable segmentations. We demonstrate InvNeRFSegs superior performance
over both SA3D and FruitNeRF on both synthetic fruit and real world soybean
datasets. This approach effectively extends 2D segmentation to high quality 3D
segmentation.



---

## SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve  Phoneme-Viseme Alignment Ambiguity

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Yihuan Huang, Jiajun Liu, Yanzhen Ren, Wuyang Liu, Juhua Tang | cs.GR | [PDF](http://arxiv.org/pdf/2504.05803v1){: .btn .btn-green } |

**Abstract**: Speech-driven talking head synthesis tasks commonly use general acoustic
features (such as HuBERT and DeepSpeech) as guided speech features. However, we
discovered that these features suffer from phoneme-viseme alignment ambiguity,
which refers to the uncertainty and imprecision in matching phonemes (speech)
with visemes (lip). To address this issue, we propose the Speech Encoder for
Lip (SE4Lip) to encode lip features from speech directly, aligning speech and
lip features in the joint embedding space by a cross-modal alignment framework.
The STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve
the fine-grained speech features. Experimental results show that SE4Lip
achieves state-of-the-art performance in both NeRF and 3DGS rendering models.
Its lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline
and produces results close to the ground truth videos.



---

## Meta-Continual Learning of Neural Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-08 | Seungyoon Woo, Junhyeog Yun, Gunhee Kim | cs.AI | [PDF](http://arxiv.org/pdf/2504.05806v1){: .btn .btn-green } |

**Abstract**: Neural Fields (NF) have gained prominence as a versatile framework for
complex data representation. This work unveils a new problem setting termed
\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel
strategy that employs a modular architecture combined with optimization-based
meta-learning. Focused on overcoming the limitations of existing methods for
continual learning of neural fields, such as catastrophic forgetting and slow
convergence, our strategy achieves high-quality reconstruction with
significantly improved learning speed. We further introduce Fisher Information
Maximization loss for neural radiance fields (FIM-NeRF), which maximizes
information gains at the sample level to enhance learning generalization, with
proved convergence guarantee and generalization bound. We perform extensive
evaluations across image, audio, video reconstruction, and view synthesis tasks
on six diverse datasets, demonstrating our method's superiority in
reconstruction quality and speed over existing MCL and CL-NF approaches.
Notably, our approach attains rapid adaptation of neural fields for city-scale
NeRF rendering with reduced parameter requirement.



---

## PanoDreamer: Consistent Text to 360-Degree Scene Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Zhexiao Xiong, Zhang Chen, Zhong Li, Yi Xu, Nathan Jacobs | cs.CV | [PDF](http://arxiv.org/pdf/2504.05152v1){: .btn .btn-green } |

**Abstract**: Automatically generating a complete 3D scene from a text description, a
reference image, or both has significant applications in fields like virtual
reality and gaming. However, current methods often generate low-quality
textures and inconsistent 3D structures. This is especially true when
extrapolating significantly beyond the field of view of the reference image. To
address these challenges, we propose PanoDreamer, a novel framework for
consistent, 3D scene generation with flexible text and image control. Our
approach employs a large language model and a warp-refine pipeline, first
generating an initial set of images and then compositing them into a 360-degree
panorama. This panorama is then lifted into 3D to form an initial point cloud.
We then use several approaches to generate additional images, from different
viewpoints, that are consistent with the initial point cloud and expand/refine
the initial point cloud. Given the resulting set of images, we utilize 3D
Gaussian Splatting to create the final 3D scene, which can then be rendered
from different viewpoints. Experiments demonstrate the effectiveness of
PanoDreamer in generating high-quality, geometrically consistent 3D scenes.

Comments:
- Accepted by CVPR 2025 Workshop on Computer Vision for Metaverse

---

## DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Wanzhou Liu, Zhexiao Xiong, Xinyu Li, Nathan Jacobs | cs.CV | [PDF](http://arxiv.org/pdf/2504.04679v1){: .btn .btn-green } |

**Abstract**: Recent novel view synthesis (NVS) techniques, including Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene
reconstruction with high-quality rendering and realistic detail recovery.
Effectively removing occlusions while preserving scene details can further
enhance the robustness and applicability of these techniques. However, existing
approaches for object and occlusion removal predominantly rely on generative
priors, which, despite filling the resulting holes, introduce new artifacts and
blurriness. Moreover, existing benchmark datasets for evaluating occlusion
removal methods lack realistic complexity and viewpoint variations. To address
these issues, we introduce DeclutterSet, a novel dataset featuring diverse
scenes with pronounced occlusions distributed across foreground, midground, and
background, exhibiting substantial relative motion across viewpoints. We
further introduce DeclutterNeRF, an occlusion removal method free from
generative priors. DeclutterNeRF introduces joint multi-view optimization of
learnable camera parameters, occlusion annealing regularization, and employs an
explainable stochastic structural similarity loss, ensuring high-quality,
artifact-free reconstructions from incomplete images. Experiments demonstrate
that DeclutterNeRF significantly outperforms state-of-the-art methods on our
proposed DeclutterSet, establishing a strong baseline for future research.

Comments:
- Accepted by CVPR 2025 4th CV4Metaverse Workshop. 15 pages, 10
  figures. Code and data at: https://github.com/wanzhouliu/declutter-nerf

---

## Let it Snow! Animating Static Gaussian Scenes With Dynamic Weather  Effects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Gal Fiebelman, Hadar Averbuch-Elor, Sagie Benaim | cs.GR | [PDF](http://arxiv.org/pdf/2504.05296v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently enabled fast and photorealistic
reconstruction of static 3D scenes. However, introducing dynamic elements that
interact naturally with such static scenes remains challenging. Accordingly, we
present a novel hybrid framework that combines Gaussian-particle
representations for incorporating physically-based global weather effects into
static 3D Gaussian Splatting scenes, correctly handling the interactions of
dynamic elements with the static scene. We follow a three-stage process: we
first map static 3D Gaussians to a particle-based representation. We then
introduce dynamic particles and simulate their motion using the Material Point
Method (MPM). Finally, we map the simulated particles back to the Gaussian
domain while introducing appearance parameters tailored for specific effects.
To correctly handle the interactions of dynamic elements with the static scene,
we introduce specialized collision handling techniques. Our approach supports a
variety of weather effects, including snowfall, rainfall, fog, and sandstorms,
and can also support falling objects, all with physically plausible motion and
appearance. Experiments demonstrate that our method significantly outperforms
existing approaches in both visual quality and physical realism.

Comments:
- Project webpage: https://galfiebelman.github.io/let-it-snow/

---

## 3D Gaussian Particle Approximation of VDB Datasets: A Study for  Scientific Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Isha Sharma, Dieter Schmalstieg | cs.GR | [PDF](http://arxiv.org/pdf/2504.04857v1){: .btn .btn-green } |

**Abstract**: The complexity and scale of Volumetric and Simulation datasets for Scientific
Visualization(SciVis) continue to grow. And the approaches and advantages of
memory-efficient data formats and storage techniques for such datasets vary.
OpenVDB library and its VDB data format excels in memory efficiency through its
hierarchical and dynamic tree structure, with active and inactive sub-trees for
data storage. It is heavily used in current production renderers for both
animation and rendering stages in VFX pipelines and photorealistic rendering of
volumes and fluids. However, it still remains to be fully leveraged in SciVis
where domains dealing with sparse scalar fields like porous media, time varying
volumes such as tornado and weather simulation or high resolution simulation of
Computational Fluid Dynamics present ample number of large challenging data
sets.Goal of this paper is not only to explore the use of OpenVDB in SciVis but
also to explore a level of detail(LOD) technique using 3D Gaussian particles
approximating voxel regions. For rendering, we utilize NVIDIA OptiX library for
ray marching through the Gaussians particles. Data modeling using 3D Gaussians
has been very popular lately due to success in stereoscopic image to 3D scene
conversion using Gaussian Splatting and Gaussian approximation and mixture
models aren't entirely new in SciVis as well. Our work explores the integration
with rendering software libraries like OpenVDB and OptiX to take advantage of
their built-in memory compaction and hardware acceleration features, while also
leveraging the performance capabilities of modern GPUs. Thus, we present a
SciVis rendering approach that uses 3D Gaussians at varying LOD in a lossy
scheme derived from VDB datasets, rather than focusing on photorealistic volume
rendering.



---

## View-Dependent Deformation Fields for 2D Editing of 3D Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Martin El Mqirmi, Noam Aigerman | cs.GR | [PDF](http://arxiv.org/pdf/2504.05544v1){: .btn .btn-green } |

**Abstract**: We propose a method for authoring non-realistic 3D objects (represented as
either 3D Gaussian Splats or meshes), that comply with 2D edits from specific
viewpoints. Namely, given a 3D object, a user chooses different viewpoints and
interactively deforms the object in the 2D image plane of each view. The method
then produces a "deformation field" - an interpolation between those 2D
deformations in a smooth manner as the viewpoint changes. Our core observation
is that the 2D deformations do not need to be tied to an underlying object, nor
share the same deformation space. We use this observation to devise a method
for authoring view-dependent deformations, holding several technical
contributions: first, a novel way to compositionality-blend between the 2D
deformations after lifting them to 3D - this enables the user to "stack" the
deformations similarly to layers in an editing software, each deformation
operating on the results of the previous; second, a novel method to apply the
3D deformation to 3D Gaussian Splats; third, an approach to author the 2D
deformations, by deforming a 2D mesh encapsulating a rendered image of the
object. We show the versatility and efficacy of our method by adding cartoonish
effects to objects, providing means to modify human characters, fitting 3D
models to given 2D sketches and caricatures, resolving occlusions, and
recreating classic non-realistic paintings as 3D models.



---

## L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Yi-Zhen Tsai, Xuechen Zhang, Zheng Li, Jiasi Chen | cs.GR | [PDF](http://arxiv.org/pdf/2504.05517v1){: .btn .btn-green } |

**Abstract**: Traditional 3D content representations include dense point clouds that
consume large amounts of data and hence network bandwidth, while newer
representations such as neural radiance fields suffer from poor frame rates due
to their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS)
can be seen as a generalization of point clouds that meet the best of both
worlds, with high visual quality and efficient rendering for real-time frame
rates. However, delivering 3DGS scenes from a hosting server to client devices
is still challenging due to high network data consumption (e.g., 1.5 GB for a
single scene). The goal of this work is to create an efficient 3D content
delivery framework that allows users to view high quality 3D scenes with 3DGS
as the underlying data representation. The main contributions of the paper are:
(1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling
algorithms to choose what splats to download at what time, and (3) Trace-driven
experiments from users wearing virtual reality headsets to evaluate the visual
quality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS
demonstrates high visual quality, achieving 16.9% higher average SSIM compared
to baselines, and also works with other compressed 3DGS representations.



---

## Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-07 | Zhicong Sun, Jacqueline Lo, Jinxing Hu | cs.RO | [PDF](http://arxiv.org/pdf/2504.04844v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping (SLAM) technology now has
photorealistic mapping capabilities thanks to the real-time high-fidelity
rendering capability of 3D Gaussian splatting (3DGS). However, due to the
static representation of scenes, current 3DGS-based SLAM encounters issues with
pose drift and failure to reconstruct accurate maps in dynamic environments. To
address this problem, we present D4DGS-SLAM, the first SLAM method based on
4DGS map representation for dynamic environments. By incorporating the temporal
dimension into scene representation, D4DGS-SLAM enables high-quality
reconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we
can obtain the dynamics, visibility, and reliability of scene points, and
filter stable static points for tracking accordingly. When optimizing Gaussian
points, we apply different isotropic regularization terms to Gaussians with
varying dynamic characteristics. Experimental results on real-world dynamic
scene datasets demonstrate that our method outperforms state-of-the-art
approaches in both camera pose tracking and map quality.

Comments:
- This paper is currently under reviewed for IROS 2025

---

## Tool-as-Interface: Learning Robot Policies from Human Tool Usage through  Imitation Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-06 | Haonan Chen, Cheng Zhu, Yunzhu Li, Katherine Driggs-Campbell | cs.RO | [PDF](http://arxiv.org/pdf/2504.04612v1){: .btn .btn-green } |

**Abstract**: Tool use is critical for enabling robots to perform complex real-world tasks,
and leveraging human tool-use data can be instrumental for teaching robots.
However, existing data collection methods like teleoperation are slow, prone to
control delays, and unsuitable for dynamic tasks. In contrast, human natural
data, where humans directly perform tasks with tools, offers natural,
unstructured interactions that are both efficient and easy to collect. Building
on the insight that humans and robots can share the same tools, we propose a
framework to transfer tool-use knowledge from human data to robots. Using two
RGB cameras, our method generates 3D reconstruction, applies Gaussian splatting
for novel view augmentation, employs segmentation models to extract
embodiment-agnostic observations, and leverages task-space tool-action
representations to train visuomotor policies. We validate our approach on
diverse real-world tasks, including meatball scooping, pan flipping, wine
bottle balancing, and other complex tasks. Our method achieves a 71\% higher
average success rate compared to diffusion policies trained with teleoperation
data and reduces data collection time by 77\%, with some tasks solvable only by
our framework. Compared to hand-held gripper, our method cuts data collection
time by 41\%. Additionally, our method bridges the embodiment gap, improves
robustness to variations in camera viewpoints and robot configurations, and
generalizes effectively across objects and spatial setups.

Comments:
- Project Page: https://tool-as-interface.github.io. 17 pages, 14
  figures

---

## Thermoxels: a voxel-based method to generate simulation-ready 3D thermal  models

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-06 | Etienne Chassaing, Florent Forest, Olga Fink, Malcolm Mielle | cs.CV | [PDF](http://arxiv.org/pdf/2504.04448v1){: .btn .btn-green } |

**Abstract**: In the European Union, buildings account for 42% of energy use and 35% of
greenhouse gas emissions. Since most existing buildings will still be in use by
2050, retrofitting is crucial for emissions reduction. However, current
building assessment methods rely mainly on qualitative thermal imaging, which
limits data-driven decisions for energy savings. On the other hand,
quantitative assessments using finite element analysis (FEA) offer precise
insights but require manual CAD design, which is tedious and error-prone.
Recent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and
Gaussian Splatting, enable precise 3D modeling from sparse images but lack
clearly defined volumes and the interfaces between them needed for FEA. We
propose Thermoxels, a novel voxel-based method able to generate FEA-compatible
models, including both geometry and temperature, from a sparse set of RGB and
thermal images. Using pairs of RGB and thermal images as input, Thermoxels
represents a scene's geometry as a set of voxels comprising color and
temperature information. After optimization, a simple process is used to
transform Thermoxels' models into tetrahedral meshes compatible with FEA. We
demonstrate Thermoxels' capability to generate RGB+Thermal meshes of 3D scenes,
surpassing other state-of-the-art methods. To showcase the practical
applications of Thermoxels' models, we conduct a simple heat conduction
simulation using FEA, achieving convergence from an initial state defined by
Thermoxels' thermal reconstruction. Additionally, we compare Thermoxels' image
synthesis abilities with current state-of-the-art methods, showing competitive
results, and discuss the limitations of existing metrics in assessing mesh
quality.

Comments:
- 7 pages, 2 figures

---

## Interpretable Single-View 3D Gaussian Splatting using Unsupervised  Hierarchical Disentangled Representation Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-05 | Yuyang Zhang, Baao Xie, Hu Zhu, Qi Wang, Huanting Guo, Xin Jin, Wenjun Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2504.04190v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has recently marked a significant advancement in 3D
reconstruction, delivering both rapid rendering and high-quality results.
However, existing 3DGS methods pose challenges in understanding underlying 3D
semantics, which hinders model controllability and interpretability. To address
it, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to
discover both coarse- and fine-grained 3D semantics via hierarchical
disentangled representation learning (DRL). Specifically, the model employs a
dual-branch architecture, consisting of a point cloud initialization branch and
a triplane-Gaussian generation branch, to achieve coarse-grained
disentanglement by separating 3D geometry and visual appearance features.
Subsequently, fine-grained semantic representations within each modality are
further discovered through DRL-based encoder-adapters. To our knowledge, this
is the first work to achieve unsupervised interpretable 3DGS. Evaluations
indicate that our model achieves 3D disentanglement while preserving
high-quality and rapid reconstruction.



---

## 3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-05 | Zhisheng Huang, Peng Wang, Jingdong Zhang, Yuan Liu, Xin Li, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.04294v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its
efficiency and quality, but like many novel view synthesis methods, it heavily
depends on accurate camera poses from Structure-from-Motion (SfM) systems.
Although recent SfM pipelines have made impressive progress, questions remain
about how to further improve both their robust performance in challenging
conditions (e.g., textureless scenes) and the precision of camera parameter
estimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework
that bridges this gap by jointly optimizing 3D Gaussians and camera parameters
from large reconstruction priors MASt3R-SfM. We note that naively performing
joint 3D Gaussian and camera optimization faces two challenges: the sensitivity
to the quality of SfM initialization, and its limited capacity for global
optimization, leading to suboptimal reconstruction results. Our 3R-GS,
overcomes these issues by incorporating optimized practices, enabling robust
scene reconstruction even with imperfect camera registration. Extensive
experiments demonstrate that 3R-GS delivers high-quality novel view synthesis
and precise camera pose estimation while remaining computationally efficient.
Project page: https://zsh523.github.io/3R-GS/



---

## NeRFlex: Resource-aware Real-time High-quality Rendering of Complex  Scenes on Mobile Devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Zhe Wang, Yifei Zhu | cs.GR | [PDF](http://arxiv.org/pdf/2504.03415v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) is a cutting-edge neural network-based
technique for novel view synthesis in 3D reconstruction. However, its
significant computational demands pose challenges for deployment on mobile
devices. While mesh-based NeRF solutions have shown potential in achieving
real-time rendering on mobile platforms, they often fail to deliver
high-quality reconstructions when rendering practical complex scenes.
Additionally, the non-negligible memory overhead caused by pre-computed
intermediate results complicates their practical application. To overcome these
challenges, we present NeRFlex, a resource-aware, high-resolution, real-time
rendering framework for complex scenes on mobile devices. NeRFlex integrates
mobile NeRF rendering with multi-NeRF representations that decompose a scene
into multiple sub-scenes, each represented by an individual NeRF network.
Crucially, NeRFlex considers both memory and computation constraints as
first-class citizens and redesigns the reconstruction process accordingly.
NeRFlex first designs a detail-oriented segmentation module to identify
sub-scenes with high-frequency details. For each NeRF network, a lightweight
profiler, built on domain knowledge, is used to accurately map configurations
to visual quality and memory usage. Based on these insights and the resource
constraints on mobile devices, NeRFlex presents a dynamic programming algorithm
to efficiently determine configurations for all NeRF representations, despite
the NP-hardness of the original decision problem. Extensive experiments on
real-world datasets and mobile devices demonstrate that NeRFlex achieves
real-time, high-quality rendering on commercial mobile devices.

Comments:
- This paper is accepted by 45th IEEE International Conference on
  Distributed Computing Systems (ICDCS 2025)

---

## HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction  via Gaussian Restoration


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Boyuan Wang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Guosheng Zhao, Chaojun Ni, Guan Huang, Lihong Liu, Xingang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2504.03536v1){: .btn .btn-green } |

**Abstract**: Single-image human reconstruction is vital for digital human modeling
applications but remains an extremely challenging task. Current approaches rely
on generative models to synthesize multi-view images for subsequent 3D
reconstruction and animation. However, directly generating multiple views from
a single human image suffers from geometric inconsistencies, resulting in
issues like fragmented or blurred limbs in the reconstructed models. To tackle
these limitations, we introduce \textbf{HumanDreamer-X}, a novel framework that
integrates multi-view human generation and reconstruction into a unified
pipeline, which significantly enhances the geometric consistency and visual
fidelity of the reconstructed 3D models. In this framework, 3D Gaussian
Splatting serves as an explicit 3D representation to provide initial geometry
and appearance priority. Building upon this foundation, \textbf{HumanFixer} is
trained to restore 3DGS renderings, which guarantee photorealistic results.
Furthermore, we delve into the inherent challenges associated with attention
mechanisms in multi-view human generation, and propose an attention modulation
strategy that effectively enhances geometric details identity consistency
across multi-view. Experimental results demonstrate that our approach markedly
improves generation and reconstruction PSNR quality metrics by 16.45% and
12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing
generalization capabilities on in-the-wild data and applicability to various
human reconstruction backbone models.

Comments:
- Project Page: https://humandreamer-x.github.io/

---

## WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-04 | Jianhao Zheng, Zihan Zhu, Valentin Bieri, Marc Pollefeys, Songyou Peng, Iro Armeni | cs.CV | [PDF](http://arxiv.org/pdf/2504.03886v1){: .btn .btn-green } |

**Abstract**: We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system
designed to handle dynamic environments by leveraging uncertainty-aware
geometric mapping. Unlike traditional SLAM systems, which assume static scenes,
our approach integrates depth and uncertainty information to enhance tracking,
mapping, and rendering performance in the presence of moving objects. We
introduce an uncertainty map, predicted by a shallow multi-layer perceptron and
DINOv2 features, to guide dynamic object removal during both tracking and
mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map
optimization, improving reconstruction accuracy. Our system is evaluated on
multiple datasets and demonstrates artifact-free view synthesis. Results
showcase WildGS-SLAM's superior performance in dynamic environments compared to
state-of-the-art methods.



---

## Compressing 3D Gaussian Splatting by Noise-Substituted Vector  Quantization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Haishan Wang, Mohammad Hassan Vali, Arno Solin | cs.CV | [PDF](http://arxiv.org/pdf/2504.03059v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D
reconstruction, achieving high-quality results with real-time radiance field
rendering. However, a key challenge is the substantial storage cost:
reconstructing a single scene typically requires millions of Gaussian splats,
each represented by 59 floating-point parameters, resulting in approximately 1
GB of memory. To address this challenge, we propose a compression method by
building separate attribute codebooks and storing only discrete code indices.
Specifically, we employ noise-substituted vector quantization technique to
jointly train the codebooks and model features, ensuring consistency between
gradient descent optimization and parameter discretization. Our method reduces
the memory consumption efficiently (around $45\times$) while maintaining
competitive reconstruction quality on standard 3D benchmark scenes. Experiments
on different codebook sizes show the trade-off between compression ratio and
image quality. Furthermore, the trained compressed model remains fully
compatible with popular 3DGS viewers and enables faster rendering speed, making
it well-suited for practical applications.

Comments:
- Appearing in Scandinavian Conference on Image Analysis (SCIA) 2025

---

## ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Yuan Zhou, Shilong Jin, Litao Hua, Wanjun Lv, Haoran Duan, Jungong Han | cs.CV | [PDF](http://arxiv.org/pdf/2504.02316v1){: .btn .btn-green } |

**Abstract**: Recent advances in zero-shot text-to-3D generation have revolutionized 3D
content creation by enabling direct synthesis from textual descriptions. While
state-of-the-art methods leverage 3D Gaussian Splatting with score distillation
to enhance multi-view rendering through pre-trained text-to-image (T2I) models,
they suffer from inherent view biases in T2I priors. These biases lead to
inconsistent 3D generation, particularly manifesting as the multi-face Janus
problem, where objects exhibit conflicting features across views. To address
this fundamental challenge, we propose ConsDreamer, a novel framework that
mitigates view bias by refining both the conditional and unconditional terms in
the score distillation process: (1) a View Disentanglement Module (VDM) that
eliminates viewpoint biases in conditional prompts by decoupling irrelevant
view components and injecting precise camera parameters; and (2) a
similarity-based partial order loss that enforces geometric consistency in the
unconditional term by aligning cosine similarities with azimuth relationships.
Extensive experiments demonstrate that ConsDreamer effectively mitigates the
multi-face Janus problem in text-to-3D generation, outperforming existing
methods in both visual quality and consistency.

Comments:
- 13 pages, 11 figures, 3 tables

---

## LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Ming-Jia Yang, Yu-Xiao Guo, Yang Liu, Bin Zhou, Xin Tong | cs.CV | [PDF](http://arxiv.org/pdf/2504.02337v1){: .btn .btn-green } |

**Abstract**: Generating realistic, room-level indoor scenes with semantically plausible
and detailed appearances from in-the-wild images is crucial for various
applications in VR, AR, and robotics. The success of NeRF-based generative
methods indicates a promising direction to address this challenge. However,
unlike their success at the object level, existing scene-level generative
methods require additional information, such as multiple views, depth images,
or semantic guidance, rather than relying solely on RGB images. This is because
NeRF-based methods necessitate prior knowledge of camera poses, which is
challenging to approximate for indoor scenes due to the complexity of defining
alignment and the difficulty of globally estimating poses from a single image,
given the unseen parts behind the camera. To address this challenge, we
redefine global poses within the framework of Local-Pose-Alignment (LPA) -- an
anchor-based multi-local-coordinate system that uses a selected number of
anchors as the roots of these coordinates. Building on this foundation, we
introduce LPA-GAN, a novel NeRF-based generative approach that incorporates
specific modifications to estimate the priors of camera poses under LPA. It
also co-optimizes the pose predictor and scene generation processes. Our
ablation study and comparisons with straightforward extensions of NeRF-based
object generative methods demonstrate the effectiveness of our approach.
Furthermore, visual comparisons with other techniques reveal that our method
achieves superior view-to-view consistency and semantic normality.



---

## MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Yash Kulthe, Andrew Gilbert, John Collomosse | cs.CV | [PDF](http://arxiv.org/pdf/2504.02517v1){: .btn .btn-green } |

**Abstract**: We present MultiNeRF, a 3D watermarking method that embeds multiple uniquely
keyed watermarks within images rendered by a single Neural Radiance Field
(NeRF) model, whilst maintaining high visual quality. Our approach extends the
TensoRF NeRF model by incorporating a dedicated watermark grid alongside the
existing geometry and appearance grids. This extension ensures higher watermark
capacity without entangling watermark signals with scene content. We propose a
FiLM-based conditional modulation mechanism that dynamically activates
watermarks based on input identifiers, allowing multiple independent watermarks
to be embedded and extracted without requiring model retraining. MultiNeRF is
validated on the NeRF-Synthetic and LLFF datasets, with statistically
significant improvements in robust capacity without compromising rendering
quality. By generalizing single-watermark NeRF methods into a flexible
multi-watermarking framework, MultiNeRF provides a scalable solution for 3D
content. attribution.



---

## MonoGS++: Fast and Accurate Monocular RGB Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Renwu Li, Wenjing Ke, Dong Li, Lu Tian, Emad Barsoum | cs.CV | [PDF](http://arxiv.org/pdf/2504.02437v1){: .btn .btn-green } |

**Abstract**: We present MonoGS++, a novel fast and accurate Simultaneous Localization and
Mapping (SLAM) method that leverages 3D Gaussian representations and operates
solely on RGB inputs. While previous 3D Gaussian Splatting (GS)-based methods
largely depended on depth sensors, our approach reduces the hardware dependency
and only requires RGB input, leveraging online visual odometry (VO) to generate
sparse point clouds in real-time. To reduce redundancy and enhance the quality
of 3D scene reconstruction, we implemented a series of methodological
enhancements in 3D Gaussian mapping. Firstly, we introduced dynamic 3D Gaussian
insertion to avoid adding redundant Gaussians in previously well-reconstructed
areas. Secondly, we introduced clarity-enhancing Gaussian densification module
and planar regularization to handle texture-less areas and flat surfaces
better. We achieved precise camera tracking results both on the synthetic
Replica and real-world TUM-RGBD datasets, comparable to those of the
state-of-the-art. Additionally, our method realized a significant 5.57x
improvement in frames per second (fps) over the previous state-of-the-art,
MonoGS.



---

## Digital-twin imaging based on descattering Gaussian splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-03 | Suguru Shimomura, Kazuki Yamanouchi, Jun Tanida | physics.optics | [PDF](http://arxiv.org/pdf/2504.02278v1){: .btn .btn-green } |

**Abstract**: Three-dimensional imaging through scattering media is important in medical
science and astronomy. We propose a digital-twin imaging method based on
Gaussian splatting to observe an object behind a scattering medium. A digital
twin model built through data assimilation, emulates the behavior of objects
and environmental changes in a virtual space. By constructing a digital twin
using point clouds composed of Gaussians and simulating the scattering process
through the convolution of a point spread function, three-dimensional objects
behind a scattering medium can be reproduced as a digital twin. In this study,
a high-contrast digital twin reproducing a three-dimensional object was
successfully constructed from degraded images, assuming that data were acquired
from wavefronts disturbed by a scattering medium. This technique reproduces
objects by integrating data processing with image measurements.



---

## Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting  Conditions with View-Adaptive Curve Adjustment

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Ziteng Cui, Xuangeng Chu, Tatsuya Harada | cs.CV | [PDF](http://arxiv.org/pdf/2504.01503v1){: .btn .btn-green } |

**Abstract**: Capturing high-quality photographs under diverse real-world lighting
conditions is challenging, as both natural lighting (e.g., low-light) and
camera exposure settings (e.g., exposure time) significantly impact image
quality. This challenge becomes more pronounced in multi-view scenarios, where
variations in lighting and image signal processor (ISP) settings across
viewpoints introduce photometric inconsistencies. Such lighting degradations
and view-dependent variations pose substantial challenges to novel view
synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel
approach to achieving high-quality novel view synthesis results under diverse
challenging lighting conditions using 3DGS. By adopting per-view color matrix
mapping and view-adaptive curve adjustments, Luminance-GS achieves
state-of-the-art (SOTA) results across various lighting conditions -- including
low-light, overexposure, and varying exposure -- while not altering the
original 3DGS explicit representation. Compared to previous NeRF- and
3DGS-based baselines, Luminance-GS provides real-time rendering speed with
improved reconstruction quality.

Comments:
- CVPR 2025, project page:
  https://cuiziteng.github.io/Luminance_GS_web/

---

## UAVTwin: Neural Digital Twins for UAVs using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Jaehoon Choi, Dongki Jung, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2504.02158v1){: .btn .btn-green } |

**Abstract**: We present UAVTwin, a method for creating digital twins from real-world
environments and facilitating data augmentation for training downstream models
embedded in unmanned aerial vehicles (UAVs). Specifically, our approach focuses
on synthesizing foreground components, such as various human instances in
motion within complex scene backgrounds, from UAV perspectives. This is
achieved by integrating 3D Gaussian Splatting (3DGS) for reconstructing
backgrounds along with controllable synthetic human models that display diverse
appearances and actions in multiple poses. To the best of our knowledge,
UAVTwin is the first approach for UAV-based perception that is capable of
generating high-fidelity digital twins based on 3DGS. The proposed work
significantly enhances downstream models through data augmentation for
real-world environments with multiple dynamic objects and significant
appearance variations-both of which typically introduce artifacts in 3DGS-based
modeling. To tackle these challenges, we propose a novel appearance modeling
strategy and a mask refinement module to enhance the training of 3D Gaussian
Splatting. We demonstrate the high quality of neural rendering by achieving a
1.23 dB improvement in PSNR compared to recent methods. Furthermore, we
validate the effectiveness of data augmentation by showing a 2.5% to 13.7%
improvement in mAP for the human detection task.



---

## FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D  Scene Reconstruction and Benchmarking

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Ulas Gunes, Matias Turkulainen, Xuqian Ren, Arno Solin, Juho Kannala, Esa Rahtu | cs.CV | [PDF](http://arxiv.org/pdf/2504.01732v1){: .btn .btn-green } |

**Abstract**: The development of large-scale 3D scene reconstruction and novel view
synthesis methods mostly rely on datasets comprising perspective images with
narrow fields of view (FoV). While effective for small-scale scenes, these
datasets require large image sets and extensive structure-from-motion (SfM)
processing, limiting scalability. To address this, we introduce a fisheye image
dataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye
lenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor
scenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense
point clouds that can be used as geometric ground-truth, enabling robust
benchmarking under challenging conditions such as occlusions and reflections.
While the baseline experiments focus on vanilla Gaussian Splatting and NeRF
based Nerfacto methods, the dataset supports diverse approaches for scene
reconstruction, novel view synthesis, and image-based rendering.

Comments:
- SCIA 2025

---

## WorldPrompter: Traversable Text-to-Scene Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Zhaoyang Zhang, Yannick Hold-Geoffroy, Miloš Hašan, Chen Ziwen, Fujun Luan, Julie Dorsey, Yiwei Hu | cs.GR | [PDF](http://arxiv.org/pdf/2504.02045v1){: .btn .btn-green } |

**Abstract**: Scene-level 3D generation is a challenging research topic, with most existing
methods generating only partial scenes and offering limited navigational
freedom. We introduce WorldPrompter, a novel generative pipeline for
synthesizing traversable 3D scenes from text prompts. We leverage panoramic
videos as an intermediate representation to model the 360{\deg} details of a
scene. WorldPrompter incorporates a conditional 360{\deg} panoramic video
generator, capable of producing a 128-frame video that simulates a person
walking through and capturing a virtual environment. The resulting video is
then reconstructed as Gaussian splats by a fast feedforward 3D reconstructor,
enabling a true walkable experience within the 3D scene. Experiments
demonstrate that our panoramic video generation model achieves convincing view
consistency across frames, enabling high-quality panoramic Gaussian splat
reconstruction and facilitating traversal over an area of the scene.
Qualitative and quantitative results also show it outperforms the
state-of-the-art 360{\deg} video generators and 3D scene generation models.



---

## Toward Real-world BEV Perception: Depth Uncertainty Estimation via  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen | cs.CV | [PDF](http://arxiv.org/pdf/2504.01957v2){: .btn .btn-green } |

**Abstract**: Bird's-eye view (BEV) perception has gained significant attention because it
provides a unified representation to fuse multiple view images and enables a
wide range of down-stream autonomous driving tasks, such as forecasting and
planning. Recent state-of-the-art models utilize projection-based methods which
formulate BEV perception as query learning to bypass explicit depth estimation.
While we observe promising advancements in this paradigm, they still fall short
of real-world applications because of the lack of uncertainty modeling and
expensive computational requirement. In this work, we introduce GaussianLSS, a
novel uncertainty-aware BEV perception framework that revisits
unprojection-based methods, specifically the Lift-Splat-Shoot (LSS) paradigm,
and enhances them with depth un-certainty modeling. GaussianLSS represents
spatial dispersion by learning a soft depth mean and computing the variance of
the depth distribution, which implicitly captures object extents. We then
transform the depth distribution into 3D Gaussians and rasterize them to
construct uncertainty-aware BEV features. We evaluate GaussianLSS on the
nuScenes dataset, achieving state-of-the-art performance compared to
unprojection-based methods. In particular, it provides significant advantages
in speed, running 2.5x faster, and in memory efficiency, using 0.3x less memory
compared to projection-based methods, while achieving competitive performance
with only a 0.4% IoU difference.

Comments:
- Accepted to CVPR'25. https://hcis-lab.github.io/GaussianLSS/

---

## High-fidelity 3D Object Generation from Single Image with RGBN-Volume  Gaussian Reconstruction Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao | cs.CV | [PDF](http://arxiv.org/pdf/2504.01512v1){: .btn .btn-green } |

**Abstract**: Recently single-view 3D generation via Gaussian splatting has emerged and
developed quickly. They learn 3D Gaussians from 2D RGB images generated from
pre-trained multi-view diffusion (MVD) models, and have shown a promising
avenue for 3D generation through a single image. Despite the current progress,
these methods still suffer from the inconsistency jointly caused by the
geometric ambiguity in the 2D images, and the lack of structure of 3D
Gaussians, leading to distorted and blurry 3D object generation. In this paper,
we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian
Reconstruction Model designed to generate high-fidelity 3D objects from
single-view images. Our key insight is a structured 3D representation can
simultaneously mitigate the afore-mentioned two issues. To this end, we propose
a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation
contains explicit 3D geometric information, eliminating the geometric ambiguity
from 2D images. It also structures Gaussians during learning so that the
optimization tends to find better local optima. Our 3D voxel representation is
obtained by a fusion module that aligns RGB features and surface normal
features, both of which can be estimated from 2D images. Extensive experiments
demonstrate the superiority of our methods over prior works in terms of
high-quality reconstruction results, robust generalization, and good
efficiency.

Comments:
- 12 pages

---

## FlowR: Flowing from Sparse to Dense 3D Reconstructions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Tobias Fischer, Samuel Rota Bulò, Yung-Hsu Yang, Nikhil Varma Keetha, Lorenzo Porzi, Norman Müller, Katja Schwarz, Jonathon Luiten, Marc Pollefeys, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2504.01647v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting enables high-quality novel view synthesis (NVS) at
real-time frame rates. However, its quality drops sharply as we depart from the
training views. Thus, dense captures are needed to match the high-quality
expectations of some applications, e.g. Virtual Reality (VR). However, such
dense captures are very laborious and expensive to obtain. Existing works have
explored using 2D generative models to alleviate this requirement by
distillation or generating additional training views. These methods are often
conditioned only on a handful of reference input views and thus do not fully
exploit the available 3D information, leading to inconsistent generation
results and reconstruction artifacts. To tackle this problem, we propose a
multi-view, flow matching model that learns a flow to connect novel view
renderings from possibly sparse reconstructions to renderings that we expect
from dense reconstructions. This enables augmenting scene captures with novel,
generated views to improve reconstruction quality. Our model is trained on a
novel dataset of 3.6M image pairs and can process up to 45 views at 540x960
resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline
consistently improves NVS in sparse- and dense-view scenarios, leading to
higher-quality reconstructions than prior works across multiple, widely-used
NVS benchmarks.

Comments:
- Project page is available at https://tobiasfshr.github.io/pub/flowr

---

## 3D Gaussian Inverse Rendering with Approximated Global Illumination

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Zirui Wu, Jianteng Chen, Laijian Li, Shaoteng Wu, Zhikai Zhu, Kang Xu, Martin R. Oswald, Jie Song | cs.GR | [PDF](http://arxiv.org/pdf/2504.01358v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting shows great potential in reconstructing photo-realistic
3D scenes. However, these methods typically bake illumination into their
representations, limiting their use for physically-based rendering and scene
editing. Although recent inverse rendering approaches aim to decompose scenes
into material and lighting components, they often rely on simplifying
assumptions that fail when editing. We present a novel approach that enables
efficient global illumination for 3D Gaussians Splatting through screen-space
ray tracing. Our key insight is that a substantial amount of indirect light can
be traced back to surfaces visible within the current view frustum. Leveraging
this observation, we augment the direct shading computed by 3D Gaussians with
Monte-Carlo screen-space ray-tracing to capture one-bounce indirect
illumination. In this way, our method enables realistic global illumination
without sacrificing the computational efficiency and editability benefits of 3D
Gaussians. Through experiments, we show that the screen-space approximation we
utilize allows for indirect illumination and supports real-time rendering and
editing. Code, data, and models will be made available at our project page:
https://wuzirui.github.io/gs-ssr.



---

## BOGausS: Better Optimized Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Stéphane Pateux, Matthieu Gendrin, Luce Morin, Théo Ladune, Xiaoran Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2504.01844v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view
synthesis. Its framework provides fast and high-fidelity rendering. Although
less complex than other solutions such as Neural Radiance Fields (NeRF), there
are still some challenges building smaller models without sacrificing quality.
In this study, we perform a careful analysis of 3DGS training process and
propose a new optimization methodology. Our Better Optimized Gaussian Splatting
(BOGausS) solution is able to generate models up to ten times lighter than the
original 3DGS with no quality degradation, thus significantly boosting the
performance of Gaussian Splatting compared to the state of the art.



---

## 3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Hao Wu, Hao Wang, Ruochong Li, Xuran Ma, Hui Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2504.01619v1){: .btn .btn-green } |

**Abstract**: Recent advancements in text-to-3D generation have shown remarkable results by
leveraging 3D priors in combination with 2D diffusion. However, previous
methods utilize 3D priors that lack detailed and complex structural
information, limiting them to generating simple objects and presenting
challenges for creating intricate structures such as bonsai. In this paper, we
propose 3DBonsai, a novel text-to-3D framework for generating 3D bonsai with
complex structures. Technically, we first design a trainable 3D space
colonization algorithm to produce bonsai structures, which are then enhanced
through random sampling and point cloud augmentation to serve as the 3D
Gaussian priors. We introduce two bonsai generation pipelines with distinct
structural levels: fine structure conditioned generation, which initializes 3D
Gaussians using a 3D structure prior to produce detailed and complex bonsai,
and coarse structure conditioned generation, which employs a multi-view
structure consistency module to align 2D and 3D structures. Moreover, we have
compiled a unified 2D and 3D Chinese-style bonsai dataset. Our experimental
results demonstrate that 3DBonsai significantly outperforms existing methods,
providing a new benchmark for structure-aware 3D bonsai generation.

Comments:
- Accepted by ICME 2025

---

## Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D  Reconstruction and Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar | cs.CV | [PDF](http://arxiv.org/pdf/2504.01960v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance
Fields (NeRF) have achieved impressive results in real-time 3D reconstruction
and novel view synthesis. However, these methods struggle in large-scale,
unconstrained environments where sparse and uneven input coverage, transient
occlusions, appearance variability, and inconsistent camera settings lead to
degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a
multi-view diffusion model to address these limitations. By generating
pseudo-observations conditioned on multi-view inputs, our method transforms
under-constrained 3D reconstruction problems into well-posed ones, enabling
robust optimization even with sparse data. GS-Diff further integrates several
enhancements, including appearance embedding, monocular depth priors, dynamic
object modeling, anisotropy regularization, and advanced rasterization
techniques, to tackle geometric and photometric challenges in real-world
settings. Experiments on four benchmarks demonstrate that GS-Diff consistently
outperforms state-of-the-art baselines by significant margins.

Comments:
- WACV ULTRRA Workshop 2025

---

## RealityAvatar: Towards Realistic Loose Clothing Modeling in Animatable  3D Gaussian Avatars

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-02 | Yahui Li, Zhi Zeng, Liming Pang, Guixuan Zhang, Shuwu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2504.01559v1){: .btn .btn-green } |

**Abstract**: Modeling animatable human avatars from monocular or multi-view videos has
been widely studied, with recent approaches leveraging neural radiance fields
(NeRFs) or 3D Gaussian Splatting (3DGS) achieving impressive results in
novel-view and novel-pose synthesis. However, existing methods often struggle
to accurately capture the dynamics of loose clothing, as they primarily rely on
global pose conditioning or static per-frame representations, leading to
oversmoothing and temporal inconsistencies in non-rigid regions. To address
this, We propose RealityAvatar, an efficient framework for high-fidelity
digital human modeling, specifically targeting loosely dressed avatars. Our
method leverages 3D Gaussian Splatting to capture complex clothing deformations
and motion dynamics while ensuring geometric consistency. By incorporating a
motion trend module and a latentbone encoder, we explicitly model
pose-dependent deformations and temporal variations in clothing behavior.
Extensive experiments on benchmark datasets demonstrate the effectiveness of
our approach in capturing fine-grained clothing deformations and motion-driven
shape variations. Our method significantly enhances structural fidelity and
perceptual quality in dynamic human reconstruction, particularly in non-rigid
regions, while achieving better consistency across temporal frames.



---

## UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene  Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Yunxuan Mao, Rong Xiong, Yue Wang, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2504.00763v1){: .btn .btn-green } |

**Abstract**: Reconstructing and decomposing dynamic urban scenes is crucial for autonomous
driving, urban planning, and scene editing. However, existing methods fail to
perform instance-aware decomposition without manual annotations, which is
crucial for instance-level scene editing.We propose UnIRe, a 3D Gaussian
Splatting (3DGS) based approach that decomposes a scene into a static
background and individual dynamic instances using only RGB images and LiDAR
point clouds. At its core, we introduce 4D superpoints, a novel representation
that clusters multi-frame LiDAR points in 4D space, enabling unsupervised
instance separation based on spatiotemporal correlations. These 4D superpoints
serve as the foundation for our decomposed 4D initialization, i.e., providing
spatial and temporal initialization to train a dynamic 3DGS for arbitrary
dynamic classes without requiring bounding boxes or object
templates.Furthermore, we introduce a smoothness regularization strategy in
both 2D and 3D space, further improving the temporal stability.Experiments on
benchmark datasets show that our method outperforms existing methods in
decomposed dynamic scene reconstruction while enabling accurate and flexible
instance-level editing, making it a practical solution for real-world
applications.



---

## Robust LiDAR-Camera Calibration with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Shuyi Zhou, Shuxiang Xie, Ryoichi Ishikawa, Takeshi Oishi | cs.RO | [PDF](http://arxiv.org/pdf/2504.00525v1){: .btn .btn-green } |

**Abstract**: LiDAR-camera systems have become increasingly popular in robotics recently. A
critical and initial step in integrating the LiDAR and camera data is the
calibration of the LiDAR-camera system. Most existing calibration methods rely
on auxiliary target objects, which often involve complex manual operations,
whereas targetless methods have yet to achieve practical effectiveness.
Recognizing that 2D Gaussian Splatting (2DGS) can reconstruct geometric
information from camera image sequences, we propose a calibration method that
estimates LiDAR-camera extrinsic parameters using geometric constraints. The
proposed method begins by reconstructing colorless 2DGS using LiDAR point
clouds. Subsequently, we update the colors of the Gaussian splats by minimizing
the photometric loss. The extrinsic parameters are optimized during this
process. Additionally, we address the limitations of the photometric loss by
incorporating the reprojection and triangulation losses, thereby enhancing the
calibration robustness and accuracy.

Comments:
- Accepted in IEEE Robotics and Automation Letters. Code available at:
  https://github.com/ShuyiZhou495/RobustCalibration

---

## Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single  Panoramic Image for Your Immerse Exploration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Zilong Huang, Jun He, Junyan Ye, Lihan Jiang, Weijia Li, Yiping Chen, Ting Han | cs.CV | [PDF](http://arxiv.org/pdf/2504.00387v1){: .btn .btn-green } |

**Abstract**: The reconstruction of immersive and realistic 3D scenes holds significant
practical importance in various fields of computer vision and computer
graphics. Typically, immersive and realistic scenes should be free from
obstructions by dynamic objects, maintain global texture consistency, and allow
for unrestricted exploration. The current mainstream methods for image-driven
scene construction involves iteratively refining the initial image using a
moving virtual camera to generate the scene. However, previous methods struggle
with visual discontinuities due to global texture inconsistencies under varying
camera poses, and they frequently exhibit scene voids caused by
foreground-background occlusions. To this end, we propose a novel layered 3D
scene reconstruction framework from panoramic image, named Scene4U.
Specifically, Scene4U integrates an open-vocabulary segmentation model with a
large language model to decompose a real panorama into multiple layers. Then,
we employs a layered repair module based on diffusion model to restore occluded
regions using visual cues and depth information, generating a hierarchical
representation of the scene. The multi-layer panorama is then initialized as a
3D Gaussian Splatting representation, followed by layered optimization, which
ultimately produces an immersive 3D scene with semantic and structural
consistency that supports free exploration. Scene4U outperforms
state-of-the-art method, improving by 24.24% in LPIPS and 24.40% in BRISQUE,
while also achieving the fastest training speed. Additionally, to demonstrate
the robustness of Scene4U and allow users to experience immersive scenes from
various landmarks, we build WorldVista3D dataset for 3D scene reconstruction,
which contains panoramic images of globally renowned sites. The implementation
code and dataset will be released at https://github.com/LongHZ140516/Scene4U .

Comments:
- CVPR 2025, 11 pages, 7 figures

---

## ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with  Multi-modal Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Qi Song, Chenghong Li, Haotong Lin, Sida Peng, Rui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2504.00437v1){: .btn .btn-green } |

**Abstract**: We present a novel approach, termed ADGaussian, for generalizable street
scene reconstruction. The proposed method enables high-quality rendering from
single-view input. Unlike prior Gaussian Splatting methods that primarily focus
on geometry refinement, we emphasize the importance of joint optimization of
image and depth features for accurate Gaussian prediction. To this end, we
first incorporate sparse LiDAR depth as an additional input modality,
formulating the Gaussian prediction process as a joint learning framework of
visual information and geometric clue. Furthermore, we propose a multi-modal
feature matching strategy coupled with a multi-scale Gaussian decoding model to
enhance the joint refinement of multi-modal features, thereby enabling
efficient multi-modal Gaussian learning. Extensive experiments on two
large-scale autonomous driving datasets, Waymo and KITTI, demonstrate that our
ADGaussian achieves state-of-the-art performance and exhibits superior
zero-shot generalization capabilities in novel-view shifting.

Comments:
- The project page can be found at
  https://maggiesong7.github.io/research/ADGaussian/

---

## Monocular and Generalizable Gaussian Talking Head Animation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Shengjie Gong, Haojie Li, Jiapeng Tang, Dongming Hu, Shuangping Huang, Hao Chen, Tianshui Chen, Zhuoman Liu | cs.CV | [PDF](http://arxiv.org/pdf/2504.00665v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce Monocular and Generalizable Gaussian Talking Head
Animation (MGGTalk), which requires monocular datasets and generalizes to
unseen identities without personalized re-training. Compared with previous 3D
Gaussian Splatting (3DGS) methods that requires elusive multi-view datasets or
tedious personalized learning/inference, MGGtalk enables more practical and
broader applications. However, in the absence of multi-view and personalized
training data, the incompleteness of geometric and appearance information poses
a significant challenge. To address these challenges, MGGTalk explores depth
information to enhance geometric and facial symmetry characteristics to
supplement both geometric and appearance features. Initially, based on the
pixel-wise geometric information obtained from depth estimation, we incorporate
symmetry operations and point cloud filtering techniques to ensure a complete
and precise position parameter for 3DGS. Subsequently, we adopt a two-stage
strategy with symmetric priors for predicting the remaining 3DGS parameters. We
begin by predicting Gaussian parameters for the visible facial regions of the
source image. These parameters are subsequently utilized to improve the
prediction of Gaussian parameters for the non-visible regions. Extensive
experiments demonstrate that MGGTalk surpasses previous state-of-the-art
methods, achieving superior performance across various metrics.

Comments:
- Accepted by CVPR 2025

---

## DropGaussian: Structural Regularization for Sparse-view Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Hyunwoo Park, Gun Ryu, Wonjun Kim | cs.CV | [PDF](http://arxiv.org/pdf/2504.00773v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in
the field of novel view synthesis due to its fast performance while yielding
the excellent image quality. However, 3DGS in sparse-view settings (e.g.,
three-view inputs) often faces with the problem of overfitting to training
views, which significantly drops the visual quality of novel view images. Many
existing approaches have tackled this issue by using strong priors, such as 2D
generative contextual information and external depth signals. In contrast, this
paper introduces a prior-free method, so-called DropGaussian, with simple
changes in 3D Gaussian splatting. Specifically, we randomly remove Gaussians
during the training process in a similar way of dropout, which allows
non-excluded Gaussians to have larger gradients while improving their
visibility. This makes the remaining Gaussians to contribute more to the
optimization process for rendering with sparse input views. Such simple
operation effectively alleviates the overfitting problem and enhances the
quality of novel view synthesis. By simply applying DropGaussian to the
original 3DGS framework, we can achieve the competitive performance with
existing prior-based 3DGS methods in sparse-view settings of benchmark datasets
without any additional complexity. The code and model are publicly available
at: https://github.com/DCVL-3D/DropGaussian release.

Comments:
- Accepted by CVPR 2025

---

## Distilling Multi-view Diffusion Models into 3D Generators

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2504.00457v3){: .btn .btn-green } |

**Abstract**: We introduce DD3G, a formulation that Distills a multi-view Diffusion model
(MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and
integrates extensive visual and spatial geometric knowledge from the MV-DM by
simulating its ordinary differential equation (ODE) trajectory, ensuring the
distilled generator generalizes better than those trained solely on 3D data.
Unlike previous amortized optimization approaches, we align the MV-DM and 3D
generator representation spaces to transfer the teacher's probabilistic flow to
the student, thus avoiding inconsistencies in optimization objectives caused by
probabilistic sampling. The introduction of probabilistic flow and the coupling
of various attributes in 3D Gaussians introduce challenges in the generation
process. To tackle this, we propose PEPD, a generator consisting of Pattern
Extraction and Progressive Decoding phases, which enables efficient fusion of
probabilistic flow and converts a single image into 3D Gaussians within 0.06
seconds. Furthermore, to reduce knowledge loss and overcome sparse-view
supervision, we design a joint optimization objective that ensures the quality
of generated samples through explicit supervision and implicit verification.
Leveraging existing 2D generation models, we compile 120k high-quality RGBA
images for distillation. Experiments on synthetic and public datasets
demonstrate the effectiveness of our method. Our project is available at:
https://qinbaigao.github.io/DD3G_project/



---

## OccludeNeRF: Geometric-aware 3D Scene Inpainting with Collaborative  Score Distillation in NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Jingyu Shi, Achleshwar Luthra, Jiazhi Li, Xiang Gao, Xiyun Song, Zongfang Lin, David Gu, Heather Yu | eess.IV | [PDF](http://arxiv.org/pdf/2504.02007v1){: .btn .btn-green } |

**Abstract**: With Neural Radiance Fields (NeRFs) arising as a powerful 3D representation,
research has investigated its various downstream tasks, including inpainting
NeRFs with 2D images. Despite successful efforts addressing the view
consistency and geometry quality, prior methods yet suffer from occlusion in
NeRF inpainting tasks, where 2D prior is severely limited in forming a faithful
reconstruction of the scene to inpaint.
  To address this, we propose a novel approach that enables cross-view
information sharing during knowledge distillation from a diffusion model,
effectively propagating occluded information across limited views.
Additionally, to align the distillation direction across multiple sampled
views, we apply a grid-based denoising strategy and incorporate additional
rendered views to enhance cross-view consistency. To assess our approach's
capability of handling occlusion cases, we construct a dataset consisting of
challenging scenes with severe occlusion, in addition to existing datasets.
Compared with baseline methods, our method demonstrates better performance in
cross-view consistency and faithfulness in reconstruction, while preserving
high rendering quality and fidelity.

Comments:
- CVPR 2025 CV4Metaverse

---

## Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Tianqi Ding, Dawei Xiang, Pablo Rivas, Liang Dong | cs.CV | [PDF](http://arxiv.org/pdf/2504.00950v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have become a popular 3D reconstruction
approach in recent years. While they produce high-quality results, they also
demand lengthy training times, often spanning days. This paper studies neural
pruning as a strategy to address these concerns. We compare pruning approaches,
including uniform sampling, importance-based methods, and coreset-based
techniques, to reduce the model size and speed up training. Our findings show
that coreset-driven pruning can achieve a 50% reduction in model size and a 35%
speedup in training, with only a slight decrease in accuracy. These results
suggest that pruning can be an effective method for improving the efficiency of
NeRF models in resource-limited settings.

Comments:
- 12 pages, 4 figures, accepted by International Conference on the AI
  Revolution: Research, Ethics, and Society (AIR-RES 2025)

---

## NeuRadar: Neural Radiance Fields for Automotive Radar Point Clouds

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-04-01 | Mahan Rafidashti, Ji Lan, Maryam Fatemi, Junsheng Fu, Lars Hammarstrand, Lennart Svensson | cs.CV | [PDF](http://arxiv.org/pdf/2504.00859v1){: .btn .btn-green } |

**Abstract**: Radar is an important sensor for autonomous driving (AD) systems due to its
robustness to adverse weather and different lighting conditions. Novel view
synthesis using neural radiance fields (NeRFs) has recently received
considerable attention in AD due to its potential to enable efficient testing
and validation but remains unexplored for radar point clouds. In this paper, we
present NeuRadar, a NeRF-based model that jointly generates radar point clouds,
camera images, and lidar point clouds. We explore set-based object detection
methods such as DETR, and propose an encoder-based solution grounded in the
NeRF geometry for improved generalizability. We propose both a deterministic
and a probabilistic point cloud representation to accurately model the radar
behavior, with the latter being able to capture radar's stochastic behavior. We
achieve realistic reconstruction results for two automotive datasets,
establishing a baseline for NeRF-based radar point cloud simulation models. In
addition, we release radar data for ZOD's Sequences and Drives to enable
further research in this field. To encourage further development of radar
NeRFs, we release the source code for NeuRadar.


