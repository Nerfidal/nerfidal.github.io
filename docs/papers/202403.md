---
layout: default
title: March 2024
parent: Papers
nav_order: 202403
---

<!---metadata--->


## Finding Waldo: Towards Efficient Exploration of NeRF Scene Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Evangelos Skartados, Mehmet Kerim Yucel, Bruno Manganelli, Anastasios Drosou, Albert Sa√†-Garriga | cs.CV | [PDF](http://arxiv.org/pdf/2403.04508v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D
reconstruction and novel view synthesis in recent years due to their remarkable
performance. Despite the huge interest in NeRF methods, a practical use case of
NeRFs has largely been ignored; the exploration of the scene space modelled by
a NeRF. In this paper, for the first time in the literature, we propose and
formally define the scene exploration framework as the efficient discovery of
NeRF model inputs (i.e. coordinates and viewing angles), using which one can
render novel views that adhere to user-selected criteria. To remedy the lack of
approaches addressing scene exploration, we first propose two baseline methods
called Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).
We then cast scene exploration as an optimization problem, and propose the
criteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient
exploration. We test all three approaches with various criteria (e.g. saliency
maximization, image quality maximization, photo-composition quality
improvement) and show that our EGPS performs more favourably than other
baselines. We finally highlight key points and limitations, and outline
directions for future research in scene exploration.

Comments:
- Accepted at ACM MMSys'24

---

## Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Nikhil Mishra, Maximilian Sieb, Pieter Abbeel, Xi Chen | cs.RO | [PDF](http://arxiv.org/pdf/2403.04114v1){: .btn .btn-green } |

**Abstract**: Deep learning methods for perception are the cornerstone of many robotic
systems. Despite their potential for impressive performance, obtaining
real-world training data is expensive, and can be impractically difficult for
some tasks. Sim-to-real transfer with domain randomization offers a potential
workaround, but often requires extensive manual tuning and results in models
that are brittle to distribution shift between sim and real. In this work, we
introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF
model that is the centerpiece of a real-to-sim pipeline for synthesizing
training data targeted to scenes and objects from the real world. COV-NeRF
extracts objects from real images and composes them into new scenes, generating
photorealistic renderings and many types of 2D and 3D supervision, including
depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the
rendering quality of modern NeRF methods, and can be used to rapidly close the
sim-to-real gap across a variety of perceptual modalities.

Comments:
- ICRA 2024

---

## Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Yuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille | eess.IV | [PDF](http://arxiv.org/pdf/2403.04116v1){: .btn .btn-green } |

**Abstract**: X-ray is widely applied for transmission imaging due to its stronger
penetration than natural light. When rendering novel view X-ray projections,
existing methods mainly based on NeRF suffer from long training time and slow
inference speed. In this paper, we propose a 3D Gaussian splatting-based
framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we
redesign a radiative Gaussian point cloud model inspired by the isotropic
nature of X-ray imaging. Our model excludes the influence of view direction
when learning to predict the radiation intensity of 3D points. Based on this
model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA
implementation. Secondly, we customize an Angle-pose Cuboid Uniform
Initialization (ACUI) strategy that directly uses the parameters of the X-ray
scanner to compute the camera information and then uniformly samples point
positions within a cuboid enclosing the scanned object. Experiments show that
our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying
less than 15% training time and over 73x inference speed. The application on
sparse-view CT reconstruction also reveals the practical values of our method.
Code and models will be publicly available at
https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training
process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .

Comments:
- The first 3D Gaussian Splatting-based method for X-ray 3D
  reconstruction

---

## DNAct: Diffusion Guided Multi-Task 3D Policy Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Ge Yan, Yueh-Hua Wu, Xiaolong Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.04115v1){: .btn .btn-green } |

**Abstract**: This paper presents DNAct, a language-conditioned multi-task policy framework
that integrates neural rendering pre-training and diffusion training to enforce
multi-modality learning in action sequence spaces. To learn a generalizable
multi-task policy with few demonstrations, the pre-training phase of DNAct
leverages neural rendering to distill 2D semantic features from foundation
models such as Stable Diffusion to a 3D space, which provides a comprehensive
semantic understanding regarding the scene. Consequently, it allows various
applications to challenging robotic tasks requiring rich 3D semantics and
accurate geometry. Furthermore, we introduce a novel approach utilizing
diffusion training to learn a vision and language feature that encapsulates the
inherent multi-modality in the multi-task demonstrations. By reconstructing the
action sequences from different tasks via the diffusion process, the model is
capable of distinguishing different modalities and thus improving the
robustness and the generalizability of the learned representation. DNAct
significantly surpasses SOTA NeRF-based multi-task manipulation approaches with
over 30% improvement in success rate. Project website: dnact.github.io.



---

## GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D  Scene Understanding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-06 | Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.03608v1){: .btn .btn-green } |

**Abstract**: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.

Comments:
- Accepted by CVPR2024

---

## Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-05 | Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2403.02751v1){: .btn .btn-green } |

**Abstract**: We present Splat-Nav, a navigation pipeline that consists of a real-time safe
planning module and a robust state estimation module designed to operate in the
Gaussian Splatting (GSplat) environment representation, a popular emerging 3D
scene representation from computer vision. We formulate rigorous collision
constraints that can be computed quickly to build a guaranteed-safe polytope
corridor through the map. We then optimize a B-spline trajectory through this
corridor. We also develop a real-time, robust state estimation module by
interpreting the GSplat representation as a point cloud. The module enables the
robot to localize its global pose with zero prior knowledge from RGB-D images
using point cloud alignment, and then track its own pose as it moves through
the scene from RGB images using image-to-point cloud localization. We also
incorporate semantics into the GSplat in order to obtain better images for
localization. All of these modules operate mainly on CPU, freeing up GPU
resources for tasks like real-time scene reconstruction. We demonstrate the
safety and robustness of our pipeline in both simulation and hardware, where we
show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude
faster than Neural Radiance Field (NeRF)-based navigation methods, thereby
enabling real-time navigation.



---

## A Deep Learning Framework for Wireless Radiation Field Reconstruction  and Channel Prediction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-05 | Haofan Lu, Christopher Vattheuer, Baharan Mirzasoleiman, Omid Abari | cs.NI | [PDF](http://arxiv.org/pdf/2403.03241v1){: .btn .btn-green } |

**Abstract**: We present NeWRF, a deep learning framework for predicting wireless channels.
Wireless channel prediction is a long-standing problem in the wireless
community and is a key technology for improving the coverage of wireless
network deployments. Today, a wireless deployment is evaluated by a site survey
which is a cumbersome process requiring an experienced engineer to perform
extensive channel measurements. To reduce the cost of site surveys, we develop
NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF).
NeWRF trains a neural network model with a sparse set of channel measurements,
and predicts the wireless channel accurately at any location in the site. We
introduce a series of techniques that integrate wireless propagation properties
into the NeRF framework to account for the fundamental differences between the
behavior of light and wireless signals. We conduct extensive evaluations of our
framework and show that our approach can accurately predict channels at
unvisited locations with significantly lower measurement density than prior
state-of-the-art



---

## DaReNeRF: Direction-aware Representation for Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-04 | Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2403.02265v1){: .btn .btn-green } |

**Abstract**: Addressing the intricate challenge of modeling and re-rendering dynamic
scenes, most recent approaches have sought to simplify these complexities using
plane-based explicit representations, overcoming the slow training time issues
associated with methods like Neural Radiance Fields (NeRF) and implicit
representations. However, the straightforward decomposition of 4D dynamic
scenes into multiple 2D plane-based representations proves insufficient for
re-rendering high-fidelity scenes with complex motions. In response, we present
a novel direction-aware representation (DaRe) approach that captures scene
dynamics from six different directions. This learned representation undergoes
an inverse dual-tree complex wavelet transformation (DTCWT) to recover
plane-based information. DaReNeRF computes features for each space-time point
by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny
MLP for color regression and leveraging volume rendering in training yield
state-of-the-art performance in novel view synthesis for complex dynamic
scenes. Notably, to address redundancy introduced by the six real and six
imaginary direction-aware wavelet coefficients, we introduce a trainable
masking approach, mitigating storage issues without significant performance
decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared
to prior art while delivering superior performance.

Comments:
- Accepted at CVPR 2024. Paper + supplementary material

---

## Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input  Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-04 | Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song | cs.CV | [PDF](http://arxiv.org/pdf/2403.02063v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis with sparse input views is important for real-world
applications like AR/VR and autonomous driving. Recent methods have integrated
depth information into NeRFs for sparse input synthesis, leveraging depth prior
for geometric and spatial understanding. However, most existing works tend to
overlook inaccuracies within depth maps and have low time efficiency. To
address these issues, we propose a depth-guided robust and fast point cloud
fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel
grid of features. A point cloud is constructed for each input view,
characterized within the voxel grid using matrices and vectors. We accumulate
the point cloud of each input view to construct the fused point cloud of the
entire scene. Each voxel determines its density and appearance by referring to
the point cloud of the entire scene. Through point cloud fusion and voxel grid
fine-tuning, inaccuracies in depth values are refined or substituted by those
from other views. Moreover, our method can achieve faster reconstruction and
greater compactness through effective vector-matrix decomposition. Experimental
results underline the superior performance and time efficiency of our approach
compared to state-of-the-art baselines.



---

## NeRF-VPT: Learning Novel View Representations with Neural Radiance  Fields via View Prompt Tuning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr | cs.CV | [PDF](http://arxiv.org/pdf/2403.01325v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have garnered remarkable success in novel view
synthesis. Nonetheless, the task of generating high-quality images for novel
views persists as a critical challenge. While the existing efforts have
exhibited commendable progress, capturing intricate details, enhancing
textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics
warrant further focused attention and advancement. In this work, we propose
NeRF-VPT, an innovative method for novel view synthesis to address these
challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning
paradigm, wherein RGB information gained from preceding rendering outcomes
serves as instructive visual prompts for subsequent rendering stages, with the
aspiration that the prior knowledge embedded in the prompts can facilitate the
gradual enhancement of rendered image quality. NeRF-VPT only requires sampling
RGB data from previous stage renderings as priors at each training stage,
without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is
plug-and-play and can be readily integrated into existing methods. By
conducting comparative analyses of our NeRF-VPT against several NeRF-based
approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,
Real Forward-Facing, Replica dataset, and a user-captured dataset, we
substantiate that our NeRF-VPT significantly elevates baseline performance and
proficiently generates more high-quality novel view images than all the
compared state-of-the-art methods. Furthermore, the cascading learning of
NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in
a significant enhancement of accuracy for sparse-view novel view synthesis. The
source code and dataset are available at
\url{https://github.com/Freedomcls/NeRF-VPT}.

Comments:
- AAAI 2024

---

## Neural radiance fields-based holography [Invited]

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba | cs.CV | [PDF](http://arxiv.org/pdf/2403.01137v1){: .btn .btn-green } |

**Abstract**: This study presents a novel approach for generating holograms based on the
neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data
is difficult in hologram computation. NeRF is a state-of-the-art technique for
3D light-field reconstruction from 2D images based on volume rendering. The
NeRF can rapidly predict new-view images that do not include a training
dataset. In this study, we constructed a rendering pipeline directly from a 3D
light field generated from 2D images by NeRF for hologram generation using deep
neural networks within a reasonable time. The pipeline comprises three main
components: the NeRF, a depth predictor, and a hologram generator, all
constructed using deep neural networks. The pipeline does not include any
physical calculations. The predicted holograms of a 3D scene viewed from any
direction were computed using the proposed pipeline. The simulation and
experimental results are presented.



---

## Neural Field Classifiers via Target Encoding and Classification Loss

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun | cs.CV | [PDF](http://arxiv.org/pdf/2403.01058v1){: .btn .btn-green } |

**Abstract**: Neural field methods have seen great progress in various long-standing tasks
in computer vision and computer graphics, including novel view synthesis and
geometry reconstruction. As existing neural field methods try to predict some
coordinate-based continuous target values, such as RGB for Neural Radiance
Field (NeRF), all of these methods are regression models and are optimized by
some regression loss. However, are regression models really better than
classification models for neural field methods? In this work, we try to visit
this very fundamental but overlooked question for neural fields from a machine
learning perspective. We successfully propose a novel Neural Field Classifier
(NFC) framework which formulates existing neural field methods as
classification tasks rather than regression tasks. The proposed NFC can easily
transform arbitrary Neural Field Regressor (NFR) into its classification
variant via employing a novel Target Encoding module and optimizing a
classification loss. By encoding a continuous regression target into a
high-dimensional discrete encoding, we naturally formulate a multi-label
classification task. Extensive experiments demonstrate the impressive
effectiveness of NFC at the nearly free extra computational costs. Moreover,
NFC also shows robustness to sparse inputs, corrupted images, and dynamic
scenes.

Comments:
- ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables

---

## DISORF: A Distributed Online NeRF Training and Rendering Framework for  Mobile Robots

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-01 | Chunlin Li, Ruofan Liang, Hanrui Fan, Zhengen Zhang, Sankeerth Durvasula, Nandita Vijaykumar | cs.RO | [PDF](http://arxiv.org/pdf/2403.00228v1){: .btn .btn-green } |

**Abstract**: We present a framework, DISORF, to enable online 3D reconstruction and
visualization of scenes captured by resource-constrained mobile robots and edge
devices. To address the limited compute capabilities of edge devices and
potentially limited network availability, we design a framework that
efficiently distributes computation between the edge device and remote server.
We leverage on-device SLAM systems to generate posed keyframes and transmit
them to remote servers that can perform high quality 3D reconstruction and
visualization at runtime by leveraging NeRF models. We identify a key challenge
with online NeRF training where naive image sampling strategies can lead to
significant degradation in rendering quality. We propose a novel shifted
exponential frame sampling method that addresses this challenge for online NeRF
training. We demonstrate the effectiveness of our framework in enabling
high-quality real-time reconstruction and visualization of unknown scenes as
they are captured and streamed from cameras in mobile robots and edge devices.


