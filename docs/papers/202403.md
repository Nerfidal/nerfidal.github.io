---
layout: default
title: March 2024
parent: Papers
nav_order: 202403
---

<!---metadata--->


## InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40  Seconds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.20309v1){: .btn .btn-green } |

**Abstract**: While novel view synthesis (NVS) has made substantial progress in 3D computer
vision, it typically requires an initial estimation of camera intrinsics and
extrinsics from dense viewpoints. This pre-processing is usually conducted via
a Structure-from-Motion (SfM) pipeline, a procedure that can be slow and
unreliable, particularly in sparse-view scenarios with insufficient matched
features for accurate reconstruction. In this work, we integrate the strengths
of point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with
end-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved
issues in NVS under unconstrained settings, which encompasses pose-free and
sparse view challenges. Our framework, InstantSplat, unifies dense stereo
priors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &
pose-free images in less than 1 minute. Specifically, InstantSplat comprises a
Coarse Geometric Initialization (CGI) module that swiftly establishes a
preliminary scene structure and camera parameters across all training views,
utilizing globally-aligned 3D point maps derived from a pre-trained dense
stereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)
module, which jointly optimizes the 3D Gaussian attributes and the initialized
poses with pose regularization. Experiments conducted on the large-scale
outdoor Tanks & Temples datasets demonstrate that InstantSplat significantly
improves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error
(ATE) by 80%. These establish InstantSplat as a viable solution for scenarios
involving posefree and sparse-view conditions. Project page:
instantsplat.github.io.



---

## HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban  Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Zhuopeng Li, Yilin Zhang, Chenming Wu, Jianke Zhu, Liangjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2403.20032v1){: .btn .btn-green } |

**Abstract**: The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural
rendering, enabling real-time production of high-quality renderings. However,
the previous 3DGS-based methods have limitations in urban scenes due to
reliance on initial Structure-from-Motion(SfM) points and difficulties in
rendering distant, sky and low-texture areas. To overcome these challenges, we
propose a hybrid optimization method named HO-Gaussian, which combines a
grid-based volume with the 3DGS pipeline. HO-Gaussian eliminates the dependency
on SfM point initialization, allowing for rendering of urban scenes, and
incorporates the Point Densitification to enhance rendering quality in
problematic regions during training. Furthermore, we introduce Gaussian
Direction Encoding as an alternative for spherical harmonics in the rendering
pipeline, which enables view-dependent color representation. To account for
multi-camera systems, we introduce neural warping to enhance object consistency
across different cameras. Experimental results on widely used autonomous
driving datasets demonstrate that HO-Gaussian achieves photo-realistic
rendering in real-time on multi-camera urban datasets.



---

## MI-NeRF: Learning a Single Face NeRF from Multiple Identities

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras | cs.CV | [PDF](http://arxiv.org/pdf/2403.19920v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce a method that learns a single dynamic neural
radiance field (NeRF) from monocular talking face videos of multiple
identities. NeRFs have shown remarkable results in modeling the 4D dynamics and
appearance of human faces. However, they require per-identity optimization.
Although recent approaches have proposed techniques to reduce the training and
rendering time, increasing the number of identities can be expensive. We
introduce MI-NeRF (multi-identity NeRF), a single unified network that models
complex non-rigid facial motion for multiple identities, using only monocular
videos of arbitrary length. The core premise in our method is to learn the
non-linear interactions between identity and non-identity specific information
with a multiplicative module. By training on multiple videos simultaneously,
MI-NeRF not only reduces the total training time compared to standard
single-identity NeRFs, but also demonstrates robustness in synthesizing novel
expressions for any input identity. We present results for both facial
expression transfer and talking face video synthesis. Our method can be further
personalized for a target identity given only a short video.

Comments:
- Project page: https://aggelinacha.github.io/MI-NeRF/

---

## Stable Surface Regularization for Fast Few-Shot NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Byeongin Joung, Byeong-Uk Lee, Jaesung Choe, Ukcheol Shin, Minjun Kang, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2403.19985v1){: .btn .btn-green } |

**Abstract**: This paper proposes an algorithm for synthesizing novel views under few-shot
setup. The main concept is to develop a stable surface regularization technique
called Annealing Signed Distance Function (ASDF), which anneals the surface in
a coarse-to-fine manner to accelerate convergence speed. We observe that the
Eikonal loss - which is a widely known geometric regularization - requires
dense training signal to shape different level-sets of SDF, leading to
low-fidelity results under few-shot training. In contrast, the proposed surface
regularization successfully reconstructs scenes and produce high-fidelity
geometry with stable training. Our method is further accelerated by utilizing
grid representation and monocular geometric priors. Finally, the proposed
approach is up to 45 times faster than existing few-shot novel view synthesis
methods, and it produces comparable results in the ScanNet dataset and
NeRF-Real dataset.

Comments:
- 3DV 2024

---

## DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Yunhao Li, Jing Wu, Lingzhe Zhao, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2403.20013v1){: .btn .btn-green } |

**Abstract**: When capturing images through the glass during rainy or snowy weather
conditions, the resulting images often contain waterdrops adhered on the glass
surface, and these waterdrops significantly degrade the image quality and
performance of many computer vision algorithms. To tackle these limitations, we
propose a method to reconstruct the clear 3D scene implicitly from multi-view
images degraded by waterdrops. Our method exploits an attention network to
predict the location of waterdrops and then train a Neural Radiance Fields to
recover the 3D scene implicitly. By leveraging the strong scene representation
capabilities of NeRF, our method can render high-quality novel-view images with
waterdrops removed. Extensive experimental results on both synthetic and real
datasets show that our method is able to generate clear 3D scenes and
outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal
methods.



---

## SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Yunhao Li, Xiaodong Wang, Ping Wang, Xin Yuan, Peidong Liu | eess.IV | [PDF](http://arxiv.org/pdf/2403.20018v1){: .btn .btn-green } |

**Abstract**: In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene representation from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, which not
only reduces storage requirements but also offers potential privacy protection.
Inspired by this, to take one step further, our approach builds upon the
powerful 3D scene representation capabilities of neural radiance fields (NeRF).
Specifically, we formulate the physical imaging process of SCI as part of the
training of NeRF, allowing us to exploit its impressive performance in
capturing complex scene structures. To assess the effectiveness of our method,
we conduct extensive evaluations using both synthetic data and real data
captured by our SCI system. Extensive experimental results demonstrate that our
proposed approach surpasses the state-of-the-art methods in terms of image
reconstruction and novel view image synthesis. Moreover, our method also
exhibits the ability to restore high frame-rate multi-view consistent images by
leveraging SCI and the rendering capabilities of NeRF. The code is available at
https://github.com/WU-CVGL/SCINeRF.



---

## Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for  Reconstructing Challenging Surfaces

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison | cs.CV | [PDF](http://arxiv.org/pdf/2403.20275v1){: .btn .btn-green } |

**Abstract**: Touch and vision go hand in hand, mutually enhancing our ability to
understand the world. From a research perspective, the problem of mixing touch
and vision is underexplored and presents interesting challenges. To this end,
we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data
(local depth maps) with multi-view vision data to achieve surface
reconstruction and novel view synthesis. Our method optimises 3D Gaussian
primitives to accurately model the object's geometry at points of contact. By
creating a framework that decreases the transmittance at touch locations, we
achieve a refined surface reconstruction, ensuring a uniformly smooth depth
map. Touch is particularly useful when considering non-Lambertian objects (e.g.
shiny or reflective surfaces) since contemporary methods tend to fail to
reconstruct with fidelity specular highlights. By combining vision and tactile
sensing, we achieve more accurate geometry reconstructions with fewer images
than prior methods. We conduct evaluation on objects with glossy and reflective
surfaces and demonstrate the effectiveness of our approach, offering
significant improvements in reconstruction quality.

Comments:
- 17 pages

---

## NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking  With Depth Completion and Denoising

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Tianchen Deng, Yanbo Wang, Hongle Xie, Hesheng Wang, Jingchuan Wang, Danwei Wang, Weidong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2403.20034v1){: .btn .btn-green } |

**Abstract**: In recent years, there have been significant advancements in 3D
reconstruction and dense RGB-D SLAM systems. One notable development is the
application of Neural Radiance Fields (NeRF) in these systems, which utilizes
implicit neural representation to encode 3D scenes. This extension of NeRF to
SLAM has shown promising results. However, the depth images obtained from
consumer-grade RGB-D sensors are often sparse and noisy, which poses
significant challenges for 3D reconstruction and affects the accuracy of the
representation of the scene geometry. Moreover, the original hierarchical
feature grid with occupancy value is inaccurate for scene geometry
representation. Furthermore, the existing methods select random pixels for
camera tracking, which leads to inaccurate localization and is not robust in
real-world indoor environments. To this end, we present NeSLAM, an advanced
framework that achieves accurate and dense depth estimation, robust camera
tracking, and realistic synthesis of novel views. First, a depth completion and
denoising network is designed to provide dense geometry prior and guide the
neural implicit representation optimization. Second, the occupancy scene
representation is replaced with Signed Distance Field (SDF) hierarchical scene
representation for high-quality reconstruction and view synthesis. Furthermore,
we also propose a NeRF-based self-supervised feature tracking algorithm for
robust real-time tracking. Experiments on various indoor datasets demonstrate
the effectiveness and accuracy of the system in reconstruction, tracking
quality, and novel view synthesis.



---

## HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation  in Urban Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Ke Wu, Kaizhao Zhang, Zhiwei Zhang, Shanshuai Yuan, Muer Tie, Julong Wei, Zijun Xu, Jieru Zhao, Zhongxue Gan, Wenchao Ding | cs.CV | [PDF](http://arxiv.org/pdf/2403.20159v1){: .btn .btn-green } |

**Abstract**: Online dense mapping of urban scenes forms a fundamental cornerstone for
scene understanding and navigation of autonomous vehicles. Recent advancements
in mapping methods are mainly based on NeRF, whose rendering speed is too slow
to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering
speed hundreds of times faster than NeRF, holds greater potential in online
dense mapping. However, integrating 3DGS into a street-view dense mapping
framework still faces two challenges, including incomplete reconstruction due
to the absence of geometric information beyond the LiDAR coverage area and
extensive computation for reconstruction in large urban scenes. To this end, we
propose HGS-Mapping, an online dense mapping framework in unbounded large-scale
scenes. To attain complete construction, our framework introduces Hybrid
Gaussian Representation, which models different parts of the entire scene using
Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian
initialization mechanism and an adaptive update method to achieve high-fidelity
and rapid reconstruction. To the best of our knowledge, we are the first to
integrate Gaussian representation into online dense mapping of urban scenes.
Our approach achieves SOTA reconstruction accuracy while only employing 66%
number of Gaussians, leading to 20% faster reconstruction speed.



---

## SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun | cs.CV | [PDF](http://arxiv.org/pdf/2403.20079v1){: .btn .btn-green } |

**Abstract**: Novel View Synthesis (NVS) for street scenes play a critical role in the
autonomous driving simulation. The current mainstream technique to achieve it
is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS). Although thrilling progress has been made, when handling
street scenes, current methods struggle to maintain rendering quality at the
viewpoint that deviates significantly from the training viewpoints. This issue
stems from the sparse training views captured by a fixed camera on a moving
vehicle. To tackle this problem, we propose a novel approach that enhances the
capacity of 3DGS by leveraging prior from a Diffusion Model along with
complementary multi-modal data. Specifically, we first fine-tune a Diffusion
Model by adding images from adjacent frames as condition, meanwhile exploiting
depth data from LiDAR point clouds to supply additional spatial information.
Then we apply the Diffusion Model to regularize the 3DGS at unseen views during
training. Experimental results validate the effectiveness of our method
compared with current state-of-the-art models, and demonstrate its advance in
rendering images from broader views.



---

## Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D  Generative Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-29 | Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2403.20153v1){: .btn .btn-green } |

**Abstract**: Recent methods for audio-driven talking head synthesis often optimize neural
radiance fields (NeRF) on a monocular talking portrait video, leveraging its
capability to render high-fidelity and 3D-consistent novel-view frames.
However, they often struggle to reconstruct complete face geometry due to the
absence of comprehensive 3D information in the input monocular videos. In this
paper, we introduce a novel audio-driven talking head synthesis framework,
called Talk3D, that can faithfully reconstruct its plausible facial geometries
by effectively adopting the pre-trained 3D-aware generative prior. Given the
personalized 3D generative model, we present a novel audio-guided attention
U-Net architecture that predicts the dynamic face variations in the NeRF space
driven by audio. Furthermore, our model is further modulated by audio-unrelated
conditioning tokens which effectively disentangle variations unrelated to audio
features. Compared to existing methods, our method excels in generating
realistic facial geometries even under extreme head poses. We also conduct
extensive experiments showing our approach surpasses state-of-the-art
benchmarks in terms of both quantitative and qualitative evaluations.

Comments:
- Project page: https://ku-cvlab.github.io/Talk3D/

---

## Sine Activated Low-Rank Matrices for Parameter Efficient Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey | cs.LG | [PDF](http://arxiv.org/pdf/2403.19243v1){: .btn .btn-green } |

**Abstract**: Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model accuracy. Our
method proves to be an adaptable enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling.
This demonstrates the wide-ranging potential and efficiency of our proposed
technique.

Comments:
- The first two authors contributed equally

---

## Mitigating Motion Blur in Neural Radiance Fields with Events and Frames

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Marco Cannici, Davide Scaramuzza | cs.CV | [PDF](http://arxiv.org/pdf/2403.19780v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have shown great potential in novel view
synthesis. However, they struggle to render sharp images when the data used for
training is affected by motion blur. On the other hand, event cameras excel in
dynamic scenes as they measure brightness changes with microsecond resolution
and are thus only marginally affected by blur. Recent methods attempt to
enhance NeRF reconstructions under camera motion by fusing frames and events.
However, they face challenges in recovering accurate color content or constrain
the NeRF to a set of predefined camera poses, harming reconstruction quality in
challenging conditions. This paper proposes a novel formulation addressing
these issues by leveraging both model- and learning-based modules. We
explicitly model the blur formation process, exploiting the event double
integral as an additional model-based prior. Additionally, we model the
event-pixel response using an end-to-end learnable response function, allowing
our method to adapt to non-idealities in the real event-camera sensor. We show,
on synthetic and real data, that the proposed approach outperforms existing
deblur NeRFs that use only frames as well as those that combine frames and
events by +6.13dB and +2.48dB, respectively.

Comments:
- IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2024

---

## SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent  Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Avinash Ummadisingu, Jongkeum Choi, Koki Yamane, Shimpei Masuda, Naoki Fukaya, Kuniyuki Takahashi | cs.RO | [PDF](http://arxiv.org/pdf/2403.19607v1){: .btn .btn-green } |

**Abstract**: Acquiring accurate depth information of transparent objects using
off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and
Robotics. Depth estimation/completion methods are typically employed and
trained on datasets with quality depth labels acquired from either simulation,
additional sensors or specialized data collection setups and known 3d models.
However, acquiring reliable depth information for datasets at scale is not
straightforward, limiting training scalability and generalization. Neural
Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide
success in novel view synthesis and shape recovery. However, heuristics and
controlled environments (lights, backgrounds, etc) are often required to
accurately capture specular surfaces. In this paper, we propose using Visual
Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to
guide the NeRF reconstruction process for these objects via the simultaneous
reconstruction of semantic fields and extensions to increase robustness. Our
proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant
performance on depth completion datasets for transparent objects and robotic
grasping.

Comments:
- 8 pages. An accompanying video is available at
  https://www.youtube.com/watch?v=S4NCoUq4bmE

---

## CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari | cs.CV | [PDF](http://arxiv.org/pdf/2403.19495v1){: .btn .btn-green } |

**Abstract**: The field of 3D reconstruction from images has rapidly evolved in the past
few years, first with the introduction of Neural Radiance Field (NeRF) and more
recently with 3D Gaussian Splatting (3DGS). The latter provides a significant
edge over NeRF in terms of the training and inference speed, as well as the
reconstruction quality. Although 3DGS works well for dense input images, the
unstructured point-cloud like representation quickly overfits to the more
challenging setup of extremely sparse input images (e.g., 3 images), creating a
representation that appears as a jumble of needles from novel views. To address
this issue, we propose regularized optimization and depth-based initialization.
Our key idea is to introduce a structured Gaussian representation that can be
controlled in 2D image space. We then constraint the Gaussians, in particular
their position, and prevent them from moving independently during optimization.
Specifically, we introduce single and multiview constraints through an implicit
convolutional decoder and a total variation loss, respectively. With the
coherency introduced to the Gaussians, we further constrain the optimization
through a flow-based loss function. To support our regularized optimization, we
propose an approach to initialize the Gaussians using monocular depth estimates
at each input view. We demonstrate significant improvements compared to the
state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.

Comments:
- Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS

---

## SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2403.19615v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian
Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs
modifying the training procedure of Gaussian splatting, our method functions at
test-time and is training-free. Specifically, SA-GS can be applied to any
pretrained Gaussian splatting field as a plugin to significantly improve the
field's anti-alising performance. The core technique is to apply 2D
scale-adaptive filters to each Gaussian during test time. As pointed out by
Mip-Splatting, observing Gaussians at different frequencies leads to mismatches
between the Gaussian scales during training and testing. Mip-Splatting resolves
this issue using 3D smoothing and 2D Mip filters, which are unfortunately not
aware of testing frequency. In this work, we show that a 2D scale-adaptive
filter that is informed of testing frequency can effectively match the Gaussian
scale, thus making the Gaussian primitive distribution remain consistent across
different testing frequencies. When scale inconsistency is eliminated, sampling
rates smaller than the scene frequency result in conventional jaggedness, and
we propose to integrate the projected 2D Gaussian within each pixel during
testing. This integration is actually a limiting case of super-sampling, which
significantly improves anti-aliasing performance over vanilla Gaussian
Splatting. Through extensive experiments using various settings and both
bounded and unbounded scenes, we show SA-GS performs comparably with or better
than Mip-Splatting. Note that super-sampling and integration are only effective
when our scale-adaptive filtering is activated. Our codes, data and models are
available at https://github.com/zsy1987/SA-GS.

Comments:
- Project page: https://kevinsong729.github.io/project-pages/SA-GS/
  Code: https://github.com/zsy1987/SA-GS

---

## TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D  DSA Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, Wenyu Liu | cs.CV | [PDF](http://arxiv.org/pdf/2403.19586v1){: .btn .btn-green } |

**Abstract**: Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical
imaging technique that provides a series of 2D images captured at different
stages and angles during the process of contrast agent filling blood vessels.
It plays a significant role in the diagnosis of cerebrovascular diseases.
Improving the rendering quality and speed under sparse sampling is important
for observing the status and location of lesions. The current methods exhibit
inadequate rendering quality in sparse views and suffer from slow rendering
speed. To overcome these limitations, we propose TOGS, a Gaussian splatting
method with opacity offset over time, which can effectively improve the
rendering quality and speed of 4D DSA. We introduce an opacity offset table for
each Gaussian to model the temporal variations in the radiance of the contrast
agent. By interpolating the opacity offset table, the opacity variation of the
Gaussian at different time points can be determined. This enables us to render
the 2D DSA image at that specific moment. Additionally, we introduced a Smooth
loss term in the loss function to mitigate overfitting issues that may arise in
the model when dealing with sparse view scenarios. During the training phase,
we randomly prune Gaussians, thereby reducing the storage overhead of the
model. The experimental results demonstrate that compared to previous methods,
this model achieves state-of-the-art reconstruction quality under the same
number of training views. Additionally, it enables real-time rendering while
maintaining low storage overhead. The code will be publicly available.



---

## GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Chongjie Ye, Yinyu Nie, Jiahao Chang, Yuantao Chen, Yihao Zhi, Xiaoguang Han | cs.CV | [PDF](http://arxiv.org/pdf/2403.19632v1){: .btn .btn-green } |

**Abstract**: We present GauStudio, a novel modular framework for modeling 3D Gaussian
Splatting (3DGS) to provide standardized, plug-and-play components for users to
easily customize and implement a 3DGS pipeline. Supported by our framework, we
propose a hybrid Gaussian representation with foreground and skyball background
models. Experiments demonstrate this representation reduces artifacts in
unbounded outdoor scenes and improves novel view synthesis. Finally, we propose
Gaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse
approach for high-fidelity mesh reconstruction from 3DGS inputs without
fine-tuning. Overall, our GauStudio framework, hybrid representation, and GauS
approach enhance 3DGS modeling and rendering capabilities, enabling
higher-quality novel view synthesis and surface reconstruction.

Comments:
- Code: https://github.com/GAP-LAB-CUHK-SZ/gaustudio

---

## Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field  Representation and Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Yujin Chen, Yinyu Nie, Benjamin Ummenhofer, Reiner Birkl, Michael Paulitsch, Matthias Müller, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2403.19319v1){: .btn .btn-green } |

**Abstract**: We present Mesh2NeRF, an approach to derive ground-truth radiance fields from
textured meshes for 3D generation tasks. Many 3D generative approaches
represent 3D scenes as radiance fields for training. Their ground-truth
radiance fields are usually fitted from multi-view renderings from a
large-scale synthetic 3D dataset, which often results in artifacts due to
occlusions or under-fitting issues. In Mesh2NeRF, we propose an analytic
solution to directly obtain ground-truth radiance fields from 3D meshes,
characterizing the density field with an occupancy function featuring a defined
surface thickness, and determining view-dependent color through a reflection
function considering both the mesh and environment lighting. Mesh2NeRF extracts
accurate radiance fields which provides direct supervision for training
generative NeRFs and single scene representation. We validate the effectiveness
of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in
PSNR for view synthesis in single scene representation on the ABO dataset, a
0.69 PSNR enhancement in the single-view conditional generation of ShapeNet
Cars, and notably improved mesh extraction from NeRF in the unconditional
generation of Objaverse Mugs.

Comments:
- Project page: https://terencecyj.github.io/projects/Mesh2NeRF/ Video:
  https://youtu.be/oufv1N3f7iY

---

## GaussianCube: Structuring Gaussian Splatting using Optimal Transport for  3D Generative Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-28 | Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo | cs.CV | [PDF](http://arxiv.org/pdf/2403.19655v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (GS) have achieved considerable improvement over Neural
Radiance Fields in terms of 3D fitting fidelity and rendering speed. However,
this unstructured representation with scattered Gaussians poses a significant
challenge for generative modeling. To address the problem, we introduce
GaussianCube, a structured GS representation that is both powerful and
efficient for generative modeling. We achieve this by first proposing a
modified densification-constrained GS fitting algorithm which can yield
high-quality fitting results using a fixed number of free Gaussians, and then
re-arranging the Gaussians into a predefined voxel grid via Optimal Transport.
The structured grid representation allows us to use standard 3D U-Net as our
backbone in diffusion generative modeling without elaborate designs. Extensive
experiments conducted on ShapeNet and OmniObject3D show that our model achieves
state-of-the-art generation results both qualitatively and quantitatively,
underscoring the potential of GaussianCube as a powerful and versatile 3D
representation.

Comments:
- Project Page: https://gaussiancube.github.io/

---

## SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable  Surface

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-27 | Jiahao Luo, Jing Liu, James Davis | cs.CV | [PDF](http://arxiv.org/pdf/2403.18784v1){: .btn .btn-green } |

**Abstract**: We present SplatFace, a novel Gaussian splatting framework designed for 3D
human face reconstruction without reliance on accurate pre-determined geometry.
Our method is designed to simultaneously deliver both high-quality novel view
rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D
Morphable Model (3DMM) to provide a surface geometric structure, making it
possible to reconstruct faces with a limited set of input images. We introduce
a joint optimization strategy that refines both the Gaussians and the morphable
surface through a synergistic non-rigid alignment process. A novel distance
metric, splat-to-surface, is proposed to improve alignment by considering both
the Gaussian position and covariance. The surface information is also utilized
to incorporate a world-space densification process, resulting in superior
reconstruction quality. Our experimental analysis demonstrates that the
proposed method is competitive with both other Gaussian splatting techniques in
novel view synthesis and other 3D reconstruction methods in producing 3D face
meshes with high geometric precision.



---

## SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable  Transient-Free 3D reconstruction from Satellite Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-27 | Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin, Bruno Vallet | cs.CV | [PDF](http://arxiv.org/pdf/2403.18711v1){: .btn .btn-green } |

**Abstract**: Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.

Comments:
- 5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP

---

## Modeling uncertainty for Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-27 | Luca Savant, Diego Valsesia, Enrico Magli | cs.CV | [PDF](http://arxiv.org/pdf/2403.18476v1){: .btn .btn-green } |

**Abstract**: We present Stochastic Gaussian Splatting (SGS): the first framework for
uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the
novel-view synthesis field by achieving impressive reconstruction quality at a
fraction of the computational cost of Neural Radiance Fields (NeRF). However,
contrary to the latter, it still lacks the ability to provide information about
the confidence associated with their outputs. To address this limitation, in
this paper, we introduce a Variational Inference-based approach that seamlessly
integrates uncertainty prediction into the common rendering pipeline of GS.
Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new
term in the loss function, enabling optimization of uncertainty estimation
alongside image reconstruction. Experimental results on the LLFF dataset
demonstrate that our method outperforms existing approaches in terms of both
image rendering quality and uncertainty estimation accuracy. Overall, our
framework equips practitioners with valuable insights into the reliability of
synthesized views, facilitating safer decision-making in real-world
applications.



---

## Gamba: Marry Gaussian Splatting with Mamba for single view 3D  reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-27 | Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.18795v2){: .btn .btn-green } |

**Abstract**: We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.



---

## DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-26 | Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala | cs.CV | [PDF](http://arxiv.org/pdf/2403.17822v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting, a novel differentiable rendering technique, has
achieved state-of-the-art novel view synthesis results with high rendering
speeds and relatively low training times. However, its performance on scenes
commonly seen in indoor datasets is poor due to the lack of geometric
constraints during optimization. We extend 3D Gaussian splatting with depth and
normal cues to tackle challenging indoor datasets and showcase techniques for
efficient mesh extraction, an important downstream application. Specifically,
we regularize the optimization procedure with depth information, enforce local
smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians
supervised by normal cues to achieve better alignment with the true scene
geometry. We improve depth estimation and novel view synthesis results over
baselines and show how this simple yet effective regularization technique can
be used to directly extract meshes from the Gaussian representation yielding
more physically accurate reconstructions on indoor scenes. Our code will be
released in https://github.com/maturk/dn-splatter.



---

## NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using  Heuristics-Guided Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-26 | Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.17537v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has been widely recognized for its excellence in
novel view synthesis and 3D scene reconstruction. However, their effectiveness
is inherently tied to the assumption of static scenes, rendering them
susceptible to undesirable artifacts when confronted with transient distractors
such as moving objects or shadows. In this work, we propose a novel paradigm,
namely "Heuristics-Guided Segmentation" (HuGS), which significantly enhances
the separation of static scenes from transient distractors by harmoniously
combining the strengths of hand-crafted heuristics and state-of-the-art
segmentation models, thus significantly transcending the limitations of
previous solutions. Furthermore, we delve into the meticulous design of
heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based
heuristics and color residual heuristics, catering to a diverse range of
texture profiles. Extensive experiments demonstrate the superiority and
robustness of our method in mitigating transient distractors for NeRFs trained
in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.

Comments:
- To appear in CVPR2024

---

## Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D  Gaussians

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-26 | Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2403.17898v1){: .btn .btn-green } |

**Abstract**: The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering
fidelity and efficiency compared to NeRF-based neural scene representations.
While demonstrating the potential for real-time rendering, 3D-GS encounters
rendering bottlenecks in large scenes with complex details due to an excessive
number of Gaussian primitives located within the viewing frustum. This
limitation is particularly noticeable in zoom-out views and can lead to
inconsistent rendering speeds in scenes with varying details. Moreover, it
often struggles to capture the corresponding level of details at different
scales with its heuristic density control operation. Inspired by the
Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an
LOD-structured 3D Gaussian approach supporting level-of-detail decomposition
for scene representation that contributes to the final rendering results. Our
model dynamically selects the appropriate level from the set of
multi-resolution anchor points, ensuring consistent rendering performance with
adaptive LOD adjustments while maintaining high-fidelity rendering results.

Comments:
- Project page: https://city-super.github.io/octree-gs/

---

## 2D Gaussian Splatting for Geometrically Accurate Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-26 | Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2403.17888v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently revolutionized radiance field
reconstruction, achieving high quality novel view synthesis and fast rendering
speed without baking. However, 3DGS fails to accurately represent surfaces due
to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian
Splatting (2DGS), a novel approach to model and reconstruct geometrically
accurate radiance fields from multi-view images. Our key idea is to collapse
the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D
Gaussians, 2D Gaussians provide view-consistent geometry while modeling
surfaces intrinsically. To accurately recover thin surfaces and achieve stable
optimization, we introduce a perspective-accurate 2D splatting process
utilizing ray-splat intersection and rasterization. Additionally, we
incorporate depth distortion and normal consistency terms to further enhance
the quality of the reconstructions. We demonstrate that our differentiable
renderer allows for noise-free and detailed geometry reconstruction while
maintaining competitive appearance quality, fast training speed, and real-time
rendering. Our code will be made publicly available.

Comments:
- 12 pages, 12 figures

---

## CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance  Fields from Sparse Inputs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-25 | Yingji Zhong, Lanqing Hong, Zhenguo Li, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2403.16885v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive capabilities for
photorealistic novel view synthesis when trained on dense inputs. However, when
trained on sparse inputs, NeRF typically encounters issues of incorrect density
or color predictions, mainly due to insufficient coverage of the scene causing
partial and sparse supervision, thus leading to significant performance
degradation. While existing works mainly consider ray-level consistency to
construct 2D learning regularization based on rendered color, depth, or
semantics on image planes, in this paper we propose a novel approach that
models 3D spatial field consistency to improve NeRF's performance with sparse
inputs. Specifically, we first adopt a voxel-based ray sampling strategy to
ensure that the sampled rays intersect with a certain voxel in 3D space. We
then randomly sample additional points within the voxel and apply a Transformer
to infer the properties of other points on each ray, which are then
incorporated into the volume rendering. By backpropagating through the
rendering loss, we enhance the consistency among neighboring points.
Additionally, we propose to use a contrastive loss on the encoder output of the
Transformer to further improve consistency within each voxel. Experiments
demonstrate that our method yields significant improvement over different
radiance fields in the sparse inputs setting, and achieves comparable
performance with current works.

Comments:
- The paper is accepted by CVPR 2024. Project page is available at
  https://zhongyingji.github.io/CVT-xRF

---

## DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-25 | Yuanze Lin, Ronald Clark, Philip Torr | cs.CV | [PDF](http://arxiv.org/pdf/2403.17237v1){: .btn .btn-green } |

**Abstract**: We present DreamPolisher, a novel Gaussian Splatting based method with
geometric guidance, tailored to learn cross-view consistency and intricate
detail from textual descriptions. While recent progress on text-to-3D
generation methods have been promising, prevailing methods often fail to ensure
view-consistency and textural richness. This problem becomes particularly
noticeable for methods that work with text input alone. To address this, we
propose a two-stage Gaussian Splatting based approach that enforces geometric
consistency among views. Initially, a coarse 3D generation undergoes refinement
via geometric optimization. Subsequently, we use a ControlNet driven refiner
coupled with the geometric consistency term to improve both texture fidelity
and overall consistency of the generated 3D asset. Empirical evaluations across
diverse textual prompts spanning various object categories demonstrate the
efficacy of DreamPolisher in generating consistent and realistic 3D objects,
aligning closely with the semantics of the textual instructions.

Comments:
- Project webpage: https://yuanze-lin.me/DreamPolisher_page/

---

## Spike-NeRF: Neural Radiance Field Based On Spike Camera

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-25 | Yijia Guo, Yuanxi Bai, Liwen Hu, Mianzhi Liu, Ziyi Guo, Lei Ma, Tiejun Huang | cs.CV | [PDF](http://arxiv.org/pdf/2403.16410v1){: .btn .btn-green } |

**Abstract**: As a neuromorphic sensor with high temporal resolution, spike cameras offer
notable advantages over traditional cameras in high-speed vision applications
such as high-speed optical estimation, depth estimation, and object tracking.
Inspired by the success of the spike camera, we proposed Spike-NeRF, the first
Neural Radiance Field derived from spike data, to achieve 3D reconstruction and
novel viewpoint synthesis of high-speed scenes. Instead of the multi-view
images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike
streams captured by a moving spike camera in a very short time. To reconstruct
a correct and stable 3D scene from high-frequency but unstable spike data, we
devised spike masks along with a distinctive loss function. We evaluate our
method qualitatively and numerically on several challenging synthetic scenes
generated by blender with the spike camera simulator. Our results demonstrate
that Spike-NeRF produces more visually appealing results than the existing
methods and the baseline we proposed in high-speed scenes. Our code and data
will be released soon.

Comments:
- This paper is accepted by ICME2024

---

## GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-25 | Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2403.16964v1){: .btn .btn-green } |

**Abstract**: Presenting a 3D scene from multiview images remains a core and long-standing
challenge in computer vision and computer graphics. Two main requirements lie
in rendering and reconstruction. Notably, SOTA rendering quality is usually
achieved with neural volumetric rendering techniques, which rely on aggregated
point/primitive-wise color and neglect the underlying scene geometry. Learning
of neural implicit surfaces is sparked from the success of neural rendering.
Current works either constrain the distribution of density fields or the shape
of primitives, resulting in degraded rendering quality and flaws on the learned
scene surfaces. The efficacy of such methods is limited by the inherent
constraints of the chosen neural representation, which struggles to capture
fine surface details, especially for larger, more intricate scenes. To address
these issues, we introduce GSDF, a novel dual-branch architecture that combines
the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS)
representation with neural Signed Distance Fields (SDF). The core idea is to
leverage and enhance the strengths of each branch while alleviating their
limitation through mutual guidance and joint supervision. We show on diverse
scenes that our design unlocks the potential for more accurate and detailed
surface reconstructions, and at the meantime benefits 3DGS rendering with
structures that are more aligned with the underlying geometry.

Comments:
- Project page: https://city-super.github.io/GSDF

---

## VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-25 | Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei | cs.CV | [PDF](http://arxiv.org/pdf/2403.17001v1){: .btn .btn-green } |

**Abstract**: Recent innovations on text-to-3D generation have featured Score Distillation
Sampling (SDS), which enables the zero-shot learning of implicit 3D models
(NeRF) by directly distilling prior knowledge from 2D diffusion models.
However, current SDS-based models still struggle with intricate text prompts
and commonly result in distorted 3D models with unrealistic textures or
cross-view inconsistency issues. In this work, we introduce a novel Visual
Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the
visual appearance knowledge in 2D visual prompt to boost text-to-3D generation.
Instead of solely supervising SDS with text prompt, VP3D first capitalizes on
2D diffusion model to generate a high-quality image from input text, which
subsequently acts as visual prompt to strengthen SDS optimization with explicit
visual appearance. Meanwhile, we couple the SDS optimization with additional
differentiable reward function that encourages rendering images of 3D models to
better visually align with 2D visual prompt and semantically match with text
prompt. Through extensive experiments, we show that the 2D Visual Prompt in our
VP3D significantly eases the learning of visual appearance of 3D models and
thus leads to higher visual fidelity with more detailed textures. It is also
appealing in view that when replacing the self-generating visual prompt with a
given reference image, VP3D is able to trigger a new task of stylized
text-to-3D generation. Our project page is available at
https://vp3d-cvpr24.github.io.

Comments:
- CVPR 2024; Project page: https://vp3d-cvpr24.github.io

---

## Semantic Is Enough: Only Semantic Information For NeRF Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-24 | Ruibo Wang, Song Zhang, Ping Huang, Donghai Zhang, Wei Yan | cs.CV | [PDF](http://arxiv.org/pdf/2403.16043v1){: .btn .btn-green } |

**Abstract**: Recent research that combines implicit 3D representation with semantic
information, like Semantic-NeRF, has proven that NeRF model could perform
excellently in rendering 3D structures with semantic labels. This research aims
to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing
solely on semantic output and removing the RGB output component. We reformulate
the model and its training procedure to leverage only the cross-entropy loss
between the model semantic output and the ground truth semantic images,
removing the colour data traditionally used in the original Semantic-NeRF
approach. We then conduct a series of identical experiments using the original
and the modified Semantic-NeRF model. Our primary objective is to obverse the
impact of this modification on the model performance by Semantic-NeRF, focusing
on tasks such as scene understanding, object detection, and segmentation. The
results offer valuable insights into the new way of rendering the scenes and
provide an avenue for further research and development in semantic-focused 3D
scene understanding.



---

## Inverse Rendering of Glossy Objects via the Neural Plenoptic Function  and Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-24 | Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau | cs.CV | [PDF](http://arxiv.org/pdf/2403.16224v1){: .btn .btn-green } |

**Abstract**: Inverse rendering aims at recovering both geometry and materials of objects.
It provides a more compatible reconstruction for conventional rendering
engines, compared with the neural radiance fields (NeRFs). On the other hand,
existing NeRF-based inverse rendering methods cannot handle glossy objects with
local light interactions well, as they typically oversimplify the illumination
as a 2D environmental map, which assumes infinite lights only. Observing the
superiority of NeRFs in recovering radiance fields, we propose a novel 5D
Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more
accurate lighting-object interactions can be formulated via the rendering
equation. We also design a material-aware cone sampling strategy to efficiently
integrate lights inside the BRDF lobes with the help of pre-filtered radiance
fields. Our method has two stages: the geometry of the target object and the
pre-filtered environmental radiance fields are reconstructed in the first
stage, and materials of the target object are estimated in the second stage
with the proposed NeP and material-aware cone sampling strategy. Extensive
experiments on the proposed real-world and synthetic datasets demonstrate that
our method can reconstruct high-fidelity geometry/materials of challenging
glossy objects with complex lighting interactions from nearby objects. Project
webpage: https://whyy.site/paper/nep

Comments:
- CVPR 2024 paper. Project webpage https://whyy.site/paper/nep

---

## latentSplat: Autoencoding Variational Gaussians for Fast Generalizable  3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-24 | Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen | cs.CV | [PDF](http://arxiv.org/pdf/2403.16292v1){: .btn .btn-green } |

**Abstract**: We present latentSplat, a method to predict semantic Gaussians in a 3D latent
space that can be splatted and decoded by a light-weight generative 2D
architecture. Existing methods for generalizable 3D reconstruction either do
not enable fast inference of high resolution novel views due to slow volume
rendering, or are limited to interpolation of close input views, even in
simpler settings with a single central object, where 360-degree generalization
is possible. In this work, we combine a regression-based approach with a
generative model, moving towards both of these capabilities within the same
method, trained purely on readily available real video data. The core of our
method are variational 3D Gaussians, a representation that efficiently encodes
varying uncertainty within a latent space consisting of 3D feature Gaussians.
From these Gaussians, specific instances can be sampled and rendered via
efficient Gaussian splatting and a fast, generative decoder network. We show
that latentSplat outperforms previous works in reconstruction quality and
generalization, while being fast and scalable to high-resolution data.

Comments:
- Project website: https://geometric-rl.mpi-inf.mpg.de/latentsplat/

---

## Are NeRFs ready for autonomous driving? Towards closing the  real-to-simulation gap

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-24 | Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson | cs.CV | [PDF](http://arxiv.org/pdf/2403.16092v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing
autonomous driving (AD) research, offering scalable closed-loop simulation and
data augmentation capabilities. However, to trust the results achieved in
simulation, one needs to ensure that AD systems perceive real and rendered data
in the same way. Although the performance of rendering methods is increasing,
many scenarios will remain inherently challenging to reconstruct faithfully. To
this end, we propose a novel perspective for addressing the real-to-simulated
data gap. Rather than solely focusing on improving rendering fidelity, we
explore simple yet effective methods to enhance perception model robustness to
NeRF artifacts without compromising performance on real data. Moreover, we
conduct the first large-scale investigation into the real-to-simulated data gap
in an AD setting using a state-of-the-art neural rendering technique.
Specifically, we evaluate object detectors and an online mapping model on real
and simulated data, and study the effects of different pre-training strategies.
Our results show notable improvements in model robustness to simulated data,
even improving real-world performance in some cases. Last, we delve into the
correlation between the real-to-simulated gap and image reconstruction metrics,
identifying FID and LPIPS as strong indicators.



---

## CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D  Gaussian Field

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-24 | Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui | cs.CV | [PDF](http://arxiv.org/pdf/2403.16095v1){: .btn .btn-green } |

**Abstract**: Recently neural radiance fields (NeRF) have been widely exploited as 3D
representations for dense simultaneous localization and mapping (SLAM). Despite
their notable successes in surface modeling and novel view synthesis, existing
NeRF-based methods are hindered by their computationally intensive and
time-consuming volume rendering pipeline. This paper presents an efficient
dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D
Gaussian field with high consistency and geometric stability. Through an
in-depth analysis of Gaussian Splatting, we propose several techniques to
construct a consistent and stable 3D Gaussian field suitable for tracking and
mapping. Additionally, a novel depth uncertainty model is proposed to ensure
the selection of valuable Gaussian primitives during optimization, thereby
improving tracking efficiency and accuracy. Experiments on various datasets
demonstrate that CG-SLAM achieves superior tracking and mapping performance
with a notable tracking speed of up to 15 Hz. We will make our source code
publicly available. Project page: https://zju3dv.github.io/cg-slam.

Comments:
- Project Page: https://zju3dv.github.io/cg-slam

---

## Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-24 | Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa | cs.CV | [PDF](http://arxiv.org/pdf/2403.16141v1){: .btn .btn-green } |

**Abstract**: Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic
scenes often involve explicit modeling of scene dynamics. However, this
approach faces challenges in modeling scene dynamics in urban environments,
where moving objects of various categories and scales are present. In such
settings, it becomes crucial to effectively eliminate moving objects to
accurately reconstruct static backgrounds. Our research introduces an
innovative method, termed here as Entity-NeRF, which combines the strengths of
knowledge-based and statistical strategies. This approach utilizes entity-wise
statistics, leveraging entity segmentation and stationary entity classification
through thing/stuff segmentation. To assess our methodology, we created an
urban scene dataset masked with moving objects. Our comprehensive experiments
demonstrate that Entity-NeRF notably outperforms existing techniques in
removing moving objects and reconstructing static urban backgrounds, both
quantitatively and qualitatively.

Comments:
- Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2024), Project website:
  https://otonari726.github.io/entitynerf/

---

## PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic  Human Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-24 | Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.16080v2){: .btn .btn-green } |

**Abstract**: High-quality human reconstruction and photo-realistic rendering of a dynamic
scene is a long-standing problem in computer vision and graphics. Despite
considerable efforts invested in developing various capture systems and
reconstruction algorithms, recent advancements still struggle with loose or
oversized clothing and overly complex poses. In part, this is due to the
challenges of acquiring high-quality human datasets. To facilitate the
development of these fields, in this paper, we present PKU-DyMVHumans, a
versatile human-centric dataset for high-fidelity reconstruction and rendering
of dynamic human scenarios from dense multi-view videos. It comprises 8.2
million frames captured by more than 56 synchronized cameras across diverse
scenarios. These sequences comprise 32 human subjects across 45 different
scenarios, each with a high-detailed appearance and realistic human motion.
Inspired by recent advancements in neural radiance field (NeRF)-based scene
representations, we carefully set up an off-the-shelf framework that is easy to
provide those state-of-the-art NeRF-based implementations and benchmark on
PKU-DyMVHumans dataset. It is paving the way for various applications like
fine-grained foreground/background decomposition, high-quality human
reconstruction and photo-realistic novel view synthesis of a dynamic scene.
Extensive studies are performed on the benchmark, demonstrating new
observations and challenges that emerge from using such high-fidelity dynamic
data. The dataset is available at: https://pku-dymvhumans.github.io.



---

## Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-24 | Junhong Zhao, Wei Ying, Yaoqiang Pan, Zhenfeng Yi, Chao Chen, Kewei Hu, Hanwen Kang | cs.CV | [PDF](http://arxiv.org/pdf/2403.15981v2){: .btn .btn-green } |

**Abstract**: Accurate collection of plant phenotyping is critical to optimising
sustainable farming practices in precision agriculture. Traditional phenotyping
in controlled laboratory environments, while valuable, falls short in
understanding plant growth under real-world conditions. Emerging sensor and
digital technologies offer a promising approach for direct phenotyping of
plants in farm environments. This study investigates a learning-based
phenotyping method using the Neural Radiance Field to achieve accurate in-situ
phenotyping of pepper plants in greenhouse environments. To quantitatively
evaluate the performance of this method, traditional point cloud registration
on 3D scanning data is implemented for comparison. Experimental result shows
that NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the
3D scanning methods. The mean distance error between the scanner-based method
and the NeRF-based method is 0.865mm. This study shows that the learning-based
NeRF method achieves similar accuracy to 3D scanning-based methods but with
improved scalability and robustness.



---

## UPNeRF: A Unified Framework for Monocular 3D Object Reconstruction and  Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-23 | Yuliang Guo, Abhinav Kumar, Cheng Zhao, Ruoyu Wang, Xinyu Huang, Liu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2403.15705v1){: .btn .btn-green } |

**Abstract**: Monocular 3D reconstruction for categorical objects heavily relies on
accurately perceiving each object's pose. While gradient-based optimization
within a NeRF framework updates initially given poses, this paper highlights
that such a scheme fails when the initial pose even moderately deviates from
the true pose. Consequently, existing methods often depend on a third-party 3D
object to provide an initial object pose, leading to increased complexity and
generalization issues. To address these challenges, we present UPNeRF, a
Unified framework integrating Pose estimation and NeRF-based reconstruction,
bringing us closer to real-time monocular 3D object reconstruction. UPNeRF
decouples the object's dimension estimation and pose refinement to resolve the
scale-depth ambiguity, and introduces an effective projected-box representation
that generalizes well cross different domains. While using a dedicated pose
estimator that smoothly integrates into an object-centric NeRF, UPNeRF is free
from external 3D detectors. UPNeRF achieves state-of-the-art results in both
reconstruction and pose estimation tasks on the nuScenes dataset. Furthermore,
UPNeRF exhibits exceptional Cross-dataset generalization on the KITTI and Waymo
datasets, surpassing prior methods with up to 50% reduction in rotation and
translation error.



---

## Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image  Collections

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-23 | Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.15704v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from unconstrained in-the-wild images remains a
meaningful but challenging task. The photometric variation and transient
occluders in those unconstrained images make it difficult to reconstruct the
original scene accurately. Previous approaches tackle the problem by
introducing a global appearance feature in Neural Radiance Fields (NeRF).
However, in the real world, the unique appearance of each tiny point in a scene
is determined by its independent intrinsic material attributes and the varying
environmental impacts it receives. Inspired by this fact, we propose Gaussian
in the wild (GS-W), a method that uses 3D Gaussian points to reconstruct the
scene and introduces separated intrinsic and dynamic appearance feature for
each point, capturing the unchanged scene appearance along with dynamic
variation like illumination and weather. Additionally, an adaptive sampling
strategy is presented to allow each Gaussian point to focus on the local and
detailed information more effectively. We also reduce the impact of transient
occluders using a 2D visibility map. More experiments have demonstrated better
reconstruction quality and details of GS-W compared to previous methods, with a
$1000\times$ increase in rendering speed.

Comments:
- 14 pages, 5 figures

---

## DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving  Environment for Real-World Performance Validation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-23 | Mu-Yi Shen, Chia-Chi Hsu, Hao-Yu Hou, Yu-Chen Huang, Wei-Fang Sun, Chia-Che Chang, Yu-Lun Liu, Chun-Yi Lee | cs.RO | [PDF](http://arxiv.org/pdf/2403.15791v1){: .btn .btn-green } |

**Abstract**: In this study, we introduce the DriveEnv-NeRF framework, which leverages
Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting
of the efficacy of autonomous driving agents in a targeted real-world scene.
Standard simulator-based rendering often fails to accurately reflect real-world
performance due to the sim-to-real gap, which represents the disparity between
virtual simulations and real-world conditions. To mitigate this gap, we propose
a workflow for building a high-fidelity simulation environment of the targeted
real-world scene using NeRF. This approach is capable of rendering realistic
images from novel viewpoints and constructing 3D meshes for emulating
collisions. The validation of these capabilities through the comparison of
success rates in both simulated and real environments demonstrates the benefits
of using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the
DriveEnv-NeRF framework can serve as a training environment for autonomous
driving agents under various lighting conditions. This approach enhances the
robustness of the agents and reduces performance degradation when deployed to
the target real scene, compared to agents fully trained using the standard
simulator rendering pipeline.

Comments:
- Project page: https://github.com/muyishen2040/DriveEnvNeRF

---

## STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-22 | Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2403.14939v1){: .btn .btn-green } |

**Abstract**: Recent progress in pre-trained diffusion models and 3D generation have
spurred interest in 4D content creation. However, achieving high-fidelity 4D
generation with spatial-temporal consistency remains a challenge. In this work,
we propose STAG4D, a novel framework that combines pre-trained diffusion models
with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing
inspiration from 3D generation techniques, we utilize a multi-view diffusion
model to initialize multi-view images anchoring on the input video frames,
where the video can be either real-world captured or generated by a video
diffusion model. To ensure the temporal consistency of the multi-view sequence
initialization, we introduce a simple yet effective fusion strategy to leverage
the first frame as a temporal anchor in the self-attention computation. With
the almost consistent multi-view sequences, we then apply the score
distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian
spatting is specially crafted for the generation task, where an adaptive
densification strategy is proposed to mitigate the unstable Gaussian gradient
for robust optimization. Notably, the proposed pipeline does not require any
pre-training or fine-tuning of diffusion networks, offering a more accessible
and practical solution for the 4D generation task. Extensive experiments
demonstrate that our method outperforms prior 4D generation works in rendering
quality, spatial-temporal consistency, and generation robustness, setting a new
state-of-the-art for 4D generation from diverse inputs, including text, image,
and video.



---

## Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-22 | Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, Qing Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.15624v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D scene understanding presents a significant challenge in
computer vision, withwide-ranging applications in embodied agents and augmented
reality systems. Previous approaches haveadopted Neural Radiance Fields (NeRFs)
to analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel
open-vocabulary scene understanding approach based on 3D Gaussian Splatting.
Our keyidea is distilling pre-trained 2D semantics into 3D Gaussians. We design
a versatile projection approachthat maps various 2Dsemantic features from
pre-trained image encoders into a novel semantic component of 3D Gaussians,
withoutthe additional training required by NeRFs. We further build a 3D
semantic network that directly predictsthe semantic component from raw 3D
Gaussians for fast inference. We explore several applications ofSemantic
Gaussians: semantic segmentation on ScanNet-20, where our approach attains a
4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene
understanding counterparts; object part segmentation,sceneediting, and
spatial-temporal segmentation with better qualitative results over 2D and 3D
baselines,highlighting its versatility and effectiveness on supporting diverse
downstream tasks.

Comments:
- Project page: see https://semantic-gaussians.github.io

---

## Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-22 | Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2403.15530v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis
results while advancing real-time rendering performance. However, it relies
heavily on the quality of the initial point cloud, resulting in blurring and
needle-like artifacts in areas with insufficient initializing points. This is
mainly attributed to the point cloud growth condition in 3DGS that only
considers the average gradient magnitude of points from observable views,
thereby failing to grow for large Gaussians that are observable for many
viewpoints while many of them are only covered in the boundaries. To this end,
we propose a novel method, named Pixel-GS, to take into account the number of
pixels covered by the Gaussian in each view during the computation of the
growth condition. We regard the covered pixel numbers as the weights to
dynamically average the gradients from different views, such that the growth of
large Gaussians can be prompted. As a result, points within the areas with
insufficient initializing points can be grown more effectively, leading to a
more accurate and detailed reconstruction. In addition, we propose a simple yet
effective strategy to scale the gradient field according to the distance to the
camera, to suppress the growth of floaters near the camera. Extensive
experiments both qualitatively and quantitatively demonstrate that our method
achieves state-of-the-art rendering quality while maintaining real-time
rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.



---

## EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic  Surgeries using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-22 | Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen | cs.CV | [PDF](http://arxiv.org/pdf/2403.15124v1){: .btn .btn-green } |

**Abstract**: Precise camera tracking, high-fidelity 3D tissue reconstruction, and
real-time online visualization are critical for intrabody medical imaging
devices such as endoscopes and capsule robots. However, existing SLAM
(Simultaneous Localization and Mapping) methods often struggle to achieve both
complete high-quality surgical field reconstruction and efficient computation,
restricting their intraoperative applications among endoscopic surgeries. In
this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic
surgeries, which integrates streamlined Gaussian representation and
differentiable rasterization to facilitate over 100 fps rendering speed during
online camera tracking and tissue reconstructing. Extensive experiments show
that EndoGSLAM achieves a better trade-off between intraoperative availability
and reconstruction quality than traditional or neural SLAM approaches, showing
tremendous potential for endoscopic surgeries. The project page is at
https://EndoGSLAM.loping151.com



---

## WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-22 | Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni | cs.CV | [PDF](http://arxiv.org/pdf/2403.15272v1){: .btn .btn-green } |

**Abstract**: Despite the advancements in deep learning for camera relocalization tasks,
obtaining ground truth pose labels required for the training process remains a
costly endeavor. While current weakly supervised methods excel in lightweight
label generation, their performance notably declines in scenarios with sparse
views. In response to this challenge, we introduce WSCLoc, a system capable of
being customized to various deep learning-based relocalization models to
enhance their performance under weakly-supervised and sparse view conditions.
This is realized with two stages. In the initial stage, WSCLoc employs a
multilayer perceptron-based structure called WFT-NeRF to co-optimize image
reconstruction quality and initial pose information. To ensure a stable
learning process, we incorporate temporal information as input. Furthermore,
instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to
explicitly enforce a scale constraint. In the second stage, we co-optimize the
pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by
Time-Encoding based Random View Synthesis and supervised by inter-frame
geometric constraints that consider pose, depth, and RGB information. We
validate our approaches on two publicly available datasets, one outdoor and one
indoor. Our experimental results demonstrate that our weakly-supervised
relocalization solutions achieve superior pose estimation accuracy in
sparse-view scenarios, comparable to state-of-the-art camera relocalization
methods. We will make our code publicly available.



---

## Gaussian Frosting: Editable Complex Radiance Fields with Real-Time  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Antoine Guédon, Vincent Lepetit | cs.CV | [PDF](http://arxiv.org/pdf/2403.14554v1){: .btn .btn-green } |

**Abstract**: We propose Gaussian Frosting, a novel mesh-based representation for
high-quality rendering and editing of complex 3D effects in real-time. Our
approach builds on the recent 3D Gaussian Splatting framework, which optimizes
a set of 3D Gaussians to approximate a radiance field from images. We propose
first extracting a base mesh from Gaussians during optimization, then building
and refining an adaptive layer of Gaussians with a variable thickness around
the mesh to better capture the fine details and volumetric effects near the
surface, such as hair or grass. We call this layer Gaussian Frosting, as it
resembles a coating of frosting on a cake. The fuzzier the material, the
thicker the frosting. We also introduce a parameterization of the Gaussians to
enforce them to stay inside the frosting layer and automatically adjust their
parameters when deforming, rescaling, editing or animating the mesh. Our
representation allows for efficient rendering using Gaussian splatting, as well
as editing and animation by modifying the base mesh. We demonstrate the
effectiveness of our method on various synthetic and real scenes, and show that
it outperforms existing surface-based approaches. We will release our code and
a web-based viewer as additional contributions. Our project page is the
following: https://anttwo.github.io/frosting/

Comments:
- Project Webpage: https://anttwo.github.io/frosting/

---

## SyncTweedies: A General Generative Framework Based on Synchronized  Diffusions


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung | cs.CV | [PDF](http://arxiv.org/pdf/2403.14370v2){: .btn .btn-green } |

**Abstract**: We introduce a general framework for generating diverse visual content,
including ambiguous images, panorama images, mesh textures, and Gaussian splat
textures, by synchronizing multiple diffusion processes. We present exhaustive
investigation into all possible scenarios for synchronizing multiple diffusion
processes through a canonical space and analyze their characteristics across
applications. In doing so, we reveal a previously unexplored case: averaging
the outputs of Tweedie's formula while conducting denoising in multiple
instance spaces. This case also provides the best quality with the widest
applicability to downstream tasks. We name this case SyncTweedies. In our
experiments generating visual content aforementioned, we demonstrate the
superior quality of generation by SyncTweedies compared to other
synchronization methods, optimization-based and iterative-update-based methods.

Comments:
- Project page: https://synctweedies.github.io/

---

## Mini-Splatting: Representing Scenes with a Constrained Number of  Gaussians


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Guangchi Fang, Bing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.14166v1){: .btn .btn-green } |

**Abstract**: In this study, we explore the challenge of efficiently representing scenes
with a constrained number of Gaussians. Our analysis shifts from traditional
graphics and 2D computer vision to the perspective of point clouds,
highlighting the inefficient spatial distribution of Gaussian representation as
a key limitation in model performance. To address this, we introduce strategies
for densification including blur split and depth reinitialization, and
simplification through Gaussian binarization and sampling. These techniques
reorganize the spatial positions of the Gaussians, resulting in significant
improvements across various datasets and benchmarks in terms of rendering
quality, resource consumption, and storage compression. Our proposed
Mini-Splatting method integrates seamlessly with the original rasterization
pipeline, providing a strong baseline for future research in
Gaussian-Splatting-based works.



---

## Leveraging Thermal Modality to Enhance Reconstruction in Low-Light  Conditions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel | cs.CV | [PDF](http://arxiv.org/pdf/2403.14053v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view
synthesis by learning the implicit volumetric representation of a scene from
multi-view images, which faithfully convey the colorimetric information.
However, sensor noises will contaminate low-value pixel signals, and the lossy
camera image signal processor will further remove near-zero intensities in
extremely dark situations, deteriorating the synthesis performance. Existing
approaches reconstruct low-light scenes from raw images but struggle to recover
texture and boundary details in dark regions. Additionally, they are unsuitable
for high-speed models relying on explicit representations. To address these
issues, we present Thermal-NeRF, which takes thermal and visible raw images as
inputs, considering the thermal camera is robust to the illumination variation
and raw images preserve any possible clues in the dark, to accomplish visible
and thermal view synthesis simultaneously. Also, the first multi-view thermal
and visible dataset (MVTV) is established to support the research on multimodal
NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and
noise smoothing and provides better synthesis performance than previous work.
Finally, we demonstrate that both modalities are beneficial to each other in 3D
reconstruction.

Comments:
- 25 pages, 13 figures

---

## MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2403.14627v1){: .btn .btn-green } |

**Abstract**: We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model
learned from sparse multi-view images. To accurately localize the Gaussian
centers, we propose to build a cost volume representation via plane sweeping in
the 3D space, where the cross-view feature similarities stored in the cost
volume can provide valuable geometry cues to the estimation of depth. We learn
the Gaussian primitives' opacities, covariances, and spherical harmonics
coefficients jointly with the Gaussian centers while only relying on
photometric supervision. We demonstrate the importance of the cost volume
representation in learning feed-forward Gaussian Splatting models via extensive
experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,
our model achieves state-of-the-art performance with the fastest feed-forward
inference speed (22 fps). Compared to the latest state-of-the-art method
pixelSplat, our model uses $10\times $ fewer parameters and infers more than
$2\times$ faster while providing higher appearance and geometry quality as well
as better cross-dataset generalization.

Comments:
- Project page: https://donydchen.github.io/mvsplat Code:
  https://github.com/donydchen/mvsplat

---

## HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin | cs.CV | [PDF](http://arxiv.org/pdf/2403.14530v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel
view synthesis, boasting rapid rendering speed with high fidelity. However, the
substantial Gaussians and their associated attributes necessitate effective
compression techniques. Nevertheless, the sparse and unorganized nature of the
point cloud of Gaussians (or anchors in our paper) presents challenges for
compression. To address this, we make use of the relations between the
unorganized anchors and the structured hash grid, leveraging their mutual
information for context modeling, and propose a Hash-grid Assisted Context
(HAC) framework for highly compact 3DGS representation. Our approach introduces
a binary hash grid to establish continuous spatial consistencies, allowing us
to unveil the inherent spatial relations of anchors through a carefully
designed context model. To facilitate entropy coding, we utilize Gaussian
distributions to accurately estimate the probability of each quantized
attribute, where an adaptive quantization module is proposed to enable
high-precision quantization of these attributes for improved fidelity
restoration. Additionally, we incorporate an adaptive masking strategy to
eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer
to explore context-based compression for 3DGS representation, resulting in a
remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while
simultaneously improving fidelity, and achieving over $11\times$ size reduction
over SOTA 3DGS compression approach Scaffold-GS. Our code is available here:
https://github.com/YihangChen-ee/HAC

Comments:
- Project Page: https://yihangchen-ee.github.io/project_hac/ Code:
  https://github.com/YihangChen-ee/HAC

---

## Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Yuanhao Gong, Lantao Yu, Guanghui Yue | cs.CV | [PDF](http://arxiv.org/pdf/2403.14244v1){: .btn .btn-green } |

**Abstract**: The 3D Gaussian splatting method has drawn a lot of attention, thanks to its
high performance in training and high quality of the rendered image. However,
it uses anisotropic Gaussian kernels to represent the scene. Although such
anisotropic kernels have advantages in representing the geometry, they lead to
difficulties in terms of computation, such as splitting or merging two kernels.
In this paper, we propose to use isotropic Gaussian kernels to avoid such
difficulties in the computation, leading to a higher performance method. The
experiments confirm that the proposed method is about {\bf 100X} faster without
losing the geometry representation accuracy. The proposed method can be applied
in a large range applications where the radiance field is needed, such as 3D
reconstruction, view synthesis, and dynamic object modeling.



---

## CombiNeRF: A Combination of Regularization Techniques for Few-Shot  Neural Radiance Field View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto | cs.CV | [PDF](http://arxiv.org/pdf/2403.14412v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have shown impressive results for novel view
synthesis when a sufficiently large amount of views are available. When dealing
with few-shot settings, i.e. with a small set of input views, the training
could overfit those views, leading to artifacts and geometric and chromatic
inconsistencies in the resulting rendering. Regularization is a valid solution
that helps NeRF generalization. On the other hand, each of the most recent NeRF
regularization techniques aim to mitigate a specific rendering problem.
Starting from this observation, in this paper we propose CombiNeRF, a framework
that synergically combines several regularization techniques, some of them
novel, in order to unify the benefits of each. In particular, we regularize
single and neighboring rays distributions and we add a smoothness term to
regularize near geometries. After these geometric approaches, we propose to
exploit Lipschitz regularization to both NeRF density and color networks and to
use encoding masks for input features regularization. We show that CombiNeRF
outperforms the state-of-the-art methods with few-shot settings in several
publicly available datasets. We also present an ablation study on the LLFF and
NeRF-Synthetic datasets that support the choices made. We release with this
paper the open-source implementation of our framework.

Comments:
- This paper has been accepted for publication at the 2024
  International Conference on 3D Vision (3DV)

---

## ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D  Decomposition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham, Qianyi Wu | cs.CV | [PDF](http://arxiv.org/pdf/2403.14619v1){: .btn .btn-green } |

**Abstract**: 3D decomposition/segmentation still remains a challenge as large-scale 3D
annotated data is not readily available. Contemporary approaches typically
leverage 2D machine-generated segments, integrating them for 3D consistency.
While the majority of these methods are based on NeRFs, they face a potential
weakness that the instance/semantic embedding features derive from independent
MLPs, thus preventing the segmentation network from learning the geometric
details of the objects directly through radiance and density. In this paper, we
propose ClusteringSDF, a novel approach to achieve both segmentation and
reconstruction in 3D via the neural implicit surface representation,
specifically Signal Distance Function (SDF), where the segmentation rendering
is directly integrated with the volume rendering of neural implicit surfaces.
Although based on ObjectSDF++, ClusteringSDF no longer requires the
ground-truth segments for supervision while maintaining the capability of
reconstructing individual object surfaces, but purely with the noisy and
inconsistent labels from pre-trained models.As the core of ClusteringSDF, we
introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D
and the experimental results on the challenging scenes from ScanNet and Replica
datasets show that ClusteringSDF can achieve competitive performance compared
against the state-of-the-art with significantly reduced training time.

Comments:
- Project Page: https://sm0kywu.github.io/ClusteringSDF/

---

## InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space  Complexity

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Jiabin Liang, Lanqing Zhang, Zhuoran Zhao, Xiangyu Xu | cs.CV | [PDF](http://arxiv.org/pdf/2403.14376v1){: .btn .btn-green } |

**Abstract**: The conventional mesh-based Level of Detail (LoD) technique, exemplified by
applications such as Google Earth and many game engines, exhibits the
capability to holistically represent a large scene even the Earth, and achieves
rendering with a space complexity of O(log n). This constrained data
requirement not only enhances rendering efficiency but also facilitates dynamic
data fetching, thereby enabling a seamless 3D navigation experience for users.
In this work, we extend this proven LoD technique to Neural Radiance Fields
(NeRF) by introducing an octree structure to represent the scenes in different
scales. This innovative approach provides a mathematically simple and elegant
representation with a rendering space complexity of O(log n), aligned with the
efficiency of mesh-based LoD techniques. We also present a novel training
strategy that maintains a complexity of O(n). This strategy allows for parallel
training with minimal overhead, ensuring the scalability and efficiency of our
proposed method. Our contribution is not only in extending the capabilities of
existing techniques but also in establishing a foundation for scalable and
efficient large-scale scene representation using NeRF and octree structures.



---

## Hyperspectral Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-21 | Gerry Chen, Sunil Kumar Narayanan, Thomas Gautier Ottou, Benjamin Missaoui, Harsh Muriki, Cédric Pradalier, Yongsheng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2403.14839v1){: .btn .btn-green } |

**Abstract**: Hyperspectral Imagery (HSI) has been used in many applications to
non-destructively determine the material and/or chemical compositions of
samples. There is growing interest in creating 3D hyperspectral
reconstructions, which could provide both spatial and spectral information
while also mitigating common HSI challenges such as non-Lambertian surfaces and
translucent objects. However, traditional 3D reconstruction with HSI is
difficult due to technological limitations of hyperspectral cameras. In recent
years, Neural Radiance Fields (NeRFs) have seen widespread success in creating
high quality volumetric 3D representations of scenes captured by a variety of
camera models. Leveraging recent advances in NeRFs, we propose computing a
hyperspectral 3D reconstruction in which every point in space and view
direction is characterized by wavelength-dependent radiance and transmittance
spectra. To evaluate our approach, a dataset containing nearly 2000
hyperspectral images across 8 scenes and 2 cameras was collected. We perform
comparisons against traditional RGB NeRF baselines and apply ablation testing
with alternative spectra representations. Finally, we demonstrate the potential
of hyperspectral NeRFs for hyperspectral super-resolution and imaging sensor
simulation. We show that our hyperspectral NeRF approach enables creating fast,
accurate volumetric 3D hyperspectral scenes and enables several new
applications and areas for future study.

Comments:
- Main paper: 15 pages + 2 pages references. Supplemental/Appendix: 6
  pages

---

## RadSplat: Radiance Field-Informed Gaussian Splatting for Robust  Real-Time Rendering with 900+ FPS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-20 | Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2403.13806v1){: .btn .btn-green } |

**Abstract**: Recent advances in view synthesis and real-time rendering have achieved
photorealistic quality at impressive rendering speeds. While Radiance
Field-based methods achieve state-of-the-art quality in challenging scenarios
such as in-the-wild captures and large-scale scenes, they often suffer from
excessively high compute requirements linked to volumetric rendering. Gaussian
Splatting-based methods, on the other hand, rely on rasterization and naturally
achieve real-time rendering but suffer from brittle optimization heuristics
that underperform on more challenging scenes. In this work, we present
RadSplat, a lightweight method for robust real-time rendering of complex
scenes. Our main contributions are threefold. First, we use radiance fields as
a prior and supervision signal for optimizing point-based scene
representations, leading to improved quality and more robust optimization.
Next, we develop a novel pruning technique reducing the overall point count
while maintaining high quality, leading to smaller and more compact scene
representations with faster inference speeds. Finally, we propose a novel
test-time filtering approach that further accelerates rendering and allows to
scale to larger, house-sized scenes. We find that our method enables
state-of-the-art synthesis of complex captures at 900+ FPS.

Comments:
- Project page at https://m-niemeyer.github.io/radsplat/

---

## MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with  Wireless Coordination

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-20 | Weiying Wang, Victor Cai, Stephanie Gil | cs.RO | [PDF](http://arxiv.org/pdf/2403.13348v1){: .btn .btn-green } |

**Abstract**: This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework
that leverages wireless signal-based coordination between robots and Neural
Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D
reconstruction, including inter-robot pose estimation, localization uncertainty
quantification, and active best-next-view selection. We introduce a method for
using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate
relative poses between robots, as well as quantifying and incorporating the
uncertainty embedded in the wireless localization of these pose estimates into
the NeRF training loss to mitigate the impact of inaccurate camera poses.
Furthermore, we propose an active view selection approach that accounts for
robot pose uncertainty when determining the next-best views to improve the 3D
reconstruction, enabling faster convergence through intelligent view selection.
Extensive experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our framework in theory and in practice. Leveraging wireless
coordination and localization uncertainty-aware training, MULAN-WC can achieve
high-quality 3d reconstruction which is close to applying the ground truth
camera poses. Furthermore, the quantification of the information gain from a
novel view enables consistent rendering quality improvement with incrementally
captured images by commending the robot the novel view position. Our hardware
experiments showcase the practicality of deploying MULAN-WC to real robotic
systems.



---

## Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation  for Natural Camera Motion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-20 | Otto Seiskari, Jerry Ylilammi, Valtteri Kaatrasalo, Pekka Rantalankila, Matias Turkulainen, Juho Kannala, Esa Rahtu, Arno Solin | cs.CV | [PDF](http://arxiv.org/pdf/2403.13327v1){: .btn .btn-green } |

**Abstract**: High-quality scene reconstruction and novel view synthesis based on Gaussian
Splatting (3DGS) typically require steady, high-quality photographs, often
impractical to capture with handheld cameras. We present a method that adapts
to camera motion and allows high-quality scene reconstruction with handheld
video data suffering from motion blur and rolling shutter distortion. Our
approach is based on detailed modelling of the physical image formation process
and utilizes velocities estimated using visual-inertial odometry (VIO). Camera
poses are considered non-static during the exposure time of a single image
frame and camera poses are further optimized in the reconstruction process. We
formulate a differentiable rendering pipeline that leverages screen space
approximation to efficiently incorporate rolling-shutter and motion blur
effects into the 3DGS framework. Our results with both synthetic and real data
demonstrate superior performance in mitigating camera motion over existing
methods, thereby advancing 3DGS in naturalistic settings.

Comments:
- Source code available at https://github.com/SpectacularAI/3dgs-deblur

---

## GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann | cs.CV | [PDF](http://arxiv.org/pdf/2403.12365v1){: .btn .btn-green } |

**Abstract**: Creating 4D fields of Gaussian Splatting from images or videos is a
challenging task due to its under-constrained nature. While the optimization
can draw photometric reference from the input videos or be regulated by
generative models, directly supervising Gaussian motions remains underexplored.
In this paper, we introduce a novel concept, Gaussian flow, which connects the
dynamics of 3D Gaussians and pixel velocities between consecutive frames. The
Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into
the image space. This differentiable process enables direct dynamic supervision
from optical flow. Our method significantly benefits 4D dynamic content
generation and 4D novel view synthesis with Gaussian Splatting, especially for
contents with rich motions that are hard to be handled by existing methods. The
common color drifting issue that happens in 4D generation is also resolved with
improved Guassian dynamics. Superior visual quality on extensive experiments
demonstrates our method's effectiveness. Quantitative and qualitative
evaluations show that our method achieves state-of-the-art results on both
tasks of 4D generation and 4D novel view synthesis. Project page:
https://zerg-overmind.github.io/GaussianFlow.github.io/



---

## RGBD GS-ICP SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Seongbo Ha, Jiung Yeon, Hyeonwoo Yu | cs.CV | [PDF](http://arxiv.org/pdf/2403.12550v2){: .btn .btn-green } |

**Abstract**: Simultaneous Localization and Mapping (SLAM) with dense representation plays
a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)
applications. Recent advancements in dense representation SLAM have highlighted
the potential of leveraging neural scene representation and 3D Gaussian
representation for high-fidelity spatial representation. In this paper, we
propose a novel dense representation SLAM approach with a fusion of Generalized
Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast
to existing methods, we utilize a single Gaussian map for both tracking and
mapping, resulting in mutual benefits. Through the exchange of covariances
between tracking and mapping processes with scale alignment techniques, we
minimize redundant computations and achieve an efficient system. Additionally,
we enhance tracking accuracy and mapping quality through our keyframe selection
methods. Experimental results demonstrate the effectiveness of our approach,
showing an incredibly fast speed up to 107 FPS (for the entire system) and
superior quality of the reconstructed map.



---

## Depth-guided NeRF Training via Earth Mover's Distance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy | cs.CV | [PDF](http://arxiv.org/pdf/2403.13206v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of
predicted viewpoints. However, the photometric loss often does not provide
enough information to disambiguate between different possible geometries
yielding the same image. Previous work has thus incorporated depth supervision
during NeRF training, leveraging dense predictions from pre-trained depth
networks as pseudo-ground truth. While these depth priors are assumed to be
perfect once filtered for noise, in practice, their accuracy is more
challenging to capture. This work proposes a novel approach to uncertainty in
depth priors for NeRF supervision. Instead of using custom-trained depth or
uncertainty priors, we use off-the-shelf pretrained diffusion models to predict
depth and capture uncertainty during the denoising process. Because we know
that depth priors are prone to errors, we propose to supervise the ray
termination distance distribution with Earth Mover's Distance instead of
enforcing the rendered depth to replicate the depth prior exactly through
L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth
metrics by a large margin while maintaining performance on photometric
measures.

Comments:
- Preprint. Under review

---

## High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided  Densification and Regularized Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson | cs.RO | [PDF](http://arxiv.org/pdf/2403.12535v1){: .btn .btn-green } |

**Abstract**: We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that
provides metrically accurate pose tracking and visually realistic
reconstruction. To this end, we first propose a Gaussian densification strategy
based on the rendering loss to map unobserved areas and refine reobserved
areas. Second, we introduce extra regularization parameters to alleviate the
forgetting problem in the continuous mapping problem, where parameters tend to
overfit the latest frame and result in decreasing rendering quality for
previous frames. Both mapping and tracking are performed with Gaussian
parameters by minimizing re-rendering loss in a differentiable way. Compared to
recent neural and concurrently developed gaussian splatting RGBD SLAM
baselines, our method achieves state-of-the-art results on the synthetic
dataset Replica and competitive results on the real-world dataset TUM.

Comments:
- submitted to IROS24

---

## GVGEN: Text-to-3D Generation with Volumetric Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He | cs.CV | [PDF](http://arxiv.org/pdf/2403.12957v1){: .btn .btn-green } |

**Abstract**: In recent years, 3D Gaussian splatting has emerged as a powerful technique
for 3D reconstruction and generation, known for its fast and high-quality
rendering capabilities. To address these shortcomings, this paper introduces a
novel diffusion-based framework, GVGEN, designed to efficiently generate 3D
Gaussian representations from text input. We propose two innovative
techniques:(1) Structured Volumetric Representation. We first arrange
disorganized 3D Gaussian points as a structured form GaussianVolume. This
transformation allows the capture of intricate texture details within a volume
composed of a fixed number of Gaussians. To better optimize the representation
of these details, we propose a unique pruning and densifying method named the
Candidate Pool Strategy, enhancing detail fidelity through selective
optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the
generation of GaussianVolume and empower the model to generate instances with
detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially
constructs a basic geometric structure, followed by the prediction of complete
Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in
qualitative and quantitative assessments compared to existing 3D generation
methods. Simultaneously, it maintains a fast generation speed ($\sim$7
seconds), effectively striking a balance between quality and efficiency.

Comments:
- project page: https://gvgen.github.io/

---

## HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2403.12722v1){: .btn .btn-green } |

**Abstract**: Holistic understanding of urban scenes based on RGB images is a challenging
yet important problem. It encompasses understanding both the geometry and
appearance to enable novel view synthesis, parsing semantic labels, and
tracking moving objects. Despite considerable progress, existing approaches
often focus on specific aspects of this task and require additional inputs such
as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we
introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic
urban scene understanding. Our main idea involves the joint optimization of
geometry, appearance, semantics, and motion using a combination of static and
dynamic 3D Gaussians, where moving object poses are regularized via physical
constraints. Our approach offers the ability to render new viewpoints in
real-time, yielding 2D and 3D semantic information with high accuracy, and
reconstruct dynamic scenes, even in scenarios where 3D bounding box detection
are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2
demonstrate the effectiveness of our approach.

Comments:
- Our project page is at https://xdimlab.github.io/hugs_website

---

## DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Zaid Tasneem, Akshat Dave, Abhishek Singh, Kushagra Tiwary, Praneeth Vepakomma, Ashok Veeraraghavan, Ramesh Raskar | cs.CV | [PDF](http://arxiv.org/pdf/2403.13199v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) show potential for transforming images
captured worldwide into immersive 3D visual experiences. However, most of this
captured visual data remains siloed in our camera rolls as these images contain
personal details. Even if made public, the problem of learning 3D
representations of billions of scenes captured daily in a centralized manner is
computationally intractable. Our approach, DecentNeRF, is the first attempt at
decentralized, crowd-sourced NeRFs that require $\sim 10^4\times$ less server
computing for a scene than a centralized approach. Instead of sending the raw
data, our approach requires users to send a 3D representation, distributing the
high computation cost of training centralized NeRFs between the users. It
learns photorealistic scene representations by decomposing users' 3D views into
personal and global NeRFs and a novel optimally weighted aggregation of only
the latter. We validate the advantage of our approach to learn NeRFs with
photorealism and minimal server computation cost on structured synthetic and
real-world photo tourism datasets. We further analyze how secure aggregation of
global NeRFs in DecentNeRF minimizes the undesired reconstruction of personal
content by the server.



---

## Learning Neural Volumetric Pose Features for Camera Localization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Jingyu Lin, Jiaqi Gu, Bojian Wu, Lubin Fan, Renjie Chen, Ligang Liu, Jieping Ye | cs.CV | [PDF](http://arxiv.org/pdf/2403.12800v1){: .btn .btn-green } |

**Abstract**: We introduce a novel neural volumetric pose feature, termed PoseMap, designed
to enhance camera localization by encapsulating the information between images
and the associated camera poses. Our framework leverages an Absolute Pose
Regression (APR) architecture, together with an augmented NeRF module. This
integration not only facilitates the generation of novel views to enrich the
training dataset but also enables the learning of effective pose features.
Additionally, we extend our architecture for self-supervised online alignment,
allowing our method to be used and fine-tuned for unlabelled images within a
unified framework. Experiments demonstrate that our method achieves 14.28% and
20.51% performance gain on average in indoor and outdoor benchmark scenes,
outperforming existing APR methods with state-of-the-art accuracy.

Comments:
- 14 pages, 9 figures

---

## Global-guided Focal Neural Radiance Field for Large-scale Scene  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.12839v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields~(NeRF) have recently been applied to render
large-scale scenes. However, their limited model capacity typically results in
blurred rendering results. Existing large-scale NeRFs primarily address this
limitation by partitioning the scene into blocks, which are subsequently
handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and
processed independently, lead to inconsistencies in geometry and appearance
across the scene. Consequently, the rendering quality fails to exhibit
significant improvement despite the expansion of model capacity. In this work,
we present global-guided focal neural radiance field (GF-NeRF) that achieves
high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a
two-stage (Global and Focal) architecture and a global-guided training
strategy. The global stage obtains a continuous representation of the entire
scene while the focal stage decomposes the scene into multiple blocks and
further processes them with distinct sub-encoders. Leveraging this two-stage
architecture, sub-encoders only need fine-tuning based on the global encoder,
thus reducing training complexity in the focal stage while maintaining
scene-wide consistency. Spatial information and error information from the
global stage also benefit the sub-encoders to focus on crucial areas and
effectively capture more details of large-scale scenes. Notably, our approach
does not rely on any prior knowledge about the target scene, attributing
GF-NeRF adaptable to various large-scale scene types, including street-view and
aerial-view scenes. We demonstrate that our method achieves high-fidelity,
natural rendering results on various types of large-scale datasets. Our project
page: https://shaomq2187.github.io/GF-NeRF/



---

## IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single  image and a NeRF model

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Matteo Bortolon, Theodore Tsesmelis, Stuart James, Fabio Poiesi, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2403.12682v1){: .btn .btn-green } |

**Abstract**: We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera
pose of a given image, building on the Neural Radiance Fields (NeRF)
formulation. IFFNeRF is specifically designed to operate in real-time and
eliminates the need for an initial pose guess that is proximate to the sought
solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface
points from within the NeRF model. From these sampled points, we cast rays and
deduce the color for each ray through pixel-level view synthesis. The camera
pose can then be estimated as the solution to a Least Squares problem by
selecting correspondences between the query image and the resulting bundle. We
facilitate this process through a learned attention mechanism, bridging the
query image embedding with the embedding of parameterized rays, thereby
matching rays pertinent to the image. Through synthetic and real evaluation
settings, we show that our method can improve the angular and translation error
accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing
at 34fps on consumer hardware and not requiring the initial pose guess.

Comments:
- Accepted ICRA 2024, Project page:
  https://mbortolon97.github.io/iffnerf/

---

## GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with  Noisy Polarization Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong | cs.CV | [PDF](http://arxiv.org/pdf/2403.11899v1){: .btn .btn-green } |

**Abstract**: Learning surfaces from neural radiance field (NeRF) became a rising topic in
Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods
demonstrated their ability to reconstruct accurate 3D shapes of Lambertian
scenes. However, their results on reflective scenes are unsatisfactory due to
the entanglement of specular radiance and complicated geometry. To address the
challenges, we propose a Gaussian-based representation of normals in SDF
fields. Supervised by polarization priors, this representation guides the
learning of geometry behind the specular reflection and captures more details
than existing methods. Moreover, we propose a reweighting strategy in the
optimization process to alleviate the noise issue of polarization priors. To
validate the effectiveness of our design, we capture polarimetric information,
and ground truth meshes in additional reflective scenes with various geometry.
We also evaluated our framework on the PANDORA dataset. Comparisons prove our
method outperforms existing neural 3D reconstruction methods in reflective
scenes by a large margin.

Comments:
- Accepted to ICLR 2024 Poster. For the Appendix, please see
  http://yukiumi13.github.io/gnerp_page

---

## FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo  Endoscopic Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir Navab, Benjamin Busam, Alexander Ladikos | cs.CV | [PDF](http://arxiv.org/pdf/2403.12198v1){: .btn .btn-green } |

**Abstract**: Reconstruction of endoscopic scenes is an important asset for various medical
applications, from post-surgery analysis to educational training. Neural
rendering has recently shown promising results in endoscopic reconstruction
with deforming tissue. However, the setup has been restricted to a static
endoscope, limited deformation, or required an external tracking device to
retrieve camera pose information of the endoscopic camera. With FLex we adress
the challenging setup of a moving endoscope within a highly dynamic environment
of deforming tissue. We propose an implicit scene separation into multiple
overlapping 4D neural radiance fields (NeRFs) and a progressive optimization
scheme jointly optimizing for reconstruction and camera poses from scratch.
This improves the ease-of-use and allows to scale reconstruction capabilities
in time to process surgical videos of 5,000 frames and more; an improvement of
more than ten times compared to the state of the art while being agnostic to
external tracking information. Extensive evaluations on the StereoMIS dataset
show that FLex significantly improves the quality of novel view synthesis while
maintaining competitive pose accuracy.



---

## UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures  for Human Avatar Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan | cs.CV | [PDF](http://arxiv.org/pdf/2403.11589v1){: .btn .btn-green } |

**Abstract**: Reconstructing photo-realistic drivable human avatars from multi-view image
sequences has been a popular and challenging topic in the field of computer
vision and graphics. While existing NeRF-based methods can achieve high-quality
novel view rendering of human models, both training and inference processes are
time-consuming. Recent approaches have utilized 3D Gaussians to represent the
human body, enabling faster training and rendering. However, they undermine the
importance of the mesh guidance and directly predict Gaussians in 3D space with
coarse mesh guidance. This hinders the learning procedure of the Gaussians and
tends to produce blurry textures. Therefore, we propose UV Gaussians, which
models the 3D human body by jointly learning mesh deformations and 2D UV-space
Gaussian textures. We utilize the embedding of UV map to learn Gaussian
textures in 2D space, leveraging the capabilities of powerful 2D networks to
extract features. Additionally, through an independent Mesh network, we
optimize pose-dependent geometric deformations, thereby guiding Gaussian
rendering and significantly enhancing rendering quality. We collect and process
a new dataset of human motion, which includes multi-view images, scanned
models, parametric model registration, and corresponding texture maps.
Experimental results demonstrate that our method achieves state-of-the-art
synthesis of novel view and novel pose. The code and data will be made
available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the
paper is accepted.



---

## Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Teppei Suzuki | cs.CV | [PDF](http://arxiv.org/pdf/2403.11460v1){: .btn .btn-green } |

**Abstract**: In this work, we present Fed3DGS, a scalable 3D reconstruction framework
based on 3D Gaussian splatting (3DGS) with federated learning. Existing
city-scale reconstruction methods typically adopt a centralized approach, which
gathers all data in a central server and reconstructs scenes. The approach
hampers scalability because it places a heavy load on the server and demands
extensive data storage when reconstructing scenes on a scale beyond city-scale.
In pursuit of a more scalable 3D reconstruction, we propose a federated
learning framework with 3DGS, which is a decentralized framework and can
potentially use distributed computational resources across millions of clients.
We tailor a distillation-based model update scheme for 3DGS and introduce
appearance modeling for handling non-IID data in the scenario of 3D
reconstruction with federated learning. We simulate our method on several
large-scale benchmarks, and our method demonstrates rendered image quality
comparable to centralized approaches. In addition, we also simulate our method
with data collected in different seasons, demonstrating that our framework can
reflect changes in the scenes and our appearance modeling captures changes due
to seasonal variations.

Comments:
- Code: https://github.com/DensoITLab/Fed3DGS

---

## View-Consistent 3D Editing with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2403.11868v2){: .btn .btn-green } |

**Abstract**: The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,
offering efficient, high-fidelity rendering and enabling precise local
manipulations. Currently, diffusion-based 2D editing models are harnessed to
modify multi-view rendered images, which then guide the editing of 3DGS models.
However, this approach faces a critical issue of multi-view inconsistency,
where the guidance images exhibit significant discrepancies across views,
leading to mode collapse and visual artifacts of 3DGS. To this end, we
introduce View-consistent Editing (VcEdit), a novel framework that seamlessly
incorporates 3DGS into image editing processes, ensuring multi-view consistency
in edited guidance images and effectively mitigating mode collapse issues.
VcEdit employs two innovative consistency modules: the Cross-attention
Consistency Module and the Editing Consistency Module, both designed to reduce
inconsistencies in edited images. By incorporating these consistency modules
into an iterative pattern, VcEdit proficiently resolves the issue of multi-view
inconsistency, facilitating high-quality 3DGS editing across a diverse range of
scenes.



---

## RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Sibi Catley-Chandar, Richard Shaw, Gregory Slabaugh, Eduardo Perez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2403.11909v1){: .btn .btn-green } |

**Abstract**: Recent advances in neural rendering have enabled highly photorealistic 3D
scene reconstruction and novel view synthesis. Despite this progress, current
state-of-the-art methods struggle to reconstruct high frequency detail, due to
factors such as a low-frequency bias of radiance fields and inaccurate camera
calibration. One approach to mitigate this issue is to enhance images
post-rendering. 2D enhancers can be pre-trained to recover some detail but are
agnostic to scene geometry and do not easily generalize to new distributions of
image degradation. Conversely, existing 3D enhancers are able to transfer
detail from nearby training images in a generalizable manner, but suffer from
inaccurate camera calibration and can propagate errors from the geometry into
rendered images. We propose a neural rendering enhancer, RoGUENeRF, which
exploits the best of both paradigms. Our method is pre-trained to learn a
general enhancer while also leveraging information from nearby training images
via robust 3D alignment and geometry-aware fusion. Our approach restores
high-frequency textures while maintaining geometric consistency and is also
robust to inaccurate camera calibration. We show that RoGUENeRF substantially
enhances the rendering quality of a wide range of neural rendering baselines,
e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the
real world 360v2 dataset.



---

## NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using  3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie | cs.CV | [PDF](http://arxiv.org/pdf/2403.11679v1){: .btn .btn-green } |

**Abstract**: We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D
Gaussian representation, that enables robust 3D semantic mapping, accurate
camera tracking, and high-quality rendering in real-time. In the system, we
propose a Spatially Consistent Feature Fusion model to reduce the effect of
erroneous estimates from pre-trained segmentation head on semantic
reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we
employ a lightweight encoder-decoder to compress the high-dimensional semantic
features into a compact 3D Gaussian representation, mitigating the burden of
excessive memory consumption. Furthermore, we leverage the advantage of 3D
Gaussian splatting, which enables efficient and differentiable novel view
rendering, and propose a Virtual Camera View Pruning method to eliminate
outlier GS points, thereby effectively enhancing the quality of scene
representations. Our NEDS-SLAM method demonstrates competitive performance over
existing dense semantic SLAM methods in terms of mapping and tracking accuracy
on Replica and ScanNet datasets, while also showing excellent capabilities in
3D dense semantic mapping.



---

## Bridging 3D Gaussian and Mesh for Freeview Video Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yuting Xiao, Xuan Wang, Jiafei Li, Hongrui Cai, Yanbo Fan, Nan Xue, Minghui Yang, Yujun Shen, Shenghua Gao | cs.GR | [PDF](http://arxiv.org/pdf/2403.11453v1){: .btn .btn-green } |

**Abstract**: This is only a preview version of GauMesh. Recently, primitive-based
rendering has been proven to achieve convincing results in solving the problem
of modeling and rendering the 3D dynamic scene from 2D images. Despite this, in
the context of novel view synthesis, each type of primitive has its inherent
defects in terms of representation ability. It is difficult to exploit the mesh
to depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D
Gaussian Splatting) method usually produces artifacts or blurry pixels in the
area with smooth geometry and sharp textures. As a result, it is difficult,
even not impossible, to represent the complex and dynamic scene with a single
type of primitive. To this end, we propose a novel approach, GauMesh, to bridge
the 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a
sequence of tracked mesh as initialization, our goal is to simultaneously
optimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians,
and the deformation field. At a specific time, we perform $\alpha$-blending on
the RGB and opacity values based on the merged and re-ordered z-buffers from
mesh and 3D Gaussian rasterizations. This produces the final rendering, which
is supervised by the ground-truth image. Experiments demonstrate that our
approach adapts the appropriate type of primitives to represent the different
parts of the dynamic scene and outperforms all the baseline methods in both
quantitative and qualitative comparisons without losing render speed.

Comments:
- 7 pages

---

## Exploring Multi-modal Neural Scene Representations With Applications on  Thermal Imaging

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger | cs.CV | [PDF](http://arxiv.org/pdf/2403.11865v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard
for the task of novel view synthesis when trained on a set of RGB images. In
this paper, we conduct a comprehensive evaluation of neural scene
representations, such as NeRFs, in the context of multi-modal learning.
Specifically, we present four different strategies of how to incorporate a
second modality, other than RGB, into NeRFs: (1) training from scratch
independently on both modalities; (2) pre-training on RGB and fine-tuning on
the second modality; (3) adding a second branch; and (4) adding a separate
component to predict (color) values of the additional modality. We chose
thermal imaging as second modality since it strongly differs from RGB in terms
of radiosity, making it challenging to integrate into neural scene
representations. For the evaluation of the proposed strategies, we captured a
new publicly available multi-view dataset, ThermalMix, consisting of six common
objects and about 360 RGB and thermal images in total. We employ cross-modality
calibration prior to data capturing, leading to high-quality alignments between
RGB and thermal images. Our findings reveal that adding a second branch to NeRF
performs best for novel view synthesis on thermal images while also yielding
compelling results on RGB. Finally, we also show that our analysis generalizes
to other modalities, including near-infrared images and depth maps. Project
page: https://mert-o.github.io/ThermalNeRF/.

Comments:
- 24 pages, 14 figures

---

## Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for  Resolving Class-imbalance Problem

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2403.11573v2){: .btn .btn-green } |

**Abstract**: Typical LiDAR-based 3D object detection models are trained in a supervised
manner with real-world data collection, which is often imbalanced over classes
(or long-tailed). To deal with it, augmenting minority-class examples by
sampling ground truth (GT) LiDAR points from a database and pasting them into a
scene of interest is often used, but challenges still remain: inflexibility in
locating GT samples and limited sample diversity. In this work, we propose to
leverage pseudo-LiDAR point clouds generated (at a low cost) from videos
capturing a surround view of miniatures or real-world objects of minor classes.
Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of
three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D
view synthesis model, (ii) object-level domain alignment with LiDAR intensity
estimation and (iii) a hybrid context-aware placement method from ground and
map information. We demonstrate the superiority and generality of our method
through performance improvements in extensive experiments conducted on three
popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the
datasets with large domain gaps captured by different LiDAR configurations. Our
code and data will be publicly available upon publication.

Comments:
- 28 pages, 12 figures, 11 tables

---

## GaussNav: Gaussian Splatting for Visual Navigation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.11625v2){: .btn .btn-green } |

**Abstract**: In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to
locate a specific object depicted in a goal image within an unexplored
environment. The primary difficulty of IIN stems from the necessity of
recognizing the target object across varying viewpoints and rejecting potential
distractors.
  Existing map-based navigation methods largely adopt the representation form
of Bird's Eye View (BEV) maps, which, however, lack the representation of
detailed textures in a scene.
  To address the above issues, we propose a new Gaussian Splatting Navigation
(abbreviated as GaussNav) framework for IIN task, which constructs a novel map
representation based on 3D Gaussian Splatting (3DGS).
  The proposed framework enables the agent to not only memorize the geometry
and semantic information of the scene, but also retain the textural features of
objects.
  Our GaussNav framework demonstrates a significant leap in performance,
evidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to
0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset.
  Our code will be made publicly available.

Comments:
- conference

---

## Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous  Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Antoine Schnepf, Karim Kassab, Jean-Yves Franceschi, Laurent Caraffa, Flavian Vasile, Jeremie Mary, Andrew Comport, Valérie Gouet-Brunet | cs.CV | [PDF](http://arxiv.org/pdf/2403.11678v1){: .btn .btn-green } |

**Abstract**: We present a method enabling the scaling of NeRFs to learn a large number of
semantically-similar scenes. We combine two techniques to improve the required
training time and memory cost per scene. First, we learn a 3D-aware latent
space in which we train Tri-Plane scene representations, hence reducing the
resolution at which scenes are learned. Moreover, we present a way to share
common information across scenes, hence allowing for a reduction of model
complexity to learn a particular scene. Our method reduces effective per-scene
memory costs by 44% and per-scene time costs by 86% when training 1000 scenes.
Our project page can be found at https://3da-ae.github.io .



---

## BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Lingzhe Zhao, Peng Wang, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2403.11831v2){: .btn .btn-green } |

**Abstract**: While neural rendering has demonstrated impressive capabilities in 3D scene
reconstruction and novel view synthesis, it heavily relies on high-quality
sharp images and accurate camera poses. Numerous approaches have been proposed
to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly
encountered in real-world scenarios such as low-light or long-exposure
conditions. However, the implicit representation of NeRF struggles to
accurately recover intricate details from severely motion-blurred images and
cannot achieve real-time rendering. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time
rendering by explicitly optimizing point clouds as Gaussian spheres.
  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle
Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian
representation and handles severe motion-blurred images with inaccurate camera
poses to achieve high-quality scene reconstruction. Our method models the
physical image formation process of motion-blurred images and jointly learns
the parameters of Gaussians while recovering camera motion trajectories during
exposure time.
  In our experiments, we demonstrate that BAD-Gaussians not only achieves
superior rendering quality compared to previous state-of-the-art deblur neural
rendering methods on both synthetic and real datasets but also enables
real-time rendering capabilities.
  Our project page and source code is available at
https://lingzhezhao.github.io/BAD-Gaussians/

Comments:
- Project Page and Source Code:
  https://lingzhezhao.github.io/BAD-Gaussians/

---

## Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from  Aerial Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui | cs.CV | [PDF](http://arxiv.org/pdf/2403.11812v1){: .btn .btn-green } |

**Abstract**: We present a neural radiance field method for urban-scale semantic and
building-level instance segmentation from aerial images by lifting noisy 2D
labels to 3D. This is a challenging problem due to two primary reasons.
Firstly, objects in urban aerial images exhibit substantial variations in size,
including buildings, cars, and roads, which pose a significant challenge for
accurate 2D segmentation. Secondly, the 2D labels generated by existing
segmentation methods suffer from the multi-view inconsistency problem,
especially in the case of aerial images, where each image captures only a small
portion of the entire scene. To overcome these limitations, we first introduce
a scale-adaptive semantic label fusion strategy that enhances the segmentation
of objects of varying sizes by combining labels predicted from different
altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then
introduce a novel cross-view instance label grouping strategy based on the 3D
scene representation to mitigate the multi-view inconsistency problem in the 2D
instance labels. Furthermore, we exploit multi-view reconstructed depth priors
to improve the geometric quality of the reconstructed radiance field, resulting
in enhanced segmentation results. Experiments on multiple real-world
urban-scale datasets demonstrate that our approach outperforms existing
methods, highlighting its effectiveness.

Comments:
- CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/

---

## ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle | cs.CV | [PDF](http://arxiv.org/pdf/2403.12154v1){: .btn .btn-green } |

**Abstract**: Thermal scene reconstruction exhibit great potential for applications across
a broad spectrum of fields, including building energy consumption analysis and
non-destructive testing. However, existing methods typically require dense
scene measurements and often rely on RGB images for 3D geometry reconstruction,
with thermal information being projected post-reconstruction. This two-step
strategy, adopted due to the lack of texture in thermal images, can lead to
disparities between the geometry and temperatures of the reconstructed objects
and those of the actual scene. To address this challenge, we propose
ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields,
capable of rendering new RGB and thermal views of a scene jointly. To overcome
the lack of texture in thermal images, we use paired RGB and thermal images to
learn scene density, while distinct networks estimate color and temperature
information. Furthermore, we introduce ThermoScenes, a new dataset to palliate
the lack of available RGB+thermal datasets for scene reconstruction.
Experimental results validate that ThermoNeRF achieves accurate thermal image
synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement
of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a
state-of-the-art NeRF method.



---

## BAGS: Building Animatable Gaussian Splatting from a Monocular Video with  Diffusion Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Tingyang Zhang, Qingzhe Gao, Weiyu Li, Libin Liu, Baoquan Chen | cs.CV | [PDF](http://arxiv.org/pdf/2403.11427v1){: .btn .btn-green } |

**Abstract**: Animatable 3D reconstruction has significant applications across various
fields, primarily relying on artists' handcraft creation. Recently, some
studies have successfully constructed animatable 3D models from monocular
videos. However, these approaches require sufficient view coverage of the
object within the input video and typically necessitate significant time and
computational costs for training and rendering. This limitation restricts the
practical applications. In this work, we propose a method to build animatable
3D Gaussian Splatting from monocular video with diffusion priors. The 3D
Gaussian representations significantly accelerate the training and rendering
process, and the diffusion priors allow the method to learn 3D models with
limited viewpoints. We also present the rigid regularization to enhance the
utilization of the priors. We perform an extensive evaluation across various
real-world videos, demonstrating its superior performance compared to the
current state-of-the-art methods.

Comments:
- https://talegqz.github.io/BAGS/

---

## Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, Houqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.11447v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene
reconstruction. However, existing methods focus mainly on extending static 3DGS
into a time-variant representation, while overlooking the rich motion
information carried by 2D observations, thus suffering from performance
degradation and model redundancy. To address the above problem, we propose a
novel motion-aware enhancement framework for dynamic scene reconstruction,
which mines useful motion cues from optical flow to improve different paradigms
of dynamic 3DGS. Specifically, we first establish a correspondence between 3D
Gaussian movements and pixel-level flow. Then a novel flow augmentation method
is introduced with additional insights into uncertainty and loss collaboration.
Moreover, for the prevalent deformation-based paradigm that presents a harder
optimization problem, a transient-aware deformation auxiliary module is
proposed. We conduct extensive experiments on both multi-view and monocular
scenes to verify the merits of our work. Compared with the baselines, our
method shows significant superiority in both rendering quality and efficiency.



---

## 3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal  Calibration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux | cs.CV | [PDF](http://arxiv.org/pdf/2403.11577v1){: .btn .btn-green } |

**Abstract**: Reliable multimodal sensor fusion algorithms require accurate spatiotemporal
calibration. Recently, targetless calibration techniques based on implicit
neural representations have proven to provide precise and robust results.
Nevertheless, such methods are inherently slow to train given the high
computational overhead caused by the large number of sampled points required
for volume rendering. With the recent introduction of 3D Gaussian Splatting as
a faster alternative to implicit representation methods, we propose to leverage
this new rendering approach to achieve faster multi-sensor calibration. We
introduce 3DGS-Calib, a new calibration method that relies on the speed and
rendering accuracy of 3D Gaussian Splatting to achieve multimodal
spatiotemporal calibration that is accurate, robust, and with a substantial
speed-up compared to methods relying on implicit neural representations. We
demonstrate the superiority of our proposal with experimental results on
sequences from KITTI-360, a widely used driving dataset.

Comments:
- Under review

---

## DVN-SLAM: Dynamic Visual Neural SLAM Based on Local-Global Encoding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Wenhua Wu, Guangming Wang, Ting Deng, Sebastian Aegidius, Stuart Shanks, Valerio Modugno, Dimitrios Kanoulas, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.11776v1){: .btn .btn-green } |

**Abstract**: Recent research on Simultaneous Localization and Mapping (SLAM) based on
implicit representation has shown promising results in indoor environments.
However, there are still some challenges: the limited scene representation
capability of implicit encodings, the uncertainty in the rendering process from
implicit representations, and the disruption of consistency by dynamic objects.
To address these challenges, we propose a real-time dynamic visual SLAM system
based on local-global fusion neural implicit representation, named DVN-SLAM. To
improve the scene representation capability, we introduce a local-global fusion
neural implicit representation that enables the construction of an implicit map
while considering both global structure and local details. To tackle
uncertainties arising from the rendering process, we design an information
concentration loss for optimization, aiming to concentrate scene information on
object surfaces. The proposed DVN-SLAM achieves competitive performance in
localization and mapping across multiple datasets. More importantly, DVN-SLAM
demonstrates robustness in dynamic scenes, a trait that sets it apart from
other NeRF-based methods.



---

## Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot  Navigation and 3D Scene Understanding with FisherRF


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Guangyi Liu, Wen Jiang, Boshu Lei, Vivek Pandey, Kostas Daniilidis, Nader Motee | cs.RO | [PDF](http://arxiv.org/pdf/2403.11396v1){: .btn .btn-green } |

**Abstract**: This work proposes a novel approach to bolster both the robot's risk
assessment and safety measures while deepening its understanding of 3D scenes,
which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian
Splatting. To further enhance these capabilities, we incorporate additional
sampled views from the environment with the RF model. One of our key
contributions is the introduction of Risk-aware Environment Masking (RaEM),
which prioritizes crucial information by selecting the next-best-view that
maximizes the expected information gain. This targeted approach aims to
minimize uncertainties surrounding the robot's path and enhance the safety of
its navigation. Our method offers a dual benefit: improved robot safety and
increased efficiency in risk-aware 3D scene reconstruction and understanding.
Extensive experiments in real-world scenarios demonstrate the effectiveness of
our proposed approach, highlighting its potential to establish a robust and
safety-focused framework for active robot exploration and 3D scene
understanding.



---

## Creating Seamless 3D Maps Using Radiance Fields

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-17 | Sai Tarun Sathyan, Thomas B. Kinsman | cs.CV | [PDF](http://arxiv.org/pdf/2403.11364v1){: .btn .btn-green } |

**Abstract**: It is desirable to create 3D object models and 3D maps from 2D input images
for applications such as navigation, virtual tourism, and urban planning. The
traditional methods of creating 3D maps, (such as photogrammetry), require a
large number of images and odometry. Additionally, traditional methods have
difficulty with reflective surfaces and specular reflections; windows and
chrome in the scene can be problematic. Google Road View is a familiar
application, which uses traditional methods to fuse a collection of 2D input
images into the illusion of a 3D map. However, Google Road View does not create
an actual 3D object model, only a collection of views. The objective of this
work is to create an actual 3D object model using updated techniques. Neural
Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the
capability to produce more precise and intricate 3D maps. Gaussian Splatting[4]
is another contemporary technique. This investigation compares Neural Radiance
Fields to Gaussian Splatting, and describes some of their inner workings. Our
primary contribution is a method for improving the results of the 3D
reconstructed models. Our results indicate that Gaussian Splatting was superior
to the NeRF technique.

Comments:
- 10 pages with figures

---

## 3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual  ReLocalization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-17 | Peng Jiang, Gaurav Pandey, Srikanth Saripalli | cs.CV | [PDF](http://arxiv.org/pdf/2403.11367v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel system designed for 3D mapping and visual
relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and
camera data to create accurate and visually plausible representations of the
environment. By leveraging LiDAR data to initiate the training of the 3D
Gaussian Splatting map, our system constructs maps that are both detailed and
geometrically accurate. To mitigate excessive GPU memory usage and facilitate
rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.
This preparation makes our method well-suited for visual localization tasks,
enabling efficient identification of correspondences between the query image
and the rendered image from the Gaussian Splatting map via normalized
cross-correlation (NCC). Additionally, we refine the camera pose of the query
image using feature-based matching and the Perspective-n-Point (PnP) technique.
The effectiveness, adaptability, and precision of our system are demonstrated
through extensive evaluation on the KITTI360 dataset.

Comments:
- 8 pages, 7 figures

---

## URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.10119v1){: .btn .btn-green } |

**Abstract**: We propose a novel rolling shutter bundle adjustment method for neural
radiance fields (NeRF), which utilizes the unordered rolling shutter (RS)
images to obtain the implicit 3D representation. Existing NeRF methods suffer
from low-quality images and inaccurate initial camera poses due to the RS
effect in the image, whereas, the previous method that incorporates the RS into
NeRF requires strict sequential data input, limiting its widespread
applicability. In constant, our method recovers the physical formation of RS
images by estimating camera poses and velocities, thereby removing the input
constraints on sequential data. Moreover, we adopt a coarse-to-fine training
strategy, in which the RS epipolar constraints of the pairwise frames in the
scene graph are used to detect the camera poses that fall into local minima.
The poses detected as outliers are corrected by the interpolation method with
neighboring poses. The experimental results validate the effectiveness of our
method over state-of-the-art works and demonstrate that the reconstruction of
3D representations is not constrained by the requirement of video sequence
input.



---

## DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Huiqiang Sun, Xingyi Li, Liao Shen, Xinyi Ye, Ke Xian, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2403.10103v1){: .btn .btn-green } |

**Abstract**: Recent advancements in dynamic neural radiance field methods have yielded
remarkable outcomes. However, these approaches rely on the assumption of sharp
input images. When faced with motion blur, existing dynamic NeRF methods often
struggle to generate high-quality novel views. In this paper, we propose
DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views
from a monocular video affected by motion blur. To account for motion blur in
input images, we simultaneously capture the camera trajectory and object
Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we
employ a global cross-time rendering approach to ensure consistent temporal
coherence across the entire scene. We curate a dataset comprising diverse
dynamic scenes that are specifically tailored for our task. Experimental
results on our dataset demonstrate that our method outperforms existing
approaches in generating sharp novel views from motion-blurred inputs while
maintaining spatial-temporal consistency of the scene.



---

## Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints  Scene Coordinate Regression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Huy-Hoang Bui, Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee | cs.CV | [PDF](http://arxiv.org/pdf/2403.10297v1){: .btn .btn-green } |

**Abstract**: Classical structural-based visual localization methods offer high accuracy
but face trade-offs in terms of storage, speed, and privacy. A recent
innovation, keypoint scene coordinate regression (KSCR) named D2S addresses
these issues by leveraging graph attention networks to enhance keypoint
relationships and predict their 3D coordinates using a simple multilayer
perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using
established 2D-3D correspondences. While KSCR achieves competitive results,
rivaling state-of-the-art image-retrieval methods like HLoc across multiple
benchmarks, its performance is hindered when data samples are limited due to
the deep learning model's reliance on extensive data. This paper proposes a
solution to this challenge by introducing a pipeline for keypoint descriptor
synthesis using Neural Radiance Field (NeRF). By generating novel poses and
feeding them into a trained NeRF model to create new views, our approach
enhances the KSCR's generalization capabilities in data-scarce environments.
The proposed system could significantly improve localization accuracy by up to
50\% and cost only a fraction of time for data synthesis. Furthermore, its
modular design allows for the integration of multiple NeRFs, offering a
versatile and efficient solution for visual localization. The implementation is
publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.



---

## Thermal-NeRF: Neural Radiance Fields from an Infrared Camera

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Tianxiang Ye, Qi Wu, Junyuan Deng, Guoqing Liu, Liu Liu, Songpengcheng Xia, Liang Pang, Wenxian Yu, Ling Pei | cs.CV | [PDF](http://arxiv.org/pdf/2403.10340v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant
potential in encoding highly-detailed 3D geometry and environmental appearance,
positioning themselves as a promising alternative to traditional explicit
representation for 3D scene reconstruction. However, the predominant reliance
on RGB imaging presupposes ideal lighting conditions: a premise frequently
unmet in robotic applications plagued by poor lighting or visual obstructions.
This limitation overlooks the capabilities of infrared (IR) cameras, which
excel in low-light detection and present a robust alternative under such
adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first
method that estimates a volumetric scene representation in the form of a NeRF
solely from IR imaging. By leveraging a thermal mapping and structural thermal
constraint derived from the thermal characteristics of IR imaging, our method
showcasing unparalleled proficiency in recovering NeRFs in visually degraded
scenes where RGB-based methods fall short. We conduct extensive experiments to
demonstrate that Thermal-NeRF can achieve superior quality compared to existing
methods. Furthermore, we contribute a dataset for IR-based NeRF applications,
paving the way for future research in IR NeRF reconstruction.



---

## FeatUp: A Model-Agnostic Framework for Features at Any Resolution

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, William T. Freeman | cs.CV | [PDF](http://arxiv.org/pdf/2403.10516v1){: .btn .btn-green } |

**Abstract**: Deep features are a cornerstone of computer vision research, capturing image
semantics and enabling the community to solve downstream tasks even in the
zero- or few-shot regime. However, these features often lack the spatial
resolution to directly perform dense prediction tasks like segmentation and
depth prediction because models aggressively pool information over large areas.
In this work, we introduce FeatUp, a task- and model-agnostic framework to
restore lost spatial information in deep features. We introduce two variants of
FeatUp: one that guides features with high-resolution signal in a single
forward pass, and one that fits an implicit model to a single image to
reconstruct features at any resolution. Both approaches use a multi-view
consistency loss with deep analogies to NeRFs. Our features retain their
original semantics and can be swapped into existing applications to yield
resolution and performance gains even without re-training. We show that FeatUp
significantly outperforms other feature upsampling and image super-resolution
approaches in class activation map generation, transfer learning for
segmentation and depth prediction, and end-to-end training for semantic
segmentation.

Comments:
- Accepted to the International Conference on Learning Representations
  (ICLR) 2024

---

## Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian  Splatting Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2403.10050v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting, emerging as a groundbreaking approach, has drawn
increasing attention for its capabilities of high-fidelity reconstruction and
real-time rendering. However, it couples the appearance and geometry of the
scene within the Gaussian attributes, which hinders the flexibility of editing
operations, such as texture swapping. To address this issue, we propose a novel
approach, namely Texture-GS, to disentangle the appearance from the geometry by
representing it as a 2D texture mapped onto the 3D surface, thereby
facilitating appearance editing. Technically, the disentanglement is achieved
by our proposed texture mapping module, which consists of a UV mapping MLP to
learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion
of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian
intersections, and a learnable texture to capture the fine-grained appearance.
Extensive experiments on the DTU dataset demonstrate that our method not only
facilitates high-fidelity appearance editing but also achieves real-time
rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.



---

## Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive  Experience

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Xiaohang Yu, Zhengxian Yang, Shi Pan, Yuqi Han, Haoxiang Wang, Jun Zhang, Shi Yan, Borong Lin, Lei Yang, Tao Yu, Lu Fang | cs.CV | [PDF](http://arxiv.org/pdf/2403.09973v1){: .btn .btn-green } |

**Abstract**: We have built a custom mobile multi-camera large-space dense light field
capture system, which provides a series of high-quality and sufficiently dense
light field images for various scenarios. Our aim is to contribute to the
development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF,
and 3D Gaussian splitting. More importantly, the collected dataset, which is
much denser than existing datasets, may also inspire space-oriented light field
reconstruction, which is potentially different from object-centric 3D
reconstruction, for immersive VR/AR experiences. We utilized a total of 40
GoPro 10 cameras, capturing images of 5k resolution. The number of photos
captured for each scene is no less than 1000, and the average density (view
number within a unit sphere) is 134.68. It is also worth noting that our system
is capable of efficiently capturing large outdoor scenes. Addressing the
current lack of large-space and dense light field datasets, we made efforts to
include elements such as sky, reflections, lights and shadows that are of
interest to researchers in the field of 3D reconstruction during the data
capture process. Finally, we validated the effectiveness of our provided
dataset on three popular algorithms and also integrated the reconstructed 3DGS
results into the Unity engine, demonstrating the potential of utilizing our
datasets to enhance the realism of virtual reality (VR) and create feasible
interactive spaces. The dataset is available at our project website.



---

## GGRt: Towards Generalizable 3D Gaussians without Pose Priors in  Real-Time

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han | cs.CV | [PDF](http://arxiv.org/pdf/2403.10147v1){: .btn .btn-green } |

**Abstract**: This paper presents GGRt, a novel approach to generalizable novel view
synthesis that alleviates the need for real camera poses, complexity in
processing high-resolution images, and lengthy optimization processes, thus
facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in
real-world scenarios. Specifically, we design a novel joint learning framework
that consists of an Iterative Pose Optimization Network (IPO-Net) and a
Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,
the proposed framework can inherently estimate robust relative pose information
from the image observations and thus primarily alleviate the requirement of
real camera poses. Moreover, we implement a deferred back-propagation mechanism
that enables high-resolution training and inference, overcoming the resolution
constraints of previous methods. To enhance the speed and efficiency, we
further introduce a progressive Gaussian cache module that dynamically adjusts
during training and inference. As the first pose-free generalizable 3D-GS
framework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at
$\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our
method outperforms existing NeRF-based pose-free techniques in terms of
inference speed and effectiveness. It can also approach the real pose-based
3D-GS methods. Our contributions provide a significant leap forward for the
integration of computer vision and computer graphics into practical
applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open
datasets and enabling real-time rendering for immersive experiences.



---

## Controllable Text-to-3D Generation via Surface-Aligned Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2403.09981v1){: .btn .btn-green } |

**Abstract**: While text-to-3D and image-to-3D generation tasks have received considerable
attention, one important but under-explored field between them is controllable
text-to-3D generation, which we mainly focus on in this work. To address this
task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network
architecture designed to enhance existing pre-trained multi-view diffusion
models by integrating additional input conditions, such as edge, depth, normal,
and scribble maps. Our innovation lies in the introduction of a conditioning
module that controls the base diffusion model using both local and global
embeddings, which are computed from the input condition images and camera
poses. Once trained, MVControl is able to offer 3D diffusion guidance for
optimization-based 3D generation. And, 2) we propose an efficient multi-stage
3D generation pipeline that leverages the benefits of recent large
reconstruction models and score distillation algorithm. Building upon our
MVControl architecture, we employ a unique hybrid diffusion guidance method to
direct the optimization process. In pursuit of efficiency, we adopt 3D
Gaussians as our representation instead of the commonly used implicit
representations. We also pioneer the use of SuGaR, a hybrid representation that
binds Gaussians to mesh triangle faces. This approach alleviates the issue of
poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained
geometry on the mesh. Extensive experiments demonstrate that our method
achieves robust generalization and enables the controllable generation of
high-quality 3D content.

Comments:
- Project page: https://lizhiqi49.github.io/MVControl/

---

## SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou | cs.CV | [PDF](http://arxiv.org/pdf/2403.10427v1){: .btn .btn-green } |

**Abstract**: Implicit neural representation methods have shown impressive advancements in
learning 3D scenes from unstructured in-the-wild photo collections but are
still limited by the large computational cost of volumetric rendering. More
recently, 3D Gaussian Splatting emerged as a much faster alternative with
superior rendering quality and training efficiency, especially for small-scale
and object-centric scenarios. Nevertheless, this technique suffers from poor
performance on unstructured in-the-wild data. To tackle this, we extend over 3D
Gaussian Splatting to handle unstructured image collections. We achieve this by
modeling appearance to seize photometric variations in the rendered images.
Additionally, we introduce a new mechanism to train transient Gaussians to
handle the presence of scene occluders in an unsupervised manner. Experiments
on diverse photo collection scenes and multi-pass acquisition of outdoor
landmarks show the effectiveness of our method over prior works achieving
state-of-the-art results with improved efficiency.



---

## FDGaussian: Fast Gaussian Splatting from Single Image via  Geometric-aware Diffusion Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2403.10242v1){: .btn .btn-green } |

**Abstract**: Reconstructing detailed 3D objects from single-view images remains a
challenging task due to the limited information available. In this paper, we
introduce FDGaussian, a novel two-stage framework for single-image 3D
reconstruction. Recent methods typically utilize pre-trained 2D diffusion
models to generate plausible novel views from the input image, yet they
encounter issues with either multi-view inconsistency or lack of geometric
fidelity. To overcome these challenges, we propose an orthogonal plane
decomposition mechanism to extract 3D geometric features from the 2D input,
enabling the generation of consistent multi-view images. Moreover, we further
accelerate the state-of-the-art Gaussian Splatting incorporating epipolar
attention to fuse images from different viewpoints. We demonstrate that
FDGaussian generates images with high consistency across different views and
reconstructs high-quality 3D objects, both qualitatively and quantitatively.
More examples can be found at our website https://qjfeng.net/FDGaussian/.



---

## GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary  Robotic Grasping

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.09637v1){: .btn .btn-green } |

**Abstract**: Constructing a 3D scene capable of accommodating open-ended language queries,
is a pivotal pursuit, particularly within the domain of robotics. Such
technology facilitates robots in executing object manipulations based on human
language directives. To tackle this challenge, some research efforts have been
dedicated to the development of language-embedded implicit fields. However,
implicit fields (e.g. NeRF) encounter limitations due to the necessity of
processing a large number of input views for reconstruction, coupled with their
inherent inefficiencies in inference. Thus, we present the GaussianGrasper,
which utilizes 3D Gaussian Splatting to explicitly represent the scene as a
collection of Gaussian primitives. Our approach takes a limited set of RGB-D
views and employs a tile-based splatting technique to create a feature field.
In particular, we propose an Efficient Feature Distillation (EFD) module that
employs contrastive learning to efficiently and accurately distill language
embeddings derived from foundational models. With the reconstructed geometry of
the Gaussian field, our method enables the pre-trained grasping model to
generate collision-free grasp pose candidates. Furthermore, we propose a
normal-guided grasp module to select the best grasp pose. Through comprehensive
real-world experiments, we demonstrate that GaussianGrasper enables robots to
accurately query and grasp objects with language instructions, providing a new
solution for language-guided manipulation tasks. Data and codes can be
available at https://github.com/MrSecant/GaussianGrasper.



---

## A New Split Algorithm for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu | cs.GR | [PDF](http://arxiv.org/pdf/2403.09143v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting models, as a novel explicit 3D representation, have
been applied in many domains recently, such as explicit geometric editing and
geometry generation. Progress has been rapid. However, due to their mixed
scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred
or needle-like effect near the surface. At the same time, 3D Gaussian splatting
models tend to flatten large untextured regions, yielding a very sparse point
cloud. These problems are caused by the non-uniform nature of 3D Gaussian
splatting models, so in this paper, we propose a new 3D Gaussian splitting
algorithm, which can produce a more uniform and surface-bounded 3D Gaussian
splatting model. Our algorithm splits an $N$-dimensional Gaussian into two
N-dimensional Gaussians. It ensures consistency of mathematical characteristics
and similarity of appearance, allowing resulting 3D Gaussian splatting models
to be more uniform and a better fit to the underlying surface, and thus more
suitable for explicit editing, point cloud extraction and other tasks.
Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form
solution, making it readily applicable to any 3D Gaussian model.

Comments:
- 11 pages, 10 figures

---

## PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF  Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2403.09079v1){: .btn .btn-green } |

**Abstract**: Autonomous vehicles rely extensively on perception systems to navigate and
interpret their surroundings. Despite significant advancements in these systems
recently, challenges persist under conditions like occlusion, extreme lighting,
or in unfamiliar urban areas. Unlike these systems, humans do not solely depend
on immediate observations to perceive the environment. In navigating new
cities, humans gradually develop a preliminary mental map to supplement
real-time perception during subsequent visits. Inspired by this human approach,
we introduce a novel framework, Pre-Sight, that leverages past traversals to
construct static prior memories, enhancing online perception in later
navigations. Our method involves optimizing a city-scale neural radiance field
with data from previous journeys to generate neural priors. These priors, rich
in semantic and geometric details, are derived without manual annotations and
can seamlessly augment various state-of-the-art perception models, improving
their efficacy with minimal additional computational cost. Experimental results
on the nuScenes dataset demonstrate the framework's high compatibility with
diverse online perception models. Specifically, it shows remarkable
improvements in HD-map construction and occupancy prediction tasks,
highlighting its potential as a new perception framework for autonomous driving
systems. Our code will be released at
https://github.com/yuantianyuan01/PreSight.



---

## RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban  Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Thang-Anh-Quan Nguyen, Luis Roldão, Nathan Piasco, Moussab Bennehar, Dzmitry Tsishkou | cs.CV | [PDF](http://arxiv.org/pdf/2403.09419v1){: .btn .btn-green } |

**Abstract**: The task of separating dynamic objects from static environments using NeRFs
has been widely studied in recent years. However, capturing large-scale scenes
still poses a challenge due to their complex geometric structures and
unconstrained dynamics. Without the help of 3D motion cues, previous methods
often require simplified setups with slow camera motion and only a few/single
dynamic actors, leading to suboptimal solutions in most urban setups. To
overcome such limitations, we present RoDUS, a pipeline for decomposing static
and dynamic elements in urban scenes, with thoughtfully separated NeRF models
for moving and non-moving components. Our approach utilizes a robust
kernel-based initialization coupled with 4D semantic information to selectively
guide the learning process. This strategy enables accurate capturing of the
dynamics in the scene, resulting in reduced artifacts caused by NeRF on
background reconstruction, all by using self-supervision. Notably, experimental
evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of
our method in decomposing challenging urban scenes into precise static and
dynamic components.



---

## 3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Frank Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, Changqing Zou | cs.CV | [PDF](http://arxiv.org/pdf/2403.09439v1){: .btn .btn-green } |

**Abstract**: Text-driven 3D scene generation techniques have made rapid progress in recent
years. Their success is mainly attributed to using existing generative models
to iteratively perform image warping and inpainting to generate 3D scenes.
However, these methods heavily rely on the outputs of existing models, leading
to error accumulation in geometry and appearance that prevent the models from
being used in various scenarios (e.g., outdoor and unreal scenarios). To
address this limitation, we generatively refine the newly generated local views
by querying and aggregating global 3D information, and then progressively
generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF
as a unified representation of the 3D scene to constrain global 3D consistency,
and propose a generative refinement network to synthesize new contents with
higher quality by exploiting the natural image prior from 2D diffusion model as
well as the global 3D information of the current scene. Our extensive
experiments demonstrate that, in comparison to previous methods, our approach
supports wide variety of scene generation and arbitrary camera trajectories
with improved visual quality and 3D consistency.

Comments:
- 11 pages, 7 figures

---

## Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III | cs.RO | [PDF](http://arxiv.org/pdf/2403.09875v1){: .btn .btn-green } |

**Abstract**: In this work, we propose a novel method to supervise 3D Gaussian Splatting
(3DGS) scenes using optical tactile sensors. Optical tactile sensors have
become widespread in their use in robotics for manipulation and object
representation; however, raw optical tactile sensor data is unsuitable to
directly supervise a 3DGS scene. Our representation leverages a Gaussian
Process Implicit Surface to implicitly represent the object, combining many
touches into a unified representation with uncertainty. We merge this model
with a monocular depth estimation network, which is aligned in a two stage
process, coarsely aligning with a depth camera and then finely adjusting to
match our touch data. For every training image, our method produces a
corresponding fused depth and uncertainty map. Utilizing this additional
information, we propose a new loss function, variance weighted depth supervised
loss, for training the 3DGS scene model. We leverage the DenseTact optical
tactile sensor and RealSense RGB-D camera to show that combining touch and
vision in this manner leads to quantitatively and qualitatively better results
than vision or touch alone in a few-view scene syntheses on opaque as well as
on reflective and transparent objects. Please see our project page at
http://armlabstanford.github.io/touch-gs



---

## The NeRFect Match: Exploring NeRF Features for Visual Localization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé | cs.CV | [PDF](http://arxiv.org/pdf/2403.09577v1){: .btn .btn-green } |

**Abstract**: In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene
representation for visual localization. Recently, NeRF has been employed to
enhance pose regression and scene coordinate regression models by augmenting
the training database, providing auxiliary supervision through rendered images,
or serving as an iterative refinement module. We extend its recognized
advantages -- its ability to provide a compact scene representation with
realistic appearances and accurate geometry -- by exploring the potential of
NeRF's internal features in establishing precise 2D-3D matches for
localization. To this end, we conduct a comprehensive examination of NeRF's
implicit knowledge, acquired through view synthesis, for matching under various
conditions. This includes exploring different matching network architectures,
extracting encoder features at multiple layers, and varying training
configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D
matching function that capitalizes on the internal knowledge of NeRF learned
via view synthesis. Our evaluation of NeRFMatch on standard localization
benchmarks, within a structure-based pipeline, sets a new state-of-the-art for
localization performance on Cambridge Landmarks.



---

## VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Nicolaj Schmid, Cornelius von Einem, Cesar Cadena, Roland Siegwart, Lorenz Hruby, Florian Tschopp | cs.RO | [PDF](http://arxiv.org/pdf/2403.09477v1){: .btn .btn-green } |

**Abstract**: Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.



---

## Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2403.09413v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has recently demonstrated impressive
capabilities in real-time novel view synthesis and 3D reconstruction. However,
3DGS heavily depends on the accurate initialization derived from
Structure-from-Motion (SfM) methods. When trained with randomly initialized
point clouds, 3DGS fails to maintain its ability to produce high-quality
images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive
analysis of SfM initialization in the frequency domain and analysis of a 1D
regression task with multiple 1D Gaussians, we propose a novel optimization
strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D
Gaussian Splatting), that successfully trains 3D Gaussians from random point
clouds. We show the effectiveness of our strategy through quantitative and
qualitative comparisons on multiple datasets, largely improving the performance
in all settings. Our project page and code can be found at
https://ku-cvlab.github.io/RAIN-GS.

Comments:
- Project Page: https://ku-cvlab.github.io/RAIN-GS

---

## NeRF-Supervised Feature Point Detection and Description

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Ali Youssef, Francisco Vasconcelos | cs.CV | [PDF](http://arxiv.org/pdf/2403.08156v1){: .btn .btn-green } |

**Abstract**: Feature point detection and description is the backbone for various computer
vision applications, such as Structure-from-Motion, visual SLAM, and visual
place recognition. While learning-based methods have surpassed traditional
handcrafted techniques, their training often relies on simplistic
homography-based simulations of multi-view perspectives, limiting model
generalisability. This paper introduces a novel approach leveraging neural
radiance fields (NeRFs) for realistic multi-view training data generation. We
create a diverse multi-view dataset using NeRFs, consisting of indoor and
outdoor scenes. Our proposed methodology adapts state-of-the-art feature
detectors and descriptors to train on NeRF-synthesised views supervised by
perspective projective geometry. Our experiments demonstrate that the proposed
methods achieve competitive or superior performance on standard benchmarks for
relative pose estimation, point cloud registration, and homography estimation
while requiring significantly less training data compared to existing
approaches.



---

## GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting  Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu | cs.CV | [PDF](http://arxiv.org/pdf/2403.08733v2){: .btn .btn-green } |

**Abstract**: We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed
by the 3D Gaussian Splatting (3DGS).
  Our method first renders a collection of images by using the 3DGS and edits
them by using a pre-trained 2D diffusion model (ControlNet) based on the input
prompt, which is then used to optimise the 3D model.
  Our key contribution is multi-view consistent editing, which enables editing
all images together instead of iteratively editing one image while updating the
3D model as in previous works.
  It leads to faster editing as well as higher visual quality.
  This is achieved by the two terms:
  (a) depth-conditioned editing that enforces geometric consistency across
multi-view images by leveraging naturally consistent depth maps.
  (b) attention-based latent code alignment that unifies the appearance of
edited images by conditioning their editing to several reference views through
self and cross-view attention between images' latent representations.
  Experiments demonstrate that our method achieves faster editing and better
visual results than previous state-of-the-art methods.

Comments:
- 17 pages

---

## StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Hongbin Xu, Weitao Chen, Feng Xiao, Baigui Sun, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2403.08310v1){: .btn .btn-green } |

**Abstract**: 4D style transfer aims at transferring arbitrary visual style to the
synthesized novel views of a dynamic 4D scene with varying viewpoints and
times. Existing efforts on 3D style transfer can effectively combine the visual
features of style images and neural radiance fields (NeRF) but fail to handle
the 4D dynamic scenes limited by the static scene assumption. Consequently, we
aim to handle the novel challenging problem of 4D style transfer for the first
time, which further requires the consistency of stylized results on dynamic
objects. In this paper, we introduce StyleDyRF, a method that represents the 4D
feature space by deforming a canonical feature volume and learns a linear style
transformation matrix on the feature volume in a data-driven fashion. To obtain
the canonical feature volume, the rays at each time step are deformed with the
geometric prior of a pre-trained dynamic NeRF to render the feature map under
the supervision of pre-trained visual encoders. With the content and style cues
in the canonical feature volume and the style image, we can learn the style
transformation matrix from their covariance matrices with lightweight neural
networks. The learned style transformation matrix can reflect a direct matching
of feature covariance from the content volume to the given style pattern, in
analogy with the optimization of the Gram matrix in traditional 2D neural style
transfer. The experimental results show that our method not only renders 4D
photorealistic style transfer results in a zero-shot manner but also
outperforms existing methods in terms of visual quality and consistency.

Comments:
- In submission. The code and model are released at:
  https://github.com/ToughStoneX/StyleDyRF

---

## GaussianImage: 1000 FPS Image Representation and Compression by 2D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, Jun Zhang | eess.IV | [PDF](http://arxiv.org/pdf/2403.08551v2){: .btn .btn-green } |

**Abstract**: Implicit neural representations (INRs) recently achieved great success in
image representation and compression, offering high visual quality and fast
rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are
available. However, this requirement often hinders their use on low-end devices
with limited memory. In response, we propose a groundbreaking paradigm of image
representation and compression by 2D Gaussian Splatting, named GaussianImage.
We first introduce 2D Gaussian to represent the image, where each Gaussian has
8 parameters including position, covariance and color. Subsequently, we unveil
a novel rendering algorithm based on accumulated summation. Remarkably, our
method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster
fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation
performance, but also delivers a faster rendering speed of 1500-2000 FPS
regardless of parameter size. Furthermore, we integrate existing vector
quantization technique to build an image codec. Experimental results
demonstrate that our codec attains rate-distortion performance comparable to
compression-based INRs such as COIN and COIN++, while facilitating decoding
speeds of approximately 1000 FPS. Additionally, preliminary proof of concept
shows that our codec surpasses COIN and COIN++ in performance when using
partial bits-back coding.



---

## ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic  Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang | cs.RO | [PDF](http://arxiv.org/pdf/2403.08321v1){: .btn .btn-green } |

**Abstract**: Performing language-conditioned robotic manipulation tasks in unstructured
environments is highly demanded for general intelligent robots. Conventional
robotic manipulation methods usually learn semantic representation of the
observation for action prediction, which ignores the scene-level spatiotemporal
dynamics for human goal completion. In this paper, we propose a dynamic
Gaussian Splatting method named ManiGaussian for multi-task robotic
manipulation, which mines scene dynamics via future scene reconstruction.
Specifically, we first formulate the dynamic Gaussian Splatting framework that
infers the semantics propagation in the Gaussian embedding space, where the
semantic representation is leveraged to predict the optimal robot action. Then,
we build a Gaussian world model to parameterize the distribution in our dynamic
Gaussian Splatting framework, which provides informative supervision in the
interactive environment via future scene reconstruction. We evaluate our
ManiGaussian on 10 RLBench tasks with 166 variations, and the results
demonstrate our framework can outperform the state-of-the-art methods by 13.1\%
in average success rate.



---

## Gaussian Splatting in Style

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2403.08498v1){: .btn .btn-green } |

**Abstract**: Scene stylization extends the work of neural style transfer to three spatial
dimensions. A vital challenge in this problem is to maintain the uniformity of
the stylized appearance across a multi-view setting. A vast majority of the
previous works achieve this by optimizing the scene with a specific style
image. In contrast, we propose a novel architecture trained on a collection of
style images, that at test time produces high quality stylized novel views. Our
work builds up on the framework of 3D Gaussian splatting. For a given scene, we
take the pretrained Gaussians and process them using a multi resolution hash
grid and a tiny MLP to obtain the conditional stylised views. The explicit
nature of 3D Gaussians give us inherent advantages over NeRF-based methods
including geometric consistency, along with having a fast training and
rendering regime. This enables our method to be useful for vast practical use
cases such as in augmented or virtual reality applications. Through our
experiments, we show our methods achieve state-of-the-art performance with
superior visual quality on various indoor and outdoor real-world data.



---

## SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2403.07547v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) has attracted considerable attention for their
exceptional ability in synthesizing novel views with high fidelity. However,
the presence of motion blur, resulting from slight camera movements during
extended shutter exposures, poses a significant challenge, potentially
compromising the quality of the reconstructed 3D scenes. While recent studies
have addressed this issue, they do not consider the continuous dynamics of
camera movements during image acquisition, leading to inaccurate scene
reconstruction. Additionally, these methods are plagued by slow training and
rendering speed. To effectively handle these issues, we propose sequential
motion understanding radiance fields (SMURF), a novel approach that employs
neural ordinary differential equation (Neural-ODE) to model continuous camera
motion and leverages the explicit volumetric representation method for faster
training and robustness to motion-blurred input images. The core idea of the
SMURF is continuous motion blurring kernel (CMBK), a unique module designed to
model a continuous camera movements for processing blurry inputs. Our model,
rigorously evaluated against benchmark datasets, demonstrates state-of-the-art
performance both quantitatively and qualitatively.

Comments:
- 25 pages, 10 figures, Code is available at
  https://github.com/Jho-Yonsei/SMURF

---

## SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.07494v1){: .btn .btn-green } |

**Abstract**: We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D
Gaussian representation, that enables accurate 3D semantic mapping, robust
camera tracking, and high-quality rendering in real-time. In this system, we
incorporate semantic feature embedding into 3D Gaussian representation, which
effectively encodes semantic information within the spatial layout of the
environment for precise semantic scene representation. Furthermore, we propose
feature-level loss for updating 3D Gaussian representation, enabling
higher-level guidance for 3D Gaussian optimization. In addition, to reduce
cumulative drift and improve reconstruction accuracy, we introduce
semantic-informed bundle adjustment leveraging semantic associations for joint
optimization of 3D Gaussian representation and camera poses, leading to more
robust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates
superior performance over existing dense semantic SLAM methods in terms of
mapping and tracking accuracy on Replica and ScanNet datasets, while also
showing excellent capabilities in novel-view semantic synthesis and 3D semantic
mapping.



---

## Q-SLAM: Quadric Representations for Monocular SLAM

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Chensheng Peng, Chenfeng Xu, Yue Wang, Mingyu Ding, Heng Yang, Masayoshi Tomizuka, Kurt Keutzer, Marco Pavone, Wei Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2403.08125v1){: .btn .btn-green } |

**Abstract**: Monocular SLAM has long grappled with the challenge of accurately modeling 3D
geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular
SLAM have shown promise, yet these methods typically focus on novel view
synthesis rather than precise 3D geometry modeling. This focus results in a
significant disconnect between NeRF applications, i.e., novel-view synthesis
and the requirements of SLAM. We identify that the gap results from the
volumetric representations used in NeRF, which are often dense and noisy. In
this study, we propose a novel approach that reimagines volumetric
representations through the lens of quadric forms. We posit that most scene
components can be effectively represented as quadric planes. Leveraging this
assumption, we reshape the volumetric representations with million of cubes by
several quadric planes, which leads to more accurate and efficient modeling of
3D scenes in SLAM contexts. Our method involves two key steps: First, we use
the quadric assumption to enhance coarse depth estimations obtained from
tracking modules, e.g., Droid-SLAM. This step alone significantly improves
depth estimation accuracy. Second, in the subsequent mapping phase, we diverge
from previous NeRF-based SLAM methods that distribute sampling points across
the entire volume space. Instead, we concentrate sampling points around quadric
planes and aggregate them using a novel quadric-decomposed Transformer.
Additionally, we introduce an end-to-end joint optimization strategy that
synchronizes pose estimation with 3D reconstruction.



---

## StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2403.07807v1){: .btn .btn-green } |

**Abstract**: We introduce StyleGaussian, a novel 3D style transfer technique that allows
instant transfer of any image's style to a 3D scene at 10 frames per second
(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style
transfer without compromising its real-time rendering ability and multi-view
consistency. It achieves instant style transfer with three steps: embedding,
transfer, and decoding. Initially, 2D VGG scene features are embedded into
reconstructed 3D Gaussians. Next, the embedded features are transformed
according to a reference style image. Finally, the transformed features are
decoded into the stylized RGB. StyleGaussian has two novel designs. The first
is an efficient feature rendering strategy that first renders low-dimensional
features and then maps them into high-dimensional features while embedding VGG
features. It cuts the memory consumption significantly and enables 3DGS to
render the high-dimensional memory-intensive features. The second is a
K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized
features, it eliminates the 2D CNN operations that compromise strict multi-view
consistency. Extensive experiments show that StyleGaussian achieves instant 3D
stylization with superior stylization quality while preserving real-time
rendering and strict multi-view consistency. Project page:
https://kunhao-liu.github.io/StyleGaussian/



---

## Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Chenhao Zhang, Yongyang Zhou, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2403.06505v1){: .btn .btn-green } |

**Abstract**: The neural radiance field (NeRF) has emerged as a prominent methodology for
synthesizing realistic images of novel views. While neural radiance
representations based on voxels or mesh individually offer distinct advantages,
excelling in either rendering quality or speed, each has limitations in the
other aspect. In response, we propose a pioneering hybrid representation named
Vosh, seamlessly combining both voxel and mesh components in hybrid rendering
for view synthesis. Vosh is meticulously crafted by optimizing the voxel grid
of NeRF, strategically with selected voxels replaced by mesh. Therefore, it
excels in fast rendering scenes with simple geometry and textures through its
mesh component, while simultaneously enabling high-quality rendering in
intricate regions by leveraging voxel component. The flexibility of Vosh is
showcased through the ability to adjust hybrid ratios, providing users the
ability to control the balance between rendering quality and speed based on
flexible usage. Experimental results demonstrates that our method achieves
commendable trade-off between rendering quality and speed, and notably has
real-time performance on mobile devices.



---

## DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with  Global-Local Depth Normalization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu | cs.CV | [PDF](http://arxiv.org/pdf/2403.06912v2){: .btn .btn-green } |

**Abstract**: Radiance fields have demonstrated impressive performance in synthesizing
novel views from sparse input views, yet prevailing methods suffer from high
training costs and slow inference speed. This paper introduces DNGaussian, a
depth-regularized framework based on 3D Gaussian radiance fields, offering
real-time and high-quality few-shot novel view synthesis at low costs. Our
motivation stems from the highly efficient representation and surprising
quality of the recent 3D Gaussian Splatting, despite it will encounter a
geometry degradation when input views decrease. In the Gaussian radiance
fields, we find this degradation in scene geometry primarily lined to the
positioning of Gaussian primitives and can be mitigated by depth constraint.
Consequently, we propose a Hard and Soft Depth Regularization to restore
accurate scene geometry under coarse monocular depth supervision while
maintaining a fine-grained color appearance. To further refine detailed
geometry reshaping, we introduce Global-Local Depth Normalization, enhancing
the focus on small local depth changes. Extensive experiments on LLFF, DTU, and
Blender datasets demonstrate that DNGaussian outperforms state-of-the-art
methods, achieving comparable or better results with significantly reduced
memory cost, a $25 \times$ reduction in training time, and over $3000 \times$
faster rendering speed.

Comments:
- Accepted at CVPR 2024. Project page:
  https://fictionarry.github.io/DNGaussian/

---

## FSViewFusion: Few-Shots View Generation of Novel Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Rukhshanda Hussain, Hui Xian Grace Lim, Borchun Chen, Mubarak Shah, Ser Nam Lim | cs.CV | [PDF](http://arxiv.org/pdf/2403.06394v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis has observed tremendous developments since the arrival
of NeRFs. However, Nerf models overfit on a single scene, lacking
generalization to out of distribution objects. Recently, diffusion models have
exhibited remarkable performance on introducing generalization in view
synthesis. Inspired by these advancements, we explore the capabilities of a
pretrained stable diffusion model for view synthesis without explicit 3D
priors. Specifically, we base our method on a personalized text to image model,
Dreambooth, given its strong ability to adapt to specific novel objects with a
few shots. Our research reveals two interesting findings. First, we observe
that Dreambooth can learn the high level concept of a view, compared to
arguably more complex strategies which involve finetuning diffusions on large
amounts of multi-view data. Second, we establish that the concept of a view can
be disentangled and transferred to a novel object irrespective of the original
object's identify from which the views are learnt. Motivated by this, we
introduce a learning strategy, FSViewFusion, which inherits a specific view
through only one image sample of a single scene, and transfers the knowledge to
a novel object, learnt from few shots, using low rank adapters. Through
extensive experiments we demonstrate that our method, albeit simple, is
efficient in generating reliable view samples for in the wild images. Code and
models will be released.



---

## FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing | cs.CV | [PDF](http://arxiv.org/pdf/2403.06908v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has achieved very impressive performance in real-time
novel view synthesis. However, it often suffers from over-reconstruction during
Gaussian densification where high-variance image regions are covered by a few
large Gaussians only, leading to blur and artifacts in the rendered images. We
design a progressive frequency regularization (FreGS) technique to tackle the
over-reconstruction issue within the frequency space. Specifically, FreGS
performs coarse-to-fine Gaussian densification by exploiting low-to-high
frequency components that can be easily extracted with low-pass and high-pass
filters in the Fourier space. By minimizing the discrepancy between the
frequency spectrum of the rendered image and the corresponding ground truth, it
achieves high-quality Gaussian densification and alleviates the
over-reconstruction of Gaussian splatting effectively. Experiments over
multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and
Deep Blending) show that FreGS achieves superior novel view synthesis and
outperforms the state-of-the-art consistently.



---

## SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields  for Robotic Inspection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Yifu Tao, Yash Bhalgat, Lanke Frank Tarimo Fu, Matias Mattamala, Nived Chebrolu, Maurice Fallon | cs.RO | [PDF](http://arxiv.org/pdf/2403.06877v1){: .btn .btn-green } |

**Abstract**: We present a neural-field-based large-scale reconstruction system that fuses
lidar and vision data to generate high-quality reconstructions that are
geometrically accurate and capture photo-realistic textures. This system adapts
the state-of-the-art neural radiance field (NeRF) representation to also
incorporate lidar data which adds strong geometric constraints on the depth and
surface normals. We exploit the trajectory from a real-time lidar SLAM system
to bootstrap a Structure-from-Motion (SfM) procedure to both significantly
reduce the computation time and to provide metric scale which is crucial for
lidar depth loss. We use submapping to scale the system to large-scale
environments captured over long trajectories. We demonstrate the reconstruction
system with data from a multi-camera, lidar sensor suite onboard a legged
robot, hand-held while scanning building scenes for 600 metres, and onboard an
aerial robot surveying a multi-storey mock disaster site-building. Website:
https://ori-drs.github.io/projects/silvr/

Comments:
- Accepted at ICRA 2024; Website:
  https://ori-drs.github.io/projects/silvr/

---

## Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View  Synthesis?

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-10 | Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen | cs.CV | [PDF](http://arxiv.org/pdf/2403.06092v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has achieved superior performance for novel view
synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a
volume rendering procedure, however, when fewer known views are given (i.e.,
few-shot view synthesis), the model is prone to overfit the given views. To
handle this issue, previous efforts have been made towards leveraging learned
priors or introducing additional regularizations. In contrast, in this paper,
we for the first time provide an orthogonal method from the perspective of
network structure. Given the observation that trivially reducing the number of
model parameters alleviates the overfitting issue, but at the cost of missing
details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs
(i.e., location and viewing direction) of the vanilla MLP into each layer to
prevent the overfitting issue without harming detailed synthesis. To further
reduce the artifacts, we propose to model colors and volume density separately
and present two regularization terms. Extensive experiments on multiple
datasets demonstrate that: 1) although the proposed mi-MLP is easy to
implement, it is surprisingly effective as it boosts the PSNR of the baseline
from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art
results on a wide range of benchmarks. We will release the code upon
publication.

Comments:
- Accepted by CVPR 2024

---

## Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous  Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-09 | Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma | cs.CV | [PDF](http://arxiv.org/pdf/2403.05907v1){: .btn .btn-green } |

**Abstract**: Recent studies have highlighted the promising application of NeRF in
autonomous driving contexts. However, the complexity of outdoor environments,
combined with the restricted viewpoints in driving scenarios, complicates the
task of precisely reconstructing scene geometry. Such challenges often lead to
diminished quality in reconstructions and extended durations for both training
and rendering. To tackle these challenges, we present Lightning NeRF. It uses
an efficient hybrid scene representation that effectively utilizes the geometry
prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly
improves the novel view synthesis performance of NeRF and reduces computational
overheads. Through evaluations on real-world datasets, such as KITTI-360,
Argoverse2, and our private dataset, we demonstrate that our approach not only
exceeds the current state-of-the-art in novel view synthesis quality but also
achieves a five-fold increase in training speed and a ten-fold improvement in
rendering speed. Codes are available at
https://github.com/VISION-SJTU/Lightning-NeRF .

Comments:
- Accepted to ICRA 2024

---

## Large Generative Model Assisted 3D Semantic Communication

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-09 | Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You | cs.IT | [PDF](http://arxiv.org/pdf/2403.05783v1){: .btn .btn-green } |

**Abstract**: Semantic Communication (SC) is a novel paradigm for data transmission in 6G.
However, there are several challenges posed when performing SC in 3D scenarios:
1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain
channel estimation. To address these issues, we propose a Generative AI Model
assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor
(3DSE), which employs generative AI models, including Segment Anything Model
(SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D
scenario based on user requirements. The extracted 3D semantics are represented
as multi-perspective images of the goal-oriented 3D object. Then, we present an
Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective
images, in which we use a semantic encoder with two output heads to perform
semantic encoding and mask redundant semantics in the latent semantic space,
respectively. Next, we design a conditional Generative adversarial network and
Diffusion model aided-Channel Estimation (GDCE) to estimate and refine the
Channel State Information (CSI) of physical channels. Finally, simulation
results demonstrate the advantages of the proposed GAM-3DSC system in
effectively transmitting the goal-oriented 3D scenario.

Comments:
- 13 pages,13 figures,1 table

---

## GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-08 | Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà | cs.CV | [PDF](http://arxiv.org/pdf/2403.05154v1){: .btn .btn-green } |

**Abstract**: We present GSEdit, a pipeline for text-guided 3D object editing based on
Gaussian Splatting models. Our method enables the editing of the style and
appearance of 3D objects without altering their main details, all in a matter
of minutes on consumer hardware. We tackle the problem by leveraging Gaussian
splatting to represent 3D scenes, and we optimize the model while progressively
varying the image supervision by means of a pretrained image-based diffusion
model. The input object may be given as a 3D triangular mesh, or directly
provided as Gaussians from a generative model such as DreamGaussian. GSEdit
ensures consistency across different viewpoints, maintaining the integrity of
the original object's information. Compared to previously proposed methods
relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making
3D editing tasks much faster. Our editing process is refined via the
application of the SDS loss, ensuring that our edits are both precise and
accurate. Our comprehensive evaluation demonstrates that GSEdit effectively
alters object shape and appearance following the given textual instructions
while preserving their coherence and detail.

Comments:
- 15 pages, 7 figures

---

## SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-08 | Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang | cs.GR | [PDF](http://arxiv.org/pdf/2403.05087v1){: .btn .btn-green } |

**Abstract**: We present SplattingAvatar, a hybrid 3D representation of photorealistic
human avatars with Gaussian Splatting embedded on a triangle mesh, which
renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We
disentangle the motion and appearance of a virtual human with explicit mesh
geometry and implicit appearance modeling with Gaussian Splatting. The
Gaussians are defined by barycentric coordinates and displacement on a triangle
mesh as Phong surfaces. We extend lifted optimization to simultaneously
optimize the parameters of the Gaussians while walking on the triangle mesh.
SplattingAvatar is a hybrid representation of virtual humans where the mesh
represents low-frequency motion and surface deformation, while the Gaussians
take over the high-frequency geometry and detailed appearance. Unlike existing
deformation methods that rely on an MLP-based linear blend skinning (LBS) field
for motion, we control the rotation and translation of the Gaussians directly
by mesh, which empowers its compatibility with various animation techniques,
e.g., skeletal animation, blend shapes, and mesh editing. Trainable from
monocular videos for both full-body and head avatars, SplattingAvatar shows
state-of-the-art rendering quality across multiple datasets.

Comments:
- [CVPR 2024] Code and data are available at
  https://github.com/initialneil/SplattingAvatar

---

## Finding Waldo: Towards Efficient Exploration of NeRF Scene Spaces

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Evangelos Skartados, Mehmet Kerim Yucel, Bruno Manganelli, Anastasios Drosou, Albert Saà-Garriga | cs.CV | [PDF](http://arxiv.org/pdf/2403.04508v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D
reconstruction and novel view synthesis in recent years due to their remarkable
performance. Despite the huge interest in NeRF methods, a practical use case of
NeRFs has largely been ignored; the exploration of the scene space modelled by
a NeRF. In this paper, for the first time in the literature, we propose and
formally define the scene exploration framework as the efficient discovery of
NeRF model inputs (i.e. coordinates and viewing angles), using which one can
render novel views that adhere to user-selected criteria. To remedy the lack of
approaches addressing scene exploration, we first propose two baseline methods
called Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).
We then cast scene exploration as an optimization problem, and propose the
criteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient
exploration. We test all three approaches with various criteria (e.g. saliency
maximization, image quality maximization, photo-composition quality
improvement) and show that our EGPS performs more favourably than other
baselines. We finally highlight key points and limitations, and outline
directions for future research in scene exploration.

Comments:
- Accepted at ACM MMSys'24

---

## BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa | cs.CV | [PDF](http://arxiv.org/pdf/2403.04926v1){: .btn .btn-green } |

**Abstract**: Recent efforts in using 3D Gaussians for scene reconstruction and novel view
synthesis can achieve impressive results on curated benchmarks; however, images
captured in real life are often blurry. In this work, we analyze the robustness
of Gaussian-Splatting-based methods against various image blur, such as motion
blur, defocus blur, downscaling blur, \etc. Under these degradations,
Gaussian-Splatting-based methods tend to overfit and produce worse results than
Neural-Radiance-Field-based methods. To address this issue, we propose Blur
Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling
capacities such that a 3D-consistent and high quality scene can be
reconstructed despite image-wise blur. Specifically, we model blur by
estimating per-pixel convolution kernels from a Blur Proposal Network (BPN).
BPN is designed to consider spatial, color, and depth variations of the scene
to maximize modeling capacity. Additionally, BPN also proposes a
quality-assessing mask, which indicates regions where blur occur. Finally, we
introduce a coarse-to-fine kernel optimization scheme; this optimization scheme
is fast and avoids sub-optimal solutions due to a sparse point cloud
initialization, which often occurs when we apply Structure-from-Motion on
blurry images. We demonstrate that BAGS achieves photorealistic renderings
under various challenging blur conditions and imaging geometry, while
significantly improving upon existing approaches.



---

## Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Nikhil Mishra, Maximilian Sieb, Pieter Abbeel, Xi Chen | cs.RO | [PDF](http://arxiv.org/pdf/2403.04114v1){: .btn .btn-green } |

**Abstract**: Deep learning methods for perception are the cornerstone of many robotic
systems. Despite their potential for impressive performance, obtaining
real-world training data is expensive, and can be impractically difficult for
some tasks. Sim-to-real transfer with domain randomization offers a potential
workaround, but often requires extensive manual tuning and results in models
that are brittle to distribution shift between sim and real. In this work, we
introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF
model that is the centerpiece of a real-to-sim pipeline for synthesizing
training data targeted to scenes and objects from the real world. COV-NeRF
extracts objects from real images and composes them into new scenes, generating
photorealistic renderings and many types of 2D and 3D supervision, including
depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the
rendering quality of modern NeRF methods, and can be used to rapidly close the
sim-to-real gap across a variety of perceptual modalities.

Comments:
- ICRA 2024

---

## DNAct: Diffusion Guided Multi-Task 3D Policy Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Ge Yan, Yueh-Hua Wu, Xiaolong Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.04115v2){: .btn .btn-green } |

**Abstract**: This paper presents DNAct, a language-conditioned multi-task policy framework
that integrates neural rendering pre-training and diffusion training to enforce
multi-modality learning in action sequence spaces. To learn a generalizable
multi-task policy with few demonstrations, the pre-training phase of DNAct
leverages neural rendering to distill 2D semantic features from foundation
models such as Stable Diffusion to a 3D space, which provides a comprehensive
semantic understanding regarding the scene. Consequently, it allows various
applications to challenging robotic tasks requiring rich 3D semantics and
accurate geometry. Furthermore, we introduce a novel approach utilizing
diffusion training to learn a vision and language feature that encapsulates the
inherent multi-modality in the multi-task demonstrations. By reconstructing the
action sequences from different tasks via the diffusion process, the model is
capable of distinguishing different modalities and thus improving the
robustness and the generalizability of the learned representation. DNAct
significantly surpasses SOTA NeRF-based multi-task manipulation approaches with
over 30% improvement in success rate. Project website: dnact.github.io.



---

## Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Yuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille | eess.IV | [PDF](http://arxiv.org/pdf/2403.04116v1){: .btn .btn-green } |

**Abstract**: X-ray is widely applied for transmission imaging due to its stronger
penetration than natural light. When rendering novel view X-ray projections,
existing methods mainly based on NeRF suffer from long training time and slow
inference speed. In this paper, we propose a 3D Gaussian splatting-based
framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we
redesign a radiative Gaussian point cloud model inspired by the isotropic
nature of X-ray imaging. Our model excludes the influence of view direction
when learning to predict the radiation intensity of 3D points. Based on this
model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA
implementation. Secondly, we customize an Angle-pose Cuboid Uniform
Initialization (ACUI) strategy that directly uses the parameters of the X-ray
scanner to compute the camera information and then uniformly samples point
positions within a cuboid enclosing the scanned object. Experiments show that
our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying
less than 15% training time and over 73x inference speed. The application on
sparse-view CT reconstruction also reveals the practical values of our method.
Code and models will be publicly available at
https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training
process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .

Comments:
- The first 3D Gaussian Splatting-based method for X-ray 3D
  reconstruction

---

## GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D  Scene Understanding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-06 | Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.03608v1){: .btn .btn-green } |

**Abstract**: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.

Comments:
- Accepted by CVPR2024

---

## A Deep Learning Framework for Wireless Radiation Field Reconstruction  and Channel Prediction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-05 | Haofan Lu, Christopher Vattheuer, Baharan Mirzasoleiman, Omid Abari | cs.NI | [PDF](http://arxiv.org/pdf/2403.03241v1){: .btn .btn-green } |

**Abstract**: We present NeWRF, a deep learning framework for predicting wireless channels.
Wireless channel prediction is a long-standing problem in the wireless
community and is a key technology for improving the coverage of wireless
network deployments. Today, a wireless deployment is evaluated by a site survey
which is a cumbersome process requiring an experienced engineer to perform
extensive channel measurements. To reduce the cost of site surveys, we develop
NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF).
NeWRF trains a neural network model with a sparse set of channel measurements,
and predicts the wireless channel accurately at any location in the site. We
introduce a series of techniques that integrate wireless propagation properties
into the NeRF framework to account for the fundamental differences between the
behavior of light and wireless signals. We conduct extensive evaluations of our
framework and show that our approach can accurately predict channels at
unvisited locations with significantly lower measurement density than prior
state-of-the-art



---

## Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-05 | Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2403.02751v1){: .btn .btn-green } |

**Abstract**: We present Splat-Nav, a navigation pipeline that consists of a real-time safe
planning module and a robust state estimation module designed to operate in the
Gaussian Splatting (GSplat) environment representation, a popular emerging 3D
scene representation from computer vision. We formulate rigorous collision
constraints that can be computed quickly to build a guaranteed-safe polytope
corridor through the map. We then optimize a B-spline trajectory through this
corridor. We also develop a real-time, robust state estimation module by
interpreting the GSplat representation as a point cloud. The module enables the
robot to localize its global pose with zero prior knowledge from RGB-D images
using point cloud alignment, and then track its own pose as it moves through
the scene from RGB images using image-to-point cloud localization. We also
incorporate semantics into the GSplat in order to obtain better images for
localization. All of these modules operate mainly on CPU, freeing up GPU
resources for tasks like real-time scene reconstruction. We demonstrate the
safety and robustness of our pipeline in both simulation and hardware, where we
show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude
faster than Neural Radiance Field (NeRF)-based navigation methods, thereby
enabling real-time navigation.



---

## Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input  Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-04 | Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song | cs.CV | [PDF](http://arxiv.org/pdf/2403.02063v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis with sparse input views is important for real-world
applications like AR/VR and autonomous driving. Recent methods have integrated
depth information into NeRFs for sparse input synthesis, leveraging depth prior
for geometric and spatial understanding. However, most existing works tend to
overlook inaccuracies within depth maps and have low time efficiency. To
address these issues, we propose a depth-guided robust and fast point cloud
fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel
grid of features. A point cloud is constructed for each input view,
characterized within the voxel grid using matrices and vectors. We accumulate
the point cloud of each input view to construct the fused point cloud of the
entire scene. Each voxel determines its density and appearance by referring to
the point cloud of the entire scene. Through point cloud fusion and voxel grid
fine-tuning, inaccuracies in depth values are refined or substituted by those
from other views. Moreover, our method can achieve faster reconstruction and
greater compactness through effective vector-matrix decomposition. Experimental
results underline the superior performance and time efficiency of our approach
compared to state-of-the-art baselines.



---

## DaReNeRF: Direction-aware Representation for Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-04 | Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2403.02265v1){: .btn .btn-green } |

**Abstract**: Addressing the intricate challenge of modeling and re-rendering dynamic
scenes, most recent approaches have sought to simplify these complexities using
plane-based explicit representations, overcoming the slow training time issues
associated with methods like Neural Radiance Fields (NeRF) and implicit
representations. However, the straightforward decomposition of 4D dynamic
scenes into multiple 2D plane-based representations proves insufficient for
re-rendering high-fidelity scenes with complex motions. In response, we present
a novel direction-aware representation (DaRe) approach that captures scene
dynamics from six different directions. This learned representation undergoes
an inverse dual-tree complex wavelet transformation (DTCWT) to recover
plane-based information. DaReNeRF computes features for each space-time point
by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny
MLP for color regression and leveraging volume rendering in training yield
state-of-the-art performance in novel view synthesis for complex dynamic
scenes. Notably, to address redundancy introduced by the six real and six
imaginary direction-aware wavelet coefficients, we introduce a trainable
masking approach, mitigating storage issues without significant performance
decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared
to prior art while delivering superior performance.

Comments:
- Accepted at CVPR 2024. Paper + supplementary material

---

## Neural radiance fields-based holography [Invited]

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba | cs.CV | [PDF](http://arxiv.org/pdf/2403.01137v1){: .btn .btn-green } |

**Abstract**: This study presents a novel approach for generating holograms based on the
neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data
is difficult in hologram computation. NeRF is a state-of-the-art technique for
3D light-field reconstruction from 2D images based on volume rendering. The
NeRF can rapidly predict new-view images that do not include a training
dataset. In this study, we constructed a rendering pipeline directly from a 3D
light field generated from 2D images by NeRF for hologram generation using deep
neural networks within a reasonable time. The pipeline comprises three main
components: the NeRF, a depth predictor, and a hologram generator, all
constructed using deep neural networks. The pipeline does not include any
physical calculations. The predicted holograms of a 3D scene viewed from any
direction were computed using the proposed pipeline. The simulation and
experimental results are presented.



---

## Neural Field Classifiers via Target Encoding and Classification Loss

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun | cs.CV | [PDF](http://arxiv.org/pdf/2403.01058v1){: .btn .btn-green } |

**Abstract**: Neural field methods have seen great progress in various long-standing tasks
in computer vision and computer graphics, including novel view synthesis and
geometry reconstruction. As existing neural field methods try to predict some
coordinate-based continuous target values, such as RGB for Neural Radiance
Field (NeRF), all of these methods are regression models and are optimized by
some regression loss. However, are regression models really better than
classification models for neural field methods? In this work, we try to visit
this very fundamental but overlooked question for neural fields from a machine
learning perspective. We successfully propose a novel Neural Field Classifier
(NFC) framework which formulates existing neural field methods as
classification tasks rather than regression tasks. The proposed NFC can easily
transform arbitrary Neural Field Regressor (NFR) into its classification
variant via employing a novel Target Encoding module and optimizing a
classification loss. By encoding a continuous regression target into a
high-dimensional discrete encoding, we naturally formulate a multi-label
classification task. Extensive experiments demonstrate the impressive
effectiveness of NFC at the nearly free extra computational costs. Moreover,
NFC also shows robustness to sparse inputs, corrupted images, and dynamic
scenes.

Comments:
- ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables

---

## NeRF-VPT: Learning Novel View Representations with Neural Radiance  Fields via View Prompt Tuning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr | cs.CV | [PDF](http://arxiv.org/pdf/2403.01325v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have garnered remarkable success in novel view
synthesis. Nonetheless, the task of generating high-quality images for novel
views persists as a critical challenge. While the existing efforts have
exhibited commendable progress, capturing intricate details, enhancing
textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics
warrant further focused attention and advancement. In this work, we propose
NeRF-VPT, an innovative method for novel view synthesis to address these
challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning
paradigm, wherein RGB information gained from preceding rendering outcomes
serves as instructive visual prompts for subsequent rendering stages, with the
aspiration that the prior knowledge embedded in the prompts can facilitate the
gradual enhancement of rendered image quality. NeRF-VPT only requires sampling
RGB data from previous stage renderings as priors at each training stage,
without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is
plug-and-play and can be readily integrated into existing methods. By
conducting comparative analyses of our NeRF-VPT against several NeRF-based
approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,
Real Forward-Facing, Replica dataset, and a user-captured dataset, we
substantiate that our NeRF-VPT significantly elevates baseline performance and
proficiently generates more high-quality novel view images than all the
compared state-of-the-art methods. Furthermore, the cascading learning of
NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in
a significant enhancement of accuracy for sparse-view novel view synthesis. The
source code and dataset are available at
\url{https://github.com/Freedomcls/NeRF-VPT}.

Comments:
- AAAI 2024

---

## DISORF: A Distributed Online NeRF Training and Rendering Framework for  Mobile Robots

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-01 | Chunlin Li, Ruofan Liang, Hanrui Fan, Zhengen Zhang, Sankeerth Durvasula, Nandita Vijaykumar | cs.RO | [PDF](http://arxiv.org/pdf/2403.00228v1){: .btn .btn-green } |

**Abstract**: We present a framework, DISORF, to enable online 3D reconstruction and
visualization of scenes captured by resource-constrained mobile robots and edge
devices. To address the limited compute capabilities of edge devices and
potentially limited network availability, we design a framework that
efficiently distributes computation between the edge device and remote server.
We leverage on-device SLAM systems to generate posed keyframes and transmit
them to remote servers that can perform high quality 3D reconstruction and
visualization at runtime by leveraging NeRF models. We identify a key challenge
with online NeRF training where naive image sampling strategies can lead to
significant degradation in rendering quality. We propose a novel shifted
exponential frame sampling method that addresses this challenge for online NeRF
training. We demonstrate the effectiveness of our framework in enabling
high-quality real-time reconstruction and visualization of unknown scenes as
they are captured and streamed from cameras in mobile robots and edge devices.


