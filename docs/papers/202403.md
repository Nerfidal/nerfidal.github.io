---
layout: default
title: March 2024
parent: Papers
nav_order: 202403
---

<!---metadata--->


## FeatUp: A Model-Agnostic Framework for Features at Any Resolution

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, William T. Freeman | cs.CV | [PDF](http://arxiv.org/pdf/2403.10516v1){: .btn .btn-green } |

**Abstract**: Deep features are a cornerstone of computer vision research, capturing image
semantics and enabling the community to solve downstream tasks even in the
zero- or few-shot regime. However, these features often lack the spatial
resolution to directly perform dense prediction tasks like segmentation and
depth prediction because models aggressively pool information over large areas.
In this work, we introduce FeatUp, a task- and model-agnostic framework to
restore lost spatial information in deep features. We introduce two variants of
FeatUp: one that guides features with high-resolution signal in a single
forward pass, and one that fits an implicit model to a single image to
reconstruct features at any resolution. Both approaches use a multi-view
consistency loss with deep analogies to NeRFs. Our features retain their
original semantics and can be swapped into existing applications to yield
resolution and performance gains even without re-training. We show that FeatUp
significantly outperforms other feature upsampling and image super-resolution
approaches in class activation map generation, transfer learning for
segmentation and depth prediction, and end-to-end training for semantic
segmentation.

Comments:
- Accepted to the International Conference on Learning Representations
  (ICLR) 2024

---

## URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.10119v1){: .btn .btn-green } |

**Abstract**: We propose a novel rolling shutter bundle adjustment method for neural
radiance fields (NeRF), which utilizes the unordered rolling shutter (RS)
images to obtain the implicit 3D representation. Existing NeRF methods suffer
from low-quality images and inaccurate initial camera poses due to the RS
effect in the image, whereas, the previous method that incorporates the RS into
NeRF requires strict sequential data input, limiting its widespread
applicability. In constant, our method recovers the physical formation of RS
images by estimating camera poses and velocities, thereby removing the input
constraints on sequential data. Moreover, we adopt a coarse-to-fine training
strategy, in which the RS epipolar constraints of the pairwise frames in the
scene graph are used to detect the camera poses that fall into local minima.
The poses detected as outliers are corrected by the interpolation method with
neighboring poses. The experimental results validate the effectiveness of our
method over state-of-the-art works and demonstrate that the reconstruction of
3D representations is not constrained by the requirement of video sequence
input.



---

## Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive  Experience

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Xiaohang Yu, Zhengxian Yang, Shi Pan, Yuqi Han, Haoxiang Wang, Jun Zhang, Shi Yan, Borong Lin, Lei Yang, Tao Yu, Lu Fang | cs.CV | [PDF](http://arxiv.org/pdf/2403.09973v1){: .btn .btn-green } |

**Abstract**: We have built a custom mobile multi-camera large-space dense light field
capture system, which provides a series of high-quality and sufficiently dense
light field images for various scenarios. Our aim is to contribute to the
development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF,
and 3D Gaussian splitting. More importantly, the collected dataset, which is
much denser than existing datasets, may also inspire space-oriented light field
reconstruction, which is potentially different from object-centric 3D
reconstruction, for immersive VR/AR experiences. We utilized a total of 40
GoPro 10 cameras, capturing images of 5k resolution. The number of photos
captured for each scene is no less than 1000, and the average density (view
number within a unit sphere) is 134.68. It is also worth noting that our system
is capable of efficiently capturing large outdoor scenes. Addressing the
current lack of large-space and dense light field datasets, we made efforts to
include elements such as sky, reflections, lights and shadows that are of
interest to researchers in the field of 3D reconstruction during the data
capture process. Finally, we validated the effectiveness of our provided
dataset on three popular algorithms and also integrated the reconstructed 3DGS
results into the Unity engine, demonstrating the potential of utilizing our
datasets to enhance the realism of virtual reality (VR) and create feasible
interactive spaces. The dataset is available at our project website.



---

## Controllable Text-to-3D Generation via Surface-Aligned Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2403.09981v1){: .btn .btn-green } |

**Abstract**: While text-to-3D and image-to-3D generation tasks have received considerable
attention, one important but under-explored field between them is controllable
text-to-3D generation, which we mainly focus on in this work. To address this
task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network
architecture designed to enhance existing pre-trained multi-view diffusion
models by integrating additional input conditions, such as edge, depth, normal,
and scribble maps. Our innovation lies in the introduction of a conditioning
module that controls the base diffusion model using both local and global
embeddings, which are computed from the input condition images and camera
poses. Once trained, MVControl is able to offer 3D diffusion guidance for
optimization-based 3D generation. And, 2) we propose an efficient multi-stage
3D generation pipeline that leverages the benefits of recent large
reconstruction models and score distillation algorithm. Building upon our
MVControl architecture, we employ a unique hybrid diffusion guidance method to
direct the optimization process. In pursuit of efficiency, we adopt 3D
Gaussians as our representation instead of the commonly used implicit
representations. We also pioneer the use of SuGaR, a hybrid representation that
binds Gaussians to mesh triangle faces. This approach alleviates the issue of
poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained
geometry on the mesh. Extensive experiments demonstrate that our method
achieves robust generalization and enables the controllable generation of
high-quality 3D content.

Comments:
- Project page: https://lizhiqi49.github.io/MVControl/

---

## Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian  Splatting Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2403.10050v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting, emerging as a groundbreaking approach, has drawn
increasing attention for its capabilities of high-fidelity reconstruction and
real-time rendering. However, it couples the appearance and geometry of the
scene within the Gaussian attributes, which hinders the flexibility of editing
operations, such as texture swapping. To address this issue, we propose a novel
approach, namely Texture-GS, to disentangle the appearance from the geometry by
representing it as a 2D texture mapped onto the 3D surface, thereby
facilitating appearance editing. Technically, the disentanglement is achieved
by our proposed texture mapping module, which consists of a UV mapping MLP to
learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion
of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian
intersections, and a learnable texture to capture the fine-grained appearance.
Extensive experiments on the DTU dataset demonstrate that our method not only
facilitates high-fidelity appearance editing but also achieves real-time
rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.



---

## DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Huiqiang Sun, Xingyi Li, Liao Shen, Xinyi Ye, Ke Xian, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2403.10103v1){: .btn .btn-green } |

**Abstract**: Recent advancements in dynamic neural radiance field methods have yielded
remarkable outcomes. However, these approaches rely on the assumption of sharp
input images. When faced with motion blur, existing dynamic NeRF methods often
struggle to generate high-quality novel views. In this paper, we propose
DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views
from a monocular video affected by motion blur. To account for motion blur in
input images, we simultaneously capture the camera trajectory and object
Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we
employ a global cross-time rendering approach to ensure consistent temporal
coherence across the entire scene. We curate a dataset comprising diverse
dynamic scenes that are specifically tailored for our task. Experimental
results on our dataset demonstrate that our method outperforms existing
approaches in generating sharp novel views from motion-blurred inputs while
maintaining spatial-temporal consistency of the scene.



---

## SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou | cs.CV | [PDF](http://arxiv.org/pdf/2403.10427v1){: .btn .btn-green } |

**Abstract**: Implicit neural representation methods have shown impressive advancements in
learning 3D scenes from unstructured in-the-wild photo collections but are
still limited by the large computational cost of volumetric rendering. More
recently, 3D Gaussian Splatting emerged as a much faster alternative with
superior rendering quality and training efficiency, especially for small-scale
and object-centric scenarios. Nevertheless, this technique suffers from poor
performance on unstructured in-the-wild data. To tackle this, we extend over 3D
Gaussian Splatting to handle unstructured image collections. We achieve this by
modeling appearance to seize photometric variations in the rendered images.
Additionally, we introduce a new mechanism to train transient Gaussians to
handle the presence of scene occluders in an unsupervised manner. Experiments
on diverse photo collection scenes and multi-pass acquisition of outdoor
landmarks show the effectiveness of our method over prior works achieving
state-of-the-art results with improved efficiency.



---

## GGRt: Towards Generalizable 3D Gaussians without Pose Priors in  Real-Time

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han | cs.CV | [PDF](http://arxiv.org/pdf/2403.10147v1){: .btn .btn-green } |

**Abstract**: This paper presents GGRt, a novel approach to generalizable novel view
synthesis that alleviates the need for real camera poses, complexity in
processing high-resolution images, and lengthy optimization processes, thus
facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in
real-world scenarios. Specifically, we design a novel joint learning framework
that consists of an Iterative Pose Optimization Network (IPO-Net) and a
Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,
the proposed framework can inherently estimate robust relative pose information
from the image observations and thus primarily alleviate the requirement of
real camera poses. Moreover, we implement a deferred back-propagation mechanism
that enables high-resolution training and inference, overcoming the resolution
constraints of previous methods. To enhance the speed and efficiency, we
further introduce a progressive Gaussian cache module that dynamically adjusts
during training and inference. As the first pose-free generalizable 3D-GS
framework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at
$\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our
method outperforms existing NeRF-based pose-free techniques in terms of
inference speed and effectiveness. It can also approach the real pose-based
3D-GS methods. Our contributions provide a significant leap forward for the
integration of computer vision and computer graphics into practical
applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open
datasets and enabling real-time rendering for immersive experiences.



---

## Thermal-NeRF: Neural Radiance Fields from an Infrared Camera

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Tianxiang Ye, Qi Wu, Junyuan Deng, Guoqing Liu, Liu Liu, Songpengcheng Xia, Liang Pang, Wenxian Yu, Ling Pei | cs.CV | [PDF](http://arxiv.org/pdf/2403.10340v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant
potential in encoding highly-detailed 3D geometry and environmental appearance,
positioning themselves as a promising alternative to traditional explicit
representation for 3D scene reconstruction. However, the predominant reliance
on RGB imaging presupposes ideal lighting conditions: a premise frequently
unmet in robotic applications plagued by poor lighting or visual obstructions.
This limitation overlooks the capabilities of infrared (IR) cameras, which
excel in low-light detection and present a robust alternative under such
adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first
method that estimates a volumetric scene representation in the form of a NeRF
solely from IR imaging. By leveraging a thermal mapping and structural thermal
constraint derived from the thermal characteristics of IR imaging, our method
showcasing unparalleled proficiency in recovering NeRFs in visually degraded
scenes where RGB-based methods fall short. We conduct extensive experiments to
demonstrate that Thermal-NeRF can achieve superior quality compared to existing
methods. Furthermore, we contribute a dataset for IR-based NeRF applications,
paving the way for future research in IR NeRF reconstruction.



---

## FDGaussian: Fast Gaussian Splatting from Single Image via  Geometric-aware Diffusion Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2403.10242v1){: .btn .btn-green } |

**Abstract**: Reconstructing detailed 3D objects from single-view images remains a
challenging task due to the limited information available. In this paper, we
introduce FDGaussian, a novel two-stage framework for single-image 3D
reconstruction. Recent methods typically utilize pre-trained 2D diffusion
models to generate plausible novel views from the input image, yet they
encounter issues with either multi-view inconsistency or lack of geometric
fidelity. To overcome these challenges, we propose an orthogonal plane
decomposition mechanism to extract 3D geometric features from the 2D input,
enabling the generation of consistent multi-view images. Moreover, we further
accelerate the state-of-the-art Gaussian Splatting incorporating epipolar
attention to fuse images from different viewpoints. We demonstrate that
FDGaussian generates images with high consistency across different views and
reconstructs high-quality 3D objects, both qualitatively and quantitatively.
More examples can be found at our website https://qjfeng.net/FDGaussian/.



---

## Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints  Scene Coordinate Regression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Huy-Hoang Bui, Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee | cs.CV | [PDF](http://arxiv.org/pdf/2403.10297v1){: .btn .btn-green } |

**Abstract**: Classical structural-based visual localization methods offer high accuracy
but face trade-offs in terms of storage, speed, and privacy. A recent
innovation, keypoint scene coordinate regression (KSCR) named D2S addresses
these issues by leveraging graph attention networks to enhance keypoint
relationships and predict their 3D coordinates using a simple multilayer
perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using
established 2D-3D correspondences. While KSCR achieves competitive results,
rivaling state-of-the-art image-retrieval methods like HLoc across multiple
benchmarks, its performance is hindered when data samples are limited due to
the deep learning model's reliance on extensive data. This paper proposes a
solution to this challenge by introducing a pipeline for keypoint descriptor
synthesis using Neural Radiance Field (NeRF). By generating novel poses and
feeding them into a trained NeRF model to create new views, our approach
enhances the KSCR's generalization capabilities in data-scarce environments.
The proposed system could significantly improve localization accuracy by up to
50\% and cost only a fraction of time for data synthesis. Furthermore, its
modular design allows for the integration of multiple NeRFs, offering a
versatile and efficient solution for visual localization. The implementation is
publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.



---

## PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF  Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2403.09079v1){: .btn .btn-green } |

**Abstract**: Autonomous vehicles rely extensively on perception systems to navigate and
interpret their surroundings. Despite significant advancements in these systems
recently, challenges persist under conditions like occlusion, extreme lighting,
or in unfamiliar urban areas. Unlike these systems, humans do not solely depend
on immediate observations to perceive the environment. In navigating new
cities, humans gradually develop a preliminary mental map to supplement
real-time perception during subsequent visits. Inspired by this human approach,
we introduce a novel framework, Pre-Sight, that leverages past traversals to
construct static prior memories, enhancing online perception in later
navigations. Our method involves optimizing a city-scale neural radiance field
with data from previous journeys to generate neural priors. These priors, rich
in semantic and geometric details, are derived without manual annotations and
can seamlessly augment various state-of-the-art perception models, improving
their efficacy with minimal additional computational cost. Experimental results
on the nuScenes dataset demonstrate the framework's high compatibility with
diverse online perception models. Specifically, it shows remarkable
improvements in HD-map construction and occupancy prediction tasks,
highlighting its potential as a new perception framework for autonomous driving
systems. Our code will be released at
https://github.com/yuantianyuan01/PreSight.



---

## Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III | cs.RO | [PDF](http://arxiv.org/pdf/2403.09875v1){: .btn .btn-green } |

**Abstract**: In this work, we propose a novel method to supervise 3D Gaussian Splatting
(3DGS) scenes using optical tactile sensors. Optical tactile sensors have
become widespread in their use in robotics for manipulation and object
representation; however, raw optical tactile sensor data is unsuitable to
directly supervise a 3DGS scene. Our representation leverages a Gaussian
Process Implicit Surface to implicitly represent the object, combining many
touches into a unified representation with uncertainty. We merge this model
with a monocular depth estimation network, which is aligned in a two stage
process, coarsely aligning with a depth camera and then finely adjusting to
match our touch data. For every training image, our method produces a
corresponding fused depth and uncertainty map. Utilizing this additional
information, we propose a new loss function, variance weighted depth supervised
loss, for training the 3DGS scene model. We leverage the DenseTact optical
tactile sensor and RealSense RGB-D camera to show that combining touch and
vision in this manner leads to quantitatively and qualitatively better results
than vision or touch alone in a few-view scene syntheses on opaque as well as
on reflective and transparent objects. Please see our project page at
http://armlabstanford.github.io/touch-gs



---

## 3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Frank Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, Changqing Zou | cs.CV | [PDF](http://arxiv.org/pdf/2403.09439v1){: .btn .btn-green } |

**Abstract**: Text-driven 3D scene generation techniques have made rapid progress in recent
years. Their success is mainly attributed to using existing generative models
to iteratively perform image warping and inpainting to generate 3D scenes.
However, these methods heavily rely on the outputs of existing models, leading
to error accumulation in geometry and appearance that prevent the models from
being used in various scenarios (e.g., outdoor and unreal scenarios). To
address this limitation, we generatively refine the newly generated local views
by querying and aggregating global 3D information, and then progressively
generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF
as a unified representation of the 3D scene to constrain global 3D consistency,
and propose a generative refinement network to synthesize new contents with
higher quality by exploiting the natural image prior from 2D diffusion model as
well as the global 3D information of the current scene. Our extensive
experiments demonstrate that, in comparison to previous methods, our approach
supports wide variety of scene generation and arbitrary camera trajectories
with improved visual quality and 3D consistency.

Comments:
- 11 pages, 7 figures

---

## VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Nicolaj Schmid, Cornelius von Einem, Cesar Cadena, Roland Siegwart, Lorenz Hruby, Florian Tschopp | cs.RO | [PDF](http://arxiv.org/pdf/2403.09477v1){: .btn .btn-green } |

**Abstract**: Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.



---

## RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban  Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Thang-Anh-Quan Nguyen, Luis Roldão, Nathan Piasco, Moussab Bennehar, Dzmitry Tsishkou | cs.CV | [PDF](http://arxiv.org/pdf/2403.09419v1){: .btn .btn-green } |

**Abstract**: The task of separating dynamic objects from static environments using NeRFs
has been widely studied in recent years. However, capturing large-scale scenes
still poses a challenge due to their complex geometric structures and
unconstrained dynamics. Without the help of 3D motion cues, previous methods
often require simplified setups with slow camera motion and only a few/single
dynamic actors, leading to suboptimal solutions in most urban setups. To
overcome such limitations, we present RoDUS, a pipeline for decomposing static
and dynamic elements in urban scenes, with thoughtfully separated NeRF models
for moving and non-moving components. Our approach utilizes a robust
kernel-based initialization coupled with 4D semantic information to selectively
guide the learning process. This strategy enables accurate capturing of the
dynamics in the scene, resulting in reduced artifacts caused by NeRF on
background reconstruction, all by using self-supervision. Notably, experimental
evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of
our method in decomposing challenging urban scenes into precise static and
dynamic components.



---

## The NeRFect Match: Exploring NeRF Features for Visual Localization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé | cs.CV | [PDF](http://arxiv.org/pdf/2403.09577v1){: .btn .btn-green } |

**Abstract**: In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene
representation for visual localization. Recently, NeRF has been employed to
enhance pose regression and scene coordinate regression models by augmenting
the training database, providing auxiliary supervision through rendered images,
or serving as an iterative refinement module. We extend its recognized
advantages -- its ability to provide a compact scene representation with
realistic appearances and accurate geometry -- by exploring the potential of
NeRF's internal features in establishing precise 2D-3D matches for
localization. To this end, we conduct a comprehensive examination of NeRF's
implicit knowledge, acquired through view synthesis, for matching under various
conditions. This includes exploring different matching network architectures,
extracting encoder features at multiple layers, and varying training
configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D
matching function that capitalizes on the internal knowledge of NeRF learned
via view synthesis. Our evaluation of NeRFMatch on standard localization
benchmarks, within a structure-based pipeline, sets a new state-of-the-art for
localization performance on Cambridge Landmarks.



---

## A New Split Algorithm for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu | cs.GR | [PDF](http://arxiv.org/pdf/2403.09143v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting models, as a novel explicit 3D representation, have
been applied in many domains recently, such as explicit geometric editing and
geometry generation. Progress has been rapid. However, due to their mixed
scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred
or needle-like effect near the surface. At the same time, 3D Gaussian splatting
models tend to flatten large untextured regions, yielding a very sparse point
cloud. These problems are caused by the non-uniform nature of 3D Gaussian
splatting models, so in this paper, we propose a new 3D Gaussian splitting
algorithm, which can produce a more uniform and surface-bounded 3D Gaussian
splatting model. Our algorithm splits an $N$-dimensional Gaussian into two
N-dimensional Gaussians. It ensures consistency of mathematical characteristics
and similarity of appearance, allowing resulting 3D Gaussian splatting models
to be more uniform and a better fit to the underlying surface, and thus more
suitable for explicit editing, point cloud extraction and other tasks.
Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form
solution, making it readily applicable to any 3D Gaussian model.

Comments:
- 11 pages, 10 figures

---

## GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary  Robotic Grasping

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.09637v1){: .btn .btn-green } |

**Abstract**: Constructing a 3D scene capable of accommodating open-ended language queries,
is a pivotal pursuit, particularly within the domain of robotics. Such
technology facilitates robots in executing object manipulations based on human
language directives. To tackle this challenge, some research efforts have been
dedicated to the development of language-embedded implicit fields. However,
implicit fields (e.g. NeRF) encounter limitations due to the necessity of
processing a large number of input views for reconstruction, coupled with their
inherent inefficiencies in inference. Thus, we present the GaussianGrasper,
which utilizes 3D Gaussian Splatting to explicitly represent the scene as a
collection of Gaussian primitives. Our approach takes a limited set of RGB-D
views and employs a tile-based splatting technique to create a feature field.
In particular, we propose an Efficient Feature Distillation (EFD) module that
employs contrastive learning to efficiently and accurately distill language
embeddings derived from foundational models. With the reconstructed geometry of
the Gaussian field, our method enables the pre-trained grasping model to
generate collision-free grasp pose candidates. Furthermore, we propose a
normal-guided grasp module to select the best grasp pose. Through comprehensive
real-world experiments, we demonstrate that GaussianGrasper enables robots to
accurately query and grasp objects with language instructions, providing a new
solution for language-guided manipulation tasks. Data and codes can be
available at https://github.com/MrSecant/GaussianGrasper.



---

## Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2403.09413v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has recently demonstrated impressive
capabilities in real-time novel view synthesis and 3D reconstruction. However,
3DGS heavily depends on the accurate initialization derived from
Structure-from-Motion (SfM) methods. When trained with randomly initialized
point clouds, 3DGS fails to maintain its ability to produce high-quality
images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive
analysis of SfM initialization in the frequency domain and analysis of a 1D
regression task with multiple 1D Gaussians, we propose a novel optimization
strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D
Gaussian Splatting), that successfully trains 3D Gaussians from random point
clouds. We show the effectiveness of our strategy through quantitative and
qualitative comparisons on multiple datasets, largely improving the performance
in all settings. Our project page and code can be found at
https://ku-cvlab.github.io/RAIN-GS.

Comments:
- Project Page: https://ku-cvlab.github.io/RAIN-GS

---

## GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting  Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu | cs.CV | [PDF](http://arxiv.org/pdf/2403.08733v2){: .btn .btn-green } |

**Abstract**: We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed
by the 3D Gaussian Splatting (3DGS).
  Our method first renders a collection of images by using the 3DGS and edits
them by using a pre-trained 2D diffusion model (ControlNet) based on the input
prompt, which is then used to optimise the 3D model.
  Our key contribution is multi-view consistent editing, which enables editing
all images together instead of iteratively editing one image while updating the
3D model as in previous works.
  It leads to faster editing as well as higher visual quality.
  This is achieved by the two terms:
  (a) depth-conditioned editing that enforces geometric consistency across
multi-view images by leveraging naturally consistent depth maps.
  (b) attention-based latent code alignment that unifies the appearance of
edited images by conditioning their editing to several reference views through
self and cross-view attention between images' latent representations.
  Experiments demonstrate that our method achieves faster editing and better
visual results than previous state-of-the-art methods.

Comments:
- 17 pages

---

## StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Hongbin Xu, Weitao Chen, Feng Xiao, Baigui Sun, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2403.08310v1){: .btn .btn-green } |

**Abstract**: 4D style transfer aims at transferring arbitrary visual style to the
synthesized novel views of a dynamic 4D scene with varying viewpoints and
times. Existing efforts on 3D style transfer can effectively combine the visual
features of style images and neural radiance fields (NeRF) but fail to handle
the 4D dynamic scenes limited by the static scene assumption. Consequently, we
aim to handle the novel challenging problem of 4D style transfer for the first
time, which further requires the consistency of stylized results on dynamic
objects. In this paper, we introduce StyleDyRF, a method that represents the 4D
feature space by deforming a canonical feature volume and learns a linear style
transformation matrix on the feature volume in a data-driven fashion. To obtain
the canonical feature volume, the rays at each time step are deformed with the
geometric prior of a pre-trained dynamic NeRF to render the feature map under
the supervision of pre-trained visual encoders. With the content and style cues
in the canonical feature volume and the style image, we can learn the style
transformation matrix from their covariance matrices with lightweight neural
networks. The learned style transformation matrix can reflect a direct matching
of feature covariance from the content volume to the given style pattern, in
analogy with the optimization of the Gram matrix in traditional 2D neural style
transfer. The experimental results show that our method not only renders 4D
photorealistic style transfer results in a zero-shot manner but also
outperforms existing methods in terms of visual quality and consistency.

Comments:
- In submission. The code and model are released at:
  https://github.com/ToughStoneX/StyleDyRF

---

## NeRF-Supervised Feature Point Detection and Description

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Ali Youssef, Francisco Vasconcelos | cs.CV | [PDF](http://arxiv.org/pdf/2403.08156v1){: .btn .btn-green } |

**Abstract**: Feature point detection and description is the backbone for various computer
vision applications, such as Structure-from-Motion, visual SLAM, and visual
place recognition. While learning-based methods have surpassed traditional
handcrafted techniques, their training often relies on simplistic
homography-based simulations of multi-view perspectives, limiting model
generalisability. This paper introduces a novel approach leveraging neural
radiance fields (NeRFs) for realistic multi-view training data generation. We
create a diverse multi-view dataset using NeRFs, consisting of indoor and
outdoor scenes. Our proposed methodology adapts state-of-the-art feature
detectors and descriptors to train on NeRF-synthesised views supervised by
perspective projective geometry. Our experiments demonstrate that the proposed
methods achieve competitive or superior performance on standard benchmarks for
relative pose estimation, point cloud registration, and homography estimation
while requiring significantly less training data compared to existing
approaches.



---

## GaussianImage: 1000 FPS Image Representation and Compression by 2D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, Jun Zhang | eess.IV | [PDF](http://arxiv.org/pdf/2403.08551v2){: .btn .btn-green } |

**Abstract**: Implicit neural representations (INRs) recently achieved great success in
image representation and compression, offering high visual quality and fast
rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are
available. However, this requirement often hinders their use on low-end devices
with limited memory. In response, we propose a groundbreaking paradigm of image
representation and compression by 2D Gaussian Splatting, named GaussianImage.
We first introduce 2D Gaussian to represent the image, where each Gaussian has
8 parameters including position, covariance and color. Subsequently, we unveil
a novel rendering algorithm based on accumulated summation. Remarkably, our
method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster
fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation
performance, but also delivers a faster rendering speed of 1500-2000 FPS
regardless of parameter size. Furthermore, we integrate existing vector
quantization technique to build an image codec. Experimental results
demonstrate that our codec attains rate-distortion performance comparable to
compression-based INRs such as COIN and COIN++, while facilitating decoding
speeds of approximately 1000 FPS. Additionally, preliminary proof of concept
shows that our codec surpasses COIN and COIN++ in performance when using
partial bits-back coding.



---

## ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic  Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang | cs.RO | [PDF](http://arxiv.org/pdf/2403.08321v1){: .btn .btn-green } |

**Abstract**: Performing language-conditioned robotic manipulation tasks in unstructured
environments is highly demanded for general intelligent robots. Conventional
robotic manipulation methods usually learn semantic representation of the
observation for action prediction, which ignores the scene-level spatiotemporal
dynamics for human goal completion. In this paper, we propose a dynamic
Gaussian Splatting method named ManiGaussian for multi-task robotic
manipulation, which mines scene dynamics via future scene reconstruction.
Specifically, we first formulate the dynamic Gaussian Splatting framework that
infers the semantics propagation in the Gaussian embedding space, where the
semantic representation is leveraged to predict the optimal robot action. Then,
we build a Gaussian world model to parameterize the distribution in our dynamic
Gaussian Splatting framework, which provides informative supervision in the
interactive environment via future scene reconstruction. We evaluate our
ManiGaussian on 10 RLBench tasks with 166 variations, and the results
demonstrate our framework can outperform the state-of-the-art methods by 13.1\%
in average success rate.



---

## Gaussian Splatting in Style

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2403.08498v1){: .btn .btn-green } |

**Abstract**: Scene stylization extends the work of neural style transfer to three spatial
dimensions. A vital challenge in this problem is to maintain the uniformity of
the stylized appearance across a multi-view setting. A vast majority of the
previous works achieve this by optimizing the scene with a specific style
image. In contrast, we propose a novel architecture trained on a collection of
style images, that at test time produces high quality stylized novel views. Our
work builds up on the framework of 3D Gaussian splatting. For a given scene, we
take the pretrained Gaussians and process them using a multi resolution hash
grid and a tiny MLP to obtain the conditional stylised views. The explicit
nature of 3D Gaussians give us inherent advantages over NeRF-based methods
including geometric consistency, along with having a fast training and
rendering regime. This enables our method to be useful for vast practical use
cases such as in augmented or virtual reality applications. Through our
experiments, we show our methods achieve state-of-the-art performance with
superior visual quality on various indoor and outdoor real-world data.



---

## StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2403.07807v1){: .btn .btn-green } |

**Abstract**: We introduce StyleGaussian, a novel 3D style transfer technique that allows
instant transfer of any image's style to a 3D scene at 10 frames per second
(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style
transfer without compromising its real-time rendering ability and multi-view
consistency. It achieves instant style transfer with three steps: embedding,
transfer, and decoding. Initially, 2D VGG scene features are embedded into
reconstructed 3D Gaussians. Next, the embedded features are transformed
according to a reference style image. Finally, the transformed features are
decoded into the stylized RGB. StyleGaussian has two novel designs. The first
is an efficient feature rendering strategy that first renders low-dimensional
features and then maps them into high-dimensional features while embedding VGG
features. It cuts the memory consumption significantly and enables 3DGS to
render the high-dimensional memory-intensive features. The second is a
K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized
features, it eliminates the 2D CNN operations that compromise strict multi-view
consistency. Extensive experiments show that StyleGaussian achieves instant 3D
stylization with superior stylization quality while preserving real-time
rendering and strict multi-view consistency. Project page:
https://kunhao-liu.github.io/StyleGaussian/



---

## Q-SLAM: Quadric Representations for Monocular SLAM

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Chensheng Peng, Chenfeng Xu, Yue Wang, Mingyu Ding, Heng Yang, Masayoshi Tomizuka, Kurt Keutzer, Marco Pavone, Wei Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2403.08125v1){: .btn .btn-green } |

**Abstract**: Monocular SLAM has long grappled with the challenge of accurately modeling 3D
geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular
SLAM have shown promise, yet these methods typically focus on novel view
synthesis rather than precise 3D geometry modeling. This focus results in a
significant disconnect between NeRF applications, i.e., novel-view synthesis
and the requirements of SLAM. We identify that the gap results from the
volumetric representations used in NeRF, which are often dense and noisy. In
this study, we propose a novel approach that reimagines volumetric
representations through the lens of quadric forms. We posit that most scene
components can be effectively represented as quadric planes. Leveraging this
assumption, we reshape the volumetric representations with million of cubes by
several quadric planes, which leads to more accurate and efficient modeling of
3D scenes in SLAM contexts. Our method involves two key steps: First, we use
the quadric assumption to enhance coarse depth estimations obtained from
tracking modules, e.g., Droid-SLAM. This step alone significantly improves
depth estimation accuracy. Second, in the subsequent mapping phase, we diverge
from previous NeRF-based SLAM methods that distribute sampling points across
the entire volume space. Instead, we concentrate sampling points around quadric
planes and aggregate them using a novel quadric-decomposed Transformer.
Additionally, we introduce an end-to-end joint optimization strategy that
synchronizes pose estimation with 3D reconstruction.



---

## SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.07494v1){: .btn .btn-green } |

**Abstract**: We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D
Gaussian representation, that enables accurate 3D semantic mapping, robust
camera tracking, and high-quality rendering in real-time. In this system, we
incorporate semantic feature embedding into 3D Gaussian representation, which
effectively encodes semantic information within the spatial layout of the
environment for precise semantic scene representation. Furthermore, we propose
feature-level loss for updating 3D Gaussian representation, enabling
higher-level guidance for 3D Gaussian optimization. In addition, to reduce
cumulative drift and improve reconstruction accuracy, we introduce
semantic-informed bundle adjustment leveraging semantic associations for joint
optimization of 3D Gaussian representation and camera poses, leading to more
robust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates
superior performance over existing dense semantic SLAM methods in terms of
mapping and tracking accuracy on Replica and ScanNet datasets, while also
showing excellent capabilities in novel-view semantic synthesis and 3D semantic
mapping.



---

## SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2403.07547v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) has attracted considerable attention for their
exceptional ability in synthesizing novel views with high fidelity. However,
the presence of motion blur, resulting from slight camera movements during
extended shutter exposures, poses a significant challenge, potentially
compromising the quality of the reconstructed 3D scenes. While recent studies
have addressed this issue, they do not consider the continuous dynamics of
camera movements during image acquisition, leading to inaccurate scene
reconstruction. Additionally, these methods are plagued by slow training and
rendering speed. To effectively handle these issues, we propose sequential
motion understanding radiance fields (SMURF), a novel approach that employs
neural ordinary differential equation (Neural-ODE) to model continuous camera
motion and leverages the explicit volumetric representation method for faster
training and robustness to motion-blurred input images. The core idea of the
SMURF is continuous motion blurring kernel (CMBK), a unique module designed to
model a continuous camera movements for processing blurry inputs. Our model,
rigorously evaluated against benchmark datasets, demonstrates state-of-the-art
performance both quantitatively and qualitatively.

Comments:
- 25 pages, 10 figures, Code is available at
  https://github.com/Jho-Yonsei/SMURF

---

## DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with  Global-Local Depth Normalization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu | cs.CV | [PDF](http://arxiv.org/pdf/2403.06912v2){: .btn .btn-green } |

**Abstract**: Radiance fields have demonstrated impressive performance in synthesizing
novel views from sparse input views, yet prevailing methods suffer from high
training costs and slow inference speed. This paper introduces DNGaussian, a
depth-regularized framework based on 3D Gaussian radiance fields, offering
real-time and high-quality few-shot novel view synthesis at low costs. Our
motivation stems from the highly efficient representation and surprising
quality of the recent 3D Gaussian Splatting, despite it will encounter a
geometry degradation when input views decrease. In the Gaussian radiance
fields, we find this degradation in scene geometry primarily lined to the
positioning of Gaussian primitives and can be mitigated by depth constraint.
Consequently, we propose a Hard and Soft Depth Regularization to restore
accurate scene geometry under coarse monocular depth supervision while
maintaining a fine-grained color appearance. To further refine detailed
geometry reshaping, we introduce Global-Local Depth Normalization, enhancing
the focus on small local depth changes. Extensive experiments on LLFF, DTU, and
Blender datasets demonstrate that DNGaussian outperforms state-of-the-art
methods, achieving comparable or better results with significantly reduced
memory cost, a $25 \times$ reduction in training time, and over $3000 \times$
faster rendering speed.

Comments:
- Accepted at CVPR 2024. Project page:
  https://fictionarry.github.io/DNGaussian/

---

## Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Chenhao Zhang, Yongyang Zhou, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2403.06505v1){: .btn .btn-green } |

**Abstract**: The neural radiance field (NeRF) has emerged as a prominent methodology for
synthesizing realistic images of novel views. While neural radiance
representations based on voxels or mesh individually offer distinct advantages,
excelling in either rendering quality or speed, each has limitations in the
other aspect. In response, we propose a pioneering hybrid representation named
Vosh, seamlessly combining both voxel and mesh components in hybrid rendering
for view synthesis. Vosh is meticulously crafted by optimizing the voxel grid
of NeRF, strategically with selected voxels replaced by mesh. Therefore, it
excels in fast rendering scenes with simple geometry and textures through its
mesh component, while simultaneously enabling high-quality rendering in
intricate regions by leveraging voxel component. The flexibility of Vosh is
showcased through the ability to adjust hybrid ratios, providing users the
ability to control the balance between rendering quality and speed based on
flexible usage. Experimental results demonstrates that our method achieves
commendable trade-off between rendering quality and speed, and notably has
real-time performance on mobile devices.



---

## FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing | cs.CV | [PDF](http://arxiv.org/pdf/2403.06908v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has achieved very impressive performance in real-time
novel view synthesis. However, it often suffers from over-reconstruction during
Gaussian densification where high-variance image regions are covered by a few
large Gaussians only, leading to blur and artifacts in the rendered images. We
design a progressive frequency regularization (FreGS) technique to tackle the
over-reconstruction issue within the frequency space. Specifically, FreGS
performs coarse-to-fine Gaussian densification by exploiting low-to-high
frequency components that can be easily extracted with low-pass and high-pass
filters in the Fourier space. By minimizing the discrepancy between the
frequency spectrum of the rendered image and the corresponding ground truth, it
achieves high-quality Gaussian densification and alleviates the
over-reconstruction of Gaussian splatting effectively. Experiments over
multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and
Deep Blending) show that FreGS achieves superior novel view synthesis and
outperforms the state-of-the-art consistently.



---

## SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields  for Robotic Inspection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Yifu Tao, Yash Bhalgat, Lanke Frank Tarimo Fu, Matias Mattamala, Nived Chebrolu, Maurice Fallon | cs.RO | [PDF](http://arxiv.org/pdf/2403.06877v1){: .btn .btn-green } |

**Abstract**: We present a neural-field-based large-scale reconstruction system that fuses
lidar and vision data to generate high-quality reconstructions that are
geometrically accurate and capture photo-realistic textures. This system adapts
the state-of-the-art neural radiance field (NeRF) representation to also
incorporate lidar data which adds strong geometric constraints on the depth and
surface normals. We exploit the trajectory from a real-time lidar SLAM system
to bootstrap a Structure-from-Motion (SfM) procedure to both significantly
reduce the computation time and to provide metric scale which is crucial for
lidar depth loss. We use submapping to scale the system to large-scale
environments captured over long trajectories. We demonstrate the reconstruction
system with data from a multi-camera, lidar sensor suite onboard a legged
robot, hand-held while scanning building scenes for 600 metres, and onboard an
aerial robot surveying a multi-storey mock disaster site-building. Website:
https://ori-drs.github.io/projects/silvr/

Comments:
- Accepted at ICRA 2024; Website:
  https://ori-drs.github.io/projects/silvr/

---

## FSViewFusion: Few-Shots View Generation of Novel Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Rukhshanda Hussain, Hui Xian Grace Lim, Borchun Chen, Mubarak Shah, Ser Nam Lim | cs.CV | [PDF](http://arxiv.org/pdf/2403.06394v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis has observed tremendous developments since the arrival
of NeRFs. However, Nerf models overfit on a single scene, lacking
generalization to out of distribution objects. Recently, diffusion models have
exhibited remarkable performance on introducing generalization in view
synthesis. Inspired by these advancements, we explore the capabilities of a
pretrained stable diffusion model for view synthesis without explicit 3D
priors. Specifically, we base our method on a personalized text to image model,
Dreambooth, given its strong ability to adapt to specific novel objects with a
few shots. Our research reveals two interesting findings. First, we observe
that Dreambooth can learn the high level concept of a view, compared to
arguably more complex strategies which involve finetuning diffusions on large
amounts of multi-view data. Second, we establish that the concept of a view can
be disentangled and transferred to a novel object irrespective of the original
object's identify from which the views are learnt. Motivated by this, we
introduce a learning strategy, FSViewFusion, which inherits a specific view
through only one image sample of a single scene, and transfers the knowledge to
a novel object, learnt from few shots, using low rank adapters. Through
extensive experiments we demonstrate that our method, albeit simple, is
efficient in generating reliable view samples for in the wild images. Code and
models will be released.



---

## Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View  Synthesis?

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-10 | Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen | cs.CV | [PDF](http://arxiv.org/pdf/2403.06092v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has achieved superior performance for novel view
synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a
volume rendering procedure, however, when fewer known views are given (i.e.,
few-shot view synthesis), the model is prone to overfit the given views. To
handle this issue, previous efforts have been made towards leveraging learned
priors or introducing additional regularizations. In contrast, in this paper,
we for the first time provide an orthogonal method from the perspective of
network structure. Given the observation that trivially reducing the number of
model parameters alleviates the overfitting issue, but at the cost of missing
details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs
(i.e., location and viewing direction) of the vanilla MLP into each layer to
prevent the overfitting issue without harming detailed synthesis. To further
reduce the artifacts, we propose to model colors and volume density separately
and present two regularization terms. Extensive experiments on multiple
datasets demonstrate that: 1) although the proposed mi-MLP is easy to
implement, it is surprisingly effective as it boosts the PSNR of the baseline
from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art
results on a wide range of benchmarks. We will release the code upon
publication.

Comments:
- Accepted by CVPR 2024

---

## Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous  Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-09 | Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma | cs.CV | [PDF](http://arxiv.org/pdf/2403.05907v1){: .btn .btn-green } |

**Abstract**: Recent studies have highlighted the promising application of NeRF in
autonomous driving contexts. However, the complexity of outdoor environments,
combined with the restricted viewpoints in driving scenarios, complicates the
task of precisely reconstructing scene geometry. Such challenges often lead to
diminished quality in reconstructions and extended durations for both training
and rendering. To tackle these challenges, we present Lightning NeRF. It uses
an efficient hybrid scene representation that effectively utilizes the geometry
prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly
improves the novel view synthesis performance of NeRF and reduces computational
overheads. Through evaluations on real-world datasets, such as KITTI-360,
Argoverse2, and our private dataset, we demonstrate that our approach not only
exceeds the current state-of-the-art in novel view synthesis quality but also
achieves a five-fold increase in training speed and a ten-fold improvement in
rendering speed. Codes are available at
https://github.com/VISION-SJTU/Lightning-NeRF .

Comments:
- Accepted to ICRA 2024

---

## Large Generative Model Assisted 3D Semantic Communication

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-09 | Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You | cs.IT | [PDF](http://arxiv.org/pdf/2403.05783v1){: .btn .btn-green } |

**Abstract**: Semantic Communication (SC) is a novel paradigm for data transmission in 6G.
However, there are several challenges posed when performing SC in 3D scenarios:
1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain
channel estimation. To address these issues, we propose a Generative AI Model
assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor
(3DSE), which employs generative AI models, including Segment Anything Model
(SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D
scenario based on user requirements. The extracted 3D semantics are represented
as multi-perspective images of the goal-oriented 3D object. Then, we present an
Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective
images, in which we use a semantic encoder with two output heads to perform
semantic encoding and mask redundant semantics in the latent semantic space,
respectively. Next, we design a conditional Generative adversarial network and
Diffusion model aided-Channel Estimation (GDCE) to estimate and refine the
Channel State Information (CSI) of physical channels. Finally, simulation
results demonstrate the advantages of the proposed GAM-3DSC system in
effectively transmitting the goal-oriented 3D scenario.

Comments:
- 13 pages,13 figures,1 table

---

## GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-08 | Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà | cs.CV | [PDF](http://arxiv.org/pdf/2403.05154v1){: .btn .btn-green } |

**Abstract**: We present GSEdit, a pipeline for text-guided 3D object editing based on
Gaussian Splatting models. Our method enables the editing of the style and
appearance of 3D objects without altering their main details, all in a matter
of minutes on consumer hardware. We tackle the problem by leveraging Gaussian
splatting to represent 3D scenes, and we optimize the model while progressively
varying the image supervision by means of a pretrained image-based diffusion
model. The input object may be given as a 3D triangular mesh, or directly
provided as Gaussians from a generative model such as DreamGaussian. GSEdit
ensures consistency across different viewpoints, maintaining the integrity of
the original object's information. Compared to previously proposed methods
relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making
3D editing tasks much faster. Our editing process is refined via the
application of the SDS loss, ensuring that our edits are both precise and
accurate. Our comprehensive evaluation demonstrates that GSEdit effectively
alters object shape and appearance following the given textual instructions
while preserving their coherence and detail.

Comments:
- 15 pages, 7 figures

---

## SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-08 | Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang | cs.GR | [PDF](http://arxiv.org/pdf/2403.05087v1){: .btn .btn-green } |

**Abstract**: We present SplattingAvatar, a hybrid 3D representation of photorealistic
human avatars with Gaussian Splatting embedded on a triangle mesh, which
renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We
disentangle the motion and appearance of a virtual human with explicit mesh
geometry and implicit appearance modeling with Gaussian Splatting. The
Gaussians are defined by barycentric coordinates and displacement on a triangle
mesh as Phong surfaces. We extend lifted optimization to simultaneously
optimize the parameters of the Gaussians while walking on the triangle mesh.
SplattingAvatar is a hybrid representation of virtual humans where the mesh
represents low-frequency motion and surface deformation, while the Gaussians
take over the high-frequency geometry and detailed appearance. Unlike existing
deformation methods that rely on an MLP-based linear blend skinning (LBS) field
for motion, we control the rotation and translation of the Gaussians directly
by mesh, which empowers its compatibility with various animation techniques,
e.g., skeletal animation, blend shapes, and mesh editing. Trainable from
monocular videos for both full-body and head avatars, SplattingAvatar shows
state-of-the-art rendering quality across multiple datasets.

Comments:
- [CVPR 2024] Code and data are available at
  https://github.com/initialneil/SplattingAvatar

---

## Finding Waldo: Towards Efficient Exploration of NeRF Scene Spaces

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Evangelos Skartados, Mehmet Kerim Yucel, Bruno Manganelli, Anastasios Drosou, Albert Saà-Garriga | cs.CV | [PDF](http://arxiv.org/pdf/2403.04508v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D
reconstruction and novel view synthesis in recent years due to their remarkable
performance. Despite the huge interest in NeRF methods, a practical use case of
NeRFs has largely been ignored; the exploration of the scene space modelled by
a NeRF. In this paper, for the first time in the literature, we propose and
formally define the scene exploration framework as the efficient discovery of
NeRF model inputs (i.e. coordinates and viewing angles), using which one can
render novel views that adhere to user-selected criteria. To remedy the lack of
approaches addressing scene exploration, we first propose two baseline methods
called Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).
We then cast scene exploration as an optimization problem, and propose the
criteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient
exploration. We test all three approaches with various criteria (e.g. saliency
maximization, image quality maximization, photo-composition quality
improvement) and show that our EGPS performs more favourably than other
baselines. We finally highlight key points and limitations, and outline
directions for future research in scene exploration.

Comments:
- Accepted at ACM MMSys'24

---

## Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Nikhil Mishra, Maximilian Sieb, Pieter Abbeel, Xi Chen | cs.RO | [PDF](http://arxiv.org/pdf/2403.04114v1){: .btn .btn-green } |

**Abstract**: Deep learning methods for perception are the cornerstone of many robotic
systems. Despite their potential for impressive performance, obtaining
real-world training data is expensive, and can be impractically difficult for
some tasks. Sim-to-real transfer with domain randomization offers a potential
workaround, but often requires extensive manual tuning and results in models
that are brittle to distribution shift between sim and real. In this work, we
introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF
model that is the centerpiece of a real-to-sim pipeline for synthesizing
training data targeted to scenes and objects from the real world. COV-NeRF
extracts objects from real images and composes them into new scenes, generating
photorealistic renderings and many types of 2D and 3D supervision, including
depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the
rendering quality of modern NeRF methods, and can be used to rapidly close the
sim-to-real gap across a variety of perceptual modalities.

Comments:
- ICRA 2024

---

## BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa | cs.CV | [PDF](http://arxiv.org/pdf/2403.04926v1){: .btn .btn-green } |

**Abstract**: Recent efforts in using 3D Gaussians for scene reconstruction and novel view
synthesis can achieve impressive results on curated benchmarks; however, images
captured in real life are often blurry. In this work, we analyze the robustness
of Gaussian-Splatting-based methods against various image blur, such as motion
blur, defocus blur, downscaling blur, \etc. Under these degradations,
Gaussian-Splatting-based methods tend to overfit and produce worse results than
Neural-Radiance-Field-based methods. To address this issue, we propose Blur
Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling
capacities such that a 3D-consistent and high quality scene can be
reconstructed despite image-wise blur. Specifically, we model blur by
estimating per-pixel convolution kernels from a Blur Proposal Network (BPN).
BPN is designed to consider spatial, color, and depth variations of the scene
to maximize modeling capacity. Additionally, BPN also proposes a
quality-assessing mask, which indicates regions where blur occur. Finally, we
introduce a coarse-to-fine kernel optimization scheme; this optimization scheme
is fast and avoids sub-optimal solutions due to a sparse point cloud
initialization, which often occurs when we apply Structure-from-Motion on
blurry images. We demonstrate that BAGS achieves photorealistic renderings
under various challenging blur conditions and imaging geometry, while
significantly improving upon existing approaches.



---

## Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Yuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille | eess.IV | [PDF](http://arxiv.org/pdf/2403.04116v1){: .btn .btn-green } |

**Abstract**: X-ray is widely applied for transmission imaging due to its stronger
penetration than natural light. When rendering novel view X-ray projections,
existing methods mainly based on NeRF suffer from long training time and slow
inference speed. In this paper, we propose a 3D Gaussian splatting-based
framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we
redesign a radiative Gaussian point cloud model inspired by the isotropic
nature of X-ray imaging. Our model excludes the influence of view direction
when learning to predict the radiation intensity of 3D points. Based on this
model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA
implementation. Secondly, we customize an Angle-pose Cuboid Uniform
Initialization (ACUI) strategy that directly uses the parameters of the X-ray
scanner to compute the camera information and then uniformly samples point
positions within a cuboid enclosing the scanned object. Experiments show that
our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying
less than 15% training time and over 73x inference speed. The application on
sparse-view CT reconstruction also reveals the practical values of our method.
Code and models will be publicly available at
https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training
process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .

Comments:
- The first 3D Gaussian Splatting-based method for X-ray 3D
  reconstruction

---

## DNAct: Diffusion Guided Multi-Task 3D Policy Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Ge Yan, Yueh-Hua Wu, Xiaolong Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.04115v2){: .btn .btn-green } |

**Abstract**: This paper presents DNAct, a language-conditioned multi-task policy framework
that integrates neural rendering pre-training and diffusion training to enforce
multi-modality learning in action sequence spaces. To learn a generalizable
multi-task policy with few demonstrations, the pre-training phase of DNAct
leverages neural rendering to distill 2D semantic features from foundation
models such as Stable Diffusion to a 3D space, which provides a comprehensive
semantic understanding regarding the scene. Consequently, it allows various
applications to challenging robotic tasks requiring rich 3D semantics and
accurate geometry. Furthermore, we introduce a novel approach utilizing
diffusion training to learn a vision and language feature that encapsulates the
inherent multi-modality in the multi-task demonstrations. By reconstructing the
action sequences from different tasks via the diffusion process, the model is
capable of distinguishing different modalities and thus improving the
robustness and the generalizability of the learned representation. DNAct
significantly surpasses SOTA NeRF-based multi-task manipulation approaches with
over 30% improvement in success rate. Project website: dnact.github.io.



---

## GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D  Scene Understanding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-06 | Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.03608v1){: .btn .btn-green } |

**Abstract**: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.

Comments:
- Accepted by CVPR2024

---

## A Deep Learning Framework for Wireless Radiation Field Reconstruction  and Channel Prediction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-05 | Haofan Lu, Christopher Vattheuer, Baharan Mirzasoleiman, Omid Abari | cs.NI | [PDF](http://arxiv.org/pdf/2403.03241v1){: .btn .btn-green } |

**Abstract**: We present NeWRF, a deep learning framework for predicting wireless channels.
Wireless channel prediction is a long-standing problem in the wireless
community and is a key technology for improving the coverage of wireless
network deployments. Today, a wireless deployment is evaluated by a site survey
which is a cumbersome process requiring an experienced engineer to perform
extensive channel measurements. To reduce the cost of site surveys, we develop
NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF).
NeWRF trains a neural network model with a sparse set of channel measurements,
and predicts the wireless channel accurately at any location in the site. We
introduce a series of techniques that integrate wireless propagation properties
into the NeRF framework to account for the fundamental differences between the
behavior of light and wireless signals. We conduct extensive evaluations of our
framework and show that our approach can accurately predict channels at
unvisited locations with significantly lower measurement density than prior
state-of-the-art



---

## Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-05 | Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2403.02751v1){: .btn .btn-green } |

**Abstract**: We present Splat-Nav, a navigation pipeline that consists of a real-time safe
planning module and a robust state estimation module designed to operate in the
Gaussian Splatting (GSplat) environment representation, a popular emerging 3D
scene representation from computer vision. We formulate rigorous collision
constraints that can be computed quickly to build a guaranteed-safe polytope
corridor through the map. We then optimize a B-spline trajectory through this
corridor. We also develop a real-time, robust state estimation module by
interpreting the GSplat representation as a point cloud. The module enables the
robot to localize its global pose with zero prior knowledge from RGB-D images
using point cloud alignment, and then track its own pose as it moves through
the scene from RGB images using image-to-point cloud localization. We also
incorporate semantics into the GSplat in order to obtain better images for
localization. All of these modules operate mainly on CPU, freeing up GPU
resources for tasks like real-time scene reconstruction. We demonstrate the
safety and robustness of our pipeline in both simulation and hardware, where we
show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude
faster than Neural Radiance Field (NeRF)-based navigation methods, thereby
enabling real-time navigation.



---

## DaReNeRF: Direction-aware Representation for Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-04 | Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2403.02265v1){: .btn .btn-green } |

**Abstract**: Addressing the intricate challenge of modeling and re-rendering dynamic
scenes, most recent approaches have sought to simplify these complexities using
plane-based explicit representations, overcoming the slow training time issues
associated with methods like Neural Radiance Fields (NeRF) and implicit
representations. However, the straightforward decomposition of 4D dynamic
scenes into multiple 2D plane-based representations proves insufficient for
re-rendering high-fidelity scenes with complex motions. In response, we present
a novel direction-aware representation (DaRe) approach that captures scene
dynamics from six different directions. This learned representation undergoes
an inverse dual-tree complex wavelet transformation (DTCWT) to recover
plane-based information. DaReNeRF computes features for each space-time point
by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny
MLP for color regression and leveraging volume rendering in training yield
state-of-the-art performance in novel view synthesis for complex dynamic
scenes. Notably, to address redundancy introduced by the six real and six
imaginary direction-aware wavelet coefficients, we introduce a trainable
masking approach, mitigating storage issues without significant performance
decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared
to prior art while delivering superior performance.

Comments:
- Accepted at CVPR 2024. Paper + supplementary material

---

## Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input  Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-04 | Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song | cs.CV | [PDF](http://arxiv.org/pdf/2403.02063v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis with sparse input views is important for real-world
applications like AR/VR and autonomous driving. Recent methods have integrated
depth information into NeRFs for sparse input synthesis, leveraging depth prior
for geometric and spatial understanding. However, most existing works tend to
overlook inaccuracies within depth maps and have low time efficiency. To
address these issues, we propose a depth-guided robust and fast point cloud
fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel
grid of features. A point cloud is constructed for each input view,
characterized within the voxel grid using matrices and vectors. We accumulate
the point cloud of each input view to construct the fused point cloud of the
entire scene. Each voxel determines its density and appearance by referring to
the point cloud of the entire scene. Through point cloud fusion and voxel grid
fine-tuning, inaccuracies in depth values are refined or substituted by those
from other views. Moreover, our method can achieve faster reconstruction and
greater compactness through effective vector-matrix decomposition. Experimental
results underline the superior performance and time efficiency of our approach
compared to state-of-the-art baselines.



---

## Neural Field Classifiers via Target Encoding and Classification Loss

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun | cs.CV | [PDF](http://arxiv.org/pdf/2403.01058v1){: .btn .btn-green } |

**Abstract**: Neural field methods have seen great progress in various long-standing tasks
in computer vision and computer graphics, including novel view synthesis and
geometry reconstruction. As existing neural field methods try to predict some
coordinate-based continuous target values, such as RGB for Neural Radiance
Field (NeRF), all of these methods are regression models and are optimized by
some regression loss. However, are regression models really better than
classification models for neural field methods? In this work, we try to visit
this very fundamental but overlooked question for neural fields from a machine
learning perspective. We successfully propose a novel Neural Field Classifier
(NFC) framework which formulates existing neural field methods as
classification tasks rather than regression tasks. The proposed NFC can easily
transform arbitrary Neural Field Regressor (NFR) into its classification
variant via employing a novel Target Encoding module and optimizing a
classification loss. By encoding a continuous regression target into a
high-dimensional discrete encoding, we naturally formulate a multi-label
classification task. Extensive experiments demonstrate the impressive
effectiveness of NFC at the nearly free extra computational costs. Moreover,
NFC also shows robustness to sparse inputs, corrupted images, and dynamic
scenes.

Comments:
- ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables

---

## NeRF-VPT: Learning Novel View Representations with Neural Radiance  Fields via View Prompt Tuning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr | cs.CV | [PDF](http://arxiv.org/pdf/2403.01325v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have garnered remarkable success in novel view
synthesis. Nonetheless, the task of generating high-quality images for novel
views persists as a critical challenge. While the existing efforts have
exhibited commendable progress, capturing intricate details, enhancing
textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics
warrant further focused attention and advancement. In this work, we propose
NeRF-VPT, an innovative method for novel view synthesis to address these
challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning
paradigm, wherein RGB information gained from preceding rendering outcomes
serves as instructive visual prompts for subsequent rendering stages, with the
aspiration that the prior knowledge embedded in the prompts can facilitate the
gradual enhancement of rendered image quality. NeRF-VPT only requires sampling
RGB data from previous stage renderings as priors at each training stage,
without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is
plug-and-play and can be readily integrated into existing methods. By
conducting comparative analyses of our NeRF-VPT against several NeRF-based
approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,
Real Forward-Facing, Replica dataset, and a user-captured dataset, we
substantiate that our NeRF-VPT significantly elevates baseline performance and
proficiently generates more high-quality novel view images than all the
compared state-of-the-art methods. Furthermore, the cascading learning of
NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in
a significant enhancement of accuracy for sparse-view novel view synthesis. The
source code and dataset are available at
\url{https://github.com/Freedomcls/NeRF-VPT}.

Comments:
- AAAI 2024

---

## Neural radiance fields-based holography [Invited]

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba | cs.CV | [PDF](http://arxiv.org/pdf/2403.01137v1){: .btn .btn-green } |

**Abstract**: This study presents a novel approach for generating holograms based on the
neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data
is difficult in hologram computation. NeRF is a state-of-the-art technique for
3D light-field reconstruction from 2D images based on volume rendering. The
NeRF can rapidly predict new-view images that do not include a training
dataset. In this study, we constructed a rendering pipeline directly from a 3D
light field generated from 2D images by NeRF for hologram generation using deep
neural networks within a reasonable time. The pipeline comprises three main
components: the NeRF, a depth predictor, and a hologram generator, all
constructed using deep neural networks. The pipeline does not include any
physical calculations. The predicted holograms of a 3D scene viewed from any
direction were computed using the proposed pipeline. The simulation and
experimental results are presented.



---

## DISORF: A Distributed Online NeRF Training and Rendering Framework for  Mobile Robots

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-01 | Chunlin Li, Ruofan Liang, Hanrui Fan, Zhengen Zhang, Sankeerth Durvasula, Nandita Vijaykumar | cs.RO | [PDF](http://arxiv.org/pdf/2403.00228v1){: .btn .btn-green } |

**Abstract**: We present a framework, DISORF, to enable online 3D reconstruction and
visualization of scenes captured by resource-constrained mobile robots and edge
devices. To address the limited compute capabilities of edge devices and
potentially limited network availability, we design a framework that
efficiently distributes computation between the edge device and remote server.
We leverage on-device SLAM systems to generate posed keyframes and transmit
them to remote servers that can perform high quality 3D reconstruction and
visualization at runtime by leveraging NeRF models. We identify a key challenge
with online NeRF training where naive image sampling strategies can lead to
significant degradation in rendering quality. We propose a novel shifted
exponential frame sampling method that addresses this challenge for online NeRF
training. We demonstrate the effectiveness of our framework in enabling
high-quality real-time reconstruction and visualization of unknown scenes as
they are captured and streamed from cameras in mobile robots and edge devices.


