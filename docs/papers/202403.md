---
layout: default
title: March 2024
parent: Papers
nav_order: 202403
---

<!---metadata--->


## RadSplat: Radiance Field-Informed Gaussian Splatting for Robust  Real-Time Rendering with 900+ FPS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-20 | Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2403.13806v1){: .btn .btn-green } |

**Abstract**: Recent advances in view synthesis and real-time rendering have achieved
photorealistic quality at impressive rendering speeds. While Radiance
Field-based methods achieve state-of-the-art quality in challenging scenarios
such as in-the-wild captures and large-scale scenes, they often suffer from
excessively high compute requirements linked to volumetric rendering. Gaussian
Splatting-based methods, on the other hand, rely on rasterization and naturally
achieve real-time rendering but suffer from brittle optimization heuristics
that underperform on more challenging scenes. In this work, we present
RadSplat, a lightweight method for robust real-time rendering of complex
scenes. Our main contributions are threefold. First, we use radiance fields as
a prior and supervision signal for optimizing point-based scene
representations, leading to improved quality and more robust optimization.
Next, we develop a novel pruning technique reducing the overall point count
while maintaining high quality, leading to smaller and more compact scene
representations with faster inference speeds. Finally, we propose a novel
test-time filtering approach that further accelerates rendering and allows to
scale to larger, house-sized scenes. We find that our method enables
state-of-the-art synthesis of complex captures at 900+ FPS.

Comments:
- Project page at https://m-niemeyer.github.io/radsplat/

---

## MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with  Wireless Coordination

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-20 | Weiying Wang, Victor Cai, Stephanie Gil | cs.RO | [PDF](http://arxiv.org/pdf/2403.13348v1){: .btn .btn-green } |

**Abstract**: This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework
that leverages wireless signal-based coordination between robots and Neural
Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D
reconstruction, including inter-robot pose estimation, localization uncertainty
quantification, and active best-next-view selection. We introduce a method for
using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate
relative poses between robots, as well as quantifying and incorporating the
uncertainty embedded in the wireless localization of these pose estimates into
the NeRF training loss to mitigate the impact of inaccurate camera poses.
Furthermore, we propose an active view selection approach that accounts for
robot pose uncertainty when determining the next-best views to improve the 3D
reconstruction, enabling faster convergence through intelligent view selection.
Extensive experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our framework in theory and in practice. Leveraging wireless
coordination and localization uncertainty-aware training, MULAN-WC can achieve
high-quality 3d reconstruction which is close to applying the ground truth
camera poses. Furthermore, the quantification of the information gain from a
novel view enables consistent rendering quality improvement with incrementally
captured images by commending the robot the novel view position. Our hardware
experiments showcase the practicality of deploying MULAN-WC to real robotic
systems.



---

## Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation  for Natural Camera Motion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-20 | Otto Seiskari, Jerry Ylilammi, Valtteri Kaatrasalo, Pekka Rantalankila, Matias Turkulainen, Juho Kannala, Esa Rahtu, Arno Solin | cs.CV | [PDF](http://arxiv.org/pdf/2403.13327v1){: .btn .btn-green } |

**Abstract**: High-quality scene reconstruction and novel view synthesis based on Gaussian
Splatting (3DGS) typically require steady, high-quality photographs, often
impractical to capture with handheld cameras. We present a method that adapts
to camera motion and allows high-quality scene reconstruction with handheld
video data suffering from motion blur and rolling shutter distortion. Our
approach is based on detailed modelling of the physical image formation process
and utilizes velocities estimated using visual-inertial odometry (VIO). Camera
poses are considered non-static during the exposure time of a single image
frame and camera poses are further optimized in the reconstruction process. We
formulate a differentiable rendering pipeline that leverages screen space
approximation to efficiently incorporate rolling-shutter and motion blur
effects into the 3DGS framework. Our results with both synthetic and real data
demonstrate superior performance in mitigating camera motion over existing
methods, thereby advancing 3DGS in naturalistic settings.

Comments:
- Source code available at https://github.com/SpectacularAI/3dgs-deblur

---

## DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Zaid Tasneem, Akshat Dave, Abhishek Singh, Kushagra Tiwary, Praneeth Vepakomma, Ashok Veeraraghavan, Ramesh Raskar | cs.CV | [PDF](http://arxiv.org/pdf/2403.13199v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) show potential for transforming images
captured worldwide into immersive 3D visual experiences. However, most of this
captured visual data remains siloed in our camera rolls as these images contain
personal details. Even if made public, the problem of learning 3D
representations of billions of scenes captured daily in a centralized manner is
computationally intractable. Our approach, DecentNeRF, is the first attempt at
decentralized, crowd-sourced NeRFs that require $\sim 10^4\times$ less server
computing for a scene than a centralized approach. Instead of sending the raw
data, our approach requires users to send a 3D representation, distributing the
high computation cost of training centralized NeRFs between the users. It
learns photorealistic scene representations by decomposing users' 3D views into
personal and global NeRFs and a novel optimally weighted aggregation of only
the latter. We validate the advantage of our approach to learn NeRFs with
photorealism and minimal server computation cost on structured synthetic and
real-world photo tourism datasets. We further analyze how secure aggregation of
global NeRFs in DecentNeRF minimizes the undesired reconstruction of personal
content by the server.



---

## Global-guided Focal Neural Radiance Field for Large-scale Scene  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.12839v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields~(NeRF) have recently been applied to render
large-scale scenes. However, their limited model capacity typically results in
blurred rendering results. Existing large-scale NeRFs primarily address this
limitation by partitioning the scene into blocks, which are subsequently
handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and
processed independently, lead to inconsistencies in geometry and appearance
across the scene. Consequently, the rendering quality fails to exhibit
significant improvement despite the expansion of model capacity. In this work,
we present global-guided focal neural radiance field (GF-NeRF) that achieves
high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a
two-stage (Global and Focal) architecture and a global-guided training
strategy. The global stage obtains a continuous representation of the entire
scene while the focal stage decomposes the scene into multiple blocks and
further processes them with distinct sub-encoders. Leveraging this two-stage
architecture, sub-encoders only need fine-tuning based on the global encoder,
thus reducing training complexity in the focal stage while maintaining
scene-wide consistency. Spatial information and error information from the
global stage also benefit the sub-encoders to focus on crucial areas and
effectively capture more details of large-scale scenes. Notably, our approach
does not rely on any prior knowledge about the target scene, attributing
GF-NeRF adaptable to various large-scale scene types, including street-view and
aerial-view scenes. We demonstrate that our method achieves high-fidelity,
natural rendering results on various types of large-scale datasets. Our project
page: https://shaomq2187.github.io/GF-NeRF/



---

## IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single  image and a NeRF model

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Matteo Bortolon, Theodore Tsesmelis, Stuart James, Fabio Poiesi, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2403.12682v1){: .btn .btn-green } |

**Abstract**: We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera
pose of a given image, building on the Neural Radiance Fields (NeRF)
formulation. IFFNeRF is specifically designed to operate in real-time and
eliminates the need for an initial pose guess that is proximate to the sought
solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface
points from within the NeRF model. From these sampled points, we cast rays and
deduce the color for each ray through pixel-level view synthesis. The camera
pose can then be estimated as the solution to a Least Squares problem by
selecting correspondences between the query image and the resulting bundle. We
facilitate this process through a learned attention mechanism, bridging the
query image embedding with the embedding of parameterized rays, thereby
matching rays pertinent to the image. Through synthetic and real evaluation
settings, we show that our method can improve the angular and translation error
accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing
at 34fps on consumer hardware and not requiring the initial pose guess.

Comments:
- Accepted ICRA 2024, Project page:
  https://mbortolon97.github.io/iffnerf/

---

## HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2403.12722v1){: .btn .btn-green } |

**Abstract**: Holistic understanding of urban scenes based on RGB images is a challenging
yet important problem. It encompasses understanding both the geometry and
appearance to enable novel view synthesis, parsing semantic labels, and
tracking moving objects. Despite considerable progress, existing approaches
often focus on specific aspects of this task and require additional inputs such
as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we
introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic
urban scene understanding. Our main idea involves the joint optimization of
geometry, appearance, semantics, and motion using a combination of static and
dynamic 3D Gaussians, where moving object poses are regularized via physical
constraints. Our approach offers the ability to render new viewpoints in
real-time, yielding 2D and 3D semantic information with high accuracy, and
reconstruct dynamic scenes, even in scenarios where 3D bounding box detection
are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2
demonstrate the effectiveness of our approach.

Comments:
- Our project page is at https://xdimlab.github.io/hugs_website

---

## Learning Neural Volumetric Pose Features for Camera Localization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Jingyu Lin, Jiaqi Gu, Bojian Wu, Lubin Fan, Renjie Chen, Ligang Liu, Jieping Ye | cs.CV | [PDF](http://arxiv.org/pdf/2403.12800v1){: .btn .btn-green } |

**Abstract**: We introduce a novel neural volumetric pose feature, termed PoseMap, designed
to enhance camera localization by encapsulating the information between images
and the associated camera poses. Our framework leverages an Absolute Pose
Regression (APR) architecture, together with an augmented NeRF module. This
integration not only facilitates the generation of novel views to enrich the
training dataset but also enables the learning of effective pose features.
Additionally, we extend our architecture for self-supervised online alignment,
allowing our method to be used and fine-tuned for unlabelled images within a
unified framework. Experiments demonstrate that our method achieves 14.28% and
20.51% performance gain on average in indoor and outdoor benchmark scenes,
outperforming existing APR methods with state-of-the-art accuracy.

Comments:
- 14 pages, 9 figures

---

## GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann | cs.CV | [PDF](http://arxiv.org/pdf/2403.12365v1){: .btn .btn-green } |

**Abstract**: Creating 4D fields of Gaussian Splatting from images or videos is a
challenging task due to its under-constrained nature. While the optimization
can draw photometric reference from the input videos or be regulated by
generative models, directly supervising Gaussian motions remains underexplored.
In this paper, we introduce a novel concept, Gaussian flow, which connects the
dynamics of 3D Gaussians and pixel velocities between consecutive frames. The
Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into
the image space. This differentiable process enables direct dynamic supervision
from optical flow. Our method significantly benefits 4D dynamic content
generation and 4D novel view synthesis with Gaussian Splatting, especially for
contents with rich motions that are hard to be handled by existing methods. The
common color drifting issue that happens in 4D generation is also resolved with
improved Guassian dynamics. Superior visual quality on extensive experiments
demonstrates our method's effectiveness. Quantitative and qualitative
evaluations show that our method achieves state-of-the-art results on both
tasks of 4D generation and 4D novel view synthesis. Project page:
https://zerg-overmind.github.io/GaussianFlow.github.io/



---

## RGBD GS-ICP SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Seongbo Ha, Jiung Yeon, Hyeonwoo Yu | cs.CV | [PDF](http://arxiv.org/pdf/2403.12550v1){: .btn .btn-green } |

**Abstract**: Simultaneous Localization and Mapping (SLAM) with dense representation plays
a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)
applications. Recent advancements in dense representation SLAM have highlighted
the potential of leveraging neural scene representation and 3D Gaussian
representation for high-fidelity spatial representation. In this paper, we
propose a novel dense representation SLAM approach with a fusion of Generalized
Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast
to existing methods, we utilize a single Gaussian map for both tracking and
mapping, resulting in mutual benefits. Through the exchange of covariances
between tracking and mapping processes with scale alignment techniques, we
minimize redundant computations and achieve an efficient system. Additionally,
we enhance tracking accuracy and mapping quality through our keyframe selection
methods. Experimental results demonstrate the effectiveness of our approach,
showing an incredibly fast speed up to 107 FPS (for the entire system) and
superior quality of the reconstructed map.



---

## High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided  Densification and Regularized Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson | cs.RO | [PDF](http://arxiv.org/pdf/2403.12535v1){: .btn .btn-green } |

**Abstract**: We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that
provides metrically accurate pose tracking and visually realistic
reconstruction. To this end, we first propose a Gaussian densification strategy
based on the rendering loss to map unobserved areas and refine reobserved
areas. Second, we introduce extra regularization parameters to alleviate the
forgetting problem in the continuous mapping problem, where parameters tend to
overfit the latest frame and result in decreasing rendering quality for
previous frames. Both mapping and tracking are performed with Gaussian
parameters by minimizing re-rendering loss in a differentiable way. Compared to
recent neural and concurrently developed gaussian splatting RGBD SLAM
baselines, our method achieves state-of-the-art results on the synthetic
dataset Replica and competitive results on the real-world dataset TUM.

Comments:
- submitted to IROS24

---

## GVGEN: Text-to-3D Generation with Volumetric Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He | cs.CV | [PDF](http://arxiv.org/pdf/2403.12957v1){: .btn .btn-green } |

**Abstract**: In recent years, 3D Gaussian splatting has emerged as a powerful technique
for 3D reconstruction and generation, known for its fast and high-quality
rendering capabilities. To address these shortcomings, this paper introduces a
novel diffusion-based framework, GVGEN, designed to efficiently generate 3D
Gaussian representations from text input. We propose two innovative
techniques:(1) Structured Volumetric Representation. We first arrange
disorganized 3D Gaussian points as a structured form GaussianVolume. This
transformation allows the capture of intricate texture details within a volume
composed of a fixed number of Gaussians. To better optimize the representation
of these details, we propose a unique pruning and densifying method named the
Candidate Pool Strategy, enhancing detail fidelity through selective
optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the
generation of GaussianVolume and empower the model to generate instances with
detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially
constructs a basic geometric structure, followed by the prediction of complete
Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in
qualitative and quantitative assessments compared to existing 3D generation
methods. Simultaneously, it maintains a fast generation speed ($\sim$7
seconds), effectively striking a balance between quality and efficiency.

Comments:
- project page: https://gvgen.github.io/

---

## Depth-guided NeRF Training via Earth Mover's Distance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-19 | Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy | cs.CV | [PDF](http://arxiv.org/pdf/2403.13206v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of
predicted viewpoints. However, the photometric loss often does not provide
enough information to disambiguate between different possible geometries
yielding the same image. Previous work has thus incorporated depth supervision
during NeRF training, leveraging dense predictions from pre-trained depth
networks as pseudo-ground truth. While these depth priors are assumed to be
perfect once filtered for noise, in practice, their accuracy is more
challenging to capture. This work proposes a novel approach to uncertainty in
depth priors for NeRF supervision. Instead of using custom-trained depth or
uncertainty priors, we use off-the-shelf pretrained diffusion models to predict
depth and capture uncertainty during the denoising process. Because we know
that depth priors are prone to errors, we propose to supervise the ray
termination distance distribution with Earth Mover's Distance instead of
enforcing the rendered depth to replicate the depth prior exactly through
L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth
metrics by a large margin while maintaining performance on photometric
measures.

Comments:
- Preprint. Under review

---

## BAGS: Building Animatable Gaussian Splatting from a Monocular Video with  Diffusion Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Tingyang Zhang, Qingzhe Gao, Weiyu Li, Libin Liu, Baoquan Chen | cs.CV | [PDF](http://arxiv.org/pdf/2403.11427v1){: .btn .btn-green } |

**Abstract**: Animatable 3D reconstruction has significant applications across various
fields, primarily relying on artists' handcraft creation. Recently, some
studies have successfully constructed animatable 3D models from monocular
videos. However, these approaches require sufficient view coverage of the
object within the input video and typically necessitate significant time and
computational costs for training and rendering. This limitation restricts the
practical applications. In this work, we propose a method to build animatable
3D Gaussian Splatting from monocular video with diffusion priors. The 3D
Gaussian representations significantly accelerate the training and rendering
process, and the diffusion priors allow the method to learn 3D models with
limited viewpoints. We also present the rigid regularization to enhance the
utilization of the priors. We perform an extensive evaluation across various
real-world videos, demonstrating its superior performance compared to the
current state-of-the-art methods.

Comments:
- https://talegqz.github.io/BAGS/

---

## BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Lingzhe Zhao, Peng Wang, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2403.11831v2){: .btn .btn-green } |

**Abstract**: While neural rendering has demonstrated impressive capabilities in 3D scene
reconstruction and novel view synthesis, it heavily relies on high-quality
sharp images and accurate camera poses. Numerous approaches have been proposed
to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly
encountered in real-world scenarios such as low-light or long-exposure
conditions. However, the implicit representation of NeRF struggles to
accurately recover intricate details from severely motion-blurred images and
cannot achieve real-time rendering. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time
rendering by explicitly optimizing point clouds as Gaussian spheres.
  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle
Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian
representation and handles severe motion-blurred images with inaccurate camera
poses to achieve high-quality scene reconstruction. Our method models the
physical image formation process of motion-blurred images and jointly learns
the parameters of Gaussians while recovering camera motion trajectories during
exposure time.
  In our experiments, we demonstrate that BAD-Gaussians not only achieves
superior rendering quality compared to previous state-of-the-art deblur neural
rendering methods on both synthetic and real datasets but also enables
real-time rendering capabilities.
  Our project page and source code is available at
https://lingzhezhao.github.io/BAD-Gaussians/

Comments:
- Project Page and Source Code:
  https://lingzhezhao.github.io/BAD-Gaussians/

---

## 3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal  Calibration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux | cs.CV | [PDF](http://arxiv.org/pdf/2403.11577v1){: .btn .btn-green } |

**Abstract**: Reliable multimodal sensor fusion algorithms require accurate spatiotemporal
calibration. Recently, targetless calibration techniques based on implicit
neural representations have proven to provide precise and robust results.
Nevertheless, such methods are inherently slow to train given the high
computational overhead caused by the large number of sampled points required
for volume rendering. With the recent introduction of 3D Gaussian Splatting as
a faster alternative to implicit representation methods, we propose to leverage
this new rendering approach to achieve faster multi-sensor calibration. We
introduce 3DGS-Calib, a new calibration method that relies on the speed and
rendering accuracy of 3D Gaussian Splatting to achieve multimodal
spatiotemporal calibration that is accurate, robust, and with a substantial
speed-up compared to methods relying on implicit neural representations. We
demonstrate the superiority of our proposal with experimental results on
sequences from KITTI-360, a widely used driving dataset.

Comments:
- Under review

---

## Bridging 3D Gaussian and Mesh for Freeview Video Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yuting Xiao, Xuan Wang, Jiafei Li, Hongrui Cai, Yanbo Fan, Nan Xue, Minghui Yang, Yujun Shen, Shenghua Gao | cs.GR | [PDF](http://arxiv.org/pdf/2403.11453v1){: .btn .btn-green } |

**Abstract**: This is only a preview version of GauMesh. Recently, primitive-based
rendering has been proven to achieve convincing results in solving the problem
of modeling and rendering the 3D dynamic scene from 2D images. Despite this, in
the context of novel view synthesis, each type of primitive has its inherent
defects in terms of representation ability. It is difficult to exploit the mesh
to depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D
Gaussian Splatting) method usually produces artifacts or blurry pixels in the
area with smooth geometry and sharp textures. As a result, it is difficult,
even not impossible, to represent the complex and dynamic scene with a single
type of primitive. To this end, we propose a novel approach, GauMesh, to bridge
the 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a
sequence of tracked mesh as initialization, our goal is to simultaneously
optimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians,
and the deformation field. At a specific time, we perform $\alpha$-blending on
the RGB and opacity values based on the merged and re-ordered z-buffers from
mesh and 3D Gaussian rasterizations. This produces the final rendering, which
is supervised by the ground-truth image. Experiments demonstrate that our
approach adapts the appropriate type of primitives to represent the different
parts of the dynamic scene and outperforms all the baseline methods in both
quantitative and qualitative comparisons without losing render speed.

Comments:
- 7 pages

---

## Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Zhiyang Guo, Wengang Zhou, Li Li, Min Wang, Houqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.11447v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene
reconstruction. However, existing methods focus mainly on extending static 3DGS
into a time-variant representation, while overlooking the rich motion
information carried by 2D observations, thus suffering from performance
degradation and model redundancy. To address the above problem, we propose a
novel motion-aware enhancement framework for dynamic scene reconstruction,
which mines useful motion cues from optical flow to improve different paradigms
of dynamic 3DGS. Specifically, we first establish a correspondence between 3D
Gaussian movements and pixel-level flow. Then a novel flow augmentation method
is introduced with additional insights into uncertainty and loss collaboration.
Moreover, for the prevalent deformation-based paradigm that presents a harder
optimization problem, a transient-aware deformation auxiliary module is
proposed. We conduct extensive experiments on both multi-view and monocular
scenes to verify the merits of our work. Compared with the baselines, our
method shows significant superiority in both rendering quality and efficiency.



---

## Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous  Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Antoine Schnepf, Karim Kassab, Jean-Yves Franceschi, Laurent Caraffa, Flavian Vasile, Jeremie Mary, Andrew Comport, Valérie Gouet-Brunet | cs.CV | [PDF](http://arxiv.org/pdf/2403.11678v1){: .btn .btn-green } |

**Abstract**: We present a method enabling the scaling of NeRFs to learn a large number of
semantically-similar scenes. We combine two techniques to improve the required
training time and memory cost per scene. First, we learn a 3D-aware latent
space in which we train Tri-Plane scene representations, hence reducing the
resolution at which scenes are learned. Moreover, we present a way to share
common information across scenes, hence allowing for a reduction of model
complexity to learn a particular scene. Our method reduces effective per-scene
memory costs by 44% and per-scene time costs by 86% when training 1000 scenes.
Our project page can be found at https://3da-ae.github.io .



---

## Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot  Navigation and 3D Scene Understanding with FisherRF


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Guangyi Liu, Wen Jiang, Boshu Lei, Vivek Pandey, Kostas Daniilidis, Nader Motee | cs.RO | [PDF](http://arxiv.org/pdf/2403.11396v1){: .btn .btn-green } |

**Abstract**: This work proposes a novel approach to bolster both the robot's risk
assessment and safety measures while deepening its understanding of 3D scenes,
which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian
Splatting. To further enhance these capabilities, we incorporate additional
sampled views from the environment with the RF model. One of our key
contributions is the introduction of Risk-aware Environment Masking (RaEM),
which prioritizes crucial information by selecting the next-best-view that
maximizes the expected information gain. This targeted approach aims to
minimize uncertainties surrounding the robot's path and enhance the safety of
its navigation. Our method offers a dual benefit: improved robot safety and
increased efficiency in risk-aware 3D scene reconstruction and understanding.
Extensive experiments in real-world scenarios demonstrate the effectiveness of
our proposed approach, highlighting its potential to establish a robust and
safety-focused framework for active robot exploration and 3D scene
understanding.



---

## Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from  Aerial Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui | cs.CV | [PDF](http://arxiv.org/pdf/2403.11812v1){: .btn .btn-green } |

**Abstract**: We present a neural radiance field method for urban-scale semantic and
building-level instance segmentation from aerial images by lifting noisy 2D
labels to 3D. This is a challenging problem due to two primary reasons.
Firstly, objects in urban aerial images exhibit substantial variations in size,
including buildings, cars, and roads, which pose a significant challenge for
accurate 2D segmentation. Secondly, the 2D labels generated by existing
segmentation methods suffer from the multi-view inconsistency problem,
especially in the case of aerial images, where each image captures only a small
portion of the entire scene. To overcome these limitations, we first introduce
a scale-adaptive semantic label fusion strategy that enhances the segmentation
of objects of varying sizes by combining labels predicted from different
altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then
introduce a novel cross-view instance label grouping strategy based on the 3D
scene representation to mitigate the multi-view inconsistency problem in the 2D
instance labels. Furthermore, we exploit multi-view reconstructed depth priors
to improve the geometric quality of the reconstructed radiance field, resulting
in enhanced segmentation results. Experiments on multiple real-world
urban-scale datasets demonstrate that our approach outperforms existing
methods, highlighting its effectiveness.

Comments:
- CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/

---

## ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle | cs.CV | [PDF](http://arxiv.org/pdf/2403.12154v1){: .btn .btn-green } |

**Abstract**: Thermal scene reconstruction exhibit great potential for applications across
a broad spectrum of fields, including building energy consumption analysis and
non-destructive testing. However, existing methods typically require dense
scene measurements and often rely on RGB images for 3D geometry reconstruction,
with thermal information being projected post-reconstruction. This two-step
strategy, adopted due to the lack of texture in thermal images, can lead to
disparities between the geometry and temperatures of the reconstructed objects
and those of the actual scene. To address this challenge, we propose
ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields,
capable of rendering new RGB and thermal views of a scene jointly. To overcome
the lack of texture in thermal images, we use paired RGB and thermal images to
learn scene density, while distinct networks estimate color and temperature
information. Furthermore, we introduce ThermoScenes, a new dataset to palliate
the lack of available RGB+thermal datasets for scene reconstruction.
Experimental results validate that ThermoNeRF achieves accurate thermal image
synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement
of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a
state-of-the-art NeRF method.



---

## DVN-SLAM: Dynamic Visual Neural SLAM Based on Local-Global Encoding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Wenhua Wu, Guangming Wang, Ting Deng, Sebastian Aegidius, Stuart Shanks, Valerio Modugno, Dimitrios Kanoulas, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.11776v1){: .btn .btn-green } |

**Abstract**: Recent research on Simultaneous Localization and Mapping (SLAM) based on
implicit representation has shown promising results in indoor environments.
However, there are still some challenges: the limited scene representation
capability of implicit encodings, the uncertainty in the rendering process from
implicit representations, and the disruption of consistency by dynamic objects.
To address these challenges, we propose a real-time dynamic visual SLAM system
based on local-global fusion neural implicit representation, named DVN-SLAM. To
improve the scene representation capability, we introduce a local-global fusion
neural implicit representation that enables the construction of an implicit map
while considering both global structure and local details. To tackle
uncertainties arising from the rendering process, we design an information
concentration loss for optimization, aiming to concentrate scene information on
object surfaces. The proposed DVN-SLAM achieves competitive performance in
localization and mapping across multiple datasets. More importantly, DVN-SLAM
demonstrates robustness in dynamic scenes, a trait that sets it apart from
other NeRF-based methods.



---

## GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with  Noisy Polarization Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong | cs.CV | [PDF](http://arxiv.org/pdf/2403.11899v1){: .btn .btn-green } |

**Abstract**: Learning surfaces from neural radiance field (NeRF) became a rising topic in
Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods
demonstrated their ability to reconstruct accurate 3D shapes of Lambertian
scenes. However, their results on reflective scenes are unsatisfactory due to
the entanglement of specular radiance and complicated geometry. To address the
challenges, we propose a Gaussian-based representation of normals in SDF
fields. Supervised by polarization priors, this representation guides the
learning of geometry behind the specular reflection and captures more details
than existing methods. Moreover, we propose a reweighting strategy in the
optimization process to alleviate the noise issue of polarization priors. To
validate the effectiveness of our design, we capture polarimetric information,
and ground truth meshes in additional reflective scenes with various geometry.
We also evaluated our framework on the PANDORA dataset. Comparisons prove our
method outperforms existing neural 3D reconstruction methods in reflective
scenes by a large margin.

Comments:
- Accepted to ICLR 2024 Poster. For the Appendix, please see
  http://yukiumi13.github.io/gnerp_page

---

## Exploring Multi-modal Neural Scene Representations With Applications on  Thermal Imaging

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger | cs.CV | [PDF](http://arxiv.org/pdf/2403.11865v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard
for the task of novel view synthesis when trained on a set of RGB images. In
this paper, we conduct a comprehensive evaluation of neural scene
representations, such as NeRFs, in the context of multi-modal learning.
Specifically, we present four different strategies of how to incorporate a
second modality, other than RGB, into NeRFs: (1) training from scratch
independently on both modalities; (2) pre-training on RGB and fine-tuning on
the second modality; (3) adding a second branch; and (4) adding a separate
component to predict (color) values of the additional modality. We chose
thermal imaging as second modality since it strongly differs from RGB in terms
of radiosity, making it challenging to integrate into neural scene
representations. For the evaluation of the proposed strategies, we captured a
new publicly available multi-view dataset, ThermalMix, consisting of six common
objects and about 360 RGB and thermal images in total. We employ cross-modality
calibration prior to data capturing, leading to high-quality alignments between
RGB and thermal images. Our findings reveal that adding a second branch to NeRF
performs best for novel view synthesis on thermal images while also yielding
compelling results on RGB. Finally, we also show that our analysis generalizes
to other modalities, including near-infrared images and depth maps. Project
page: https://mert-o.github.io/ThermalNeRF/.

Comments:
- 24 pages, 14 figures

---

## RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Sibi Catley-Chandar, Richard Shaw, Gregory Slabaugh, Eduardo Perez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2403.11909v1){: .btn .btn-green } |

**Abstract**: Recent advances in neural rendering have enabled highly photorealistic 3D
scene reconstruction and novel view synthesis. Despite this progress, current
state-of-the-art methods struggle to reconstruct high frequency detail, due to
factors such as a low-frequency bias of radiance fields and inaccurate camera
calibration. One approach to mitigate this issue is to enhance images
post-rendering. 2D enhancers can be pre-trained to recover some detail but are
agnostic to scene geometry and do not easily generalize to new distributions of
image degradation. Conversely, existing 3D enhancers are able to transfer
detail from nearby training images in a generalizable manner, but suffer from
inaccurate camera calibration and can propagate errors from the geometry into
rendered images. We propose a neural rendering enhancer, RoGUENeRF, which
exploits the best of both paradigms. Our method is pre-trained to learn a
general enhancer while also leveraging information from nearby training images
via robust 3D alignment and geometry-aware fusion. Our approach restores
high-frequency textures while maintaining geometric consistency and is also
robust to inaccurate camera calibration. We show that RoGUENeRF substantially
enhances the rendering quality of a wide range of neural rendering baselines,
e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the
real world 360v2 dataset.



---

## FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo  Endoscopic Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir Navab, Benjamin Busam, Alexander Ladikos | cs.CV | [PDF](http://arxiv.org/pdf/2403.12198v1){: .btn .btn-green } |

**Abstract**: Reconstruction of endoscopic scenes is an important asset for various medical
applications, from post-surgery analysis to educational training. Neural
rendering has recently shown promising results in endoscopic reconstruction
with deforming tissue. However, the setup has been restricted to a static
endoscope, limited deformation, or required an external tracking device to
retrieve camera pose information of the endoscopic camera. With FLex we adress
the challenging setup of a moving endoscope within a highly dynamic environment
of deforming tissue. We propose an implicit scene separation into multiple
overlapping 4D neural radiance fields (NeRFs) and a progressive optimization
scheme jointly optimizing for reconstruction and camera poses from scratch.
This improves the ease-of-use and allows to scale reconstruction capabilities
in time to process surgical videos of 5,000 frames and more; an improvement of
more than ten times compared to the state of the art while being agnostic to
external tracking information. Extensive evaluations on the StereoMIS dataset
show that FLex significantly improves the quality of novel view synthesis while
maintaining competitive pose accuracy.



---

## UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures  for Human Avatar Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan | cs.CV | [PDF](http://arxiv.org/pdf/2403.11589v1){: .btn .btn-green } |

**Abstract**: Reconstructing photo-realistic drivable human avatars from multi-view image
sequences has been a popular and challenging topic in the field of computer
vision and graphics. While existing NeRF-based methods can achieve high-quality
novel view rendering of human models, both training and inference processes are
time-consuming. Recent approaches have utilized 3D Gaussians to represent the
human body, enabling faster training and rendering. However, they undermine the
importance of the mesh guidance and directly predict Gaussians in 3D space with
coarse mesh guidance. This hinders the learning procedure of the Gaussians and
tends to produce blurry textures. Therefore, we propose UV Gaussians, which
models the 3D human body by jointly learning mesh deformations and 2D UV-space
Gaussian textures. We utilize the embedding of UV map to learn Gaussian
textures in 2D space, leveraging the capabilities of powerful 2D networks to
extract features. Additionally, through an independent Mesh network, we
optimize pose-dependent geometric deformations, thereby guiding Gaussian
rendering and significantly enhancing rendering quality. We collect and process
a new dataset of human motion, which includes multi-view images, scanned
models, parametric model registration, and corresponding texture maps.
Experimental results demonstrate that our method achieves state-of-the-art
synthesis of novel view and novel pose. The code and data will be made
available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the
paper is accepted.



---

## NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using  3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie | cs.CV | [PDF](http://arxiv.org/pdf/2403.11679v1){: .btn .btn-green } |

**Abstract**: We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D
Gaussian representation, that enables robust 3D semantic mapping, accurate
camera tracking, and high-quality rendering in real-time. In the system, we
propose a Spatially Consistent Feature Fusion model to reduce the effect of
erroneous estimates from pre-trained segmentation head on semantic
reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we
employ a lightweight encoder-decoder to compress the high-dimensional semantic
features into a compact 3D Gaussian representation, mitigating the burden of
excessive memory consumption. Furthermore, we leverage the advantage of 3D
Gaussian splatting, which enables efficient and differentiable novel view
rendering, and propose a Virtual Camera View Pruning method to eliminate
outlier GS points, thereby effectively enhancing the quality of scene
representations. Our NEDS-SLAM method demonstrates competitive performance over
existing dense semantic SLAM methods in terms of mapping and tracking accuracy
on Replica and ScanNet datasets, while also showing excellent capabilities in
3D dense semantic mapping.



---

## Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Teppei Suzuki | cs.CV | [PDF](http://arxiv.org/pdf/2403.11460v1){: .btn .btn-green } |

**Abstract**: In this work, we present Fed3DGS, a scalable 3D reconstruction framework
based on 3D Gaussian splatting (3DGS) with federated learning. Existing
city-scale reconstruction methods typically adopt a centralized approach, which
gathers all data in a central server and reconstructs scenes. The approach
hampers scalability because it places a heavy load on the server and demands
extensive data storage when reconstructing scenes on a scale beyond city-scale.
In pursuit of a more scalable 3D reconstruction, we propose a federated
learning framework with 3DGS, which is a decentralized framework and can
potentially use distributed computational resources across millions of clients.
We tailor a distillation-based model update scheme for 3DGS and introduce
appearance modeling for handling non-IID data in the scenario of 3D
reconstruction with federated learning. We simulate our method on several
large-scale benchmarks, and our method demonstrates rendered image quality
comparable to centralized approaches. In addition, we also simulate our method
with data collected in different seasons, demonstrating that our framework can
reflect changes in the scenes and our appearance modeling captures changes due
to seasonal variations.

Comments:
- Code: https://github.com/DensoITLab/Fed3DGS

---

## View-Consistent 3D Editing with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2403.11868v2){: .btn .btn-green } |

**Abstract**: The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,
offering efficient, high-fidelity rendering and enabling precise local
manipulations. Currently, diffusion-based 2D editing models are harnessed to
modify multi-view rendered images, which then guide the editing of 3DGS models.
However, this approach faces a critical issue of multi-view inconsistency,
where the guidance images exhibit significant discrepancies across views,
leading to mode collapse and visual artifacts of 3DGS. To this end, we
introduce View-consistent Editing (VcEdit), a novel framework that seamlessly
incorporates 3DGS into image editing processes, ensuring multi-view consistency
in edited guidance images and effectively mitigating mode collapse issues.
VcEdit employs two innovative consistency modules: the Cross-attention
Consistency Module and the Editing Consistency Module, both designed to reduce
inconsistencies in edited images. By incorporating these consistency modules
into an iterative pattern, VcEdit proficiently resolves the issue of multi-view
inconsistency, facilitating high-quality 3DGS editing across a diverse range of
scenes.



---

## GaussNav: Gaussian Splatting for Visual Navigation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.11625v2){: .btn .btn-green } |

**Abstract**: In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to
locate a specific object depicted in a goal image within an unexplored
environment. The primary difficulty of IIN stems from the necessity of
recognizing the target object across varying viewpoints and rejecting potential
distractors.
  Existing map-based navigation methods largely adopt the representation form
of Bird's Eye View (BEV) maps, which, however, lack the representation of
detailed textures in a scene.
  To address the above issues, we propose a new Gaussian Splatting Navigation
(abbreviated as GaussNav) framework for IIN task, which constructs a novel map
representation based on 3D Gaussian Splatting (3DGS).
  The proposed framework enables the agent to not only memorize the geometry
and semantic information of the scene, but also retain the textural features of
objects.
  Our GaussNav framework demonstrates a significant leap in performance,
evidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to
0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset.
  Our code will be made publicly available.

Comments:
- conference

---

## Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for  Resolving Class-imbalance Problem

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-18 | Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2403.11573v2){: .btn .btn-green } |

**Abstract**: Typical LiDAR-based 3D object detection models are trained in a supervised
manner with real-world data collection, which is often imbalanced over classes
(or long-tailed). To deal with it, augmenting minority-class examples by
sampling ground truth (GT) LiDAR points from a database and pasting them into a
scene of interest is often used, but challenges still remain: inflexibility in
locating GT samples and limited sample diversity. In this work, we propose to
leverage pseudo-LiDAR point clouds generated (at a low cost) from videos
capturing a surround view of miniatures or real-world objects of minor classes.
Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of
three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D
view synthesis model, (ii) object-level domain alignment with LiDAR intensity
estimation and (iii) a hybrid context-aware placement method from ground and
map information. We demonstrate the superiority and generality of our method
through performance improvements in extensive experiments conducted on three
popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the
datasets with large domain gaps captured by different LiDAR configurations. Our
code and data will be publicly available upon publication.

Comments:
- 28 pages, 12 figures, 11 tables

---

## 3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual  ReLocalization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-17 | Peng Jiang, Gaurav Pandey, Srikanth Saripalli | cs.CV | [PDF](http://arxiv.org/pdf/2403.11367v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel system designed for 3D mapping and visual
relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and
camera data to create accurate and visually plausible representations of the
environment. By leveraging LiDAR data to initiate the training of the 3D
Gaussian Splatting map, our system constructs maps that are both detailed and
geometrically accurate. To mitigate excessive GPU memory usage and facilitate
rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.
This preparation makes our method well-suited for visual localization tasks,
enabling efficient identification of correspondences between the query image
and the rendered image from the Gaussian Splatting map via normalized
cross-correlation (NCC). Additionally, we refine the camera pose of the query
image using feature-based matching and the Perspective-n-Point (PnP) technique.
The effectiveness, adaptability, and precision of our system are demonstrated
through extensive evaluation on the KITTI360 dataset.

Comments:
- 8 pages, 7 figures

---

## Creating Seamless 3D Maps Using Radiance Fields

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-17 | Sai Tarun Sathyan, Thomas B. Kinsman | cs.CV | [PDF](http://arxiv.org/pdf/2403.11364v1){: .btn .btn-green } |

**Abstract**: It is desirable to create 3D object models and 3D maps from 2D input images
for applications such as navigation, virtual tourism, and urban planning. The
traditional methods of creating 3D maps, (such as photogrammetry), require a
large number of images and odometry. Additionally, traditional methods have
difficulty with reflective surfaces and specular reflections; windows and
chrome in the scene can be problematic. Google Road View is a familiar
application, which uses traditional methods to fuse a collection of 2D input
images into the illusion of a 3D map. However, Google Road View does not create
an actual 3D object model, only a collection of views. The objective of this
work is to create an actual 3D object model using updated techniques. Neural
Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the
capability to produce more precise and intricate 3D maps. Gaussian Splatting[4]
is another contemporary technique. This investigation compares Neural Radiance
Fields to Gaussian Splatting, and describes some of their inner workings. Our
primary contribution is a method for improving the results of the 3D
reconstructed models. Our results indicate that Gaussian Splatting was superior
to the NeRF technique.

Comments:
- 10 pages with figures

---

## Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive  Experience

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Xiaohang Yu, Zhengxian Yang, Shi Pan, Yuqi Han, Haoxiang Wang, Jun Zhang, Shi Yan, Borong Lin, Lei Yang, Tao Yu, Lu Fang | cs.CV | [PDF](http://arxiv.org/pdf/2403.09973v1){: .btn .btn-green } |

**Abstract**: We have built a custom mobile multi-camera large-space dense light field
capture system, which provides a series of high-quality and sufficiently dense
light field images for various scenarios. Our aim is to contribute to the
development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF,
and 3D Gaussian splitting. More importantly, the collected dataset, which is
much denser than existing datasets, may also inspire space-oriented light field
reconstruction, which is potentially different from object-centric 3D
reconstruction, for immersive VR/AR experiences. We utilized a total of 40
GoPro 10 cameras, capturing images of 5k resolution. The number of photos
captured for each scene is no less than 1000, and the average density (view
number within a unit sphere) is 134.68. It is also worth noting that our system
is capable of efficiently capturing large outdoor scenes. Addressing the
current lack of large-space and dense light field datasets, we made efforts to
include elements such as sky, reflections, lights and shadows that are of
interest to researchers in the field of 3D reconstruction during the data
capture process. Finally, we validated the effectiveness of our provided
dataset on three popular algorithms and also integrated the reconstructed 3DGS
results into the Unity engine, demonstrating the potential of utilizing our
datasets to enhance the realism of virtual reality (VR) and create feasible
interactive spaces. The dataset is available at our project website.



---

## URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Li | cs.CV | [PDF](http://arxiv.org/pdf/2403.10119v1){: .btn .btn-green } |

**Abstract**: We propose a novel rolling shutter bundle adjustment method for neural
radiance fields (NeRF), which utilizes the unordered rolling shutter (RS)
images to obtain the implicit 3D representation. Existing NeRF methods suffer
from low-quality images and inaccurate initial camera poses due to the RS
effect in the image, whereas, the previous method that incorporates the RS into
NeRF requires strict sequential data input, limiting its widespread
applicability. In constant, our method recovers the physical formation of RS
images by estimating camera poses and velocities, thereby removing the input
constraints on sequential data. Moreover, we adopt a coarse-to-fine training
strategy, in which the RS epipolar constraints of the pairwise frames in the
scene graph are used to detect the camera poses that fall into local minima.
The poses detected as outliers are corrected by the interpolation method with
neighboring poses. The experimental results validate the effectiveness of our
method over state-of-the-art works and demonstrate that the reconstruction of
3D representations is not constrained by the requirement of video sequence
input.



---

## Controllable Text-to-3D Generation via Surface-Aligned Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2403.09981v1){: .btn .btn-green } |

**Abstract**: While text-to-3D and image-to-3D generation tasks have received considerable
attention, one important but under-explored field between them is controllable
text-to-3D generation, which we mainly focus on in this work. To address this
task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network
architecture designed to enhance existing pre-trained multi-view diffusion
models by integrating additional input conditions, such as edge, depth, normal,
and scribble maps. Our innovation lies in the introduction of a conditioning
module that controls the base diffusion model using both local and global
embeddings, which are computed from the input condition images and camera
poses. Once trained, MVControl is able to offer 3D diffusion guidance for
optimization-based 3D generation. And, 2) we propose an efficient multi-stage
3D generation pipeline that leverages the benefits of recent large
reconstruction models and score distillation algorithm. Building upon our
MVControl architecture, we employ a unique hybrid diffusion guidance method to
direct the optimization process. In pursuit of efficiency, we adopt 3D
Gaussians as our representation instead of the commonly used implicit
representations. We also pioneer the use of SuGaR, a hybrid representation that
binds Gaussians to mesh triangle faces. This approach alleviates the issue of
poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained
geometry on the mesh. Extensive experiments demonstrate that our method
achieves robust generalization and enables the controllable generation of
high-quality 3D content.

Comments:
- Project page: https://lizhiqi49.github.io/MVControl/

---

## Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian  Splatting Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Tian-Xing Xu, Wenbo Hu, Yu-Kun Lai, Ying Shan, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2403.10050v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting, emerging as a groundbreaking approach, has drawn
increasing attention for its capabilities of high-fidelity reconstruction and
real-time rendering. However, it couples the appearance and geometry of the
scene within the Gaussian attributes, which hinders the flexibility of editing
operations, such as texture swapping. To address this issue, we propose a novel
approach, namely Texture-GS, to disentangle the appearance from the geometry by
representing it as a 2D texture mapped onto the 3D surface, thereby
facilitating appearance editing. Technically, the disentanglement is achieved
by our proposed texture mapping module, which consists of a UV mapping MLP to
learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion
of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian
intersections, and a learnable texture to capture the fine-grained appearance.
Extensive experiments on the DTU dataset demonstrate that our method not only
facilitates high-fidelity appearance editing but also achieves real-time
rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.



---

## DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Huiqiang Sun, Xingyi Li, Liao Shen, Xinyi Ye, Ke Xian, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2403.10103v1){: .btn .btn-green } |

**Abstract**: Recent advancements in dynamic neural radiance field methods have yielded
remarkable outcomes. However, these approaches rely on the assumption of sharp
input images. When faced with motion blur, existing dynamic NeRF methods often
struggle to generate high-quality novel views. In this paper, we propose
DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views
from a monocular video affected by motion blur. To account for motion blur in
input images, we simultaneously capture the camera trajectory and object
Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we
employ a global cross-time rendering approach to ensure consistent temporal
coherence across the entire scene. We curate a dataset comprising diverse
dynamic scenes that are specifically tailored for our task. Experimental
results on our dataset demonstrate that our method outperforms existing
approaches in generating sharp novel views from motion-blurred inputs while
maintaining spatial-temporal consistency of the scene.



---

## FeatUp: A Model-Agnostic Framework for Features at Any Resolution

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, William T. Freeman | cs.CV | [PDF](http://arxiv.org/pdf/2403.10516v1){: .btn .btn-green } |

**Abstract**: Deep features are a cornerstone of computer vision research, capturing image
semantics and enabling the community to solve downstream tasks even in the
zero- or few-shot regime. However, these features often lack the spatial
resolution to directly perform dense prediction tasks like segmentation and
depth prediction because models aggressively pool information over large areas.
In this work, we introduce FeatUp, a task- and model-agnostic framework to
restore lost spatial information in deep features. We introduce two variants of
FeatUp: one that guides features with high-resolution signal in a single
forward pass, and one that fits an implicit model to a single image to
reconstruct features at any resolution. Both approaches use a multi-view
consistency loss with deep analogies to NeRFs. Our features retain their
original semantics and can be swapped into existing applications to yield
resolution and performance gains even without re-training. We show that FeatUp
significantly outperforms other feature upsampling and image super-resolution
approaches in class activation map generation, transfer learning for
segmentation and depth prediction, and end-to-end training for semantic
segmentation.

Comments:
- Accepted to the International Conference on Learning Representations
  (ICLR) 2024

---

## GGRt: Towards Generalizable 3D Gaussians without Pose Priors in  Real-Time

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han | cs.CV | [PDF](http://arxiv.org/pdf/2403.10147v1){: .btn .btn-green } |

**Abstract**: This paper presents GGRt, a novel approach to generalizable novel view
synthesis that alleviates the need for real camera poses, complexity in
processing high-resolution images, and lengthy optimization processes, thus
facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in
real-world scenarios. Specifically, we design a novel joint learning framework
that consists of an Iterative Pose Optimization Network (IPO-Net) and a
Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,
the proposed framework can inherently estimate robust relative pose information
from the image observations and thus primarily alleviate the requirement of
real camera poses. Moreover, we implement a deferred back-propagation mechanism
that enables high-resolution training and inference, overcoming the resolution
constraints of previous methods. To enhance the speed and efficiency, we
further introduce a progressive Gaussian cache module that dynamically adjusts
during training and inference. As the first pose-free generalizable 3D-GS
framework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at
$\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our
method outperforms existing NeRF-based pose-free techniques in terms of
inference speed and effectiveness. It can also approach the real pose-based
3D-GS methods. Our contributions provide a significant leap forward for the
integration of computer vision and computer graphics into practical
applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open
datasets and enabling real-time rendering for immersive experiences.



---

## Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints  Scene Coordinate Regression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Huy-Hoang Bui, Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee | cs.CV | [PDF](http://arxiv.org/pdf/2403.10297v1){: .btn .btn-green } |

**Abstract**: Classical structural-based visual localization methods offer high accuracy
but face trade-offs in terms of storage, speed, and privacy. A recent
innovation, keypoint scene coordinate regression (KSCR) named D2S addresses
these issues by leveraging graph attention networks to enhance keypoint
relationships and predict their 3D coordinates using a simple multilayer
perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using
established 2D-3D correspondences. While KSCR achieves competitive results,
rivaling state-of-the-art image-retrieval methods like HLoc across multiple
benchmarks, its performance is hindered when data samples are limited due to
the deep learning model's reliance on extensive data. This paper proposes a
solution to this challenge by introducing a pipeline for keypoint descriptor
synthesis using Neural Radiance Field (NeRF). By generating novel poses and
feeding them into a trained NeRF model to create new views, our approach
enhances the KSCR's generalization capabilities in data-scarce environments.
The proposed system could significantly improve localization accuracy by up to
50\% and cost only a fraction of time for data synthesis. Furthermore, its
modular design allows for the integration of multiple NeRFs, offering a
versatile and efficient solution for visual localization. The implementation is
publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.



---

## SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou | cs.CV | [PDF](http://arxiv.org/pdf/2403.10427v1){: .btn .btn-green } |

**Abstract**: Implicit neural representation methods have shown impressive advancements in
learning 3D scenes from unstructured in-the-wild photo collections but are
still limited by the large computational cost of volumetric rendering. More
recently, 3D Gaussian Splatting emerged as a much faster alternative with
superior rendering quality and training efficiency, especially for small-scale
and object-centric scenarios. Nevertheless, this technique suffers from poor
performance on unstructured in-the-wild data. To tackle this, we extend over 3D
Gaussian Splatting to handle unstructured image collections. We achieve this by
modeling appearance to seize photometric variations in the rendered images.
Additionally, we introduce a new mechanism to train transient Gaussians to
handle the presence of scene occluders in an unsupervised manner. Experiments
on diverse photo collection scenes and multi-pass acquisition of outdoor
landmarks show the effectiveness of our method over prior works achieving
state-of-the-art results with improved efficiency.



---

## Thermal-NeRF: Neural Radiance Fields from an Infrared Camera

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Tianxiang Ye, Qi Wu, Junyuan Deng, Guoqing Liu, Liu Liu, Songpengcheng Xia, Liang Pang, Wenxian Yu, Ling Pei | cs.CV | [PDF](http://arxiv.org/pdf/2403.10340v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant
potential in encoding highly-detailed 3D geometry and environmental appearance,
positioning themselves as a promising alternative to traditional explicit
representation for 3D scene reconstruction. However, the predominant reliance
on RGB imaging presupposes ideal lighting conditions: a premise frequently
unmet in robotic applications plagued by poor lighting or visual obstructions.
This limitation overlooks the capabilities of infrared (IR) cameras, which
excel in low-light detection and present a robust alternative under such
adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first
method that estimates a volumetric scene representation in the form of a NeRF
solely from IR imaging. By leveraging a thermal mapping and structural thermal
constraint derived from the thermal characteristics of IR imaging, our method
showcasing unparalleled proficiency in recovering NeRFs in visually degraded
scenes where RGB-based methods fall short. We conduct extensive experiments to
demonstrate that Thermal-NeRF can achieve superior quality compared to existing
methods. Furthermore, we contribute a dataset for IR-based NeRF applications,
paving the way for future research in IR NeRF reconstruction.



---

## FDGaussian: Fast Gaussian Splatting from Single Image via  Geometric-aware Diffusion Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-15 | Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2403.10242v1){: .btn .btn-green } |

**Abstract**: Reconstructing detailed 3D objects from single-view images remains a
challenging task due to the limited information available. In this paper, we
introduce FDGaussian, a novel two-stage framework for single-image 3D
reconstruction. Recent methods typically utilize pre-trained 2D diffusion
models to generate plausible novel views from the input image, yet they
encounter issues with either multi-view inconsistency or lack of geometric
fidelity. To overcome these challenges, we propose an orthogonal plane
decomposition mechanism to extract 3D geometric features from the 2D input,
enabling the generation of consistent multi-view images. Moreover, we further
accelerate the state-of-the-art Gaussian Splatting incorporating epipolar
attention to fuse images from different viewpoints. We demonstrate that
FDGaussian generates images with high consistency across different views and
reconstructs high-quality 3D objects, both qualitatively and quantitatively.
More examples can be found at our website https://qjfeng.net/FDGaussian/.



---

## GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary  Robotic Grasping

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.09637v1){: .btn .btn-green } |

**Abstract**: Constructing a 3D scene capable of accommodating open-ended language queries,
is a pivotal pursuit, particularly within the domain of robotics. Such
technology facilitates robots in executing object manipulations based on human
language directives. To tackle this challenge, some research efforts have been
dedicated to the development of language-embedded implicit fields. However,
implicit fields (e.g. NeRF) encounter limitations due to the necessity of
processing a large number of input views for reconstruction, coupled with their
inherent inefficiencies in inference. Thus, we present the GaussianGrasper,
which utilizes 3D Gaussian Splatting to explicitly represent the scene as a
collection of Gaussian primitives. Our approach takes a limited set of RGB-D
views and employs a tile-based splatting technique to create a feature field.
In particular, we propose an Efficient Feature Distillation (EFD) module that
employs contrastive learning to efficiently and accurately distill language
embeddings derived from foundational models. With the reconstructed geometry of
the Gaussian field, our method enables the pre-trained grasping model to
generate collision-free grasp pose candidates. Furthermore, we propose a
normal-guided grasp module to select the best grasp pose. Through comprehensive
real-world experiments, we demonstrate that GaussianGrasper enables robots to
accurately query and grasp objects with language instructions, providing a new
solution for language-guided manipulation tasks. Data and codes can be
available at https://github.com/MrSecant/GaussianGrasper.



---

## 3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Frank Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, Changqing Zou | cs.CV | [PDF](http://arxiv.org/pdf/2403.09439v1){: .btn .btn-green } |

**Abstract**: Text-driven 3D scene generation techniques have made rapid progress in recent
years. Their success is mainly attributed to using existing generative models
to iteratively perform image warping and inpainting to generate 3D scenes.
However, these methods heavily rely on the outputs of existing models, leading
to error accumulation in geometry and appearance that prevent the models from
being used in various scenarios (e.g., outdoor and unreal scenarios). To
address this limitation, we generatively refine the newly generated local views
by querying and aggregating global 3D information, and then progressively
generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF
as a unified representation of the 3D scene to constrain global 3D consistency,
and propose a generative refinement network to synthesize new contents with
higher quality by exploiting the natural image prior from 2D diffusion model as
well as the global 3D information of the current scene. Our extensive
experiments demonstrate that, in comparison to previous methods, our approach
supports wide variety of scene generation and arbitrary camera trajectories
with improved visual quality and 3D consistency.

Comments:
- 11 pages, 7 figures

---

## RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban  Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Thang-Anh-Quan Nguyen, Luis Roldão, Nathan Piasco, Moussab Bennehar, Dzmitry Tsishkou | cs.CV | [PDF](http://arxiv.org/pdf/2403.09419v1){: .btn .btn-green } |

**Abstract**: The task of separating dynamic objects from static environments using NeRFs
has been widely studied in recent years. However, capturing large-scale scenes
still poses a challenge due to their complex geometric structures and
unconstrained dynamics. Without the help of 3D motion cues, previous methods
often require simplified setups with slow camera motion and only a few/single
dynamic actors, leading to suboptimal solutions in most urban setups. To
overcome such limitations, we present RoDUS, a pipeline for decomposing static
and dynamic elements in urban scenes, with thoughtfully separated NeRF models
for moving and non-moving components. Our approach utilizes a robust
kernel-based initialization coupled with 4D semantic information to selectively
guide the learning process. This strategy enables accurate capturing of the
dynamics in the scene, resulting in reduced artifacts caused by NeRF on
background reconstruction, all by using self-supervision. Notably, experimental
evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of
our method in decomposing challenging urban scenes into precise static and
dynamic components.



---

## PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF  Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2403.09079v1){: .btn .btn-green } |

**Abstract**: Autonomous vehicles rely extensively on perception systems to navigate and
interpret their surroundings. Despite significant advancements in these systems
recently, challenges persist under conditions like occlusion, extreme lighting,
or in unfamiliar urban areas. Unlike these systems, humans do not solely depend
on immediate observations to perceive the environment. In navigating new
cities, humans gradually develop a preliminary mental map to supplement
real-time perception during subsequent visits. Inspired by this human approach,
we introduce a novel framework, Pre-Sight, that leverages past traversals to
construct static prior memories, enhancing online perception in later
navigations. Our method involves optimizing a city-scale neural radiance field
with data from previous journeys to generate neural priors. These priors, rich
in semantic and geometric details, are derived without manual annotations and
can seamlessly augment various state-of-the-art perception models, improving
their efficacy with minimal additional computational cost. Experimental results
on the nuScenes dataset demonstrate the framework's high compatibility with
diverse online perception models. Specifically, it shows remarkable
improvements in HD-map construction and occupancy prediction tasks,
highlighting its potential as a new perception framework for autonomous driving
systems. Our code will be released at
https://github.com/yuantianyuan01/PreSight.



---

## Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III | cs.RO | [PDF](http://arxiv.org/pdf/2403.09875v1){: .btn .btn-green } |

**Abstract**: In this work, we propose a novel method to supervise 3D Gaussian Splatting
(3DGS) scenes using optical tactile sensors. Optical tactile sensors have
become widespread in their use in robotics for manipulation and object
representation; however, raw optical tactile sensor data is unsuitable to
directly supervise a 3DGS scene. Our representation leverages a Gaussian
Process Implicit Surface to implicitly represent the object, combining many
touches into a unified representation with uncertainty. We merge this model
with a monocular depth estimation network, which is aligned in a two stage
process, coarsely aligning with a depth camera and then finely adjusting to
match our touch data. For every training image, our method produces a
corresponding fused depth and uncertainty map. Utilizing this additional
information, we propose a new loss function, variance weighted depth supervised
loss, for training the 3DGS scene model. We leverage the DenseTact optical
tactile sensor and RealSense RGB-D camera to show that combining touch and
vision in this manner leads to quantitatively and qualitatively better results
than vision or touch alone in a few-view scene syntheses on opaque as well as
on reflective and transparent objects. Please see our project page at
http://armlabstanford.github.io/touch-gs



---

## The NeRFect Match: Exploring NeRF Features for Visual Localization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé | cs.CV | [PDF](http://arxiv.org/pdf/2403.09577v1){: .btn .btn-green } |

**Abstract**: In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene
representation for visual localization. Recently, NeRF has been employed to
enhance pose regression and scene coordinate regression models by augmenting
the training database, providing auxiliary supervision through rendered images,
or serving as an iterative refinement module. We extend its recognized
advantages -- its ability to provide a compact scene representation with
realistic appearances and accurate geometry -- by exploring the potential of
NeRF's internal features in establishing precise 2D-3D matches for
localization. To this end, we conduct a comprehensive examination of NeRF's
implicit knowledge, acquired through view synthesis, for matching under various
conditions. This includes exploring different matching network architectures,
extracting encoder features at multiple layers, and varying training
configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D
matching function that capitalizes on the internal knowledge of NeRF learned
via view synthesis. Our evaluation of NeRFMatch on standard localization
benchmarks, within a structure-based pipeline, sets a new state-of-the-art for
localization performance on Cambridge Landmarks.



---

## A New Split Algorithm for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu | cs.GR | [PDF](http://arxiv.org/pdf/2403.09143v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting models, as a novel explicit 3D representation, have
been applied in many domains recently, such as explicit geometric editing and
geometry generation. Progress has been rapid. However, due to their mixed
scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred
or needle-like effect near the surface. At the same time, 3D Gaussian splatting
models tend to flatten large untextured regions, yielding a very sparse point
cloud. These problems are caused by the non-uniform nature of 3D Gaussian
splatting models, so in this paper, we propose a new 3D Gaussian splitting
algorithm, which can produce a more uniform and surface-bounded 3D Gaussian
splatting model. Our algorithm splits an $N$-dimensional Gaussian into two
N-dimensional Gaussians. It ensures consistency of mathematical characteristics
and similarity of appearance, allowing resulting 3D Gaussian splatting models
to be more uniform and a better fit to the underlying surface, and thus more
suitable for explicit editing, point cloud extraction and other tasks.
Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form
solution, making it readily applicable to any 3D Gaussian model.

Comments:
- 11 pages, 10 figures

---

## Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2403.09413v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has recently demonstrated impressive
capabilities in real-time novel view synthesis and 3D reconstruction. However,
3DGS heavily depends on the accurate initialization derived from
Structure-from-Motion (SfM) methods. When trained with randomly initialized
point clouds, 3DGS fails to maintain its ability to produce high-quality
images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive
analysis of SfM initialization in the frequency domain and analysis of a 1D
regression task with multiple 1D Gaussians, we propose a novel optimization
strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D
Gaussian Splatting), that successfully trains 3D Gaussians from random point
clouds. We show the effectiveness of our strategy through quantitative and
qualitative comparisons on multiple datasets, largely improving the performance
in all settings. Our project page and code can be found at
https://ku-cvlab.github.io/RAIN-GS.

Comments:
- Project Page: https://ku-cvlab.github.io/RAIN-GS

---

## VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-14 | Nicolaj Schmid, Cornelius von Einem, Cesar Cadena, Roland Siegwart, Lorenz Hruby, Florian Tschopp | cs.RO | [PDF](http://arxiv.org/pdf/2403.09477v1){: .btn .btn-green } |

**Abstract**: Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.



---

## GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting  Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu | cs.CV | [PDF](http://arxiv.org/pdf/2403.08733v2){: .btn .btn-green } |

**Abstract**: We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed
by the 3D Gaussian Splatting (3DGS).
  Our method first renders a collection of images by using the 3DGS and edits
them by using a pre-trained 2D diffusion model (ControlNet) based on the input
prompt, which is then used to optimise the 3D model.
  Our key contribution is multi-view consistent editing, which enables editing
all images together instead of iteratively editing one image while updating the
3D model as in previous works.
  It leads to faster editing as well as higher visual quality.
  This is achieved by the two terms:
  (a) depth-conditioned editing that enforces geometric consistency across
multi-view images by leveraging naturally consistent depth maps.
  (b) attention-based latent code alignment that unifies the appearance of
edited images by conditioning their editing to several reference views through
self and cross-view attention between images' latent representations.
  Experiments demonstrate that our method achieves faster editing and better
visual results than previous state-of-the-art methods.

Comments:
- 17 pages

---

## StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Hongbin Xu, Weitao Chen, Feng Xiao, Baigui Sun, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2403.08310v1){: .btn .btn-green } |

**Abstract**: 4D style transfer aims at transferring arbitrary visual style to the
synthesized novel views of a dynamic 4D scene with varying viewpoints and
times. Existing efforts on 3D style transfer can effectively combine the visual
features of style images and neural radiance fields (NeRF) but fail to handle
the 4D dynamic scenes limited by the static scene assumption. Consequently, we
aim to handle the novel challenging problem of 4D style transfer for the first
time, which further requires the consistency of stylized results on dynamic
objects. In this paper, we introduce StyleDyRF, a method that represents the 4D
feature space by deforming a canonical feature volume and learns a linear style
transformation matrix on the feature volume in a data-driven fashion. To obtain
the canonical feature volume, the rays at each time step are deformed with the
geometric prior of a pre-trained dynamic NeRF to render the feature map under
the supervision of pre-trained visual encoders. With the content and style cues
in the canonical feature volume and the style image, we can learn the style
transformation matrix from their covariance matrices with lightweight neural
networks. The learned style transformation matrix can reflect a direct matching
of feature covariance from the content volume to the given style pattern, in
analogy with the optimization of the Gram matrix in traditional 2D neural style
transfer. The experimental results show that our method not only renders 4D
photorealistic style transfer results in a zero-shot manner but also
outperforms existing methods in terms of visual quality and consistency.

Comments:
- In submission. The code and model are released at:
  https://github.com/ToughStoneX/StyleDyRF

---

## NeRF-Supervised Feature Point Detection and Description

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Ali Youssef, Francisco Vasconcelos | cs.CV | [PDF](http://arxiv.org/pdf/2403.08156v1){: .btn .btn-green } |

**Abstract**: Feature point detection and description is the backbone for various computer
vision applications, such as Structure-from-Motion, visual SLAM, and visual
place recognition. While learning-based methods have surpassed traditional
handcrafted techniques, their training often relies on simplistic
homography-based simulations of multi-view perspectives, limiting model
generalisability. This paper introduces a novel approach leveraging neural
radiance fields (NeRFs) for realistic multi-view training data generation. We
create a diverse multi-view dataset using NeRFs, consisting of indoor and
outdoor scenes. Our proposed methodology adapts state-of-the-art feature
detectors and descriptors to train on NeRF-synthesised views supervised by
perspective projective geometry. Our experiments demonstrate that the proposed
methods achieve competitive or superior performance on standard benchmarks for
relative pose estimation, point cloud registration, and homography estimation
while requiring significantly less training data compared to existing
approaches.



---

## GaussianImage: 1000 FPS Image Representation and Compression by 2D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, Jun Zhang | eess.IV | [PDF](http://arxiv.org/pdf/2403.08551v2){: .btn .btn-green } |

**Abstract**: Implicit neural representations (INRs) recently achieved great success in
image representation and compression, offering high visual quality and fast
rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are
available. However, this requirement often hinders their use on low-end devices
with limited memory. In response, we propose a groundbreaking paradigm of image
representation and compression by 2D Gaussian Splatting, named GaussianImage.
We first introduce 2D Gaussian to represent the image, where each Gaussian has
8 parameters including position, covariance and color. Subsequently, we unveil
a novel rendering algorithm based on accumulated summation. Remarkably, our
method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster
fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation
performance, but also delivers a faster rendering speed of 1500-2000 FPS
regardless of parameter size. Furthermore, we integrate existing vector
quantization technique to build an image codec. Experimental results
demonstrate that our codec attains rate-distortion performance comparable to
compression-based INRs such as COIN and COIN++, while facilitating decoding
speeds of approximately 1000 FPS. Additionally, preliminary proof of concept
shows that our codec surpasses COIN and COIN++ in performance when using
partial bits-back coding.



---

## ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic  Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang | cs.RO | [PDF](http://arxiv.org/pdf/2403.08321v1){: .btn .btn-green } |

**Abstract**: Performing language-conditioned robotic manipulation tasks in unstructured
environments is highly demanded for general intelligent robots. Conventional
robotic manipulation methods usually learn semantic representation of the
observation for action prediction, which ignores the scene-level spatiotemporal
dynamics for human goal completion. In this paper, we propose a dynamic
Gaussian Splatting method named ManiGaussian for multi-task robotic
manipulation, which mines scene dynamics via future scene reconstruction.
Specifically, we first formulate the dynamic Gaussian Splatting framework that
infers the semantics propagation in the Gaussian embedding space, where the
semantic representation is leveraged to predict the optimal robot action. Then,
we build a Gaussian world model to parameterize the distribution in our dynamic
Gaussian Splatting framework, which provides informative supervision in the
interactive environment via future scene reconstruction. We evaluate our
ManiGaussian on 10 RLBench tasks with 166 variations, and the results
demonstrate our framework can outperform the state-of-the-art methods by 13.1\%
in average success rate.



---

## Gaussian Splatting in Style

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-13 | Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2403.08498v1){: .btn .btn-green } |

**Abstract**: Scene stylization extends the work of neural style transfer to three spatial
dimensions. A vital challenge in this problem is to maintain the uniformity of
the stylized appearance across a multi-view setting. A vast majority of the
previous works achieve this by optimizing the scene with a specific style
image. In contrast, we propose a novel architecture trained on a collection of
style images, that at test time produces high quality stylized novel views. Our
work builds up on the framework of 3D Gaussian splatting. For a given scene, we
take the pretrained Gaussians and process them using a multi resolution hash
grid and a tiny MLP to obtain the conditional stylised views. The explicit
nature of 3D Gaussians give us inherent advantages over NeRF-based methods
including geometric consistency, along with having a fast training and
rendering regime. This enables our method to be useful for vast practical use
cases such as in augmented or virtual reality applications. Through our
experiments, we show our methods achieve state-of-the-art performance with
superior visual quality on various indoor and outdoor real-world data.



---

## StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2403.07807v1){: .btn .btn-green } |

**Abstract**: We introduce StyleGaussian, a novel 3D style transfer technique that allows
instant transfer of any image's style to a 3D scene at 10 frames per second
(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style
transfer without compromising its real-time rendering ability and multi-view
consistency. It achieves instant style transfer with three steps: embedding,
transfer, and decoding. Initially, 2D VGG scene features are embedded into
reconstructed 3D Gaussians. Next, the embedded features are transformed
according to a reference style image. Finally, the transformed features are
decoded into the stylized RGB. StyleGaussian has two novel designs. The first
is an efficient feature rendering strategy that first renders low-dimensional
features and then maps them into high-dimensional features while embedding VGG
features. It cuts the memory consumption significantly and enables 3DGS to
render the high-dimensional memory-intensive features. The second is a
K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized
features, it eliminates the 2D CNN operations that compromise strict multi-view
consistency. Extensive experiments show that StyleGaussian achieves instant 3D
stylization with superior stylization quality while preserving real-time
rendering and strict multi-view consistency. Project page:
https://kunhao-liu.github.io/StyleGaussian/



---

## Q-SLAM: Quadric Representations for Monocular SLAM

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Chensheng Peng, Chenfeng Xu, Yue Wang, Mingyu Ding, Heng Yang, Masayoshi Tomizuka, Kurt Keutzer, Marco Pavone, Wei Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2403.08125v1){: .btn .btn-green } |

**Abstract**: Monocular SLAM has long grappled with the challenge of accurately modeling 3D
geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular
SLAM have shown promise, yet these methods typically focus on novel view
synthesis rather than precise 3D geometry modeling. This focus results in a
significant disconnect between NeRF applications, i.e., novel-view synthesis
and the requirements of SLAM. We identify that the gap results from the
volumetric representations used in NeRF, which are often dense and noisy. In
this study, we propose a novel approach that reimagines volumetric
representations through the lens of quadric forms. We posit that most scene
components can be effectively represented as quadric planes. Leveraging this
assumption, we reshape the volumetric representations with million of cubes by
several quadric planes, which leads to more accurate and efficient modeling of
3D scenes in SLAM contexts. Our method involves two key steps: First, we use
the quadric assumption to enhance coarse depth estimations obtained from
tracking modules, e.g., Droid-SLAM. This step alone significantly improves
depth estimation accuracy. Second, in the subsequent mapping phase, we diverge
from previous NeRF-based SLAM methods that distribute sampling points across
the entire volume space. Instead, we concentrate sampling points around quadric
planes and aggregate them using a novel quadric-decomposed Transformer.
Additionally, we introduce an end-to-end joint optimization strategy that
synchronizes pose estimation with 3D reconstruction.



---

## SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2403.07547v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) has attracted considerable attention for their
exceptional ability in synthesizing novel views with high fidelity. However,
the presence of motion blur, resulting from slight camera movements during
extended shutter exposures, poses a significant challenge, potentially
compromising the quality of the reconstructed 3D scenes. While recent studies
have addressed this issue, they do not consider the continuous dynamics of
camera movements during image acquisition, leading to inaccurate scene
reconstruction. Additionally, these methods are plagued by slow training and
rendering speed. To effectively handle these issues, we propose sequential
motion understanding radiance fields (SMURF), a novel approach that employs
neural ordinary differential equation (Neural-ODE) to model continuous camera
motion and leverages the explicit volumetric representation method for faster
training and robustness to motion-blurred input images. The core idea of the
SMURF is continuous motion blurring kernel (CMBK), a unique module designed to
model a continuous camera movements for processing blurry inputs. Our model,
rigorously evaluated against benchmark datasets, demonstrates state-of-the-art
performance both quantitatively and qualitatively.

Comments:
- 25 pages, 10 figures, Code is available at
  https://github.com/Jho-Yonsei/SMURF

---

## SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-12 | Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.07494v1){: .btn .btn-green } |

**Abstract**: We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D
Gaussian representation, that enables accurate 3D semantic mapping, robust
camera tracking, and high-quality rendering in real-time. In this system, we
incorporate semantic feature embedding into 3D Gaussian representation, which
effectively encodes semantic information within the spatial layout of the
environment for precise semantic scene representation. Furthermore, we propose
feature-level loss for updating 3D Gaussian representation, enabling
higher-level guidance for 3D Gaussian optimization. In addition, to reduce
cumulative drift and improve reconstruction accuracy, we introduce
semantic-informed bundle adjustment leveraging semantic associations for joint
optimization of 3D Gaussian representation and camera poses, leading to more
robust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates
superior performance over existing dense semantic SLAM methods in terms of
mapping and tracking accuracy on Replica and ScanNet datasets, while also
showing excellent capabilities in novel-view semantic synthesis and 3D semantic
mapping.



---

## DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with  Global-Local Depth Normalization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu | cs.CV | [PDF](http://arxiv.org/pdf/2403.06912v2){: .btn .btn-green } |

**Abstract**: Radiance fields have demonstrated impressive performance in synthesizing
novel views from sparse input views, yet prevailing methods suffer from high
training costs and slow inference speed. This paper introduces DNGaussian, a
depth-regularized framework based on 3D Gaussian radiance fields, offering
real-time and high-quality few-shot novel view synthesis at low costs. Our
motivation stems from the highly efficient representation and surprising
quality of the recent 3D Gaussian Splatting, despite it will encounter a
geometry degradation when input views decrease. In the Gaussian radiance
fields, we find this degradation in scene geometry primarily lined to the
positioning of Gaussian primitives and can be mitigated by depth constraint.
Consequently, we propose a Hard and Soft Depth Regularization to restore
accurate scene geometry under coarse monocular depth supervision while
maintaining a fine-grained color appearance. To further refine detailed
geometry reshaping, we introduce Global-Local Depth Normalization, enhancing
the focus on small local depth changes. Extensive experiments on LLFF, DTU, and
Blender datasets demonstrate that DNGaussian outperforms state-of-the-art
methods, achieving comparable or better results with significantly reduced
memory cost, a $25 \times$ reduction in training time, and over $3000 \times$
faster rendering speed.

Comments:
- Accepted at CVPR 2024. Project page:
  https://fictionarry.github.io/DNGaussian/

---

## Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Chenhao Zhang, Yongyang Zhou, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2403.06505v1){: .btn .btn-green } |

**Abstract**: The neural radiance field (NeRF) has emerged as a prominent methodology for
synthesizing realistic images of novel views. While neural radiance
representations based on voxels or mesh individually offer distinct advantages,
excelling in either rendering quality or speed, each has limitations in the
other aspect. In response, we propose a pioneering hybrid representation named
Vosh, seamlessly combining both voxel and mesh components in hybrid rendering
for view synthesis. Vosh is meticulously crafted by optimizing the voxel grid
of NeRF, strategically with selected voxels replaced by mesh. Therefore, it
excels in fast rendering scenes with simple geometry and textures through its
mesh component, while simultaneously enabling high-quality rendering in
intricate regions by leveraging voxel component. The flexibility of Vosh is
showcased through the ability to adjust hybrid ratios, providing users the
ability to control the balance between rendering quality and speed based on
flexible usage. Experimental results demonstrates that our method achieves
commendable trade-off between rendering quality and speed, and notably has
real-time performance on mobile devices.



---

## FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing | cs.CV | [PDF](http://arxiv.org/pdf/2403.06908v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has achieved very impressive performance in real-time
novel view synthesis. However, it often suffers from over-reconstruction during
Gaussian densification where high-variance image regions are covered by a few
large Gaussians only, leading to blur and artifacts in the rendered images. We
design a progressive frequency regularization (FreGS) technique to tackle the
over-reconstruction issue within the frequency space. Specifically, FreGS
performs coarse-to-fine Gaussian densification by exploiting low-to-high
frequency components that can be easily extracted with low-pass and high-pass
filters in the Fourier space. By minimizing the discrepancy between the
frequency spectrum of the rendered image and the corresponding ground truth, it
achieves high-quality Gaussian densification and alleviates the
over-reconstruction of Gaussian splatting effectively. Experiments over
multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and
Deep Blending) show that FreGS achieves superior novel view synthesis and
outperforms the state-of-the-art consistently.



---

## SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields  for Robotic Inspection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Yifu Tao, Yash Bhalgat, Lanke Frank Tarimo Fu, Matias Mattamala, Nived Chebrolu, Maurice Fallon | cs.RO | [PDF](http://arxiv.org/pdf/2403.06877v1){: .btn .btn-green } |

**Abstract**: We present a neural-field-based large-scale reconstruction system that fuses
lidar and vision data to generate high-quality reconstructions that are
geometrically accurate and capture photo-realistic textures. This system adapts
the state-of-the-art neural radiance field (NeRF) representation to also
incorporate lidar data which adds strong geometric constraints on the depth and
surface normals. We exploit the trajectory from a real-time lidar SLAM system
to bootstrap a Structure-from-Motion (SfM) procedure to both significantly
reduce the computation time and to provide metric scale which is crucial for
lidar depth loss. We use submapping to scale the system to large-scale
environments captured over long trajectories. We demonstrate the reconstruction
system with data from a multi-camera, lidar sensor suite onboard a legged
robot, hand-held while scanning building scenes for 600 metres, and onboard an
aerial robot surveying a multi-storey mock disaster site-building. Website:
https://ori-drs.github.io/projects/silvr/

Comments:
- Accepted at ICRA 2024; Website:
  https://ori-drs.github.io/projects/silvr/

---

## FSViewFusion: Few-Shots View Generation of Novel Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-11 | Rukhshanda Hussain, Hui Xian Grace Lim, Borchun Chen, Mubarak Shah, Ser Nam Lim | cs.CV | [PDF](http://arxiv.org/pdf/2403.06394v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis has observed tremendous developments since the arrival
of NeRFs. However, Nerf models overfit on a single scene, lacking
generalization to out of distribution objects. Recently, diffusion models have
exhibited remarkable performance on introducing generalization in view
synthesis. Inspired by these advancements, we explore the capabilities of a
pretrained stable diffusion model for view synthesis without explicit 3D
priors. Specifically, we base our method on a personalized text to image model,
Dreambooth, given its strong ability to adapt to specific novel objects with a
few shots. Our research reveals two interesting findings. First, we observe
that Dreambooth can learn the high level concept of a view, compared to
arguably more complex strategies which involve finetuning diffusions on large
amounts of multi-view data. Second, we establish that the concept of a view can
be disentangled and transferred to a novel object irrespective of the original
object's identify from which the views are learnt. Motivated by this, we
introduce a learning strategy, FSViewFusion, which inherits a specific view
through only one image sample of a single scene, and transfers the knowledge to
a novel object, learnt from few shots, using low rank adapters. Through
extensive experiments we demonstrate that our method, albeit simple, is
efficient in generating reliable view samples for in the wild images. Code and
models will be released.



---

## Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View  Synthesis?

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-10 | Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen | cs.CV | [PDF](http://arxiv.org/pdf/2403.06092v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has achieved superior performance for novel view
synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a
volume rendering procedure, however, when fewer known views are given (i.e.,
few-shot view synthesis), the model is prone to overfit the given views. To
handle this issue, previous efforts have been made towards leveraging learned
priors or introducing additional regularizations. In contrast, in this paper,
we for the first time provide an orthogonal method from the perspective of
network structure. Given the observation that trivially reducing the number of
model parameters alleviates the overfitting issue, but at the cost of missing
details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs
(i.e., location and viewing direction) of the vanilla MLP into each layer to
prevent the overfitting issue without harming detailed synthesis. To further
reduce the artifacts, we propose to model colors and volume density separately
and present two regularization terms. Extensive experiments on multiple
datasets demonstrate that: 1) although the proposed mi-MLP is easy to
implement, it is surprisingly effective as it boosts the PSNR of the baseline
from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art
results on a wide range of benchmarks. We will release the code upon
publication.

Comments:
- Accepted by CVPR 2024

---

## Large Generative Model Assisted 3D Semantic Communication

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-09 | Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You | cs.IT | [PDF](http://arxiv.org/pdf/2403.05783v1){: .btn .btn-green } |

**Abstract**: Semantic Communication (SC) is a novel paradigm for data transmission in 6G.
However, there are several challenges posed when performing SC in 3D scenarios:
1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain
channel estimation. To address these issues, we propose a Generative AI Model
assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor
(3DSE), which employs generative AI models, including Segment Anything Model
(SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D
scenario based on user requirements. The extracted 3D semantics are represented
as multi-perspective images of the goal-oriented 3D object. Then, we present an
Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective
images, in which we use a semantic encoder with two output heads to perform
semantic encoding and mask redundant semantics in the latent semantic space,
respectively. Next, we design a conditional Generative adversarial network and
Diffusion model aided-Channel Estimation (GDCE) to estimate and refine the
Channel State Information (CSI) of physical channels. Finally, simulation
results demonstrate the advantages of the proposed GAM-3DSC system in
effectively transmitting the goal-oriented 3D scenario.

Comments:
- 13 pages,13 figures,1 table

---

## Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous  Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-09 | Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma | cs.CV | [PDF](http://arxiv.org/pdf/2403.05907v1){: .btn .btn-green } |

**Abstract**: Recent studies have highlighted the promising application of NeRF in
autonomous driving contexts. However, the complexity of outdoor environments,
combined with the restricted viewpoints in driving scenarios, complicates the
task of precisely reconstructing scene geometry. Such challenges often lead to
diminished quality in reconstructions and extended durations for both training
and rendering. To tackle these challenges, we present Lightning NeRF. It uses
an efficient hybrid scene representation that effectively utilizes the geometry
prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly
improves the novel view synthesis performance of NeRF and reduces computational
overheads. Through evaluations on real-world datasets, such as KITTI-360,
Argoverse2, and our private dataset, we demonstrate that our approach not only
exceeds the current state-of-the-art in novel view synthesis quality but also
achieves a five-fold increase in training speed and a ten-fold improvement in
rendering speed. Codes are available at
https://github.com/VISION-SJTU/Lightning-NeRF .

Comments:
- Accepted to ICRA 2024

---

## GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-08 | Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà | cs.CV | [PDF](http://arxiv.org/pdf/2403.05154v1){: .btn .btn-green } |

**Abstract**: We present GSEdit, a pipeline for text-guided 3D object editing based on
Gaussian Splatting models. Our method enables the editing of the style and
appearance of 3D objects without altering their main details, all in a matter
of minutes on consumer hardware. We tackle the problem by leveraging Gaussian
splatting to represent 3D scenes, and we optimize the model while progressively
varying the image supervision by means of a pretrained image-based diffusion
model. The input object may be given as a 3D triangular mesh, or directly
provided as Gaussians from a generative model such as DreamGaussian. GSEdit
ensures consistency across different viewpoints, maintaining the integrity of
the original object's information. Compared to previously proposed methods
relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making
3D editing tasks much faster. Our editing process is refined via the
application of the SDS loss, ensuring that our edits are both precise and
accurate. Our comprehensive evaluation demonstrates that GSEdit effectively
alters object shape and appearance following the given textual instructions
while preserving their coherence and detail.

Comments:
- 15 pages, 7 figures

---

## SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-08 | Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, Zeyu Wang | cs.GR | [PDF](http://arxiv.org/pdf/2403.05087v1){: .btn .btn-green } |

**Abstract**: We present SplattingAvatar, a hybrid 3D representation of photorealistic
human avatars with Gaussian Splatting embedded on a triangle mesh, which
renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We
disentangle the motion and appearance of a virtual human with explicit mesh
geometry and implicit appearance modeling with Gaussian Splatting. The
Gaussians are defined by barycentric coordinates and displacement on a triangle
mesh as Phong surfaces. We extend lifted optimization to simultaneously
optimize the parameters of the Gaussians while walking on the triangle mesh.
SplattingAvatar is a hybrid representation of virtual humans where the mesh
represents low-frequency motion and surface deformation, while the Gaussians
take over the high-frequency geometry and detailed appearance. Unlike existing
deformation methods that rely on an MLP-based linear blend skinning (LBS) field
for motion, we control the rotation and translation of the Gaussians directly
by mesh, which empowers its compatibility with various animation techniques,
e.g., skeletal animation, blend shapes, and mesh editing. Trainable from
monocular videos for both full-body and head avatars, SplattingAvatar shows
state-of-the-art rendering quality across multiple datasets.

Comments:
- [CVPR 2024] Code and data are available at
  https://github.com/initialneil/SplattingAvatar

---

## Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Yuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille | eess.IV | [PDF](http://arxiv.org/pdf/2403.04116v1){: .btn .btn-green } |

**Abstract**: X-ray is widely applied for transmission imaging due to its stronger
penetration than natural light. When rendering novel view X-ray projections,
existing methods mainly based on NeRF suffer from long training time and slow
inference speed. In this paper, we propose a 3D Gaussian splatting-based
framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we
redesign a radiative Gaussian point cloud model inspired by the isotropic
nature of X-ray imaging. Our model excludes the influence of view direction
when learning to predict the radiation intensity of 3D points. Based on this
model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA
implementation. Secondly, we customize an Angle-pose Cuboid Uniform
Initialization (ACUI) strategy that directly uses the parameters of the X-ray
scanner to compute the camera information and then uniformly samples point
positions within a cuboid enclosing the scanned object. Experiments show that
our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying
less than 15% training time and over 73x inference speed. The application on
sparse-view CT reconstruction also reveals the practical values of our method.
Code and models will be publicly available at
https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training
process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .

Comments:
- The first 3D Gaussian Splatting-based method for X-ray 3D
  reconstruction

---

## DNAct: Diffusion Guided Multi-Task 3D Policy Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Ge Yan, Yueh-Hua Wu, Xiaolong Wang | cs.RO | [PDF](http://arxiv.org/pdf/2403.04115v2){: .btn .btn-green } |

**Abstract**: This paper presents DNAct, a language-conditioned multi-task policy framework
that integrates neural rendering pre-training and diffusion training to enforce
multi-modality learning in action sequence spaces. To learn a generalizable
multi-task policy with few demonstrations, the pre-training phase of DNAct
leverages neural rendering to distill 2D semantic features from foundation
models such as Stable Diffusion to a 3D space, which provides a comprehensive
semantic understanding regarding the scene. Consequently, it allows various
applications to challenging robotic tasks requiring rich 3D semantics and
accurate geometry. Furthermore, we introduce a novel approach utilizing
diffusion training to learn a vision and language feature that encapsulates the
inherent multi-modality in the multi-task demonstrations. By reconstructing the
action sequences from different tasks via the diffusion process, the model is
capable of distinguishing different modalities and thus improving the
robustness and the generalizability of the learned representation. DNAct
significantly surpasses SOTA NeRF-based multi-task manipulation approaches with
over 30% improvement in success rate. Project website: dnact.github.io.



---

## Finding Waldo: Towards Efficient Exploration of NeRF Scene Spaces

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Evangelos Skartados, Mehmet Kerim Yucel, Bruno Manganelli, Anastasios Drosou, Albert Saà-Garriga | cs.CV | [PDF](http://arxiv.org/pdf/2403.04508v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D
reconstruction and novel view synthesis in recent years due to their remarkable
performance. Despite the huge interest in NeRF methods, a practical use case of
NeRFs has largely been ignored; the exploration of the scene space modelled by
a NeRF. In this paper, for the first time in the literature, we propose and
formally define the scene exploration framework as the efficient discovery of
NeRF model inputs (i.e. coordinates and viewing angles), using which one can
render novel views that adhere to user-selected criteria. To remedy the lack of
approaches addressing scene exploration, we first propose two baseline methods
called Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).
We then cast scene exploration as an optimization problem, and propose the
criteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient
exploration. We test all three approaches with various criteria (e.g. saliency
maximization, image quality maximization, photo-composition quality
improvement) and show that our EGPS performs more favourably than other
baselines. We finally highlight key points and limitations, and outline
directions for future research in scene exploration.

Comments:
- Accepted at ACM MMSys'24

---

## BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa | cs.CV | [PDF](http://arxiv.org/pdf/2403.04926v1){: .btn .btn-green } |

**Abstract**: Recent efforts in using 3D Gaussians for scene reconstruction and novel view
synthesis can achieve impressive results on curated benchmarks; however, images
captured in real life are often blurry. In this work, we analyze the robustness
of Gaussian-Splatting-based methods against various image blur, such as motion
blur, defocus blur, downscaling blur, \etc. Under these degradations,
Gaussian-Splatting-based methods tend to overfit and produce worse results than
Neural-Radiance-Field-based methods. To address this issue, we propose Blur
Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling
capacities such that a 3D-consistent and high quality scene can be
reconstructed despite image-wise blur. Specifically, we model blur by
estimating per-pixel convolution kernels from a Blur Proposal Network (BPN).
BPN is designed to consider spatial, color, and depth variations of the scene
to maximize modeling capacity. Additionally, BPN also proposes a
quality-assessing mask, which indicates regions where blur occur. Finally, we
introduce a coarse-to-fine kernel optimization scheme; this optimization scheme
is fast and avoids sub-optimal solutions due to a sparse point cloud
initialization, which often occurs when we apply Structure-from-Motion on
blurry images. We demonstrate that BAGS achieves photorealistic renderings
under various challenging blur conditions and imaging geometry, while
significantly improving upon existing approaches.



---

## Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-07 | Nikhil Mishra, Maximilian Sieb, Pieter Abbeel, Xi Chen | cs.RO | [PDF](http://arxiv.org/pdf/2403.04114v1){: .btn .btn-green } |

**Abstract**: Deep learning methods for perception are the cornerstone of many robotic
systems. Despite their potential for impressive performance, obtaining
real-world training data is expensive, and can be impractically difficult for
some tasks. Sim-to-real transfer with domain randomization offers a potential
workaround, but often requires extensive manual tuning and results in models
that are brittle to distribution shift between sim and real. In this work, we
introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF
model that is the centerpiece of a real-to-sim pipeline for synthesizing
training data targeted to scenes and objects from the real world. COV-NeRF
extracts objects from real images and composes them into new scenes, generating
photorealistic renderings and many types of 2D and 3D supervision, including
depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the
rendering quality of modern NeRF methods, and can be used to rapidly close the
sim-to-real gap across a variety of perceptual modalities.

Comments:
- ICRA 2024

---

## GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D  Scene Understanding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-06 | Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2403.03608v1){: .btn .btn-green } |

**Abstract**: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.

Comments:
- Accepted by CVPR2024

---

## Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-05 | Timothy Chen, Ola Shorinwa, Weijia Zeng, Joseph Bruno, Philip Dames, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2403.02751v1){: .btn .btn-green } |

**Abstract**: We present Splat-Nav, a navigation pipeline that consists of a real-time safe
planning module and a robust state estimation module designed to operate in the
Gaussian Splatting (GSplat) environment representation, a popular emerging 3D
scene representation from computer vision. We formulate rigorous collision
constraints that can be computed quickly to build a guaranteed-safe polytope
corridor through the map. We then optimize a B-spline trajectory through this
corridor. We also develop a real-time, robust state estimation module by
interpreting the GSplat representation as a point cloud. The module enables the
robot to localize its global pose with zero prior knowledge from RGB-D images
using point cloud alignment, and then track its own pose as it moves through
the scene from RGB images using image-to-point cloud localization. We also
incorporate semantics into the GSplat in order to obtain better images for
localization. All of these modules operate mainly on CPU, freeing up GPU
resources for tasks like real-time scene reconstruction. We demonstrate the
safety and robustness of our pipeline in both simulation and hardware, where we
show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude
faster than Neural Radiance Field (NeRF)-based navigation methods, thereby
enabling real-time navigation.



---

## A Deep Learning Framework for Wireless Radiation Field Reconstruction  and Channel Prediction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-05 | Haofan Lu, Christopher Vattheuer, Baharan Mirzasoleiman, Omid Abari | cs.NI | [PDF](http://arxiv.org/pdf/2403.03241v1){: .btn .btn-green } |

**Abstract**: We present NeWRF, a deep learning framework for predicting wireless channels.
Wireless channel prediction is a long-standing problem in the wireless
community and is a key technology for improving the coverage of wireless
network deployments. Today, a wireless deployment is evaluated by a site survey
which is a cumbersome process requiring an experienced engineer to perform
extensive channel measurements. To reduce the cost of site surveys, we develop
NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF).
NeWRF trains a neural network model with a sparse set of channel measurements,
and predicts the wireless channel accurately at any location in the site. We
introduce a series of techniques that integrate wireless propagation properties
into the NeRF framework to account for the fundamental differences between the
behavior of light and wireless signals. We conduct extensive evaluations of our
framework and show that our approach can accurately predict channels at
unvisited locations with significantly lower measurement density than prior
state-of-the-art



---

## DaReNeRF: Direction-aware Representation for Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-04 | Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2403.02265v1){: .btn .btn-green } |

**Abstract**: Addressing the intricate challenge of modeling and re-rendering dynamic
scenes, most recent approaches have sought to simplify these complexities using
plane-based explicit representations, overcoming the slow training time issues
associated with methods like Neural Radiance Fields (NeRF) and implicit
representations. However, the straightforward decomposition of 4D dynamic
scenes into multiple 2D plane-based representations proves insufficient for
re-rendering high-fidelity scenes with complex motions. In response, we present
a novel direction-aware representation (DaRe) approach that captures scene
dynamics from six different directions. This learned representation undergoes
an inverse dual-tree complex wavelet transformation (DTCWT) to recover
plane-based information. DaReNeRF computes features for each space-time point
by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny
MLP for color regression and leveraging volume rendering in training yield
state-of-the-art performance in novel view synthesis for complex dynamic
scenes. Notably, to address redundancy introduced by the six real and six
imaginary direction-aware wavelet coefficients, we introduce a trainable
masking approach, mitigating storage issues without significant performance
decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared
to prior art while delivering superior performance.

Comments:
- Accepted at CVPR 2024. Paper + supplementary material

---

## Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input  Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-04 | Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song | cs.CV | [PDF](http://arxiv.org/pdf/2403.02063v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis with sparse input views is important for real-world
applications like AR/VR and autonomous driving. Recent methods have integrated
depth information into NeRFs for sparse input synthesis, leveraging depth prior
for geometric and spatial understanding. However, most existing works tend to
overlook inaccuracies within depth maps and have low time efficiency. To
address these issues, we propose a depth-guided robust and fast point cloud
fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel
grid of features. A point cloud is constructed for each input view,
characterized within the voxel grid using matrices and vectors. We accumulate
the point cloud of each input view to construct the fused point cloud of the
entire scene. Each voxel determines its density and appearance by referring to
the point cloud of the entire scene. Through point cloud fusion and voxel grid
fine-tuning, inaccuracies in depth values are refined or substituted by those
from other views. Moreover, our method can achieve faster reconstruction and
greater compactness through effective vector-matrix decomposition. Experimental
results underline the superior performance and time efficiency of our approach
compared to state-of-the-art baselines.



---

## Neural Field Classifiers via Target Encoding and Classification Loss

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun | cs.CV | [PDF](http://arxiv.org/pdf/2403.01058v1){: .btn .btn-green } |

**Abstract**: Neural field methods have seen great progress in various long-standing tasks
in computer vision and computer graphics, including novel view synthesis and
geometry reconstruction. As existing neural field methods try to predict some
coordinate-based continuous target values, such as RGB for Neural Radiance
Field (NeRF), all of these methods are regression models and are optimized by
some regression loss. However, are regression models really better than
classification models for neural field methods? In this work, we try to visit
this very fundamental but overlooked question for neural fields from a machine
learning perspective. We successfully propose a novel Neural Field Classifier
(NFC) framework which formulates existing neural field methods as
classification tasks rather than regression tasks. The proposed NFC can easily
transform arbitrary Neural Field Regressor (NFR) into its classification
variant via employing a novel Target Encoding module and optimizing a
classification loss. By encoding a continuous regression target into a
high-dimensional discrete encoding, we naturally formulate a multi-label
classification task. Extensive experiments demonstrate the impressive
effectiveness of NFC at the nearly free extra computational costs. Moreover,
NFC also shows robustness to sparse inputs, corrupted images, and dynamic
scenes.

Comments:
- ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables

---

## NeRF-VPT: Learning Novel View Representations with Neural Radiance  Fields via View Prompt Tuning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr | cs.CV | [PDF](http://arxiv.org/pdf/2403.01325v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have garnered remarkable success in novel view
synthesis. Nonetheless, the task of generating high-quality images for novel
views persists as a critical challenge. While the existing efforts have
exhibited commendable progress, capturing intricate details, enhancing
textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics
warrant further focused attention and advancement. In this work, we propose
NeRF-VPT, an innovative method for novel view synthesis to address these
challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning
paradigm, wherein RGB information gained from preceding rendering outcomes
serves as instructive visual prompts for subsequent rendering stages, with the
aspiration that the prior knowledge embedded in the prompts can facilitate the
gradual enhancement of rendered image quality. NeRF-VPT only requires sampling
RGB data from previous stage renderings as priors at each training stage,
without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is
plug-and-play and can be readily integrated into existing methods. By
conducting comparative analyses of our NeRF-VPT against several NeRF-based
approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,
Real Forward-Facing, Replica dataset, and a user-captured dataset, we
substantiate that our NeRF-VPT significantly elevates baseline performance and
proficiently generates more high-quality novel view images than all the
compared state-of-the-art methods. Furthermore, the cascading learning of
NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in
a significant enhancement of accuracy for sparse-view novel view synthesis. The
source code and dataset are available at
\url{https://github.com/Freedomcls/NeRF-VPT}.

Comments:
- AAAI 2024

---

## Neural radiance fields-based holography [Invited]

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-02 | Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba | cs.CV | [PDF](http://arxiv.org/pdf/2403.01137v1){: .btn .btn-green } |

**Abstract**: This study presents a novel approach for generating holograms based on the
neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data
is difficult in hologram computation. NeRF is a state-of-the-art technique for
3D light-field reconstruction from 2D images based on volume rendering. The
NeRF can rapidly predict new-view images that do not include a training
dataset. In this study, we constructed a rendering pipeline directly from a 3D
light field generated from 2D images by NeRF for hologram generation using deep
neural networks within a reasonable time. The pipeline comprises three main
components: the NeRF, a depth predictor, and a hologram generator, all
constructed using deep neural networks. The pipeline does not include any
physical calculations. The predicted holograms of a 3D scene viewed from any
direction were computed using the proposed pipeline. The simulation and
experimental results are presented.



---

## DISORF: A Distributed Online NeRF Training and Rendering Framework for  Mobile Robots

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-03-01 | Chunlin Li, Ruofan Liang, Hanrui Fan, Zhengen Zhang, Sankeerth Durvasula, Nandita Vijaykumar | cs.RO | [PDF](http://arxiv.org/pdf/2403.00228v1){: .btn .btn-green } |

**Abstract**: We present a framework, DISORF, to enable online 3D reconstruction and
visualization of scenes captured by resource-constrained mobile robots and edge
devices. To address the limited compute capabilities of edge devices and
potentially limited network availability, we design a framework that
efficiently distributes computation between the edge device and remote server.
We leverage on-device SLAM systems to generate posed keyframes and transmit
them to remote servers that can perform high quality 3D reconstruction and
visualization at runtime by leveraging NeRF models. We identify a key challenge
with online NeRF training where naive image sampling strategies can lead to
significant degradation in rendering quality. We propose a novel shifted
exponential frame sampling method that addresses this challenge for online NeRF
training. We demonstrate the effectiveness of our framework in enabling
high-quality real-time reconstruction and visualization of unknown scenes as
they are captured and streamed from cameras in mobile robots and edge devices.


