---
layout: default
title: August 2024
parent: Papers
nav_order: 202408
---

<!---metadata--->


## Progressive Radiance Distillation for Inverse Rendering with Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Keyang Ye, Qiming Hou, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2408.07595v1){: .btn .btn-green } |

**Abstract**: We propose progressive radiance distillation, an inverse rendering method
that combines physically-based rendering with Gaussian-based radiance field
rendering using a distillation progress map. Taking multi-view images as input,
our method starts from a pre-trained radiance field guidance, and distills
physically-based light and material parameters from the radiance field using an
image-fitting process. The distillation progress map is initialized to a small
value, which favors radiance field rendering. During early iterations when
fitted light and material parameters are far from convergence, the radiance
field fallback ensures the sanity of image loss gradients and avoids local
minima that attracts under-fit states. As fitted parameters converge, the
physical model gradually takes over and the distillation progress increases
correspondingly. In presence of light paths unmodeled by the physical model,
the distillation progress never finishes on affected pixels and the learned
radiance field stays in the final rendering. With this designed tolerance for
physical model limitations, we prevent unmodeled color components from leaking
into light and material parameters, alleviating relighting artifacts.
Meanwhile, the remaining radiance field compensates for the limitations of the
physical model, guaranteeing high-quality novel views synthesis. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
techniques quality-wise in both novel view synthesis and relighting. The idea
of progressive radiance distillation is not limited to Gaussian splatting. We
show that it also has positive effects for prominently specular scenes when
adapted to a mesh-based inverse rendering method.



---

## 3D Gaussian Editing with A Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Guan Luo, Tian-Xing Xu, Ying-Tian Liu, Xiao-Xiong Fan, Fang-Lue Zhang, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.07540v1){: .btn .btn-green } |

**Abstract**: The modeling and manipulation of 3D scenes captured from the real world are
pivotal in various applications, attracting growing research interest. While
previous works on editing have achieved interesting results through
manipulating 3D meshes, they often require accurately reconstructed meshes to
perform editing, which limits their application in 3D content generation. To
address this gap, we introduce a novel single-image-driven 3D scene editing
approach based on 3D Gaussian Splatting, enabling intuitive manipulation via
directly editing the content on a 2D image plane. Our method learns to optimize
the 3D Gaussians to align with an edited version of the image rendered from a
user-specified viewpoint of the original scene. To capture long-range object
deformation, we introduce positional loss into the optimization process of 3D
Gaussian Splatting and enable gradient propagation through reparameterization.
To handle occluded 3D Gaussians when rendering from the specified viewpoint, we
build an anchor-based structure and employ a coarse-to-fine optimization
strategy capable of handling long-range deformation while maintaining
structural stability. Furthermore, we design a novel masking strategy to
adaptively identify non-rigid deformation regions for fine-scale modeling.
Extensive experiments show the effectiveness of our method in handling
geometric details, long-range, and non-rigid deformation, demonstrating
superior editing flexibility and quality compared to previous approaches.

Comments:
- 10 pages, 12 figures

---

## Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Hyunjee Lee, Youngsik Yun, Jeongmin Bae, Seoha Kim, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2408.07416v1){: .btn .btn-green } |

**Abstract**: Understanding the 3D semantics of a scene is a fundamental problem for
various scenarios such as embodied agents. While NeRFs and 3DGS excel at
novel-view synthesis, previous methods for understanding their semantics have
been limited to incomplete 3D understanding: their segmentation results are 2D
masks and their supervision is anchored at 2D pixels. This paper revisits the
problem set to pursue a better 3D understanding of a scene modeled by NeRFs and
3DGS as follows. 1) We directly supervise the 3D points to train the language
embedding field. It achieves state-of-the-art accuracy without relying on
multi-scale language embeddings. 2) We transfer the pre-trained language field
to 3DGS, achieving the first real-time rendering speed without sacrificing
training time or accuracy. 3) We introduce a 3D querying and evaluation
protocol for assessing the reconstructed geometry and semantics together. Code,
checkpoints, and annotations will be available online. Project page:
https://hyunji12.github.io/Open3DRF

Comments:
- Project page: https://hyunji12.github.io/Open3DRF

---

## SpectralGaussians: Semantic, spectral 3D Gaussian splatting for  multi-spectral scene representation, visualization and analysis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Saptarshi Neil Sinha, Holger Graf, Michael Weinmann | cs.CV | [PDF](http://arxiv.org/pdf/2408.06975v1){: .btn .btn-green } |

**Abstract**: We propose a novel cross-spectral rendering framework based on 3D Gaussian
Splatting (3DGS) that generates realistic and semantically meaningful splats
from registered multi-view spectrum and segmentation maps. This extension
enhances the representation of scenes with multiple spectra, providing insights
into the underlying materials and segmentation. We introduce an improved
physically-based rendering approach for Gaussian splats, estimating reflectance
and lights per spectra, thereby enhancing accuracy and realism. In a
comprehensive quantitative and qualitative evaluation, we demonstrate the
superior performance of our approach with respect to other recent
learning-based spectral scene representation approaches (i.e., XNeRF and
SpectralNeRF) as well as other non-spectral state-of-the-art learning-based
approaches. Our work also demonstrates the potential of spectral scene
understanding for precise scene editing techniques like style transfer,
inpainting, and removal. Thereby, our contributions address challenges in
multi-spectral scene representation, rendering, and editing, offering new
possibilities for diverse applications.



---

## Potamoi: Accelerating Neural Rendering via a Unified Streaming  Architecture

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Yu Feng, Weikai Lin, Zihan Liu, Jingwen Leng, Minyi Guo, Han Zhao, Xiaofeng Hou, Jieru Zhao, Yuhao Zhu | cs.AR | [PDF](http://arxiv.org/pdf/2408.06608v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has emerged as a promising alternative for
photorealistic rendering. Despite recent algorithmic advancements, achieving
real-time performance on today's resource-constrained devices remains
challenging. In this paper, we identify the primary bottlenecks in current NeRF
algorithms and introduce a unified algorithm-architecture co-design, Potamoi,
designed to accommodate various NeRF algorithms. Specifically, we introduce a
runtime system featuring a plug-and-play algorithm, SpaRW, which significantly
reduces the per-frame computational workload and alleviates compute
inefficiencies. Furthermore, our unified streaming pipeline coupled with
customized hardware support effectively tames both SRAM and DRAM inefficiencies
by minimizing repetitive DRAM access and completely eliminating SRAM bank
conflicts. When evaluated against a baseline utilizing a dedicated DNN
accelerator, our framework demonstrates a speed-up and energy reduction of
53.1$\times$ and 67.7$\times$, respectively, all while maintaining high visual
quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.

Comments:
- arXiv admin note: substantial text overlap with arXiv:2404.11852

---

## ActiveNeRF: Learning Accurate 3D Geometry by Active Pattern Projection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Jianyu Tao, Changping Hu, Edward Yang, Jing Xu, Rui Chen | cs.CV | [PDF](http://arxiv.org/pdf/2408.06592v1){: .btn .btn-green } |

**Abstract**: NeRFs have achieved incredible success in novel view synthesis. However, the
accuracy of the implicit geometry is unsatisfactory because the passive static
environmental illumination has low spatial frequency and cannot provide enough
information for accurate geometry reconstruction. In this work, we propose
ActiveNeRF, a 3D geometry reconstruction framework, which improves the geometry
quality of NeRF by actively projecting patterns of high spatial frequency onto
the scene using a projector which has a constant relative pose to the camera.
We design a learnable active pattern rendering pipeline which jointly learns
the scene geometry and the active pattern. We find that, by adding the active
pattern and imposing its consistency across different views, our proposed
method outperforms state of the art geometry reconstruction methods
qualitatively and quantitatively in both simulation and real experiments. Code
is avaliable at https://github.com/hcp16/active_nerf

Comments:
- 18 pages, 10 figures

---

## HDRGS: High Dynamic Range Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Jiahao Wu, Lu Xiao, Chao Wang, Rui Peng, Kaiqiang Xiong, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.06543v1){: .btn .btn-green } |

**Abstract**: Recent years have witnessed substantial advancements in the field of 3D
reconstruction from 2D images, particularly following the introduction of the
neural radiance field (NeRF) technique. However, reconstructing a 3D high
dynamic range (HDR) radiance field, which aligns more closely with real-world
conditions, from 2D multi-exposure low dynamic range (LDR) images continues to
pose significant challenges. Approaches to this issue fall into two categories:
grid-based and implicit-based. Implicit methods, using multi-layer perceptrons
(MLP), face inefficiencies, limited solvability, and overfitting risks.
Conversely, grid-based methods require significant memory and struggle with
image quality and long training times. In this paper, we introduce Gaussian
Splatting-a recent, high-quality, real-time 3D reconstruction technique-into
this domain. We further develop the High Dynamic Range Gaussian Splatting
(HDR-GS) method, designed to address the aforementioned challenges. This method
enhances color dimensionality by including luminance and uses an asymmetric
grid for tone-mapping, swiftly and precisely converting pixel irradiance to
color. Our approach improves HDR scene recovery accuracy and integrates a novel
coarse-to-fine strategy to speed up model convergence, enhancing robustness
against sparse viewpoints and exposure extremes, and preventing local optima.
Extensive testing confirms that our method surpasses current state-of-the-art
techniques in both synthetic and real-world scenarios. Code will be released at
\url{https://github.com/WuJH2001/HDRGS}



---

## Developing Smart MAVs for Autonomous Inspection in GPS-denied  Constructions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Paoqiang Pan, Kewei Hu, Xiao Huang, Wei Ying, Xiaoxuan Xie, Yue Ma, Naizhong Zhang, Hanwen Kang | cs.RO | [PDF](http://arxiv.org/pdf/2408.06030v1){: .btn .btn-green } |

**Abstract**: Smart Micro Aerial Vehicles (MAVs) have transformed infrastructure inspection
by enabling efficient, high-resolution monitoring at various stages of
construction, including hard-to-reach areas. Traditional manual operation of
drones in GPS-denied environments, such as industrial facilities and
infrastructure, is labour-intensive, tedious and prone to error. This study
presents an innovative framework for smart MAV inspections in such complex and
GPS-denied indoor environments. The framework features a hierarchical
perception and planning system that identifies regions of interest and
optimises task paths. It also presents an advanced MAV system with enhanced
localisation and motion planning capabilities, integrated with Neural
Reconstruction technology for comprehensive 3D reconstruction of building
structures. The effectiveness of the framework was empirically validated in a
4,000 square meters indoor infrastructure facility with an interior length of
80 metres, a width of 50 metres and a height of 7 metres. The main structure
consists of columns and walls. Experimental results show that our MAV system
performs exceptionally well in autonomous inspection tasks, achieving a 100\%
success rate in generating and executing scan paths. Extensive experiments
validate the manoeuvrability of our developed MAV, achieving a 100\% success
rate in motion planning with a tracking error of less than 0.1 metres. In
addition, the enhanced reconstruction method using 3D Gaussian Splatting
technology enables the generation of high-fidelity rendering models from the
acquired data. Overall, our novel method represents a significant advancement
in the use of robotics for infrastructure inspection.



---

## HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2408.06019v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel 3D head avatar creation approach capable of
generalizing from few-shot in-the-wild data with high-fidelity and animatable
robustness. Given the underconstrained nature of this problem, incorporating
prior knowledge is essential. Therefore, we propose a framework comprising
prior learning and avatar creation phases. The prior learning phase leverages
3D head priors derived from a large-scale multi-view dynamic dataset, and the
avatar creation phase applies these priors for few-shot personalization. Our
approach effectively captures these priors by utilizing a Gaussian
Splatting-based auto-decoder network with part-based dynamic modeling. Our
method employs identity-shared encoding with personalized latent codes for
individual identities to learn the attributes of Gaussian primitives. During
the avatar creation phase, we achieve fast head avatar personalization by
leveraging inversion and fine-tuning strategies. Extensive experiments
demonstrate that our model effectively exploits head priors and successfully
generalizes them to few-shot personalization, achieving photo-realistic
rendering quality, multi-view consistency, and stable animation.

Comments:
- Project page: https://headgap.github.io/

---

## Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for  Anti-aliasing Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Jiameng Li, Yue Shi, Jiezhang Cao, Bingbing Ni, Wenjun Zhang, Kai Zhang, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2408.06286v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has attracted great attention in novel view
synthesis because of its superior rendering efficiency and high fidelity.
However, the trained Gaussians suffer from severe zooming degradation due to
non-adjustable representation derived from single-scale training. Though some
methods attempt to tackle this problem via post-processing techniques such as
selective rendering or filtering techniques towards primitives, the
scale-specific information is not involved in Gaussians. In this paper, we
propose a unified optimization method to make Gaussians adaptive for arbitrary
scales by self-adjusting the primitive properties (e.g., color, shape and size)
and distribution (e.g., position). Inspired by the mipmap technique, we design
pseudo ground-truth for the target scale and propose a scale-consistency
guidance loss to inject scale information into 3D Gaussians. Our method is a
plug-in module, applicable for any 3DGS models to solve the zoom-in and
zoom-out aliasing. Extensive experiments demonstrate the effectiveness of our
method. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dB
for zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.

Comments:
- 9 pages

---

## 3D Reconstruction of Protein Structures from Multi-view AFM Images using  Neural Radiance Fields (NeRFs)

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Jaydeep Rade, Ethan Herron, Soumik Sarkar, Anwesha Sarkar, Adarsh Krishnamurthy | cs.CV | [PDF](http://arxiv.org/pdf/2408.06244v1){: .btn .btn-green } |

**Abstract**: Recent advancements in deep learning for predicting 3D protein structures
have shown promise, particularly when leveraging inputs like protein sequences
and Cryo-Electron microscopy (Cryo-EM) images. However, these techniques often
fall short when predicting the structures of protein complexes (PCs), which
involve multiple proteins. In our study, we investigate using atomic force
microscopy (AFM) combined with deep learning to predict the 3D structures of
PCs. AFM generates height maps that depict the PCs in various random
orientations, providing a rich information for training a neural network to
predict the 3D structures. We then employ the pre-trained UpFusion model (which
utilizes a conditional diffusion model for synthesizing novel views) to train
an instance-specific NeRF model for 3D reconstruction. The performance of
UpFusion is evaluated through zero-shot predictions of 3D protein structures
using AFM images. The challenge, however, lies in the time-intensive and
impractical nature of collecting actual AFM images. To address this, we use a
virtual AFM imaging process that transforms a `PDB' protein file into
multi-view 2D virtual AFM images via volume rendering techniques. We
extensively validate the UpFusion architecture using both virtual and actual
multi-view AFM images. Our results include a comparison of structures predicted
with varying numbers of views and different sets of views. This novel approach
holds significant potential for enhancing the accuracy of protein complex
structure predictions with further fine-tuning of the UpFusion network.



---

## Radiance Field Learners As UAV First-Person Viewers

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-10 | Liqi Yan, Qifan Wang, Junhan Zhao, Qiang Guan, Zheng Tang, Jianhui Zhang, Dongfang Liu | cs.CV | [PDF](http://arxiv.org/pdf/2408.05533v1){: .btn .btn-green } |

**Abstract**: First-Person-View (FPV) holds immense potential for revolutionizing the
trajectory of Unmanned Aerial Vehicles (UAVs), offering an exhilarating avenue
for navigating complex building structures. Yet, traditional Neural Radiance
Field (NeRF) methods face challenges such as sampling single points per
iteration and requiring an extensive array of views for supervision. UAV videos
exacerbate these issues with limited viewpoints and significant spatial scale
variations, resulting in inadequate detail rendering across diverse scales. In
response, we introduce FPV-NeRF, addressing these challenges through three key
facets: (1) Temporal consistency. Leveraging spatio-temporal continuity ensures
seamless coherence between frames; (2) Global structure. Incorporating various
global features during point sampling preserves space integrity; (3) Local
granularity. Employing a comprehensive framework and multi-resolution
supervision for multi-scale scene feature representation tackles the
intricacies of UAV video spatial scales. Additionally, due to the scarcity of
publicly available FPV videos, we introduce an innovative view synthesis method
using NeRF to generate FPV perspectives from UAV footage, enhancing spatial
perception for drones. Our novel dataset spans diverse trajectories, from
outdoor to indoor environments, in the UAV domain, differing significantly from
traditional NeRF scenarios. Through extensive experiments encompassing both
interior and exterior building structures, FPV-NeRF demonstrates a superior
understanding of the UAV flying space, outperforming state-of-the-art methods
in our curated UAV dataset. Explore our project page for further insights:
https://fpv-nerf.github.io/.

Comments:
- Accepted to ECCV 2024

---

## Visual SLAM with 3D Gaussian Primitives and Depth Priors Enabling Novel  View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-10 | Zhongche Qu, Zhi Zhang, Cong Liu, Jianhua Yin | cs.CV | [PDF](http://arxiv.org/pdf/2408.05635v1){: .btn .btn-green } |

**Abstract**: Conventional geometry-based SLAM systems lack dense 3D reconstruction
capabilities since their data association usually relies on feature
correspondences. Additionally, learning-based SLAM systems often fall short in
terms of real-time performance and accuracy. Balancing real-time performance
with dense 3D reconstruction capabilities is a challenging problem. In this
paper, we propose a real-time RGB-D SLAM system that incorporates a novel view
synthesis technique, 3D Gaussian Splatting, for 3D scene representation and
pose estimation. This technique leverages the real-time rendering performance
of 3D Gaussian Splatting with rasterization and allows for differentiable
optimization in real time through CUDA implementation. We also enable mesh
reconstruction from 3D Gaussians for explicit dense 3D reconstruction. To
estimate accurate camera poses, we utilize a rotation-translation decoupled
strategy with inverse optimization. This involves iteratively updating both in
several iterations through gradient-based optimization. This process includes
differentiably rendering RGB, depth, and silhouette maps and updating the
camera parameters to minimize a combined loss of photometric loss, depth
geometry loss, and visibility loss, given the existing 3D Gaussian map.
However, 3D Gaussian Splatting (3DGS) struggles to accurately represent
surfaces due to the multi-view inconsistency of 3D Gaussians, which can lead to
reduced accuracy in both camera pose estimation and scene reconstruction. To
address this, we utilize depth priors as additional regularization to enforce
geometric constraints, thereby improving the accuracy of both pose estimation
and 3D reconstruction. We also provide extensive experimental results on public
benchmark datasets to demonstrate the effectiveness of our proposed methods in
terms of pose accuracy, geometric accuracy, and rendering performance.



---

## FewShotNeRF: Meta-Learning-based Novel View Synthesis for Rapid  Scene-Specific Adaptation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Piraveen Sivakumar, Paul Janson, Jathushan Rajasegaran, Thanuja Ambegoda | cs.CV | [PDF](http://arxiv.org/pdf/2408.04803v1){: .btn .btn-green } |

**Abstract**: In this paper, we address the challenge of generating novel views of
real-world objects with limited multi-view images through our proposed
approach, FewShotNeRF. Our method utilizes meta-learning to acquire optimal
initialization, facilitating rapid adaptation of a Neural Radiance Field (NeRF)
to specific scenes. The focus of our meta-learning process is on capturing
shared geometry and textures within a category, embedded in the weight
initialization. This approach expedites the learning process of NeRFs and
leverages recent advancements in positional encodings to reduce the time
required for fitting a NeRF to a scene, thereby accelerating the inner loop
optimization of meta-learning. Notably, our method enables meta-learning on a
large number of 3D scenes to establish a robust 3D prior for various
categories. Through extensive evaluations on the Common Objects in 3D open
source dataset, we empirically demonstrate the efficacy and potential of
meta-learning in generating high-quality novel views of objects.



---

## Self-augmented Gaussian Splatting with Structure-aware Masks for  Sparse-view 3D Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Lingbei Meng, Bi'an Du, Wei Hu | cs.CV | [PDF](http://arxiv.org/pdf/2408.04831v2){: .btn .btn-green } |

**Abstract**: Sparse-view 3D reconstruction stands as a formidable challenge in computer
vision, aiming to build complete three-dimensional models from a limited array
of viewing perspectives. This task confronts several difficulties: 1) the
limited number of input images that lack consistent information; 2) dependence
on the quality of input images; and 3) the substantial size of model
parameters. To address these challenges, we propose a self-augmented
coarse-to-fine Gaussian splatting paradigm, enhanced with a structure-aware
mask, for sparse-view 3D reconstruction. In particular, our method initially
employs a coarse Gaussian model to obtain a basic 3D representation from
sparse-view inputs. Subsequently, we develop a fine Gaussian network to enhance
consistent and detailed representation of the output with both 3D geometry
augmentation and perceptual view augmentation. During training, we design a
structure-aware masking strategy to further improve the model's robustness
against sparse inputs and noise.Experimental results on the MipNeRF360 and
OmniObject3D datasets demonstrate that the proposed method achieves
state-of-the-art performances for sparse input views in both perceptual quality
and efficiency.



---

## DreamCouple: Exploring High Quality Text-to-3D Generation Via Rectified  Flow

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Hangyu Li, Xiangxiang Chu, Dingyuan Shi | cs.CV | [PDF](http://arxiv.org/pdf/2408.05008v1){: .btn .btn-green } |

**Abstract**: The Score Distillation Sampling (SDS), which exploits pretrained
text-to-image model diffusion models as priors to 3D model training, has
achieved significant success. Currently, the flow-based diffusion model has
become a new trend for generations. Yet, adapting SDS to flow-based diffusion
models in 3D generation remains unexplored. Our work is aimed to bridge this
gap. In this paper, we adapt SDS to rectified flow and re-examine the
over-smoothing issue under this novel framework. The issue can be explained
that the model learns an average of multiple ODE trajectories. Then we propose
DreamCouple, which instead of randomly sampling noise, uses a rectified flow
model to find the coupled noise. Its Unique Couple Matching (UCM) loss guides
the model to learn different trajectories and thus solves the over-smoothing
issue. We apply our method to both NeRF and 3D Gaussian splatting and achieve
state-of-the-art performances. We also identify some other interesting open
questions such as initialization issues for NeRF and faster training
convergence. Our code will be released soon.

Comments:
- Tech Report

---

## InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Xin-Yi Yu, Jun-Xin Yu, Li-Bo Zhou, Yan Wei, Lin-Lin Ou | cs.CV | [PDF](http://arxiv.org/pdf/2408.04249v1){: .btn .btn-green } |

**Abstract**: We present InstantStyleGaussian, an innovative 3D style transfer method based
on the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target
style image, it quickly generates new 3D GS scenes. Our approach operates on
pre-reconstructed GS scenes, combining diffusion models with an improved
iterative dataset update strategy. It utilizes diffusion models to generate
target style images, adds these new images to the training dataset, and uses
this dataset to iteratively update and optimize the GS scenes. Extensive
experimental results demonstrate that our method ensures high-quality stylized
scenes while offering significant advantages in style transfer speed and
consistency.



---

## A Review of 3D Reconstruction Techniques for Deformable Tissues in  Robotic Surgery

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Mengya Xu, Ziqi Guo, An Wang, Long Bai, Hongliang Ren | cs.CV | [PDF](http://arxiv.org/pdf/2408.04426v1){: .btn .btn-green } |

**Abstract**: As a crucial and intricate task in robotic minimally invasive surgery,
reconstructing surgical scenes using stereo or monocular endoscopic video holds
immense potential for clinical applications. NeRF-based techniques have
recently garnered attention for the ability to reconstruct scenes implicitly.
On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly
using 3D Gaussians and projects them onto a 2D plane as a replacement for the
complex volume rendering in NeRF. However, these methods face challenges
regarding surgical scene reconstruction, such as slow inference, dynamic
scenes, and surgical tool occlusion. This work explores and reviews
state-of-the-art (SOTA) approaches, discussing their innovations and
implementation principles. Furthermore, we replicate the models and conduct
testing and evaluation on two datasets. The test results demonstrate that with
advancements in these techniques, achieving real-time, high-quality
reconstructions becomes feasible.

Comments:
- To appear in MICCAI 2024 EARTH Workshop. Code availability:
  https://github.com/Epsilon404/surgicalnerf

---

## Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs  Gaussian-Based Methods

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Yiming Zhou, Zixuan Zeng, Andi Chen, Xiaofan Zhou, Haowei Ni, Shiyao Zhang, Panfeng Li, Liangxi Liu, Mengyao Zheng, Xupeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2408.04268v1){: .btn .btn-green } |

**Abstract**: Exploring the capabilities of Neural Radiance Fields (NeRF) and
Gaussian-based methods in the context of 3D scene reconstruction, this study
contrasts these modern approaches with traditional Simultaneous Localization
and Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we
assess performance based on tracking accuracy, mapping fidelity, and view
synthesis. Findings reveal that NeRF excels in view synthesis, offering unique
capabilities in generating new perspectives from existing data, albeit at
slower processing speeds. Conversely, Gaussian-based methods provide rapid
processing and significant expressiveness but lack comprehensive scene
completion. Enhanced by global optimization and loop closure techniques, newer
methods like NICE-SLAM and SplaTAM not only surpass older frameworks such as
ORB-SLAM2 in terms of robustness but also demonstrate superior performance in
dynamic and complex environments. This comparative analysis bridges theoretical
research with practical implications, shedding light on future developments in
robust 3D scene reconstruction across various real-world applications.

Comments:
- Accepted by 2024 6th International Conference on Data-driven
  Optimization of Complex Systems

---

## PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time  High-Quality Relighting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Yijia Guo, Yuanxi Bai, Liwen Hu, Ziyi Guo, Mianzhi Liu, Yu Cai, Tiejun Huang, Lei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2408.03538v1){: .btn .btn-green } |

**Abstract**: We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a
real-time high-quality relighting method for Gaussian splats in low-frequency
lighting environments that captures soft shadows and interreflections by
precomputing 3D Gaussian splats' radiance transfer. Existing studies have
demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'
efficiency for dynamic lighting scenarios. However, the current relighting
method based on 3DGS still struggles to compute high-quality shadow and
indirect illumination in real time for dynamic light, leading to unrealistic
rendering results. We solve this problem by precomputing the expensive
transport simulations required for complex transfer functions like shadowing,
the resulting transfer functions are represented as dense sets of vectors or
matrices for every Gaussian splat. We introduce distinct precomputing methods
tailored for training and rendering stages, along with unique ray tracing and
indirect lighting precomputation techniques for 3D Gaussian splats to
accelerate training speed and compute accurate indirect lighting related to
environment light. Experimental analyses demonstrate that our approach achieves
state-of-the-art visual quality while maintaining competitive training times
and allows high-quality real-time (30+ fps) relighting for dynamic light and
relatively complex scenes at 1080p resolution.



---

## Goal-oriented Semantic Communication for the Metaverse Application

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Zhe Wang, Nan Li, Yansha Deng | eess.SY | [PDF](http://arxiv.org/pdf/2408.03646v1){: .btn .btn-green } |

**Abstract**: With the emergence of the metaverse and its role in enabling real-time
simulation and analysis of real-world counterparts, an increasing number of
personalized metaverse scenarios are being created to influence entertainment
experiences and social behaviors. However, compared to traditional image and
video entertainment applications, the exact transmission of the vast amount of
metaverse-associated information significantly challenges the capacity of
existing bit-oriented communication networks. Moreover, the current metaverse
also witnesses a growing goal shift for transmitting the meaning behind
custom-designed content, such as user-designed buildings and avatars, rather
than exact copies of physical objects. To meet this growing goal shift and
bandwidth challenge, this paper proposes a goal-oriented semantic communication
framework for metaverse application (GSCM) to explore and define semantic
information through the goal levels. Specifically, we first analyze the
traditional image communication framework in metaverse construction and then
detail our proposed semantic information along with the end-to-end wireless
communication. We then describe the designed modules of the GSCM framework,
including goal-oriented semantic information extraction, base knowledge
definition, and neural radiance field (NeRF) based metaverse construction.
Finally, numerous experiments have been conducted to demonstrate that, compared
to image communication, our proposed GSCM framework decreases transmission
latency by up to 92.6% and enhances the virtual object operation accuracy and
metaverse construction clearance by up to 45.6% and 44.7%, respectively.



---

## 3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Zhe Jun Tang, Tat-Jen Cham | cs.CV | [PDF](http://arxiv.org/pdf/2408.03753v1){: .btn .btn-green } |

**Abstract**: The use of 3D Gaussians as representation of radiance fields has enabled high
quality novel view synthesis at real-time rendering speed. However, the choice
of optimising the outgoing radiance of each Gaussian independently as spherical
harmonics results in unsatisfactory view dependent effects. In response to
these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian
Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering
quality. Instead of optimising a single outgoing radiance parameter, 3iGS
enhances 3DGS view-dependent effects by expressing the outgoing radiance as a
function of a local illumination field and Bidirectional Reflectance
Distribution Function (BRDF) features. We optimise a continuous incident
illumination field through a Tensorial Factorisation representation, while
separately fine-tuning the BRDF features of each 3D Gaussian relative to this
illumination field. Our methodology significantly enhances the rendering
quality of specular view-dependent effects of 3DGS, while maintaining rapid
training and rendering speeds.

Comments:
- The 18th European Conference on Computer Vision ECCV 2024

---

## Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2408.03822v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has recently emerged as an alternative
representation that leverages a 3D Gaussian-based representation and introduces
an approximated volumetric rendering, achieving very fast rendering speed and
promising image quality. Furthermore, subsequent studies have successfully
extended 3DGS to dynamic 3D scenes, demonstrating its wide range of
applications. However, a significant drawback arises as 3DGS and its following
methods entail a substantial number of Gaussians to maintain the high fidelity
of the rendered images, which requires a large amount of memory and storage. To
address this critical issue, we place a specific emphasis on two key
objectives: reducing the number of Gaussian points without sacrificing
performance and compressing the Gaussian attributes, such as view-dependent
color and covariance. To this end, we propose a learnable mask strategy that
significantly reduces the number of Gaussians while preserving high
performance. In addition, we propose a compact but effective representation of
view-dependent color by employing a grid-based neural field rather than relying
on spherical harmonics. Finally, we learn codebooks to compactly represent the
geometric and temporal attributes by residual vector quantization. With model
compression techniques such as quantization and entropy coding, we consistently
show over 25x reduced storage and enhanced rendering speed compared to 3DGS for
static scenes, while maintaining the quality of the scene representation. For
dynamic scenes, our approach achieves more than 12x storage efficiency and
retains a high-quality reconstruction compared to the existing state-of-the-art
methods. Our work provides a comprehensive framework for 3D scene
representation, achieving high performance, fast training, compactness, and
real-time rendering. Our project page is available at
https://maincold2.github.io/c3dgs/.

Comments:
- Project page: https://maincold2.github.io/c3dgs/

---

## Towards Real-Time Gaussian Splatting: Accelerating 3DGS through  Photometric SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Yan Song Hu, Dayou Mao, Yuhao Chen, John Zelek | cs.RO | [PDF](http://arxiv.org/pdf/2408.03825v1){: .btn .btn-green } |

**Abstract**: Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous
Localization and Mapping (VSLAM) demonstrate the generation of high-quality
volumetric reconstructions from monocular video streams. However, despite these
promising advancements, current 3DGS integrations have reduced tracking
performance and lower operating speeds compared to traditional VSLAM. To
address these issues, we propose integrating 3DGS with Direct Sparse Odometry,
a monocular photometric SLAM system. We have done preliminary experiments
showing that using Direct Sparse Odometry point cloud outputs, as opposed to
standard structure-from-motion methods, significantly shortens the training
time needed to achieve high-quality renders. Reducing 3DGS training time
enables the development of 3DGS-integrated SLAM systems that operate in
real-time on mobile hardware. These promising initial findings suggest further
exploration is warranted in combining traditional VSLAM systems with 3DGS.

Comments:
- This extended abstract has been submitted to be presented at an IEEE
  conference. It will be made available online by IEEE but will not be
  published in IEEE Xplore. Copyright may be transferred without notice, after
  which this version may no longer be accessible

---

## LumiGauss: High-Fidelity Outdoor Relighting with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, Marek Kowalski | cs.CV | [PDF](http://arxiv.org/pdf/2408.04474v1){: .btn .btn-green } |

**Abstract**: Decoupling lighting from geometry using unconstrained photo collections is
notoriously challenging. Solving it would benefit many users, as creating
complex 3D assets takes days of manual labor. Many previous works have
attempted to address this issue, often at the expense of output fidelity, which
questions the practicality of such methods.
  We introduce LumiGauss, a technique that tackles 3D reconstruction of scenes
and environmental lighting through 2D Gaussian Splatting. Our approach yields
high-quality scene reconstructions and enables realistic lighting synthesis
under novel environment maps. We also propose a method for enhancing the
quality of shadows, common in outdoor scenes, by exploiting spherical harmonics
properties. Our approach facilitates seamless integration with game engines and
enables the use of fast precomputed radiance transfer.
  We validate our method on the NeRF-OSR dataset, demonstrating superior
performance over baseline methods. Moreover, LumiGauss can synthesize realistic
images when applying novel environment maps.

Comments:
- Includes video files in src

---

## RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel  View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic | cs.CV | [PDF](http://arxiv.org/pdf/2408.03356v1){: .btn .btn-green } |

**Abstract**: Differentiable volumetric rendering-based methods made significant progress
in novel view synthesis. On one hand, innovative methods have replaced the
Neural Radiance Fields (NeRF) network with locally parameterized structures,
enabling high-quality renderings in a reasonable time. On the other hand,
approaches have used differentiable splatting instead of NeRF's ray casting to
optimize radiance fields rapidly using Gaussian kernels, allowing for fine
adaptation to the scene. However, differentiable ray casting of irregularly
spaced kernels has been scarcely explored, while splatting, despite enabling
fast rendering times, is susceptible to clearly visible artifacts.
  Our work closes this gap by providing a physically consistent formulation of
the emitted radiance c and density {\sigma}, decomposed with Gaussian functions
associated with Spherical Gaussians/Harmonics for all-frequency colorimetric
representation. We also introduce a method enabling differentiable ray casting
of irregularly distributed Gaussians using an algorithm that integrates
radiance fields slab by slab and leverages a BVH structure. This allows our
approach to finely adapt to the scene while avoiding splatting artifacts. As a
result, we achieve superior rendering quality compared to the state-of-the-art
while maintaining reasonable training times and achieving inference speeds of
25 FPS on the Blender dataset. Project page with videos and code:
https://raygauss.github.io/

Comments:
- Project page with videos and code: https://raygauss.github.io/

---

## MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Tengfei Wang, Zongqian Zhan, Rui Xia, Linxia Ji, Xin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.03060v1){: .btn .btn-green } |

**Abstract**: Over the last few decades, image-based building surface reconstruction has
garnered substantial research interest and has been applied across various
fields, such as heritage preservation, architectural planning, etc. Compared to
the traditional photogrammetric and NeRF-based solutions, recently, Gaussian
fields-based methods have exhibited significant potential in generating surface
meshes due to their time-efficient training and detailed 3D information
preservation. However, most gaussian fields-based methods are trained with all
image pixels, encompassing building and nonbuilding areas, which results in a
significant noise for building meshes and degeneration in time efficiency. This
paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to
generate accurate surface reconstruction for building in a time-efficient way.
The framework first applies EfficientSAM and COLMAP to generate multi-level
masks of building and the corresponding masked point clouds. Subsequently, the
masked gaussian fields are trained by integrating two innovative losses: a
multi-level perceptual masked loss focused on constructing building regions and
a boundary loss aimed at enhancing the details of the boundaries between
different masks. Finally, we improve the tetrahedral surface mesh extraction
method based on the masked gaussian spheres. Comprehensive experiments on UAV
images demonstrate that, compared to the traditional method and several
NeRF-based and Gaussian-based SOTA solutions, our approach significantly
improves both the accuracy and efficiency of building surface reconstruction.
Notably, as a byproduct, there is an additional gain in the novel view
synthesis of building.



---

## Efficient NeRF Optimization -- Not All Samples Remain Equally Hard

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Juuso Korhonen, Goutham Rangu, Hamed R. Tavakoli, Juho Kannala | cs.CV | [PDF](http://arxiv.org/pdf/2408.03193v1){: .btn .btn-green } |

**Abstract**: We propose an application of online hard sample mining for efficient training
of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality
for many 3D reconstruction and rendering tasks but require substantial
computational resources. The encoding of the scene information within the NeRF
network parameters necessitates stochastic sampling. We observe that during the
training, a major part of the compute time and memory usage is spent on
processing already learnt samples, which no longer affect the model update
significantly. We identify the backward pass on the stochastic samples as the
computational bottleneck during the optimization. We thus perform the first
forward pass in inference mode as a relatively low-cost search for hard
samples. This is followed by building the computational graph and updating the
NeRF network parameters using only the hard samples. To demonstrate the
effectiveness of the proposed approach, we apply our method to Instant-NGP,
resulting in significant improvements of the view-synthesis quality over the
baseline (1 dB improvement on average per training time, or 2x speedup to reach
the same PSNR level) along with approx. 40% memory savings coming from using
only the hard samples to build the computational graph. As our method only
interfaces with the network module, we expect it to be widely applicable.



---

## PanicleNeRF: low-cost, high-precision in-field phenotypingof rice  panicles with smartphone

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-04 | Xin Yang, Xuqi Lu, Pengyao Xie, Ziyue Guo, Hui Fang, Haowei Fu, Xiaochun Hu, Zhenbiao Sun, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2408.02053v1){: .btn .btn-green } |

**Abstract**: The rice panicle traits significantly influence grain yield, making them a
primary target for rice phenotyping studies. However, most existing techniques
are limited to controlled indoor environments and difficult to capture the rice
panicle traits under natural growth conditions. Here, we developed PanicleNeRF,
a novel method that enables high-precision and low-cost reconstruction of rice
panicle three-dimensional (3D) models in the field using smartphone. The
proposed method combined the large model Segment Anything Model (SAM) and the
small model You Only Look Once version 8 (YOLOv8) to achieve high-precision
segmentation of rice panicle images. The NeRF technique was then employed for
3D reconstruction using the images with 2D segmentation. Finally, the resulting
point clouds are processed to successfully extract panicle traits. The results
show that PanicleNeRF effectively addressed the 2D image segmentation task,
achieving a mean F1 Score of 86.9% and a mean Intersection over Union (IoU) of
79.8%, with nearly double the boundary overlap (BO) performance compared to
YOLOv8. As for point cloud quality, PanicleNeRF significantly outperformed
traditional SfM-MVS (structure-from-motion and multi-view stereo) methods, such
as COLMAP and Metashape. The panicle length was then accurately extracted with
the rRMSE of 2.94% for indica and 1.75% for japonica rice. The panicle volume
estimated from 3D point clouds strongly correlated with the grain number (R2 =
0.85 for indica and 0.82 for japonica) and grain mass (0.80 for indica and 0.76
for japonica). This method provides a low-cost solution for high-throughput
in-field phenotyping of rice panicles, accelerating the efficiency of rice
breeding.



---

## E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-03 | Yunshan Qi, Jia Li, Yifan Zhao, Yu Zhang, Lin Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2408.01840v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieve impressive rendering performance by
learning volumetric 3D representation from several images of different views.
However, it is difficult to reconstruct a sharp NeRF from blurry input as it
often occurs in the wild. To solve this problem, we propose a novel Efficient
Event-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and
event streams. To effectively introduce event streams into the neural
volumetric representation learning process, we propose an event-enhanced blur
rendering loss and an event rendering loss, which guide the network via
modeling the real blur process and event generation process, respectively.
Specifically, we leverage spatial-temporal information from the event stream to
evenly distribute learning attention over temporal blur while simultaneously
focusing on blurry texture through the spatial attention. Moreover, a camera
pose estimation framework for real-world data is built with the guidance of the
events to generalize the method to practical applications. Compared to previous
image-based or event-based NeRF, our framework makes more profound use of the
internal relationship between events and images. Extensive experiments on both
synthetic data and real-world data demonstrate that E$^3$NeRF can effectively
learn a sharp NeRF from blurry images, especially in non-uniform motion and
low-light scenes.



---

## FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and  Fisheye Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-03 | Yifan Wu, Tianyi Cheng, Peixu Xin, Janusz Konrad | cs.CV | [PDF](http://arxiv.org/pdf/2408.01878v1){: .btn .btn-green } |

**Abstract**: Previous studies aiming to optimize and bundle-adjust camera poses using
Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated
impressive capabilities in 3D scene reconstruction. However, these approaches
have been designed for pinhole-camera pose optimization and do not perform well
under radial image distortions such as those in fisheye cameras. Furthermore,
inaccurate depth initialization in DBARF results in erroneous geometric
information affecting the overall convergence and quality of results. In this
paper, we propose adaptive GRUs with a flexible bundle-adjustment method
adapted to radial distortions and incorporate feature-based recurrent neural
networks to generate continuous novel views from fisheye datasets. Other NeRF
methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray
distance loss for distorted pose refinement, causing severe artifacts, long
rendering time, and are difficult to use in downstream tasks, where the dense
voxel representation generated by a NeRF method needs to be converted into a
mesh representation. We also address depth initialization issues by adding
MiDaS-based depth priors for pinhole images. Through extensive experiments, we
demonstrate the generalization capacity of FBINeRF and show high-fidelity
results for both pinhole-camera and fisheye-camera NeRFs.

Comments:
- 18 pages

---

## IG-SLAM: Instant Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | F. Aykut Sarikamis, A. Aydin Alatan | cs.CV | [PDF](http://arxiv.org/pdf/2408.01126v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently shown promising results as an alternative
scene representation in SLAM systems to neural implicit representations.
However, current methods either lack dense depth maps to supervise the mapping
process or detailed training designs that consider the scale of the
environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only
SLAM system that employs robust Dense-SLAM methods for tracking and combines
them with Gaussian Splatting. A 3D map of the environment is constructed using
accurate pose and dense depth provided by tracking. Additionally, we utilize
depth uncertainty in map optimization to improve 3D reconstruction. Our decay
strategy in map optimization enhances convergence and allows the system to run
at 10 fps in a single process. We demonstrate competitive performance with
state-of-the-art RGB-only SLAM systems while achieving faster operation speeds.
We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC
datasets. The system achieves photo-realistic 3D reconstruction in large-scale
sequences, particularly in the EuRoC dataset.

Comments:
- 8 pages, 3 page ref, 5 figures

---

## A General Framework to Boost 3D GS Initialization for Text-to-3D  Generation by Lexical Richness


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Lutao Jiang, Hangyu Li, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.01269v1){: .btn .btn-green } |

**Abstract**: Text-to-3D content creation has recently received much attention, especially
with the prevalence of 3D Gaussians Splatting. In general, GS-based methods
comprise two key stages: initialization and rendering optimization. To achieve
initialization, existing works directly apply random sphere initialization or
3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such
strategies suffer from two critical yet challenging problems: 1) the final
shapes are still similar to the initial ones even after training; 2) shapes can
be produced only from simple texts, e.g., "a dog", not for lexically richer
texts, e.g., "a dog is sitting on the top of the airplane". To address these
problems, this paper proposes a novel general framework to boost the 3D GS
Initialization for text-to-3D generation upon the lexical richness. Our key
idea is to aggregate 3D Gaussians into spatially uniform voxels to represent
complex shapes while enabling the spatial interaction among the 3D Gaussians
and semantic interaction between Gaussians and texts. Specifically, we first
construct a voxelized representation, where each voxel holds a 3D Gaussian with
its position, scale, and rotation fixed while setting opacity as the sole
factor to determine a position's occupancy. We then design an initialization
network mainly consisting of two novel components: 1) Global Information
Perception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design
enables each 3D Gaussian to assimilate the spatial information from other areas
and semantic information from texts. Extensive experiments show the superiority
of our framework of high-quality 3D GS initialization against the existing
methods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.
Also, our framework can be seamlessly plugged into SoTA training frameworks,
e.g., LucidDreamer, for semantically consistent text-to-3D generation.



---

## NeRFoot: Robot-Footprint Estimation for Image-Based Visual Servoing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Daoxin Zhong, Luke Robinson, Daniele De Martini | cs.RO | [PDF](http://arxiv.org/pdf/2408.01251v1){: .btn .btn-green } |

**Abstract**: This paper investigates the utility of Neural Radiance Fields (NeRF) models
in extending the regions of operation of a mobile robot, controlled by
Image-Based Visual Servoing (IBVS) via static CCTV cameras. Using NeRF as a
3D-representation prior, the robot's footprint may be extrapolated
geometrically and used to train a CNN-based network to extract it online from
the robot's appearance alone. The resulting footprint results in a tighter
bound than a robot-wide bounding box, allowing the robot's controller to
prescribe more optimal trajectories and expand its safe operational floor area.



---

## Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation  with Volumetric Visual Data Fusion


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Ke Li, Reinhard Bacher, Susanne Schmidt, Wim Leemans, Frank Steinicke | cs.RO | [PDF](http://arxiv.org/pdf/2408.01225v1){: .btn .btn-green } |

**Abstract**: We introduce Reality Fusion, a novel robot teleoperation system that
localizes, streams, projects, and merges a typical onboard depth sensor with a
photorealistic, high resolution, high framerate, and wide field of view (FoV)
rendering of the complex remote environment represented as 3D Gaussian splats
(3DGS). Our framework enables robust egocentric and exocentric robot
teleoperation in immersive VR, with the 3DGS effectively extending spatial
information of a depth sensor with limited FoV and balancing the trade-off
between data streaming costs and data visual quality. We evaluated our
framework through a user study with 24 participants, which revealed that
Reality Fusion leads to significantly better user performance, situation
awareness, and user preferences. To support further research and development,
we provide an open-source implementation with an easy-to-replicate custom-made
telepresence robot, a high-performance virtual reality 3DGS renderer, and an
immersive robot control package. (Source code:
https://github.com/uhhhci/RealityFusion)

Comments:
- Accepted, to appear at IROS 2024

---

## UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with  Ultrasound Reflection Direction Parameterization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Ziwen Guo, Zi Fang, Zhuang Fu | cs.AI | [PDF](http://arxiv.org/pdf/2408.00860v2){: .btn .btn-green } |

**Abstract**: Three-dimensional ultrasound imaging is a critical technology widely used in
medical diagnostics. However, traditional 3D ultrasound imaging methods have
limitations such as fixed resolution, low storage efficiency, and insufficient
contextual connectivity, leading to poor performance in handling complex
artifacts and reflection characteristics. Recently, techniques based on NeRF
(Neural Radiance Fields) have made significant progress in view synthesis and
3D reconstruction, but there remains a research gap in high-quality ultrasound
imaging. To address these issues, we propose a new model, UlRe-NeRF, which
combines implicit neural networks and explicit ultrasound volume rendering into
an ultrasound neural rendering architecture. This model incorporates reflection
direction parameterization and harmonic encoding, using a directional MLP
module to generate view-dependent high-frequency reflection intensity
estimates, and a spatial MLP module to produce the medium's physical property
parameters. These parameters are used in the volume rendering process to
accurately reproduce the propagation and reflection behavior of ultrasound
waves in the medium. Experimental results demonstrate that the UlRe-NeRF model
significantly enhances the realism and accuracy of high-fidelity ultrasound
image reconstruction, especially in handling complex medium structures.



---

## LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Zhenyu Bao, Guibiao Liao, Kaichen Zhou, Kanglin Liu, Qing Li, Guoping Qiu | cs.CV | [PDF](http://arxiv.org/pdf/2408.00254v1){: .btn .btn-green } |

**Abstract**: Despite the photorealistic novel view synthesis (NVS) performance achieved by
the original 3D Gaussian splatting (3DGS), its rendering quality significantly
degrades with sparse input views. This performance drop is mainly caused by the
limited number of initial points generated from the sparse input, insufficient
supervision during the training process, and inadequate regularization of the
oversized Gaussian ellipsoids. To handle these issues, we propose the
LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis
task. In specific, we propose a loop-based Progressive Gaussian Initialization
(PGI) strategy that could iteratively densify the initialized point cloud using
the rendered pseudo images during the training process. Then, the sparse and
reliable depth from the Structure from Motion, and the window-based dense
monocular depth are leveraged to provide precise geometric supervision via the
proposed Depth-alignment Regularization (DAR). Additionally, we introduce a
novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian
ellipsoids leading to large pixel errors. Comprehensive experiments on four
datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art
methods for sparse-input novel view synthesis, across indoor, outdoor, and
object-level scenes with various image resolutions.

Comments:
- 13 pages, 10 figures
