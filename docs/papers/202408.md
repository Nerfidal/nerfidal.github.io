---
layout: default
title: August 2024
parent: Papers
nav_order: 202408
---

<!---metadata--->


## Subsurface Scattering for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-22 | Jan-Niklas Dihlmann, Arjun Majumdar, Andreas Engelhardt, Raphael Braun, Hendrik P. A. Lensch | cs.CV | [PDF](http://arxiv.org/pdf/2408.12282v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction and relighting of objects made from scattering materials
present a significant challenge due to the complex light transport beneath the
surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at
real-time speeds. While 3D Gaussians efficiently approximate an object's
surface, they fail to capture the volumetric properties of subsurface
scattering. We propose a framework for optimizing an object's shape together
with the radiance transfer field given multi-view OLAT (one light at a time)
data. Our method decomposes the scene into an explicit surface represented as
3D Gaussians, with a spatially varying BRDF, and an implicit volumetric
representation of the scattering component. A learned incident light field
accounts for shadowing. We optimize all parameters jointly via ray-traced
differentiable rendering. Our approach enables material editing, relighting and
novel view synthesis at interactive rates. We show successful application on
synthetic data and introduce a newly acquired multi-view multi-light dataset of
objects in a light-stage setup. Compared to previous work we achieve comparable
or better results at a fraction of optimization and rendering time while
enabling detailed control over material attributes. Project page
https://sss.jdihlmann.com/

Comments:
- Project page: https://sss.jdihlmann.com/

---

## Irregularity Inspection using Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Tianqi Ding, Dawei Xiang | cs.CV | [PDF](http://arxiv.org/pdf/2408.11251v1){: .btn .btn-green } |

**Abstract**: With the increasing growth of industrialization, more and more industries are
relying on machine automation for production. However, defect detection in
large-scale production machinery is becoming increasingly important. Due to
their large size and height, it is often challenging for professionals to
conduct defect inspections on such large machinery. For example, the inspection
of aging and misalignment of components on tall machinery like towers requires
companies to assign dedicated personnel. Employees need to climb the towers and
either visually inspect or take photos to detect safety hazards in these large
machines. Direct visual inspection is limited by its low level of automation,
lack of precision, and safety concerns associated with personnel climbing the
towers. Therefore, in this paper, we propose a system based on neural network
modeling (NeRF) of 3D twin models. By comparing two digital models, this system
enables defect detection at the 3D interface of an object.



---

## Pano2Room: Novel View Synthesis from a Single Indoor Panorama

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Guo Pu, Yiming Zhao, Zhouhui Lian | cs.CV | [PDF](http://arxiv.org/pdf/2408.11413v1){: .btn .btn-green } |

**Abstract**: Recent single-view 3D generative methods have made significant advancements
by leveraging knowledge distilled from extensive 3D object datasets. However,
challenges persist in the synthesis of 3D scenes from a single view, primarily
due to the complexity of real-world environments and the limited availability
of high-quality prior resources. In this paper, we introduce a novel approach
called Pano2Room, designed to automatically reconstruct high-quality 3D indoor
scenes from a single panoramic image. These panoramic images can be easily
generated using a panoramic RGBD inpainter from captures at a single location
with any camera. The key idea is to initially construct a preliminary mesh from
the input panorama, and iteratively refine this mesh using a panoramic RGBD
inpainter while collecting photo-realistic 3D-consistent pseudo novel views.
Finally, the refined mesh is converted into a 3D Gaussian Splatting field and
trained with the collected pseudo novel views. This pipeline enables the
reconstruction of real-world 3D scenes, even in the presence of large
occlusions, and facilitates the synthesis of photo-realistic novel views with
detailed geometry. Extensive qualitative and quantitative experiments have been
conducted to validate the superiority of our method in single-panorama indoor
novel synthesis compared to the state-of-the-art. Our code and data are
available at \url{https://github.com/TrickyGo/Pano2Room}.

Comments:
- SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers '24),
  December 3--6, 2024, Tokyo, Japan

---

## GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation  with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Wanshui Gan, Fang Liu, Hongbin Xu, Ningkai Mo, Naoto Yokoya | cs.CV | [PDF](http://arxiv.org/pdf/2408.11447v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianOcc, a systematic method that investigates the two
usages of Gaussian splatting for fully self-supervised and efficient 3D
occupancy estimation in surround views. First, traditional methods for
self-supervised 3D occupancy estimation still require ground truth 6D poses
from sensors during training. To address this limitation, we propose Gaussian
Splatting for Projection (GSP) module to provide accurate scale information for
fully self-supervised training from adjacent view projection. Additionally,
existing methods rely on volume rendering for final 3D voxel representation
learning using 2D signals (depth maps, semantic maps), which is both
time-consuming and less effective. We propose Gaussian Splatting from Voxel
space (GSV) to leverage the fast rendering properties of Gaussian splatting. As
a result, the proposed GaussianOcc method enables fully self-supervised (no
ground truth pose) 3D occupancy estimation in competitive performance with low
computational cost (2.7 times faster in training and 5 times faster in
rendering).

Comments:
- Project page: https://ganwanshui.github.io/GaussianOcc/

---

## Visual Localization in 3D Maps: Comparing Point Cloud, Mesh, and NeRF  Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Lintong Zhang, Yifu Tao, Jiarong Lin, Fu Zhang, Maurice Fallon | cs.CV | [PDF](http://arxiv.org/pdf/2408.11966v1){: .btn .btn-green } |

**Abstract**: This paper introduces and assesses a cross-modal global visual localization
system that can localize camera images within a color 3D map representation
built using both visual and lidar sensing. We present three different
state-of-the-art methods for creating the color 3D maps: point clouds, meshes,
and neural radiance fields (NeRF). Our system constructs a database of
synthetic RGB and depth image pairs from these representations. This database
serves as the basis for global localization. We present an automatic approach
that builds this database by synthesizing novel images of the scene and
exploiting the 3D structure encoded in the different representations. Next, we
present a global localization system that relies on the synthetic image
database to accurately estimate the 6 DoF camera poses of monocular query
images. Our localization approach relies on different learning-based global
descriptors and feature detectors which enable robust image retrieval and
matching despite the domain gap between (real) query camera images and the
synthetic database images. We assess the system's performance through extensive
real-world experiments in both indoor and outdoor settings, in order to
evaluate the effectiveness of each map representation and the benefits against
traditional structure-from-motion localization approaches. Our results show
that all three map representations can achieve consistent localization success
rates of 55% and higher across various environments. NeRF synthesized images
show superior performance, localizing query images at an average success rate
of 72%. Furthermore, we demonstrate that our synthesized database enables
global localization even when the map creation data and the localization
sequence are captured when travelling in opposite directions. Our system,
operating in real-time on a mobile laptop equipped with a GPU, achieves a
processing rate of 1Hz.



---

## Robust 3D Gaussian Splatting for Novel View Synthesis in Presence of  Distractors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Paul Ungermann, Armin Ettenhofer, Matthias Nie√üner, Barbara Roessle | cs.CV | [PDF](http://arxiv.org/pdf/2408.11697v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has shown impressive novel view synthesis results;
nonetheless, it is vulnerable to dynamic objects polluting the input data of an
otherwise static scene, so called distractors. Distractors have severe impact
on the rendering quality as they get represented as view-dependent effects or
result in floating artifacts. Our goal is to identify and ignore such
distractors during the 3D Gaussian optimization to obtain a clean
reconstruction. To this end, we take a self-supervised approach that looks at
the image residuals during the optimization to determine areas that have likely
been falsified by a distractor. In addition, we leverage a pretrained
segmentation network to provide object awareness, enabling more accurate
exclusion of distractors. This way, we obtain segmentation masks of distractors
to effectively ignore them in the loss formulation. We demonstrate that our
approach is robust to various distractors and strongly improves rendering
quality on distractor-polluted scenes, improving PSNR by 1.86dB compared to 3D
Gaussian Splatting.

Comments:
- GCPR 2024, Project Page:
  https://paulungermann.github.io/Robust3DGaussians , Video:
  https://www.youtube.com/watch?v=P9unyR7yK3E

---

## DeRainGS: Gaussian Splatting for Enhanced Scene Reconstruction in Rainy  Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Shuhong Liu, Xiang Chen, Hongming Chen, Quanfeng Xu, Mingrui Li | cs.CV | [PDF](http://arxiv.org/pdf/2408.11540v2){: .btn .btn-green } |

**Abstract**: Reconstruction under adverse rainy conditions poses significant challenges
due to reduced visibility and the distortion of visual perception. These
conditions can severely impair the quality of geometric maps, which is
essential for applications ranging from autonomous planning to environmental
monitoring. In response to these challenges, this study introduces the novel
task of 3D Reconstruction in Rainy Environments (3DRRE), specifically designed
to address the complexities of reconstructing 3D scenes under rainy conditions.
To benchmark this task, we construct the HydroViews dataset that comprises a
diverse collection of both synthesized and real-world scene images
characterized by various intensities of rain streaks and raindrops.
Furthermore, we propose DeRainGS, the first 3DGS method tailored for
reconstruction in adverse rainy environments. Extensive experiments across a
wide range of rain scenarios demonstrate that our method delivers
state-of-the-art performance, remarkably outperforming existing occlusion-free
methods.



---

## DEGAS: Detailed Expressions on Full-Body Gaussian Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Zhijing Shao, Duotun Wang, Qing-Yao Tian, Yao-Dong Yang, Hengyu Meng, Zeyu Cai, Bo Dong, Yu Zhang, Kang Zhang, Zeyu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.10588v1){: .btn .btn-green } |

**Abstract**: Although neural rendering has made significant advancements in creating
lifelike, animatable full-body and head avatars, incorporating detailed
expressions into full-body avatars remains largely unexplored. We present
DEGAS, the first 3D Gaussian Splatting (3DGS)-based modeling method for
full-body avatars with rich facial expressions. Trained on multiview videos of
a given subject, our method learns a conditional variational autoencoder that
takes both the body motion and facial expression as driving signals to generate
Gaussian maps in the UV layout. To drive the facial expressions, instead of the
commonly used 3D Morphable Models (3DMMs) in 3D head avatars, we propose to
adopt the expression latent space trained solely on 2D portrait images,
bridging the gap between 2D talking faces and 3D avatars. Leveraging the
rendering capability of 3DGS and the rich expressiveness of the expression
latent space, the learned avatars can be reenacted to reproduce photorealistic
rendering images with subtle and accurate facial expressions. Experiments on an
existing dataset and our newly proposed dataset of full-body talking avatars
demonstrate the efficacy of our method. We also propose an audio-driven
extension of our method with the help of 2D talking faces, opening new
possibilities to interactive AI agents.



---

## Learning Part-aware 3D Representations by Fusing 2D Gaussians and  Superquadrics

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2408.10789v1){: .btn .btn-green } |

**Abstract**: Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D
Gaussians, are commonly used to represent 3D objects or scenes. However, humans
usually perceive 3D objects or scenes at a higher level as a composition of
parts or structures rather than points or voxels. Representing 3D as semantic
parts can benefit further understanding and applications. We aim to solve
part-aware 3D reconstruction, which parses objects or scenes into semantic
parts. In this paper, we introduce a hybrid representation of superquadrics and
2D Gaussians, trying to dig 3D structural clues from multi-view image inputs.
Accurate structured geometry reconstruction and high-quality rendering are
achieved at the same time. We incorporate parametric superquadrics in mesh
forms into 2D Gaussians by attaching Gaussian centers to faces in meshes.
During the training, superquadrics parameters are iteratively optimized, and
Gaussians are deformed accordingly, resulting in an efficient hybrid
representation. On the one hand, this hybrid representation inherits the
advantage of superquadrics to represent different shape primitives, supporting
flexible part decomposition of scenes. On the other hand, 2D Gaussians are
incorporated to model the complex texture and geometry details, ensuring
high-quality rendering and geometry reconstruction. The reconstruction is fully
unsupervised. We conduct extensive experiments on data from DTU and ShapeNet
datasets, in which the method decomposes scenes into reasonable parts,
outperforming existing state-of-the-art approaches.



---

## GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Zirui Wang, Ming Cheng, Victor Adrian Prisacariu, Tristan Braud | cs.CV | [PDF](http://arxiv.org/pdf/2408.11085v1){: .btn .btn-green } |

**Abstract**: We leverage 3D Gaussian Splatting (3DGS) as a scene representation and
propose a novel test-time camera pose refinement framework, GSLoc. This
framework enhances the localization accuracy of state-of-the-art absolute pose
regression and scene coordinate regression methods. The 3DGS model renders
high-quality synthetic images and depth maps to facilitate the establishment of
2D-3D correspondences. GSLoc obviates the need for training feature extractors
or descriptors by operating directly on RGB images, utilizing the 3D vision
foundation model, MASt3R, for precise 2D matching. To improve the robustness of
our model in challenging outdoor environments, we incorporate an
exposure-adaptive module within the 3DGS framework. Consequently, GSLoc enables
efficient pose refinement given a single RGB query and a coarse initial pose
estimation. Our proposed approach surpasses leading NeRF-based optimization
methods in both accuracy and runtime across indoor and outdoor visual
localization benchmarks, achieving state-of-the-art accuracy on two indoor
datasets.

Comments:
- The project page is available at https://gsloc.active.vision

---

## ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their  Self-Supervised Pretraining

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Qi Ma, Yue Li, Bin Ren, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2408.10906v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become the de facto method of 3D
representation in many vision tasks. This calls for the 3D understanding
directly in this representation space. To facilitate the research in this
direction, we first build a large-scale dataset of 3DGS using the commonly used
ShapeNet and ModelNet datasets. Our dataset ShapeSplat consists of 65K objects
from 87 unique categories, whose labels are in accordance with the respective
datasets. The creation of this dataset utilized the compute equivalent of 2 GPU
years on a TITAN XP GPU.
  We utilize our dataset for unsupervised pretraining and supervised finetuning
for classification and segmentation tasks. To this end, we introduce
\textbf{\textit{Gaussian-MAE}}, which highlights the unique benefits of
representation learning from Gaussian parameters. Through exhaustive
experiments, we provide several valuable insights. In particular, we show that
(1) the distribution of the optimized GS centroids significantly differs from
the uniformly sampled point cloud (used for initialization) counterpart; (2)
this change in distribution results in degradation in classification but
improvement in segmentation tasks when using only the centroids; (3) to
leverage additional Gaussian parameters, we propose Gaussian feature grouping
in a normalized feature space, along with splats pooling layer, offering a
tailored solution to effectively group and embed similar Gaussians, which leads
to notable improvement in finetuning tasks.



---

## TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature  Tracks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Jinjie Mai, Wenxuan Zhu, Sara Rojas, Jesus Zarzar, Abdullah Hamdi, Guocheng Qian, Bing Li, Silvio Giancola, Bernard Ghanem | cs.CV | [PDF](http://arxiv.org/pdf/2408.10739v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) generally require many images with accurate
poses for accurate novel view synthesis, which does not reflect realistic
setups where views can be sparse and poses can be noisy. Previous solutions for
learning NeRFs with sparse views and noisy poses only consider local geometry
consistency with pairs of views. Closely following \textit{bundle adjustment}
in Structure-from-Motion (SfM), we introduce TrackNeRF for more globally
consistent geometry reconstruction and more accurate pose optimization.
TrackNeRF introduces \textit{feature tracks}, \ie connected pixel trajectories
across \textit{all} visible views that correspond to the \textit{same} 3D
points. By enforcing reprojection consistency among feature tracks, TrackNeRF
encourages holistic 3D consistency explicitly. Through extensive experiments,
TrackNeRF sets a new benchmark in noisy and sparse view reconstruction. In
particular, TrackNeRF shows significant improvements over the state-of-the-art
BARF and SPARF by $\sim8$ and $\sim1$ in terms of PSNR on DTU under various
sparse and noisy view setups. The code is available at
\href{https://tracknerf.github.io/}.

Comments:
- ECCV 2024 (supplemental pages included)

---

## $R^2$-Mesh: Reinforcement Learning Powered Mesh Reconstruction via  Geometry and Appearance Refinement

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Haoyang Wang, Liming Liu, Quanlu Jia, Jiangkai Wu, Haodan Zhang, Peiheng Wang, Xinggong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.10135v1){: .btn .btn-green } |

**Abstract**: Mesh reconstruction based on Neural Radiance Fields (NeRF) is popular in a
variety of applications such as computer graphics, virtual reality, and medical
imaging due to its efficiency in handling complex geometric structures and
facilitating real-time rendering. However, existing works often fail to capture
fine geometric details accurately and struggle with optimizing rendering
quality. To address these challenges, we propose a novel algorithm that
progressively generates and optimizes meshes from multi-view images. Our
approach initiates with the training of a NeRF model to establish an initial
Signed Distance Field (SDF) and a view-dependent appearance field.
Subsequently, we iteratively refine the SDF through a differentiable mesh
extraction method, continuously updating both the vertex positions and their
connectivity based on the loss from mesh differentiable rasterization, while
also optimizing the appearance representation. To further leverage
high-fidelity and detail-rich representations from NeRF, we propose an
online-learning strategy based on Upper Confidence Bound (UCB) to enhance
viewpoints by adaptively incorporating images rendered by the initial NeRF
model into the training dataset. Through extensive experiments, we demonstrate
that our method delivers highly competitive and robust performance in both mesh
rendering quality and geometric quality.



---

## DiscoNeRF: Class-Agnostic Object Field for 3D Object Discovery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Corentin Dumery, Aoxiang Fan, Ren Li, Nicolas Talabot, Pascal Fua | cs.CV | [PDF](http://arxiv.org/pdf/2408.09928v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a powerful tool for modeling 3D
scenes from multiple images. However, NeRFs remain difficult to segment into
semantically meaningful regions. Previous approaches to 3D segmentation of
NeRFs either require user interaction to isolate a single object, or they rely
on 2D semantic masks with a limited number of classes for supervision. As a
consequence, they generalize poorly to class-agnostic masks automatically
generated in real scenes. This is attributable to the ambiguity arising from
zero-shot segmentation, yielding inconsistent masks across views. In contrast,
we propose a method that is robust to inconsistent segmentations and
successfully decomposes the scene into a set of objects of any class. By
introducing a limited number of competing object slots against which masks are
matched, a meaningful object representation emerges that best explains the 2D
supervision and minimizes an additional regularization term. Our experiments
demonstrate the ability of our method to generate 3D panoptic segmentations on
complex scenes, and extract high-quality 3D assets from NeRFs that can then be
used in virtual 3D environments.



---

## SG-GS: Photo-realistic Animatable Human Avatars with Semantically-Guided  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Haoyu Zhao, Chen Yang, Hao Wang, Xingyue Zhao, Wei Shen | cs.CV | [PDF](http://arxiv.org/pdf/2408.09665v1){: .btn .btn-green } |

**Abstract**: Reconstructing photo-realistic animatable human avatars from monocular videos
remains challenging in computer vision and graphics. Recently, methods using 3D
Gaussians to represent the human body have emerged, offering faster
optimization and real-time rendering. However, due to ignoring the crucial role
of human body semantic information which represents the intrinsic structure and
connections within the human body, they fail to achieve fine-detail
reconstruction of dynamic human avatars. To address this issue, we propose
SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven rigid
deformation, and non-rigid cloth dynamics deformation to create photo-realistic
animatable human avatars from monocular videos. We then design a Semantic
Human-Body Annotator (SHA) which utilizes SMPL's semantic prior for efficient
body part semantic labeling. The generated labels are used to guide the
optimization of Gaussian semantic attributes. To address the limited receptive
field of point-level MLPs for local features, we also propose a 3D network that
integrates geometric and semantic associations for human avatar deformation. We
further implement three key strategies to enhance the semantic accuracy of 3D
Gaussians and rendering quality: semantic projection with 2D regularization,
semantic-guided density regularization and semantic-aware regularization with
neighborhood consistency. Extensive experiments demonstrate that SG-GS achieves
state-of-the-art geometry and appearance reconstruction performance.

Comments:
- 12 pages, 5 figures

---

## Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane  Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Minye Wu, Tinne Tuytelaars | cs.CV | [PDF](http://arxiv.org/pdf/2408.10041v1){: .btn .btn-green } |

**Abstract**: Recent advancements in photo-realistic novel view synthesis have been
significantly driven by Gaussian Splatting (3DGS). Nevertheless, the explicit
nature of 3DGS data entails considerable storage requirements, highlighting a
pressing need for more efficient data representations. To address this, we
present Implicit Gaussian Splatting (IGS), an innovative hybrid model that
integrates explicit point clouds with implicit feature embeddings through a
multi-level tri-plane architecture. This architecture features 2D feature grids
at various resolutions across different levels, facilitating continuous spatial
domain representation and enhancing spatial correlations among Gaussian
primitives. Building upon this foundation, we introduce a level-based
progressive training scheme, which incorporates explicit spatial
regularization. This method capitalizes on spatial correlations to enhance both
the rendering quality and the compactness of the IGS representation.
Furthermore, we propose a novel compression pipeline tailored for both point
clouds and 2D feature grids, considering the entropy variations across
different levels. Extensive experimental evaluations demonstrate that our
algorithm can deliver high-quality rendering using only a few MBs, effectively
balancing storage efficiency and rendering fidelity, and yielding results that
are competitive with the state-of-the-art.



---

## LoopSplat: Loop Closure by Registering 3D Gaussian Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Liyuan Zhu, Yue Li, Erik Sandstr√∂m, Shengyu Huang, Konrad Schindler, Iro Armeni | cs.CV | [PDF](http://arxiv.org/pdf/2408.10154v2){: .btn .btn-green } |

**Abstract**: Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats
(3DGS) has recently shown promise towards more accurate, dense 3D scene maps.
However, existing 3DGS-based methods fail to address the global consistency of
the scene via loop closure and/or global bundle adjustment. To this end, we
propose LoopSplat, which takes RGB-D images as input and performs dense mapping
with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure
online and computes relative loop edge constraints between submaps directly via
3DGS registration, leading to improvements in efficiency and accuracy over
traditional global-to-local point cloud registration. It uses a robust pose
graph optimization formulation and rigidly aligns the submaps to achieve global
consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD,
ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking,
mapping, and rendering compared to existing methods for dense RGB-D SLAM. Code
is available at loopsplat.github.io.

Comments:
- Project page: https://loopsplat.github.io/

---

## CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian  Splatting and Contrastive Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen | cs.CV | [PDF](http://arxiv.org/pdf/2408.09663v2){: .btn .btn-green } |

**Abstract**: Recent advancements in human avatar synthesis have utilized radiance fields
to reconstruct photo-realistic animatable human avatars. However, both
NeRFs-based and 3DGS-based methods struggle with maintaining 3D consistency and
exhibit suboptimal detail reconstruction, especially with sparse inputs. To
address this challenge, we propose CHASE, which introduces supervision from
intrinsic 3D consistency across poses and 3D geometry contrastive learning,
achieving performance comparable with sparse inputs to that with full inputs.
Following previous work, we first integrate a skeleton-driven rigid deformation
and a non-rigid cloth dynamics deformation to coordinate the movements of
individual Gaussians during animation, reconstructing basic avatar with coarse
3D consistency. To improve 3D consistency under sparse inputs, we design
Dynamic Avatar Adjustment(DAA) to adjust deformed Gaussians based on a selected
similar pose/image from the dataset. Minimizing the difference between the
image rendered by adjusted Gaussians and the image with the similar pose serves
as an additional form of supervision for avatar. Furthermore, we propose a 3D
geometry contrastive learning strategy to maintain the 3D global consistency of
generated avatars. Though CHASE is designed for sparse inputs, it surprisingly
outperforms current SOTA methods \textbf{in both full and sparse settings} on
the ZJU-MoCap and H36M datasets, demonstrating that our CHASE successfully
maintains avatar's 3D consistency, hence improving rendering quality.

Comments:
- 13 pages, 6 figures

---

## S^3D-NeRF: Single-Shot Speech-Driven Neural Radiance Field for High  Fidelity Talking Head Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-18 | Dongze Li, Kang Zhao, Wei Wang, Yifeng Ma, Bo Peng, Yingya Zhang, Jing Dong | cs.CV | [PDF](http://arxiv.org/pdf/2408.09347v1){: .btn .btn-green } |

**Abstract**: Talking head synthesis is a practical technique with wide applications.
Current Neural Radiance Field (NeRF) based approaches have shown their
superiority on driving one-shot talking heads with videos or signals regressed
from audio. However, most of them failed to take the audio as driven
information directly, unable to enjoy the flexibility and availability of
speech. Since mapping audio signals to face deformation is non-trivial, we
design a Single-Shot Speech-Driven Neural Radiance Field (S^3D-NeRF) method in
this paper to tackle the following three difficulties: learning a
representative appearance feature for each identity, modeling motion of
different face regions with audio, and keeping the temporal consistency of the
lip area. To this end, we introduce a Hierarchical Facial Appearance Encoder to
learn multi-scale representations for catching the appearance of different
speakers, and elaborate a Cross-modal Facial Deformation Field to perform
speech animation according to the relationship between the audio signal and
different face regions. Moreover, to enhance the temporal consistency of the
important lip area, we introduce a lip-sync discriminator to penalize the
out-of-sync audio-visual sequences. Extensive experiments have shown that our
S^3D-NeRF surpasses previous arts on both video fidelity and audio-lip
synchronization.

Comments:
- ECCV 2024

---

## SSNeRF: Sparse View Semi-supervised Neural Radiance Fields with  Augmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-17 | Xiao Cao, Beibei Lin, Bo Wang, Zhiyong Huang, Robby T. Tan | cs.CV | [PDF](http://arxiv.org/pdf/2408.09144v1){: .btn .btn-green } |

**Abstract**: Sparse view NeRF is challenging because limited input images lead to an under
constrained optimization problem for volume rendering. Existing methods address
this issue by relying on supplementary information, such as depth maps.
However, generating this supplementary information accurately remains
problematic and often leads to NeRF producing images with undesired artifacts.
To address these artifacts and enhance robustness, we propose SSNeRF, a sparse
view semi supervised NeRF method based on a teacher student framework. Our key
idea is to challenge the NeRF module with progressively severe sparse view
degradation while providing high confidence pseudo labels. This approach helps
the NeRF model become aware of noise and incomplete information associated with
sparse views, thus improving its robustness. The novelty of SSNeRF lies in its
sparse view specific augmentations and semi supervised learning mechanism. In
this approach, the teacher NeRF generates novel views along with confidence
scores, while the student NeRF, perturbed by the augmented input, learns from
the high confidence pseudo labels. Our sparse view degradation augmentation
progressively injects noise into volume rendering weights, perturbs feature
maps in vulnerable layers, and simulates sparse view blurriness. These
augmentation strategies force the student NeRF to recognize degradation and
produce clearer rendered views. By transferring the student's parameters to the
teacher, the teacher gains increased robustness in subsequent training
iterations. Extensive experiments demonstrate the effectiveness of our SSNeRF
in generating novel views with less sparse view degradation. We will release
code upon acceptance.



---

## HybridOcc: NeRF Enhanced Transformer-based Multi-Camera 3D Occupancy  Prediction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-17 | Xiao Zhao, Bo Chen, Mingyang Sun, Dingkang Yang, Youxing Wang, Xukun Zhang, Mingcheng Li, Dongliang Kou, Xiaoyi Wei, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.09104v1){: .btn .btn-green } |

**Abstract**: Vision-based 3D semantic scene completion (SSC) describes autonomous driving
scenes through 3D volume representations. However, the occlusion of invisible
voxels by scene surfaces poses challenges to current SSC methods in
hallucinating refined 3D geometry. This paper proposes HybridOcc, a hybrid 3D
volume query proposal method generated by Transformer framework and NeRF
representation and refined in a coarse-to-fine SSC prediction framework.
HybridOcc aggregates contextual features through the Transformer paradigm based
on hybrid query proposals while combining it with NeRF representation to obtain
depth supervision. The Transformer branch contains multiple scales and uses
spatial cross-attention for 2D to 3D transformation. The newly designed NeRF
branch implicitly infers scene occupancy through volume rendering, including
visible and invisible voxels, and explicitly captures scene depth rather than
generating RGB color. Furthermore, we present an innovative occupancy-aware ray
sampling method to orient the SSC task instead of focusing on the scene
surface, further improving the overall performance. Extensive experiments on
nuScenes and SemanticKITTI datasets demonstrate the effectiveness of our
HybridOcc on the SSC task.

Comments:
- Accepted to IEEE RAL

---

## Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark  Images Using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-17 | Sheng Ye, Zhen-Hui Dong, Yubin Hu, Yu-Hui Wen, Yong-Jin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2408.09130v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently emerged as a powerful representation that
can synthesize remarkable novel views using consistent multi-view images as
input. However, we notice that images captured in dark environments where the
scenes are not fully illuminated can exhibit considerable brightness variations
and multi-view inconsistency, which poses great challenges to 3D Gaussian
Splatting and severely degrades its performance. To tackle this problem, we
propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera
imaging, we represent a consistent radiance field of the physical world using a
set of anisotropic 3D Gaussians, and design a camera response module to
compensate for multi-view inconsistencies. We also introduce a step-based
gradient scaling strategy to constrain Gaussians near the camera, which turn
out to be floaters, from splitting and cloning. Experiments on our proposed
benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings
without ghosting and floater artifacts and significantly outperforms existing
methods. Furthermore, we can also synthesize light-up images by controlling
exposure levels that clearly show details in shadow areas.

Comments:
- accepted by PG 2024

---

## GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion  Prior and Parametric Light Source Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-16 | Kang Du, Zhihao Liang, Zeyu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.08524v1){: .btn .btn-green } |

**Abstract**: We present GS-ID, a novel framework for illumination decomposition on
Gaussian Splatting, achieving photorealistic novel view synthesis and intuitive
light editing. Illumination decomposition is an ill-posed problem facing three
main challenges: 1) priors for geometry and material are often lacking; 2)
complex illumination conditions involve multiple unknown light sources; and 3)
calculating surface shading with numerous light sources is computationally
expensive. To address these challenges, we first introduce intrinsic diffusion
priors to estimate the attributes for physically based rendering. Then we
divide the illumination into environmental and direct components for joint
optimization. Last, we employ deferred rendering to reduce the computational
load. Our framework uses a learnable environment map and Spherical Gaussians
(SGs) to represent light sources parametrically, therefore enabling
controllable and photorealistic relighting on Gaussian Splatting. Extensive
experiments and applications demonstrate that GS-ID produces state-of-the-art
illumination decomposition results while achieving better geometry
reconstruction and rendering performance.

Comments:
- 15 pages, 13 figures

---

## VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-16 | Albert Gassol Puigjaner, Edoardo Mello Rella, Erik Sandstr√∂m, Ajad Chhatkuli, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2408.08766v1){: .btn .btn-green } |

**Abstract**: Implicit surfaces via neural radiance fields (NeRF) have shown surprising
accuracy in surface reconstruction. Despite their success in reconstructing
richly textured surfaces, existing methods struggle with planar regions with
weak textures, which account for the majority of indoor scenes. In this paper,
we address indoor dense surface reconstruction by revisiting key aspects of
NeRF in order to use the recently proposed Vector Field (VF) as the implicit
representation. VF is defined by the unit vector directed to the nearest
surface point. It therefore flips direction at the surface and equals to the
explicit surface normals. Except for this flip, VF remains constant along
planar surfaces and provides a strong inductive bias in representing planar
surfaces. Concretely, we develop a novel density-VF relationship and a training
scheme that allows us to learn VF via volume rendering By doing this, VF-NeRF
can model large planar surfaces and sharp corners accurately. We show that,
when depth cues are available, our method further improves and achieves
state-of-the-art results in reconstructing indoor scenes and rendering novel
views. We extensively evaluate VF-NeRF on indoor datasets and run ablations of
its components.

Comments:
- 15 pages

---

## Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-16 | Wei Sun, Xiaosong Zhang, Fang Wan, Yanzhao Zhou, Yuan Li, Qixiang Ye, Jianbin Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2408.08723v1){: .btn .btn-green } |

**Abstract**: Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processed
camera poses--referred to as SfM-free methods--is crucial for promoting rapid
response capabilities and enhancing robustness against variable operating
conditions. Recent SfM-free methods have integrated pose optimization,
designing end-to-end frameworks for joint camera pose estimation and NVS.
However, most existing works rely on per-pixel image loss functions, such as L2
loss. In SfM-free methods, inaccurate initial poses lead to misalignment issue,
which, under the constraints of per-pixel image loss functions, results in
excessive gradients, causing unstable optimization and poor convergence for
NVS. In this study, we propose a correspondence-guided SfM-free 3D Gaussian
splatting for NVS. We use correspondences between the target and the rendered
result to achieve better pixel alignment, facilitating the optimization of
relative poses between frames. We then apply the learned poses to optimize the
entire scene. Each 2D screen-space pixel is associated with its corresponding
3D Gaussians through approximated surface rendering to facilitate gradient back
propagation. Experimental results underline the superior performance and time
efficiency of the proposed approach compared to the state-of-the-art baselines.

Comments:
- arXiv admin note: text overlap with arXiv:2312.07504 by other authors

---

## WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-15 | Huapeng Li, Wenxuan Song, Tianao Xu, Alexandre Elsig, Jonas Kulhanek | cs.CV | [PDF](http://arxiv.org/pdf/2408.08206v1){: .btn .btn-green } |

**Abstract**: The underwater 3D scene reconstruction is a challenging, yet interesting
problem with applications ranging from naval robots to VR experiences. The
problem was successfully tackled by fully volumetric NeRF-based methods which
can model both the geometry and the medium (water). Unfortunately, these
methods are slow to train and do not offer real-time rendering. More recently,
3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs.
However, because it is an explicit method that renders only the geometry, it
cannot render the medium and is therefore unsuited for underwater
reconstruction. Therefore, we propose a novel approach that fuses volumetric
rendering with 3DGS to handle underwater data effectively. Our method employs
3DGS for explicit geometry representation and a separate volumetric field
(queried once per pixel) for capturing the scattering medium. This dual
representation further allows the restoration of the scenes by removing the
scattering medium. Our method outperforms state-of-the-art NeRF-based methods
in rendering quality on the underwater SeaThru-NeRF dataset. Furthermore, it
does so while offering real-time rendering performance, addressing the
efficiency limitations of existing methods. Web:
https://water-splatting.github.io

Comments:
- Web: https://water-splatting.github.io

---

## FlashGS: Efficient 3D Gaussian Splatting for Large-scale and  High-resolution Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-15 | Guofeng Feng, Siyan Chen, Rong Fu, Zimu Liao, Yi Wang, Tao Liu, Zhilin Pei, Hengjie Li, Xingcheng Zhang, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2408.07967v2){: .btn .btn-green } |

**Abstract**: This work introduces FlashGS, an open-source CUDA Python library, designed to
facilitate the efficient differentiable rasterization of 3D Gaussian Splatting
through algorithmic and kernel-level optimizations. FlashGS is developed based
on the observations from a comprehensive analysis of the rendering process to
enhance computational efficiency and bring the technique to wide adoption. The
paper includes a suite of optimization strategies, encompassing redundancy
elimination, efficient pipelining, refined control and scheduling mechanisms,
and memory access optimizations, all of which are meticulously integrated to
amplify the performance of the rasterization process. An extensive evaluation
of FlashGS' performance has been conducted across a diverse spectrum of
synthetic and real-world large-scale scenes, encompassing a variety of image
resolutions. The empirical findings demonstrate that FlashGS consistently
achieves an average 4x acceleration over mobile consumer GPUs, coupled with
reduced memory consumption. These results underscore the superior performance
and resource optimization capabilities of FlashGS, positioning it as a
formidable tool in the domain of 3D rendering.



---

## 3D Gaussian Editing with A Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Guan Luo, Tian-Xing Xu, Ying-Tian Liu, Xiao-Xiong Fan, Fang-Lue Zhang, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.07540v1){: .btn .btn-green } |

**Abstract**: The modeling and manipulation of 3D scenes captured from the real world are
pivotal in various applications, attracting growing research interest. While
previous works on editing have achieved interesting results through
manipulating 3D meshes, they often require accurately reconstructed meshes to
perform editing, which limits their application in 3D content generation. To
address this gap, we introduce a novel single-image-driven 3D scene editing
approach based on 3D Gaussian Splatting, enabling intuitive manipulation via
directly editing the content on a 2D image plane. Our method learns to optimize
the 3D Gaussians to align with an edited version of the image rendered from a
user-specified viewpoint of the original scene. To capture long-range object
deformation, we introduce positional loss into the optimization process of 3D
Gaussian Splatting and enable gradient propagation through reparameterization.
To handle occluded 3D Gaussians when rendering from the specified viewpoint, we
build an anchor-based structure and employ a coarse-to-fine optimization
strategy capable of handling long-range deformation while maintaining
structural stability. Furthermore, we design a novel masking strategy to
adaptively identify non-rigid deformation regions for fine-scale modeling.
Extensive experiments show the effectiveness of our method in handling
geometric details, long-range, and non-rigid deformation, demonstrating
superior editing flexibility and quality compared to previous approaches.

Comments:
- 10 pages, 12 figures

---

## Progressive Radiance Distillation for Inverse Rendering with Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Keyang Ye, Qiming Hou, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2408.07595v1){: .btn .btn-green } |

**Abstract**: We propose progressive radiance distillation, an inverse rendering method
that combines physically-based rendering with Gaussian-based radiance field
rendering using a distillation progress map. Taking multi-view images as input,
our method starts from a pre-trained radiance field guidance, and distills
physically-based light and material parameters from the radiance field using an
image-fitting process. The distillation progress map is initialized to a small
value, which favors radiance field rendering. During early iterations when
fitted light and material parameters are far from convergence, the radiance
field fallback ensures the sanity of image loss gradients and avoids local
minima that attracts under-fit states. As fitted parameters converge, the
physical model gradually takes over and the distillation progress increases
correspondingly. In presence of light paths unmodeled by the physical model,
the distillation progress never finishes on affected pixels and the learned
radiance field stays in the final rendering. With this designed tolerance for
physical model limitations, we prevent unmodeled color components from leaking
into light and material parameters, alleviating relighting artifacts.
Meanwhile, the remaining radiance field compensates for the limitations of the
physical model, guaranteeing high-quality novel views synthesis. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
techniques quality-wise in both novel view synthesis and relighting. The idea
of progressive radiance distillation is not limited to Gaussian splatting. We
show that it also has positive effects for prominently specular scenes when
adapted to a mesh-based inverse rendering method.



---

## Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Hyunjee Lee, Youngsik Yun, Jeongmin Bae, Seoha Kim, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2408.07416v2){: .btn .btn-green } |

**Abstract**: Understanding the 3D semantics of a scene is a fundamental problem for
various scenarios such as embodied agents. While NeRFs and 3DGS excel at
novel-view synthesis, previous methods for understanding their semantics have
been limited to incomplete 3D understanding: their segmentation results are 2D
masks and their supervision is anchored at 2D pixels. This paper revisits the
problem set to pursue a better 3D understanding of a scene modeled by NeRFs and
3DGS as follows. 1) We directly supervise the 3D points to train the language
embedding field. It achieves state-of-the-art accuracy without relying on
multi-scale language embeddings. 2) We transfer the pre-trained language field
to 3DGS, achieving the first real-time rendering speed without sacrificing
training time or accuracy. 3) We introduce a 3D querying and evaluation
protocol for assessing the reconstructed geometry and semantics together. Code,
checkpoints, and annotations will be available online. Project page:
https://hyunji12.github.io/Open3DRF

Comments:
- Project page: https://hyunji12.github.io/Open3DRF

---

## ActiveNeRF: Learning Accurate 3D Geometry by Active Pattern Projection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Jianyu Tao, Changping Hu, Edward Yang, Jing Xu, Rui Chen | cs.CV | [PDF](http://arxiv.org/pdf/2408.06592v1){: .btn .btn-green } |

**Abstract**: NeRFs have achieved incredible success in novel view synthesis. However, the
accuracy of the implicit geometry is unsatisfactory because the passive static
environmental illumination has low spatial frequency and cannot provide enough
information for accurate geometry reconstruction. In this work, we propose
ActiveNeRF, a 3D geometry reconstruction framework, which improves the geometry
quality of NeRF by actively projecting patterns of high spatial frequency onto
the scene using a projector which has a constant relative pose to the camera.
We design a learnable active pattern rendering pipeline which jointly learns
the scene geometry and the active pattern. We find that, by adding the active
pattern and imposing its consistency across different views, our proposed
method outperforms state of the art geometry reconstruction methods
qualitatively and quantitatively in both simulation and real experiments. Code
is avaliable at https://github.com/hcp16/active_nerf

Comments:
- 18 pages, 10 figures

---

## HDRGS: High Dynamic Range Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Jiahao Wu, Lu Xiao, Chao Wang, Rui Peng, Kaiqiang Xiong, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.06543v1){: .btn .btn-green } |

**Abstract**: Recent years have witnessed substantial advancements in the field of 3D
reconstruction from 2D images, particularly following the introduction of the
neural radiance field (NeRF) technique. However, reconstructing a 3D high
dynamic range (HDR) radiance field, which aligns more closely with real-world
conditions, from 2D multi-exposure low dynamic range (LDR) images continues to
pose significant challenges. Approaches to this issue fall into two categories:
grid-based and implicit-based. Implicit methods, using multi-layer perceptrons
(MLP), face inefficiencies, limited solvability, and overfitting risks.
Conversely, grid-based methods require significant memory and struggle with
image quality and long training times. In this paper, we introduce Gaussian
Splatting-a recent, high-quality, real-time 3D reconstruction technique-into
this domain. We further develop the High Dynamic Range Gaussian Splatting
(HDR-GS) method, designed to address the aforementioned challenges. This method
enhances color dimensionality by including luminance and uses an asymmetric
grid for tone-mapping, swiftly and precisely converting pixel irradiance to
color. Our approach improves HDR scene recovery accuracy and integrates a novel
coarse-to-fine strategy to speed up model convergence, enhancing robustness
against sparse viewpoints and exposure extremes, and preventing local optima.
Extensive testing confirms that our method surpasses current state-of-the-art
techniques in both synthetic and real-world scenarios. Code will be released at
\url{https://github.com/WuJH2001/HDRGS}



---

## NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance  Fields in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Rishit Dagli, Atsuhiro Hibi, Rahul G. Krishnan, Pascal N. Tyrrell | cs.CV | [PDF](http://arxiv.org/pdf/2408.10258v2){: .btn .btn-green } |

**Abstract**: Current methods for performing 3D reconstruction and novel view synthesis
(NVS) in ultrasound imaging data often face severe artifacts when training
NeRF-based approaches. The artifacts produced by current approaches differ from
NeRF floaters in general scenes because of the unique nature of ultrasound
capture. Furthermore, existing models fail to produce reasonable 3D
reconstructions when ultrasound data is captured or obtained casually in
uncontrolled environments, which is common in clinical settings. Consequently,
existing reconstruction and NVS methods struggle to handle ultrasound motion,
fail to capture intricate details, and cannot model transparent and reflective
surfaces. In this work, we introduced NeRF-US, which incorporates 3D-geometry
guidance for border probability and scattering density into NeRF training,
while also utilizing ultrasound-specific rendering over traditional volume
rendering. These 3D priors are learned through a diffusion model. Through
experiments conducted on our new "Ultrasound in the Wild" dataset, we observed
accurate, clinically plausible, artifact-free reconstructions.



---

## Potamoi: Accelerating Neural Rendering via a Unified Streaming  Architecture

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Yu Feng, Weikai Lin, Zihan Liu, Jingwen Leng, Minyi Guo, Han Zhao, Xiaofeng Hou, Jieru Zhao, Yuhao Zhu | cs.AR | [PDF](http://arxiv.org/pdf/2408.06608v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has emerged as a promising alternative for
photorealistic rendering. Despite recent algorithmic advancements, achieving
real-time performance on today's resource-constrained devices remains
challenging. In this paper, we identify the primary bottlenecks in current NeRF
algorithms and introduce a unified algorithm-architecture co-design, Potamoi,
designed to accommodate various NeRF algorithms. Specifically, we introduce a
runtime system featuring a plug-and-play algorithm, SpaRW, which significantly
reduces the per-frame computational workload and alleviates compute
inefficiencies. Furthermore, our unified streaming pipeline coupled with
customized hardware support effectively tames both SRAM and DRAM inefficiencies
by minimizing repetitive DRAM access and completely eliminating SRAM bank
conflicts. When evaluated against a baseline utilizing a dedicated DNN
accelerator, our framework demonstrates a speed-up and energy reduction of
53.1$\times$ and 67.7$\times$, respectively, all while maintaining high visual
quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.

Comments:
- arXiv admin note: substantial text overlap with arXiv:2404.11852

---

## SpectralGaussians: Semantic, spectral 3D Gaussian splatting for  multi-spectral scene representation, visualization and analysis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Saptarshi Neil Sinha, Holger Graf, Michael Weinmann | cs.CV | [PDF](http://arxiv.org/pdf/2408.06975v1){: .btn .btn-green } |

**Abstract**: We propose a novel cross-spectral rendering framework based on 3D Gaussian
Splatting (3DGS) that generates realistic and semantically meaningful splats
from registered multi-view spectrum and segmentation maps. This extension
enhances the representation of scenes with multiple spectra, providing insights
into the underlying materials and segmentation. We introduce an improved
physically-based rendering approach for Gaussian splats, estimating reflectance
and lights per spectra, thereby enhancing accuracy and realism. In a
comprehensive quantitative and qualitative evaluation, we demonstrate the
superior performance of our approach with respect to other recent
learning-based spectral scene representation approaches (i.e., XNeRF and
SpectralNeRF) as well as other non-spectral state-of-the-art learning-based
approaches. Our work also demonstrates the potential of spectral scene
understanding for precise scene editing techniques like style transfer,
inpainting, and removal. Thereby, our contributions address challenges in
multi-spectral scene representation, rendering, and editing, offering new
possibilities for diverse applications.



---

## Developing Smart MAVs for Autonomous Inspection in GPS-denied  Constructions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Paoqiang Pan, Kewei Hu, Xiao Huang, Wei Ying, Xiaoxuan Xie, Yue Ma, Naizhong Zhang, Hanwen Kang | cs.RO | [PDF](http://arxiv.org/pdf/2408.06030v1){: .btn .btn-green } |

**Abstract**: Smart Micro Aerial Vehicles (MAVs) have transformed infrastructure inspection
by enabling efficient, high-resolution monitoring at various stages of
construction, including hard-to-reach areas. Traditional manual operation of
drones in GPS-denied environments, such as industrial facilities and
infrastructure, is labour-intensive, tedious and prone to error. This study
presents an innovative framework for smart MAV inspections in such complex and
GPS-denied indoor environments. The framework features a hierarchical
perception and planning system that identifies regions of interest and
optimises task paths. It also presents an advanced MAV system with enhanced
localisation and motion planning capabilities, integrated with Neural
Reconstruction technology for comprehensive 3D reconstruction of building
structures. The effectiveness of the framework was empirically validated in a
4,000 square meters indoor infrastructure facility with an interior length of
80 metres, a width of 50 metres and a height of 7 metres. The main structure
consists of columns and walls. Experimental results show that our MAV system
performs exceptionally well in autonomous inspection tasks, achieving a 100\%
success rate in generating and executing scan paths. Extensive experiments
validate the manoeuvrability of our developed MAV, achieving a 100\% success
rate in motion planning with a tracking error of less than 0.1 metres. In
addition, the enhanced reconstruction method using 3D Gaussian Splatting
technology enables the generation of high-fidelity rendering models from the
acquired data. Overall, our novel method represents a significant advancement
in the use of robotics for infrastructure inspection.



---

## HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2408.06019v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel 3D head avatar creation approach capable of
generalizing from few-shot in-the-wild data with high-fidelity and animatable
robustness. Given the underconstrained nature of this problem, incorporating
prior knowledge is essential. Therefore, we propose a framework comprising
prior learning and avatar creation phases. The prior learning phase leverages
3D head priors derived from a large-scale multi-view dynamic dataset, and the
avatar creation phase applies these priors for few-shot personalization. Our
approach effectively captures these priors by utilizing a Gaussian
Splatting-based auto-decoder network with part-based dynamic modeling. Our
method employs identity-shared encoding with personalized latent codes for
individual identities to learn the attributes of Gaussian primitives. During
the avatar creation phase, we achieve fast head avatar personalization by
leveraging inversion and fine-tuning strategies. Extensive experiments
demonstrate that our model effectively exploits head priors and successfully
generalizes them to few-shot personalization, achieving photo-realistic
rendering quality, multi-view consistency, and stable animation.

Comments:
- Project page: https://headgap.github.io/

---

## Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for  Anti-aliasing Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Jiameng Li, Yue Shi, Jiezhang Cao, Bingbing Ni, Wenjun Zhang, Kai Zhang, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2408.06286v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has attracted great attention in novel view
synthesis because of its superior rendering efficiency and high fidelity.
However, the trained Gaussians suffer from severe zooming degradation due to
non-adjustable representation derived from single-scale training. Though some
methods attempt to tackle this problem via post-processing techniques such as
selective rendering or filtering techniques towards primitives, the
scale-specific information is not involved in Gaussians. In this paper, we
propose a unified optimization method to make Gaussians adaptive for arbitrary
scales by self-adjusting the primitive properties (e.g., color, shape and size)
and distribution (e.g., position). Inspired by the mipmap technique, we design
pseudo ground-truth for the target scale and propose a scale-consistency
guidance loss to inject scale information into 3D Gaussians. Our method is a
plug-in module, applicable for any 3DGS models to solve the zoom-in and
zoom-out aliasing. Extensive experiments demonstrate the effectiveness of our
method. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dB
for zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.

Comments:
- 9 pages

---

## 3D Reconstruction of Protein Structures from Multi-view AFM Images using  Neural Radiance Fields (NeRFs)

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Jaydeep Rade, Ethan Herron, Soumik Sarkar, Anwesha Sarkar, Adarsh Krishnamurthy | cs.CV | [PDF](http://arxiv.org/pdf/2408.06244v1){: .btn .btn-green } |

**Abstract**: Recent advancements in deep learning for predicting 3D protein structures
have shown promise, particularly when leveraging inputs like protein sequences
and Cryo-Electron microscopy (Cryo-EM) images. However, these techniques often
fall short when predicting the structures of protein complexes (PCs), which
involve multiple proteins. In our study, we investigate using atomic force
microscopy (AFM) combined with deep learning to predict the 3D structures of
PCs. AFM generates height maps that depict the PCs in various random
orientations, providing a rich information for training a neural network to
predict the 3D structures. We then employ the pre-trained UpFusion model (which
utilizes a conditional diffusion model for synthesizing novel views) to train
an instance-specific NeRF model for 3D reconstruction. The performance of
UpFusion is evaluated through zero-shot predictions of 3D protein structures
using AFM images. The challenge, however, lies in the time-intensive and
impractical nature of collecting actual AFM images. To address this, we use a
virtual AFM imaging process that transforms a `PDB' protein file into
multi-view 2D virtual AFM images via volume rendering techniques. We
extensively validate the UpFusion architecture using both virtual and actual
multi-view AFM images. Our results include a comparison of structures predicted
with varying numbers of views and different sets of views. This novel approach
holds significant potential for enhancing the accuracy of protein complex
structure predictions with further fine-tuning of the UpFusion network.



---

## Visual SLAM with 3D Gaussian Primitives and Depth Priors Enabling Novel  View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-10 | Zhongche Qu, Zhi Zhang, Cong Liu, Jianhua Yin | cs.CV | [PDF](http://arxiv.org/pdf/2408.05635v1){: .btn .btn-green } |

**Abstract**: Conventional geometry-based SLAM systems lack dense 3D reconstruction
capabilities since their data association usually relies on feature
correspondences. Additionally, learning-based SLAM systems often fall short in
terms of real-time performance and accuracy. Balancing real-time performance
with dense 3D reconstruction capabilities is a challenging problem. In this
paper, we propose a real-time RGB-D SLAM system that incorporates a novel view
synthesis technique, 3D Gaussian Splatting, for 3D scene representation and
pose estimation. This technique leverages the real-time rendering performance
of 3D Gaussian Splatting with rasterization and allows for differentiable
optimization in real time through CUDA implementation. We also enable mesh
reconstruction from 3D Gaussians for explicit dense 3D reconstruction. To
estimate accurate camera poses, we utilize a rotation-translation decoupled
strategy with inverse optimization. This involves iteratively updating both in
several iterations through gradient-based optimization. This process includes
differentiably rendering RGB, depth, and silhouette maps and updating the
camera parameters to minimize a combined loss of photometric loss, depth
geometry loss, and visibility loss, given the existing 3D Gaussian map.
However, 3D Gaussian Splatting (3DGS) struggles to accurately represent
surfaces due to the multi-view inconsistency of 3D Gaussians, which can lead to
reduced accuracy in both camera pose estimation and scene reconstruction. To
address this, we utilize depth priors as additional regularization to enforce
geometric constraints, thereby improving the accuracy of both pose estimation
and 3D reconstruction. We also provide extensive experimental results on public
benchmark datasets to demonstrate the effectiveness of our proposed methods in
terms of pose accuracy, geometric accuracy, and rendering performance.



---

## Radiance Field Learners As UAV First-Person Viewers

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-10 | Liqi Yan, Qifan Wang, Junhan Zhao, Qiang Guan, Zheng Tang, Jianhui Zhang, Dongfang Liu | cs.CV | [PDF](http://arxiv.org/pdf/2408.05533v1){: .btn .btn-green } |

**Abstract**: First-Person-View (FPV) holds immense potential for revolutionizing the
trajectory of Unmanned Aerial Vehicles (UAVs), offering an exhilarating avenue
for navigating complex building structures. Yet, traditional Neural Radiance
Field (NeRF) methods face challenges such as sampling single points per
iteration and requiring an extensive array of views for supervision. UAV videos
exacerbate these issues with limited viewpoints and significant spatial scale
variations, resulting in inadequate detail rendering across diverse scales. In
response, we introduce FPV-NeRF, addressing these challenges through three key
facets: (1) Temporal consistency. Leveraging spatio-temporal continuity ensures
seamless coherence between frames; (2) Global structure. Incorporating various
global features during point sampling preserves space integrity; (3) Local
granularity. Employing a comprehensive framework and multi-resolution
supervision for multi-scale scene feature representation tackles the
intricacies of UAV video spatial scales. Additionally, due to the scarcity of
publicly available FPV videos, we introduce an innovative view synthesis method
using NeRF to generate FPV perspectives from UAV footage, enhancing spatial
perception for drones. Our novel dataset spans diverse trajectories, from
outdoor to indoor environments, in the UAV domain, differing significantly from
traditional NeRF scenarios. Through extensive experiments encompassing both
interior and exterior building structures, FPV-NeRF demonstrates a superior
understanding of the UAV flying space, outperforming state-of-the-art methods
in our curated UAV dataset. Explore our project page for further insights:
https://fpv-nerf.github.io/.

Comments:
- Accepted to ECCV 2024

---

## Self-augmented Gaussian Splatting with Structure-aware Masks for  Sparse-view 3D Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Lingbei Meng, Bi'an Du, Wei Hu | cs.CV | [PDF](http://arxiv.org/pdf/2408.04831v2){: .btn .btn-green } |

**Abstract**: Sparse-view 3D reconstruction stands as a formidable challenge in computer
vision, aiming to build complete three-dimensional models from a limited array
of viewing perspectives. This task confronts several difficulties: 1) the
limited number of input images that lack consistent information; 2) dependence
on the quality of input images; and 3) the substantial size of model
parameters. To address these challenges, we propose a self-augmented
coarse-to-fine Gaussian splatting paradigm, enhanced with a structure-aware
mask, for sparse-view 3D reconstruction. In particular, our method initially
employs a coarse Gaussian model to obtain a basic 3D representation from
sparse-view inputs. Subsequently, we develop a fine Gaussian network to enhance
consistent and detailed representation of the output with both 3D geometry
augmentation and perceptual view augmentation. During training, we design a
structure-aware masking strategy to further improve the model's robustness
against sparse inputs and noise.Experimental results on the MipNeRF360 and
OmniObject3D datasets demonstrate that the proposed method achieves
state-of-the-art performances for sparse input views in both perceptual quality
and efficiency.



---

## DreamCouple: Exploring High Quality Text-to-3D Generation Via Rectified  Flow

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Hangyu Li, Xiangxiang Chu, Dingyuan Shi | cs.CV | [PDF](http://arxiv.org/pdf/2408.05008v1){: .btn .btn-green } |

**Abstract**: The Score Distillation Sampling (SDS), which exploits pretrained
text-to-image model diffusion models as priors to 3D model training, has
achieved significant success. Currently, the flow-based diffusion model has
become a new trend for generations. Yet, adapting SDS to flow-based diffusion
models in 3D generation remains unexplored. Our work is aimed to bridge this
gap. In this paper, we adapt SDS to rectified flow and re-examine the
over-smoothing issue under this novel framework. The issue can be explained
that the model learns an average of multiple ODE trajectories. Then we propose
DreamCouple, which instead of randomly sampling noise, uses a rectified flow
model to find the coupled noise. Its Unique Couple Matching (UCM) loss guides
the model to learn different trajectories and thus solves the over-smoothing
issue. We apply our method to both NeRF and 3D Gaussian splatting and achieve
state-of-the-art performances. We also identify some other interesting open
questions such as initialization issues for NeRF and faster training
convergence. Our code will be released soon.

Comments:
- Tech Report

---

## FewShotNeRF: Meta-Learning-based Novel View Synthesis for Rapid  Scene-Specific Adaptation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Piraveen Sivakumar, Paul Janson, Jathushan Rajasegaran, Thanuja Ambegoda | cs.CV | [PDF](http://arxiv.org/pdf/2408.04803v1){: .btn .btn-green } |

**Abstract**: In this paper, we address the challenge of generating novel views of
real-world objects with limited multi-view images through our proposed
approach, FewShotNeRF. Our method utilizes meta-learning to acquire optimal
initialization, facilitating rapid adaptation of a Neural Radiance Field (NeRF)
to specific scenes. The focus of our meta-learning process is on capturing
shared geometry and textures within a category, embedded in the weight
initialization. This approach expedites the learning process of NeRFs and
leverages recent advancements in positional encodings to reduce the time
required for fitting a NeRF to a scene, thereby accelerating the inner loop
optimization of meta-learning. Notably, our method enables meta-learning on a
large number of 3D scenes to establish a robust 3D prior for various
categories. Through extensive evaluations on the Common Objects in 3D open
source dataset, we empirically demonstrate the efficacy and potential of
meta-learning in generating high-quality novel views of objects.



---

## InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Xin-Yi Yu, Jun-Xin Yu, Li-Bo Zhou, Yan Wei, Lin-Lin Ou | cs.CV | [PDF](http://arxiv.org/pdf/2408.04249v1){: .btn .btn-green } |

**Abstract**: We present InstantStyleGaussian, an innovative 3D style transfer method based
on the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target
style image, it quickly generates new 3D GS scenes. Our approach operates on
pre-reconstructed GS scenes, combining diffusion models with an improved
iterative dataset update strategy. It utilizes diffusion models to generate
target style images, adds these new images to the training dataset, and uses
this dataset to iteratively update and optimize the GS scenes. Extensive
experimental results demonstrate that our method ensures high-quality stylized
scenes while offering significant advantages in style transfer speed and
consistency.



---

## A Review of 3D Reconstruction Techniques for Deformable Tissues in  Robotic Surgery

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Mengya Xu, Ziqi Guo, An Wang, Long Bai, Hongliang Ren | cs.CV | [PDF](http://arxiv.org/pdf/2408.04426v1){: .btn .btn-green } |

**Abstract**: As a crucial and intricate task in robotic minimally invasive surgery,
reconstructing surgical scenes using stereo or monocular endoscopic video holds
immense potential for clinical applications. NeRF-based techniques have
recently garnered attention for the ability to reconstruct scenes implicitly.
On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly
using 3D Gaussians and projects them onto a 2D plane as a replacement for the
complex volume rendering in NeRF. However, these methods face challenges
regarding surgical scene reconstruction, such as slow inference, dynamic
scenes, and surgical tool occlusion. This work explores and reviews
state-of-the-art (SOTA) approaches, discussing their innovations and
implementation principles. Furthermore, we replicate the models and conduct
testing and evaluation on two datasets. The test results demonstrate that with
advancements in these techniques, achieving real-time, high-quality
reconstructions becomes feasible.

Comments:
- To appear in MICCAI 2024 EARTH Workshop. Code availability:
  https://github.com/Epsilon404/surgicalnerf

---

## Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs  Gaussian-Based Methods

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Yiming Zhou, Zixuan Zeng, Andi Chen, Xiaofan Zhou, Haowei Ni, Shiyao Zhang, Panfeng Li, Liangxi Liu, Mengyao Zheng, Xupeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2408.04268v1){: .btn .btn-green } |

**Abstract**: Exploring the capabilities of Neural Radiance Fields (NeRF) and
Gaussian-based methods in the context of 3D scene reconstruction, this study
contrasts these modern approaches with traditional Simultaneous Localization
and Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we
assess performance based on tracking accuracy, mapping fidelity, and view
synthesis. Findings reveal that NeRF excels in view synthesis, offering unique
capabilities in generating new perspectives from existing data, albeit at
slower processing speeds. Conversely, Gaussian-based methods provide rapid
processing and significant expressiveness but lack comprehensive scene
completion. Enhanced by global optimization and loop closure techniques, newer
methods like NICE-SLAM and SplaTAM not only surpass older frameworks such as
ORB-SLAM2 in terms of robustness but also demonstrate superior performance in
dynamic and complex environments. This comparative analysis bridges theoretical
research with practical implications, shedding light on future developments in
robust 3D scene reconstruction across various real-world applications.

Comments:
- Accepted by 2024 6th International Conference on Data-driven
  Optimization of Complex Systems

---

## 3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Zhe Jun Tang, Tat-Jen Cham | cs.CV | [PDF](http://arxiv.org/pdf/2408.03753v1){: .btn .btn-green } |

**Abstract**: The use of 3D Gaussians as representation of radiance fields has enabled high
quality novel view synthesis at real-time rendering speed. However, the choice
of optimising the outgoing radiance of each Gaussian independently as spherical
harmonics results in unsatisfactory view dependent effects. In response to
these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian
Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering
quality. Instead of optimising a single outgoing radiance parameter, 3iGS
enhances 3DGS view-dependent effects by expressing the outgoing radiance as a
function of a local illumination field and Bidirectional Reflectance
Distribution Function (BRDF) features. We optimise a continuous incident
illumination field through a Tensorial Factorisation representation, while
separately fine-tuning the BRDF features of each 3D Gaussian relative to this
illumination field. Our methodology significantly enhances the rendering
quality of specular view-dependent effects of 3DGS, while maintaining rapid
training and rendering speeds.

Comments:
- The 18th European Conference on Computer Vision ECCV 2024

---

## PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time  High-Quality Relighting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Yijia Guo, Yuanxi Bai, Liwen Hu, Ziyi Guo, Mianzhi Liu, Yu Cai, Tiejun Huang, Lei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2408.03538v1){: .btn .btn-green } |

**Abstract**: We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a
real-time high-quality relighting method for Gaussian splats in low-frequency
lighting environments that captures soft shadows and interreflections by
precomputing 3D Gaussian splats' radiance transfer. Existing studies have
demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'
efficiency for dynamic lighting scenarios. However, the current relighting
method based on 3DGS still struggles to compute high-quality shadow and
indirect illumination in real time for dynamic light, leading to unrealistic
rendering results. We solve this problem by precomputing the expensive
transport simulations required for complex transfer functions like shadowing,
the resulting transfer functions are represented as dense sets of vectors or
matrices for every Gaussian splat. We introduce distinct precomputing methods
tailored for training and rendering stages, along with unique ray tracing and
indirect lighting precomputation techniques for 3D Gaussian splats to
accelerate training speed and compute accurate indirect lighting related to
environment light. Experimental analyses demonstrate that our approach achieves
state-of-the-art visual quality while maintaining competitive training times
and allows high-quality real-time (30+ fps) relighting for dynamic light and
relatively complex scenes at 1080p resolution.



---

## Goal-oriented Semantic Communication for the Metaverse Application

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Zhe Wang, Nan Li, Yansha Deng | eess.SY | [PDF](http://arxiv.org/pdf/2408.03646v1){: .btn .btn-green } |

**Abstract**: With the emergence of the metaverse and its role in enabling real-time
simulation and analysis of real-world counterparts, an increasing number of
personalized metaverse scenarios are being created to influence entertainment
experiences and social behaviors. However, compared to traditional image and
video entertainment applications, the exact transmission of the vast amount of
metaverse-associated information significantly challenges the capacity of
existing bit-oriented communication networks. Moreover, the current metaverse
also witnesses a growing goal shift for transmitting the meaning behind
custom-designed content, such as user-designed buildings and avatars, rather
than exact copies of physical objects. To meet this growing goal shift and
bandwidth challenge, this paper proposes a goal-oriented semantic communication
framework for metaverse application (GSCM) to explore and define semantic
information through the goal levels. Specifically, we first analyze the
traditional image communication framework in metaverse construction and then
detail our proposed semantic information along with the end-to-end wireless
communication. We then describe the designed modules of the GSCM framework,
including goal-oriented semantic information extraction, base knowledge
definition, and neural radiance field (NeRF) based metaverse construction.
Finally, numerous experiments have been conducted to demonstrate that, compared
to image communication, our proposed GSCM framework decreases transmission
latency by up to 92.6% and enhances the virtual object operation accuracy and
metaverse construction clearance by up to 45.6% and 44.7%, respectively.



---

## Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2408.03822v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has recently emerged as an alternative
representation that leverages a 3D Gaussian-based representation and introduces
an approximated volumetric rendering, achieving very fast rendering speed and
promising image quality. Furthermore, subsequent studies have successfully
extended 3DGS to dynamic 3D scenes, demonstrating its wide range of
applications. However, a significant drawback arises as 3DGS and its following
methods entail a substantial number of Gaussians to maintain the high fidelity
of the rendered images, which requires a large amount of memory and storage. To
address this critical issue, we place a specific emphasis on two key
objectives: reducing the number of Gaussian points without sacrificing
performance and compressing the Gaussian attributes, such as view-dependent
color and covariance. To this end, we propose a learnable mask strategy that
significantly reduces the number of Gaussians while preserving high
performance. In addition, we propose a compact but effective representation of
view-dependent color by employing a grid-based neural field rather than relying
on spherical harmonics. Finally, we learn codebooks to compactly represent the
geometric and temporal attributes by residual vector quantization. With model
compression techniques such as quantization and entropy coding, we consistently
show over 25x reduced storage and enhanced rendering speed compared to 3DGS for
static scenes, while maintaining the quality of the scene representation. For
dynamic scenes, our approach achieves more than 12x storage efficiency and
retains a high-quality reconstruction compared to the existing state-of-the-art
methods. Our work provides a comprehensive framework for 3D scene
representation, achieving high performance, fast training, compactness, and
real-time rendering. Our project page is available at
https://maincold2.github.io/c3dgs/.

Comments:
- Project page: https://maincold2.github.io/c3dgs/

---

## Towards Real-Time Gaussian Splatting: Accelerating 3DGS through  Photometric SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Yan Song Hu, Dayou Mao, Yuhao Chen, John Zelek | cs.RO | [PDF](http://arxiv.org/pdf/2408.03825v1){: .btn .btn-green } |

**Abstract**: Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous
Localization and Mapping (VSLAM) demonstrate the generation of high-quality
volumetric reconstructions from monocular video streams. However, despite these
promising advancements, current 3DGS integrations have reduced tracking
performance and lower operating speeds compared to traditional VSLAM. To
address these issues, we propose integrating 3DGS with Direct Sparse Odometry,
a monocular photometric SLAM system. We have done preliminary experiments
showing that using Direct Sparse Odometry point cloud outputs, as opposed to
standard structure-from-motion methods, significantly shortens the training
time needed to achieve high-quality renders. Reducing 3DGS training time
enables the development of 3DGS-integrated SLAM systems that operate in
real-time on mobile hardware. These promising initial findings suggest further
exploration is warranted in combining traditional VSLAM systems with 3DGS.

Comments:
- This extended abstract has been submitted to be presented at an IEEE
  conference. It will be made available online by IEEE but will not be
  published in IEEE Xplore. Copyright may be transferred without notice, after
  which this version may no longer be accessible

---

## MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Tengfei Wang, Zongqian Zhan, Rui Xia, Linxia Ji, Xin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.03060v1){: .btn .btn-green } |

**Abstract**: Over the last few decades, image-based building surface reconstruction has
garnered substantial research interest and has been applied across various
fields, such as heritage preservation, architectural planning, etc. Compared to
the traditional photogrammetric and NeRF-based solutions, recently, Gaussian
fields-based methods have exhibited significant potential in generating surface
meshes due to their time-efficient training and detailed 3D information
preservation. However, most gaussian fields-based methods are trained with all
image pixels, encompassing building and nonbuilding areas, which results in a
significant noise for building meshes and degeneration in time efficiency. This
paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to
generate accurate surface reconstruction for building in a time-efficient way.
The framework first applies EfficientSAM and COLMAP to generate multi-level
masks of building and the corresponding masked point clouds. Subsequently, the
masked gaussian fields are trained by integrating two innovative losses: a
multi-level perceptual masked loss focused on constructing building regions and
a boundary loss aimed at enhancing the details of the boundaries between
different masks. Finally, we improve the tetrahedral surface mesh extraction
method based on the masked gaussian spheres. Comprehensive experiments on UAV
images demonstrate that, compared to the traditional method and several
NeRF-based and Gaussian-based SOTA solutions, our approach significantly
improves both the accuracy and efficiency of building surface reconstruction.
Notably, as a byproduct, there is an additional gain in the novel view
synthesis of building.



---

## Efficient NeRF Optimization -- Not All Samples Remain Equally Hard

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Juuso Korhonen, Goutham Rangu, Hamed R. Tavakoli, Juho Kannala | cs.CV | [PDF](http://arxiv.org/pdf/2408.03193v1){: .btn .btn-green } |

**Abstract**: We propose an application of online hard sample mining for efficient training
of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality
for many 3D reconstruction and rendering tasks but require substantial
computational resources. The encoding of the scene information within the NeRF
network parameters necessitates stochastic sampling. We observe that during the
training, a major part of the compute time and memory usage is spent on
processing already learnt samples, which no longer affect the model update
significantly. We identify the backward pass on the stochastic samples as the
computational bottleneck during the optimization. We thus perform the first
forward pass in inference mode as a relatively low-cost search for hard
samples. This is followed by building the computational graph and updating the
NeRF network parameters using only the hard samples. To demonstrate the
effectiveness of the proposed approach, we apply our method to Instant-NGP,
resulting in significant improvements of the view-synthesis quality over the
baseline (1 dB improvement on average per training time, or 2x speedup to reach
the same PSNR level) along with approx. 40% memory savings coming from using
only the hard samples to build the computational graph. As our method only
interfaces with the network module, we expect it to be widely applicable.



---

## LumiGauss: High-Fidelity Outdoor Relighting with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, Marek Kowalski | cs.CV | [PDF](http://arxiv.org/pdf/2408.04474v1){: .btn .btn-green } |

**Abstract**: Decoupling lighting from geometry using unconstrained photo collections is
notoriously challenging. Solving it would benefit many users, as creating
complex 3D assets takes days of manual labor. Many previous works have
attempted to address this issue, often at the expense of output fidelity, which
questions the practicality of such methods.
  We introduce LumiGauss, a technique that tackles 3D reconstruction of scenes
and environmental lighting through 2D Gaussian Splatting. Our approach yields
high-quality scene reconstructions and enables realistic lighting synthesis
under novel environment maps. We also propose a method for enhancing the
quality of shadows, common in outdoor scenes, by exploiting spherical harmonics
properties. Our approach facilitates seamless integration with game engines and
enables the use of fast precomputed radiance transfer.
  We validate our method on the NeRF-OSR dataset, demonstrating superior
performance over baseline methods. Moreover, LumiGauss can synthesize realistic
images when applying novel environment maps.

Comments:
- Includes video files in src

---

## RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel  View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic | cs.CV | [PDF](http://arxiv.org/pdf/2408.03356v1){: .btn .btn-green } |

**Abstract**: Differentiable volumetric rendering-based methods made significant progress
in novel view synthesis. On one hand, innovative methods have replaced the
Neural Radiance Fields (NeRF) network with locally parameterized structures,
enabling high-quality renderings in a reasonable time. On the other hand,
approaches have used differentiable splatting instead of NeRF's ray casting to
optimize radiance fields rapidly using Gaussian kernels, allowing for fine
adaptation to the scene. However, differentiable ray casting of irregularly
spaced kernels has been scarcely explored, while splatting, despite enabling
fast rendering times, is susceptible to clearly visible artifacts.
  Our work closes this gap by providing a physically consistent formulation of
the emitted radiance c and density {\sigma}, decomposed with Gaussian functions
associated with Spherical Gaussians/Harmonics for all-frequency colorimetric
representation. We also introduce a method enabling differentiable ray casting
of irregularly distributed Gaussians using an algorithm that integrates
radiance fields slab by slab and leverages a BVH structure. This allows our
approach to finely adapt to the scene while avoiding splatting artifacts. As a
result, we achieve superior rendering quality compared to the state-of-the-art
while maintaining reasonable training times and achieving inference speeds of
25 FPS on the Blender dataset. Project page with videos and code:
https://raygauss.github.io/

Comments:
- Project page with videos and code: https://raygauss.github.io/

---

## PanicleNeRF: low-cost, high-precision in-field phenotypingof rice  panicles with smartphone

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-04 | Xin Yang, Xuqi Lu, Pengyao Xie, Ziyue Guo, Hui Fang, Haowei Fu, Xiaochun Hu, Zhenbiao Sun, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2408.02053v1){: .btn .btn-green } |

**Abstract**: The rice panicle traits significantly influence grain yield, making them a
primary target for rice phenotyping studies. However, most existing techniques
are limited to controlled indoor environments and difficult to capture the rice
panicle traits under natural growth conditions. Here, we developed PanicleNeRF,
a novel method that enables high-precision and low-cost reconstruction of rice
panicle three-dimensional (3D) models in the field using smartphone. The
proposed method combined the large model Segment Anything Model (SAM) and the
small model You Only Look Once version 8 (YOLOv8) to achieve high-precision
segmentation of rice panicle images. The NeRF technique was then employed for
3D reconstruction using the images with 2D segmentation. Finally, the resulting
point clouds are processed to successfully extract panicle traits. The results
show that PanicleNeRF effectively addressed the 2D image segmentation task,
achieving a mean F1 Score of 86.9% and a mean Intersection over Union (IoU) of
79.8%, with nearly double the boundary overlap (BO) performance compared to
YOLOv8. As for point cloud quality, PanicleNeRF significantly outperformed
traditional SfM-MVS (structure-from-motion and multi-view stereo) methods, such
as COLMAP and Metashape. The panicle length was then accurately extracted with
the rRMSE of 2.94% for indica and 1.75% for japonica rice. The panicle volume
estimated from 3D point clouds strongly correlated with the grain number (R2 =
0.85 for indica and 0.82 for japonica) and grain mass (0.80 for indica and 0.76
for japonica). This method provides a low-cost solution for high-throughput
in-field phenotyping of rice panicles, accelerating the efficiency of rice
breeding.



---

## E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-03 | Yunshan Qi, Jia Li, Yifan Zhao, Yu Zhang, Lin Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2408.01840v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieve impressive rendering performance by
learning volumetric 3D representation from several images of different views.
However, it is difficult to reconstruct a sharp NeRF from blurry input as it
often occurs in the wild. To solve this problem, we propose a novel Efficient
Event-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and
event streams. To effectively introduce event streams into the neural
volumetric representation learning process, we propose an event-enhanced blur
rendering loss and an event rendering loss, which guide the network via
modeling the real blur process and event generation process, respectively.
Specifically, we leverage spatial-temporal information from the event stream to
evenly distribute learning attention over temporal blur while simultaneously
focusing on blurry texture through the spatial attention. Moreover, a camera
pose estimation framework for real-world data is built with the guidance of the
events to generalize the method to practical applications. Compared to previous
image-based or event-based NeRF, our framework makes more profound use of the
internal relationship between events and images. Extensive experiments on both
synthetic data and real-world data demonstrate that E$^3$NeRF can effectively
learn a sharp NeRF from blurry images, especially in non-uniform motion and
low-light scenes.



---

## FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and  Fisheye Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-03 | Yifan Wu, Tianyi Cheng, Peixu Xin, Janusz Konrad | cs.CV | [PDF](http://arxiv.org/pdf/2408.01878v1){: .btn .btn-green } |

**Abstract**: Previous studies aiming to optimize and bundle-adjust camera poses using
Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated
impressive capabilities in 3D scene reconstruction. However, these approaches
have been designed for pinhole-camera pose optimization and do not perform well
under radial image distortions such as those in fisheye cameras. Furthermore,
inaccurate depth initialization in DBARF results in erroneous geometric
information affecting the overall convergence and quality of results. In this
paper, we propose adaptive GRUs with a flexible bundle-adjustment method
adapted to radial distortions and incorporate feature-based recurrent neural
networks to generate continuous novel views from fisheye datasets. Other NeRF
methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray
distance loss for distorted pose refinement, causing severe artifacts, long
rendering time, and are difficult to use in downstream tasks, where the dense
voxel representation generated by a NeRF method needs to be converted into a
mesh representation. We also address depth initialization issues by adding
MiDaS-based depth priors for pinhole images. Through extensive experiments, we
demonstrate the generalization capacity of FBINeRF and show high-fidelity
results for both pinhole-camera and fisheye-camera NeRFs.

Comments:
- 18 pages

---

## IG-SLAM: Instant Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | F. Aykut Sarikamis, A. Aydin Alatan | cs.CV | [PDF](http://arxiv.org/pdf/2408.01126v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently shown promising results as an alternative
scene representation in SLAM systems to neural implicit representations.
However, current methods either lack dense depth maps to supervise the mapping
process or detailed training designs that consider the scale of the
environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only
SLAM system that employs robust Dense-SLAM methods for tracking and combines
them with Gaussian Splatting. A 3D map of the environment is constructed using
accurate pose and dense depth provided by tracking. Additionally, we utilize
depth uncertainty in map optimization to improve 3D reconstruction. Our decay
strategy in map optimization enhances convergence and allows the system to run
at 10 fps in a single process. We demonstrate competitive performance with
state-of-the-art RGB-only SLAM systems while achieving faster operation speeds.
We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC
datasets. The system achieves photo-realistic 3D reconstruction in large-scale
sequences, particularly in the EuRoC dataset.

Comments:
- 8 pages, 3 page ref, 5 figures

---

## A General Framework to Boost 3D GS Initialization for Text-to-3D  Generation by Lexical Richness


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Lutao Jiang, Hangyu Li, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.01269v1){: .btn .btn-green } |

**Abstract**: Text-to-3D content creation has recently received much attention, especially
with the prevalence of 3D Gaussians Splatting. In general, GS-based methods
comprise two key stages: initialization and rendering optimization. To achieve
initialization, existing works directly apply random sphere initialization or
3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such
strategies suffer from two critical yet challenging problems: 1) the final
shapes are still similar to the initial ones even after training; 2) shapes can
be produced only from simple texts, e.g., "a dog", not for lexically richer
texts, e.g., "a dog is sitting on the top of the airplane". To address these
problems, this paper proposes a novel general framework to boost the 3D GS
Initialization for text-to-3D generation upon the lexical richness. Our key
idea is to aggregate 3D Gaussians into spatially uniform voxels to represent
complex shapes while enabling the spatial interaction among the 3D Gaussians
and semantic interaction between Gaussians and texts. Specifically, we first
construct a voxelized representation, where each voxel holds a 3D Gaussian with
its position, scale, and rotation fixed while setting opacity as the sole
factor to determine a position's occupancy. We then design an initialization
network mainly consisting of two novel components: 1) Global Information
Perception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design
enables each 3D Gaussian to assimilate the spatial information from other areas
and semantic information from texts. Extensive experiments show the superiority
of our framework of high-quality 3D GS initialization against the existing
methods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.
Also, our framework can be seamlessly plugged into SoTA training frameworks,
e.g., LucidDreamer, for semantically consistent text-to-3D generation.



---

## NeRFoot: Robot-Footprint Estimation for Image-Based Visual Servoing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Daoxin Zhong, Luke Robinson, Daniele De Martini | cs.RO | [PDF](http://arxiv.org/pdf/2408.01251v1){: .btn .btn-green } |

**Abstract**: This paper investigates the utility of Neural Radiance Fields (NeRF) models
in extending the regions of operation of a mobile robot, controlled by
Image-Based Visual Servoing (IBVS) via static CCTV cameras. Using NeRF as a
3D-representation prior, the robot's footprint may be extrapolated
geometrically and used to train a CNN-based network to extract it online from
the robot's appearance alone. The resulting footprint results in a tighter
bound than a robot-wide bounding box, allowing the robot's controller to
prescribe more optimal trajectories and expand its safe operational floor area.



---

## Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation  with Volumetric Visual Data Fusion


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Ke Li, Reinhard Bacher, Susanne Schmidt, Wim Leemans, Frank Steinicke | cs.RO | [PDF](http://arxiv.org/pdf/2408.01225v1){: .btn .btn-green } |

**Abstract**: We introduce Reality Fusion, a novel robot teleoperation system that
localizes, streams, projects, and merges a typical onboard depth sensor with a
photorealistic, high resolution, high framerate, and wide field of view (FoV)
rendering of the complex remote environment represented as 3D Gaussian splats
(3DGS). Our framework enables robust egocentric and exocentric robot
teleoperation in immersive VR, with the 3DGS effectively extending spatial
information of a depth sensor with limited FoV and balancing the trade-off
between data streaming costs and data visual quality. We evaluated our
framework through a user study with 24 participants, which revealed that
Reality Fusion leads to significantly better user performance, situation
awareness, and user preferences. To support further research and development,
we provide an open-source implementation with an easy-to-replicate custom-made
telepresence robot, a high-performance virtual reality 3DGS renderer, and an
immersive robot control package. (Source code:
https://github.com/uhhhci/RealityFusion)

Comments:
- Accepted, to appear at IROS 2024

---

## UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with  Ultrasound Reflection Direction Parameterization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Ziwen Guo, Zi Fang, Zhuang Fu | cs.AI | [PDF](http://arxiv.org/pdf/2408.00860v2){: .btn .btn-green } |

**Abstract**: Three-dimensional ultrasound imaging is a critical technology widely used in
medical diagnostics. However, traditional 3D ultrasound imaging methods have
limitations such as fixed resolution, low storage efficiency, and insufficient
contextual connectivity, leading to poor performance in handling complex
artifacts and reflection characteristics. Recently, techniques based on NeRF
(Neural Radiance Fields) have made significant progress in view synthesis and
3D reconstruction, but there remains a research gap in high-quality ultrasound
imaging. To address these issues, we propose a new model, UlRe-NeRF, which
combines implicit neural networks and explicit ultrasound volume rendering into
an ultrasound neural rendering architecture. This model incorporates reflection
direction parameterization and harmonic encoding, using a directional MLP
module to generate view-dependent high-frequency reflection intensity
estimates, and a spatial MLP module to produce the medium's physical property
parameters. These parameters are used in the volume rendering process to
accurately reproduce the propagation and reflection behavior of ultrasound
waves in the medium. Experimental results demonstrate that the UlRe-NeRF model
significantly enhances the realism and accuracy of high-fidelity ultrasound
image reconstruction, especially in handling complex medium structures.



---

## LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Zhenyu Bao, Guibiao Liao, Kaichen Zhou, Kanglin Liu, Qing Li, Guoping Qiu | cs.CV | [PDF](http://arxiv.org/pdf/2408.00254v1){: .btn .btn-green } |

**Abstract**: Despite the photorealistic novel view synthesis (NVS) performance achieved by
the original 3D Gaussian splatting (3DGS), its rendering quality significantly
degrades with sparse input views. This performance drop is mainly caused by the
limited number of initial points generated from the sparse input, insufficient
supervision during the training process, and inadequate regularization of the
oversized Gaussian ellipsoids. To handle these issues, we propose the
LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis
task. In specific, we propose a loop-based Progressive Gaussian Initialization
(PGI) strategy that could iteratively densify the initialized point cloud using
the rendered pseudo images during the training process. Then, the sparse and
reliable depth from the Structure from Motion, and the window-based dense
monocular depth are leveraged to provide precise geometric supervision via the
proposed Depth-alignment Regularization (DAR). Additionally, we introduce a
novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian
ellipsoids leading to large pixel errors. Comprehensive experiments on four
datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art
methods for sparse-input novel view synthesis, across indoor, outdoor, and
object-level scenes with various image resolutions.

Comments:
- 13 pages, 10 figures
