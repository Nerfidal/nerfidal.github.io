---
layout: default
title: August 2024
parent: Papers
nav_order: 202408
---

<!---metadata--->


## Towards Real-Time Gaussian Splatting: Accelerating 3DGS through  Photometric SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Yan Song Hu, Dayou Mao, Yuhao Chen, John Zelek | cs.RO | [PDF](http://arxiv.org/pdf/2408.03825v1){: .btn .btn-green } |

**Abstract**: Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous
Localization and Mapping (VSLAM) demonstrate the generation of high-quality
volumetric reconstructions from monocular video streams. However, despite these
promising advancements, current 3DGS integrations have reduced tracking
performance and lower operating speeds compared to traditional VSLAM. To
address these issues, we propose integrating 3DGS with Direct Sparse Odometry,
a monocular photometric SLAM system. We have done preliminary experiments
showing that using Direct Sparse Odometry point cloud outputs, as opposed to
standard structure-from-motion methods, significantly shortens the training
time needed to achieve high-quality renders. Reducing 3DGS training time
enables the development of 3DGS-integrated SLAM systems that operate in
real-time on mobile hardware. These promising initial findings suggest further
exploration is warranted in combining traditional VSLAM systems with 3DGS.

Comments:
- This extended abstract has been submitted to be presented at an IEEE
  conference. It will be made available online by IEEE but will not be
  published in IEEE Xplore. Copyright may be transferred without notice, after
  which this version may no longer be accessible

---

## PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time  High-Quality Relighting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Yijia Guo, Yuanxi Bai, Liwen Hu, Ziyi Guo, Mianzhi Liu, Yu Cai, Tiejun Huang, Lei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2408.03538v1){: .btn .btn-green } |

**Abstract**: We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a
real-time high-quality relighting method for Gaussian splats in low-frequency
lighting environments that captures soft shadows and interreflections by
precomputing 3D Gaussian splats' radiance transfer. Existing studies have
demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'
efficiency for dynamic lighting scenarios. However, the current relighting
method based on 3DGS still struggles to compute high-quality shadow and
indirect illumination in real time for dynamic light, leading to unrealistic
rendering results. We solve this problem by precomputing the expensive
transport simulations required for complex transfer functions like shadowing,
the resulting transfer functions are represented as dense sets of vectors or
matrices for every Gaussian splat. We introduce distinct precomputing methods
tailored for training and rendering stages, along with unique ray tracing and
indirect lighting precomputation techniques for 3D Gaussian splats to
accelerate training speed and compute accurate indirect lighting related to
environment light. Experimental analyses demonstrate that our approach achieves
state-of-the-art visual quality while maintaining competitive training times
and allows high-quality real-time (30+ fps) relighting for dynamic light and
relatively complex scenes at 1080p resolution.



---

## Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2408.03822v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has recently emerged as an alternative
representation that leverages a 3D Gaussian-based representation and introduces
an approximated volumetric rendering, achieving very fast rendering speed and
promising image quality. Furthermore, subsequent studies have successfully
extended 3DGS to dynamic 3D scenes, demonstrating its wide range of
applications. However, a significant drawback arises as 3DGS and its following
methods entail a substantial number of Gaussians to maintain the high fidelity
of the rendered images, which requires a large amount of memory and storage. To
address this critical issue, we place a specific emphasis on two key
objectives: reducing the number of Gaussian points without sacrificing
performance and compressing the Gaussian attributes, such as view-dependent
color and covariance. To this end, we propose a learnable mask strategy that
significantly reduces the number of Gaussians while preserving high
performance. In addition, we propose a compact but effective representation of
view-dependent color by employing a grid-based neural field rather than relying
on spherical harmonics. Finally, we learn codebooks to compactly represent the
geometric and temporal attributes by residual vector quantization. With model
compression techniques such as quantization and entropy coding, we consistently
show over 25x reduced storage and enhanced rendering speed compared to 3DGS for
static scenes, while maintaining the quality of the scene representation. For
dynamic scenes, our approach achieves more than 12x storage efficiency and
retains a high-quality reconstruction compared to the existing state-of-the-art
methods. Our work provides a comprehensive framework for 3D scene
representation, achieving high performance, fast training, compactness, and
real-time rendering. Our project page is available at
https://maincold2.github.io/c3dgs/.

Comments:
- Project page: https://maincold2.github.io/c3dgs/

---

## 3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Zhe Jun Tang, Tat-Jen Cham | cs.CV | [PDF](http://arxiv.org/pdf/2408.03753v1){: .btn .btn-green } |

**Abstract**: The use of 3D Gaussians as representation of radiance fields has enabled high
quality novel view synthesis at real-time rendering speed. However, the choice
of optimising the outgoing radiance of each Gaussian independently as spherical
harmonics results in unsatisfactory view dependent effects. In response to
these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian
Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering
quality. Instead of optimising a single outgoing radiance parameter, 3iGS
enhances 3DGS view-dependent effects by expressing the outgoing radiance as a
function of a local illumination field and Bidirectional Reflectance
Distribution Function (BRDF) features. We optimise a continuous incident
illumination field through a Tensorial Factorisation representation, while
separately fine-tuning the BRDF features of each 3D Gaussian relative to this
illumination field. Our methodology significantly enhances the rendering
quality of specular view-dependent effects of 3DGS, while maintaining rapid
training and rendering speeds.

Comments:
- The 18th European Conference on Computer Vision ECCV 2024

---

## Goal-oriented Semantic Communication for the Metaverse Application

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Zhe Wang, Nan Li, Yansha Deng | eess.SY | [PDF](http://arxiv.org/pdf/2408.03646v1){: .btn .btn-green } |

**Abstract**: With the emergence of the metaverse and its role in enabling real-time
simulation and analysis of real-world counterparts, an increasing number of
personalized metaverse scenarios are being created to influence entertainment
experiences and social behaviors. However, compared to traditional image and
video entertainment applications, the exact transmission of the vast amount of
metaverse-associated information significantly challenges the capacity of
existing bit-oriented communication networks. Moreover, the current metaverse
also witnesses a growing goal shift for transmitting the meaning behind
custom-designed content, such as user-designed buildings and avatars, rather
than exact copies of physical objects. To meet this growing goal shift and
bandwidth challenge, this paper proposes a goal-oriented semantic communication
framework for metaverse application (GSCM) to explore and define semantic
information through the goal levels. Specifically, we first analyze the
traditional image communication framework in metaverse construction and then
detail our proposed semantic information along with the end-to-end wireless
communication. We then describe the designed modules of the GSCM framework,
including goal-oriented semantic information extraction, base knowledge
definition, and neural radiance field (NeRF) based metaverse construction.
Finally, numerous experiments have been conducted to demonstrate that, compared
to image communication, our proposed GSCM framework decreases transmission
latency by up to 92.6% and enhances the virtual object operation accuracy and
metaverse construction clearance by up to 45.6% and 44.7%, respectively.



---

## Efficient NeRF Optimization -- Not All Samples Remain Equally Hard

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Juuso Korhonen, Goutham Rangu, Hamed R. Tavakoli, Juho Kannala | cs.CV | [PDF](http://arxiv.org/pdf/2408.03193v1){: .btn .btn-green } |

**Abstract**: We propose an application of online hard sample mining for efficient training
of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality
for many 3D reconstruction and rendering tasks but require substantial
computational resources. The encoding of the scene information within the NeRF
network parameters necessitates stochastic sampling. We observe that during the
training, a major part of the compute time and memory usage is spent on
processing already learnt samples, which no longer affect the model update
significantly. We identify the backward pass on the stochastic samples as the
computational bottleneck during the optimization. We thus perform the first
forward pass in inference mode as a relatively low-cost search for hard
samples. This is followed by building the computational graph and updating the
NeRF network parameters using only the hard samples. To demonstrate the
effectiveness of the proposed approach, we apply our method to Instant-NGP,
resulting in significant improvements of the view-synthesis quality over the
baseline (1 dB improvement on average per training time, or 2x speedup to reach
the same PSNR level) along with approx. 40% memory savings coming from using
only the hard samples to build the computational graph. As our method only
interfaces with the network module, we expect it to be widely applicable.



---

## MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Tengfei Wang, Zongqian Zhan, Rui Xia, Linxia Ji, Xin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.03060v1){: .btn .btn-green } |

**Abstract**: Over the last few decades, image-based building surface reconstruction has
garnered substantial research interest and has been applied across various
fields, such as heritage preservation, architectural planning, etc. Compared to
the traditional photogrammetric and NeRF-based solutions, recently, Gaussian
fields-based methods have exhibited significant potential in generating surface
meshes due to their time-efficient training and detailed 3D information
preservation. However, most gaussian fields-based methods are trained with all
image pixels, encompassing building and nonbuilding areas, which results in a
significant noise for building meshes and degeneration in time efficiency. This
paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to
generate accurate surface reconstruction for building in a time-efficient way.
The framework first applies EfficientSAM and COLMAP to generate multi-level
masks of building and the corresponding masked point clouds. Subsequently, the
masked gaussian fields are trained by integrating two innovative losses: a
multi-level perceptual masked loss focused on constructing building regions and
a boundary loss aimed at enhancing the details of the boundaries between
different masks. Finally, we improve the tetrahedral surface mesh extraction
method based on the masked gaussian spheres. Comprehensive experiments on UAV
images demonstrate that, compared to the traditional method and several
NeRF-based and Gaussian-based SOTA solutions, our approach significantly
improves both the accuracy and efficiency of building surface reconstruction.
Notably, as a byproduct, there is an additional gain in the novel view
synthesis of building.



---

## RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel  View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic | cs.CV | [PDF](http://arxiv.org/pdf/2408.03356v1){: .btn .btn-green } |

**Abstract**: Differentiable volumetric rendering-based methods made significant progress
in novel view synthesis. On one hand, innovative methods have replaced the
Neural Radiance Fields (NeRF) network with locally parameterized structures,
enabling high-quality renderings in a reasonable time. On the other hand,
approaches have used differentiable splatting instead of NeRF's ray casting to
optimize radiance fields rapidly using Gaussian kernels, allowing for fine
adaptation to the scene. However, differentiable ray casting of irregularly
spaced kernels has been scarcely explored, while splatting, despite enabling
fast rendering times, is susceptible to clearly visible artifacts.
  Our work closes this gap by providing a physically consistent formulation of
the emitted radiance c and density {\sigma}, decomposed with Gaussian functions
associated with Spherical Gaussians/Harmonics for all-frequency colorimetric
representation. We also introduce a method enabling differentiable ray casting
of irregularly distributed Gaussians using an algorithm that integrates
radiance fields slab by slab and leverages a BVH structure. This allows our
approach to finely adapt to the scene while avoiding splatting artifacts. As a
result, we achieve superior rendering quality compared to the state-of-the-art
while maintaining reasonable training times and achieving inference speeds of
25 FPS on the Blender dataset. Project page with videos and code:
https://raygauss.github.io/

Comments:
- Project page with videos and code: https://raygauss.github.io/

---

## PanicleNeRF: low-cost, high-precision in-field phenotypingof rice  panicles with smartphone

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-04 | Xin Yang, Xuqi Lu, Pengyao Xie, Ziyue Guo, Hui Fang, Haowei Fu, Xiaochun Hu, Zhenbiao Sun, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2408.02053v1){: .btn .btn-green } |

**Abstract**: The rice panicle traits significantly influence grain yield, making them a
primary target for rice phenotyping studies. However, most existing techniques
are limited to controlled indoor environments and difficult to capture the rice
panicle traits under natural growth conditions. Here, we developed PanicleNeRF,
a novel method that enables high-precision and low-cost reconstruction of rice
panicle three-dimensional (3D) models in the field using smartphone. The
proposed method combined the large model Segment Anything Model (SAM) and the
small model You Only Look Once version 8 (YOLOv8) to achieve high-precision
segmentation of rice panicle images. The NeRF technique was then employed for
3D reconstruction using the images with 2D segmentation. Finally, the resulting
point clouds are processed to successfully extract panicle traits. The results
show that PanicleNeRF effectively addressed the 2D image segmentation task,
achieving a mean F1 Score of 86.9% and a mean Intersection over Union (IoU) of
79.8%, with nearly double the boundary overlap (BO) performance compared to
YOLOv8. As for point cloud quality, PanicleNeRF significantly outperformed
traditional SfM-MVS (structure-from-motion and multi-view stereo) methods, such
as COLMAP and Metashape. The panicle length was then accurately extracted with
the rRMSE of 2.94% for indica and 1.75% for japonica rice. The panicle volume
estimated from 3D point clouds strongly correlated with the grain number (R2 =
0.85 for indica and 0.82 for japonica) and grain mass (0.80 for indica and 0.76
for japonica). This method provides a low-cost solution for high-throughput
in-field phenotyping of rice panicles, accelerating the efficiency of rice
breeding.



---

## FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and  Fisheye Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-03 | Yifan Wu, Tianyi Cheng, Peixu Xin, Janusz Konrad | cs.CV | [PDF](http://arxiv.org/pdf/2408.01878v1){: .btn .btn-green } |

**Abstract**: Previous studies aiming to optimize and bundle-adjust camera poses using
Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated
impressive capabilities in 3D scene reconstruction. However, these approaches
have been designed for pinhole-camera pose optimization and do not perform well
under radial image distortions such as those in fisheye cameras. Furthermore,
inaccurate depth initialization in DBARF results in erroneous geometric
information affecting the overall convergence and quality of results. In this
paper, we propose adaptive GRUs with a flexible bundle-adjustment method
adapted to radial distortions and incorporate feature-based recurrent neural
networks to generate continuous novel views from fisheye datasets. Other NeRF
methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray
distance loss for distorted pose refinement, causing severe artifacts, long
rendering time, and are difficult to use in downstream tasks, where the dense
voxel representation generated by a NeRF method needs to be converted into a
mesh representation. We also address depth initialization issues by adding
MiDaS-based depth priors for pinhole images. Through extensive experiments, we
demonstrate the generalization capacity of FBINeRF and show high-fidelity
results for both pinhole-camera and fisheye-camera NeRFs.

Comments:
- 18 pages

---

## E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-03 | Yunshan Qi, Jia Li, Yifan Zhao, Yu Zhang, Lin Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2408.01840v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieve impressive rendering performance by
learning volumetric 3D representation from several images of different views.
However, it is difficult to reconstruct a sharp NeRF from blurry input as it
often occurs in the wild. To solve this problem, we propose a novel Efficient
Event-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and
event streams. To effectively introduce event streams into the neural
volumetric representation learning process, we propose an event-enhanced blur
rendering loss and an event rendering loss, which guide the network via
modeling the real blur process and event generation process, respectively.
Specifically, we leverage spatial-temporal information from the event stream to
evenly distribute learning attention over temporal blur while simultaneously
focusing on blurry texture through the spatial attention. Moreover, a camera
pose estimation framework for real-world data is built with the guidance of the
events to generalize the method to practical applications. Compared to previous
image-based or event-based NeRF, our framework makes more profound use of the
internal relationship between events and images. Extensive experiments on both
synthetic data and real-world data demonstrate that E$^3$NeRF can effectively
learn a sharp NeRF from blurry images, especially in non-uniform motion and
low-light scenes.



---

## IG-SLAM: Instant Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | F. Aykut Sarikamis, A. Aydin Alatan | cs.CV | [PDF](http://arxiv.org/pdf/2408.01126v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently shown promising results as an alternative
scene representation in SLAM systems to neural implicit representations.
However, current methods either lack dense depth maps to supervise the mapping
process or detailed training designs that consider the scale of the
environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only
SLAM system that employs robust Dense-SLAM methods for tracking and combines
them with Gaussian Splatting. A 3D map of the environment is constructed using
accurate pose and dense depth provided by tracking. Additionally, we utilize
depth uncertainty in map optimization to improve 3D reconstruction. Our decay
strategy in map optimization enhances convergence and allows the system to run
at 10 fps in a single process. We demonstrate competitive performance with
state-of-the-art RGB-only SLAM systems while achieving faster operation speeds.
We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC
datasets. The system achieves photo-realistic 3D reconstruction in large-scale
sequences, particularly in the EuRoC dataset.

Comments:
- 8 pages, 3 page ref, 5 figures

---

## A General Framework to Boost 3D GS Initialization for Text-to-3D  Generation by Lexical Richness


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Lutao Jiang, Hangyu Li, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.01269v1){: .btn .btn-green } |

**Abstract**: Text-to-3D content creation has recently received much attention, especially
with the prevalence of 3D Gaussians Splatting. In general, GS-based methods
comprise two key stages: initialization and rendering optimization. To achieve
initialization, existing works directly apply random sphere initialization or
3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such
strategies suffer from two critical yet challenging problems: 1) the final
shapes are still similar to the initial ones even after training; 2) shapes can
be produced only from simple texts, e.g., "a dog", not for lexically richer
texts, e.g., "a dog is sitting on the top of the airplane". To address these
problems, this paper proposes a novel general framework to boost the 3D GS
Initialization for text-to-3D generation upon the lexical richness. Our key
idea is to aggregate 3D Gaussians into spatially uniform voxels to represent
complex shapes while enabling the spatial interaction among the 3D Gaussians
and semantic interaction between Gaussians and texts. Specifically, we first
construct a voxelized representation, where each voxel holds a 3D Gaussian with
its position, scale, and rotation fixed while setting opacity as the sole
factor to determine a position's occupancy. We then design an initialization
network mainly consisting of two novel components: 1) Global Information
Perception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design
enables each 3D Gaussian to assimilate the spatial information from other areas
and semantic information from texts. Extensive experiments show the superiority
of our framework of high-quality 3D GS initialization against the existing
methods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.
Also, our framework can be seamlessly plugged into SoTA training frameworks,
e.g., LucidDreamer, for semantically consistent text-to-3D generation.



---

## Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation  with Volumetric Visual Data Fusion


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Ke Li, Reinhard Bacher, Susanne Schmidt, Wim Leemans, Frank Steinicke | cs.RO | [PDF](http://arxiv.org/pdf/2408.01225v1){: .btn .btn-green } |

**Abstract**: We introduce Reality Fusion, a novel robot teleoperation system that
localizes, streams, projects, and merges a typical onboard depth sensor with a
photorealistic, high resolution, high framerate, and wide field of view (FoV)
rendering of the complex remote environment represented as 3D Gaussian splats
(3DGS). Our framework enables robust egocentric and exocentric robot
teleoperation in immersive VR, with the 3DGS effectively extending spatial
information of a depth sensor with limited FoV and balancing the trade-off
between data streaming costs and data visual quality. We evaluated our
framework through a user study with 24 participants, which revealed that
Reality Fusion leads to significantly better user performance, situation
awareness, and user preferences. To support further research and development,
we provide an open-source implementation with an easy-to-replicate custom-made
telepresence robot, a high-performance virtual reality 3DGS renderer, and an
immersive robot control package. (Source code:
https://github.com/uhhhci/RealityFusion)

Comments:
- Accepted, to appear at IROS 2024

---

## NeRFoot: Robot-Footprint Estimation for Image-Based Visual Servoing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Daoxin Zhong, Luke Robinson, Daniele De Martini | cs.RO | [PDF](http://arxiv.org/pdf/2408.01251v1){: .btn .btn-green } |

**Abstract**: This paper investigates the utility of Neural Radiance Fields (NeRF) models
in extending the regions of operation of a mobile robot, controlled by
Image-Based Visual Servoing (IBVS) via static CCTV cameras. Using NeRF as a
3D-representation prior, the robot's footprint may be extrapolated
geometrically and used to train a CNN-based network to extract it online from
the robot's appearance alone. The resulting footprint results in a tighter
bound than a robot-wide bounding box, allowing the robot's controller to
prescribe more optimal trajectories and expand its safe operational floor area.



---

## UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with  Ultrasound Reflection Direction Parameterization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Ziwen Guo, Zi Fang, Zhuang Fu | cs.AI | [PDF](http://arxiv.org/pdf/2408.00860v2){: .btn .btn-green } |

**Abstract**: Three-dimensional ultrasound imaging is a critical technology widely used in
medical diagnostics. However, traditional 3D ultrasound imaging methods have
limitations such as fixed resolution, low storage efficiency, and insufficient
contextual connectivity, leading to poor performance in handling complex
artifacts and reflection characteristics. Recently, techniques based on NeRF
(Neural Radiance Fields) have made significant progress in view synthesis and
3D reconstruction, but there remains a research gap in high-quality ultrasound
imaging. To address these issues, we propose a new model, UlRe-NeRF, which
combines implicit neural networks and explicit ultrasound volume rendering into
an ultrasound neural rendering architecture. This model incorporates reflection
direction parameterization and harmonic encoding, using a directional MLP
module to generate view-dependent high-frequency reflection intensity
estimates, and a spatial MLP module to produce the medium's physical property
parameters. These parameters are used in the volume rendering process to
accurately reproduce the propagation and reflection behavior of ultrasound
waves in the medium. Experimental results demonstrate that the UlRe-NeRF model
significantly enhances the realism and accuracy of high-fidelity ultrasound
image reconstruction, especially in handling complex medium structures.



---

## LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Zhenyu Bao, Guibiao Liao, Kaichen Zhou, Kanglin Liu, Qing Li, Guoping Qiu | cs.CV | [PDF](http://arxiv.org/pdf/2408.00254v1){: .btn .btn-green } |

**Abstract**: Despite the photorealistic novel view synthesis (NVS) performance achieved by
the original 3D Gaussian splatting (3DGS), its rendering quality significantly
degrades with sparse input views. This performance drop is mainly caused by the
limited number of initial points generated from the sparse input, insufficient
supervision during the training process, and inadequate regularization of the
oversized Gaussian ellipsoids. To handle these issues, we propose the
LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis
task. In specific, we propose a loop-based Progressive Gaussian Initialization
(PGI) strategy that could iteratively densify the initialized point cloud using
the rendered pseudo images during the training process. Then, the sparse and
reliable depth from the Structure from Motion, and the window-based dense
monocular depth are leveraged to provide precise geometric supervision via the
proposed Depth-alignment Regularization (DAR). Additionally, we introduce a
novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian
ellipsoids leading to large pixel errors. Comprehensive experiments on four
datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art
methods for sparse-input novel view synthesis, across indoor, outdoor, and
object-level scenes with various image resolutions.

Comments:
- 13 pages, 10 figures
