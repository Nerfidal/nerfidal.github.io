---
layout: default
title: August 2024
parent: Papers
nav_order: 202408
---

<!---metadata--->


## A General Framework to Boost 3D GS Initialization for Text-to-3D  Generation by Lexical Richness


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Lutao Jiang, Hangyu Li, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.01269v1){: .btn .btn-green } |

**Abstract**: Text-to-3D content creation has recently received much attention, especially
with the prevalence of 3D Gaussians Splatting. In general, GS-based methods
comprise two key stages: initialization and rendering optimization. To achieve
initialization, existing works directly apply random sphere initialization or
3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such
strategies suffer from two critical yet challenging problems: 1) the final
shapes are still similar to the initial ones even after training; 2) shapes can
be produced only from simple texts, e.g., "a dog", not for lexically richer
texts, e.g., "a dog is sitting on the top of the airplane". To address these
problems, this paper proposes a novel general framework to boost the 3D GS
Initialization for text-to-3D generation upon the lexical richness. Our key
idea is to aggregate 3D Gaussians into spatially uniform voxels to represent
complex shapes while enabling the spatial interaction among the 3D Gaussians
and semantic interaction between Gaussians and texts. Specifically, we first
construct a voxelized representation, where each voxel holds a 3D Gaussian with
its position, scale, and rotation fixed while setting opacity as the sole
factor to determine a position's occupancy. We then design an initialization
network mainly consisting of two novel components: 1) Global Information
Perception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design
enables each 3D Gaussian to assimilate the spatial information from other areas
and semantic information from texts. Extensive experiments show the superiority
of our framework of high-quality 3D GS initialization against the existing
methods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.
Also, our framework can be seamlessly plugged into SoTA training frameworks,
e.g., LucidDreamer, for semantically consistent text-to-3D generation.



---

## Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation  with Volumetric Visual Data Fusion


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Ke Li, Reinhard Bacher, Susanne Schmidt, Wim Leemans, Frank Steinicke | cs.RO | [PDF](http://arxiv.org/pdf/2408.01225v1){: .btn .btn-green } |

**Abstract**: We introduce Reality Fusion, a novel robot teleoperation system that
localizes, streams, projects, and merges a typical onboard depth sensor with a
photorealistic, high resolution, high framerate, and wide field of view (FoV)
rendering of the complex remote environment represented as 3D Gaussian splats
(3DGS). Our framework enables robust egocentric and exocentric robot
teleoperation in immersive VR, with the 3DGS effectively extending spatial
information of a depth sensor with limited FoV and balancing the trade-off
between data streaming costs and data visual quality. We evaluated our
framework through a user study with 24 participants, which revealed that
Reality Fusion leads to significantly better user performance, situation
awareness, and user preferences. To support further research and development,
we provide an open-source implementation with an easy-to-replicate custom-made
telepresence robot, a high-performance virtual reality 3DGS renderer, and an
immersive robot control package. (Source code:
https://github.com/uhhhci/RealityFusion)

Comments:
- Accepted, to appear at IROS 2024

---

## IG-SLAM: Instant Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Furkan Aykut Sarikamis, Abdullah Aydin Alatan | cs.CV | [PDF](http://arxiv.org/pdf/2408.01126v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently shown promising results as an alternative
scene representation in SLAM systems to neural implicit representations.
However, current methods either lack dense depth maps to supervise the mapping
process or detailed training designs that consider the scale of the
environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only
SLAM system that employs robust Dense-SLAM methods for tracking and combines
them with Gaussian Splatting. A 3D map of the environment is constructed using
accurate pose and dense depth provided by tracking. Additionally, we utilize
depth uncertainty in map optimization to improve 3D reconstruction. Our decay
strategy in map optimization enhances convergence and allows the system to run
at 10 fps in a single process. We demonstrate competitive performance with
state-of-the-art RGB-only SLAM systems while achieving faster operation speeds.
We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC
datasets. The system achieves photo-realistic 3D reconstruction in large-scale
sequences, particularly in the EuRoC dataset.

Comments:
- 8 pages, 3 page ref, 5 figures, 3DV submission

---

## NeRFoot: Robot-Footprint Estimation for Image-Based Visual Servoing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Daoxin Zhong, Luke Robinson, Daniele De Martini | cs.RO | [PDF](http://arxiv.org/pdf/2408.01251v1){: .btn .btn-green } |

**Abstract**: This paper investigates the utility of Neural Radiance Fields (NeRF) models
in extending the regions of operation of a mobile robot, controlled by
Image-Based Visual Servoing (IBVS) via static CCTV cameras. Using NeRF as a
3D-representation prior, the robot's footprint may be extrapolated
geometrically and used to train a CNN-based network to extract it online from
the robot's appearance alone. The resulting footprint results in a tighter
bound than a robot-wide bounding box, allowing the robot's controller to
prescribe more optimal trajectories and expand its safe operational floor area.



---

## UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with  Ultrasound Reflection Direction Parameterization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Ziwen Guo, Zi Fang, Zhuang Fu | cs.AI | [PDF](http://arxiv.org/pdf/2408.00860v1){: .btn .btn-green } |

**Abstract**: Three-dimensional ultrasound imaging is a critical technology widely used in
medical diagnostics. However, traditional 3D ultrasound imaging methods have
limitations such as fixed resolution, low storage efficiency, and insufficient
contextual connectivity, leading to poor performance in handling complex
artifacts and reflection characteristics. Recently, techniques based on NeRF
(Neural Radiance Fields) have made significant progress in view synthesis and
3D reconstruction, but there remains a research gap in high-quality ultrasound
imaging. To address these issues, we propose a new model, UlRe-NeRF, which
combines implicit neural networks and explicit ultrasound volume rendering into
an ultrasound neural rendering architecture. This model incorporates reflection
direction parameterization and harmonic encoding, using a directional MLP
module to generate view-dependent high-frequency reflection intensity
estimates, and a spatial MLP module to produce the medium's physical property
parameters. These parameters are used in the volume rendering process to
accurately reproduce the propagation and reflection behavior of ultrasound
waves in the medium. Experimental results demonstrate that the UlRe-NeRF model
significantly enhances the realism and accuracy of high-fidelity ultrasound
image reconstruction, especially in handling complex medium structures.



---

## LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Zhenyu Bao, Guibiao Liao, Kaichen Zhou, Kanglin Liu, Qing Li, Guoping Qiu | cs.CV | [PDF](http://arxiv.org/pdf/2408.00254v1){: .btn .btn-green } |

**Abstract**: Despite the photorealistic novel view synthesis (NVS) performance achieved by
the original 3D Gaussian splatting (3DGS), its rendering quality significantly
degrades with sparse input views. This performance drop is mainly caused by the
limited number of initial points generated from the sparse input, insufficient
supervision during the training process, and inadequate regularization of the
oversized Gaussian ellipsoids. To handle these issues, we propose the
LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis
task. In specific, we propose a loop-based Progressive Gaussian Initialization
(PGI) strategy that could iteratively densify the initialized point cloud using
the rendered pseudo images during the training process. Then, the sparse and
reliable depth from the Structure from Motion, and the window-based dense
monocular depth are leveraged to provide precise geometric supervision via the
proposed Depth-alignment Regularization (DAR). Additionally, we introduce a
novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian
ellipsoids leading to large pixel errors. Comprehensive experiments on four
datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art
methods for sparse-input novel view synthesis, across indoor, outdoor, and
object-level scenes with various image resolutions.

Comments:
- 13 pages, 10 figures
