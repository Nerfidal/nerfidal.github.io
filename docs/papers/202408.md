---
layout: default
title: August 2024
parent: Papers
nav_order: 202408
---

<!---metadata--->


## UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-31 | Mostafa Mansour, Ahmed Abdelsalam, Ari Happonen, Jari Porras, Esa Rahtu | cs.CV | [PDF](http://arxiv.org/pdf/2409.00362v1){: .btn .btn-green } |

**Abstract**: Recent advancements in monocular neural depth estimation, particularly those
achieved by the UniDepth network, have prompted the investigation of
integrating UniDepth within a Gaussian splatting framework for monocular
SLAM.This study presents UDGS-SLAM, a novel approach that eliminates the
necessity of RGB-D sensors for depth estimation within Gaussian splatting
framework. UDGS-SLAM employs statistical filtering to ensure local consistency
of the estimated depth and jointly optimizes camera trajectory and Gaussian
scene representation parameters. The proposed method achieves high-fidelity
rendered images and low ATERMSE of the camera trajectory. The performance of
UDGS-SLAM is rigorously evaluated using the TUM RGB-D dataset and benchmarked
against several baseline methods, demonstrating superior performance across
various scenarios. Additionally, an ablation study is conducted to validate
design choices and investigate the impact of different network backbone
encoders on system performance.



---

## 3D Gaussian Splatting for Large-scale 3D Surface Reconstruction from  Aerial Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-31 | YuanZheng Wu, Jin Liu, Shunping Ji | cs.CV | [PDF](http://arxiv.org/pdf/2409.00381v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention.
However, the unstructured nature of 3DGS poses challenges for large-scale
surface reconstruction from aerial images. To address this gap, we propose the
first large-scale surface reconstruction method for multi-view stereo (MVS)
aerial images based on 3DGS, named Aerial Gaussian Splatting (AGS). Initially,
we introduce a data chunking method tailored for large-scale aerial imagery,
making the modern 3DGS technology feasible for surface reconstruction over
extensive scenes. Additionally, we integrate the Ray-Gaussian Intersection
method to obtain normal and depth information, facilitating geometric
constraints. Finally, we introduce a multi-view geometric consistency
constraint to enhance global geometric consistency and improve reconstruction
accuracy. Our experiments on multiple datasets demonstrate for the first time
that the GS-based technique can match traditional aerial MVS methods on
geometric accuracy, and beat state-of-the-art GS-based methods on geometry and
rendering quality.

Comments:
- 11 pages

---

## OG-Mapping: Octree-based Structured 3D Gaussians for Online Dense  Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-30 | Meng Wang, Junyi Wang, Changqun Xia, Chen Wang, Yue Qi | cs.CV | [PDF](http://arxiv.org/pdf/2408.17223v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has recently demonstrated promising advancements
in RGB-D online dense mapping. Nevertheless, existing methods excessively rely
on per-pixel depth cues to perform map densification, which leads to
significant redundancy and increased sensitivity to depth noise. Additionally,
explicitly storing 3D Gaussian parameters of room-scale scene poses a
significant storage challenge. In this paper, we introduce OG-Mapping, which
leverages the robust scene structural representation capability of sparse
octrees, combined with structured 3D Gaussian representations, to achieve
efficient and robust online dense mapping. Moreover, OG-Mapping employs an
anchor-based progressive map refinement strategy to recover the scene
structures at multiple levels of detail. Instead of maintaining a small number
of active keyframes with a fixed keyframe window as previous approaches do, a
dynamic keyframe window is employed to allow OG-Mapping to better tackle false
local minima and forgetting issues. Experimental results demonstrate that
OG-Mapping delivers more robust and superior realism mapping results than
existing Gaussian-based RGB-D online mapping methods with a compact model, and
no additional post-processing is required.



---

## ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features  from Multi-View Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-30 | Xiaoshuai Zhang, Zhicheng Wang, Howard Zhou, Soham Ghosh, Danushen Gnanapragasam, Varun Jampani, Hao Su, Leonidas Guibas | cs.CV | [PDF](http://arxiv.org/pdf/2408.17027v1){: .btn .btn-green } |

**Abstract**: To advance the state of the art in the creation of 3D foundation models, this
paper introduces the ConDense framework for 3D pre-training utilizing existing
pre-trained 2D networks and large-scale multi-view datasets. We propose a novel
2D-3D joint training scheme to extract co-embedded 2D and 3D features in an
end-to-end pipeline, where 2D-3D feature consistency is enforced through a
volume rendering NeRF-like ray marching process. Using dense per pixel features
we are able to 1) directly distill the learned priors from 2D models to 3D
models and create useful 3D backbones, 2) extract more consistent and less
noisy 2D features, 3) formulate a consistent embedding space where 2D, 3D, and
other modalities of data (e.g., natural language prompts) can be jointly
queried. Furthermore, besides dense features, ConDense can be trained to
extract sparse features (e.g., key points), also with 2D-3D consistency --
condensing 3D NeRF representations into compact sets of decorated key points.
We demonstrate that our pre-trained model provides good initialization for
various 3D tasks including 3D classification and segmentation, outperforming
other 3D pre-training methods by a significant margin. It also enables, by
exploiting our sparse features, additional useful downstream tasks, such as
matching 2D images to 3D scenes, detecting duplicate 3D scenes, and querying a
repository of 3D scenes through natural language -- all quite efficiently and
without any per-scene fine-tuning.

Comments:
- ECCV 2024

---

## 2DGH: 2D Gaussian-Hermite Splatting for High-quality Rendering and  Better Geometry Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-30 | Ruihan Yu, Tianyu Huang, Jingwang Ling, Feng Xu | cs.CV | [PDF](http://arxiv.org/pdf/2408.16982v1){: .btn .btn-green } |

**Abstract**: 2D Gaussian Splatting has recently emerged as a significant method in 3D
reconstruction, enabling novel view synthesis and geometry reconstruction
simultaneously. While the well-known Gaussian kernel is broadly used, its lack
of anisotropy and deformation ability leads to dim and vague edges at object
silhouettes, limiting the reconstruction quality of current Gaussian splatting
methods. To enhance the representation power, we draw inspiration from quantum
physics and propose to use the Gaussian-Hermite kernel as the new primitive in
Gaussian splatting. The new kernel takes a unified mathematical form and
extends the Gaussian function, which serves as the zero-rank term in the
updated formulation. Our experiments demonstrate the extraordinary performance
of Gaussian-Hermite kernel in both geometry reconstruction and novel-view
synthesis tasks. The proposed kernel outperforms traditional Gaussian Splatting
kernels, showcasing its potential for high-quality 3D reconstruction and
rendering.



---

## Generic Objects as Pose Probes for Few-Shot View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-29 | Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2408.16690v2){: .btn .btn-green } |

**Abstract**: Radiance fields including NeRFs and 3D Gaussians demonstrate great potential
in high-fidelity rendering and scene reconstruction, while they require a
substantial number of posed images as inputs. COLMAP is frequently employed for
preprocessing to estimate poses, while it necessitates a large number of
feature matches to operate effectively, and it struggles with scenes
characterized by sparse features, large baselines between images, or a limited
number of input images. We aim to tackle few-view NeRF reconstruction using
only 3 to 6 unposed scene images. Traditional methods often use calibration
boards but they are not common in images. We propose a novel idea of utilizing
everyday objects, commonly found in both images and real life, as "pose
probes". The probe object is automatically segmented by SAM, whose shape is
initialized from a cube. We apply a dual-branch volume rendering optimization
(object NeRF and scene NeRF) to constrain the pose optimization and jointly
refine the geometry. Specifically, object poses of two views are first
estimated by PnP matching in an SDF representation, which serves as initial
poses. PnP matching, requiring only a few features, is suitable for
feature-sparse scenes. Additional views are incrementally incorporated to
refine poses from preceding views. In experiments, PoseProbe achieves
state-of-the-art performance in both pose estimation and novel view synthesis
across multiple datasets. We demonstrate its effectiveness, particularly in
few-view and large-baseline scenes where COLMAP struggles. In ablations, using
different objects in a scene yields comparable performance. Our project page is
available at: \href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this
https URL}



---

## GameIR: A Large-Scale Synthesized Ground-Truth Dataset for Image  Restoration over Gaming Content

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-29 | Lebin Zhou, Kun Han, Nam Ling, Wei Wang, Wei Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2408.16866v1){: .btn .btn-green } |

**Abstract**: Image restoration methods like super-resolution and image synthesis have been
successfully used in commercial cloud gaming products like NVIDIA's DLSS.
However, restoration over gaming content is not well studied by the general
public. The discrepancy is mainly caused by the lack of ground-truth gaming
training data that match the test cases. Due to the unique characteristics of
gaming content, the common approach of generating pseudo training data by
degrading the original HR images results in inferior restoration performance.
In this work, we develop GameIR, a large-scale high-quality
computer-synthesized ground-truth dataset to fill in the blanks, targeting at
two different applications. The first is super-resolution with deferred
rendering, to support the gaming solution of rendering and transferring LR
images only and restoring HR images on the client side. We provide 19200 LR-HR
paired ground-truth frames coming from 640 videos rendered at 720p and 1440p
for this task. The second is novel view synthesis (NVS), to support the
multiview gaming solution of rendering and transferring part of the multiview
frames and generating the remaining frames on the client side. This task has
57,600 HR frames from 960 videos of 160 scenes with 6 camera views. In addition
to the RGB frames, the GBuffers during the deferred rendering stage are also
provided, which can be used to help restoration. Furthermore, we evaluate
several SOTA super-resolution algorithms and NeRF-based NVS algorithms over our
dataset, which demonstrates the effectiveness of our ground-truth GameIR data
in improving restoration performance for gaming content. Also, we test the
method of incorporating the GBuffers as additional input information for
helping super-resolution and NVS. We release our dataset and models to the
general public to facilitate research on restoration methods over gaming
content.



---

## Spurfies: Sparse Surface Reconstruction using Local Geometry Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-29 | Kevin Raj, Christopher Wewer, Raza Yunus, Eddy Ilg, Jan Eric Lenssen | cs.CV | [PDF](http://arxiv.org/pdf/2408.16544v1){: .btn .btn-green } |

**Abstract**: We introduce Spurfies, a novel method for sparse-view surface reconstruction
that disentangles appearance and geometry information to utilize local geometry
priors trained on synthetic data. Recent research heavily focuses on 3D
reconstruction using dense multi-view setups, typically requiring hundreds of
images. However, these methods often struggle with few-view scenarios. Existing
sparse-view reconstruction techniques often rely on multi-view stereo networks
that need to learn joint priors for geometry and appearance from a large amount
of data. In contrast, we introduce a neural point representation that
disentangles geometry and appearance to train a local geometry prior using a
subset of the synthetic ShapeNet dataset only. During inference, we utilize
this surface prior as additional constraint for surface and appearance
reconstruction from sparse input views via differentiable volume rendering,
restricting the space of possible solutions. We validate the effectiveness of
our method on the DTU dataset and demonstrate that it outperforms previous
state of the art by 35% in surface quality while achieving competitive novel
view synthesis quality. Moreover, in contrast to previous works, our method can
be applied to larger, unbounded scenes, such as Mip-NeRF 360.

Comments:
- https://geometric-rl.mpi-inf.mpg.de/spurfies/

---

## NeRF-CA: Dynamic Reconstruction of X-ray Coronary Angiography with  Extremely Sparse-views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-29 | Kirsten W. H. Maas, Danny Ruijters, Anna Vilanova, Nicola Pezzotti | eess.IV | [PDF](http://arxiv.org/pdf/2408.16355v1){: .btn .btn-green } |

**Abstract**: Dynamic three-dimensional (4D) reconstruction from two-dimensional X-ray
coronary angiography (CA) remains a significant clinical problem. Challenges
include sparse-view settings, intra-scan motion, and complex vessel morphology
such as structure sparsity and background occlusion. Existing CA reconstruction
methods often require extensive user interaction or large training datasets. On
the other hand, Neural Radiance Field (NeRF), a promising deep learning
technique, has successfully reconstructed high-fidelity static scenes for
natural and medical scenes. Recent work, however, identified that sparse-views,
background occlusion, and dynamics still pose a challenge when applying NeRF in
the X-ray angiography context. Meanwhile, many successful works for natural
scenes propose regularization for sparse-view reconstruction or scene
decomposition to handle dynamics. However, these techniques do not directly
translate to the CA context, where both challenges and background occlusion are
significant. This paper introduces NeRF-CA, the first step toward a 4D CA
reconstruction method that achieves reconstructions from sparse coronary
angiograms with cardiac motion. We leverage the motion of the coronary artery
to decouple the scene into a dynamic coronary artery component and static
background. We combine this scene decomposition with tailored regularization
techniques. These techniques enforce the separation of the coronary artery from
the background by enforcing dynamic structure sparsity and scene smoothness. By
uniquely combining these approaches, we achieve 4D reconstructions from as few
as four angiogram sequences. This setting aligns with clinical workflows while
outperforming state-of-the-art X-ray sparse-view NeRF reconstruction
techniques. We validate our approach quantitatively and qualitatively using 4D
phantom datasets and ablation studies.



---

## ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion  Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-29 | Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan | cs.CV | [PDF](http://arxiv.org/pdf/2408.16767v1){: .btn .btn-green } |

**Abstract**: Advancements in 3D scene reconstruction have transformed 2D images from the
real world into 3D models, producing realistic 3D results from hundreds of
input photos. Despite great success in dense-view reconstruction scenarios,
rendering a detailed scene from insufficient captured views is still an
ill-posed optimization problem, often resulting in artifacts and distortions in
unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction
paradigm that reframes the ambiguous reconstruction challenge as a temporal
generation task. The key insight is to unleash the strong generative prior of
large pre-trained video diffusion models for sparse-view reconstruction.
However, 3D view consistency struggles to be accurately preserved in directly
generated video frames from pre-trained models. To address this, given limited
input views, the proposed ReconX first constructs a global point cloud and
encodes it into a contextual space as the 3D structure condition. Guided by the
condition, the video diffusion model then synthesizes video frames that are
both detail-preserved and exhibit a high degree of 3D consistency, ensuring the
coherence of the scene from various perspectives. Finally, we recover the 3D
scene from the generated video through a confidence-aware 3D Gaussian Splatting
optimization scheme. Extensive experiments on various real-world datasets show
the superiority of our ReconX over state-of-the-art methods in terms of quality
and generalizability.

Comments:
- Project page: https://liuff19.github.io/ReconX

---

## OmniRe: Omni Urban Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-29 | Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.16760v1){: .btn .btn-green } |

**Abstract**: We introduce OmniRe, a holistic approach for efficiently reconstructing
high-fidelity dynamic urban scenes from on-device logs. Recent methods for
modeling driving sequences using neural radiance fields or Gaussian Splatting
have demonstrated the potential of reconstructing challenging dynamic scenes,
but often overlook pedestrians and other non-vehicle dynamic actors, hindering
a complete pipeline for dynamic urban scene reconstruction. To that end, we
propose a comprehensive 3DGS framework for driving scenes, named OmniRe, that
allows for accurate, full-length reconstruction of diverse dynamic objects in a
driving log. OmniRe builds dynamic neural scene graphs based on Gaussian
representations and constructs multiple local canonical spaces that model
various dynamic actors, including vehicles, pedestrians, and cyclists, among
many others. This capability is unmatched by existing methods. OmniRe allows us
to holistically reconstruct different objects present in the scene,
subsequently enabling the simulation of reconstructed scenarios with all actors
participating in real-time (~60Hz). Extensive evaluations on the Waymo dataset
show that our approach outperforms prior state-of-the-art methods
quantitatively and qualitatively by a large margin. We believe our work fills a
critical gap in driving reconstruction.

Comments:
- See the project page for code, video results and demos:
  https://ziyc.github.io/omnire/

---

## Towards Realistic Example-based Modeling via 3D Gaussian Stitching

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-28 | Xinyu Gao, Ziyi Yang, Bingchen Gong, Xiaoguang Han, Sipeng Yang, Xiaogang Jin | cs.CV | [PDF](http://arxiv.org/pdf/2408.15708v1){: .btn .btn-green } |

**Abstract**: Using parts of existing models to rebuild new models, commonly termed as
example-based modeling, is a classical methodology in the realm of computer
graphics. Previous works mostly focus on shape composition, making them very
hard to use for realistic composition of 3D objects captured from real-world
scenes. This leads to combining multiple NeRFs into a single 3D scene to
achieve seamless appearance blending. However, the current SeamlessNeRF method
struggles to achieve interactive editing and harmonious stitching for
real-world scenes due to its gradient-based strategy and grid-based
representation. To this end, we present an example-based modeling method that
combines multiple Gaussian fields in a point-based representation using
sample-guided synthesis. Specifically, as for composition, we create a GUI to
segment and transform multiple fields in real time, easily obtaining a
semantically meaningful composition of models represented by 3D Gaussian
Splatting (3DGS). For texture blending, due to the discrete and irregular
nature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF
is not supported. Thus, a novel sampling-based cloning method is proposed to
harmonize the blending while preserving the original rich texture and content.
Our workflow consists of three steps: 1) real-time segmentation and
transformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis
to identify boundary points in the intersecting area between the source and
target models, and 3) two-phase optimization of the target model using
sampling-based cloning and gradient constraints. Extensive experimental results
validate that our approach significantly outperforms previous works in terms of
realistic synthesis, demonstrating its practicality. More demos are available
at https://ingra14m.github.io/gs_stitching_website.



---

## G-Style: Stylized Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-28 | Áron Samuel Kovács, Pedro Hermosilla, Renata G. Raidou | cs.GR | [PDF](http://arxiv.org/pdf/2408.15695v2){: .btn .btn-green } |

**Abstract**: We introduce G-Style, a novel algorithm designed to transfer the style of an
image onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting
is a powerful 3D representation for novel view synthesis, as -- compared to
other approaches based on Neural Radiance Fields -- it provides fast scene
renderings and user control over the scene. Recent pre-prints have demonstrated
that the style of Gaussian Splatting scenes can be modified using an image
exemplar. However, since the scene geometry remains fixed during the
stylization process, current solutions fall short of producing satisfactory
results. Our algorithm aims to address these limitations by following a
three-step process: In a pre-processing step, we remove undesirable Gaussians
with large projection areas or highly elongated shapes. Subsequently, we
combine several losses carefully designed to preserve different scales of the
style in the image, while maintaining as much as possible the integrity of the
original scene content. During the stylization process and following the
original design of Gaussian Splatting, we split Gaussians where additional
detail is necessary within our scene by tracking the gradient of the stylized
color. Our experiments demonstrate that G-Style generates high-quality
stylizations within just a few minutes, outperforming existing methods both
qualitatively and quantitatively.



---

## Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-27 | Saining Zhang, Baijun Ye, Xiaoxue Chen, Yuantao Chen, Zongzheng Zhang, Cheng Peng, Yongliang Shi, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2408.15242v1){: .btn .btn-green } |

**Abstract**: Robust and realistic rendering for large-scale road scenes is essential in
autonomous driving simulation. Recently, 3D Gaussian Splatting (3D-GS) has made
groundbreaking progress in neural rendering, but the general fidelity of
large-scale road scene renderings is often limited by the input imagery, which
usually has a narrow field of view and focuses mainly on the street-level local
area. Intuitively, the data from the drone's perspective can provide a
complementary viewpoint for the data from the ground vehicle's perspective,
enhancing the completeness of scene reconstruction and rendering. However,
training naively with aerial and ground images, which exhibit large view
disparity, poses a significant convergence challenge for 3D-GS, and does not
demonstrate remarkable improvements in performance on road views. In order to
enhance the novel view synthesis of road views and to effectively use the
aerial information, we design an uncertainty-aware training method that allows
aerial images to assist in the synthesis of areas where ground images have poor
learning outcomes instead of weighting all pixels equally in 3D-GS training
like prior work did. We are the first to introduce the cross-view uncertainty
to 3D-GS by matching the car-view ensemble-based rendering uncertainty to
aerial images, weighting the contribution of each pixel to the training
process. Additionally, to systematically quantify evaluation metrics, we
assemble a high-quality synthesized dataset comprising both aerial and ground
images for road scenes.

Comments:
- BMVC2024 Project Page: https://sainingzhang.github.io/project/uc-gs/
  Code: https://github.com/SainingZhang/uc-gs/

---

## Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm  with Hybrid Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-27 | Haozhe Lou, Yurong Liu, Yike Pan, Yiran Geng, Jianteng Chen, Wenlong Ma, Chenglong Li, Lin Wang, Hengzhen Feng, Lu Shi, Liyi Luo, Yongliang Shi | cs.RO | [PDF](http://arxiv.org/pdf/2408.14873v1){: .btn .btn-green } |

**Abstract**: Real2Sim2Real plays a critical role in robotic arm control and reinforcement
learning, yet bridging this gap remains a significant challenge due to the
complex physical properties of robots and the objects they manipulate. Existing
methods lack a comprehensive solution to accurately reconstruct real-world
objects with spatial representations and their associated physics attributes.
  We propose a Real2Sim pipeline with a hybrid representation model that
integrates mesh geometry, 3D Gaussian kernels, and physics attributes to
enhance the digital asset representation of robotic arms.
  This hybrid representation is implemented through a Gaussian-Mesh-Pixel
binding technique, which establishes an isomorphic mapping between mesh
vertices and Gaussian models. This enables a fully differentiable rendering
pipeline that can be optimized through numerical solvers, achieves
high-fidelity rendering via Gaussian Splatting, and facilitates physically
plausible simulation of the robotic arm's interaction with its environment
using mesh-based methods.
  The code,full presentation and datasets will be made publicly available at
our website https://robostudioapp.com



---

## LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive  Streaming

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-27 | Yuang Shi, Simone Gasparini, Géraldine Morin, Wei Tsang Ooi | cs.CV | [PDF](http://arxiv.org/pdf/2408.14823v1){: .btn .btn-green } |

**Abstract**: The rise of Extended Reality (XR) requires efficient streaming of 3D online
worlds, challenging current 3DGS representations to adapt to
bandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS
that supports adaptive streaming and progressive rendering. Our method
constructs a layered structure for cumulative representation, incorporates
dynamic opacity optimization to maintain visual fidelity, and utilizes
occupancy maps to efficiently manage Gaussian splats. This proposed model
offers a progressive representation supporting a continuous rendering quality
adapted for bandwidth-aware streaming. Extensive experiments validate the
effectiveness of our approach in balancing visual fidelity with the compactness
of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in
LPIPS, and 318.41% reduction in model size, and shows its potential for
bandwidth-adapted 3D streaming and rendering applications.



---

## GeoTransfer : Generalizable Few-Shot Multi-View Reconstruction via  Transfer Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-27 | Shubhendu Jena, Franck Multon, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2408.14724v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel approach for sparse 3D reconstruction by
leveraging the expressive power of Neural Radiance Fields (NeRFs) and fast
transfer of their features to learn accurate occupancy fields. Existing 3D
reconstruction methods from sparse inputs still struggle with capturing
intricate geometric details and can suffer from limitations in handling
occluded regions. On the other hand, NeRFs excel in modeling complex scenes but
do not offer means to extract meaningful geometry. Our proposed method offers
the best of both worlds by transferring the information encoded in NeRF
features to derive an accurate occupancy field representation. We utilize a
pre-trained, generalizable state-of-the-art NeRF network to capture detailed
scene radiance information, and rapidly transfer this knowledge to train a
generalizable implicit occupancy network. This process helps in leveraging the
knowledge of the scene geometry encoded in the generalizable NeRF prior and
refining it to learn occupancy fields, facilitating a more precise
generalizable representation of 3D space. The transfer learning approach leads
to a dramatic reduction in training time, by orders of magnitude (i.e. from
several days to 3.5 hrs), obviating the need to train generalizable sparse
surface reconstruction methods from scratch. Additionally, we introduce a novel
loss on volumetric rendering weights that helps in the learning of accurate
occupancy fields, along with a normal loss that helps in global smoothing of
the occupancy fields. We evaluate our approach on the DTU dataset and
demonstrate state-of-the-art performance in terms of reconstruction accuracy,
especially in challenging scenarios with sparse input data and occluded
regions. We furthermore demonstrate the generalization capabilities of our
method by showing qualitative results on the Blended MVS dataset without any
retraining.



---

## Learning-based Multi-View Stereo: A Survey

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-27 | Fangjinhua Wang, Qingtian Zhu, Di Chang, Quankai Gao, Junlin Han, Tong Zhang, Richard Hartley, Marc Pollefeys | cs.CV | [PDF](http://arxiv.org/pdf/2408.15235v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction aims to recover the dense 3D structure of a scene. It plays
an essential role in various applications such as Augmented/Virtual Reality
(AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene
captured from different viewpoints, Multi-View Stereo (MVS) algorithms
synthesize a comprehensive 3D representation, enabling precise reconstruction
in complex environments. Due to its efficiency and effectiveness, MVS has
become a pivotal method for image-based 3D reconstruction. Recently, with the
success of deep learning, many learning-based MVS methods have been proposed,
achieving impressive performance against traditional methods. We categorize
these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D
Gaussian Splatting-based, and large feed-forward methods. Among these, we focus
significantly on depth map-based methods, which are the main family of MVS due
to their conciseness, flexibility and scalability. In this survey, we provide a
comprehensive review of the literature at the time of this writing. We
investigate these learning-based methods, summarize their performances on
popular benchmarks, and discuss promising future research directions in this
area.



---

## FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-26 | Chunran Zheng, Wei Xu, Zuhao Zou, Tong Hua, Chongjian Yuan, Dongjiao He, Bingyang Zhou, Zheng Liu, Jiarong Lin, Fangcheng Zhu, Yunfan Ren, Rong Wang, Fanle Meng, Fu Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2408.14035v2){: .btn .btn-green } |

**Abstract**: This paper proposes FAST-LIVO2: a fast, direct LiDAR-inertial-visual odometry
framework to achieve accurate and robust state estimation in SLAM tasks and
provide great potential in real-time, onboard robotic applications. FAST-LIVO2
fuses the IMU, LiDAR and image measurements efficiently through an ESIKF. To
address the dimension mismatch between the heterogeneous LiDAR and image
measurements, we use a sequential update strategy in the Kalman filter. To
enhance the efficiency, we use direct methods for both the visual and LiDAR
fusion, where the LiDAR module registers raw points without extracting edge or
plane features and the visual module minimizes direct photometric errors
without extracting ORB or FAST corner features. The fusion of both visual and
LiDAR measurements is based on a single unified voxel map where the LiDAR
module constructs the geometric structure for registering new LiDAR scans and
the visual module attaches image patches to the LiDAR points. To enhance the
accuracy of image alignment, we use plane priors from the LiDAR points in the
voxel map (and even refine the plane prior) and update the reference patch
dynamically after new images are aligned. Furthermore, to enhance the
robustness of image alignment, FAST-LIVO2 employs an on-demanding raycast
operation and estimates the image exposure time in real time. Lastly, we detail
three applications of FAST-LIVO2: UAV onboard navigation demonstrating the
system's computation efficiency for real-time onboard navigation, airborne
mapping showcasing the system's mapping accuracy, and 3D model rendering
(mesh-based and NeRF-based) underscoring the suitability of our reconstructed
dense map for subsequent rendering tasks. We open source our code, dataset and
application on GitHub to benefit the robotics community.

Comments:
- 30 pages, 31 figures, due to the limitation that 'The abstract field
  cannot exceed 1,920 characters', the abstract presented here is shorter than
  the one in the PDF file

---

## Avatar Concept Slider: Manipulate Concepts In Your Human Avatar With  Fine-grained Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-26 | Yixuan He, Lin Geng Foo, Ajmal Saeed Mian, Hossein Rahmani, Jun Jiu | cs.CV | [PDF](http://arxiv.org/pdf/2408.13995v1){: .btn .btn-green } |

**Abstract**: Language based editing of 3D human avatars to precisely match user
requirements is challenging due to the inherent ambiguity and limited
expressiveness of natural language. To overcome this, we propose the Avatar
Concept Slider (ACS), a 3D avatar editing method that allows precise
manipulation of semantic concepts in human avatars towards a specified
intermediate point between two extremes of concepts, akin to moving a knob
along a slider track. To achieve this, our ACS has three designs. 1) A Concept
Sliding Loss based on Linear Discriminant Analysis to pinpoint the
concept-specific axis for precise editing. 2) An Attribute Preserving Loss
based on Principal Component Analysis for improved preservation of avatar
identity during editing. 3) A 3D Gaussian Splatting primitive selection
mechanism based on concept-sensitivity, which updates only the primitives that
are the most sensitive to our target concept, to improve efficiency. Results
demonstrate that our ACS enables fine-grained 3D avatar editing with efficient
feedback, without harming the avatar quality or compromising the avatar's
identifying attributes.



---

## DynaSurfGS: Dynamic Surface Reconstruction with Planar-based Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-26 | Weiwei Cai, Weicai Ye, Peng Ye, Tong He, Tao Chen | cs.CV | [PDF](http://arxiv.org/pdf/2408.13972v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction has garnered significant attention in recent
years due to its capabilities in high-quality and real-time rendering. Among
various methodologies, constructing a 4D spatial-temporal representation, such
as 4D-GS, has gained popularity for its high-quality rendered images. However,
these methods often produce suboptimal surfaces, as the discrete 3D Gaussian
point clouds fail to align with the object's surface precisely. To address this
problem, we propose DynaSurfGS to achieve both photorealistic rendering and
high-fidelity surface reconstruction of dynamic scenarios. Specifically, the
DynaSurfGS framework first incorporates Gaussian features from 4D neural voxels
with the planar-based Gaussian Splatting to facilitate precise surface
reconstruction. It leverages normal regularization to enforce the smoothness of
the surface of dynamic objects. It also incorporates the as-rigid-as-possible
(ARAP) constraint to maintain the approximate rigidity of local neighborhoods
of 3D Gaussians between timesteps and ensure that adjacent 3D Gaussians remain
closely aligned throughout. Extensive experiments demonstrate that DynaSurfGS
surpasses state-of-the-art methods in both high-fidelity surface reconstruction
and photorealistic rendering.

Comments:
- homepage: https://open3dvlab.github.io/DynaSurfGS/, code:
  https://github.com/Open3DVLab/DynaSurfGS

---

## SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with  Panoramic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-25 | Wenrui Li, Yapeng Mi, Fucheng Cai, Zhe Yang, Wangmeng Zuo, Xingtao Wang, Xiaopeng Fan | cs.CV | [PDF](http://arxiv.org/pdf/2408.13711v1){: .btn .btn-green } |

**Abstract**: Text-driven 3D scene generation has seen significant advancements recently.
However, most existing methods generate single-view images using generative
models and then stitch them together in 3D space. This independent generation
for each view often results in spatial inconsistency and implausibility in the
3D scenes. To address this challenge, we proposed a novel text-driven
3D-consistent scene generation model: SceneDreamer360. Our proposed method
leverages a text-driven panoramic image generation model as a prior for 3D
scene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency
across multi-view panoramic images. Specifically, SceneDreamer360 enhances the
fine-tuned Panfusion generator with a three-stage panoramic enhancement,
enabling the generation of high-resolution, detail-rich panoramic images.
During the 3D scene construction, a novel point cloud fusion initialization
method is used, producing higher quality and spatially consistent point clouds.
Our extensive experiments demonstrate that compared to other methods,
SceneDreamer360 with its panoramic image generation and 3DGS can produce higher
quality, spatially consistent, and visually appealing 3D scenes from any text
prompt. Our codes are available at
\url{https://github.com/liwrui/SceneDreamer360}.



---

## Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-25 | Brandon Smart, Chuanxia Zheng, Iro Laina, Victor Adrian Prisacariu | cs.CV | [PDF](http://arxiv.org/pdf/2408.13912v2){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Splatt3R, a pose-free, feed-forward method for
in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given
uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without
requiring any camera parameters or depth information. For generalizability, we
build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R,
by extending it to deal with both 3D structure and appearance. Specifically,
unlike the original MASt3R which reconstructs only 3D point clouds, we predict
the additional Gaussian attributes required to construct a Gaussian primitive
for each point. Hence, unlike other novel view synthesis methods, Splatt3R is
first trained by optimizing the 3D point cloud's geometry loss, and then a
novel view synthesis objective. By doing this, we avoid the local minima
present in training 3D Gaussian Splats from stereo views. We also propose a
novel loss masking strategy that we empirically find is critical for strong
performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++
dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild
images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and
the resultant splats can be rendered in real-time.

Comments:
- Our project page can be found at: https://splatt3r.active.vision/

---

## TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View  Images with Transformers

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-25 | Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.13770v1){: .btn .btn-green } |

**Abstract**: Compared with previous 3D reconstruction methods like Nerf, recent
Generalizable 3D Gaussian Splatting (G-3DGS) methods demonstrate impressive
efficiency even in the sparse-view setting. However, the promising
reconstruction performance of existing G-3DGS methods relies heavily on
accurate multi-view feature matching, which is quite challenging. Especially
for the scenes that have many non-overlapping areas between various views and
contain numerous similar regions, the matching performance of existing methods
is poor and the reconstruction precision is limited. To address this problem,
we develop a strategy that utilizes a predicted depth confidence map to guide
accurate local feature matching. In addition, we propose to utilize the
knowledge of existing monocular depth estimation models as prior to boost the
depth estimation precision in non-overlapping areas between views. Combining
the proposed strategies, we present a novel G-3DGS method named TranSplat,
which obtains the best performance on both the RealEstate10K and ACID
benchmarks while maintaining competitive speed and presenting strong
cross-dataset generalization ability. Our code, and demos will be available at:
https://xingyoujun.github.io/transplat.



---

## G3DST: Generalizing 3D Style Transfer with Neural Radiance Fields across  Scenes and Styles

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-24 | Adil Meric, Umut Kocasari, Matthias Nießner, Barbara Roessle | cs.CV | [PDF](http://arxiv.org/pdf/2408.13508v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have emerged as a powerful tool for creating
highly detailed and photorealistic scenes. Existing methods for NeRF-based 3D
style transfer need extensive per-scene optimization for single or multiple
styles, limiting the applicability and efficiency of 3D style transfer. In this
work, we overcome the limitations of existing methods by rendering stylized
novel views from a NeRF without the need for per-scene or per-style
optimization. To this end, we take advantage of a generalizable NeRF model to
facilitate style transfer in 3D, thereby enabling the use of a single learned
model across various scenes. By incorporating a hypernetwork into a
generalizable NeRF, our approach enables on-the-fly generation of stylized
novel views. Moreover, we introduce a novel flow-based multi-view consistency
loss to preserve consistency across multiple views. We evaluate our method
across various scenes and artistic styles and show its performance in
generating high-quality and multi-view consistent stylized images without the
need for a scene-specific implicit model. Our findings demonstrate that this
approach not only achieves a good visual quality comparable to that of
per-scene methods but also significantly enhances efficiency and applicability,
marking a notable advancement in the field of 3D style transfer.

Comments:
- GCPR 2024, Project page: https://mericadil.github.io/G3DST/

---

## S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D  Control Points

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-23 | Bing He, Yunuo Chen, Guo Lu, Li Song, Wenjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.13036v1){: .btn .btn-green } |

**Abstract**: Recently, the dynamic scene reconstruction using Gaussians has garnered
increased interest. Mainstream approaches typically employ a global deformation
field to warp a 3D scene in the canonical space. However, the inherently
low-frequency nature of implicit neural fields often leads to ineffective
representations of complex motions. Moreover, their structural rigidity can
hinder adaptation to scenes with varying resolutions and durations. To overcome
these challenges, we introduce a novel approach utilizing discrete 3D control
points. This method models local rays physically and establishes a
motion-decoupling coordinate system, which effectively merges traditional
graphics with learnable pipelines for a robust and efficient local
6-degrees-of-freedom (6-DoF) motion representation. Additionally, we have
developed a generalized framework that incorporates our control points with
Gaussians. Starting from an initial 3D reconstruction, our workflow decomposes
the streaming 4D real-world reconstruction into four independent submodules: 3D
segmentation, 3D control points generation, object-wise motion manipulation,
and residual compensation. Our experiments demonstrate that this method
outperforms existing state-of-the-art 4D Gaussian Splatting techniques on both
the Neu3DV and CMU-Panoptic datasets. Our approach also significantly
accelerates training, with the optimization of our 3D control points achievable
within just 2 seconds per frame on a single NVIDIA 4070 GPU.



---

## FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting  for Customizable Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-23 | Yunji Seo, Young Sun Choi, Hyun Seung Son, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2408.12894v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) achieves fast and high-quality renderings by
using numerous small Gaussians, which leads to significant memory consumption.
This reliance on a large number of Gaussians restricts the application of
3DGS-based models on low-cost devices due to memory limitations. However,
simply reducing the number of Gaussians to accommodate devices with less memory
capacity leads to inferior quality compared to the quality that can be achieved
on high-end hardware. To address this lack of scalability, we propose
integrating a Flexible Level of Detail (FLoD) to 3DGS, to allow a scene to be
rendered at varying levels of detail according to hardware capabilities. While
existing 3DGSs with LoD focus on detailed reconstruction, our method provides
reconstructions using a small number of Gaussians for reduced memory
requirements, and a larger number of Gaussians for greater detail. Experiments
demonstrate our various rendering options with tradeoffs between rendering
quality and memory usage, thereby allowing real-time rendering across different
memory constraints. Furthermore, we show that our method generalizes to
different 3DGS frameworks, indicating its potential for integration into future
state-of-the-art developments. Project page:
https://3dgs-flod.github.io/flod.github.io/

Comments:
- Project page: https://3dgs-flod.github.io/flod.github.io/

---

## SIn-NeRF2NeRF: Editing 3D Scenes with Instructions through Segmentation  and Inpainting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-23 | Jiseung Hong, Changmin Lee, Gyusang Yu | cs.CV | [PDF](http://arxiv.org/pdf/2408.13285v1){: .btn .btn-green } |

**Abstract**: TL;DR Perform 3D object editing selectively by disentangling it from the
background scene. Instruct-NeRF2NeRF (in2n) is a promising method that enables
editing of 3D scenes composed of Neural Radiance Field (NeRF) using text
prompts. However, it is challenging to perform geometrical modifications such
as shrinking, scaling, or moving on both the background and object
simultaneously. In this project, we enable geometrical changes of objects
within the 3D scene by selectively editing the object after separating it from
the scene. We perform object segmentation and background inpainting
respectively, and demonstrate various examples of freely resizing or moving
disentangled objects within the three-dimensional space.

Comments:
- Code is available at: https://github.com/KAISTChangmin/SIn-NeRF2NeRF

---

## BiGS: Bidirectional Gaussian Primitives for Relightable 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-23 | Zhenyuan Liu, Yu Guo, Xinyuan Li, Bernd Bickel, Ran Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.13370v1){: .btn .btn-green } |

**Abstract**: We present Bidirectional Gaussian Primitives, an image-based novel view
synthesis technique designed to represent and render 3D objects with surface
and volumetric materials under dynamic illumination. Our approach integrates
light intrinsic decomposition into the Gaussian splatting framework, enabling
real-time relighting of 3D objects. To unify surface and volumetric material
within a cohesive appearance model, we adopt a light- and view-dependent
scattering representation via bidirectional spherical harmonics. Our model does
not use a specific surface normal-related reflectance function, making it more
compatible with volumetric representations like Gaussian splatting, where the
normals are undefined. We demonstrate our method by reconstructing and
rendering objects with complex materials. Using One-Light-At-a-Time (OLAT) data
as input, we can reproduce photorealistic appearances under novel lighting
conditions in real time.



---

## Subsurface Scattering for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-22 | Jan-Niklas Dihlmann, Arjun Majumdar, Andreas Engelhardt, Raphael Braun, Hendrik P. A. Lensch | cs.CV | [PDF](http://arxiv.org/pdf/2408.12282v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction and relighting of objects made from scattering materials
present a significant challenge due to the complex light transport beneath the
surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at
real-time speeds. While 3D Gaussians efficiently approximate an object's
surface, they fail to capture the volumetric properties of subsurface
scattering. We propose a framework for optimizing an object's shape together
with the radiance transfer field given multi-view OLAT (one light at a time)
data. Our method decomposes the scene into an explicit surface represented as
3D Gaussians, with a spatially varying BRDF, and an implicit volumetric
representation of the scattering component. A learned incident light field
accounts for shadowing. We optimize all parameters jointly via ray-traced
differentiable rendering. Our approach enables material editing, relighting and
novel view synthesis at interactive rates. We show successful application on
synthetic data and introduce a newly acquired multi-view multi-light dataset of
objects in a light-stage setup. Compared to previous work we achieve comparable
or better results at a fraction of optimization and rendering time while
enabling detailed control over material attributes. Project page
https://sss.jdihlmann.com/

Comments:
- Project page: https://sss.jdihlmann.com/

---

## GSFusion: Online RGB-D Mapping Where Gaussian Splatting Meets TSDF  Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-22 | Jiaxin Wei, Stefan Leutenegger | cs.CV | [PDF](http://arxiv.org/pdf/2408.12677v2){: .btn .btn-green } |

**Abstract**: Traditional volumetric fusion algorithms preserve the spatial structure of 3D
scenes, which is beneficial for many tasks in computer vision and robotics.
However, they often lack realism in terms of visualization. Emerging 3D
Gaussian splatting bridges this gap, but existing Gaussian-based reconstruction
methods often suffer from artifacts and inconsistencies with the underlying 3D
structure, and struggle with real-time optimization, unable to provide users
with immediate feedback in high quality. One of the bottlenecks arises from the
massive amount of Gaussian parameters that need to be updated during
optimization. Instead of using 3D Gaussian as a standalone map representation,
we incorporate it into a volumetric mapping system to take advantage of
geometric information and propose to use a quadtree data structure on images to
drastically reduce the number of splats initialized. In this way, we
simultaneously generate a compact 3D Gaussian map with fewer artifacts and a
volumetric map on the fly. Our method, GSFusion, significantly enhances
computational efficiency without sacrificing rendering quality, as demonstrated
on both synthetic and real datasets. Code will be available at
https://github.com/goldoak/GSFusion.



---

## DeRainGS: Gaussian Splatting for Enhanced Scene Reconstruction in Rainy  Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Shuhong Liu, Xiang Chen, Hongming Chen, Quanfeng Xu, Mingrui Li | cs.CV | [PDF](http://arxiv.org/pdf/2408.11540v2){: .btn .btn-green } |

**Abstract**: Reconstruction under adverse rainy conditions poses significant challenges
due to reduced visibility and the distortion of visual perception. These
conditions can severely impair the quality of geometric maps, which is
essential for applications ranging from autonomous planning to environmental
monitoring. In response to these challenges, this study introduces the novel
task of 3D Reconstruction in Rainy Environments (3DRRE), specifically designed
to address the complexities of reconstructing 3D scenes under rainy conditions.
To benchmark this task, we construct the HydroViews dataset that comprises a
diverse collection of both synthesized and real-world scene images
characterized by various intensities of rain streaks and raindrops.
Furthermore, we propose DeRainGS, the first 3DGS method tailored for
reconstruction in adverse rainy environments. Extensive experiments across a
wide range of rain scenarios demonstrate that our method delivers
state-of-the-art performance, remarkably outperforming existing occlusion-free
methods.



---

## Visual Localization in 3D Maps: Comparing Point Cloud, Mesh, and NeRF  Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Lintong Zhang, Yifu Tao, Jiarong Lin, Fu Zhang, Maurice Fallon | cs.CV | [PDF](http://arxiv.org/pdf/2408.11966v1){: .btn .btn-green } |

**Abstract**: This paper introduces and assesses a cross-modal global visual localization
system that can localize camera images within a color 3D map representation
built using both visual and lidar sensing. We present three different
state-of-the-art methods for creating the color 3D maps: point clouds, meshes,
and neural radiance fields (NeRF). Our system constructs a database of
synthetic RGB and depth image pairs from these representations. This database
serves as the basis for global localization. We present an automatic approach
that builds this database by synthesizing novel images of the scene and
exploiting the 3D structure encoded in the different representations. Next, we
present a global localization system that relies on the synthetic image
database to accurately estimate the 6 DoF camera poses of monocular query
images. Our localization approach relies on different learning-based global
descriptors and feature detectors which enable robust image retrieval and
matching despite the domain gap between (real) query camera images and the
synthetic database images. We assess the system's performance through extensive
real-world experiments in both indoor and outdoor settings, in order to
evaluate the effectiveness of each map representation and the benefits against
traditional structure-from-motion localization approaches. Our results show
that all three map representations can achieve consistent localization success
rates of 55% and higher across various environments. NeRF synthesized images
show superior performance, localizing query images at an average success rate
of 72%. Furthermore, we demonstrate that our synthesized database enables
global localization even when the map creation data and the localization
sequence are captured when travelling in opposite directions. Our system,
operating in real-time on a mobile laptop equipped with a GPU, achieves a
processing rate of 1Hz.



---

## Pano2Room: Novel View Synthesis from a Single Indoor Panorama

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Guo Pu, Yiming Zhao, Zhouhui Lian | cs.CV | [PDF](http://arxiv.org/pdf/2408.11413v2){: .btn .btn-green } |

**Abstract**: Recent single-view 3D generative methods have made significant advancements
by leveraging knowledge distilled from extensive 3D object datasets. However,
challenges persist in the synthesis of 3D scenes from a single view, primarily
due to the complexity of real-world environments and the limited availability
of high-quality prior resources. In this paper, we introduce a novel approach
called Pano2Room, designed to automatically reconstruct high-quality 3D indoor
scenes from a single panoramic image. These panoramic images can be easily
generated using a panoramic RGBD inpainter from captures at a single location
with any camera. The key idea is to initially construct a preliminary mesh from
the input panorama, and iteratively refine this mesh using a panoramic RGBD
inpainter while collecting photo-realistic 3D-consistent pseudo novel views.
Finally, the refined mesh is converted into a 3D Gaussian Splatting field and
trained with the collected pseudo novel views. This pipeline enables the
reconstruction of real-world 3D scenes, even in the presence of large
occlusions, and facilitates the synthesis of photo-realistic novel views with
detailed geometry. Extensive qualitative and quantitative experiments have been
conducted to validate the superiority of our method in single-panorama indoor
novel synthesis compared to the state-of-the-art. Our code and data are
available at \url{https://github.com/TrickyGo/Pano2Room}.

Comments:
- SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers '24),
  December 3--6, 2024, Tokyo, Japan

---

## Irregularity Inspection using Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Tianqi Ding, Dawei Xiang | cs.CV | [PDF](http://arxiv.org/pdf/2408.11251v1){: .btn .btn-green } |

**Abstract**: With the increasing growth of industrialization, more and more industries are
relying on machine automation for production. However, defect detection in
large-scale production machinery is becoming increasingly important. Due to
their large size and height, it is often challenging for professionals to
conduct defect inspections on such large machinery. For example, the inspection
of aging and misalignment of components on tall machinery like towers requires
companies to assign dedicated personnel. Employees need to climb the towers and
either visually inspect or take photos to detect safety hazards in these large
machines. Direct visual inspection is limited by its low level of automation,
lack of precision, and safety concerns associated with personnel climbing the
towers. Therefore, in this paper, we propose a system based on neural network
modeling (NeRF) of 3D twin models. By comparing two digital models, this system
enables defect detection at the 3D interface of an object.



---

## GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation  with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Wanshui Gan, Fang Liu, Hongbin Xu, Ningkai Mo, Naoto Yokoya | cs.CV | [PDF](http://arxiv.org/pdf/2408.11447v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianOcc, a systematic method that investigates the two
usages of Gaussian splatting for fully self-supervised and efficient 3D
occupancy estimation in surround views. First, traditional methods for
self-supervised 3D occupancy estimation still require ground truth 6D poses
from sensors during training. To address this limitation, we propose Gaussian
Splatting for Projection (GSP) module to provide accurate scale information for
fully self-supervised training from adjacent view projection. Additionally,
existing methods rely on volume rendering for final 3D voxel representation
learning using 2D signals (depth maps, semantic maps), which is both
time-consuming and less effective. We propose Gaussian Splatting from Voxel
space (GSV) to leverage the fast rendering properties of Gaussian splatting. As
a result, the proposed GaussianOcc method enables fully self-supervised (no
ground truth pose) 3D occupancy estimation in competitive performance with low
computational cost (2.7 times faster in training and 5 times faster in
rendering).

Comments:
- Project page: https://ganwanshui.github.io/GaussianOcc/

---

## Robust 3D Gaussian Splatting for Novel View Synthesis in Presence of  Distractors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-21 | Paul Ungermann, Armin Ettenhofer, Matthias Nießner, Barbara Roessle | cs.CV | [PDF](http://arxiv.org/pdf/2408.11697v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has shown impressive novel view synthesis results;
nonetheless, it is vulnerable to dynamic objects polluting the input data of an
otherwise static scene, so called distractors. Distractors have severe impact
on the rendering quality as they get represented as view-dependent effects or
result in floating artifacts. Our goal is to identify and ignore such
distractors during the 3D Gaussian optimization to obtain a clean
reconstruction. To this end, we take a self-supervised approach that looks at
the image residuals during the optimization to determine areas that have likely
been falsified by a distractor. In addition, we leverage a pretrained
segmentation network to provide object awareness, enabling more accurate
exclusion of distractors. This way, we obtain segmentation masks of distractors
to effectively ignore them in the loss formulation. We demonstrate that our
approach is robust to various distractors and strongly improves rendering
quality on distractor-polluted scenes, improving PSNR by 1.86dB compared to 3D
Gaussian Splatting.

Comments:
- GCPR 2024, Project Page:
  https://paulungermann.github.io/Robust3DGaussians , Video:
  https://www.youtube.com/watch?v=P9unyR7yK3E

---

## Learning Part-aware 3D Representations by Fusing 2D Gaussians and  Superquadrics

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2408.10789v1){: .btn .btn-green } |

**Abstract**: Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D
Gaussians, are commonly used to represent 3D objects or scenes. However, humans
usually perceive 3D objects or scenes at a higher level as a composition of
parts or structures rather than points or voxels. Representing 3D as semantic
parts can benefit further understanding and applications. We aim to solve
part-aware 3D reconstruction, which parses objects or scenes into semantic
parts. In this paper, we introduce a hybrid representation of superquadrics and
2D Gaussians, trying to dig 3D structural clues from multi-view image inputs.
Accurate structured geometry reconstruction and high-quality rendering are
achieved at the same time. We incorporate parametric superquadrics in mesh
forms into 2D Gaussians by attaching Gaussian centers to faces in meshes.
During the training, superquadrics parameters are iteratively optimized, and
Gaussians are deformed accordingly, resulting in an efficient hybrid
representation. On the one hand, this hybrid representation inherits the
advantage of superquadrics to represent different shape primitives, supporting
flexible part decomposition of scenes. On the other hand, 2D Gaussians are
incorporated to model the complex texture and geometry details, ensuring
high-quality rendering and geometry reconstruction. The reconstruction is fully
unsupervised. We conduct extensive experiments on data from DTU and ShapeNet
datasets, in which the method decomposes scenes into reasonable parts,
outperforming existing state-of-the-art approaches.



---

## DEGAS: Detailed Expressions on Full-Body Gaussian Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Zhijing Shao, Duotun Wang, Qing-Yao Tian, Yao-Dong Yang, Hengyu Meng, Zeyu Cai, Bo Dong, Yu Zhang, Kang Zhang, Zeyu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.10588v1){: .btn .btn-green } |

**Abstract**: Although neural rendering has made significant advancements in creating
lifelike, animatable full-body and head avatars, incorporating detailed
expressions into full-body avatars remains largely unexplored. We present
DEGAS, the first 3D Gaussian Splatting (3DGS)-based modeling method for
full-body avatars with rich facial expressions. Trained on multiview videos of
a given subject, our method learns a conditional variational autoencoder that
takes both the body motion and facial expression as driving signals to generate
Gaussian maps in the UV layout. To drive the facial expressions, instead of the
commonly used 3D Morphable Models (3DMMs) in 3D head avatars, we propose to
adopt the expression latent space trained solely on 2D portrait images,
bridging the gap between 2D talking faces and 3D avatars. Leveraging the
rendering capability of 3DGS and the rich expressiveness of the expression
latent space, the learned avatars can be reenacted to reproduce photorealistic
rendering images with subtle and accurate facial expressions. Experiments on an
existing dataset and our newly proposed dataset of full-body talking avatars
demonstrate the efficacy of our method. We also propose an audio-driven
extension of our method with the help of 2D talking faces, opening new
possibilities to interactive AI agents.



---

## ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their  Self-Supervised Pretraining

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Qi Ma, Yue Li, Bin Ren, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2408.10906v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become the de facto method of 3D
representation in many vision tasks. This calls for the 3D understanding
directly in this representation space. To facilitate the research in this
direction, we first build a large-scale dataset of 3DGS using the commonly used
ShapeNet and ModelNet datasets. Our dataset ShapeSplat consists of 65K objects
from 87 unique categories, whose labels are in accordance with the respective
datasets. The creation of this dataset utilized the compute equivalent of 2 GPU
years on a TITAN XP GPU.
  We utilize our dataset for unsupervised pretraining and supervised finetuning
for classification and segmentation tasks. To this end, we introduce
\textbf{\textit{Gaussian-MAE}}, which highlights the unique benefits of
representation learning from Gaussian parameters. Through exhaustive
experiments, we provide several valuable insights. In particular, we show that
(1) the distribution of the optimized GS centroids significantly differs from
the uniformly sampled point cloud (used for initialization) counterpart; (2)
this change in distribution results in degradation in classification but
improvement in segmentation tasks when using only the centroids; (3) to
leverage additional Gaussian parameters, we propose Gaussian feature grouping
in a normalized feature space, along with splats pooling layer, offering a
tailored solution to effectively group and embed similar Gaussians, which leads
to notable improvement in finetuning tasks.



---

## GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Zirui Wang, Ming Cheng, Victor Adrian Prisacariu, Tristan Braud | cs.CV | [PDF](http://arxiv.org/pdf/2408.11085v1){: .btn .btn-green } |

**Abstract**: We leverage 3D Gaussian Splatting (3DGS) as a scene representation and
propose a novel test-time camera pose refinement framework, GSLoc. This
framework enhances the localization accuracy of state-of-the-art absolute pose
regression and scene coordinate regression methods. The 3DGS model renders
high-quality synthetic images and depth maps to facilitate the establishment of
2D-3D correspondences. GSLoc obviates the need for training feature extractors
or descriptors by operating directly on RGB images, utilizing the 3D vision
foundation model, MASt3R, for precise 2D matching. To improve the robustness of
our model in challenging outdoor environments, we incorporate an
exposure-adaptive module within the 3DGS framework. Consequently, GSLoc enables
efficient pose refinement given a single RGB query and a coarse initial pose
estimation. Our proposed approach surpasses leading NeRF-based optimization
methods in both accuracy and runtime across indoor and outdoor visual
localization benchmarks, achieving state-of-the-art accuracy on two indoor
datasets.

Comments:
- The project page is available at https://gsloc.active.vision

---

## TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature  Tracks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-20 | Jinjie Mai, Wenxuan Zhu, Sara Rojas, Jesus Zarzar, Abdullah Hamdi, Guocheng Qian, Bing Li, Silvio Giancola, Bernard Ghanem | cs.CV | [PDF](http://arxiv.org/pdf/2408.10739v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) generally require many images with accurate
poses for accurate novel view synthesis, which does not reflect realistic
setups where views can be sparse and poses can be noisy. Previous solutions for
learning NeRFs with sparse views and noisy poses only consider local geometry
consistency with pairs of views. Closely following \textit{bundle adjustment}
in Structure-from-Motion (SfM), we introduce TrackNeRF for more globally
consistent geometry reconstruction and more accurate pose optimization.
TrackNeRF introduces \textit{feature tracks}, \ie connected pixel trajectories
across \textit{all} visible views that correspond to the \textit{same} 3D
points. By enforcing reprojection consistency among feature tracks, TrackNeRF
encourages holistic 3D consistency explicitly. Through extensive experiments,
TrackNeRF sets a new benchmark in noisy and sparse view reconstruction. In
particular, TrackNeRF shows significant improvements over the state-of-the-art
BARF and SPARF by $\sim8$ and $\sim1$ in terms of PSNR on DTU under various
sparse and noisy view setups. The code is available at
\href{https://tracknerf.github.io/}.

Comments:
- ECCV 2024 (supplemental pages included)

---

## LoopSplat: Loop Closure by Registering 3D Gaussian Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Liyuan Zhu, Yue Li, Erik Sandström, Shengyu Huang, Konrad Schindler, Iro Armeni | cs.CV | [PDF](http://arxiv.org/pdf/2408.10154v2){: .btn .btn-green } |

**Abstract**: Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats
(3DGS) has recently shown promise towards more accurate, dense 3D scene maps.
However, existing 3DGS-based methods fail to address the global consistency of
the scene via loop closure and/or global bundle adjustment. To this end, we
propose LoopSplat, which takes RGB-D images as input and performs dense mapping
with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure
online and computes relative loop edge constraints between submaps directly via
3DGS registration, leading to improvements in efficiency and accuracy over
traditional global-to-local point cloud registration. It uses a robust pose
graph optimization formulation and rigidly aligns the submaps to achieve global
consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD,
ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking,
mapping, and rendering compared to existing methods for dense RGB-D SLAM. Code
is available at loopsplat.github.io.

Comments:
- Project page: https://loopsplat.github.io/

---

## Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane  Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Minye Wu, Tinne Tuytelaars | cs.CV | [PDF](http://arxiv.org/pdf/2408.10041v1){: .btn .btn-green } |

**Abstract**: Recent advancements in photo-realistic novel view synthesis have been
significantly driven by Gaussian Splatting (3DGS). Nevertheless, the explicit
nature of 3DGS data entails considerable storage requirements, highlighting a
pressing need for more efficient data representations. To address this, we
present Implicit Gaussian Splatting (IGS), an innovative hybrid model that
integrates explicit point clouds with implicit feature embeddings through a
multi-level tri-plane architecture. This architecture features 2D feature grids
at various resolutions across different levels, facilitating continuous spatial
domain representation and enhancing spatial correlations among Gaussian
primitives. Building upon this foundation, we introduce a level-based
progressive training scheme, which incorporates explicit spatial
regularization. This method capitalizes on spatial correlations to enhance both
the rendering quality and the compactness of the IGS representation.
Furthermore, we propose a novel compression pipeline tailored for both point
clouds and 2D feature grids, considering the entropy variations across
different levels. Extensive experimental evaluations demonstrate that our
algorithm can deliver high-quality rendering using only a few MBs, effectively
balancing storage efficiency and rendering fidelity, and yielding results that
are competitive with the state-of-the-art.



---

## CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian  Splatting and Contrastive Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen | cs.CV | [PDF](http://arxiv.org/pdf/2408.09663v2){: .btn .btn-green } |

**Abstract**: Recent advancements in human avatar synthesis have utilized radiance fields
to reconstruct photo-realistic animatable human avatars. However, both
NeRFs-based and 3DGS-based methods struggle with maintaining 3D consistency and
exhibit suboptimal detail reconstruction, especially with sparse inputs. To
address this challenge, we propose CHASE, which introduces supervision from
intrinsic 3D consistency across poses and 3D geometry contrastive learning,
achieving performance comparable with sparse inputs to that with full inputs.
Following previous work, we first integrate a skeleton-driven rigid deformation
and a non-rigid cloth dynamics deformation to coordinate the movements of
individual Gaussians during animation, reconstructing basic avatar with coarse
3D consistency. To improve 3D consistency under sparse inputs, we design
Dynamic Avatar Adjustment(DAA) to adjust deformed Gaussians based on a selected
similar pose/image from the dataset. Minimizing the difference between the
image rendered by adjusted Gaussians and the image with the similar pose serves
as an additional form of supervision for avatar. Furthermore, we propose a 3D
geometry contrastive learning strategy to maintain the 3D global consistency of
generated avatars. Though CHASE is designed for sparse inputs, it surprisingly
outperforms current SOTA methods \textbf{in both full and sparse settings} on
the ZJU-MoCap and H36M datasets, demonstrating that our CHASE successfully
maintains avatar's 3D consistency, hence improving rendering quality.

Comments:
- 13 pages, 6 figures

---

## DiscoNeRF: Class-Agnostic Object Field for 3D Object Discovery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Corentin Dumery, Aoxiang Fan, Ren Li, Nicolas Talabot, Pascal Fua | cs.CV | [PDF](http://arxiv.org/pdf/2408.09928v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a powerful tool for modeling 3D
scenes from multiple images. However, NeRFs remain difficult to segment into
semantically meaningful regions. Previous approaches to 3D segmentation of
NeRFs either require user interaction to isolate a single object, or they rely
on 2D semantic masks with a limited number of classes for supervision. As a
consequence, they generalize poorly to class-agnostic masks automatically
generated in real scenes. This is attributable to the ambiguity arising from
zero-shot segmentation, yielding inconsistent masks across views. In contrast,
we propose a method that is robust to inconsistent segmentations and
successfully decomposes the scene into a set of objects of any class. By
introducing a limited number of competing object slots against which masks are
matched, a meaningful object representation emerges that best explains the 2D
supervision and minimizes an additional regularization term. Our experiments
demonstrate the ability of our method to generate 3D panoptic segmentations on
complex scenes, and extract high-quality 3D assets from NeRFs that can then be
used in virtual 3D environments.



---

## $R^2$-Mesh: Reinforcement Learning Powered Mesh Reconstruction via  Geometry and Appearance Refinement

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Haoyang Wang, Liming Liu, Quanlu Jia, Jiangkai Wu, Haodan Zhang, Peiheng Wang, Xinggong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.10135v1){: .btn .btn-green } |

**Abstract**: Mesh reconstruction based on Neural Radiance Fields (NeRF) is popular in a
variety of applications such as computer graphics, virtual reality, and medical
imaging due to its efficiency in handling complex geometric structures and
facilitating real-time rendering. However, existing works often fail to capture
fine geometric details accurately and struggle with optimizing rendering
quality. To address these challenges, we propose a novel algorithm that
progressively generates and optimizes meshes from multi-view images. Our
approach initiates with the training of a NeRF model to establish an initial
Signed Distance Field (SDF) and a view-dependent appearance field.
Subsequently, we iteratively refine the SDF through a differentiable mesh
extraction method, continuously updating both the vertex positions and their
connectivity based on the loss from mesh differentiable rasterization, while
also optimizing the appearance representation. To further leverage
high-fidelity and detail-rich representations from NeRF, we propose an
online-learning strategy based on Upper Confidence Bound (UCB) to enhance
viewpoints by adaptively incorporating images rendered by the initial NeRF
model into the training dataset. Through extensive experiments, we demonstrate
that our method delivers highly competitive and robust performance in both mesh
rendering quality and geometric quality.



---

## SG-GS: Photo-realistic Animatable Human Avatars with Semantically-Guided  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-19 | Haoyu Zhao, Chen Yang, Hao Wang, Xingyue Zhao, Wei Shen | cs.CV | [PDF](http://arxiv.org/pdf/2408.09665v1){: .btn .btn-green } |

**Abstract**: Reconstructing photo-realistic animatable human avatars from monocular videos
remains challenging in computer vision and graphics. Recently, methods using 3D
Gaussians to represent the human body have emerged, offering faster
optimization and real-time rendering. However, due to ignoring the crucial role
of human body semantic information which represents the intrinsic structure and
connections within the human body, they fail to achieve fine-detail
reconstruction of dynamic human avatars. To address this issue, we propose
SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven rigid
deformation, and non-rigid cloth dynamics deformation to create photo-realistic
animatable human avatars from monocular videos. We then design a Semantic
Human-Body Annotator (SHA) which utilizes SMPL's semantic prior for efficient
body part semantic labeling. The generated labels are used to guide the
optimization of Gaussian semantic attributes. To address the limited receptive
field of point-level MLPs for local features, we also propose a 3D network that
integrates geometric and semantic associations for human avatar deformation. We
further implement three key strategies to enhance the semantic accuracy of 3D
Gaussians and rendering quality: semantic projection with 2D regularization,
semantic-guided density regularization and semantic-aware regularization with
neighborhood consistency. Extensive experiments demonstrate that SG-GS achieves
state-of-the-art geometry and appearance reconstruction performance.

Comments:
- 12 pages, 5 figures

---

## S^3D-NeRF: Single-Shot Speech-Driven Neural Radiance Field for High  Fidelity Talking Head Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-18 | Dongze Li, Kang Zhao, Wei Wang, Yifeng Ma, Bo Peng, Yingya Zhang, Jing Dong | cs.CV | [PDF](http://arxiv.org/pdf/2408.09347v1){: .btn .btn-green } |

**Abstract**: Talking head synthesis is a practical technique with wide applications.
Current Neural Radiance Field (NeRF) based approaches have shown their
superiority on driving one-shot talking heads with videos or signals regressed
from audio. However, most of them failed to take the audio as driven
information directly, unable to enjoy the flexibility and availability of
speech. Since mapping audio signals to face deformation is non-trivial, we
design a Single-Shot Speech-Driven Neural Radiance Field (S^3D-NeRF) method in
this paper to tackle the following three difficulties: learning a
representative appearance feature for each identity, modeling motion of
different face regions with audio, and keeping the temporal consistency of the
lip area. To this end, we introduce a Hierarchical Facial Appearance Encoder to
learn multi-scale representations for catching the appearance of different
speakers, and elaborate a Cross-modal Facial Deformation Field to perform
speech animation according to the relationship between the audio signal and
different face regions. Moreover, to enhance the temporal consistency of the
important lip area, we introduce a lip-sync discriminator to penalize the
out-of-sync audio-visual sequences. Extensive experiments have shown that our
S^3D-NeRF surpasses previous arts on both video fidelity and audio-lip
synchronization.

Comments:
- ECCV 2024

---

## HybridOcc: NeRF Enhanced Transformer-based Multi-Camera 3D Occupancy  Prediction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-17 | Xiao Zhao, Bo Chen, Mingyang Sun, Dingkang Yang, Youxing Wang, Xukun Zhang, Mingcheng Li, Dongliang Kou, Xiaoyi Wei, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.09104v1){: .btn .btn-green } |

**Abstract**: Vision-based 3D semantic scene completion (SSC) describes autonomous driving
scenes through 3D volume representations. However, the occlusion of invisible
voxels by scene surfaces poses challenges to current SSC methods in
hallucinating refined 3D geometry. This paper proposes HybridOcc, a hybrid 3D
volume query proposal method generated by Transformer framework and NeRF
representation and refined in a coarse-to-fine SSC prediction framework.
HybridOcc aggregates contextual features through the Transformer paradigm based
on hybrid query proposals while combining it with NeRF representation to obtain
depth supervision. The Transformer branch contains multiple scales and uses
spatial cross-attention for 2D to 3D transformation. The newly designed NeRF
branch implicitly infers scene occupancy through volume rendering, including
visible and invisible voxels, and explicitly captures scene depth rather than
generating RGB color. Furthermore, we present an innovative occupancy-aware ray
sampling method to orient the SSC task instead of focusing on the scene
surface, further improving the overall performance. Extensive experiments on
nuScenes and SemanticKITTI datasets demonstrate the effectiveness of our
HybridOcc on the SSC task.

Comments:
- Accepted to IEEE RAL

---

## SSNeRF: Sparse View Semi-supervised Neural Radiance Fields with  Augmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-17 | Xiao Cao, Beibei Lin, Bo Wang, Zhiyong Huang, Robby T. Tan | cs.CV | [PDF](http://arxiv.org/pdf/2408.09144v1){: .btn .btn-green } |

**Abstract**: Sparse view NeRF is challenging because limited input images lead to an under
constrained optimization problem for volume rendering. Existing methods address
this issue by relying on supplementary information, such as depth maps.
However, generating this supplementary information accurately remains
problematic and often leads to NeRF producing images with undesired artifacts.
To address these artifacts and enhance robustness, we propose SSNeRF, a sparse
view semi supervised NeRF method based on a teacher student framework. Our key
idea is to challenge the NeRF module with progressively severe sparse view
degradation while providing high confidence pseudo labels. This approach helps
the NeRF model become aware of noise and incomplete information associated with
sparse views, thus improving its robustness. The novelty of SSNeRF lies in its
sparse view specific augmentations and semi supervised learning mechanism. In
this approach, the teacher NeRF generates novel views along with confidence
scores, while the student NeRF, perturbed by the augmented input, learns from
the high confidence pseudo labels. Our sparse view degradation augmentation
progressively injects noise into volume rendering weights, perturbs feature
maps in vulnerable layers, and simulates sparse view blurriness. These
augmentation strategies force the student NeRF to recognize degradation and
produce clearer rendered views. By transferring the student's parameters to the
teacher, the teacher gains increased robustness in subsequent training
iterations. Extensive experiments demonstrate the effectiveness of our SSNeRF
in generating novel views with less sparse view degradation. We will release
code upon acceptance.



---

## Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark  Images Using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-17 | Sheng Ye, Zhen-Hui Dong, Yubin Hu, Yu-Hui Wen, Yong-Jin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2408.09130v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently emerged as a powerful representation that
can synthesize remarkable novel views using consistent multi-view images as
input. However, we notice that images captured in dark environments where the
scenes are not fully illuminated can exhibit considerable brightness variations
and multi-view inconsistency, which poses great challenges to 3D Gaussian
Splatting and severely degrades its performance. To tackle this problem, we
propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera
imaging, we represent a consistent radiance field of the physical world using a
set of anisotropic 3D Gaussians, and design a camera response module to
compensate for multi-view inconsistencies. We also introduce a step-based
gradient scaling strategy to constrain Gaussians near the camera, which turn
out to be floaters, from splitting and cloning. Experiments on our proposed
benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings
without ghosting and floater artifacts and significantly outperforms existing
methods. Furthermore, we can also synthesize light-up images by controlling
exposure levels that clearly show details in shadow areas.

Comments:
- accepted by PG 2024

---

## VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-16 | Albert Gassol Puigjaner, Edoardo Mello Rella, Erik Sandström, Ajad Chhatkuli, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2408.08766v1){: .btn .btn-green } |

**Abstract**: Implicit surfaces via neural radiance fields (NeRF) have shown surprising
accuracy in surface reconstruction. Despite their success in reconstructing
richly textured surfaces, existing methods struggle with planar regions with
weak textures, which account for the majority of indoor scenes. In this paper,
we address indoor dense surface reconstruction by revisiting key aspects of
NeRF in order to use the recently proposed Vector Field (VF) as the implicit
representation. VF is defined by the unit vector directed to the nearest
surface point. It therefore flips direction at the surface and equals to the
explicit surface normals. Except for this flip, VF remains constant along
planar surfaces and provides a strong inductive bias in representing planar
surfaces. Concretely, we develop a novel density-VF relationship and a training
scheme that allows us to learn VF via volume rendering By doing this, VF-NeRF
can model large planar surfaces and sharp corners accurately. We show that,
when depth cues are available, our method further improves and achieves
state-of-the-art results in reconstructing indoor scenes and rendering novel
views. We extensively evaluate VF-NeRF on indoor datasets and run ablations of
its components.

Comments:
- 15 pages

---

## GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion  Prior and Parametric Light Source Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-16 | Kang Du, Zhihao Liang, Zeyu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.08524v1){: .btn .btn-green } |

**Abstract**: We present GS-ID, a novel framework for illumination decomposition on
Gaussian Splatting, achieving photorealistic novel view synthesis and intuitive
light editing. Illumination decomposition is an ill-posed problem facing three
main challenges: 1) priors for geometry and material are often lacking; 2)
complex illumination conditions involve multiple unknown light sources; and 3)
calculating surface shading with numerous light sources is computationally
expensive. To address these challenges, we first introduce intrinsic diffusion
priors to estimate the attributes for physically based rendering. Then we
divide the illumination into environmental and direct components for joint
optimization. Last, we employ deferred rendering to reduce the computational
load. Our framework uses a learnable environment map and Spherical Gaussians
(SGs) to represent light sources parametrically, therefore enabling
controllable and photorealistic relighting on Gaussian Splatting. Extensive
experiments and applications demonstrate that GS-ID produces state-of-the-art
illumination decomposition results while achieving better geometry
reconstruction and rendering performance.

Comments:
- 15 pages, 13 figures

---

## Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-16 | Wei Sun, Xiaosong Zhang, Fang Wan, Yanzhao Zhou, Yuan Li, Qixiang Ye, Jianbin Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2408.08723v1){: .btn .btn-green } |

**Abstract**: Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processed
camera poses--referred to as SfM-free methods--is crucial for promoting rapid
response capabilities and enhancing robustness against variable operating
conditions. Recent SfM-free methods have integrated pose optimization,
designing end-to-end frameworks for joint camera pose estimation and NVS.
However, most existing works rely on per-pixel image loss functions, such as L2
loss. In SfM-free methods, inaccurate initial poses lead to misalignment issue,
which, under the constraints of per-pixel image loss functions, results in
excessive gradients, causing unstable optimization and poor convergence for
NVS. In this study, we propose a correspondence-guided SfM-free 3D Gaussian
splatting for NVS. We use correspondences between the target and the rendered
result to achieve better pixel alignment, facilitating the optimization of
relative poses between frames. We then apply the learned poses to optimize the
entire scene. Each 2D screen-space pixel is associated with its corresponding
3D Gaussians through approximated surface rendering to facilitate gradient back
propagation. Experimental results underline the superior performance and time
efficiency of the proposed approach compared to the state-of-the-art baselines.

Comments:
- arXiv admin note: text overlap with arXiv:2312.07504 by other authors

---

## WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-15 | Huapeng Li, Wenxuan Song, Tianao Xu, Alexandre Elsig, Jonas Kulhanek | cs.CV | [PDF](http://arxiv.org/pdf/2408.08206v1){: .btn .btn-green } |

**Abstract**: The underwater 3D scene reconstruction is a challenging, yet interesting
problem with applications ranging from naval robots to VR experiences. The
problem was successfully tackled by fully volumetric NeRF-based methods which
can model both the geometry and the medium (water). Unfortunately, these
methods are slow to train and do not offer real-time rendering. More recently,
3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs.
However, because it is an explicit method that renders only the geometry, it
cannot render the medium and is therefore unsuited for underwater
reconstruction. Therefore, we propose a novel approach that fuses volumetric
rendering with 3DGS to handle underwater data effectively. Our method employs
3DGS for explicit geometry representation and a separate volumetric field
(queried once per pixel) for capturing the scattering medium. This dual
representation further allows the restoration of the scenes by removing the
scattering medium. Our method outperforms state-of-the-art NeRF-based methods
in rendering quality on the underwater SeaThru-NeRF dataset. Furthermore, it
does so while offering real-time rendering performance, addressing the
efficiency limitations of existing methods. Web:
https://water-splatting.github.io

Comments:
- Web: https://water-splatting.github.io

---

## FlashGS: Efficient 3D Gaussian Splatting for Large-scale and  High-resolution Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-15 | Guofeng Feng, Siyan Chen, Rong Fu, Zimu Liao, Yi Wang, Tao Liu, Zhilin Pei, Hengjie Li, Xingcheng Zhang, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2408.07967v2){: .btn .btn-green } |

**Abstract**: This work introduces FlashGS, an open-source CUDA Python library, designed to
facilitate the efficient differentiable rasterization of 3D Gaussian Splatting
through algorithmic and kernel-level optimizations. FlashGS is developed based
on the observations from a comprehensive analysis of the rendering process to
enhance computational efficiency and bring the technique to wide adoption. The
paper includes a suite of optimization strategies, encompassing redundancy
elimination, efficient pipelining, refined control and scheduling mechanisms,
and memory access optimizations, all of which are meticulously integrated to
amplify the performance of the rasterization process. An extensive evaluation
of FlashGS' performance has been conducted across a diverse spectrum of
synthetic and real-world large-scale scenes, encompassing a variety of image
resolutions. The empirical findings demonstrate that FlashGS consistently
achieves an average 4x acceleration over mobile consumer GPUs, coupled with
reduced memory consumption. These results underscore the superior performance
and resource optimization capabilities of FlashGS, positioning it as a
formidable tool in the domain of 3D rendering.



---

## Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Hyunjee Lee, Youngsik Yun, Jeongmin Bae, Seoha Kim, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2408.07416v2){: .btn .btn-green } |

**Abstract**: Understanding the 3D semantics of a scene is a fundamental problem for
various scenarios such as embodied agents. While NeRFs and 3DGS excel at
novel-view synthesis, previous methods for understanding their semantics have
been limited to incomplete 3D understanding: their segmentation results are 2D
masks and their supervision is anchored at 2D pixels. This paper revisits the
problem set to pursue a better 3D understanding of a scene modeled by NeRFs and
3DGS as follows. 1) We directly supervise the 3D points to train the language
embedding field. It achieves state-of-the-art accuracy without relying on
multi-scale language embeddings. 2) We transfer the pre-trained language field
to 3DGS, achieving the first real-time rendering speed without sacrificing
training time or accuracy. 3) We introduce a 3D querying and evaluation
protocol for assessing the reconstructed geometry and semantics together. Code,
checkpoints, and annotations will be available online. Project page:
https://hyunji12.github.io/Open3DRF

Comments:
- Project page: https://hyunji12.github.io/Open3DRF

---

## Progressive Radiance Distillation for Inverse Rendering with Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Keyang Ye, Qiming Hou, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2408.07595v1){: .btn .btn-green } |

**Abstract**: We propose progressive radiance distillation, an inverse rendering method
that combines physically-based rendering with Gaussian-based radiance field
rendering using a distillation progress map. Taking multi-view images as input,
our method starts from a pre-trained radiance field guidance, and distills
physically-based light and material parameters from the radiance field using an
image-fitting process. The distillation progress map is initialized to a small
value, which favors radiance field rendering. During early iterations when
fitted light and material parameters are far from convergence, the radiance
field fallback ensures the sanity of image loss gradients and avoids local
minima that attracts under-fit states. As fitted parameters converge, the
physical model gradually takes over and the distillation progress increases
correspondingly. In presence of light paths unmodeled by the physical model,
the distillation progress never finishes on affected pixels and the learned
radiance field stays in the final rendering. With this designed tolerance for
physical model limitations, we prevent unmodeled color components from leaking
into light and material parameters, alleviating relighting artifacts.
Meanwhile, the remaining radiance field compensates for the limitations of the
physical model, guaranteeing high-quality novel views synthesis. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
techniques quality-wise in both novel view synthesis and relighting. The idea
of progressive radiance distillation is not limited to Gaussian splatting. We
show that it also has positive effects for prominently specular scenes when
adapted to a mesh-based inverse rendering method.



---

## 3D Gaussian Editing with A Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-14 | Guan Luo, Tian-Xing Xu, Ying-Tian Liu, Xiao-Xiong Fan, Fang-Lue Zhang, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2408.07540v1){: .btn .btn-green } |

**Abstract**: The modeling and manipulation of 3D scenes captured from the real world are
pivotal in various applications, attracting growing research interest. While
previous works on editing have achieved interesting results through
manipulating 3D meshes, they often require accurately reconstructed meshes to
perform editing, which limits their application in 3D content generation. To
address this gap, we introduce a novel single-image-driven 3D scene editing
approach based on 3D Gaussian Splatting, enabling intuitive manipulation via
directly editing the content on a 2D image plane. Our method learns to optimize
the 3D Gaussians to align with an edited version of the image rendered from a
user-specified viewpoint of the original scene. To capture long-range object
deformation, we introduce positional loss into the optimization process of 3D
Gaussian Splatting and enable gradient propagation through reparameterization.
To handle occluded 3D Gaussians when rendering from the specified viewpoint, we
build an anchor-based structure and employ a coarse-to-fine optimization
strategy capable of handling long-range deformation while maintaining
structural stability. Furthermore, we design a novel masking strategy to
adaptively identify non-rigid deformation regions for fine-scale modeling.
Extensive experiments show the effectiveness of our method in handling
geometric details, long-range, and non-rigid deformation, demonstrating
superior editing flexibility and quality compared to previous approaches.

Comments:
- 10 pages, 12 figures

---

## HDRGS: High Dynamic Range Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Jiahao Wu, Lu Xiao, Chao Wang, Rui Peng, Kaiqiang Xiong, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.06543v1){: .btn .btn-green } |

**Abstract**: Recent years have witnessed substantial advancements in the field of 3D
reconstruction from 2D images, particularly following the introduction of the
neural radiance field (NeRF) technique. However, reconstructing a 3D high
dynamic range (HDR) radiance field, which aligns more closely with real-world
conditions, from 2D multi-exposure low dynamic range (LDR) images continues to
pose significant challenges. Approaches to this issue fall into two categories:
grid-based and implicit-based. Implicit methods, using multi-layer perceptrons
(MLP), face inefficiencies, limited solvability, and overfitting risks.
Conversely, grid-based methods require significant memory and struggle with
image quality and long training times. In this paper, we introduce Gaussian
Splatting-a recent, high-quality, real-time 3D reconstruction technique-into
this domain. We further develop the High Dynamic Range Gaussian Splatting
(HDR-GS) method, designed to address the aforementioned challenges. This method
enhances color dimensionality by including luminance and uses an asymmetric
grid for tone-mapping, swiftly and precisely converting pixel irradiance to
color. Our approach improves HDR scene recovery accuracy and integrates a novel
coarse-to-fine strategy to speed up model convergence, enhancing robustness
against sparse viewpoints and exposure extremes, and preventing local optima.
Extensive testing confirms that our method surpasses current state-of-the-art
techniques in both synthetic and real-world scenarios. Code will be released at
\url{https://github.com/WuJH2001/HDRGS}



---

## ActiveNeRF: Learning Accurate 3D Geometry by Active Pattern Projection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Jianyu Tao, Changping Hu, Edward Yang, Jing Xu, Rui Chen | cs.CV | [PDF](http://arxiv.org/pdf/2408.06592v1){: .btn .btn-green } |

**Abstract**: NeRFs have achieved incredible success in novel view synthesis. However, the
accuracy of the implicit geometry is unsatisfactory because the passive static
environmental illumination has low spatial frequency and cannot provide enough
information for accurate geometry reconstruction. In this work, we propose
ActiveNeRF, a 3D geometry reconstruction framework, which improves the geometry
quality of NeRF by actively projecting patterns of high spatial frequency onto
the scene using a projector which has a constant relative pose to the camera.
We design a learnable active pattern rendering pipeline which jointly learns
the scene geometry and the active pattern. We find that, by adding the active
pattern and imposing its consistency across different views, our proposed
method outperforms state of the art geometry reconstruction methods
qualitatively and quantitatively in both simulation and real experiments. Code
is avaliable at https://github.com/hcp16/active_nerf

Comments:
- 18 pages, 10 figures

---

## NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance  Fields in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Rishit Dagli, Atsuhiro Hibi, Rahul G. Krishnan, Pascal N. Tyrrell | cs.CV | [PDF](http://arxiv.org/pdf/2408.10258v2){: .btn .btn-green } |

**Abstract**: Current methods for performing 3D reconstruction and novel view synthesis
(NVS) in ultrasound imaging data often face severe artifacts when training
NeRF-based approaches. The artifacts produced by current approaches differ from
NeRF floaters in general scenes because of the unique nature of ultrasound
capture. Furthermore, existing models fail to produce reasonable 3D
reconstructions when ultrasound data is captured or obtained casually in
uncontrolled environments, which is common in clinical settings. Consequently,
existing reconstruction and NVS methods struggle to handle ultrasound motion,
fail to capture intricate details, and cannot model transparent and reflective
surfaces. In this work, we introduced NeRF-US, which incorporates 3D-geometry
guidance for border probability and scattering density into NeRF training,
while also utilizing ultrasound-specific rendering over traditional volume
rendering. These 3D priors are learned through a diffusion model. Through
experiments conducted on our new "Ultrasound in the Wild" dataset, we observed
accurate, clinically plausible, artifact-free reconstructions.



---

## SpectralGaussians: Semantic, spectral 3D Gaussian splatting for  multi-spectral scene representation, visualization and analysis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Saptarshi Neil Sinha, Holger Graf, Michael Weinmann | cs.CV | [PDF](http://arxiv.org/pdf/2408.06975v1){: .btn .btn-green } |

**Abstract**: We propose a novel cross-spectral rendering framework based on 3D Gaussian
Splatting (3DGS) that generates realistic and semantically meaningful splats
from registered multi-view spectrum and segmentation maps. This extension
enhances the representation of scenes with multiple spectra, providing insights
into the underlying materials and segmentation. We introduce an improved
physically-based rendering approach for Gaussian splats, estimating reflectance
and lights per spectra, thereby enhancing accuracy and realism. In a
comprehensive quantitative and qualitative evaluation, we demonstrate the
superior performance of our approach with respect to other recent
learning-based spectral scene representation approaches (i.e., XNeRF and
SpectralNeRF) as well as other non-spectral state-of-the-art learning-based
approaches. Our work also demonstrates the potential of spectral scene
understanding for precise scene editing techniques like style transfer,
inpainting, and removal. Thereby, our contributions address challenges in
multi-spectral scene representation, rendering, and editing, offering new
possibilities for diverse applications.



---

## Potamoi: Accelerating Neural Rendering via a Unified Streaming  Architecture

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-13 | Yu Feng, Weikai Lin, Zihan Liu, Jingwen Leng, Minyi Guo, Han Zhao, Xiaofeng Hou, Jieru Zhao, Yuhao Zhu | cs.AR | [PDF](http://arxiv.org/pdf/2408.06608v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has emerged as a promising alternative for
photorealistic rendering. Despite recent algorithmic advancements, achieving
real-time performance on today's resource-constrained devices remains
challenging. In this paper, we identify the primary bottlenecks in current NeRF
algorithms and introduce a unified algorithm-architecture co-design, Potamoi,
designed to accommodate various NeRF algorithms. Specifically, we introduce a
runtime system featuring a plug-and-play algorithm, SpaRW, which significantly
reduces the per-frame computational workload and alleviates compute
inefficiencies. Furthermore, our unified streaming pipeline coupled with
customized hardware support effectively tames both SRAM and DRAM inefficiencies
by minimizing repetitive DRAM access and completely eliminating SRAM bank
conflicts. When evaluated against a baseline utilizing a dedicated DNN
accelerator, our framework demonstrates a speed-up and energy reduction of
53.1$\times$ and 67.7$\times$, respectively, all while maintaining high visual
quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.

Comments:
- arXiv admin note: substantial text overlap with arXiv:2404.11852

---

## Developing Smart MAVs for Autonomous Inspection in GPS-denied  Constructions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Paoqiang Pan, Kewei Hu, Xiao Huang, Wei Ying, Xiaoxuan Xie, Yue Ma, Naizhong Zhang, Hanwen Kang | cs.RO | [PDF](http://arxiv.org/pdf/2408.06030v1){: .btn .btn-green } |

**Abstract**: Smart Micro Aerial Vehicles (MAVs) have transformed infrastructure inspection
by enabling efficient, high-resolution monitoring at various stages of
construction, including hard-to-reach areas. Traditional manual operation of
drones in GPS-denied environments, such as industrial facilities and
infrastructure, is labour-intensive, tedious and prone to error. This study
presents an innovative framework for smart MAV inspections in such complex and
GPS-denied indoor environments. The framework features a hierarchical
perception and planning system that identifies regions of interest and
optimises task paths. It also presents an advanced MAV system with enhanced
localisation and motion planning capabilities, integrated with Neural
Reconstruction technology for comprehensive 3D reconstruction of building
structures. The effectiveness of the framework was empirically validated in a
4,000 square meters indoor infrastructure facility with an interior length of
80 metres, a width of 50 metres and a height of 7 metres. The main structure
consists of columns and walls. Experimental results show that our MAV system
performs exceptionally well in autonomous inspection tasks, achieving a 100\%
success rate in generating and executing scan paths. Extensive experiments
validate the manoeuvrability of our developed MAV, achieving a 100\% success
rate in motion planning with a tracking error of less than 0.1 metres. In
addition, the enhanced reconstruction method using 3D Gaussian Splatting
technology enables the generation of high-fidelity rendering models from the
acquired data. Overall, our novel method represents a significant advancement
in the use of robotics for infrastructure inspection.



---

## 3D Reconstruction of Protein Structures from Multi-view AFM Images using  Neural Radiance Fields (NeRFs)

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Jaydeep Rade, Ethan Herron, Soumik Sarkar, Anwesha Sarkar, Adarsh Krishnamurthy | cs.CV | [PDF](http://arxiv.org/pdf/2408.06244v1){: .btn .btn-green } |

**Abstract**: Recent advancements in deep learning for predicting 3D protein structures
have shown promise, particularly when leveraging inputs like protein sequences
and Cryo-Electron microscopy (Cryo-EM) images. However, these techniques often
fall short when predicting the structures of protein complexes (PCs), which
involve multiple proteins. In our study, we investigate using atomic force
microscopy (AFM) combined with deep learning to predict the 3D structures of
PCs. AFM generates height maps that depict the PCs in various random
orientations, providing a rich information for training a neural network to
predict the 3D structures. We then employ the pre-trained UpFusion model (which
utilizes a conditional diffusion model for synthesizing novel views) to train
an instance-specific NeRF model for 3D reconstruction. The performance of
UpFusion is evaluated through zero-shot predictions of 3D protein structures
using AFM images. The challenge, however, lies in the time-intensive and
impractical nature of collecting actual AFM images. To address this, we use a
virtual AFM imaging process that transforms a `PDB' protein file into
multi-view 2D virtual AFM images via volume rendering techniques. We
extensively validate the UpFusion architecture using both virtual and actual
multi-view AFM images. Our results include a comparison of structures predicted
with varying numbers of views and different sets of views. This novel approach
holds significant potential for enhancing the accuracy of protein complex
structure predictions with further fine-tuning of the UpFusion network.



---

## Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for  Anti-aliasing Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Jiameng Li, Yue Shi, Jiezhang Cao, Bingbing Ni, Wenjun Zhang, Kai Zhang, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2408.06286v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has attracted great attention in novel view
synthesis because of its superior rendering efficiency and high fidelity.
However, the trained Gaussians suffer from severe zooming degradation due to
non-adjustable representation derived from single-scale training. Though some
methods attempt to tackle this problem via post-processing techniques such as
selective rendering or filtering techniques towards primitives, the
scale-specific information is not involved in Gaussians. In this paper, we
propose a unified optimization method to make Gaussians adaptive for arbitrary
scales by self-adjusting the primitive properties (e.g., color, shape and size)
and distribution (e.g., position). Inspired by the mipmap technique, we design
pseudo ground-truth for the target scale and propose a scale-consistency
guidance loss to inject scale information into 3D Gaussians. Our method is a
plug-in module, applicable for any 3DGS models to solve the zoom-in and
zoom-out aliasing. Extensive experiments demonstrate the effectiveness of our
method. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dB
for zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.

Comments:
- 9 pages

---

## HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-12 | Xiaozheng Zheng, Chao Wen, Zhaohu Li, Weiyi Zhang, Zhuo Su, Xu Chang, Yang Zhao, Zheng Lv, Xiaoyuan Zhang, Yongjie Zhang, Guidong Wang, Lan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2408.06019v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel 3D head avatar creation approach capable of
generalizing from few-shot in-the-wild data with high-fidelity and animatable
robustness. Given the underconstrained nature of this problem, incorporating
prior knowledge is essential. Therefore, we propose a framework comprising
prior learning and avatar creation phases. The prior learning phase leverages
3D head priors derived from a large-scale multi-view dynamic dataset, and the
avatar creation phase applies these priors for few-shot personalization. Our
approach effectively captures these priors by utilizing a Gaussian
Splatting-based auto-decoder network with part-based dynamic modeling. Our
method employs identity-shared encoding with personalized latent codes for
individual identities to learn the attributes of Gaussian primitives. During
the avatar creation phase, we achieve fast head avatar personalization by
leveraging inversion and fine-tuning strategies. Extensive experiments
demonstrate that our model effectively exploits head priors and successfully
generalizes them to few-shot personalization, achieving photo-realistic
rendering quality, multi-view consistency, and stable animation.

Comments:
- Project page: https://headgap.github.io/

---

## Visual SLAM with 3D Gaussian Primitives and Depth Priors Enabling Novel  View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-10 | Zhongche Qu, Zhi Zhang, Cong Liu, Jianhua Yin | cs.CV | [PDF](http://arxiv.org/pdf/2408.05635v1){: .btn .btn-green } |

**Abstract**: Conventional geometry-based SLAM systems lack dense 3D reconstruction
capabilities since their data association usually relies on feature
correspondences. Additionally, learning-based SLAM systems often fall short in
terms of real-time performance and accuracy. Balancing real-time performance
with dense 3D reconstruction capabilities is a challenging problem. In this
paper, we propose a real-time RGB-D SLAM system that incorporates a novel view
synthesis technique, 3D Gaussian Splatting, for 3D scene representation and
pose estimation. This technique leverages the real-time rendering performance
of 3D Gaussian Splatting with rasterization and allows for differentiable
optimization in real time through CUDA implementation. We also enable mesh
reconstruction from 3D Gaussians for explicit dense 3D reconstruction. To
estimate accurate camera poses, we utilize a rotation-translation decoupled
strategy with inverse optimization. This involves iteratively updating both in
several iterations through gradient-based optimization. This process includes
differentiably rendering RGB, depth, and silhouette maps and updating the
camera parameters to minimize a combined loss of photometric loss, depth
geometry loss, and visibility loss, given the existing 3D Gaussian map.
However, 3D Gaussian Splatting (3DGS) struggles to accurately represent
surfaces due to the multi-view inconsistency of 3D Gaussians, which can lead to
reduced accuracy in both camera pose estimation and scene reconstruction. To
address this, we utilize depth priors as additional regularization to enforce
geometric constraints, thereby improving the accuracy of both pose estimation
and 3D reconstruction. We also provide extensive experimental results on public
benchmark datasets to demonstrate the effectiveness of our proposed methods in
terms of pose accuracy, geometric accuracy, and rendering performance.



---

## Radiance Field Learners As UAV First-Person Viewers

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-10 | Liqi Yan, Qifan Wang, Junhan Zhao, Qiang Guan, Zheng Tang, Jianhui Zhang, Dongfang Liu | cs.CV | [PDF](http://arxiv.org/pdf/2408.05533v1){: .btn .btn-green } |

**Abstract**: First-Person-View (FPV) holds immense potential for revolutionizing the
trajectory of Unmanned Aerial Vehicles (UAVs), offering an exhilarating avenue
for navigating complex building structures. Yet, traditional Neural Radiance
Field (NeRF) methods face challenges such as sampling single points per
iteration and requiring an extensive array of views for supervision. UAV videos
exacerbate these issues with limited viewpoints and significant spatial scale
variations, resulting in inadequate detail rendering across diverse scales. In
response, we introduce FPV-NeRF, addressing these challenges through three key
facets: (1) Temporal consistency. Leveraging spatio-temporal continuity ensures
seamless coherence between frames; (2) Global structure. Incorporating various
global features during point sampling preserves space integrity; (3) Local
granularity. Employing a comprehensive framework and multi-resolution
supervision for multi-scale scene feature representation tackles the
intricacies of UAV video spatial scales. Additionally, due to the scarcity of
publicly available FPV videos, we introduce an innovative view synthesis method
using NeRF to generate FPV perspectives from UAV footage, enhancing spatial
perception for drones. Our novel dataset spans diverse trajectories, from
outdoor to indoor environments, in the UAV domain, differing significantly from
traditional NeRF scenarios. Through extensive experiments encompassing both
interior and exterior building structures, FPV-NeRF demonstrates a superior
understanding of the UAV flying space, outperforming state-of-the-art methods
in our curated UAV dataset. Explore our project page for further insights:
https://fpv-nerf.github.io/.

Comments:
- Accepted to ECCV 2024

---

## DreamCouple: Exploring High Quality Text-to-3D Generation Via Rectified  Flow

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Hangyu Li, Xiangxiang Chu, Dingyuan Shi | cs.CV | [PDF](http://arxiv.org/pdf/2408.05008v1){: .btn .btn-green } |

**Abstract**: The Score Distillation Sampling (SDS), which exploits pretrained
text-to-image model diffusion models as priors to 3D model training, has
achieved significant success. Currently, the flow-based diffusion model has
become a new trend for generations. Yet, adapting SDS to flow-based diffusion
models in 3D generation remains unexplored. Our work is aimed to bridge this
gap. In this paper, we adapt SDS to rectified flow and re-examine the
over-smoothing issue under this novel framework. The issue can be explained
that the model learns an average of multiple ODE trajectories. Then we propose
DreamCouple, which instead of randomly sampling noise, uses a rectified flow
model to find the coupled noise. Its Unique Couple Matching (UCM) loss guides
the model to learn different trajectories and thus solves the over-smoothing
issue. We apply our method to both NeRF and 3D Gaussian splatting and achieve
state-of-the-art performances. We also identify some other interesting open
questions such as initialization issues for NeRF and faster training
convergence. Our code will be released soon.

Comments:
- Tech Report

---

## Self-augmented Gaussian Splatting with Structure-aware Masks for  Sparse-view 3D Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Lingbei Meng, Bi'an Du, Wei Hu | cs.CV | [PDF](http://arxiv.org/pdf/2408.04831v2){: .btn .btn-green } |

**Abstract**: Sparse-view 3D reconstruction stands as a formidable challenge in computer
vision, aiming to build complete three-dimensional models from a limited array
of viewing perspectives. This task confronts several difficulties: 1) the
limited number of input images that lack consistent information; 2) dependence
on the quality of input images; and 3) the substantial size of model
parameters. To address these challenges, we propose a self-augmented
coarse-to-fine Gaussian splatting paradigm, enhanced with a structure-aware
mask, for sparse-view 3D reconstruction. In particular, our method initially
employs a coarse Gaussian model to obtain a basic 3D representation from
sparse-view inputs. Subsequently, we develop a fine Gaussian network to enhance
consistent and detailed representation of the output with both 3D geometry
augmentation and perceptual view augmentation. During training, we design a
structure-aware masking strategy to further improve the model's robustness
against sparse inputs and noise.Experimental results on the MipNeRF360 and
OmniObject3D datasets demonstrate that the proposed method achieves
state-of-the-art performances for sparse input views in both perceptual quality
and efficiency.



---

## FewShotNeRF: Meta-Learning-based Novel View Synthesis for Rapid  Scene-Specific Adaptation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-09 | Piraveen Sivakumar, Paul Janson, Jathushan Rajasegaran, Thanuja Ambegoda | cs.CV | [PDF](http://arxiv.org/pdf/2408.04803v1){: .btn .btn-green } |

**Abstract**: In this paper, we address the challenge of generating novel views of
real-world objects with limited multi-view images through our proposed
approach, FewShotNeRF. Our method utilizes meta-learning to acquire optimal
initialization, facilitating rapid adaptation of a Neural Radiance Field (NeRF)
to specific scenes. The focus of our meta-learning process is on capturing
shared geometry and textures within a category, embedded in the weight
initialization. This approach expedites the learning process of NeRFs and
leverages recent advancements in positional encodings to reduce the time
required for fitting a NeRF to a scene, thereby accelerating the inner loop
optimization of meta-learning. Notably, our method enables meta-learning on a
large number of 3D scenes to establish a robust 3D prior for various
categories. Through extensive evaluations on the Common Objects in 3D open
source dataset, we empirically demonstrate the efficacy and potential of
meta-learning in generating high-quality novel views of objects.



---

## Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs  Gaussian-Based Methods

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Yiming Zhou, Zixuan Zeng, Andi Chen, Xiaofan Zhou, Haowei Ni, Shiyao Zhang, Panfeng Li, Liangxi Liu, Mengyao Zheng, Xupeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2408.04268v1){: .btn .btn-green } |

**Abstract**: Exploring the capabilities of Neural Radiance Fields (NeRF) and
Gaussian-based methods in the context of 3D scene reconstruction, this study
contrasts these modern approaches with traditional Simultaneous Localization
and Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we
assess performance based on tracking accuracy, mapping fidelity, and view
synthesis. Findings reveal that NeRF excels in view synthesis, offering unique
capabilities in generating new perspectives from existing data, albeit at
slower processing speeds. Conversely, Gaussian-based methods provide rapid
processing and significant expressiveness but lack comprehensive scene
completion. Enhanced by global optimization and loop closure techniques, newer
methods like NICE-SLAM and SplaTAM not only surpass older frameworks such as
ORB-SLAM2 in terms of robustness but also demonstrate superior performance in
dynamic and complex environments. This comparative analysis bridges theoretical
research with practical implications, shedding light on future developments in
robust 3D scene reconstruction across various real-world applications.

Comments:
- Accepted by 2024 6th International Conference on Data-driven
  Optimization of Complex Systems

---

## InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Xin-Yi Yu, Jun-Xin Yu, Li-Bo Zhou, Yan Wei, Lin-Lin Ou | cs.CV | [PDF](http://arxiv.org/pdf/2408.04249v1){: .btn .btn-green } |

**Abstract**: We present InstantStyleGaussian, an innovative 3D style transfer method based
on the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target
style image, it quickly generates new 3D GS scenes. Our approach operates on
pre-reconstructed GS scenes, combining diffusion models with an improved
iterative dataset update strategy. It utilizes diffusion models to generate
target style images, adds these new images to the training dataset, and uses
this dataset to iteratively update and optimize the GS scenes. Extensive
experimental results demonstrate that our method ensures high-quality stylized
scenes while offering significant advantages in style transfer speed and
consistency.



---

## A Review of 3D Reconstruction Techniques for Deformable Tissues in  Robotic Surgery

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-08 | Mengya Xu, Ziqi Guo, An Wang, Long Bai, Hongliang Ren | cs.CV | [PDF](http://arxiv.org/pdf/2408.04426v1){: .btn .btn-green } |

**Abstract**: As a crucial and intricate task in robotic minimally invasive surgery,
reconstructing surgical scenes using stereo or monocular endoscopic video holds
immense potential for clinical applications. NeRF-based techniques have
recently garnered attention for the ability to reconstruct scenes implicitly.
On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly
using 3D Gaussians and projects them onto a 2D plane as a replacement for the
complex volume rendering in NeRF. However, these methods face challenges
regarding surgical scene reconstruction, such as slow inference, dynamic
scenes, and surgical tool occlusion. This work explores and reviews
state-of-the-art (SOTA) approaches, discussing their innovations and
implementation principles. Furthermore, we replicate the models and conduct
testing and evaluation on two datasets. The test results demonstrate that with
advancements in these techniques, achieving real-time, high-quality
reconstructions becomes feasible.

Comments:
- To appear in MICCAI 2024 EARTH Workshop. Code availability:
  https://github.com/Epsilon404/surgicalnerf

---

## 3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Zhe Jun Tang, Tat-Jen Cham | cs.CV | [PDF](http://arxiv.org/pdf/2408.03753v1){: .btn .btn-green } |

**Abstract**: The use of 3D Gaussians as representation of radiance fields has enabled high
quality novel view synthesis at real-time rendering speed. However, the choice
of optimising the outgoing radiance of each Gaussian independently as spherical
harmonics results in unsatisfactory view dependent effects. In response to
these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian
Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering
quality. Instead of optimising a single outgoing radiance parameter, 3iGS
enhances 3DGS view-dependent effects by expressing the outgoing radiance as a
function of a local illumination field and Bidirectional Reflectance
Distribution Function (BRDF) features. We optimise a continuous incident
illumination field through a Tensorial Factorisation representation, while
separately fine-tuning the BRDF features of each 3D Gaussian relative to this
illumination field. Our methodology significantly enhances the rendering
quality of specular view-dependent effects of 3DGS, while maintaining rapid
training and rendering speeds.

Comments:
- The 18th European Conference on Computer Vision ECCV 2024

---

## Goal-oriented Semantic Communication for the Metaverse Application

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Zhe Wang, Nan Li, Yansha Deng | eess.SY | [PDF](http://arxiv.org/pdf/2408.03646v1){: .btn .btn-green } |

**Abstract**: With the emergence of the metaverse and its role in enabling real-time
simulation and analysis of real-world counterparts, an increasing number of
personalized metaverse scenarios are being created to influence entertainment
experiences and social behaviors. However, compared to traditional image and
video entertainment applications, the exact transmission of the vast amount of
metaverse-associated information significantly challenges the capacity of
existing bit-oriented communication networks. Moreover, the current metaverse
also witnesses a growing goal shift for transmitting the meaning behind
custom-designed content, such as user-designed buildings and avatars, rather
than exact copies of physical objects. To meet this growing goal shift and
bandwidth challenge, this paper proposes a goal-oriented semantic communication
framework for metaverse application (GSCM) to explore and define semantic
information through the goal levels. Specifically, we first analyze the
traditional image communication framework in metaverse construction and then
detail our proposed semantic information along with the end-to-end wireless
communication. We then describe the designed modules of the GSCM framework,
including goal-oriented semantic information extraction, base knowledge
definition, and neural radiance field (NeRF) based metaverse construction.
Finally, numerous experiments have been conducted to demonstrate that, compared
to image communication, our proposed GSCM framework decreases transmission
latency by up to 92.6% and enhances the virtual object operation accuracy and
metaverse construction clearance by up to 45.6% and 44.7%, respectively.



---

## PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time  High-Quality Relighting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Yijia Guo, Yuanxi Bai, Liwen Hu, Ziyi Guo, Mianzhi Liu, Yu Cai, Tiejun Huang, Lei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2408.03538v1){: .btn .btn-green } |

**Abstract**: We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a
real-time high-quality relighting method for Gaussian splats in low-frequency
lighting environments that captures soft shadows and interreflections by
precomputing 3D Gaussian splats' radiance transfer. Existing studies have
demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'
efficiency for dynamic lighting scenarios. However, the current relighting
method based on 3DGS still struggles to compute high-quality shadow and
indirect illumination in real time for dynamic light, leading to unrealistic
rendering results. We solve this problem by precomputing the expensive
transport simulations required for complex transfer functions like shadowing,
the resulting transfer functions are represented as dense sets of vectors or
matrices for every Gaussian splat. We introduce distinct precomputing methods
tailored for training and rendering stages, along with unique ray tracing and
indirect lighting precomputation techniques for 3D Gaussian splats to
accelerate training speed and compute accurate indirect lighting related to
environment light. Experimental analyses demonstrate that our approach achieves
state-of-the-art visual quality while maintaining competitive training times
and allows high-quality real-time (30+ fps) relighting for dynamic light and
relatively complex scenes at 1080p resolution.



---

## Towards Real-Time Gaussian Splatting: Accelerating 3DGS through  Photometric SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Yan Song Hu, Dayou Mao, Yuhao Chen, John Zelek | cs.RO | [PDF](http://arxiv.org/pdf/2408.03825v1){: .btn .btn-green } |

**Abstract**: Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous
Localization and Mapping (VSLAM) demonstrate the generation of high-quality
volumetric reconstructions from monocular video streams. However, despite these
promising advancements, current 3DGS integrations have reduced tracking
performance and lower operating speeds compared to traditional VSLAM. To
address these issues, we propose integrating 3DGS with Direct Sparse Odometry,
a monocular photometric SLAM system. We have done preliminary experiments
showing that using Direct Sparse Odometry point cloud outputs, as opposed to
standard structure-from-motion methods, significantly shortens the training
time needed to achieve high-quality renders. Reducing 3DGS training time
enables the development of 3DGS-integrated SLAM systems that operate in
real-time on mobile hardware. These promising initial findings suggest further
exploration is warranted in combining traditional VSLAM systems with 3DGS.

Comments:
- This extended abstract has been submitted to be presented at an IEEE
  conference. It will be made available online by IEEE but will not be
  published in IEEE Xplore. Copyright may be transferred without notice, after
  which this version may no longer be accessible

---

## Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-07 | Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2408.03822v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has recently emerged as an alternative
representation that leverages a 3D Gaussian-based representation and introduces
an approximated volumetric rendering, achieving very fast rendering speed and
promising image quality. Furthermore, subsequent studies have successfully
extended 3DGS to dynamic 3D scenes, demonstrating its wide range of
applications. However, a significant drawback arises as 3DGS and its following
methods entail a substantial number of Gaussians to maintain the high fidelity
of the rendered images, which requires a large amount of memory and storage. To
address this critical issue, we place a specific emphasis on two key
objectives: reducing the number of Gaussian points without sacrificing
performance and compressing the Gaussian attributes, such as view-dependent
color and covariance. To this end, we propose a learnable mask strategy that
significantly reduces the number of Gaussians while preserving high
performance. In addition, we propose a compact but effective representation of
view-dependent color by employing a grid-based neural field rather than relying
on spherical harmonics. Finally, we learn codebooks to compactly represent the
geometric and temporal attributes by residual vector quantization. With model
compression techniques such as quantization and entropy coding, we consistently
show over 25x reduced storage and enhanced rendering speed compared to 3DGS for
static scenes, while maintaining the quality of the scene representation. For
dynamic scenes, our approach achieves more than 12x storage efficiency and
retains a high-quality reconstruction compared to the existing state-of-the-art
methods. Our work provides a comprehensive framework for 3D scene
representation, achieving high performance, fast training, compactness, and
real-time rendering. Our project page is available at
https://maincold2.github.io/c3dgs/.

Comments:
- Project page: https://maincold2.github.io/c3dgs/

---

## MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Tengfei Wang, Zongqian Zhan, Rui Xia, Linxia Ji, Xin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.03060v1){: .btn .btn-green } |

**Abstract**: Over the last few decades, image-based building surface reconstruction has
garnered substantial research interest and has been applied across various
fields, such as heritage preservation, architectural planning, etc. Compared to
the traditional photogrammetric and NeRF-based solutions, recently, Gaussian
fields-based methods have exhibited significant potential in generating surface
meshes due to their time-efficient training and detailed 3D information
preservation. However, most gaussian fields-based methods are trained with all
image pixels, encompassing building and nonbuilding areas, which results in a
significant noise for building meshes and degeneration in time efficiency. This
paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to
generate accurate surface reconstruction for building in a time-efficient way.
The framework first applies EfficientSAM and COLMAP to generate multi-level
masks of building and the corresponding masked point clouds. Subsequently, the
masked gaussian fields are trained by integrating two innovative losses: a
multi-level perceptual masked loss focused on constructing building regions and
a boundary loss aimed at enhancing the details of the boundaries between
different masks. Finally, we improve the tetrahedral surface mesh extraction
method based on the masked gaussian spheres. Comprehensive experiments on UAV
images demonstrate that, compared to the traditional method and several
NeRF-based and Gaussian-based SOTA solutions, our approach significantly
improves both the accuracy and efficiency of building surface reconstruction.
Notably, as a byproduct, there is an additional gain in the novel view
synthesis of building.



---

## Efficient NeRF Optimization -- Not All Samples Remain Equally Hard

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Juuso Korhonen, Goutham Rangu, Hamed R. Tavakoli, Juho Kannala | cs.CV | [PDF](http://arxiv.org/pdf/2408.03193v1){: .btn .btn-green } |

**Abstract**: We propose an application of online hard sample mining for efficient training
of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality
for many 3D reconstruction and rendering tasks but require substantial
computational resources. The encoding of the scene information within the NeRF
network parameters necessitates stochastic sampling. We observe that during the
training, a major part of the compute time and memory usage is spent on
processing already learnt samples, which no longer affect the model update
significantly. We identify the backward pass on the stochastic samples as the
computational bottleneck during the optimization. We thus perform the first
forward pass in inference mode as a relatively low-cost search for hard
samples. This is followed by building the computational graph and updating the
NeRF network parameters using only the hard samples. To demonstrate the
effectiveness of the proposed approach, we apply our method to Instant-NGP,
resulting in significant improvements of the view-synthesis quality over the
baseline (1 dB improvement on average per training time, or 2x speedup to reach
the same PSNR level) along with approx. 40% memory savings coming from using
only the hard samples to build the computational graph. As our method only
interfaces with the network module, we expect it to be widely applicable.



---

## RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel  View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic | cs.CV | [PDF](http://arxiv.org/pdf/2408.03356v1){: .btn .btn-green } |

**Abstract**: Differentiable volumetric rendering-based methods made significant progress
in novel view synthesis. On one hand, innovative methods have replaced the
Neural Radiance Fields (NeRF) network with locally parameterized structures,
enabling high-quality renderings in a reasonable time. On the other hand,
approaches have used differentiable splatting instead of NeRF's ray casting to
optimize radiance fields rapidly using Gaussian kernels, allowing for fine
adaptation to the scene. However, differentiable ray casting of irregularly
spaced kernels has been scarcely explored, while splatting, despite enabling
fast rendering times, is susceptible to clearly visible artifacts.
  Our work closes this gap by providing a physically consistent formulation of
the emitted radiance c and density {\sigma}, decomposed with Gaussian functions
associated with Spherical Gaussians/Harmonics for all-frequency colorimetric
representation. We also introduce a method enabling differentiable ray casting
of irregularly distributed Gaussians using an algorithm that integrates
radiance fields slab by slab and leverages a BVH structure. This allows our
approach to finely adapt to the scene while avoiding splatting artifacts. As a
result, we achieve superior rendering quality compared to the state-of-the-art
while maintaining reasonable training times and achieving inference speeds of
25 FPS on the Blender dataset. Project page with videos and code:
https://raygauss.github.io/

Comments:
- Project page with videos and code: https://raygauss.github.io/

---

## LumiGauss: High-Fidelity Outdoor Relighting with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-06 | Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, Marek Kowalski | cs.CV | [PDF](http://arxiv.org/pdf/2408.04474v1){: .btn .btn-green } |

**Abstract**: Decoupling lighting from geometry using unconstrained photo collections is
notoriously challenging. Solving it would benefit many users, as creating
complex 3D assets takes days of manual labor. Many previous works have
attempted to address this issue, often at the expense of output fidelity, which
questions the practicality of such methods.
  We introduce LumiGauss, a technique that tackles 3D reconstruction of scenes
and environmental lighting through 2D Gaussian Splatting. Our approach yields
high-quality scene reconstructions and enables realistic lighting synthesis
under novel environment maps. We also propose a method for enhancing the
quality of shadows, common in outdoor scenes, by exploiting spherical harmonics
properties. Our approach facilitates seamless integration with game engines and
enables the use of fast precomputed radiance transfer.
  We validate our method on the NeRF-OSR dataset, demonstrating superior
performance over baseline methods. Moreover, LumiGauss can synthesize realistic
images when applying novel environment maps.

Comments:
- Includes video files in src

---

## PanicleNeRF: low-cost, high-precision in-field phenotypingof rice  panicles with smartphone

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-04 | Xin Yang, Xuqi Lu, Pengyao Xie, Ziyue Guo, Hui Fang, Haowei Fu, Xiaochun Hu, Zhenbiao Sun, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2408.02053v1){: .btn .btn-green } |

**Abstract**: The rice panicle traits significantly influence grain yield, making them a
primary target for rice phenotyping studies. However, most existing techniques
are limited to controlled indoor environments and difficult to capture the rice
panicle traits under natural growth conditions. Here, we developed PanicleNeRF,
a novel method that enables high-precision and low-cost reconstruction of rice
panicle three-dimensional (3D) models in the field using smartphone. The
proposed method combined the large model Segment Anything Model (SAM) and the
small model You Only Look Once version 8 (YOLOv8) to achieve high-precision
segmentation of rice panicle images. The NeRF technique was then employed for
3D reconstruction using the images with 2D segmentation. Finally, the resulting
point clouds are processed to successfully extract panicle traits. The results
show that PanicleNeRF effectively addressed the 2D image segmentation task,
achieving a mean F1 Score of 86.9% and a mean Intersection over Union (IoU) of
79.8%, with nearly double the boundary overlap (BO) performance compared to
YOLOv8. As for point cloud quality, PanicleNeRF significantly outperformed
traditional SfM-MVS (structure-from-motion and multi-view stereo) methods, such
as COLMAP and Metashape. The panicle length was then accurately extracted with
the rRMSE of 2.94% for indica and 1.75% for japonica rice. The panicle volume
estimated from 3D point clouds strongly correlated with the grain number (R2 =
0.85 for indica and 0.82 for japonica) and grain mass (0.80 for indica and 0.76
for japonica). This method provides a low-cost solution for high-throughput
in-field phenotyping of rice panicles, accelerating the efficiency of rice
breeding.



---

## E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-03 | Yunshan Qi, Jia Li, Yifan Zhao, Yu Zhang, Lin Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2408.01840v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieve impressive rendering performance by
learning volumetric 3D representation from several images of different views.
However, it is difficult to reconstruct a sharp NeRF from blurry input as it
often occurs in the wild. To solve this problem, we propose a novel Efficient
Event-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and
event streams. To effectively introduce event streams into the neural
volumetric representation learning process, we propose an event-enhanced blur
rendering loss and an event rendering loss, which guide the network via
modeling the real blur process and event generation process, respectively.
Specifically, we leverage spatial-temporal information from the event stream to
evenly distribute learning attention over temporal blur while simultaneously
focusing on blurry texture through the spatial attention. Moreover, a camera
pose estimation framework for real-world data is built with the guidance of the
events to generalize the method to practical applications. Compared to previous
image-based or event-based NeRF, our framework makes more profound use of the
internal relationship between events and images. Extensive experiments on both
synthetic data and real-world data demonstrate that E$^3$NeRF can effectively
learn a sharp NeRF from blurry images, especially in non-uniform motion and
low-light scenes.



---

## FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and  Fisheye Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-03 | Yifan Wu, Tianyi Cheng, Peixu Xin, Janusz Konrad | cs.CV | [PDF](http://arxiv.org/pdf/2408.01878v1){: .btn .btn-green } |

**Abstract**: Previous studies aiming to optimize and bundle-adjust camera poses using
Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated
impressive capabilities in 3D scene reconstruction. However, these approaches
have been designed for pinhole-camera pose optimization and do not perform well
under radial image distortions such as those in fisheye cameras. Furthermore,
inaccurate depth initialization in DBARF results in erroneous geometric
information affecting the overall convergence and quality of results. In this
paper, we propose adaptive GRUs with a flexible bundle-adjustment method
adapted to radial distortions and incorporate feature-based recurrent neural
networks to generate continuous novel views from fisheye datasets. Other NeRF
methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray
distance loss for distorted pose refinement, causing severe artifacts, long
rendering time, and are difficult to use in downstream tasks, where the dense
voxel representation generated by a NeRF method needs to be converted into a
mesh representation. We also address depth initialization issues by adding
MiDaS-based depth priors for pinhole images. Through extensive experiments, we
demonstrate the generalization capacity of FBINeRF and show high-fidelity
results for both pinhole-camera and fisheye-camera NeRFs.

Comments:
- 18 pages

---

## A General Framework to Boost 3D GS Initialization for Text-to-3D  Generation by Lexical Richness


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Lutao Jiang, Hangyu Li, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2408.01269v1){: .btn .btn-green } |

**Abstract**: Text-to-3D content creation has recently received much attention, especially
with the prevalence of 3D Gaussians Splatting. In general, GS-based methods
comprise two key stages: initialization and rendering optimization. To achieve
initialization, existing works directly apply random sphere initialization or
3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such
strategies suffer from two critical yet challenging problems: 1) the final
shapes are still similar to the initial ones even after training; 2) shapes can
be produced only from simple texts, e.g., "a dog", not for lexically richer
texts, e.g., "a dog is sitting on the top of the airplane". To address these
problems, this paper proposes a novel general framework to boost the 3D GS
Initialization for text-to-3D generation upon the lexical richness. Our key
idea is to aggregate 3D Gaussians into spatially uniform voxels to represent
complex shapes while enabling the spatial interaction among the 3D Gaussians
and semantic interaction between Gaussians and texts. Specifically, we first
construct a voxelized representation, where each voxel holds a 3D Gaussian with
its position, scale, and rotation fixed while setting opacity as the sole
factor to determine a position's occupancy. We then design an initialization
network mainly consisting of two novel components: 1) Global Information
Perception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design
enables each 3D Gaussian to assimilate the spatial information from other areas
and semantic information from texts. Extensive experiments show the superiority
of our framework of high-quality 3D GS initialization against the existing
methods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.
Also, our framework can be seamlessly plugged into SoTA training frameworks,
e.g., LucidDreamer, for semantically consistent text-to-3D generation.



---

## IG-SLAM: Instant Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | F. Aykut Sarikamis, A. Aydin Alatan | cs.CV | [PDF](http://arxiv.org/pdf/2408.01126v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently shown promising results as an alternative
scene representation in SLAM systems to neural implicit representations.
However, current methods either lack dense depth maps to supervise the mapping
process or detailed training designs that consider the scale of the
environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only
SLAM system that employs robust Dense-SLAM methods for tracking and combines
them with Gaussian Splatting. A 3D map of the environment is constructed using
accurate pose and dense depth provided by tracking. Additionally, we utilize
depth uncertainty in map optimization to improve 3D reconstruction. Our decay
strategy in map optimization enhances convergence and allows the system to run
at 10 fps in a single process. We demonstrate competitive performance with
state-of-the-art RGB-only SLAM systems while achieving faster operation speeds.
We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC
datasets. The system achieves photo-realistic 3D reconstruction in large-scale
sequences, particularly in the EuRoC dataset.

Comments:
- 8 pages, 3 page ref, 5 figures

---

## NeRFoot: Robot-Footprint Estimation for Image-Based Visual Servoing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Daoxin Zhong, Luke Robinson, Daniele De Martini | cs.RO | [PDF](http://arxiv.org/pdf/2408.01251v1){: .btn .btn-green } |

**Abstract**: This paper investigates the utility of Neural Radiance Fields (NeRF) models
in extending the regions of operation of a mobile robot, controlled by
Image-Based Visual Servoing (IBVS) via static CCTV cameras. Using NeRF as a
3D-representation prior, the robot's footprint may be extrapolated
geometrically and used to train a CNN-based network to extract it online from
the robot's appearance alone. The resulting footprint results in a tighter
bound than a robot-wide bounding box, allowing the robot's controller to
prescribe more optimal trajectories and expand its safe operational floor area.



---

## Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation  with Volumetric Visual Data Fusion


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-02 | Ke Li, Reinhard Bacher, Susanne Schmidt, Wim Leemans, Frank Steinicke | cs.RO | [PDF](http://arxiv.org/pdf/2408.01225v1){: .btn .btn-green } |

**Abstract**: We introduce Reality Fusion, a novel robot teleoperation system that
localizes, streams, projects, and merges a typical onboard depth sensor with a
photorealistic, high resolution, high framerate, and wide field of view (FoV)
rendering of the complex remote environment represented as 3D Gaussian splats
(3DGS). Our framework enables robust egocentric and exocentric robot
teleoperation in immersive VR, with the 3DGS effectively extending spatial
information of a depth sensor with limited FoV and balancing the trade-off
between data streaming costs and data visual quality. We evaluated our
framework through a user study with 24 participants, which revealed that
Reality Fusion leads to significantly better user performance, situation
awareness, and user preferences. To support further research and development,
we provide an open-source implementation with an easy-to-replicate custom-made
telepresence robot, a high-performance virtual reality 3DGS renderer, and an
immersive robot control package. (Source code:
https://github.com/uhhhci/RealityFusion)

Comments:
- Accepted, to appear at IROS 2024

---

## LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Zhenyu Bao, Guibiao Liao, Kaichen Zhou, Kanglin Liu, Qing Li, Guoping Qiu | cs.CV | [PDF](http://arxiv.org/pdf/2408.00254v1){: .btn .btn-green } |

**Abstract**: Despite the photorealistic novel view synthesis (NVS) performance achieved by
the original 3D Gaussian splatting (3DGS), its rendering quality significantly
degrades with sparse input views. This performance drop is mainly caused by the
limited number of initial points generated from the sparse input, insufficient
supervision during the training process, and inadequate regularization of the
oversized Gaussian ellipsoids. To handle these issues, we propose the
LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis
task. In specific, we propose a loop-based Progressive Gaussian Initialization
(PGI) strategy that could iteratively densify the initialized point cloud using
the rendered pseudo images during the training process. Then, the sparse and
reliable depth from the Structure from Motion, and the window-based dense
monocular depth are leveraged to provide precise geometric supervision via the
proposed Depth-alignment Regularization (DAR). Additionally, we introduce a
novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian
ellipsoids leading to large pixel errors. Comprehensive experiments on four
datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art
methods for sparse-input novel view synthesis, across indoor, outdoor, and
object-level scenes with various image resolutions.

Comments:
- 13 pages, 10 figures

---

## UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with  Ultrasound Reflection Direction Parameterization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-08-01 | Ziwen Guo, Zi Fang, Zhuang Fu | cs.AI | [PDF](http://arxiv.org/pdf/2408.00860v2){: .btn .btn-green } |

**Abstract**: Three-dimensional ultrasound imaging is a critical technology widely used in
medical diagnostics. However, traditional 3D ultrasound imaging methods have
limitations such as fixed resolution, low storage efficiency, and insufficient
contextual connectivity, leading to poor performance in handling complex
artifacts and reflection characteristics. Recently, techniques based on NeRF
(Neural Radiance Fields) have made significant progress in view synthesis and
3D reconstruction, but there remains a research gap in high-quality ultrasound
imaging. To address these issues, we propose a new model, UlRe-NeRF, which
combines implicit neural networks and explicit ultrasound volume rendering into
an ultrasound neural rendering architecture. This model incorporates reflection
direction parameterization and harmonic encoding, using a directional MLP
module to generate view-dependent high-frequency reflection intensity
estimates, and a spatial MLP module to produce the medium's physical property
parameters. These parameters are used in the volume rendering process to
accurately reproduce the propagation and reflection behavior of ultrasound
waves in the medium. Experimental results demonstrate that the UlRe-NeRF model
significantly enhances the realism and accuracy of high-fidelity ultrasound
image reconstruction, especially in handling complex medium structures.


