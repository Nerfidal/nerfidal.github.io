---
layout: default
title: October 2025
parent: Papers
nav_order: 202510
---

<!---metadata--->


## 4-Doodle: Text to 3D Sketches that Move!

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-29 | Hao Chen, Jiaqi Wang, Yonggang Qi, Ke Li, Kaiyue Pang, Yi-Zhe Song | cs.GR | [PDF](http://arxiv.org/pdf/2510.25319v1){: .btn .btn-green } |

**Abstract**: We present a novel task: text-to-3D sketch animation, which aims to bring
freeform sketches to life in dynamic 3D space. Unlike prior works focused on
photorealistic content generation, we target sparse, stylized, and
view-consistent 3D vector sketches, a lightweight and interpretable medium
well-suited for visual communication and prototyping. However, this task is
very challenging: (i) no paired dataset exists for text and 3D (or 4D)
sketches; (ii) sketches require structural abstraction that is difficult to
model with conventional 3D representations like NeRFs or point clouds; and
(iii) animating such sketches demands temporal coherence and multi-view
consistency, which current pipelines do not address. Therefore, we propose
4-Doodle, the first training-free framework for generating dynamic 3D sketches
from text. It leverages pretrained image and video diffusion models through a
dual-space distillation scheme: one space captures multi-view-consistent
geometry using differentiable B\'ezier curves, while the other encodes motion
dynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion),
which optimizes from a single view per step, our multi-view optimization
ensures structural alignment and avoids view ambiguity, critical for sparse
sketches. Furthermore, we introduce a structure-aware motion module that
separates shape-preserving trajectories from deformation-aware changes,
enabling expressive motion such as flipping, rotation, and articulated
movement. Extensive experiments show that our method produces temporally
realistic and structurally stable 3D sketch animations, outperforming existing
baselines in both fidelity and controllability. We hope this work serves as a
step toward more intuitive and accessible 4D content creation.



---

## $D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-29 | Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.25173v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian Splatting (GS) has shown great potential for urban scene
reconstruction in the field of autonomous driving. However, current urban scene
reconstruction methods often depend on multimodal sensors as inputs,
\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR
point clouds can largely mitigate ill-posedness in reconstruction, acquiring
such accurate LiDAR data is still challenging in practice: i) precise
spatiotemporal calibration between LiDAR and other sensors is required, as they
may not capture data simultaneously; ii) reprojection errors arise from spatial
misalignment when LiDAR and cameras are mounted at different locations. To
avoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a
LiDAR-free urban scene reconstruction framework. In this work, we obtain
geometry priors that are as effective as LiDAR while being denser and more
accurate. $\textbf{First}$, we initialize a dense point cloud by
back-projecting multi-view metric depth predictions. This point cloud is then
optimized by a Progressive Pruning strategy to improve the global consistency.
$\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense
metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors
from a depth foundation model to enhance the depth maps rendered by Gaussians.
In turn, the enhanced depths provide stronger geometric constraints during
Gaussian training. $\textbf{Finally}$, we improve the accuracy of ground
geometry by constraining the shape and normal attributes of Gaussians within
road regions. Extensive experiments on the Waymo dataset demonstrate that our
method consistently outperforms state-of-the-art methods, producing more
accurate geometry even when compared with those using ground-truth LiDAR data.



---

## AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit  Structured Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-29 | Xiyu Zhang, Chong Bao, Yipeng Chen, Hongjia Zhai, Yitong Dong, Hujun Bao, Zhaopeng Cui, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.25129v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction of indoor and urban environments is a prominent research
topic with various downstream applications. However, existing geometric priors
for addressing low-texture regions in indoor and urban settings often lack
global consistency. Moreover, Gaussian Splatting and implicit SDF fields often
suffer from discontinuities or exhibit computational inefficiencies, resulting
in a loss of detail. To address these issues, we propose an Atlanta-world
guided implicit-structured Gaussian Splatting that achieves smooth indoor and
urban scene reconstruction while preserving high-frequency details and
rendering efficiency. By leveraging the Atlanta-world model, we ensure the
accurate surface reconstruction for low-texture regions, while the proposed
novel implicit-structured GS representations provide smoothness without
sacrificing efficiency and high-frequency details. Specifically, we propose a
semantic GS representation to predict the probability of all semantic regions
and deploy a structure plane regularization with learnable plane indicators for
global accurate surface reconstruction. Extensive experiments demonstrate that
our method outperforms state-of-the-art approaches in both indoor and urban
scenes, delivering superior surface reconstruction quality.

Comments:
- 18 pages, 11 figures. NeurIPS 2025; Project page:
  https://zju3dv.github.io/AtlasGS/

---

## NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-28 | Mingyu Jeong, Eunsung Kim, Sehun Park, Andrew Jaeyong Choi | cs.RO | [PDF](http://arxiv.org/pdf/2510.24335v1){: .btn .btn-green } |

**Abstract**: We present NVSim, a framework that automatically constructs large-scale,
navigable indoor simulators from only common image sequences, overcoming the
cost and scalability limitations of traditional 3D scanning. Our approach
adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed
floors a common issue in robotic traversal data. We introduce Floor-Aware
Gaussian Splatting to ensure a clean, navigable ground plane, and a novel
mesh-free traversability checking algorithm that constructs a topological graph
by directly analyzing rendered views. We demonstrate our system's ability to
generate valid, large-scale navigation graphs from real-world data. A video
demonstration is avilable at https://youtu.be/tTiIQt6nXC8

Comments:
- 9 pages, 10 figures

---

## LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal  Open-vocabulary Multi-goal Visual Navigation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-28 | Haotian Zhou, Xiaole Wang, He Li, Fusheng Sun, Shengyu Guo, Guolei Qi, Jianghuan Xu, Huijing Zhao | cs.RO | [PDF](http://arxiv.org/pdf/2510.24118v1){: .btn .btn-green } |

**Abstract**: Navigating to a designated goal using visual information is a fundamental
capability for intelligent robots. Most classical visual navigation methods are
restricted to single-goal, single-modality, and closed set goal settings. To
address the practical demands of multi-modal, open-vocabulary goal queries and
multi-goal visual navigation, we propose LagMemo, a navigation system that
leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo
constructs a unified 3D language memory. With incoming task goals, the system
queries the memory, predicts candidate goal locations, and integrates a local
perception-based verification mechanism to dynamically match and validate goals
during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a
high-quality core split distilled from GOAT-Bench tailored to multi-modal
open-vocabulary multi-goal visual navigation. Experimental results show that
LagMemo's memory module enables effective multi-modal open-vocabulary goal
localization, and that LagMemo outperforms state-of-the-art methods in
multi-goal visual navigation. Project page:
https://weekgoodday.github.io/lagmemo



---

## A Survey on Collaborative SLAM with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-28 | Phuc Nguyen Xuan, Thanh Nguyen Canh, Huu-Hung Nguyen, Nak Young Chong, Xiem HoangVan | cs.RO | [PDF](http://arxiv.org/pdf/2510.23988v1){: .btn .btn-green } |

**Abstract**: This survey comprehensively reviews the evolving field of multi-robot
collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian
Splatting (3DGS). As an explicit scene representation, 3DGS has enabled
unprecedented real-time, high-fidelity rendering, ideal for robotics. However,
its use in multi-robot systems introduces significant challenges in maintaining
global consistency, managing communication, and fusing data from heterogeneous
sources. We systematically categorize approaches by their architecture --
centralized, distributed -- and analyze core components like multi-agent
consistency and alignment, communication-efficient, Gaussian representation,
semantic distillation, fusion and pose optimization, and real-time scalability.
In addition, a summary of critical datasets and evaluation metrics is provided
to contextualize performance. Finally, we identify key open challenges and
chart future research directions, including lifelong mapping, semantic
association and mapping, multi-model for robustness, and bridging the Sim2Real
gap.



---

## PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by  Vision-Language Planar Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-27 | Xirui Jin, Renbiao Jin, Boying Li, Danping Zou, Wenxian Yu | cs.CV | [PDF](http://arxiv.org/pdf/2510.23930v1){: .btn .btn-green } |

**Abstract**: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

Comments:
- Accepted by NeurIPS 2025. Project page: https://planargs.github.io

---

## VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-27 | Hoonhee Cho, Jae-Young Kang, Giwon Lee, Hyemin Yang, Heejun Park, Seokwoo Jung, Kuk-Jin Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2510.23205v1){: .btn .btn-green } |

**Abstract**: End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm
that unifies perception, prediction, and planning into a holistic, data-driven
framework. However, achieving robustness to varying camera viewpoints, a common
real-world challenge due to diverse vehicle configurations, remains an open
problem. In this work, we propose VR-Drive, a novel E2E-AD framework that
addresses viewpoint generalization by jointly learning 3D scene reconstruction
as an auxiliary task to enable planning-aware view synthesis. Unlike prior
scene-specific synthesis approaches, VR-Drive adopts a feed-forward inference
strategy that supports online training-time augmentation from sparse views
without additional annotations. To further improve viewpoint consistency, we
introduce a viewpoint-mixed memory bank that facilitates temporal interaction
across multiple viewpoints and a viewpoint-consistent distillation strategy
that transfers knowledge from original to synthesized views. Trained in a fully
end-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and
improves planning under viewpoint shifts. In addition, we release a new
benchmark dataset to evaluate E2E-AD performance under novel camera viewpoints,
enabling comprehensive analysis. Our results demonstrate that VR-Drive is a
scalable and robust solution for the real-world deployment of end-to-end
autonomous driving systems.

Comments:
- Accepted by NeurIPS2025

---

## EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-27 | Taoyu Wu, Yiyi Miao, Jiaxin Guo, Ziyan Chen, Sihang Zhao, Zhuoxiao Li, Zhe Tang, Baoru Huang, Limin Yu | cs.CV | [PDF](http://arxiv.org/pdf/2510.23087v1){: .btn .btn-green } |

**Abstract**: In robot-assisted minimally invasive surgery, accurate 3D reconstruction from
endoscopic video is vital for downstream tasks and improved outcomes. However,
endoscopic scenarios present unique challenges, including photometric
inconsistencies, non-rigid tissue motion, and view-dependent highlights. Most
3DGS-based methods that rely solely on appearance constraints for optimizing
3DGS are often insufficient in this context, as these dynamic visual artifacts
can mislead the optimization process and lead to inaccurate reconstructions. To
address these limitations, we present EndoWave, a unified spatio-temporal
Gaussian Splatting framework by incorporating an optical flow-based geometric
constraint and a multi-resolution rational wavelet supervision. First, we adopt
a unified spatio-temporal Gaussian representation that directly optimizes
primitives in a 4D domain. Second, we propose a geometric constraint derived
from optical flow to enhance temporal coherence and effectively constrain the
3D structure of the scene. Third, we propose a multi-resolution rational
orthogonal wavelet as a constraint, which can effectively separate the details
of the endoscope and enhance the rendering performance. Extensive evaluations
on two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our
method EndoWave achieves state-of-the-art reconstruction quality and visual
accuracy compared to the baseline method.



---

## Explicit Memory through Online 3D Gaussian Splatting Improves  Class-Agnostic Video Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-27 | Anthony Opipari, Aravindhan K Krishnan, Shreekant Gayaka, Min Sun, Cheng-Hao Kuo, Arnie Sen, Odest Chadwicke Jenkins | cs.RO | [PDF](http://arxiv.org/pdf/2510.23521v1){: .btn .btn-green } |

**Abstract**: Remembering where object segments were predicted in the past is useful for
improving the accuracy and consistency of class-agnostic video segmentation
algorithms. Existing video segmentation algorithms typically use either no
object-level memory (e.g. FastSAM) or they use implicit memories in the form of
recurrent neural network features (e.g. SAM2). In this paper, we augment both
types of segmentation models using an explicit 3D memory and show that the
resulting models have more accurate and consistent predictions. For this, we
develop an online 3D Gaussian Splatting (3DGS) technique to store predicted
object-level segments generated throughout the duration of a video. Based on
this 3DGS representation, a set of fusion techniques are developed, named
FastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve
their respective foundation models' predictions. Ablation experiments are used
to validate the proposed techniques' design and hyperparameter settings.
Results from both real-world and simulated benchmarking experiments show that
models which use explicit 3D memories result in more accurate and consistent
predictions than those which use no memory or only implicit neural network
memories. Project Page: https://topipari.com/projects/FastSAM-Splat/

Comments:
- Accepted in IEEE Robotics and Automation Letters September 2025

---

## Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained  Feature Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-27 | Pranav Saxena | cs.CV | [PDF](http://arxiv.org/pdf/2510.22930v1){: .btn .btn-green } |

**Abstract**: Modeling open-vocabulary language fields in 3D is essential for intuitive
human-AI interaction and querying within physical environments.
State-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting
to efficiently construct these language fields, encoding features distilled
from high-dimensional models like CLIP. However, this efficiency is currently
offset by the requirement to train a scene-specific language autoencoder for
feature compression, introducing a costly, per-scene optimization bottleneck
that hinders deployment scalability. In this work, we introduce Gen-LangSplat,
that eliminates this requirement by replacing the scene-wise autoencoder with a
generalized autoencoder, pre-trained extensively on the large-scale ScanNet
dataset. This architectural shift enables the use of a fixed, compact latent
space for language features across any new scene without any scene-specific
training. By removing this dependency, our entire language field construction
process achieves a efficiency boost while delivering querying performance
comparable to, or exceeding, the original LangSplat method. To validate our
design choice, we perform a thorough ablation study empirically determining the
optimal latent embedding dimension and quantifying representational fidelity
using Mean Squared Error and cosine similarity between the original and
reprojected 512-dimensional CLIP embeddings. Our results demonstrate that
generalized embeddings can efficiently and accurately support open-vocabulary
querying in novel 3D scenes, paving the way for scalable, real-time interactive
3D AI applications.



---

## Scaling Up Occupancy-centric Driving Scene Generation: Dataset and  Method

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-27 | Bohan Li, Xin Jin, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2510.22973v1){: .btn .btn-green } |

**Abstract**: Driving scene generation is a critical domain for autonomous driving,
enabling downstream applications, including perception and planning evaluation.
Occupancy-centric methods have recently achieved state-of-the-art results by
offering consistent conditioning across frames and modalities; however, their
performance heavily depends on annotated occupancy data, which still remains
scarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic
occupancy dataset to date, constructed from the widely used Nuplan benchmark.
Its scale and diversity facilitate not only large-scale generative modeling but
also autonomous driving downstream applications. Based on this dataset, we
develop a unified framework that jointly synthesizes high-quality semantic
occupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates
a spatio-temporal disentangled architecture to support high-fidelity spatial
expansion and temporal forecasting of 4D dynamic occupancy. To bridge modal
gaps, we further propose two novel techniques: a Gaussian splatting-based
sparse point map rendering strategy that enhances multi-view video generation,
and a sensor-aware embedding strategy that explicitly models LiDAR sensor
properties for realistic multi-LiDAR simulation. Extensive experiments
demonstrate that our method achieves superior generation fidelity and
scalability compared to existing approaches, and validates its practical value
in downstream tasks. Repo:
https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2

Comments:
- https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2

---

## LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical  Explicit-Implicit Representation Collaboration Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-26 | Wenkai Zhu, Xu Li, Qimin Xu, Benwu Wang, Kun Wei, Yiming Peng, Zihang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2510.22669v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting SLAM has emerged as a widely used technique for
high-fidelity mapping in spatial intelligence. However, existing methods often
rely on a single representation scheme, which limits their performance in
large-scale dynamic outdoor scenes and leads to cumulative pose errors and
scale ambiguity. To address these challenges, we propose \textbf{LVD-GS}, a
novel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human
chain-of-thought process for information seeking, we introduce a hierarchical
collaborative representation module that facilitates mutual reinforcement for
mapping optimization, effectively mitigating scale drift and enhancing
reconstruction robustness. Furthermore, to effectively eliminate the influence
of dynamic objects, we propose a joint dynamic modeling module that generates
fine-grained dynamic masks by fusing open-world segmentation with implicit
residual constraints, guided by uncertainty estimates from DINO-Depth features.
Extensive evaluations on KITTI, nuScenes, and self-collected datasets
demonstrate that our approach achieves state-of-the-art performance compared to
existing methods.



---

## Edge Collaborative Gaussian Splatting with Integrated Rendering and  Communication

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-26 | Yujie Wan, Chenxuan Liu, Shuai Wang, Tong Zhang, James Jianqiao Yu, Kejiang Ye, Dusit Niyato, Chengzhong Xu | cs.IT | [PDF](http://arxiv.org/pdf/2510.22718v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting (GS) struggles with degraded rendering quality on low-cost
devices. To address this issue, we present edge collaborative GS (ECO-GS),
where each user can switch between a local small GS model to guarantee
timeliness and a remote large GS model to guarantee fidelity. However, deciding
how to engage the large GS model is nontrivial, due to the interdependency
between rendering requirements and resource conditions. To this end, we propose
integrated rendering and communication (IRAC), which jointly optimizes
collaboration status (i.e., deciding whether to engage large GS) and edge power
allocation (i.e., enabling remote rendering) under communication constraints
across different users by minimizing a newly-derived GS switching function.
Despite the nonconvexity of the problem, we propose an efficient penalty
majorization minimization (PMM) algorithm to obtain the critical point
solution. Furthermore, we develop an imitation learning optimization (ILO)
algorithm, which reduces the computational time by over 100x compared to PMM.
Experiments demonstrate the superiority of PMM and the real-time execution
capability of ILO.

Comments:
- 5 pages and 7 figures, submitted for possible publication

---

## RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and  Low-light Environment Resilience

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-26 | Huilin Yin, Zhaolin Yang, Linchuan Zhang, Gerhard Rigoll, Johannes Betz | cs.RO | [PDF](http://arxiv.org/pdf/2510.22600v1){: .btn .btn-green } |

**Abstract**: The reliability of Simultaneous Localization and Mapping (SLAM) is severely
constrained in environments where visual inputs suffer from noise and low
illumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM
frameworks achieve high-fidelity mapping under clean conditions, they remain
vulnerable to compounded degradations that degrade mapping and tracking
performance. A key observation underlying our work is that the original 3DGS
rendering pipeline inherently behaves as an implicit low-pass filter,
attenuating high-frequency noise but also risking over-smoothing. Building on
this insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for
noise and low-light resilience. The framework integrates three innovations: a
Structure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples
rendered appearance, depth, and edge cues; an adaptive tracking objective with
residual balancing regularization; and a Contrastive Language-Image Pretraining
(CLIP)-based enhancement module, selectively activated under compounded
degradations to restore semantic and structural fidelity. Comprehensive
experiments on Replica, TUM, and real-world sequences show that RoGER-SLAM
consistently improves trajectory accuracy and reconstruction quality compared
with other 3DGS-SLAM systems, especially under adverse imaging conditions.

Comments:
- 13 pages, 11 figures, under review

---

## DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose  Alignment Loss

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-26 | Jing Yang, Yufeng Yang | cs.CV | [PDF](http://arxiv.org/pdf/2510.22473v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 2D and 3D generative models have expanded the
capabilities of computer vision. However, generating high-quality 4D dynamic
content from a single static image remains a significant challenge. Traditional
methods have limitations in modeling temporal dependencies and accurately
capturing dynamic geometry changes, especially when considering variations in
camera perspective. To address this issue, we propose DynaPose4D, an innovative
solution that integrates 4D Gaussian Splatting (4DGS) techniques with
Category-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D
Gaussian Splatting to construct a 3D model from single images, then predicts
multi-view pose keypoints based on one-shot support from a chosen view,
leveraging supervisory signals to enhance motion consistency. Experimental
results show that DynaPose4D achieves excellent coherence, consistency, and
fluidity in dynamic motion generation. These findings not only validate the
efficacy of the DynaPose4D framework but also indicate its potential
applications in the domains of computer vision and animation production.



---

## Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting  Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-26 | Shashank N. Sridhara, Birendra Kathariya, Fangjun Pu, Peng Yin, Eduardo Pavez, Antonio Ortega | eess.IV | [PDF](http://arxiv.org/pdf/2510.22812v1){: .btn .btn-green } |

**Abstract**: We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D
Gaussian Splatting (3DGS) data. While 3DGS has recently become popular for
novel view synthesis, the size of trained models limits its deployment in
bandwidth-constrained applications such as volumetric media streaming. To
address this, we propose a learned hierarchical latent representation that
builds upon the principles of "overfitted" learned image compression (e.g.,
Cool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS
data have irregular spatial distributions of Gaussians (geometry) and consist
of multiple attributes (signals) defined on the irregular geometry. Our codec
is designed to account for these differences between images and 3DGS.
Specifically, we leverage the octree structure of the voxelized 3DGS geometry
to obtain a hierarchical multi-resolution representation. Our approach overfits
latents to each Gaussian attribute under a global rate constraint. These
latents are decoded independently through a lightweight decoder network. To
estimate the bitrate during training, we employ an autoregressive probability
model that leverages octree-derived contexts from the 3D point structure. The
multi-resolution latents, decoder, and autoregressive entropy coding networks
are jointly optimized for each Gaussian attribute. Experiments demonstrate that
the proposed RALHE compression framework achieves a rendering PSNR gain of up
to 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS
compression methods.

Comments:
- 10 Pages, 5 Figures

---

## I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media  Interactions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-25 | Shuhong Liu, Lin Gu, Ziteng Cui, Xuangeng Chu, Tatsuya Harada | cs.CV | [PDF](http://arxiv.org/pdf/2510.22161v1){: .btn .btn-green } |

**Abstract**: Participating in efforts to endow generative AI with the 3D physical world
perception, we propose I2-NeRF, a novel neural radiance field framework that
enhances isometric and isotropic metric perception under media degradation.
While existing NeRF models predominantly rely on object-centric sampling,
I2-NeRF introduces a reverse-stratified upsampling strategy to achieve
near-uniform sampling across 3D space, thereby preserving isometry. We further
present a general radiative formulation for media degradation that unifies
emission, absorption, and scattering into a particle model governed by the
Beer-Lambert attenuation law. By composing the direct and media-induced
in-scatter radiance, this formulation extends naturally to complex media
environments such as underwater, haze, and even low-light scenes. By treating
light propagation uniformly in both vertical and horizontal directions, I2-NeRF
enables isotropic metric perception and can even estimate medium properties
such as water depth. Experiments on real-world datasets demonstrate that our
method significantly improves both reconstruction fidelity and physical
plausibility compared to existing approaches.



---

## DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-25 | Yaokun Li, Lihe Ding, Xiao Chen, Guang Tan, Tianfan Xue | cs.CV | [PDF](http://arxiv.org/pdf/2510.22213v1){: .btn .btn-green } |

**Abstract**: Generating dynamic and interactive 3D objects, such as trees, has wide
applications in virtual reality, games, and world simulation. Nevertheless,
existing methods still face various challenges in generating realistic 4D
motion for complex real trees. In this paper, we propose DynamicTree, the first
framework that can generate long-term, interactive animation of 3D Gaussian
Splatting trees. Unlike prior optimization-based methods, our approach
generates dynamics in a fast feed-forward manner. The key success of our
approach is the use of a compact sparse voxel spectrum to represent the tree
movement. Given a 3D tree from Gaussian Splatting reconstruction, our pipeline
first generates mesh motion using the sparse voxel spectrum and then binds
Gaussians to deform the mesh. Additionally, the proposed sparse voxel spectrum
can also serve as a basis for fast modal analysis under external forces,
allowing real-time interactive responses. To train our model, we also introduce
4DTree, the first large-scale synthetic 4D tree dataset containing 8,786
animated tree meshes with semantic labels and 100-frame motion sequences.
Extensive experiments demonstrate that our method achieves realistic and
responsive tree animations, significantly outperforming existing approaches in
both visual quality and computational efficiency.

Comments:
- Project Page:
  https://dynamictree-dev.github.io/DynamicTree.github.io/

---

## Towards Physically Executable 3D Gaussian for Embodied Navigation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-24 | Bingchen Miao, Rong Wei, Zhiqi Ge, Xiaoquan sun, Shiqi Gao, Jingzhe Zhu, Renhan Wang, Siliang Tang, Jun Xiao, Rui Tang, Juncheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2510.21307v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic
real-time rendering capabilities, is regarded as an effective tool for
narrowing the sim-to-real gap. However, it lacks fine-grained semantics and
physical executability for Visual-Language Navigation (VLN). To address this,
we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments
for 3D Navigation), a new paradigm that upgrades 3DGS into an executable,
semantically and physically aligned environment. It comprises two components:
(1) Object-Centric Semantic Grounding, which adds object-level fine-grained
annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds
collision objects into 3DGS and constructs rich physical interfaces. We release
InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and
introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data.
Experiments show that 3DGS scene data is more difficult to converge, while
exhibiting strong generalizability, improving baseline performance by 31% on
the VLN-CE Unseen task. The data and code will be available soon.

Comments:
- Download link of InteriorGS:
  https://huggingface.co/datasets/spatialverse/InteriorGS

---

## COS3D: Collaborative Open-Vocabulary 3D Segmentation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-23 | Runsong Zhu, Ka-Hei Hui, Zhengzhe Liu, Qianyi Wu, Weiliang Tang, Shi Qiu, Pheng-Ann Heng, Chi-Wing Fu | cs.CV | [PDF](http://arxiv.org/pdf/2510.20238v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D segmentation is a fundamental yet challenging task,
requiring a mutual understanding of both segmentation and language. However,
existing Gaussian-splatting-based methods rely either on a single 3D language
field, leading to inferior segmentation, or on pre-computed class-agnostic
segmentations, suffering from error accumulation. To address these limitations,
we present COS3D, a new collaborative prompt-segmentation framework that
contributes to effectively integrating complementary language and segmentation
cues throughout its entire pipeline. We first introduce the new concept of
collaborative field, comprising an instance field and a language field, as the
cornerstone for collaboration. During training, to effectively construct the
collaborative field, our key idea is to capture the intrinsic relationship
between the instance field and language field, through a novel
instance-to-language feature mapping and designing an efficient two-stage
training strategy. During inference, to bridge distinct characteristics of the
two fields, we further design an adaptive language-to-instance prompt
refinement, promoting high-quality prompt-segmentation inference. Extensive
experiments not only demonstrate COS3D's leading performance over existing
methods on two widely-used benchmarks but also show its high potential to
various applications,~\ie, novel image-based 3D segmentation, hierarchical
segmentation, and robotics. The code is publicly available at
\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.

Comments:
- NeurIPS 2025. The code is publicly available at
  \href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}

---

## From Far and Near: Perceptual Evaluation of Crowd Representations Across  Levels of Detail

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-23 | Xiaohan Sun, Carol O'Sullivan | cs.CV | [PDF](http://arxiv.org/pdf/2510.20558v1){: .btn .btn-green } |

**Abstract**: In this paper, we investigate how users perceive the visual quality of crowd
character representations at different levels of detail (LoD) and viewing
distances. Each representation: geometric meshes, image-based impostors, Neural
Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between
visual fidelity and computational performance. Our qualitative and quantitative
results provide insights to guide the design of perceptually optimized LoD
strategies for crowd rendering.



---

## GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic  Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-23 | Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang | cs.RO | [PDF](http://arxiv.org/pdf/2510.20813v1){: .btn .btn-green } |

**Abstract**: This paper presents GSWorld, a robust, photo-realistic simulator for robotics
manipulation that combines 3D Gaussian Splatting with physics engines. Our
framework advocates "closing the loop" of developing manipulation policies with
reproducible evaluation of policies learned from real-robot data and sim2real
policy training without using real robots. To enable photo-realistic rendering
of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian
Scene Description File), that infuses Gaussian-on-Mesh representation with
robot URDF and other objects. With a streamlined reconstruction pipeline, we
curate a database of GSDF that contains 3 robot embodiments for single-arm and
bimanual manipulation, as well as more than 40 objects. Combining GSDF with
physics engines, we demonstrate several immediate interesting applications: (1)
learning zero-shot sim2real pixel-to-action manipulation policy with
photo-realistic rendering, (2) automated high-quality DAgger data collection
for adapting policies to deployment environments, (3) reproducible benchmarking
of real-robot manipulation policies in simulation, (4) simulation data
collection by virtual teleoperation, and (5) zero-shot sim2real visual
reinforcement learning. Website: https://3dgsworld.github.io/.



---

## Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous  Parking

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-23 | Zixuan Wu, Hengyuan Zhang, Ting-Hsuan Chen, Yuliang Guo, David Paz, Xinyu Huang, Liu Ren | cs.RO | [PDF](http://arxiv.org/pdf/2510.20335v1){: .btn .btn-green } |

**Abstract**: Parking is a critical pillar of driving safety. While recent end-to-end (E2E)
approaches have achieved promising in-domain results, robustness under domain
shifts (e.g., weather and lighting changes) remains a key challenge. Rather
than relying on additional data, in this paper, we propose Dino-Diffusion
Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates
visual foundation models with diffusion-based planning to enable generalized
perception and robust motion planning under distribution shifts. We train our
pipeline in CARLA at regular setting and transfer it to more adversarial
settings in a zero-shot fashion. Our model consistently achieves a parking
success rate above 90% across all tested out-of-distribution (OOD) scenarios,
with ablation studies confirming that both the network architecture and
algorithmic design significantly enhance cross-domain performance over existing
baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment
reconstructed from a real-world parking lot demonstrates promising sim-to-real
transfer.

Comments:
- Code is at
  https://github.com/ChampagneAndfragrance/Dino_Diffusion_Parking_Official

---

## VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view  Driving Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-22 | Junhong Lin, Kangli Wang, Shunzhou Wang, Songlin Fan, Ge Li, Wei Gao | cs.CV | [PDF](http://arxiv.org/pdf/2510.19578v1){: .btn .btn-green } |

**Abstract**: Feed-forward surround-view autonomous driving scene reconstruction offers
fast, generalizable inference ability, which faces the core challenge of
ensuring generalization while elevating novel view quality. Due to the
surround-view with minimal overlap regions, existing methods typically fail to
ensure geometric consistency and reconstruction quality for novel views. To
tackle this tension, we claim that geometric information must be learned
explicitly, and the resulting features should be leveraged to guide the
elevating of semantic quality in novel views. In this paper, we introduce
\textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end
learning framework designed to address this challenge. To achieve generalizable
geometric estimation, we design a lightweight variant of the VGGT architecture
to efficiently distill its geometric priors from the pre-trained VGGT to the
geometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale
geometry tokens to predict Gaussian parameters for novel view rendering, which
shares the same patch backbone as the geometry branch. Finally, we integrate
multi-scale features from both geometry and Gaussian head branches to jointly
supervise a semantic refinement model, optimizing rendering quality through
feature-consistent learning. Experiments on nuScenes demonstrate that our
approach significantly outperforms state-of-the-art methods in both objective
metrics and subjective quality under various settings, which validates VGD's
scalability and high-fidelity surround-view reconstruction.

Comments:
- 10 pages, 7 figures

---

## GRASPLAT: Enabling dexterous grasping through novel view synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-22 | Matteo Bortolon, Nuno Ferreira Duarte, Plinio Moreno, Fabio Poiesi, José Santos-Victor, Alessio Del Bue | cs.RO | [PDF](http://arxiv.org/pdf/2510.19200v1){: .btn .btn-green } |

**Abstract**: Achieving dexterous robotic grasping with multi-fingered hands remains a
significant challenge. While existing methods rely on complete 3D scans to
predict grasp poses, these approaches face limitations due to the difficulty of
acquiring high-quality 3D data in real-world scenarios. In this paper, we
introduce GRASPLAT, a novel grasping framework that leverages consistent 3D
information while being trained solely on RGB images. Our key insight is that
by synthesizing physically plausible images of a hand grasping an object, we
can regress the corresponding hand joints for a successful grasp. To achieve
this, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of
real hand-object interactions, enabling end-to-end training with RGB data.
Unlike prior methods, our approach incorporates a photometric loss that refines
grasp predictions by minimizing discrepancies between rendered and real images.
We conduct extensive experiments on both synthetic and real-world grasping
datasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9%
over existing image-based methods. Project page:
https://mbortolon97.github.io/grasplat/

Comments:
- Accepted IROS 2025

---

## Extreme Views: 3DGS Filter for Novel View Synthesis from  Out-of-Distribution Camera Poses

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-22 | Damian Bowness, Charalambos Poullis | cs.CV | [PDF](http://arxiv.org/pdf/2510.20027v1){: .btn .btn-green } |

**Abstract**: When viewing a 3D Gaussian Splatting (3DGS) model from camera positions
significantly outside the training data distribution, substantial visual noise
commonly occurs. These artifacts result from the lack of training data in these
extrapolated regions, leading to uncertain density, color, and geometry
predictions from the model.
  To address this issue, we propose a novel real-time render-aware filtering
method. Our approach leverages sensitivity scores derived from intermediate
gradients, explicitly targeting instabilities caused by anisotropic
orientations rather than isotropic variance. This filtering method directly
addresses the core issue of generative uncertainty, allowing 3D reconstruction
systems to maintain high visual fidelity even when users freely navigate
outside the original training viewpoints.
  Experimental evaluation demonstrates that our method substantially improves
visual quality, realism, and consistency compared to existing Neural Radiance
Field (NeRF)-based approaches such as BayesRays. Critically, our filter
seamlessly integrates into existing 3DGS rendering pipelines in real-time,
unlike methods that require extensive post-hoc retraining or fine-tuning.
  Code and results at https://damian-bowness.github.io/EV3DGS



---

## MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-22 | In-Hwan Jin, Hyeongju Mun, Joonsoo Kim, Kugjin Yun, Kyeongbo Kong | cs.CV | [PDF](http://arxiv.org/pdf/2510.19210v1){: .btn .btn-green } |

**Abstract**: Recent advances in dynamic scene reconstruction have significantly benefited
from 3D Gaussian Splatting, yet existing methods show inconsistent performance
across diverse scenes, indicating no single approach effectively handles all
dynamic challenges. To overcome these limitations, we propose Mixture of
Experts for Dynamic Gaussian Splatting (MoE-GS), a unified framework
integrating multiple specialized experts via a novel Volume-aware Pixel Router.
Our router adaptively blends expert outputs by projecting volumetric
Gaussian-level weights into pixel space through differentiable weight
splatting, ensuring spatially and temporally coherent results. Although MoE-GS
improves rendering quality, the increased model capacity and reduced FPS are
inherent to the MoE architecture. To mitigate this, we explore two
complementary directions: (1) single-pass multi-expert rendering and gate-aware
Gaussian pruning, which improve efficiency within the MoE framework, and (2) a
distillation strategy that transfers MoE performance to individual experts,
enabling lightweight deployment without architectural changes. To the best of
our knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts
techniques into dynamic Gaussian splatting. Extensive experiments on the N3V
and Technicolor datasets demonstrate that MoE-GS consistently outperforms
state-of-the-art methods with improved efficiency. Video demonstrations are
available at https://anonymous.4open.science/w/MoE-GS-68BA/.



---

## AegisRF: Adversarial Perturbations Guided with Sensitivity for  Protecting Intellectual Property of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-22 | Woo Jae Kim, Kyu Beom Han, Yoonki Cho, Youngju Na, Junsik Jung, Sooel Son, Sung-eui Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2510.19371v1){: .btn .btn-green } |

**Abstract**: As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D
scene representation and novel view synthesis, protecting their intellectual
property (IP) from unauthorized use is becoming increasingly crucial. In this
work, we aim to protect the IP of NeRFs by injecting adversarial perturbations
that disrupt their unauthorized applications. However, perturbing the 3D
geometry of NeRFs can easily deform the underlying scene structure and thus
substantially degrade the rendering quality, which has led existing attempts to
avoid geometric perturbations or restrict them to explicit spaces like meshes.
To overcome this limitation, we introduce a learnable sensitivity to quantify
the spatially varying impact of geometric perturbations on rendering quality.
Building upon this, we propose AegisRF, a novel framework that consists of a
Perturbation Field, which injects adversarial perturbations into the
pre-rendering outputs (color and volume density) of NeRF models to fool an
unauthorized downstream target model, and a Sensitivity Field, which learns the
sensitivity to adaptively constrain geometric perturbations, preserving
rendering quality while disrupting unauthorized use. Our experimental
evaluations demonstrate the generalized applicability of AegisRF across diverse
downstream tasks and modalities, including multi-view image classification and
voxel-based 3D localization, while maintaining high visual fidelity. Codes are
available at https://github.com/wkim97/AegisRF.

Comments:
- BMVC 2025

---

## Advances in 4D Representation: Geometry, Motion, and Interaction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-22 | Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.19255v1){: .btn .btn-green } |

**Abstract**: We present a survey on 4D generation and reconstruction, a fast-evolving
subfield of computer graphics whose developments have been propelled by recent
advances in neural fields, geometric and motion deep learning, as well 3D
generative artificial intelligence (GenAI). While our survey is not the first
of its kind, we build our coverage of the domain from a unique and distinctive
perspective of 4D representations\/}, to model 3D geometry evolving over time
while exhibiting motion and interaction. Specifically, instead of offering an
exhaustive enumeration of many works, we take a more selective approach by
focusing on representative works to highlight both the desirable properties and
ensuing challenges of each representation under different computation,
application, and data scenarios. The main take-away message we aim to convey to
the readers is on how to select and then customize the appropriate 4D
representations for their tasks. Organizationally, we separate the 4D
representations based on three key pillars: geometry, motion, and interaction.
Our discourse will not only encompass the most popular representations of
today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),
but also bring attention to relatively under-explored representations in the 4D
context, such as structured models and long-range motions. Throughout our
survey, we will reprise the role of large language models (LLMs) and video
foundational models (VFMs) in a variety of 4D applications, while steering our
discussion towards their current limitations and how they can be addressed. We
also provide a dedicated coverage on what 4D datasets are currently available,
as well as what is lacking, in driving the subfield forward. Project
page:https://mingrui-zhao.github.io/4DRep-GMI/

Comments:
- 21 pages. Project Page: https://mingrui-zhao.github.io/4DRep-GMI/

---

## Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from  Alternating-exposure Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-21 | Jinfeng Liu, Lingtong Kong, Mi Zhou, Jinwen Chen, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.18489v1){: .btn .btn-green } |

**Abstract**: We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D
high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR)
videos captured with alternating exposures. To tackle such a challenging
problem, we present a unified framework with two-stage optimization approach
based on Gaussian Splatting. The first stage learns a video HDR Gaussian
representation in orthographic camera coordinate space, eliminating the need
for camera poses and enabling robust initial HDR video reconstruction. The
second stage transforms video Gaussians into world space and jointly refines
the world Gaussians with camera poses. Furthermore, we propose a temporal
luminance regularization strategy to enhance the temporal consistency of the
HDR appearance. Since our task has not been studied before, we construct a new
evaluation benchmark using publicly available datasets for HDR video
reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR
significantly outperforms alternative solutions adapted from state-of-the-art
methods in both rendering quality and speed.

Comments:
- Project page is available at
  https://liujf1226.github.io/Mono4DGS-HDR/

---

## OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with  Context-aware Cross-view Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-21 | Tianyu Huang, Runnan Chen, Dongting Hu, Fengming Huang, Mingming Gong, Tongliang Liu | cs.CV | [PDF](http://arxiv.org/pdf/2510.18253v1){: .btn .btn-green } |

**Abstract**: Understanding 3D scenes is pivotal for autonomous driving, robotics, and
augmented reality. Recent semantic Gaussian Splatting approaches leverage
large-scale 2D vision models to project 2D semantic features onto 3D scenes.
However, they suffer from two major limitations: (1) insufficient contextual
cues for individual masks during preprocessing and (2) inconsistencies and
missing details when fusing multi-view features from these 2D models. In this
paper, we introduce \textbf{OpenInsGaussian}, an \textbf{Open}-vocabulary
\textbf{Ins}tance \textbf{Gaussian} segmentation framework with Context-aware
Cross-view Fusion. Our method consists of two modules: Context-Aware Feature
Extraction, which augments each mask with rich semantic context, and
Attention-Driven Feature Aggregation, which selectively fuses multi-view
features to mitigate alignment errors and incompleteness. Through extensive
experiments on benchmark datasets, OpenInsGaussian achieves state-of-the-art
results in open-vocabulary 3D Gaussian segmentation, outperforming existing
baselines by a large margin. These findings underscore the robustness and
generality of our proposed approach, marking a significant step forward in 3D
scene understanding and its practical deployment across diverse real-world
scenarios.



---

## Moving Light Adaptive Colonoscopy Reconstruction via  Illumination-Attenuation-Aware 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-21 | Hao Wang, Ying Zhou, Haoyu Zhao, Rui Wang, Qiang Hu, Xing Zhang, Qiang Li, Zhiwei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2510.18739v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time
view synthesis in colonoscopy, enabling critical applications such as virtual
colonoscopy and lesion tracking. However, the vanilla 3DGS assumes static
illumination and that observed appearance depends solely on viewing angle,
which causes incompatibility with the photometric variations in colonoscopic
scenes induced by dynamic light source/camera. This mismatch forces most 3DGS
methods to introduce structure-violating vaporous Gaussian blobs between the
camera and tissues to compensate for illumination attenuation, ultimately
degrading the quality of 3D reconstructions. Previous works only consider the
illumination attenuation caused by light distance, ignoring the physical
characters of light source and camera. In this paper, we propose ColIAGS, an
improved 3DGS framework tailored for colonoscopy. To mimic realistic appearance
under varying illumination, we introduce an Improved Appearance Modeling with
two types of illumination attenuation factors, which enables Gaussians to adapt
to photometric variations while preserving geometry accuracy. To ensure the
geometry approximation condition of appearance modeling, we propose an Improved
Geometry Modeling using high-dimensional view embedding to enhance Gaussian
geometry attribute prediction. Furthermore, another cosine embedding input is
leveraged to generate illumination attenuation solutions in an implicit manner.
Comprehensive experimental results on standard benchmarks demonstrate that our
proposed ColIAGS achieves the dual capabilities of novel view synthesis and
accurate geometric reconstruction. It notably outperforms other
state-of-the-art methods by achieving superior rendering fidelity while
significantly reducing Depth MSE. Code will be available.



---

## Re-Activating Frozen Primitives for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-21 | Yuxin Cheng, Binxiao Huang, Wenyong Zhou, Taiqiang Wu, Zhengwu Liu, Graziano Chesi, Ngai Wong | cs.CV | [PDF](http://arxiv.org/pdf/2510.19653v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) achieves real-time photorealistic novel view
synthesis, yet struggles with complex scenes due to over-reconstruction
artifacts, manifesting as local blurring and needle-shape distortions. While
recent approaches attribute these issues to insufficient splitting of
large-scale Gaussians, we identify two fundamental limitations: gradient
magnitude dilution during densification and the primitive frozen phenomenon,
where essential Gaussian densification is inhibited in complex regions while
suboptimally scaled Gaussians become trapped in local optima. To address these
challenges, we introduce ReAct-GS, a method founded on the principle of
re-activation. Our approach features: (1) an importance-aware densification
criterion incorporating $\alpha$-blending weights from multiple viewpoints to
re-activate stalled primitive growth in complex regions, and (2) a
re-activation mechanism that revitalizes frozen primitives through adaptive
parameter perturbations. Comprehensive experiments across diverse real-world
datasets demonstrate that ReAct-GS effectively eliminates over-reconstruction
artifacts and achieves state-of-the-art performance on standard novel view
synthesis metrics while preserving intricate geometric details. Additionally,
our re-activation mechanism yields consistent improvements when integrated with
other 3D-GS variants such as Pixel-GS, demonstrating its broad applicability.



---

## From Volume Rendering to 3D Gaussian Splatting: Theory and Applications

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-20 | Vitor Pereira Matias, Daniel Perazzo, Vinicius Silva, Alberto Raposo, Luiz Velho, Afonso Paiva, Tiago Novello | cs.CV | [PDF](http://arxiv.org/pdf/2510.18101v1){: .btn .btn-green } |

**Abstract**: The problem of 3D reconstruction from posed images is undergoing a
fundamental transformation, driven by continuous advances in 3D Gaussian
Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians,
3DGS enables efficient rasterization through volumetric splatting, offering
thus a seamless integration with common graphics pipelines. Despite its
real-time rendering capabilities for novel view synthesis, 3DGS suffers from a
high memory footprint, the tendency to bake lighting effects directly into its
representation, and limited support for secondary-ray effects. This tutorial
provides a concise yet comprehensive overview of the 3DGS pipeline, starting
from its splatting formulation and then exploring the main efforts in
addressing its limitations. Finally, we survey a range of applications that
leverage 3DGS for surface reconstruction, avatar modeling, animation, and
content generation-highlighting its efficient rendering and suitability for
feed-forward pipelines.

Comments:
- Accepted at the Conference on Graphics, Patterns and Images
  (SIBGRAPI), math focused, 5 equations, 5 Figure, 5 pages of text and 1 of
  bibligraphy

---

## GSPlane: Concise and Accurate Planar Reconstruction via Structured  Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-20 | Ruitong Gan, Junran Peng, Yang Liu, Chuanchen Luo, Qing Li, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.17095v1){: .btn .btn-green } |

**Abstract**: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.



---

## Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop  Conditions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-20 | Zhiqiang Teng, Beibei Lin, Tingting Chen, Zifeng Yuan, Xuanyi Li, Xuanyu Zhang, Shunli Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.17719v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.



---

## Initialize to Generalize: A Stronger Initialization Pipeline for  Sparse-View 3DGS

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-20 | Feng Zhou, Wenkai Guo, Pu Cao, Zhicheng Zhang, Jianqin Yin | cs.CV | [PDF](http://arxiv.org/pdf/2510.17479v1){: .btn .btn-green } |

**Abstract**: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

Comments:
- A preprint paper

---

## Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant  Structures with Gaussian Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-20 | Simeon Adebola, Chung Min Kim, Justin Kerr, Shuangyu Xie, Prithvi Akella, Jose Luis Susa Rincon, Eugen Solowjow, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2510.17783v1){: .btn .btn-green } |

**Abstract**: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

Comments:
- 2025 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2025)

---

## HouseTour: A Virtual Real Estate A(I)gent

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-20 | Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni | cs.CV | [PDF](http://arxiv.org/pdf/2510.18054v1){: .btn .btn-green } |

**Abstract**: We introduce HouseTour, a method for spatially-aware 3D camera trajectory and
natural language summary generation from a collection of images depicting an
existing 3D space. Unlike existing vision-language models (VLMs), which
struggle with geometric reasoning, our approach generates smooth video
trajectories via a diffusion process constrained by known camera poses and
integrates this information into the VLM for 3D-grounded descriptions. We
synthesize the final video using 3D Gaussian splatting to render novel views
along the trajectory. To support this task, we present the HouseTour dataset,
which includes over 1,200 house-tour videos with camera poses, 3D
reconstructions, and real estate descriptions. Experiments demonstrate that
incorporating 3D camera trajectories into the text generation process improves
performance over methods handling each task independently. We evaluate both
individual and end-to-end performance, introducing a new joint metric. Our work
enables automated, professional-quality video creation for real estate and
touristic applications without requiring specialized expertise or equipment.

Comments:
- Published on ICCV 2025

---

## GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-19 | Junbo Li, Weimin Yuan, Yinuo Wang, Yue Zeng, Shihao Shu, Cai Meng, Xiangzhi Bai | cs.CV | [PDF](http://arxiv.org/pdf/2510.16777v1){: .btn .btn-green } |

**Abstract**: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.



---

## 2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-19 | Haofan Ren, Qingsong Yan, Ming Lu, Rongfeng Lu, Zunjie Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2510.16837v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.



---

## HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D  Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-18 | Haocheng Tang, Ruoke Yan, Xinhui Yin, Qi Zhang, Xinfeng Zhang, Siwei Ma, Wen Gao, Chuanmin Jia | cs.CV | [PDF](http://arxiv.org/pdf/2510.16463v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

Comments:
- ACM International Conference on Multimedia 2025

---

## REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation  and Editing on Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-18 | Changyue Shi, Minghao Chen, Yiping Mao, Chuxiao Yang, Xinyuan Hu, Jiajun Ding, Zhou Yu | cs.CV | [PDF](http://arxiv.org/pdf/2510.16410v1){: .btn .btn-green } |

**Abstract**: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.



---

## PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-17 | Ting-Yu Yen, Yu-Sheng Chiu, Shih-Hsuan Hung, Peter Wonka, Hung-Kuo Chu | cs.CV | [PDF](http://arxiv.org/pdf/2510.15386v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality,
real-time novel-view synthesis from multi-view images. However, most existing
methods assume the object is captured in a single, static pose, resulting in
incomplete reconstructions that miss occluded or self-occluded regions. We
introduce PFGS, a pose-aware 3DGS framework that addresses the practical
challenge of reconstructing complete objects from multi-pose image captures.
Given images of an object in one main pose and several auxiliary poses, PFGS
iteratively fuses each auxiliary set into a unified 3DGS representation of the
main pose. Our pose-aware fusion strategy combines global and local
registration to merge views effectively and refine the 3DGS model. While recent
advances in 3D foundation models have improved registration robustness and
efficiency, they remain limited by high memory demands and suboptimal accuracy.
PFGS overcomes these challenges by incorporating them more intelligently into
the registration process: it leverages background features for per-pose camera
pose estimation and employs foundation models for cross-pose registration. This
design captures the best of both approaches while resolving background
inconsistency issues. Experimental results demonstrate that PFGS consistently
outperforms strong baselines in both qualitative and quantitative evaluations,
producing more complete reconstructions and higher-fidelity 3DGS models.



---

## Proactive Scene Decomposition and Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-17 | Baicheng Li, Zike Yan, Dong Wu, Hongbin Zha | cs.CV | [PDF](http://arxiv.org/pdf/2510.16272v1){: .btn .btn-green } |

**Abstract**: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.



---

## GaussGym: An open-source real-to-sim framework for learning locomotion  from pixels

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-17 | Alejandro Escontrela, Justin Kerr, Arthur Allshire, Jonas Frey, Rocky Duan, Carmelo Sferrazza, Pieter Abbeel | cs.RO | [PDF](http://arxiv.org/pdf/2510.15352v1){: .btn .btn-green } |

**Abstract**: We present a novel approach for photorealistic robot simulation that
integrates 3D Gaussian Splatting as a drop-in renderer within vectorized
physics simulators such as IsaacGym. This enables unprecedented speed --
exceeding 100,000 steps per second on consumer GPUs -- while maintaining high
visual fidelity, which we showcase across diverse tasks. We additionally
demonstrate its applicability in a sim-to-real robotics setting. Beyond
depth-based sensing, our results highlight how rich visual semantics improve
navigation and decision-making, such as avoiding undesirable regions. We
further showcase the ease of incorporating thousands of environments from
iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs
from generative video models like Veo, enabling rapid creation of realistic
training worlds. This work bridges high-throughput simulation and high-fidelity
perception, advancing scalable and generalizable robot learning. All code and
data will be open-sourced for the community to build upon. Videos, code, and
data available at https://escontrela.me/gauss_gym/.



---

## Leveraging Learned Image Prior for 3D Gaussian Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Seungjoo Shin, Jaesik Park, Sunghyun Cho | cs.CV | [PDF](http://arxiv.org/pdf/2510.14705v1){: .btn .btn-green } |

**Abstract**: Compression techniques for 3D Gaussian Splatting (3DGS) have recently
achieved considerable success in minimizing storage overhead for 3D Gaussians
while preserving high rendering quality. Despite the impressive storage
reduction, the lack of learned priors restricts further advances in the
rate-distortion trade-off for 3DGS compression tasks. To address this, we
introduce a novel 3DGS compression framework that leverages the powerful
representational capacity of learned image priors to recover
compression-induced quality degradation. Built upon initially compressed
Gaussians, our restoration network effectively models the compression artifacts
in the image space between degraded and original Gaussians. To enhance the
rate-distortion performance, we provide coarse rendering residuals into the
restoration network as side information. By leveraging the supervision of
restored images, the compressed Gaussians are refined, resulting in a highly
compact representation with enhanced rendering performance. Our framework is
designed to be compatible with existing Gaussian compression methods, making it
broadly applicable across different baselines. Extensive experiments validate
the effectiveness of our framework, demonstrating superior rate-distortion
performance and outperforming the rendering quality of state-of-the-art 3DGS
compression methods while requiring substantially less storage.

Comments:
- Accepted to ICCV 2025 Workshop on ECLR

---

## Virtually Being: Customizing Camera-Controllable Video Diffusion Models  with Multi-View Performance Captures

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Levent Taşel, Yiwei Zhao, Ryan Burgert, Mingming He, Oliver Hermann, Oliver Pilarski, Rahul Garg, Paul Debevec, Ning Yu | cs.CV | [PDF](http://arxiv.org/pdf/2510.14179v1){: .btn .btn-green } |

**Abstract**: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

Comments:
- Accepted to SIGGRAPH Asia 2025

---

## GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and  Geometric Filtering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang | cs.CV | [PDF](http://arxiv.org/pdf/2510.14270v1){: .btn .btn-green } |

**Abstract**: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.



---

## BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian  Splatting Training on GPU

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Junyi Wu, Jiaming Xu, Jinhao Li, Yongkang Zhou, Jiayi Pan, Xingyang Li, Guohao Dai | cs.CV | [PDF](http://arxiv.org/pdf/2510.14564v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

Comments:
- Accepted by ASP-DAC 2026

---

## SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from  Unposed Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Jiaxin Guo, Tongfan Guan, Wenzhen Dong, Wenzhao Zheng, Wenting Wang, Yue Wang, Yeung Yam, Yun-Hui Liu | cs.CV | [PDF](http://arxiv.org/pdf/2510.15072v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable,
on-the-fly reconstruction of sequential input views. However, existing methods
often predict per-pixel Gaussians and combine Gaussians from all views as the
scene representation, leading to substantial redundancies and geometric
inconsistencies in long-duration video sequences. To address this, we propose
SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction.
To our best knowledge, SaLon3R is the first online generalizable GS method
capable of reconstructing over 50 views in over 10 FPS, with 50% to 90%
redundancy removal. Our method introduces compact anchor primitives to
eliminate redundancy through differentiable saliency-aware Gaussian
quantization, coupled with a 3D Point Transformer that refines anchor
attributes and saliency to resolve cross-frame geometric and photometric
inconsistencies. Specifically, we first leverage a 3D reconstruction backbone
to predict dense per-pixel Gaussians and a saliency map encoding regional
geometric complexity. Redundant Gaussians are compressed into compact anchors
by prioritizing high-complexity regions. The 3D Point Transformer then learns
spatial structural priors in 3D space from training data to refine anchor
attributes and saliency, enabling regionally adaptive Gaussian decoding for
geometric fidelity. Without known camera parameters or test-time optimization,
our approach effectively resolves artifacts and prunes the redundant 3DGS in a
single feed-forward pass. Experiments on multiple datasets demonstrate our
state-of-the-art performance on both novel view synthesis and depth estimation,
demonstrating superior efficiency, robustness, and generalization ability for
long-term generalizable 3D reconstruction. Project Page:
https://wrld.github.io/SaLon3R/.



---

## Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Siddharth Tourani, Jayaram Reddy, Akash Kumbar, Satyajit Tourani, Nishant Goyal, Madhava Krishna, N. Dinesh Reddy, Muhammad Haris Khan | cs.CV | [PDF](http://arxiv.org/pdf/2510.13381v1){: .btn .btn-green } |

**Abstract**: Dynamic scene rendering and reconstruction play a crucial role in computer
vision and augmented reality. Recent methods based on 3D Gaussian Splatting
(3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban
scenes they require both camera and LiDAR data, ground-truth 3D segmentations
and motion data in the form of tracklets or pre-defined object templates such
as SMPL. In this work, we explore whether a combination of 2D object agnostic
priors in the form of depth and point tracking coupled with a signed distance
function (SDF) representation for dynamic objects can be used to relax some of
these requirements. We present a novel approach that integrates Signed Distance
Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust
object representation by harnessing the strengths of both methods. Our unified
optimization framework enhances the geometric accuracy of 3D Gaussian splatting
and improves deformation modeling within the SDF, resulting in a more adaptable
and precise representation. We demonstrate that our method achieves
state-of-the-art performance in rendering metrics even without LiDAR data on
urban scenes. When incorporating LiDAR, our approach improved further in
reconstructing and generating novel views across diverse object categories,
without ground-truth 3D motion annotation. Additionally, our method enables
various scene editing tasks, including scene decomposition, and scene
composition.

Comments:
- Accepted at ICCV-2025, project page: https://dynamic-ugsdf.github.io/

---

## InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive  3D Object Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Jungmin Lee, Seonghyuk Hong, Juyong Lee, Jaeyoon Lee, Jongwon Choi | cs.CV | [PDF](http://arxiv.org/pdf/2510.17864v1){: .btn .btn-green } |

**Abstract**: We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that
bridges the gap between high-fidelity RGB surface details and subsurface X-ray
structures. The fusion of RGB and X-ray imaging is invaluable in fields such as
medical diagnostics, cultural heritage restoration, and manufacturing. We
collect new paired RGB and X-ray data, perform hierarchical fitting to align
RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to
ensure consistent internal structures. InsideOut effectively addresses the
challenges posed by disparate data representations between the two modalities
and limited paired datasets. This approach significantly extends the
applicability of 3DGS, enhancing visualization, simulation, and non-destructive
testing capabilities across various domains.

Comments:
- Published at ICCV 2025

---

## Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from  Unstructured Phone Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Emanuel Garbin, Guy Adam, Oded Krams, Zohar Barzelay, Eran Guendelman, Michael Schwarz, Matteo Presutto, Moran Vatelmacher, Yigal Shenkman, Eli Peker, Itai Druker, Uri Patish, Yoav Blum, Max Bluvstein, Junxuan Li, Rawal Khirodkar, Shunsuke Saito | cs.CV | [PDF](http://arxiv.org/pdf/2510.14081v2){: .btn .btn-green } |

**Abstract**: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.



---

## Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Naruya Kondo, Yuto Asano, Yoichi Ochiai | cs.CG | [PDF](http://arxiv.org/pdf/2510.13978v1){: .btn .btn-green } |

**Abstract**: We present Instant Skinned Gaussian Avatars, a real-time and cross-platform
3D avatar system. Many approaches have been proposed to animate Gaussian
Splatting, but they often require camera arrays, long preprocessing times, or
high-end GPUs. Some methods attempt to convert Gaussian Splatting into
mesh-based representations, achieving lightweight performance but sacrificing
visual fidelity. In contrast, our system efficiently animates Gaussian
Splatting by leveraging parallel splat-wise processing to dynamically follow
the underlying skinned mesh in real time while preserving high visual fidelity.
From smartphone-based 3D scanning to on-device preprocessing, the entire
process takes just around five minutes, with the avatar generation step itself
completed in only about 30 seconds. Our system enables users to instantly
transform their real-world appearance into a 3D avatar, making it ideal for
seamless integration with social media and metaverse applications. Website:
https://sites.google.com/view/gaussian-vrm

Comments:
- Accepted to SUI 2025 Demo Track

---

## STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client  Selection and Power Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Zhen Li, Xibin Jin, Guoliang Li, Shuai Wang, Miaowen Wen, Huseyin Arslan, Derrick Wing Kwan Ng, Chengzhong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.13186v1){: .btn .btn-green } |

**Abstract**: Edge Gaussian splatting (EGS), which aggregates data from distributed clients
and trains a global GS model at the edge server, is an emerging paradigm for
scene reconstruction. Unlike traditional edge resource management methods that
emphasize communication throughput or general-purpose learning performance, EGS
explicitly aims to maximize the GS qualities, rendering existing approaches
inapplicable. To address this problem, this paper formulates a novel
GS-oriented objective function that distinguishes the heterogeneous view
contributions of different clients. However, evaluating this function in turn
requires clients' images, leading to a causality dilemma. To this end, this
paper further proposes a sample-then-transmit EGS (or STT-GS for short)
strategy, which first samples a subset of images as pilot data from each client
for loss prediction. Based on the first-stage evaluation, communication
resources are then prioritized towards more valuable clients. To achieve
efficient sampling, a feature-domain clustering (FDC) scheme is proposed to
select the most representative data and pilot transmission time minimization
(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint
client selection and power control (JCSPC) framework to maximize the
GS-oriented function under communication resource constraints. Despite the
nonconvexity of the problem, we propose a low-complexity efficient solution
based on the penalty alternating majorization minimization (PAMM) algorithm.
Experiments unveil that the proposed scheme significantly outperforms existing
benchmarks on real-world datasets. It is found that the GS-oriented objective
can be accurately predicted with low sampling ratios (e.g.,10%), and our method
achieves an excellent tradeoff between view contributions and communication
costs.



---

## VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a  Video Generator


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Truong, Federico Tombari, Konrad Schindler | cs.CV | [PDF](http://arxiv.org/pdf/2510.13454v1){: .btn .btn-green } |

**Abstract**: The rapid progress of large, pretrained models for both visual content
generation and 3D reconstruction opens up new possibilities for text-to-3D
generation. Intuitively, one could obtain a formidable 3D scene generator if
one were able to combine the power of a modern latent text-to-video model as
"generator" with the geometric abilities of a recent (feedforward) 3D
reconstruction system as "decoder". We introduce VIST3A, a general framework
that does just that, addressing two main challenges. First, the two components
must be joined in a way that preserves the rich knowledge encoded in their
weights. We revisit model stitching, i.e., we identify the layer in the 3D
decoder that best matches the latent representation produced by the
text-to-video generator and stitch the two parts together. That operation
requires only a small dataset and no labels. Second, the text-to-video
generator must be aligned with the stitched 3D decoder, to ensure that the
generated latents are decodable into consistent, perceptually convincing 3D
scene geometry. To that end, we adapt direct reward finetuning, a popular
technique for human preference alignment. We evaluate the proposed VIST3A
approach with different video generators and 3D reconstruction models. All
tested pairings markedly improve over prior text-to-3D models that output
Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also
enables high-quality text-to-pointmap generation.

Comments:
- Project page: https://gohyojun15.github.io/VIST3A/

---

## G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Junfeng Ni, Yixin Chen, Zhifei Yang, Yu Liu, Ruijie Lu, Song-Chun Zhu, Siyuan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2510.12099v1){: .btn .btn-green } |

**Abstract**: Despite recent advances in leveraging generative prior from pre-trained
diffusion models for 3D scene reconstruction, existing methods still face two
critical limitations. First, due to the lack of reliable geometric supervision,
they struggle to produce high-quality reconstructions even in observed regions,
let alone in unobserved areas. Second, they lack effective mechanisms to
mitigate multi-view inconsistencies in the generated images, leading to severe
shape-appearance ambiguities and degraded scene geometry. In this paper, we
identify accurate geometry as the fundamental prerequisite for effectively
exploiting generative models to enhance 3D scene reconstruction. We first
propose to leverage the prevalence of planar structures to derive accurate
metric-scale depth maps, providing reliable supervision in both observed and
unobserved regions. Furthermore, we incorporate this geometry guidance
throughout the generative pipeline to improve visibility mask estimation, guide
novel view selection, and enhance multi-view consistency when inpainting with
video diffusion models, resulting in accurate and consistent scene completion.
Extensive experiments on Replica, ScanNet++, and DeepBlending show that our
method consistently outperforms existing baselines in both geometry and
appearance reconstruction, particularly for unobserved regions. Moreover, our
method naturally supports single-view inputs and unposed videos, with strong
generalizability in both indoor and outdoor scenarios with practical real-world
applicability. The project page is available at
https://dali-jack.github.io/g4splat-web/.

Comments:
- Project page: https://dali-jack.github.io/g4splat-web/

---

## UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Yusen Xie, Zhenmin Huang, Jianhao Jiao, Dimitrios Kanoulas, Jun Ma | cs.CV | [PDF](http://arxiv.org/pdf/2510.12174v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose UniGS, a unified map representation and
differentiable framework for high-fidelity multimodal 3D reconstruction based
on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated
rasterization pipeline capable of rendering photo-realistic RGB images,
geometrically accurate depth maps, consistent surface normals, and semantic
logits simultaneously. We redesign the rasterization to render depth via
differentiable ray-ellipsoid intersection rather than using Gaussian centers,
enabling effective optimization of rotation and scale attribute through
analytic depth gradients. Furthermore, we derive the analytic gradient
formulation for surface normal rendering, ensuring geometric consistency among
reconstructed 3D scenes. To improve computational and storage efficiency, we
introduce a learnable attribute that enables differentiable pruning of
Gaussians with minimal contribution during training. Quantitative and
qualitative experiments demonstrate state-of-the-art reconstruction accuracy
across all modalities, validating the efficacy of our geometry-aware paradigm.
Source code and multimodal viewer will be available on GitHub.



---

## Hybrid Gaussian Splatting for Novel Urban View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Mohamed Omran, Farhad Zanjani, Davide Abati, Jens Petersen, Amirhossein Habibian | cs.CV | [PDF](http://arxiv.org/pdf/2510.12308v1){: .btn .btn-green } |

**Abstract**: This paper describes the Qualcomm AI Research solution to the RealADSim-NVS
challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge
concerns novel view synthesis in street scenes, and participants are required
to generate, starting from car-centric frames captured during some training
traversals, renders of the same urban environment as viewed from a different
traversal (e.g. different street lane or car direction). Our solution is
inspired by hybrid methods in scene generation and generative simulators
merging gaussian splatting and diffusion models, and it is composed of two
stages: First, we fit a 3D reconstruction of the scene and render novel views
as seen from the target cameras. Then, we enhance the resulting frames with a
dedicated single-step diffusion model. We discuss specific choices made in the
initialization of gaussian primitives as well as the finetuning of the enhancer
model and its training data curation. We report the performance of our model
design and we ablate its components in terms of novel view quality as measured
by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our
proposal reaches an aggregated score of 0.432, achieving the second place
overall.

Comments:
- ICCV 2025 RealADSim Workshop

---

## SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Haithem Turki, Qi Wu, Xin Kang, Janick Martinez Esturo, Shengyu Huang, Ruilong Li, Zan Gojcic, Riccardo de Lutio | cs.CV | [PDF](http://arxiv.org/pdf/2510.12901v2){: .btn .btn-green } |

**Abstract**: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

Comments:
- Project page: https://research.nvidia.com/labs/sil/projects/simuli

---

## PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Ying A, Wenzhang Sun, Chang Zeng, Chunfeng Wang, Hao Li, Jianxun Cui | cs.CV | [PDF](http://arxiv.org/pdf/2510.12282v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet
current methods face a stark trade-off between fidelity and computational cost.
This inefficiency stems from their semantically agnostic design, which
allocates resources uniformly, treating static backgrounds and safety-critical
objects with equal importance. To address this, we introduce Priority-Adaptive
Gaussian Splatting (PAGS), a framework that injects task-aware semantic
priorities directly into the 3D reconstruction and rendering pipeline. PAGS
introduces two core contributions: (1) Semantically-Guided Pruning and
Regularization strategy, which employs a hybrid importance metric to
aggressively simplify non-critical scene elements while preserving fine-grained
details on objects vital for navigation. (2) Priority-Driven Rendering
pipeline, which employs a priority-based depth pre-pass to aggressively cull
occluded primitives and accelerate the final shading computations. Extensive
experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves
exceptional reconstruction quality, particularly on safety-critical objects,
while significantly reducing training time and boosting rendering speeds to
over 350 FPS.



---

## Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Fengzhi Guo, Chih-Chuan Hsu, Sihao Ding, Cheng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.12768v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D scenes from monocular input is fundamentally
under-constrained, with ambiguities arising from occlusion and extreme novel
views. While dynamic Gaussian Splatting offers an efficient representation,
vanilla models optimize all Gaussian primitives uniformly, ignoring whether
they are well or poorly observed. This limitation leads to motion drifts under
occlusion and degraded synthesis when extrapolating to unseen views. We argue
that uncertainty matters: Gaussians with recurring observations across views
and time act as reliable anchors to guide motion, whereas those with limited
visibility are treated as less reliable. To this end, we introduce USplat4D, a
novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates
reliable motion cues to enhance 4D reconstruction. Our key insight is to
estimate time-varying per-Gaussian uncertainty and leverages it to construct a
spatio-temporal graph for uncertainty-aware optimization. Experiments on
diverse real and synthetic datasets show that explicitly modeling uncertainty
consistently improves dynamic Gaussian Splatting models, yielding more stable
geometry under occlusion and high-quality synthesis at extreme viewpoints.

Comments:
- Project page: https://tamu-visual-ai.github.io/usplat4d/

---

## BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | An Zhao, Piaopiao Yu, Zhe Zhu, Mingqiang Wei | cs.CV | [PDF](http://arxiv.org/pdf/2510.12493v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene
reconstruction. However, reconstructing high-quality 3D scenes from
motion-blurred images caused by camera motion poses a significant challenge.The
performance of existing 3DGS-based deblurring methods are limited due to their
inherent mechanisms, such as extreme dependence on the accuracy of camera poses
and inability to effectively control erroneous Gaussian primitives
densification caused by motion blur. To solve these problems, we introduce a
novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D
scenes from motion-blurred images. BSGS contains two stages. First, Camera Pose
Refinement roughly optimizes camera poses to reduce motion-induced distortions.
Second, with fixed rough camera poses, Global RigidTransformation further
corrects motion-induced blur distortions. To alleviate multi-subframe gradient
conflicts, we propose a subframe gradient aggregation strategy to optimize both
stages. Furthermore, a space-time bi-stage optimization strategy is introduced
to dynamically adjust primitive densification thresholds and prevent premature
noisy Gaussian generation in blurred regions. Comprehensive experiments verify
the effectiveness of our proposed deblurring method and show its superiority
over the state of the arts.Our source code is available at
https://github.com/wsxujm/bsgs

Comments:
- Accept by ACM MM 2025

---

## VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via  View Alignment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Qing Li, Huifang Feng, Xun Gong, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2510.11473v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently emerged as an efficient solution for
high-quality and real-time novel view synthesis. However, its capability for
accurate surface reconstruction remains underexplored. Due to the discrete and
unstructured nature of Gaussians, supervision based solely on image rendering
loss often leads to inaccurate geometry and inconsistent multi-view alignment.
In this work, we propose a novel method that enhances the geometric
representation of 3D Gaussians through view alignment (VA). Specifically, we
incorporate edge-aware image cues into the rendering loss to improve surface
boundary delineation. To enforce geometric consistency across views, we
introduce a visibility-aware photometric alignment loss that models occlusions
and encourages accurate spatial relationships among Gaussians. To further
mitigate ambiguities caused by lighting variations, we incorporate normal-based
constraints to refine the spatial orientation of Gaussians and improve local
surface estimation. Additionally, we leverage deep image feature embeddings to
enforce cross-view consistency, enhancing the robustness of the learned
geometry under varying viewpoints and illumination. Extensive experiments on
standard benchmarks demonstrate that our method achieves state-of-the-art
performance in both surface reconstruction and novel view synthesis. The source
code is available at https://github.com/LeoQLi/VA-GS.

Comments:
- Accepted by NeurIPS 2025

---

## GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in  Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Anastasiya Pechko, Piotr Borycki, Joanna Waczyńska, Daniel Barczyk, Agata Szymańska, Sławomir Tadeja, Przemysław Spurek | cs.GR | [PDF](http://arxiv.org/pdf/2510.11878v1){: .btn .btn-green } |

**Abstract**: As the demand for immersive 3D content grows, the need for intuitive and
efficient interaction methods becomes paramount. Current techniques for
physically manipulating 3D content within Virtual Reality (VR) often face
significant limitations, including reliance on engineering-intensive processes
and simplified geometric representations, such as tetrahedral cages, which can
compromise visual fidelity and physical accuracy. In this paper, we introduce
\our{} (\textbf{G}aussian \textbf{S}platting for \textbf{V}irtual
\textbf{E}nvironment \textbf{R}endering and \textbf{S}cene \textbf{E}diting), a
novel method designed to overcome these challenges by directly integrating an
object's mesh with a Gaussian Splatting (GS) representation. Our approach
enables more precise surface approximation, leading to highly realistic
deformations and interactions. By leveraging existing 3D mesh assets, \our{}
facilitates seamless content reuse and simplifies the development workflow.
Moreover, our system is designed to be physics-engine-agnostic, granting
developers robust deployment flexibility. This versatile architecture delivers
a highly realistic, adaptable, and intuitive approach to interactive 3D
manipulation. We rigorously validate our method against the current
state-of-the-art technique that couples VR with GS in a comparative user study
involving 18 participants. Specifically, we demonstrate that our approach is
statistically significantly better for physics-aware stretching manipulation
and is also more consistent in other physics-based manipulations like twisting
and shaking. Further evaluation across various interactions and scenes confirms
that our method consistently delivers high and reliable performance, showing
its potential as a plausible alternative to existing methods.



---

## MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent  Material Inference

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Wenyuan Zhang, Jimin Tang, Weiqi Zhang, Yi Fang, Yu-Shen Liu, Zhizhong Han | cs.CV | [PDF](http://arxiv.org/pdf/2510.11387v1){: .btn .btn-green } |

**Abstract**: Modeling reflections from 2D images is essential for photorealistic rendering
and novel view synthesis. Recent approaches enhance Gaussian primitives with
reflection-related material attributes to enable physically based rendering
(PBR) with Gaussian Splatting. However, the material inference often lacks
sufficient constraints, especially under limited environment modeling,
resulting in illumination aliasing and reduced generalization. In this work, we
revisit the problem from a multi-view perspective and show that multi-view
consistent material inference with more physically-based environment modeling
is key to learning accurate reflections with Gaussian Splatting. To this end,
we enforce 2D Gaussians to produce multi-view consistent material maps during
deferred shading. We also track photometric variations across views to identify
highly reflective regions, which serve as strong priors for reflection strength
terms. To handle indirect illumination caused by inter-object occlusions, we
further introduce an environment modeling strategy through ray tracing with
2DGS, enabling photorealistic rendering of indirect radiance. Experiments on
widely used benchmarks show that our method faithfully recovers both
illumination and geometry, achieving state-of-the-art rendering quality in
novel views synthesis.

Comments:
- Accepted by NeurIPS 2025. Project Page:
  https://wen-yuan-zhang.github.io/MaterialRefGS

---

## Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event  Streams


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik | cs.CV | [PDF](http://arxiv.org/pdf/2510.11717v1){: .btn .btn-green } |

**Abstract**: Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.



---

## Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Yuxin Cheng, Binxiao Huang, Taiqiang Wu, Wenyong Zhou, Chenchen Ding, Zhengwu Liu, Graziano Chesi, Ngai Wong | cs.CV | [PDF](http://arxiv.org/pdf/2510.10993v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian inpainting, a critical technique for numerous applications in
virtual reality and multimedia, has made significant progress with pretrained
diffusion models. However, ensuring multi-view consistency, an essential
requirement for high-quality inpainting, remains a key challenge. In this work,
we present PAInpainter, a novel approach designed to advance 3D Gaussian
inpainting by leveraging perspective-aware content propagation and consistency
verification across multi-view inpainted images. Our method iteratively refines
inpainting and optimizes the 3D Gaussian representation with multiple views
adaptively sampled from a perspective graph. By propagating inpainted images as
prior information and verifying consistency across neighboring views,
PAInpainter substantially enhances global consistency and texture fidelity in
restored 3D scenes. Extensive experiments demonstrate the superiority of
PAInpainter over existing methods. Our approach achieves superior 3D inpainting
quality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and
NeRFiller datasets, respectively, highlighting its effectiveness and
generalization capability.



---

## Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for  Uncertainty-Aware Sim-to-Real Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Maggie Wang, Stephen Tian, Aiden Swann, Ola Shorinwa, Jiajun Wu, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2510.11689v1){: .btn .btn-green } |

**Abstract**: Learning robotic manipulation policies directly in the real world can be
expensive and time-consuming. While reinforcement learning (RL) policies
trained in simulation present a scalable alternative, effective sim-to-real
transfer remains challenging, particularly for tasks that require precise
dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL
pipeline that combines vision-language model (VLM)-inferred physical parameter
estimates with interactive adaptation through uncertainty-aware fusion. Our
approach consists of three core components: (1) high-fidelity geometric
reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions
over physical parameters, and (3) online physical parameter estimation from
interaction data. Phys2Real conditions policies on interpretable physical
parameters, refining VLM predictions with online estimates via ensemble-based
uncertainty quantification. On planar pushing tasks of a T-block with varying
center of mass (CoM) and a hammer with an off-center mass distribution,
Phys2Real achieves substantial improvements over a domain randomization
baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%
in the challenging top-weighted T-block, and 15% faster average task completion
for hammer pushing. Ablation studies indicate that the combination of VLM and
interaction information is essential for success. Project website:
https://phys2real.github.io/ .



---

## Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-12 | Xuankai Zhang, Junjin Xiao, Qing Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.10691v1){: .btn .btn-green } |

**Abstract**: This paper presents a unified framework that allows high-quality dynamic
Gaussian Splatting from both defocused and motion-blurred monocular videos. Due
to the significant difference between the formation processes of defocus blur
and motion blur, existing methods are tailored for either one of them, lacking
the ability to simultaneously deal with both of them. Although the two can be
jointly modeled as blur kernel-based convolution, the inherent difficulty in
estimating accurate blur kernels greatly limits the progress in this direction.
In this work, we go a step further towards this direction. Particularly, we
propose to estimate per-pixel reliable blur kernels using a blur prediction
network that exploits blur-related scene and camera information and is subject
to a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian
densification strategy to mitigate the lack of Gaussians for incomplete
regions, and boost the performance of novel view synthesis by incorporating
unseen view information to constrain scene optimization. Extensive experiments
show that our method outperforms the state-of-the-art methods in generating
photorealistic novel view synthesis from defocused and motion-blurred monocular
videos. Our code and trained model will be made publicly available.



---

## Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided  Framework

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-12 | Shanzhi Yin, Bolin Chen, Xinju Wu, Ru-Ling Liao, Jie Chen, Shiqi Wang, Yan Ye | eess.IV | [PDF](http://arxiv.org/pdf/2510.10492v1){: .btn .btn-green } |

**Abstract**: This paper proposes an efficient 3D avatar coding framework that leverages
compact human priors and canonical-to-target transformation to enable
high-quality 3D human avatar video compression at ultra-low bit rates. The
framework begins by training a canonical Gaussian avatar using articulated
splatting in a network-free manner, which serves as the foundation for avatar
appearance modeling. Simultaneously, a human-prior template is employed to
capture temporal body movements through compact parametric representations.
This decomposition of appearance and temporal evolution minimizes redundancy,
enabling efficient compression: the canonical avatar is shared across the
sequence, requiring compression only once, while the temporal parameters,
consisting of just 94 parameters per frame, are transmitted with minimal
bit-rate. For each frame, the target human avatar is generated by deforming
canonical avatar via Linear Blend Skinning transformation, facilitating
temporal coherent video reconstruction and novel view synthesis. Experimental
results demonstrate that the proposed method significantly outperforms
conventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting
compression method in terms of rate-distortion performance on mainstream
multi-view human video datasets, paving the way for seamless immersive
multimedia experiences in meta-verse applications.

Comments:
- 10 pages, 4 figures

---

## High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic  Manipulation Learning with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-12 | Haoyu Zhao, Cheng Zeng, Linghao Zhuang, Yaxi Zhao, Shengke Xue, Hao Wang, Xingyue Zhao, Zhongyu Li, Kehan Li, Siteng Huang, Mingxiu Chen, Xin Li, Deli Zhao, Hua Zou | cs.RO | [PDF](http://arxiv.org/pdf/2510.10637v1){: .btn .btn-green } |

**Abstract**: The scalability of robotic learning is fundamentally bottlenecked by the
significant cost and labor of real-world data collection. While simulated data
offers a scalable alternative, it often fails to generalize to the real world
due to significant gaps in visual appearance, physical properties, and object
interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real
framework that converts multi-view real-world images into scalable,
high-fidelity, and physically interactive simulation environments for robotic
manipulation. Our approach reconstructs scenes using a hybrid representation:
3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the
environment, while mesh primitives for interactive objects ensure accurate
physics simulation. Crucially, we pioneer the use of a Multi-modal Large
Language Model (MLLM) to automate the creation of physically plausible,
articulated assets. The MLLM analyzes visual data to infer not only physical
properties (e.g., density, stiffness) but also complex kinematic structures
(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained
entirely on data generated by RoboSimGS achieve successful zero-shot
sim-to-real transfer across a diverse set of real-world manipulation tasks.
Furthermore, data from RoboSimGS significantly enhances the performance and
generalization capabilities of SOTA methods. Our results validate RoboSimGS as
a powerful and scalable solution for bridging the sim-to-real gap.

Comments:
- 13 pages, 6 figures

---

## Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Jiahui Lu, Haihong Xiao, Xueyan Zhao, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2510.10097v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced
3D reconstruction and novel view synthesis, but remain heavily dependent on
accurate camera poses and dense viewpoint coverage. These requirements limit
their applicability in sparse-view settings, where pose estimation becomes
unreliable and supervision is insufficient. To overcome these challenges, we
introduce Gesplat, a 3DGS-based framework that enables robust novel view
synthesis and geometrically consistent reconstruction from unposed sparse
images. Unlike prior works that rely on COLMAP for sparse point cloud
initialization, we leverage the VGGT foundation model to obtain more reliable
initial poses and dense point clouds. Our approach integrates several key
innovations: 1) a hybrid Gaussian representation with dual position-shape
optimization enhanced by inter-view matching consistency; 2) a graph-guided
attribute refinement module to enhance scene details; and 3) flow-based depth
regularization that improves depth estimation accuracy for more effective
supervision. Comprehensive quantitative and qualitative experiments demonstrate
that our approach achieves more robust performance on both forward-facing and
large-scale complex datasets compared to other pose-free methods.



---

## CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Zhigang Cheng, Mingchao Sun, Yu Liu, Zengye Ge, Luyang Tang, Mu Xu, Yangyan Li, Peng Pan | cs.GR | [PDF](http://arxiv.org/pdf/2510.09997v1){: .btn .btn-green } |

**Abstract**: Level of Detail (LoD) is a fundamental technique in real-time computer
graphics for managing the rendering costs of complex scenes while preserving
visual fidelity. Traditionally, LoD is implemented using discrete levels
(DLoD), where multiple, distinct versions of a model are swapped out at
different distances. This long-standing paradigm, however, suffers from two
major drawbacks: it requires significant storage for multiple model copies and
causes jarring visual ``popping" artifacts during transitions, degrading the
user experience. We argue that the explicit, primitive-based nature of the
emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm:
Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality
scaling within a single, unified model, thereby circumventing the core problems
of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a
continuous LoD mechanism directly into a 3DGS representation. Our method
introduces a learnable, distance-dependent decay parameter for each Gaussian
primitive, which dynamically adjusts its opacity based on viewpoint proximity.
This allows for the progressive and smooth filtering of less significant
primitives, effectively creating a continuous spectrum of detail within one
model. To train this model to be robust across all distances, we introduce a
virtual distance scaling mechanism and a novel coarse-to-fine training strategy
with rendered point count regularization. Our approach not only eliminates the
storage overhead and visual artifacts of discrete methods but also reduces the
primitive count and memory footprint of the final model. Extensive experiments
demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a
single model, delivering high-fidelity results across a wide range of
performance targets.



---

## Opacity-Gradient Driven Density Control for Compact and Efficient  Few-Shot 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Abdelrhman Elrawy, Emad A. Mohammed | cs.CV | [PDF](http://arxiv.org/pdf/2510.10257v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its
standard adaptive density control (ADC) can lead to overfitting and bloated
reconstructions. While state-of-the-art methods like FSGS improve quality, they
often do so by significantly increasing the primitive count. This paper
presents a framework that revises the core 3DGS optimization to prioritize
efficiency. We replace the standard positional gradient heuristic with a novel
densification trigger that uses the opacity gradient as a lightweight proxy for
rendering error. We find this aggressive densification is only effective when
paired with a more conservative pruning schedule, which prevents destructive
optimization cycles. Combined with a standard depth-correlation loss for
geometric guidance, our framework demonstrates a fundamental improvement in
efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k
vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a
reduction of approximately 70%. This dramatic gain in compactness is achieved
with a modest trade-off in reconstruction metrics, establishing a new
state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view
synthesis.



---

## Color3D: Controllable and Consistent 3D Colorization with Personalized  Colorizer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Yecong Wan, Mingwen Shao, Renlong Wu, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2510.10152v1){: .btn .btn-green } |

**Abstract**: In this work, we present Color3D, a highly adaptable framework for colorizing
both static and dynamic 3D scenes from monochromatic inputs, delivering
visually diverse and chromatically vibrant reconstructions with flexible
user-guided control. In contrast to existing methods that focus solely on
static scenarios and enforce multi-view consistency by averaging color
variations which inevitably sacrifice both chromatic richness and
controllability, our approach is able to preserve color diversity and
steerability while ensuring cross-view and cross-time consistency. In
particular, the core insight of our method is to colorize only a single key
view and then fine-tune a personalized colorizer to propagate its color to
novel views and time steps. Through personalization, the colorizer learns a
scene-specific deterministic color mapping underlying the reference view,
enabling it to consistently project corresponding colors to the content in
novel views and video frames via its inherent inductive bias. Once trained, the
personalized colorizer can be applied to infer consistent chrominance for all
other images, enabling direct reconstruction of colorful 3D scenes with a
dedicated Lab color space Gaussian splatting representation. The proposed
framework ingeniously recasts complicated 3D colorization as a more tractable
single image paradigm, allowing seamless integration of arbitrary image
colorization models with enhanced flexibility and controllability. Extensive
experiments across diverse static and dynamic 3D colorization benchmarks
substantiate that our method can deliver more consistent and chromatically rich
renderings with precise user control. Project Page
https://yecongwan.github.io/Color3D/.

Comments:
- Project Page https://yecongwan.github.io/Color3D/

---

## P-4DGS: Predictive 4D Gaussian Splatting with 90$\times$ Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Henan Wang, Hanxin Zhu, Xinliang Gong, Tianyu He, Xin Li, Zhibo Chen | cs.CV | [PDF](http://arxiv.org/pdf/2510.10030v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has garnered significant attention due to its
superior scene representation fidelity and real-time rendering performance,
especially for dynamic 3D scene reconstruction (\textit{i.e.}, 4D
reconstruction). However, despite achieving promising results, most existing
algorithms overlook the substantial temporal and spatial redundancies inherent
in dynamic scenes, leading to prohibitive memory consumption. To address this,
we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene
modeling. Inspired by intra- and inter-frame prediction techniques commonly
used in video compression, we first design a 3D anchor point-based
spatial-temporal prediction module to fully exploit the spatial-temporal
correlations across different 3D Gaussian primitives. Subsequently, we employ
an adaptive quantization strategy combined with context-based entropy coding to
further reduce the size of the 3D anchor points, thereby achieving enhanced
compression efficiency. To evaluate the rate-distortion performance of our
proposed P-4DGS in comparison with other dynamic 3DGS representations, we
conduct extensive experiments on both synthetic and real-world datasets.
Experimental results demonstrate that our approach achieves state-of-the-art
reconstruction quality and the fastest rendering speed, with a remarkably low
storage footprint (around \textbf{1MB} on average), achieving up to
\textbf{40$\times$} and \textbf{90$\times$} compression on synthetic and
real-world scenes, respectively.



---

## VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene  Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Yicheng He, Jingwen Yu, Guangcheng Chen, Hong Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2510.09962v1){: .btn .btn-green } |

**Abstract**: Maintaining an up-to-date map that accurately reflects recent changes in the
environment is crucial, especially for robots that repeatedly traverse the same
space. Failing to promptly update the changed regions can degrade map quality,
resulting in poor localization, inefficient operations, and even lost robots.
3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online
map reconstruction due to its dense, differentiable, and photorealistic
properties, yet accurately and efficiently updating the regions of change
remains a challenge. In this paper, we propose VG-Mapping, a novel online
3DGS-based mapping system tailored for such semi-static scenes. Our approach
introduces a hybrid representation that augments 3DGS with a TSDF-based voxel
map to efficiently identify changed regions in a scene, along with a
variation-aware density control strategy that inserts or deletes Gaussian
primitives in regions undergoing change. Furthermore, to address the absence of
public benchmarks for this task, we construct a RGB-D dataset comprising both
synthetic and real-world semi-static environments. Experimental results
demonstrate that our method substantially improves the rendering quality and
map update efficiency in semi-static scenes. The code and dataset are available
at https://github.com/heyicheng-never/VG-Mapping.



---

## Two-Stage Gaussian Splatting Optimization for Outdoor Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Deborah Pintani, Ariel Caputo, Noah Lewis, Marc Stamminger, Fabio Pellacini, Andrea Giachetti | cs.GR | [PDF](http://arxiv.org/pdf/2510.09489v1){: .btn .btn-green } |

**Abstract**: Outdoor scene reconstruction remains challenging due to the stark contrast
between well-textured, nearby regions and distant backgrounds dominated by low
detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian
Splatting framework that explicitly separates and optimizes these regions,
yielding higher-fidelity novel view synthesis. In stage one, background
primitives are initialized within a spherical shell and optimized using a loss
that combines a background-only photometric term with two geometric
regularizers: one constraining Gaussians to remain inside the shell, and
another aligning them with local tangential planes. In stage two, foreground
Gaussians are initialized from a Structure-from-Motion reconstruction, added
and refined using the standard rendering loss, while the background set remains
fixed but contributes to the final image formation. Experiments on diverse
outdoor datasets show that our method reduces background artifacts and improves
perceptual quality compared to state-of-the-art baselines. Moreover, the
explicit background separation enables automatic, object-free environment map
estimation, opening new possibilities for photorealistic outdoor rendering and
mixed-reality applications.



---

## FLOWING: Implicit Neural Flows for Structure-Preserving Morphing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Arthur Bizzi, Matias Grynberg, Vitor Matias, Daniel Perazzo, João Paulo Lima, Luiz Velho, Nuno Gonçalves, João Pereira, Guilherme Schardong, Tiago Novello | cs.CV | [PDF](http://arxiv.org/pdf/2510.09537v1){: .btn .btn-green } |

**Abstract**: Morphing is a long-standing problem in vision and computer graphics,
requiring a time-dependent warping for feature alignment and a blending for
smooth interpolation. Recently, multilayer perceptrons (MLPs) have been
explored as implicit neural representations (INRs) for modeling such
deformations, due to their meshlessness and differentiability; however,
extracting coherent and accurate morphings from standard MLPs typically relies
on costly regularizations, which often lead to unstable training and prevent
effective feature alignment. To overcome these limitations, we propose FLOWING
(FLOW morphING), a framework that recasts warping as the construction of a
differential vector flow, naturally ensuring continuity, invertibility, and
temporal coherence by encoding structural flow properties directly into the
network architectures. This flow-centric approach yields principled and stable
transformations, enabling accurate and structure-preserving morphing of both 2D
images and 3D shapes. Extensive experiments across a range of applications -
including face and image morphing, as well as Gaussian Splatting morphing -
show that FLOWING achieves state-of-the-art morphing quality with faster
convergence. Code and pretrained models are available at
http://schardong.github.io/flowing.

Comments:
- 10 pages main paper; 9 pages references and appendix

---

## Vision Language Models: A Survey of 26K Papers

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Fengming Lin | cs.CV | [PDF](http://arxiv.org/pdf/2510.09586v1){: .btn .btn-green } |

**Abstract**: We present a transparent, reproducible measurement of research trends across
26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles
and abstracts are normalized, phrase-protected, and matched against a
hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained
cues about tasks, architectures, training regimes, objectives, datasets, and
co-mentioned modalities. The analysis quantifies three macro shifts: (1) a
sharp rise of multimodal vision-language-LLM work, which increasingly reframes
classic perception as instruction following and multi-step reasoning; (2)
steady expansion of generative methods, with diffusion research consolidating
around controllability, distillation, and speed; and (3) resilient 3D and video
activity, with composition moving from NeRFs to Gaussian splatting and a
growing emphasis on human- and agent-centric understanding. Within VLMs,
parameter-efficient adaptation like prompting/adapters/LoRA and lightweight
vision-language bridges dominate; training practice shifts from building
encoders from scratch to instruction tuning and finetuning strong backbones;
contrastive objectives recede relative to cross-entropy/ranking and
distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and
ICLR the highest VLM share, while reliability themes such as efficiency or
robustness diffuse across areas. We release the lexicon and methodology to
enable auditing and extension. Limitations include lexicon recall and
abstract-only scope, but the longitudinal signals are consistent across venues
and years.

Comments:
- VLM/LLM Learning Notes

---

## HERO: Hardware-Efficient RL-based Optimization Framework for NeRF  Quantization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Yipu Zhang, Chaofang Ma, Jinming Ge, Lin Jiang, Jiang Xu, Wei Zhang | cs.AR | [PDF](http://arxiv.org/pdf/2510.09010v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction
method, delivering high-quality results for AR/VR applications. While
quantization methods and hardware accelerators have been proposed to enhance
NeRF's computational efficiency, existing approaches face crucial limitations.
Current quantization methods operate without considering hardware architecture,
resulting in sub-optimal solutions within the vast design space encompassing
accuracy, latency, and model size. Additionally, existing NeRF accelerators
heavily rely on human experts to explore this design space, making the
optimization process time-consuming, inefficient, and unlikely to discover
optimal solutions. To address these challenges, we introduce HERO, a
reinforcement learning framework performing hardware-aware quantization for
NeRF. Our framework integrates a NeRF accelerator simulator to generate
real-time hardware feedback, enabling fully automated adaptation to hardware
constraints. Experimental results demonstrate that HERO achieves 1.31-1.33
$\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a
more compact model size compared to CAQ, a previous state-of-the-art NeRF
quantization framework. These results validate our framework's capability to
effectively navigate the complex design space between hardware and algorithm
requirements, discovering superior quantization policies for NeRF
implementation. Code is available at https://github.com/ypzhng/HERO.

Comments:
- Accepted by ASPDAC 2026

---

## LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Minkwan Kim, Seungmin Lee, Junho Kim, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2510.09881v1){: .btn .btn-green } |

**Abstract**: Recent advances in novel-view synthesis can create the photo-realistic
visualization of real-world environments from conventional camera captures.
However, acquiring everyday environments from casual captures faces challenges
due to frequent scene changes, which require dense observations both spatially
and temporally. We propose long-term Gaussian scene chronology from sparse-view
updates, coined LTGS, an efficient scene representation that can embrace
everyday changes from highly under-constrained casual captures. Given an
incomplete and unstructured Gaussian splatting representation obtained from an
initial set of input images, we robustly model the long-term chronology of the
scene despite abrupt movements and subtle environmental variations. We
construct objects as template Gaussians, which serve as structural, reusable
priors for shared object tracks. Then, the object templates undergo a further
refinement pipeline that modulates the priors to adapt to temporally varying
environments based on few-shot observations. Once trained, our framework is
generalizable across multiple time steps through simple transformations,
significantly enhancing the scalability for a temporal evolution of 3D
environments. As existing datasets do not explicitly represent the long-term
real-world changes with a sparse capture setup, we collect real-world datasets
to evaluate the practicality of our pipeline. Experiments demonstrate that our
framework achieves superior reconstruction quality compared to other baselines
while enabling fast and light-weight updates.



---

## Geometry-Aware Scene Configurations for Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Minkwan Kim, Changwoon Choi, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2510.09880v1){: .btn .btn-green } |

**Abstract**: We propose scene-adaptive strategies to efficiently allocate representation
capacity for generating immersive experiences of indoor environments from
incomplete observations. Indoor scenes with multiple rooms often exhibit
irregular layouts with varying complexity, containing clutter, occlusion, and
flat walls. We maximize the utilization of limited resources with guidance from
geometric priors, which are often readily available after pre-processing
stages. We record observation statistics on the estimated geometric scaffold
and guide the optimal placement of bases, which greatly improves upon the
uniform basis arrangements adopted by previous scalable Neural Radiance Field
(NeRF) representations. We also suggest scene-adaptive virtual viewpoints to
compensate for geometric deficiencies inherent in view configurations in the
input trajectory and impose the necessary regularization. We present a
comprehensive analysis and discussion regarding rendering quality and memory
requirements in several large-scale indoor scenes, demonstrating significant
enhancements compared to baselines that employ regular placements.



---

## Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic  Urban Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Yikang Zhang, Rui Fan | cs.CV | [PDF](http://arxiv.org/pdf/2510.09364v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has demonstrated impressive performance in
synthesizing high-fidelity novel views. Nonetheless, its effectiveness
critically depends on the quality of the initialized point cloud. Specifically,
achieving uniform and complete point coverage over the underlying scene
structure requires overlapping observation frustums, an assumption that is
often violated in unbounded, dynamic urban environments. Training Gaussian
models with partially initialized point clouds often leads to distortions and
artifacts, as camera rays may fail to intersect valid surfaces, resulting in
incorrect gradient propagation to Gaussian primitives associated with occluded
or invisible geometry. Additionally, existing densification strategies simply
clone and split Gaussian primitives from existing ones, incapable of
reconstructing missing structures. To address these limitations, we propose
VAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban
scenes. Our method identifies unreliable geometry structures via voxel-based
visibility reasoning, selects informative supporting views through
diversity-aware view selection, and recovers missing structures via patch
matching-based multi-view stereo reconstruction. This design enables the
generation of new Gaussian primitives guided by reliable geometric priors, even
in regions lacking initial points. Extensive experiments on the Waymo and
nuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS
approaches and significantly improves the quality of reconstructed geometry for
both static and dynamic objects. Source code will be released upon publication.



---

## DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event  Stream

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Junhao He, Jiaxu Wang, Jia Li, Mingyuan Sun, Qiang Zhang, Jiahang Cao, Ziyi Zhang, Yi Gu, Jingkai Sun, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.07752v1){: .btn .btn-green } |

**Abstract**: Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB
videos is challenging. This is because large inter-frame motions will increase
the uncertainty of the solution space. For example, one pixel in the first
frame might have more choices to reach the corresponding pixel in the second
frame. Event cameras can asynchronously capture rapid visual changes and are
robust to motion blur, but they do not provide color information. Intuitively,
the event stream can provide deterministic constraints for the inter-frame
large motion by the event trajectories. Hence, combining
low-temporal-resolution images with high-framerate event streams can address
this challenge. However, it is challenging to jointly optimize Dynamic 3DGS
using both RGB and event modalities due to the significant discrepancy between
these two data modalities. This paper introduces a novel framework that jointly
optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event
motion priors to guide the optimization of the deformation fields. First, we
extract the motion priors encoded in event streams by using the proposed LoCM
unsupervised fine-tuning framework to adapt an event flow estimator to a
certain unseen scene. Then, we present the geometry-aware data association
method to build the event-Gaussian motion correspondence, which is the primary
foundation of the pipeline, accompanied by two useful strategies, namely motion
decomposition and inter-frame pseudo-label. Extensive experiments show that our
method outperforms existing image and event-based approaches across synthetic
and real scenes and prove that our method can effectively optimize dynamic 3DGS
with the help of event data.

Comments:
- Accepted by TVCG

---

## Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Ankit Gahlawat, Anirban Mukherjee, Dinesh Babu Jayagopi | cs.CV | [PDF](http://arxiv.org/pdf/2510.08096v1){: .btn .btn-green } |

**Abstract**: Accurate face parsing under extreme viewing angles remains a significant
challenge due to limited labeled data in such poses. Manual annotation is
costly and often impractical at scale. We propose a novel label refinement
pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate
segmentation masks from noisy multiview predictions. By jointly fitting two
3DGS models, one to RGB images and one to their initial segmentation maps, our
method enforces multiview consistency through shared geometry, enabling the
synthesis of pose-diverse training data with only minimal post-processing.
Fine-tuning a face parsing model on this refined dataset significantly improves
accuracy on challenging head poses, while maintaining strong performance on
standard views. Extensive experiments, including human evaluations, demonstrate
that our approach achieves superior results compared to state-of-the-art
methods, despite requiring no ground-truth 3D annotations and using only a
small set of initial images. Our method offers a scalable and effective
solution for improving face parsing robustness in real-world settings.

Comments:
- Accepted to VCIP 2025 (International Conference on Visual
  Communications and Image Processing 2025)

---

## CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal  Reconstruction Model for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni, Chenjing Ding, Dan Xu, Lewei Lu, Zehuan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2510.07944v1){: .btn .btn-green } |

**Abstract**: Generative models have been widely applied to world modeling for environment
simulation and future state prediction. With advancements in autonomous
driving, there is a growing demand not only for high-fidelity video generation
under various controls, but also for producing diverse and meaningful
information such as depth estimation. To address this, we propose CVD-STORM, a
cross-view video diffusion model utilizing a spatial-temporal reconstruction
Variational Autoencoder (VAE) that generates long-term, multi-view videos with
4D reconstruction capabilities under various control inputs. Our approach first
fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its
ability to encode 3D structures and temporal dynamics. Subsequently, we
integrate this VAE into the video diffusion process to significantly improve
generation quality. Experimental results demonstrate that our model achieves
substantial improvements in both FID and FVD metrics. Additionally, the
jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic
scenes, providing valuable geometric information for comprehensive scene
understanding.



---

## ReSplat: Learning Recurrent Gaussian Splats

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Haofei Xu, Daniel Barath, Andreas Geiger, Marc Pollefeys | cs.CV | [PDF](http://arxiv.org/pdf/2510.08575v1){: .btn .btn-green } |

**Abstract**: While feed-forward Gaussian splatting models provide computational efficiency
and effectively handle sparse input settings, their performance is
fundamentally limited by the reliance on a single forward pass during
inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting
model that iteratively refines 3D Gaussians without explicitly computing
gradients. Our key insight is that the Gaussian splatting rendering error
serves as a rich feedback signal, guiding the recurrent network to learn
effective Gaussian updates. This feedback signal naturally adapts to unseen
data distributions at test time, enabling robust generalization. To initialize
the recurrent process, we introduce a compact reconstruction model that
operates in a $16 \times$ subsampled space, producing $16 \times$ fewer
Gaussians than previous per-pixel Gaussian models. This substantially reduces
computational overhead and allows for efficient Gaussian updates. Extensive
experiments across varying of input views (2, 8, 16), resolutions ($256 \times
256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate
that our method achieves state-of-the-art performance while significantly
reducing the number of Gaussians and improving the rendering speed. Our project
page is at https://haofeixu.github.io/resplat/.

Comments:
- Project page: https://haofeixu.github.io/resplat/

---

## Splat the Net: Radiance Fields with Splattable Neural Primitives

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Xilong Zhou, Bao-Huy Nguyen, Loïc Magne, Vladislav Golyanik, Thomas Leimkühler, Christian Theobalt | cs.GR | [PDF](http://arxiv.org/pdf/2510.08491v1){: .btn .btn-green } |

**Abstract**: Radiance fields have emerged as a predominant representation for modeling 3D
scene appearance. Neural formulations such as Neural Radiance Fields provide
high expressivity but require costly ray marching for rendering, whereas
primitive-based methods such as 3D Gaussian Splatting offer real-time
efficiency through splatting, yet at the expense of representational power.
Inspired by advances in both these directions, we introduce splattable neural
primitives, a new volumetric representation that reconciles the expressivity of
neural models with the efficiency of primitive-based splatting. Each primitive
encodes a bounded neural density field parameterized by a shallow neural
network. Our formulation admits an exact analytical solution for line
integrals, enabling efficient computation of perspectively accurate splatting
kernels. As a result, our representation supports integration along view rays
without the need for costly ray marching. The primitives flexibly adapt to
scene geometry and, being larger than prior analytic primitives, reduce the
number required per scene. On novel-view synthesis benchmarks, our approach
matches the quality and speed of 3D Gaussian Splatting while using $10\times$
fewer primitives and $6\times$ fewer parameters. These advantages arise
directly from the representation itself, without reliance on complex control or
adaptation frameworks. The project page is
https://vcai.mpi-inf.mpg.de/projects/SplatNet/.



---

## An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit  Data Reuse Strategies

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Binzhe Yuan, Xiangyu Zhang, Zeyu Zheng, Yuefeng Zhang, Haochuan Wan, Zhechen Yuan, Junsheng Chen, Yunxiang He, Junran Ding, Xiaoming Zhang, Chaolin Rao, Wenyan Su, Pingqiang Zhou, Jingyi Yu, Xin Lou | eess.IV | [PDF](http://arxiv.org/pdf/2510.07667v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) have transformed 3D reconstruction and
rendering, facilitating photorealistic image synthesis from sparse viewpoints.
This work introduces an explicit data reuse neural rendering (EDR-NR)
architecture, which reduces frequent external memory accesses (EMAs) and cache
misses by exploiting the spatial locality from three phases, including rays,
ray packets (RPs), and samples. The EDR-NR architecture features a four-stage
scheduler that clusters rays on the basis of Z-order, prioritize lagging rays
when ray divergence happens, reorders RPs based on spatial proximity, and
issues samples out-of-orderly (OoO) according to the availability of on-chip
feature data. In addition, a four-tier hierarchical RP marching (HRM) technique
is integrated with an axis-aligned bounding box (AABB) to facilitate spatial
skipping (SS), reducing redundant computations and improving throughput.
Moreover, a balanced allocation strategy for feature storage is proposed to
mitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area
of 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized
energy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X
increase in normalized throughput, and a 53.42% reduction in on-chip SRAM
consumption compared to state-of-the-art accelerators.

Comments:
- 11 pages, 17 figures, 2 tables

---

## D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and  Accurate Sparse-View Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Meixi Song, Xin Lin, Dizhe Zhang, Haodong Li, Xiangtai Li, Bo Du, Lu Qi | cs.CV | [PDF](http://arxiv.org/pdf/2510.08566v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) enable real-time,
high-fidelity novel view synthesis (NVS) with explicit 3D representations.
However, performance degradation and instability remain significant under
sparse-view conditions. In this work, we identify two key failure modes under
sparse-view conditions: overfitting in regions with excessive Gaussian density
near the camera, and underfitting in distant areas with insufficient Gaussian
coverage. To address these challenges, we propose a unified framework D$^2$GS,
comprising two key components: a Depth-and-Density Guided Dropout strategy that
suppresses overfitting by adaptively masking redundant Gaussians based on
density and depth, and a Distance-Aware Fidelity Enhancement module that
improves reconstruction quality in under-fitted far-field areas through
targeted supervision. Moreover, we introduce a new evaluation metric to
quantify the stability of learned Gaussian distributions, providing insights
into the robustness of the sparse-view 3DGS. Extensive experiments on multiple
datasets demonstrate that our method significantly improves both visual quality
and robustness under sparse view conditions. The project page can be found at:
https://insta360-research-team.github.io/DDGS-website/.



---

## PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale  3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Houqiang Zhong, Zhenglong Wu, Sihua Fu, Zihan Zheng, Xin Jin, Xiaoyun Zhang, Li Song, Qiang Hu | cs.CV | [PDF](http://arxiv.org/pdf/2510.07830v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic
rendering in compact scenes, but scaling to large urban environments introduces
severe aliasing artifacts and optimization instability, especially under
high-resolution (e.g., 4K) rendering. These artifacts, manifesting as
flickering textures and jagged edges, arise from the mismatch between Gaussian
primitives and the multi-scale nature of urban geometry. While existing
``divide-and-conquer'' pipelines address scalability, they fail to resolve this
fidelity gap. In this paper, we propose PrismGS, a physically-grounded
regularization framework that improves the intrinsic rendering behavior of 3D
Gaussians. PrismGS integrates two synergistic regularizers. The first is
pyramidal multi-scale supervision, which enforces consistency by supervising
the rendering against a pre-filtered image pyramid. This compels the model to
learn an inherently anti-aliased representation that remains coherent across
different viewing scales, directly mitigating flickering textures. This is
complemented by an explicit size regularization that imposes a
physically-grounded lower bound on the dimensions of the 3D Gaussians. This
prevents the formation of degenerate, view-dependent primitives, leading to
more stable and plausible geometric surfaces and reducing jagged edges. Our
method is plug-and-play and compatible with existing pipelines. Extensive
experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS
achieves state-of-the-art performance, yielding significant PSNR gains around
1.5 dB against CityGaussian, while maintaining its superior quality and
robustness under demanding 4K rendering.



---

## ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral  Probes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Jian Gao, Mengqi Yuan, Yifei Zeng, Chang Zeng, Zhihao Li, Zhenyu Chen, Weichao Qiu, Xiao-Xiao Long, Hao Zhu, Xun Cao, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2510.07729v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) enables immersive rendering, but realistic 3D
object-scene composition remains challenging. Baked appearance and shadow
information in GS radiance fields cause inconsistencies when combining objects
and scenes. Addressing this requires relightable object reconstruction and
scene lighting estimation. For relightable object reconstruction, existing
Gaussian-based inverse rendering methods often rely on ray tracing, leading to
low efficiency. We introduce Surface Octahedral Probes (SOPs), which store
lighting and occlusion information and allow efficient 3D querying via
interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x
speedup in reconstruction and enable real-time shadow computation in Gaussian
scenes. For lighting estimation, existing Gaussian-based inverse rendering
methods struggle to model intricate light transport and often fail in complex
scenes, while learning-based methods predict lighting from a single image and
are viewpoint-sensitive. We observe that 3D object-scene composition primarily
concerns the object's appearance and nearby shadows. Thus, we simplify the
challenging task of full scene lighting estimation by focusing on the
environment lighting at the object's placement. Specifically, we capture a 360
degrees reconstructed radiance field of the scene at the location and fine-tune
a diffusion model to complete the lighting. Building on these advances, we
propose ComGS, a novel 3D object-scene composition framework. Our method
achieves high-quality, real-time rendering at around 28 FPS, produces visually
harmonious results with vivid shadows, and requires only 36 seconds for
editing. Code and dataset are available at
https://nju-3dv.github.io/projects/ComGS/.



---

## SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D  Novel View Synthesis


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-08 | Jipeng Lyu, Jiahua Dong, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2510.06694v1){: .btn .btn-green } |

**Abstract**: Persistent dynamic scene modeling for tracking and novel-view synthesis
remains challenging due to the difficulty of capturing accurate deformations
while maintaining computational efficiency. We propose SCas4D, a cascaded
optimization framework that leverages structural patterns in 3D Gaussian
Splatting for dynamic scenes. The key idea is that real-world deformations
often exhibit hierarchical patterns, where groups of Gaussians share similar
transformations. By progressively refining deformations from coarse part-level
to fine point-level, SCas4D achieves convergence within 100 iterations per time
frame and produces results comparable to existing methods with only
one-twentieth of the training iterations. The approach also demonstrates
effectiveness in self-supervised articulated object segmentation, novel view
synthesis, and dense point tracking tasks.

Comments:
- Published in Transactions on Machine Learning Research (06/2025)

---

## Capture and Interact: Rapid 3D Object Acquisition and Rendering with  Gaussian Splatting in Unity

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-08 | Islomjon Shukhratov, Sergey Gorinsky | cs.GR | [PDF](http://arxiv.org/pdf/2510.06802v1){: .btn .btn-green } |

**Abstract**: Capturing and rendering three-dimensional (3D) objects in real time remain a
significant challenge, yet hold substantial potential for applications in
augmented reality, digital twin systems, remote collaboration and prototyping.
We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS)
to enable rapid acquisition and interactive rendering of real-world objects
using a mobile device, cloud processing and a local computer. Users scan an
object with a smartphone video, upload it for automated 3D reconstruction, and
visualize it interactively in Unity at an average of 150 frames per second
(fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and
Unity rendering to support real-time telepresence. Our experiments show that
the pipeline processes scans in approximately 10 minutes on a graphics
processing unit (GPU) achieving real-time rendering on the laptop.



---

## Generating Surface for Text-to-3D using 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-08 | Huanning Dong, Fan Li, Ping Kuang, Jianwen Min | cs.CV | [PDF](http://arxiv.org/pdf/2510.06967v1){: .btn .btn-green } |

**Abstract**: Recent advancements in Text-to-3D modeling have shown significant potential
for the creation of 3D content. However, due to the complex geometric shapes of
objects in the natural world, generating 3D content remains a challenging task.
Current methods either leverage 2D diffusion priors to recover 3D geometry, or
train the model directly based on specific 3D representations. In this paper,
we propose a novel method named DirectGaussian, which focuses on generating the
surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize
conditional text generation models and the surface of a 3D object is rendered
by 2D Gaussian splatting with multi-view normal and texture priors. For
multi-view geometric consistency problems, DirectGaussian incorporates
curvature constraints on the generated surface during optimization process.
Through extensive experiments, we demonstrate that our framework is capable of
achieving diverse and high-fidelity 3D content creation.



---

## RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy  Reduction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-08 | Leshu Li, Jiayin Qin, Jie Peng, Zishen Wan, Huaizhi Qu, Ye Han, Pingqing Zheng, Hongsen Zhang, Yu Cao, Tianlong Chen, Yang Katie Zhao | cs.AR | [PDF](http://arxiv.org/pdf/2510.06644v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping
(SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering
efficiency and accuracy, but have not yet been adopted in resource-constrained
edge devices due to insufficient speed. Addressing this, we identify notable
redundancies across the SLAM pipeline for acceleration. While conceptually
straightforward, practical approaches are required to minimize the overhead
associated with identifying and eliminating these redundancies. In response, we
propose RTGS, an algorithm-hardware co-design framework that comprehensively
reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the
overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline.
On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to
remove the redundant Gaussians by reusing gradients computed during
backpropagation; and (2) a dynamic downsampling technique that directly reuses
the keyframe identification and alpha computing steps to eliminate redundant
pixels. On the hardware side, we propose (1) a subtile-level streaming strategy
and a pixel-level pairwise scheduling strategy that mitigates workload
imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration
information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates
the rendering backpropagation by reusing intermediate data computed during
rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory
accesses caused by atomic operations while enabling pipelined aggregation.
Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on
four datasets and three algorithms, with up to 82.5x energy efficiency over the
baseline and negligible quality loss. Code is available at
https://github.com/UMN-ZhaoLab/RTGS.

Comments:
- Accepted by MICRO2025

---

## ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head  Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-07 | Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du | cs.CV | [PDF](http://arxiv.org/pdf/2510.05488v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time
rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on
tens of thousands of 3D Gaussian points (Gaussians), with the number of
Gaussians fixed after training. However, many practical applications require
adjustable levels of detail (LOD) to balance rendering efficiency and visual
quality. In this work, we propose "ArchitectHead", the first framework for
creating 3D Gaussian head avatars that support continuous control over LOD. Our
key idea is to parameterize the Gaussians in a 2D UV feature space and propose
a UV feature field composed of multi-level learnable feature maps to encode
their latent features. A lightweight neural network-based decoder then
transforms these latent features into 3D Gaussian attributes for rendering.
ArchitectHead controls the number of Gaussians by dynamically resampling
feature maps from the UV feature field at the desired resolutions. This method
enables efficient and continuous control of LOD without retraining.
Experimental results show that ArchitectHead achieves state-of-the-art (SOTA)
quality in self and cross-identity reenactment tasks at the highest LOD, while
maintaining near SOTA performance at lower LODs. At the lowest LOD, our method
uses only 6.2\% of the Gaussians while the quality degrades moderately (L1 Loss
+7.9\%, PSNR --0.97\%, SSIM --0.6\%, LPIPS Loss +24.1\%), and the rendering
speed nearly doubles.



---

## Active Next-Best-View Optimization for Risk-Averse Path Planning


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-07 | Amirhossein Mollaei Khass, Guangyi Liu, Vivek Pandey, Wen Jiang, Boshu Lei, Kostas Daniilidis, Nader Motee | cs.RO | [PDF](http://arxiv.org/pdf/2510.06481v1){: .btn .btn-green } |

**Abstract**: Safe navigation in uncertain environments requires planning methods that
integrate risk aversion with active perception. In this work, we present a
unified framework that refines a coarse reference path by constructing
tail-sensitive risk maps from Average Value-at-Risk statistics on an
online-updated 3D Gaussian-splat Radiance Field. These maps enable the
generation of locally safe and feasible trajectories. In parallel, we formulate
Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose
manifold, where Riemannian gradient descent maximizes an expected information
gain objective to reduce uncertainty most critical for imminent motion. Our
approach advances the state-of-the-art by coupling risk-averse path refinement
with NBV planning, while introducing scalable gradient decompositions that
support efficient online updates in complex environments. We demonstrate the
effectiveness of the proposed framework through extensive computational
studies.



---

## Optimized Minimal 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-04 | Minseo Lee, Byeonghyeon Lee, Lucas Yunkyu Lee, Eunsoo Lee, Sangmin Kim, Seunghyeon Song, Joo Chan Lee, Jong Hwan Ko, Jaesik Park, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2510.03857v1){: .btn .btn-green } |

**Abstract**: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

Comments:
- 17 pages, 8 figures

---

## SketchPlan: Diffusion Based Drone Planning From Human Sketches

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Sixten Norelius, Aaron O. Feldman, Mac Schwager | cs.CV | [PDF](http://arxiv.org/pdf/2510.03545v1){: .btn .btn-green } |

**Abstract**: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

Comments:
- Code available at https://github.com/sixnor/SketchPlan

---

## Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled  Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Zhiting Mei, Ola Shorinwa, Anirudha Majumdar | cs.CV | [PDF](http://arxiv.org/pdf/2510.03104v1){: .btn .btn-green } |

**Abstract**: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.



---

## FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Yibin Zhao, Yihan Pan, Jun Nan, Jianjun Yi | cs.CV | [PDF](http://arxiv.org/pdf/2510.02691v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.



---

## GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Xinran Zhang, Hanqi Zhu, Yifan Duan, Yanyong Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2510.02884v1){: .btn .btn-green } |

**Abstract**: Constructing and sharing 3D maps is essential for many applications,
including autonomous driving and augmented reality. Recently, 3D Gaussian
splatting has emerged as a promising approach for accurate 3D reconstruction.
However, a practical map-sharing system that features high-fidelity, continuous
updates, and network efficiency remains elusive. To address these challenges,
we introduce GS-Share, a photorealistic map-sharing system with a compact
representation. The core of GS-Share includes anchor-based global map
construction, virtual-image-based map enhancement, and incremental map update.
We evaluate GS-Share against state-of-the-art methods, demonstrating that our
system achieves higher fidelity, particularly for extrapolated views, with
improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively.
Furthermore, GS-Share is significantly more compact, reducing map transmission
overhead by 36%.

Comments:
- 11 pages, 11 figures

---

## ROGR: Relightable 3D Objects using Generative Relighting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Jiapeng Tang, Matthew Lavine, Dor Verbin, Stephan J. Garbin, Matthias Nießner, Ricardo Martin Brualla, Pratul P. Srinivasan, Philipp Henzler | cs.CV | [PDF](http://arxiv.org/pdf/2510.03163v1){: .btn .btn-green } |

**Abstract**: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

Comments:
- NeurIPS 2025 Spotlight. Project page:
  https://tangjiapeng.github.io/ROGR

---

## From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.02732v1){: .btn .btn-green } |

**Abstract**: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.



---

## SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang | cs.RO | [PDF](http://arxiv.org/pdf/2510.02469v1){: .btn .btn-green } |

**Abstract**: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/



---

## MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust  Physics-Based Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Changmin Lee, Jihyun Lee, Tae-Kyun Kim | cs.GR | [PDF](http://arxiv.org/pdf/2510.01619v1){: .btn .btn-green } |

**Abstract**: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

Comments:
- Accepted to NeurIPS 2025

---

## Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy  Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Georgios Kouros, Minye Wu, Tinne Tuytelaars | cs.GR | [PDF](http://arxiv.org/pdf/2510.02069v1){: .btn .btn-green } |

**Abstract**: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.



---

## ROI-GS: Interest-based Local Quality 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Quoc-Anh Bui, Gilles Rougeron, Géraldine Morin, Simone Gasparini | cs.GR | [PDF](http://arxiv.org/pdf/2510.01978v1){: .btn .btn-green } |

**Abstract**: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

Comments:
- 4 pages, 3 figures, 2 tables

---

## GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object  Morphing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Mengtian Li, Yunshu Bai, Yimin Chu, Yijun Shen, Zhongmei Li, Weifeng Ge, Zhifeng Xie, Chaofeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2510.02034v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

Comments:
- Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

---

## 4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Lei Liu, Can Wang, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01991v1){: .btn .btn-green } |

**Abstract**: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.



---

## Performance-Guided Refinement for Visual Aerial Navigation using  Editable Gaussian Splatting in FalconGym 2.0

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Yan Miao, Ege Yuceel, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2510.02248v1){: .btn .btn-green } |

**Abstract**: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.



---

## StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided  Illusions

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, Wei-Chen Chiu | cs.CV | [PDF](http://arxiv.org/pdf/2510.02314v1){: .btn .btn-green } |

**Abstract**: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

Comments:
- ICCV 2025. Project page: https://hentci.github.io/stealthattack/

---

## GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for  Mobile Robotics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Diram Tabaa, Gianni Di Caro | cs.RO | [PDF](http://arxiv.org/pdf/2510.01848v1){: .btn .btn-green } |

**Abstract**: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.



---

## LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for  Large-Scale Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Sheng-Hsiang Hung, Ting-Yu Yen, Wei-Fang Sun, Simon See, Shih-Hsuan Hung, Hung-Kuo Chu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01767v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.



---

## Instant4D: 4D Gaussian Splatting in Minutes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-01 | Zhanpeng Luo, Haoxi Ran, Li Lu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01119v1){: .btn .btn-green } |

**Abstract**: Dynamic view synthesis has seen significant advances, yet reconstructing
scenes from uncalibrated, casual video remains challenging due to slow
optimization and complex parameter estimation. In this work, we present
Instant4D, a monocular reconstruction system that leverages native 4D
representation to efficiently process casual video sequences within minutes,
without calibrated cameras or depth sensors. Our method begins with geometric
recovery through deep visual SLAM, followed by grid pruning to optimize scene
representation. Our design significantly reduces redundancy while maintaining
geometric integrity, cutting model size to under 10% of its original footprint.
To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian
representation, achieving a 30x speed-up and reducing training time to within
two minutes, while maintaining competitive performance across several
benchmarks. Our method reconstruct a single video within 10 minutes on the
Dycheck dataset or for a typical 200-frame video. We further apply our model to
in-the-wild videos, showcasing its generalizability. Our project website is
published at https://instant4d.github.io/.

Comments:
- Accepted by NeurIPS 25

---

## Multi-level Dynamic Style Transfer for NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-01 | Zesheng Li, Shuaibo Li, Wei Ma, Jianwei Guo, Hongbin Zha | cs.CV | [PDF](http://arxiv.org/pdf/2510.00592v1){: .btn .btn-green } |

**Abstract**: As the application of neural radiance fields (NeRFs) in various 3D vision
tasks continues to expand, numerous NeRF-based style transfer techniques have
been developed. However, existing methods typically integrate style statistics
into the original NeRF pipeline, often leading to suboptimal results in both
content preservation and artistic stylization. In this paper, we present
multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that
reengineers the NeRF pipeline specifically for stylization and incorporates an
innovative dynamic style injection module. Particularly, we propose a
multi-level feature adaptor that helps generate a multi-level feature grid
representation from the content radiance field, effectively capturing the
multi-scale spatial structure of the scene. In addition, we present a dynamic
style injection module that learns to extract relevant style features and
adaptively integrates them into the content patterns. The stylized multi-level
features are then transformed into the final stylized view through our proposed
multi-level cascade decoder. Furthermore, we extend our 3D style transfer
method to support omni-view style transfer using 3D style references. Extensive
experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D
style transfer, preserving multi-scale spatial structures while effectively
transferring stylistic characteristics.

Comments:
- Accepted by Computational Visual Media Journal (CVMJ)
