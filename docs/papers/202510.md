---
layout: default
title: October 2025
parent: Papers
nav_order: 202510
---

<!---metadata--->


## ROGR: Relightable 3D Objects using Generative Relighting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Jiapeng Tang, Matthew Lavine, Dor Verbin, Stephan J. Garbin, Matthias Nießner, Ricardo Martin Brualla, Pratul P. Srinivasan, Philipp Henzler | cs.CV | [PDF](http://arxiv.org/pdf/2510.03163v1){: .btn .btn-green } |

**Abstract**: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

Comments:
- NeurIPS 2025 Spotlight. Project page:
  https://tangjiapeng.github.io/ROGR

---

## FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Yibin Zhao, Yihan Pan, Jun Nan, Jianjun Yi | cs.CV | [PDF](http://arxiv.org/pdf/2510.02691v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.



---

## Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled  Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Zhiting Mei, Ola Shorinwa, Anirudha Majumdar | cs.CV | [PDF](http://arxiv.org/pdf/2510.03104v1){: .btn .btn-green } |

**Abstract**: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.



---

## GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Xinran Zhang, Hanqi Zhu, Yifan Duan, Yanyong Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2510.02884v1){: .btn .btn-green } |

**Abstract**: Constructing and sharing 3D maps is essential for many applications,
including autonomous driving and augmented reality. Recently, 3D Gaussian
splatting has emerged as a promising approach for accurate 3D reconstruction.
However, a practical map-sharing system that features high-fidelity, continuous
updates, and network efficiency remains elusive. To address these challenges,
we introduce GS-Share, a photorealistic map-sharing system with a compact
representation. The core of GS-Share includes anchor-based global map
construction, virtual-image-based map enhancement, and incremental map update.
We evaluate GS-Share against state-of-the-art methods, demonstrating that our
system achieves higher fidelity, particularly for extrapolated views, with
improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively.
Furthermore, GS-Share is significantly more compact, reducing map transmission
overhead by 36%.

Comments:
- 11 pages, 11 figures

---

## From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.02732v1){: .btn .btn-green } |

**Abstract**: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.



---

## LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for  Large-Scale Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Sheng-Hsiang Hung, Ting-Yu Yen, Wei-Fang Sun, Simon See, Shih-Hsuan Hung, Hung-Kuo Chu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01767v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.



---

## 4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Lei Liu, Can Wang, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01991v1){: .btn .btn-green } |

**Abstract**: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.



---

## GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object  Morphing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Mengtian Li, Yunshu Bai, Yimin Chu, Yijun Shen, Zhongmei Li, Weifeng Ge, Zhifeng Xie, Chaofeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2510.02034v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

Comments:
- Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

---

## ROI-GS: Interest-based Local Quality 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Quoc-Anh Bui, Gilles Rougeron, Géraldine Morin, Simone Gasparini | cs.GR | [PDF](http://arxiv.org/pdf/2510.01978v1){: .btn .btn-green } |

**Abstract**: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

Comments:
- 4 pages, 3 figures, 2 tables

---

## Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy  Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Georgios Kouros, Minye Wu, Tinne Tuytelaars | cs.GR | [PDF](http://arxiv.org/pdf/2510.02069v1){: .btn .btn-green } |

**Abstract**: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.



---

## SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang | cs.RO | [PDF](http://arxiv.org/pdf/2510.02469v1){: .btn .btn-green } |

**Abstract**: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/



---

## MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust  Physics-Based Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Changmin Lee, Jihyun Lee, Tae-Kyun Kim | cs.GR | [PDF](http://arxiv.org/pdf/2510.01619v1){: .btn .btn-green } |

**Abstract**: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

Comments:
- Accepted to NeurIPS 2025

---

## GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for  Mobile Robotics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Diram Tabaa, Gianni Di Caro | cs.RO | [PDF](http://arxiv.org/pdf/2510.01848v1){: .btn .btn-green } |

**Abstract**: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.



---

## StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided  Illusions

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, Wei-Chen Chiu | cs.CV | [PDF](http://arxiv.org/pdf/2510.02314v1){: .btn .btn-green } |

**Abstract**: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

Comments:
- ICCV 2025. Project page: https://hentci.github.io/stealthattack/

---

## Performance-Guided Refinement for Visual Aerial Navigation using  Editable Gaussian Splatting in FalconGym 2.0

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Yan Miao, Ege Yuceel, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2510.02248v1){: .btn .btn-green } |

**Abstract**: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.



---

## Instant4D: 4D Gaussian Splatting in Minutes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-01 | Zhanpeng Luo, Haoxi Ran, Li Lu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01119v1){: .btn .btn-green } |

**Abstract**: Dynamic view synthesis has seen significant advances, yet reconstructing
scenes from uncalibrated, casual video remains challenging due to slow
optimization and complex parameter estimation. In this work, we present
Instant4D, a monocular reconstruction system that leverages native 4D
representation to efficiently process casual video sequences within minutes,
without calibrated cameras or depth sensors. Our method begins with geometric
recovery through deep visual SLAM, followed by grid pruning to optimize scene
representation. Our design significantly reduces redundancy while maintaining
geometric integrity, cutting model size to under 10% of its original footprint.
To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian
representation, achieving a 30x speed-up and reducing training time to within
two minutes, while maintaining competitive performance across several
benchmarks. Our method reconstruct a single video within 10 minutes on the
Dycheck dataset or for a typical 200-frame video. We further apply our model to
in-the-wild videos, showcasing its generalizability. Our project website is
published at https://instant4d.github.io/.

Comments:
- Accepted by NeurIPS 25

---

## Multi-level Dynamic Style Transfer for NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-01 | Zesheng Li, Shuaibo Li, Wei Ma, Jianwei Guo, Hongbin Zha | cs.CV | [PDF](http://arxiv.org/pdf/2510.00592v1){: .btn .btn-green } |

**Abstract**: As the application of neural radiance fields (NeRFs) in various 3D vision
tasks continues to expand, numerous NeRF-based style transfer techniques have
been developed. However, existing methods typically integrate style statistics
into the original NeRF pipeline, often leading to suboptimal results in both
content preservation and artistic stylization. In this paper, we present
multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that
reengineers the NeRF pipeline specifically for stylization and incorporates an
innovative dynamic style injection module. Particularly, we propose a
multi-level feature adaptor that helps generate a multi-level feature grid
representation from the content radiance field, effectively capturing the
multi-scale spatial structure of the scene. In addition, we present a dynamic
style injection module that learns to extract relevant style features and
adaptively integrates them into the content patterns. The stylized multi-level
features are then transformed into the final stylized view through our proposed
multi-level cascade decoder. Furthermore, we extend our 3D style transfer
method to support omni-view style transfer using 3D style references. Extensive
experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D
style transfer, preserving multi-scale spatial structures while effectively
transferring stylistic characteristics.

Comments:
- Accepted by Computational Visual Media Journal (CVMJ)
