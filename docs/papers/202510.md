---
layout: default
title: October 2025
parent: Papers
nav_order: 202510
---

<!---metadata--->


## StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided  Illusions

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, Wei-Chen Chiu | cs.CV | [PDF](http://arxiv.org/pdf/2510.02314v1){: .btn .btn-green } |

**Abstract**: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

Comments:
- ICCV 2025. Project page: https://hentci.github.io/stealthattack/

---

## GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for  Mobile Robotics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Diram Tabaa, Gianni Di Caro | cs.RO | [PDF](http://arxiv.org/pdf/2510.01848v1){: .btn .btn-green } |

**Abstract**: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.



---

## Performance-Guided Refinement for Visual Aerial Navigation using  Editable Gaussian Splatting in FalconGym 2.0

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Yan Miao, Ege Yuceel, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2510.02248v1){: .btn .btn-green } |

**Abstract**: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.



---

## MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust  Physics-Based Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Changmin Lee, Jihyun Lee, Tae-Kyun Kim | cs.GR | [PDF](http://arxiv.org/pdf/2510.01619v1){: .btn .btn-green } |

**Abstract**: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

Comments:
- Accepted to NeurIPS 2025

---

## LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for  Large-Scale Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Sheng-Hsiang Hung, Ting-Yu Yen, Wei-Fang Sun, Simon See, Shih-Hsuan Hung, Hung-Kuo Chu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01767v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.



---

## Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy  Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Georgios Kouros, Minye Wu, Tinne Tuytelaars | cs.GR | [PDF](http://arxiv.org/pdf/2510.02069v1){: .btn .btn-green } |

**Abstract**: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.



---

## ROI-GS: Interest-based Local Quality 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Quoc-Anh Bui, Gilles Rougeron, Géraldine Morin, Simone Gasparini | cs.GR | [PDF](http://arxiv.org/pdf/2510.01978v1){: .btn .btn-green } |

**Abstract**: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

Comments:
- 4 pages, 3 figures, 2 tables

---

## GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object  Morphing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Mengtian Li, Yunshu Bai, Yimin Chu, Yijun Shen, Zhongmei Li, Weifeng Ge, Zhifeng Xie, Chaofeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2510.02034v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

Comments:
- Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

---

## 4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Lei Liu, Can Wang, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01991v1){: .btn .btn-green } |

**Abstract**: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.



---

## Instant4D: 4D Gaussian Splatting in Minutes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-01 | Zhanpeng Luo, Haoxi Ran, Li Lu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01119v1){: .btn .btn-green } |

**Abstract**: Dynamic view synthesis has seen significant advances, yet reconstructing
scenes from uncalibrated, casual video remains challenging due to slow
optimization and complex parameter estimation. In this work, we present
Instant4D, a monocular reconstruction system that leverages native 4D
representation to efficiently process casual video sequences within minutes,
without calibrated cameras or depth sensors. Our method begins with geometric
recovery through deep visual SLAM, followed by grid pruning to optimize scene
representation. Our design significantly reduces redundancy while maintaining
geometric integrity, cutting model size to under 10% of its original footprint.
To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian
representation, achieving a 30x speed-up and reducing training time to within
two minutes, while maintaining competitive performance across several
benchmarks. Our method reconstruct a single video within 10 minutes on the
Dycheck dataset or for a typical 200-frame video. We further apply our model to
in-the-wild videos, showcasing its generalizability. Our project website is
published at https://instant4d.github.io/.

Comments:
- Accepted by NeurIPS 25

---

## Multi-level Dynamic Style Transfer for NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-01 | Zesheng Li, Shuaibo Li, Wei Ma, Jianwei Guo, Hongbin Zha | cs.CV | [PDF](http://arxiv.org/pdf/2510.00592v1){: .btn .btn-green } |

**Abstract**: As the application of neural radiance fields (NeRFs) in various 3D vision
tasks continues to expand, numerous NeRF-based style transfer techniques have
been developed. However, existing methods typically integrate style statistics
into the original NeRF pipeline, often leading to suboptimal results in both
content preservation and artistic stylization. In this paper, we present
multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that
reengineers the NeRF pipeline specifically for stylization and incorporates an
innovative dynamic style injection module. Particularly, we propose a
multi-level feature adaptor that helps generate a multi-level feature grid
representation from the content radiance field, effectively capturing the
multi-scale spatial structure of the scene. In addition, we present a dynamic
style injection module that learns to extract relevant style features and
adaptively integrates them into the content patterns. The stylized multi-level
features are then transformed into the final stylized view through our proposed
multi-level cascade decoder. Furthermore, we extend our 3D style transfer
method to support omni-view style transfer using 3D style references. Extensive
experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D
style transfer, preserving multi-scale spatial structures while effectively
transferring stylistic characteristics.

Comments:
- Accepted by Computational Visual Media Journal (CVMJ)
