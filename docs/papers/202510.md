---
layout: default
title: October 2025
parent: Papers
nav_order: 202510
---

<!---metadata--->


## Leveraging Learned Image Prior for 3D Gaussian Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Seungjoo Shin, Jaesik Park, Sunghyun Cho | cs.CV | [PDF](http://arxiv.org/pdf/2510.14705v1){: .btn .btn-green } |

**Abstract**: Compression techniques for 3D Gaussian Splatting (3DGS) have recently
achieved considerable success in minimizing storage overhead for 3D Gaussians
while preserving high rendering quality. Despite the impressive storage
reduction, the lack of learned priors restricts further advances in the
rate-distortion trade-off for 3DGS compression tasks. To address this, we
introduce a novel 3DGS compression framework that leverages the powerful
representational capacity of learned image priors to recover
compression-induced quality degradation. Built upon initially compressed
Gaussians, our restoration network effectively models the compression artifacts
in the image space between degraded and original Gaussians. To enhance the
rate-distortion performance, we provide coarse rendering residuals into the
restoration network as side information. By leveraging the supervision of
restored images, the compressed Gaussians are refined, resulting in a highly
compact representation with enhanced rendering performance. Our framework is
designed to be compatible with existing Gaussian compression methods, making it
broadly applicable across different baselines. Extensive experiments validate
the effectiveness of our framework, demonstrating superior rate-distortion
performance and outperforming the rendering quality of state-of-the-art 3DGS
compression methods while requiring substantially less storage.

Comments:
- Accepted to ICCV 2025 Workshop on ECLR

---

## GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and  Geometric Filtering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang | cs.CV | [PDF](http://arxiv.org/pdf/2510.14270v1){: .btn .btn-green } |

**Abstract**: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.



---

## Virtually Being: Customizing Camera-Controllable Video Diffusion Models  with Multi-View Performance Captures

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Levent Ta≈üel, Yiwei Zhao, Ryan Burgert, Mingming He, Oliver Hermann, Oliver Pilarski, Rahul Garg, Paul Debevec, Ning Yu | cs.CV | [PDF](http://arxiv.org/pdf/2510.14179v1){: .btn .btn-green } |

**Abstract**: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

Comments:
- Accepted to SIGGRAPH Asia 2025

---

## BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian  Splatting Training on GPU

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-16 | Junyi Wu, Jiaming Xu, Jinhao Li, Yongkang Zhou, Jiayi Pan, Xingyang Li, Guohao Dai | cs.CV | [PDF](http://arxiv.org/pdf/2510.14564v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

Comments:
- Accepted by ASP-DAC 2026

---

## VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a  Video Generator


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Truong, Federico Tombari, Konrad Schindler | cs.CV | [PDF](http://arxiv.org/pdf/2510.13454v1){: .btn .btn-green } |

**Abstract**: The rapid progress of large, pretrained models for both visual content
generation and 3D reconstruction opens up new possibilities for text-to-3D
generation. Intuitively, one could obtain a formidable 3D scene generator if
one were able to combine the power of a modern latent text-to-video model as
"generator" with the geometric abilities of a recent (feedforward) 3D
reconstruction system as "decoder". We introduce VIST3A, a general framework
that does just that, addressing two main challenges. First, the two components
must be joined in a way that preserves the rich knowledge encoded in their
weights. We revisit model stitching, i.e., we identify the layer in the 3D
decoder that best matches the latent representation produced by the
text-to-video generator and stitch the two parts together. That operation
requires only a small dataset and no labels. Second, the text-to-video
generator must be aligned with the stitched 3D decoder, to ensure that the
generated latents are decodable into consistent, perceptually convincing 3D
scene geometry. To that end, we adapt direct reward finetuning, a popular
technique for human preference alignment. We evaluate the proposed VIST3A
approach with different video generators and 3D reconstruction models. All
tested pairings markedly improve over prior text-to-3D models that output
Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also
enables high-quality text-to-pointmap generation.

Comments:
- Project page: https://gohyojun15.github.io/VIST3A/

---

## Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Siddharth Tourani, Jayaram Reddy, Akash Kumbar, Satyajit Tourani, Nishant Goyal, Madhava Krishna, N. Dinesh Reddy, Muhammad Haris Khan | cs.CV | [PDF](http://arxiv.org/pdf/2510.13381v1){: .btn .btn-green } |

**Abstract**: Dynamic scene rendering and reconstruction play a crucial role in computer
vision and augmented reality. Recent methods based on 3D Gaussian Splatting
(3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban
scenes they require both camera and LiDAR data, ground-truth 3D segmentations
and motion data in the form of tracklets or pre-defined object templates such
as SMPL. In this work, we explore whether a combination of 2D object agnostic
priors in the form of depth and point tracking coupled with a signed distance
function (SDF) representation for dynamic objects can be used to relax some of
these requirements. We present a novel approach that integrates Signed Distance
Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust
object representation by harnessing the strengths of both methods. Our unified
optimization framework enhances the geometric accuracy of 3D Gaussian splatting
and improves deformation modeling within the SDF, resulting in a more adaptable
and precise representation. We demonstrate that our method achieves
state-of-the-art performance in rendering metrics even without LiDAR data on
urban scenes. When incorporating LiDAR, our approach improved further in
reconstructing and generating novel views across diverse object categories,
without ground-truth 3D motion annotation. Additionally, our method enables
various scene editing tasks, including scene decomposition, and scene
composition.

Comments:
- Accepted at ICCV-2025, project page: https://dynamic-ugsdf.github.io/

---

## STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client  Selection and Power Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Zhen Li, Xibin Jin, Guoliang Li, Shuai Wang, Miaowen Wen, Huseyin Arslan, Derrick Wing Kwan Ng, Chengzhong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.13186v1){: .btn .btn-green } |

**Abstract**: Edge Gaussian splatting (EGS), which aggregates data from distributed clients
and trains a global GS model at the edge server, is an emerging paradigm for
scene reconstruction. Unlike traditional edge resource management methods that
emphasize communication throughput or general-purpose learning performance, EGS
explicitly aims to maximize the GS qualities, rendering existing approaches
inapplicable. To address this problem, this paper formulates a novel
GS-oriented objective function that distinguishes the heterogeneous view
contributions of different clients. However, evaluating this function in turn
requires clients' images, leading to a causality dilemma. To this end, this
paper further proposes a sample-then-transmit EGS (or STT-GS for short)
strategy, which first samples a subset of images as pilot data from each client
for loss prediction. Based on the first-stage evaluation, communication
resources are then prioritized towards more valuable clients. To achieve
efficient sampling, a feature-domain clustering (FDC) scheme is proposed to
select the most representative data and pilot transmission time minimization
(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint
client selection and power control (JCSPC) framework to maximize the
GS-oriented function under communication resource constraints. Despite the
nonconvexity of the problem, we propose a low-complexity efficient solution
based on the penalty alternating majorization minimization (PAMM) algorithm.
Experiments unveil that the proposed scheme significantly outperforms existing
benchmarks on real-world datasets. It is found that the GS-oriented objective
can be accurately predicted with low sampling ratios (e.g.,10%), and our method
achieves an excellent tradeoff between view contributions and communication
costs.



---

## Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Naruya Kondo, Yuto Asano, Yoichi Ochiai | cs.CG | [PDF](http://arxiv.org/pdf/2510.13978v1){: .btn .btn-green } |

**Abstract**: We present Instant Skinned Gaussian Avatars, a real-time and cross-platform
3D avatar system. Many approaches have been proposed to animate Gaussian
Splatting, but they often require camera arrays, long preprocessing times, or
high-end GPUs. Some methods attempt to convert Gaussian Splatting into
mesh-based representations, achieving lightweight performance but sacrificing
visual fidelity. In contrast, our system efficiently animates Gaussian
Splatting by leveraging parallel splat-wise processing to dynamically follow
the underlying skinned mesh in real time while preserving high visual fidelity.
From smartphone-based 3D scanning to on-device preprocessing, the entire
process takes just around five minutes, with the avatar generation step itself
completed in only about 30 seconds. Our system enables users to instantly
transform their real-world appearance into a 3D avatar, making it ideal for
seamless integration with social media and metaverse applications. Website:
https://sites.google.com/view/gaussian-vrm

Comments:
- Accepted to SUI 2025 Demo Track

---

## Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from  Unstructured Phone Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-15 | Emanuel Garbin, Guy Adam, Oded Krams, Zohar Barzelay, Eran Guendelman, Michael Schwarz, Moran Vatelmacher, Yigal Shenkman, Eli Peker, Itai Druker, Uri Patish, Yoav Blum, Max Bluvstein, Junxuan Li, Rawal Khirodkar, Shunsuke Saito | cs.CV | [PDF](http://arxiv.org/pdf/2510.14081v1){: .btn .btn-green } |

**Abstract**: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.



---

## SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Haithem Turki, Qi Wu, Xin Kang, Janick Martinez Esturo, Shengyu Huang, Ruilong Li, Zan Gojcic, Riccardo de Lutio | cs.CV | [PDF](http://arxiv.org/pdf/2510.12901v2){: .btn .btn-green } |

**Abstract**: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

Comments:
- Project page: https://research.nvidia.com/labs/sil/projects/simuli

---

## Hybrid Gaussian Splatting for Novel Urban View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Mohamed Omran, Farhad Zanjani, Davide Abati, Jens Petersen, Amirhossein Habibian | cs.CV | [PDF](http://arxiv.org/pdf/2510.12308v1){: .btn .btn-green } |

**Abstract**: This paper describes the Qualcomm AI Research solution to the RealADSim-NVS
challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge
concerns novel view synthesis in street scenes, and participants are required
to generate, starting from car-centric frames captured during some training
traversals, renders of the same urban environment as viewed from a different
traversal (e.g. different street lane or car direction). Our solution is
inspired by hybrid methods in scene generation and generative simulators
merging gaussian splatting and diffusion models, and it is composed of two
stages: First, we fit a 3D reconstruction of the scene and render novel views
as seen from the target cameras. Then, we enhance the resulting frames with a
dedicated single-step diffusion model. We discuss specific choices made in the
initialization of gaussian primitives as well as the finetuning of the enhancer
model and its training data curation. We report the performance of our model
design and we ablate its components in terms of novel view quality as measured
by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our
proposal reaches an aggregated score of 0.432, achieving the second place
overall.

Comments:
- ICCV 2025 RealADSim Workshop

---

## PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Ying A, Wenzhang Sun, Chang Zeng, Chunfeng Wang, Hao Li, Jianxun Cui | cs.CV | [PDF](http://arxiv.org/pdf/2510.12282v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet
current methods face a stark trade-off between fidelity and computational cost.
This inefficiency stems from their semantically agnostic design, which
allocates resources uniformly, treating static backgrounds and safety-critical
objects with equal importance. To address this, we introduce Priority-Adaptive
Gaussian Splatting (PAGS), a framework that injects task-aware semantic
priorities directly into the 3D reconstruction and rendering pipeline. PAGS
introduces two core contributions: (1) Semantically-Guided Pruning and
Regularization strategy, which employs a hybrid importance metric to
aggressively simplify non-critical scene elements while preserving fine-grained
details on objects vital for navigation. (2) Priority-Driven Rendering
pipeline, which employs a priority-based depth pre-pass to aggressively cull
occluded primitives and accelerate the final shading computations. Extensive
experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves
exceptional reconstruction quality, particularly on safety-critical objects,
while significantly reducing training time and boosting rendering speeds to
over 350 FPS.



---

## UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Yusen Xie, Zhenmin Huang, Jianhao Jiao, Dimitrios Kanoulas, Jun Ma | cs.CV | [PDF](http://arxiv.org/pdf/2510.12174v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose UniGS, a unified map representation and
differentiable framework for high-fidelity multimodal 3D reconstruction based
on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated
rasterization pipeline capable of rendering photo-realistic RGB images,
geometrically accurate depth maps, consistent surface normals, and semantic
logits simultaneously. We redesign the rasterization to render depth via
differentiable ray-ellipsoid intersection rather than using Gaussian centers,
enabling effective optimization of rotation and scale attribute through
analytic depth gradients. Furthermore, we derive the analytic gradient
formulation for surface normal rendering, ensuring geometric consistency among
reconstructed 3D scenes. To improve computational and storage efficiency, we
introduce a learnable attribute that enables differentiable pruning of
Gaussians with minimal contribution during training. Quantitative and
qualitative experiments demonstrate state-of-the-art reconstruction accuracy
across all modalities, validating the efficacy of our geometry-aware paradigm.
Source code and multimodal viewer will be available on GitHub.



---

## Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Fengzhi Guo, Chih-Chuan Hsu, Sihao Ding, Cheng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.12768v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D scenes from monocular input is fundamentally
under-constrained, with ambiguities arising from occlusion and extreme novel
views. While dynamic Gaussian Splatting offers an efficient representation,
vanilla models optimize all Gaussian primitives uniformly, ignoring whether
they are well or poorly observed. This limitation leads to motion drifts under
occlusion and degraded synthesis when extrapolating to unseen views. We argue
that uncertainty matters: Gaussians with recurring observations across views
and time act as reliable anchors to guide motion, whereas those with limited
visibility are treated as less reliable. To this end, we introduce USplat4D, a
novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates
reliable motion cues to enhance 4D reconstruction. Our key insight is to
estimate time-varying per-Gaussian uncertainty and leverages it to construct a
spatio-temporal graph for uncertainty-aware optimization. Experiments on
diverse real and synthetic datasets show that explicitly modeling uncertainty
consistently improves dynamic Gaussian Splatting models, yielding more stable
geometry under occlusion and high-quality synthesis at extreme viewpoints.

Comments:
- Project page: https://tamu-visual-ai.github.io/usplat4d/

---

## BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | An Zhao, Piaopiao Yu, Zhe Zhu, Mingqiang Wei | cs.CV | [PDF](http://arxiv.org/pdf/2510.12493v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene
reconstruction.However, reconstructing high-quality 3D scenes from
motion-blurred images caused by camera motion poses a significant challenge.The
performance of existing 3DGS-based deblurring methods are limited due to their
inherent mechanisms, such as extreme dependence on the accuracy of camera poses
and inability to effectively control erroneous Gaussian primitives
densification caused by motion blur.To solve these problems, we introduce a
novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D
scenes from motion-blurred images.BSGS contains two stages. First, Camera Pose
Refinement roughly optimizes camera poses to reduce motion-induced distortions.
Second, with fixed rough camera poses, Global RigidTransformation further
corrects motion-induced blur distortions.To alleviate multi-subframe gradient
conflicts, we propose a subframe gradient aggregation strategy to optimize both
stages.Furthermore, a space-time bi-stage optimization strategy is introduced
to dynamically adjust primitive densification thresholds and prevent premature
noisy Gaussian generation in blurred regions. Comprehensive experiments verify
the effectiveness of our proposed deblurring method and show its superiority
over the state of the arts.



---

## G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-14 | Junfeng Ni, Yixin Chen, Zhifei Yang, Yu Liu, Ruijie Lu, Song-Chun Zhu, Siyuan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2510.12099v1){: .btn .btn-green } |

**Abstract**: Despite recent advances in leveraging generative prior from pre-trained
diffusion models for 3D scene reconstruction, existing methods still face two
critical limitations. First, due to the lack of reliable geometric supervision,
they struggle to produce high-quality reconstructions even in observed regions,
let alone in unobserved areas. Second, they lack effective mechanisms to
mitigate multi-view inconsistencies in the generated images, leading to severe
shape-appearance ambiguities and degraded scene geometry. In this paper, we
identify accurate geometry as the fundamental prerequisite for effectively
exploiting generative models to enhance 3D scene reconstruction. We first
propose to leverage the prevalence of planar structures to derive accurate
metric-scale depth maps, providing reliable supervision in both observed and
unobserved regions. Furthermore, we incorporate this geometry guidance
throughout the generative pipeline to improve visibility mask estimation, guide
novel view selection, and enhance multi-view consistency when inpainting with
video diffusion models, resulting in accurate and consistent scene completion.
Extensive experiments on Replica, ScanNet++, and DeepBlending show that our
method consistently outperforms existing baselines in both geometry and
appearance reconstruction, particularly for unobserved regions. Moreover, our
method naturally supports single-view inputs and unposed videos, with strong
generalizability in both indoor and outdoor scenarios with practical real-world
applicability. The project page is available at
https://dali-jack.github.io/g4splat-web/.

Comments:
- Project page: https://dali-jack.github.io/g4splat-web/

---

## Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for  Uncertainty-Aware Sim-to-Real Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Maggie Wang, Stephen Tian, Aiden Swann, Ola Shorinwa, Jiajun Wu, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2510.11689v1){: .btn .btn-green } |

**Abstract**: Learning robotic manipulation policies directly in the real world can be
expensive and time-consuming. While reinforcement learning (RL) policies
trained in simulation present a scalable alternative, effective sim-to-real
transfer remains challenging, particularly for tasks that require precise
dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL
pipeline that combines vision-language model (VLM)-inferred physical parameter
estimates with interactive adaptation through uncertainty-aware fusion. Our
approach consists of three core components: (1) high-fidelity geometric
reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions
over physical parameters, and (3) online physical parameter estimation from
interaction data. Phys2Real conditions policies on interpretable physical
parameters, refining VLM predictions with online estimates via ensemble-based
uncertainty quantification. On planar pushing tasks of a T-block with varying
center of mass (CoM) and a hammer with an off-center mass distribution,
Phys2Real achieves substantial improvements over a domain randomization
baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%
in the challenging top-weighted T-block, and 15% faster average task completion
for hammer pushing. Ablation studies indicate that the combination of VLM and
interaction information is essential for success. Project website:
https://phys2real.github.io/ .



---

## MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent  Material Inference

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Wenyuan Zhang, Jimin Tang, Weiqi Zhang, Yi Fang, Yu-Shen Liu, Zhizhong Han | cs.CV | [PDF](http://arxiv.org/pdf/2510.11387v1){: .btn .btn-green } |

**Abstract**: Modeling reflections from 2D images is essential for photorealistic rendering
and novel view synthesis. Recent approaches enhance Gaussian primitives with
reflection-related material attributes to enable physically based rendering
(PBR) with Gaussian Splatting. However, the material inference often lacks
sufficient constraints, especially under limited environment modeling,
resulting in illumination aliasing and reduced generalization. In this work, we
revisit the problem from a multi-view perspective and show that multi-view
consistent material inference with more physically-based environment modeling
is key to learning accurate reflections with Gaussian Splatting. To this end,
we enforce 2D Gaussians to produce multi-view consistent material maps during
deferred shading. We also track photometric variations across views to identify
highly reflective regions, which serve as strong priors for reflection strength
terms. To handle indirect illumination caused by inter-object occlusions, we
further introduce an environment modeling strategy through ray tracing with
2DGS, enabling photorealistic rendering of indirect radiance. Experiments on
widely used benchmarks show that our method faithfully recovers both
illumination and geometry, achieving state-of-the-art rendering quality in
novel views synthesis.

Comments:
- Accepted by NeurIPS 2025. Project Page:
  https://wen-yuan-zhang.github.io/MaterialRefGS

---

## VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via  View Alignment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Qing Li, Huifang Feng, Xun Gong, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2510.11473v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently emerged as an efficient solution for
high-quality and real-time novel view synthesis. However, its capability for
accurate surface reconstruction remains underexplored. Due to the discrete and
unstructured nature of Gaussians, supervision based solely on image rendering
loss often leads to inaccurate geometry and inconsistent multi-view alignment.
In this work, we propose a novel method that enhances the geometric
representation of 3D Gaussians through view alignment (VA). Specifically, we
incorporate edge-aware image cues into the rendering loss to improve surface
boundary delineation. To enforce geometric consistency across views, we
introduce a visibility-aware photometric alignment loss that models occlusions
and encourages accurate spatial relationships among Gaussians. To further
mitigate ambiguities caused by lighting variations, we incorporate normal-based
constraints to refine the spatial orientation of Gaussians and improve local
surface estimation. Additionally, we leverage deep image feature embeddings to
enforce cross-view consistency, enhancing the robustness of the learned
geometry under varying viewpoints and illumination. Extensive experiments on
standard benchmarks demonstrate that our method achieves state-of-the-art
performance in both surface reconstruction and novel view synthesis. The source
code is available at https://github.com/LeoQLi/VA-GS.

Comments:
- Accepted by NeurIPS 2025

---

## Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event  Streams


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Takuya Nakabayashi, Navami Kairanda, Hideo Saito, Vladislav Golyanik | cs.CV | [PDF](http://arxiv.org/pdf/2510.11717v1){: .btn .btn-green } |

**Abstract**: Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.



---

## Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Yuxin Cheng, Binxiao Huang, Taiqiang Wu, Wenyong Zhou, Chenchen Ding, Zhengwu Liu, Graziano Chesi, Ngai Wong | cs.CV | [PDF](http://arxiv.org/pdf/2510.10993v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian inpainting, a critical technique for numerous applications in
virtual reality and multimedia, has made significant progress with pretrained
diffusion models. However, ensuring multi-view consistency, an essential
requirement for high-quality inpainting, remains a key challenge. In this work,
we present PAInpainter, a novel approach designed to advance 3D Gaussian
inpainting by leveraging perspective-aware content propagation and consistency
verification across multi-view inpainted images. Our method iteratively refines
inpainting and optimizes the 3D Gaussian representation with multiple views
adaptively sampled from a perspective graph. By propagating inpainted images as
prior information and verifying consistency across neighboring views,
PAInpainter substantially enhances global consistency and texture fidelity in
restored 3D scenes. Extensive experiments demonstrate the superiority of
PAInpainter over existing methods. Our approach achieves superior 3D inpainting
quality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and
NeRFiller datasets, respectively, highlighting its effectiveness and
generalization capability.



---

## GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in  Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-13 | Anastasiya Pechko, Piotr Borycki, Joanna Waczy≈Ñska, Daniel Barczyk, Agata Szyma≈Ñska, S≈Çawomir Tadeja, Przemys≈Çaw Spurek | cs.GR | [PDF](http://arxiv.org/pdf/2510.11878v1){: .btn .btn-green } |

**Abstract**: As the demand for immersive 3D content grows, the need for intuitive and
efficient interaction methods becomes paramount. Current techniques for
physically manipulating 3D content within Virtual Reality (VR) often face
significant limitations, including reliance on engineering-intensive processes
and simplified geometric representations, such as tetrahedral cages, which can
compromise visual fidelity and physical accuracy. In this paper, we introduce
\our{} (\textbf{G}aussian \textbf{S}platting for \textbf{V}irtual
\textbf{E}nvironment \textbf{R}endering and \textbf{S}cene \textbf{E}diting), a
novel method designed to overcome these challenges by directly integrating an
object's mesh with a Gaussian Splatting (GS) representation. Our approach
enables more precise surface approximation, leading to highly realistic
deformations and interactions. By leveraging existing 3D mesh assets, \our{}
facilitates seamless content reuse and simplifies the development workflow.
Moreover, our system is designed to be physics-engine-agnostic, granting
developers robust deployment flexibility. This versatile architecture delivers
a highly realistic, adaptable, and intuitive approach to interactive 3D
manipulation. We rigorously validate our method against the current
state-of-the-art technique that couples VR with GS in a comparative user study
involving 18 participants. Specifically, we demonstrate that our approach is
statistically significantly better for physics-aware stretching manipulation
and is also more consistent in other physics-based manipulations like twisting
and shaking. Further evaluation across various interactions and scenes confirms
that our method consistently delivers high and reliable performance, showing
its potential as a plausible alternative to existing methods.



---

## Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided  Framework

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-12 | Shanzhi Yin, Bolin Chen, Xinju Wu, Ru-Ling Liao, Jie Chen, Shiqi Wang, Yan Ye | eess.IV | [PDF](http://arxiv.org/pdf/2510.10492v1){: .btn .btn-green } |

**Abstract**: This paper proposes an efficient 3D avatar coding framework that leverages
compact human priors and canonical-to-target transformation to enable
high-quality 3D human avatar video compression at ultra-low bit rates. The
framework begins by training a canonical Gaussian avatar using articulated
splatting in a network-free manner, which serves as the foundation for avatar
appearance modeling. Simultaneously, a human-prior template is employed to
capture temporal body movements through compact parametric representations.
This decomposition of appearance and temporal evolution minimizes redundancy,
enabling efficient compression: the canonical avatar is shared across the
sequence, requiring compression only once, while the temporal parameters,
consisting of just 94 parameters per frame, are transmitted with minimal
bit-rate. For each frame, the target human avatar is generated by deforming
canonical avatar via Linear Blend Skinning transformation, facilitating
temporal coherent video reconstruction and novel view synthesis. Experimental
results demonstrate that the proposed method significantly outperforms
conventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting
compression method in terms of rate-distortion performance on mainstream
multi-view human video datasets, paving the way for seamless immersive
multimedia experiences in meta-verse applications.

Comments:
- 10 pages, 4 figures

---

## Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-12 | Xuankai Zhang, Junjin Xiao, Qing Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.10691v1){: .btn .btn-green } |

**Abstract**: This paper presents a unified framework that allows high-quality dynamic
Gaussian Splatting from both defocused and motion-blurred monocular videos. Due
to the significant difference between the formation processes of defocus blur
and motion blur, existing methods are tailored for either one of them, lacking
the ability to simultaneously deal with both of them. Although the two can be
jointly modeled as blur kernel-based convolution, the inherent difficulty in
estimating accurate blur kernels greatly limits the progress in this direction.
In this work, we go a step further towards this direction. Particularly, we
propose to estimate per-pixel reliable blur kernels using a blur prediction
network that exploits blur-related scene and camera information and is subject
to a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian
densification strategy to mitigate the lack of Gaussians for incomplete
regions, and boost the performance of novel view synthesis by incorporating
unseen view information to constrain scene optimization. Extensive experiments
show that our method outperforms the state-of-the-art methods in generating
photorealistic novel view synthesis from defocused and motion-blurred monocular
videos. Our code and trained model will be made publicly available.



---

## High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic  Manipulation Learning with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-12 | Haoyu Zhao, Cheng Zeng, Linghao Zhuang, Yaxi Zhao, Shengke Xue, Hao Wang, Xingyue Zhao, Zhongyu Li, Kehan Li, Siteng Huang, Mingxiu Chen, Xin Li, Deli Zhao, Hua Zou | cs.RO | [PDF](http://arxiv.org/pdf/2510.10637v1){: .btn .btn-green } |

**Abstract**: The scalability of robotic learning is fundamentally bottlenecked by the
significant cost and labor of real-world data collection. While simulated data
offers a scalable alternative, it often fails to generalize to the real world
due to significant gaps in visual appearance, physical properties, and object
interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real
framework that converts multi-view real-world images into scalable,
high-fidelity, and physically interactive simulation environments for robotic
manipulation. Our approach reconstructs scenes using a hybrid representation:
3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the
environment, while mesh primitives for interactive objects ensure accurate
physics simulation. Crucially, we pioneer the use of a Multi-modal Large
Language Model (MLLM) to automate the creation of physically plausible,
articulated assets. The MLLM analyzes visual data to infer not only physical
properties (e.g., density, stiffness) but also complex kinematic structures
(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained
entirely on data generated by RoboSimGS achieve successful zero-shot
sim-to-real transfer across a diverse set of real-world manipulation tasks.
Furthermore, data from RoboSimGS significantly enhances the performance and
generalization capabilities of SOTA methods. Our results validate RoboSimGS as
a powerful and scalable solution for bridging the sim-to-real gap.

Comments:
- 13 pages, 6 figures

---

## VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene  Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Yicheng He, Jingwen Yu, Guangcheng Chen, Hong Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2510.09962v1){: .btn .btn-green } |

**Abstract**: Maintaining an up-to-date map that accurately reflects recent changes in the
environment is crucial, especially for robots that repeatedly traverse the same
space. Failing to promptly update the changed regions can degrade map quality,
resulting in poor localization, inefficient operations, and even lost robots.
3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online
map reconstruction due to its dense, differentiable, and photorealistic
properties, yet accurately and efficiently updating the regions of change
remains a challenge. In this paper, we propose VG-Mapping, a novel online
3DGS-based mapping system tailored for such semi-static scenes. Our approach
introduces a hybrid representation that augments 3DGS with a TSDF-based voxel
map to efficiently identify changed regions in a scene, along with a
variation-aware density control strategy that inserts or deletes Gaussian
primitives in regions undergoing change. Furthermore, to address the absence of
public benchmarks for this task, we construct a RGB-D dataset comprising both
synthetic and real-world semi-static environments. Experimental results
demonstrate that our method substantially improves the rendering quality and
map update efficiency in semi-static scenes. The code and dataset are available
at https://github.com/heyicheng-never/VG-Mapping.



---

## CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Zhigang Cheng, Mingchao Sun, Yu Liu, Zengye Ge, Luyang Tang, Mu Xu, Yangyan Li, Peng Pan | cs.GR | [PDF](http://arxiv.org/pdf/2510.09997v1){: .btn .btn-green } |

**Abstract**: Level of Detail (LoD) is a fundamental technique in real-time computer
graphics for managing the rendering costs of complex scenes while preserving
visual fidelity. Traditionally, LoD is implemented using discrete levels
(DLoD), where multiple, distinct versions of a model are swapped out at
different distances. This long-standing paradigm, however, suffers from two
major drawbacks: it requires significant storage for multiple model copies and
causes jarring visual ``popping" artifacts during transitions, degrading the
user experience. We argue that the explicit, primitive-based nature of the
emerging 3D Gaussian Splatting (3DGS) technique enables a more ideal paradigm:
Continuous LoD (CLoD). A CLoD approach facilitates smooth, seamless quality
scaling within a single, unified model, thereby circumventing the core problems
of DLOD. To this end, we introduce CLoD-GS, a framework that integrates a
continuous LoD mechanism directly into a 3DGS representation. Our method
introduces a learnable, distance-dependent decay parameter for each Gaussian
primitive, which dynamically adjusts its opacity based on viewpoint proximity.
This allows for the progressive and smooth filtering of less significant
primitives, effectively creating a continuous spectrum of detail within one
model. To train this model to be robust across all distances, we introduce a
virtual distance scaling mechanism and a novel coarse-to-fine training strategy
with rendered point count regularization. Our approach not only eliminates the
storage overhead and visual artifacts of discrete methods but also reduces the
primitive count and memory footprint of the final model. Extensive experiments
demonstrate that CLoD-GS achieves smooth, quality-scalable rendering from a
single model, delivering high-fidelity results across a wide range of
performance targets.



---

## Opacity-Gradient Driven Density Control for Compact and Efficient  Few-Shot 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Abdelrhman Elrawy, Emad A. Mohammed | cs.CV | [PDF](http://arxiv.org/pdf/2510.10257v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its
standard adaptive density control (ADC) can lead to overfitting and bloated
reconstructions. While state-of-the-art methods like FSGS improve quality, they
often do so by significantly increasing the primitive count. This paper
presents a framework that revises the core 3DGS optimization to prioritize
efficiency. We replace the standard positional gradient heuristic with a novel
densification trigger that uses the opacity gradient as a lightweight proxy for
rendering error. We find this aggressive densification is only effective when
paired with a more conservative pruning schedule, which prevents destructive
optimization cycles. Combined with a standard depth-correlation loss for
geometric guidance, our framework demonstrates a fundamental improvement in
efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k
vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a
reduction of approximately 70%. This dramatic gain in compactness is achieved
with a modest trade-off in reconstruction metrics, establishing a new
state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view
synthesis.



---

## Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Jiahui Lu, Haihong Xiao, Xueyan Zhao, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2510.10097v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced
3D reconstruction and novel view synthesis, but remain heavily dependent on
accurate camera poses and dense viewpoint coverage. These requirements limit
their applicability in sparse-view settings, where pose estimation becomes
unreliable and supervision is insufficient. To overcome these challenges, we
introduce Gesplat, a 3DGS-based framework that enables robust novel view
synthesis and geometrically consistent reconstruction from unposed sparse
images. Unlike prior works that rely on COLMAP for sparse point cloud
initialization, we leverage the VGGT foundation model to obtain more reliable
initial poses and dense point clouds. Our approach integrates several key
innovations: 1) a hybrid Gaussian representation with dual position-shape
optimization enhanced by inter-view matching consistency; 2) a graph-guided
attribute refinement module to enhance scene details; and 3) flow-based depth
regularization that improves depth estimation accuracy for more effective
supervision. Comprehensive quantitative and qualitative experiments demonstrate
that our approach achieves more robust performance on both forward-facing and
large-scale complex datasets compared to other pose-free methods.



---

## Color3D: Controllable and Consistent 3D Colorization with Personalized  Colorizer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Yecong Wan, Mingwen Shao, Renlong Wu, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2510.10152v1){: .btn .btn-green } |

**Abstract**: In this work, we present Color3D, a highly adaptable framework for colorizing
both static and dynamic 3D scenes from monochromatic inputs, delivering
visually diverse and chromatically vibrant reconstructions with flexible
user-guided control. In contrast to existing methods that focus solely on
static scenarios and enforce multi-view consistency by averaging color
variations which inevitably sacrifice both chromatic richness and
controllability, our approach is able to preserve color diversity and
steerability while ensuring cross-view and cross-time consistency. In
particular, the core insight of our method is to colorize only a single key
view and then fine-tune a personalized colorizer to propagate its color to
novel views and time steps. Through personalization, the colorizer learns a
scene-specific deterministic color mapping underlying the reference view,
enabling it to consistently project corresponding colors to the content in
novel views and video frames via its inherent inductive bias. Once trained, the
personalized colorizer can be applied to infer consistent chrominance for all
other images, enabling direct reconstruction of colorful 3D scenes with a
dedicated Lab color space Gaussian splatting representation. The proposed
framework ingeniously recasts complicated 3D colorization as a more tractable
single image paradigm, allowing seamless integration of arbitrary image
colorization models with enhanced flexibility and controllability. Extensive
experiments across diverse static and dynamic 3D colorization benchmarks
substantiate that our method can deliver more consistent and chromatically rich
renderings with precise user control. Project Page
https://yecongwan.github.io/Color3D/.

Comments:
- Project Page https://yecongwan.github.io/Color3D/

---

## P-4DGS: Predictive 4D Gaussian Splatting with 90$\times$ Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-11 | Henan Wang, Hanxin Zhu, Xinliang Gong, Tianyu He, Xin Li, Zhibo Chen | cs.CV | [PDF](http://arxiv.org/pdf/2510.10030v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has garnered significant attention due to its
superior scene representation fidelity and real-time rendering performance,
especially for dynamic 3D scene reconstruction (\textit{i.e.}, 4D
reconstruction). However, despite achieving promising results, most existing
algorithms overlook the substantial temporal and spatial redundancies inherent
in dynamic scenes, leading to prohibitive memory consumption. To address this,
we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene
modeling. Inspired by intra- and inter-frame prediction techniques commonly
used in video compression, we first design a 3D anchor point-based
spatial-temporal prediction module to fully exploit the spatial-temporal
correlations across different 3D Gaussian primitives. Subsequently, we employ
an adaptive quantization strategy combined with context-based entropy coding to
further reduce the size of the 3D anchor points, thereby achieving enhanced
compression efficiency. To evaluate the rate-distortion performance of our
proposed P-4DGS in comparison with other dynamic 3DGS representations, we
conduct extensive experiments on both synthetic and real-world datasets.
Experimental results demonstrate that our approach achieves state-of-the-art
reconstruction quality and the fastest rendering speed, with a remarkably low
storage footprint (around \textbf{1MB} on average), achieving up to
\textbf{40$\times$} and \textbf{90$\times$} compression on synthetic and
real-world scenes, respectively.



---

## Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic  Urban Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Yikang Zhang, Rui Fan | cs.CV | [PDF](http://arxiv.org/pdf/2510.09364v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has demonstrated impressive performance in
synthesizing high-fidelity novel views. Nonetheless, its effectiveness
critically depends on the quality of the initialized point cloud. Specifically,
achieving uniform and complete point coverage over the underlying scene
structure requires overlapping observation frustums, an assumption that is
often violated in unbounded, dynamic urban environments. Training Gaussian
models with partially initialized point clouds often leads to distortions and
artifacts, as camera rays may fail to intersect valid surfaces, resulting in
incorrect gradient propagation to Gaussian primitives associated with occluded
or invisible geometry. Additionally, existing densification strategies simply
clone and split Gaussian primitives from existing ones, incapable of
reconstructing missing structures. To address these limitations, we propose
VAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban
scenes. Our method identifies unreliable geometry structures via voxel-based
visibility reasoning, selects informative supporting views through
diversity-aware view selection, and recovers missing structures via patch
matching-based multi-view stereo reconstruction. This design enables the
generation of new Gaussian primitives guided by reliable geometric priors, even
in regions lacking initial points. Extensive experiments on the Waymo and
nuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS
approaches and significantly improves the quality of reconstructed geometry for
both static and dynamic objects. Source code will be released upon publication.



---

## HERO: Hardware-Efficient RL-based Optimization Framework for NeRF  Quantization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Yipu Zhang, Chaofang Ma, Jinming Ge, Lin Jiang, Jiang Xu, Wei Zhang | cs.AR | [PDF](http://arxiv.org/pdf/2510.09010v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction
method, delivering high-quality results for AR/VR applications. While
quantization methods and hardware accelerators have been proposed to enhance
NeRF's computational efficiency, existing approaches face crucial limitations.
Current quantization methods operate without considering hardware architecture,
resulting in sub-optimal solutions within the vast design space encompassing
accuracy, latency, and model size. Additionally, existing NeRF accelerators
heavily rely on human experts to explore this design space, making the
optimization process time-consuming, inefficient, and unlikely to discover
optimal solutions. To address these challenges, we introduce HERO, a
reinforcement learning framework performing hardware-aware quantization for
NeRF. Our framework integrates a NeRF accelerator simulator to generate
real-time hardware feedback, enabling fully automated adaptation to hardware
constraints. Experimental results demonstrate that HERO achieves 1.31-1.33
$\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a
more compact model size compared to CAQ, a previous state-of-the-art NeRF
quantization framework. These results validate our framework's capability to
effectively navigate the complex design space between hardware and algorithm
requirements, discovering superior quantization policies for NeRF
implementation. Code is available at https://github.com/ypzhng/HERO.

Comments:
- Accepted by ASPDAC 2026

---

## Two-Stage Gaussian Splatting Optimization for Outdoor Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Deborah Pintani, Ariel Caputo, Noah Lewis, Marc Stamminger, Fabio Pellacini, Andrea Giachetti | cs.GR | [PDF](http://arxiv.org/pdf/2510.09489v1){: .btn .btn-green } |

**Abstract**: Outdoor scene reconstruction remains challenging due to the stark contrast
between well-textured, nearby regions and distant backgrounds dominated by low
detail, uneven illumination, and sky effects. We introduce a two-stage Gaussian
Splatting framework that explicitly separates and optimizes these regions,
yielding higher-fidelity novel view synthesis. In stage one, background
primitives are initialized within a spherical shell and optimized using a loss
that combines a background-only photometric term with two geometric
regularizers: one constraining Gaussians to remain inside the shell, and
another aligning them with local tangential planes. In stage two, foreground
Gaussians are initialized from a Structure-from-Motion reconstruction, added
and refined using the standard rendering loss, while the background set remains
fixed but contributes to the final image formation. Experiments on diverse
outdoor datasets show that our method reduces background artifacts and improves
perceptual quality compared to state-of-the-art baselines. Moreover, the
explicit background separation enables automatic, object-free environment map
estimation, opening new possibilities for photorealistic outdoor rendering and
mixed-reality applications.



---

## LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Minkwan Kim, Seungmin Lee, Junho Kim, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2510.09881v1){: .btn .btn-green } |

**Abstract**: Recent advances in novel-view synthesis can create the photo-realistic
visualization of real-world environments from conventional camera captures.
However, acquiring everyday environments from casual captures faces challenges
due to frequent scene changes, which require dense observations both spatially
and temporally. We propose long-term Gaussian scene chronology from sparse-view
updates, coined LTGS, an efficient scene representation that can embrace
everyday changes from highly under-constrained casual captures. Given an
incomplete and unstructured Gaussian splatting representation obtained from an
initial set of input images, we robustly model the long-term chronology of the
scene despite abrupt movements and subtle environmental variations. We
construct objects as template Gaussians, which serve as structural, reusable
priors for shared object tracks. Then, the object templates undergo a further
refinement pipeline that modulates the priors to adapt to temporally varying
environments based on few-shot observations. Once trained, our framework is
generalizable across multiple time steps through simple transformations,
significantly enhancing the scalability for a temporal evolution of 3D
environments. As existing datasets do not explicitly represent the long-term
real-world changes with a sparse capture setup, we collect real-world datasets
to evaluate the practicality of our pipeline. Experiments demonstrate that our
framework achieves superior reconstruction quality compared to other baselines
while enabling fast and light-weight updates.



---

## Vision Language Models: A Survey of 26K Papers

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Fengming Lin | cs.CV | [PDF](http://arxiv.org/pdf/2510.09586v1){: .btn .btn-green } |

**Abstract**: We present a transparent, reproducible measurement of research trends across
26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles
and abstracts are normalized, phrase-protected, and matched against a
hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained
cues about tasks, architectures, training regimes, objectives, datasets, and
co-mentioned modalities. The analysis quantifies three macro shifts: (1) a
sharp rise of multimodal vision-language-LLM work, which increasingly reframes
classic perception as instruction following and multi-step reasoning; (2)
steady expansion of generative methods, with diffusion research consolidating
around controllability, distillation, and speed; and (3) resilient 3D and video
activity, with composition moving from NeRFs to Gaussian splatting and a
growing emphasis on human- and agent-centric understanding. Within VLMs,
parameter-efficient adaptation like prompting/adapters/LoRA and lightweight
vision-language bridges dominate; training practice shifts from building
encoders from scratch to instruction tuning and finetuning strong backbones;
contrastive objectives recede relative to cross-entropy/ranking and
distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and
ICLR the highest VLM share, while reliability themes such as efficiency or
robustness diffuse across areas. We release the lexicon and methodology to
enable auditing and extension. Limitations include lexicon recall and
abstract-only scope, but the longitudinal signals are consistent across venues
and years.

Comments:
- VLM/LLM Learning Notes

---

## Geometry-Aware Scene Configurations for Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Minkwan Kim, Changwoon Choi, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2510.09880v1){: .btn .btn-green } |

**Abstract**: We propose scene-adaptive strategies to efficiently allocate representation
capacity for generating immersive experiences of indoor environments from
incomplete observations. Indoor scenes with multiple rooms often exhibit
irregular layouts with varying complexity, containing clutter, occlusion, and
flat walls. We maximize the utilization of limited resources with guidance from
geometric priors, which are often readily available after pre-processing
stages. We record observation statistics on the estimated geometric scaffold
and guide the optimal placement of bases, which greatly improves upon the
uniform basis arrangements adopted by previous scalable Neural Radiance Field
(NeRF) representations. We also suggest scene-adaptive virtual viewpoints to
compensate for geometric deficiencies inherent in view configurations in the
input trajectory and impose the necessary regularization. We present a
comprehensive analysis and discussion regarding rendering quality and memory
requirements in several large-scale indoor scenes, demonstrating significant
enhancements compared to baselines that employ regular placements.



---

## FLOWING: Implicit Neural Flows for Structure-Preserving Morphing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-10 | Arthur Bizzi, Matias Grynberg, Vitor Matias, Daniel Perazzo, Jo√£o Paulo Lima, Luiz Velho, Nuno Gon√ßalves, Jo√£o Pereira, Guilherme Schardong, Tiago Novello | cs.CV | [PDF](http://arxiv.org/pdf/2510.09537v1){: .btn .btn-green } |

**Abstract**: Morphing is a long-standing problem in vision and computer graphics,
requiring a time-dependent warping for feature alignment and a blending for
smooth interpolation. Recently, multilayer perceptrons (MLPs) have been
explored as implicit neural representations (INRs) for modeling such
deformations, due to their meshlessness and differentiability; however,
extracting coherent and accurate morphings from standard MLPs typically relies
on costly regularizations, which often lead to unstable training and prevent
effective feature alignment. To overcome these limitations, we propose FLOWING
(FLOW morphING), a framework that recasts warping as the construction of a
differential vector flow, naturally ensuring continuity, invertibility, and
temporal coherence by encoding structural flow properties directly into the
network architectures. This flow-centric approach yields principled and stable
transformations, enabling accurate and structure-preserving morphing of both 2D
images and 3D shapes. Extensive experiments across a range of applications -
including face and image morphing, as well as Gaussian Splatting morphing -
show that FLOWING achieves state-of-the-art morphing quality with faster
convergence. Code and pretrained models are available at
http://schardong.github.io/flowing.

Comments:
- 10 pages main paper; 9 pages references and appendix

---

## ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral  Probes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Jian Gao, Mengqi Yuan, Yifei Zeng, Chang Zeng, Zhihao Li, Zhenyu Chen, Weichao Qiu, Xiao-Xiao Long, Hao Zhu, Xun Cao, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2510.07729v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) enables immersive rendering, but realistic 3D
object-scene composition remains challenging. Baked appearance and shadow
information in GS radiance fields cause inconsistencies when combining objects
and scenes. Addressing this requires relightable object reconstruction and
scene lighting estimation. For relightable object reconstruction, existing
Gaussian-based inverse rendering methods often rely on ray tracing, leading to
low efficiency. We introduce Surface Octahedral Probes (SOPs), which store
lighting and occlusion information and allow efficient 3D querying via
interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x
speedup in reconstruction and enable real-time shadow computation in Gaussian
scenes. For lighting estimation, existing Gaussian-based inverse rendering
methods struggle to model intricate light transport and often fail in complex
scenes, while learning-based methods predict lighting from a single image and
are viewpoint-sensitive. We observe that 3D object-scene composition primarily
concerns the object's appearance and nearby shadows. Thus, we simplify the
challenging task of full scene lighting estimation by focusing on the
environment lighting at the object's placement. Specifically, we capture a 360
degrees reconstructed radiance field of the scene at the location and fine-tune
a diffusion model to complete the lighting. Building on these advances, we
propose ComGS, a novel 3D object-scene composition framework. Our method
achieves high-quality, real-time rendering at around 28 FPS, produces visually
harmonious results with vivid shadows, and requires only 36 seconds for
editing. Code and dataset are available at
https://nju-3dv.github.io/projects/ComGS/.



---

## PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale  3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Houqiang Zhong, Zhenglong Wu, Sihua Fu, Zihan Zheng, Xin Jin, Xiaoyun Zhang, Li Song, Qiang Hu | cs.CV | [PDF](http://arxiv.org/pdf/2510.07830v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic
rendering in compact scenes, but scaling to large urban environments introduces
severe aliasing artifacts and optimization instability, especially under
high-resolution (e.g., 4K) rendering. These artifacts, manifesting as
flickering textures and jagged edges, arise from the mismatch between Gaussian
primitives and the multi-scale nature of urban geometry. While existing
``divide-and-conquer'' pipelines address scalability, they fail to resolve this
fidelity gap. In this paper, we propose PrismGS, a physically-grounded
regularization framework that improves the intrinsic rendering behavior of 3D
Gaussians. PrismGS integrates two synergistic regularizers. The first is
pyramidal multi-scale supervision, which enforces consistency by supervising
the rendering against a pre-filtered image pyramid. This compels the model to
learn an inherently anti-aliased representation that remains coherent across
different viewing scales, directly mitigating flickering textures. This is
complemented by an explicit size regularization that imposes a
physically-grounded lower bound on the dimensions of the 3D Gaussians. This
prevents the formation of degenerate, view-dependent primitives, leading to
more stable and plausible geometric surfaces and reducing jagged edges. Our
method is plug-and-play and compatible with existing pipelines. Extensive
experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS
achieves state-of-the-art performance, yielding significant PSNR gains around
1.5 dB against CityGaussian, while maintaining its superior quality and
robustness under demanding 4K rendering.



---

## D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and  Accurate Sparse-View Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Meixi Song, Xin Lin, Dizhe Zhang, Haodong Li, Xiangtai Li, Bo Du, Lu Qi | cs.CV | [PDF](http://arxiv.org/pdf/2510.08566v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) enable real-time,
high-fidelity novel view synthesis (NVS) with explicit 3D representations.
However, performance degradation and instability remain significant under
sparse-view conditions. In this work, we identify two key failure modes under
sparse-view conditions: overfitting in regions with excessive Gaussian density
near the camera, and underfitting in distant areas with insufficient Gaussian
coverage. To address these challenges, we propose a unified framework D$^2$GS,
comprising two key components: a Depth-and-Density Guided Dropout strategy that
suppresses overfitting by adaptively masking redundant Gaussians based on
density and depth, and a Distance-Aware Fidelity Enhancement module that
improves reconstruction quality in under-fitted far-field areas through
targeted supervision. Moreover, we introduce a new evaluation metric to
quantify the stability of learned Gaussian distributions, providing insights
into the robustness of the sparse-view 3DGS. Extensive experiments on multiple
datasets demonstrate that our method significantly improves both visual quality
and robustness under sparse view conditions. The project page can be found at:
https://insta360-research-team.github.io/DDGS-website/.



---

## An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit  Data Reuse Strategies

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Binzhe Yuan, Xiangyu Zhang, Zeyu Zheng, Yuefeng Zhang, Haochuan Wan, Zhechen Yuan, Junsheng Chen, Yunxiang He, Junran Ding, Xiaoming Zhang, Chaolin Rao, Wenyan Su, Pingqiang Zhou, Jingyi Yu, Xin Lou | eess.IV | [PDF](http://arxiv.org/pdf/2510.07667v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) have transformed 3D reconstruction and
rendering, facilitating photorealistic image synthesis from sparse viewpoints.
This work introduces an explicit data reuse neural rendering (EDR-NR)
architecture, which reduces frequent external memory accesses (EMAs) and cache
misses by exploiting the spatial locality from three phases, including rays,
ray packets (RPs), and samples. The EDR-NR architecture features a four-stage
scheduler that clusters rays on the basis of Z-order, prioritize lagging rays
when ray divergence happens, reorders RPs based on spatial proximity, and
issues samples out-of-orderly (OoO) according to the availability of on-chip
feature data. In addition, a four-tier hierarchical RP marching (HRM) technique
is integrated with an axis-aligned bounding box (AABB) to facilitate spatial
skipping (SS), reducing redundant computations and improving throughput.
Moreover, a balanced allocation strategy for feature storage is proposed to
mitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area
of 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized
energy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X
increase in normalized throughput, and a 53.42% reduction in on-chip SRAM
consumption compared to state-of-the-art accelerators.

Comments:
- 11 pages, 17 figures, 2 tables

---

## DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event  Stream

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Junhao He, Jiaxu Wang, Jia Li, Mingyuan Sun, Qiang Zhang, Jiahang Cao, Ziyi Zhang, Yi Gu, Jingkai Sun, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.07752v1){: .btn .btn-green } |

**Abstract**: Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB
videos is challenging. This is because large inter-frame motions will increase
the uncertainty of the solution space. For example, one pixel in the first
frame might have more choices to reach the corresponding pixel in the second
frame. Event cameras can asynchronously capture rapid visual changes and are
robust to motion blur, but they do not provide color information. Intuitively,
the event stream can provide deterministic constraints for the inter-frame
large motion by the event trajectories. Hence, combining
low-temporal-resolution images with high-framerate event streams can address
this challenge. However, it is challenging to jointly optimize Dynamic 3DGS
using both RGB and event modalities due to the significant discrepancy between
these two data modalities. This paper introduces a novel framework that jointly
optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event
motion priors to guide the optimization of the deformation fields. First, we
extract the motion priors encoded in event streams by using the proposed LoCM
unsupervised fine-tuning framework to adapt an event flow estimator to a
certain unseen scene. Then, we present the geometry-aware data association
method to build the event-Gaussian motion correspondence, which is the primary
foundation of the pipeline, accompanied by two useful strategies, namely motion
decomposition and inter-frame pseudo-label. Extensive experiments show that our
method outperforms existing image and event-based approaches across synthetic
and real scenes and prove that our method can effectively optimize dynamic 3DGS
with the help of event data.

Comments:
- Accepted by TVCG

---

## Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Ankit Gahlawat, Anirban Mukherjee, Dinesh Babu Jayagopi | cs.CV | [PDF](http://arxiv.org/pdf/2510.08096v1){: .btn .btn-green } |

**Abstract**: Accurate face parsing under extreme viewing angles remains a significant
challenge due to limited labeled data in such poses. Manual annotation is
costly and often impractical at scale. We propose a novel label refinement
pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate
segmentation masks from noisy multiview predictions. By jointly fitting two
3DGS models, one to RGB images and one to their initial segmentation maps, our
method enforces multiview consistency through shared geometry, enabling the
synthesis of pose-diverse training data with only minimal post-processing.
Fine-tuning a face parsing model on this refined dataset significantly improves
accuracy on challenging head poses, while maintaining strong performance on
standard views. Extensive experiments, including human evaluations, demonstrate
that our approach achieves superior results compared to state-of-the-art
methods, despite requiring no ground-truth 3D annotations and using only a
small set of initial images. Our method offers a scalable and effective
solution for improving face parsing robustness in real-world settings.

Comments:
- Accepted to VCIP 2025 (International Conference on Visual
  Communications and Image Processing 2025)

---

## Splat the Net: Radiance Fields with Splattable Neural Primitives

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Xilong Zhou, Bao-Huy Nguyen, Lo√Øc Magne, Vladislav Golyanik, Thomas Leimk√ºhler, Christian Theobalt | cs.GR | [PDF](http://arxiv.org/pdf/2510.08491v1){: .btn .btn-green } |

**Abstract**: Radiance fields have emerged as a predominant representation for modeling 3D
scene appearance. Neural formulations such as Neural Radiance Fields provide
high expressivity but require costly ray marching for rendering, whereas
primitive-based methods such as 3D Gaussian Splatting offer real-time
efficiency through splatting, yet at the expense of representational power.
Inspired by advances in both these directions, we introduce splattable neural
primitives, a new volumetric representation that reconciles the expressivity of
neural models with the efficiency of primitive-based splatting. Each primitive
encodes a bounded neural density field parameterized by a shallow neural
network. Our formulation admits an exact analytical solution for line
integrals, enabling efficient computation of perspectively accurate splatting
kernels. As a result, our representation supports integration along view rays
without the need for costly ray marching. The primitives flexibly adapt to
scene geometry and, being larger than prior analytic primitives, reduce the
number required per scene. On novel-view synthesis benchmarks, our approach
matches the quality and speed of 3D Gaussian Splatting while using $10\times$
fewer primitives and $6\times$ fewer parameters. These advantages arise
directly from the representation itself, without reliance on complex control or
adaptation frameworks. The project page is
https://vcai.mpi-inf.mpg.de/projects/SplatNet/.



---

## ReSplat: Learning Recurrent Gaussian Splats

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Haofei Xu, Daniel Barath, Andreas Geiger, Marc Pollefeys | cs.CV | [PDF](http://arxiv.org/pdf/2510.08575v1){: .btn .btn-green } |

**Abstract**: While feed-forward Gaussian splatting models provide computational efficiency
and effectively handle sparse input settings, their performance is
fundamentally limited by the reliance on a single forward pass during
inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting
model that iteratively refines 3D Gaussians without explicitly computing
gradients. Our key insight is that the Gaussian splatting rendering error
serves as a rich feedback signal, guiding the recurrent network to learn
effective Gaussian updates. This feedback signal naturally adapts to unseen
data distributions at test time, enabling robust generalization. To initialize
the recurrent process, we introduce a compact reconstruction model that
operates in a $16 \times$ subsampled space, producing $16 \times$ fewer
Gaussians than previous per-pixel Gaussian models. This substantially reduces
computational overhead and allows for efficient Gaussian updates. Extensive
experiments across varying of input views (2, 8, 16), resolutions ($256 \times
256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate
that our method achieves state-of-the-art performance while significantly
reducing the number of Gaussians and improving the rendering speed. Our project
page is at https://haofeixu.github.io/resplat/.

Comments:
- Project page: https://haofeixu.github.io/resplat/

---

## CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal  Reconstruction Model for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-09 | Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni, Chenjing Ding, Dan Xu, Lewei Lu, Zehuan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2510.07944v1){: .btn .btn-green } |

**Abstract**: Generative models have been widely applied to world modeling for environment
simulation and future state prediction. With advancements in autonomous
driving, there is a growing demand not only for high-fidelity video generation
under various controls, but also for producing diverse and meaningful
information such as depth estimation. To address this, we propose CVD-STORM, a
cross-view video diffusion model utilizing a spatial-temporal reconstruction
Variational Autoencoder (VAE) that generates long-term, multi-view videos with
4D reconstruction capabilities under various control inputs. Our approach first
fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its
ability to encode 3D structures and temporal dynamics. Subsequently, we
integrate this VAE into the video diffusion process to significantly improve
generation quality. Experimental results demonstrate that our model achieves
substantial improvements in both FID and FVD metrics. Additionally, the
jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic
scenes, providing valuable geometric information for comprehensive scene
understanding.



---

## Capture and Interact: Rapid 3D Object Acquisition and Rendering with  Gaussian Splatting in Unity

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-08 | Islomjon Shukhratov, Sergey Gorinsky | cs.GR | [PDF](http://arxiv.org/pdf/2510.06802v1){: .btn .btn-green } |

**Abstract**: Capturing and rendering three-dimensional (3D) objects in real time remain a
significant challenge, yet hold substantial potential for applications in
augmented reality, digital twin systems, remote collaboration and prototyping.
We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS)
to enable rapid acquisition and interactive rendering of real-world objects
using a mobile device, cloud processing and a local computer. Users scan an
object with a smartphone video, upload it for automated 3D reconstruction, and
visualize it interactively in Unity at an average of 150 frames per second
(fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and
Unity rendering to support real-time telepresence. Our experiments show that
the pipeline processes scans in approximately 10 minutes on a graphics
processing unit (GPU) achieving real-time rendering on the laptop.



---

## SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D  Novel View Synthesis


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-08 | Jipeng Lyu, Jiahua Dong, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2510.06694v1){: .btn .btn-green } |

**Abstract**: Persistent dynamic scene modeling for tracking and novel-view synthesis
remains challenging due to the difficulty of capturing accurate deformations
while maintaining computational efficiency. We propose SCas4D, a cascaded
optimization framework that leverages structural patterns in 3D Gaussian
Splatting for dynamic scenes. The key idea is that real-world deformations
often exhibit hierarchical patterns, where groups of Gaussians share similar
transformations. By progressively refining deformations from coarse part-level
to fine point-level, SCas4D achieves convergence within 100 iterations per time
frame and produces results comparable to existing methods with only
one-twentieth of the training iterations. The approach also demonstrates
effectiveness in self-supervised articulated object segmentation, novel view
synthesis, and dense point tracking tasks.

Comments:
- Published in Transactions on Machine Learning Research (06/2025)

---

## Generating Surface for Text-to-3D using 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-08 | Huanning Dong, Fan Li, Ping Kuang, Jianwen Min | cs.CV | [PDF](http://arxiv.org/pdf/2510.06967v1){: .btn .btn-green } |

**Abstract**: Recent advancements in Text-to-3D modeling have shown significant potential
for the creation of 3D content. However, due to the complex geometric shapes of
objects in the natural world, generating 3D content remains a challenging task.
Current methods either leverage 2D diffusion priors to recover 3D geometry, or
train the model directly based on specific 3D representations. In this paper,
we propose a novel method named DirectGaussian, which focuses on generating the
surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize
conditional text generation models and the surface of a 3D object is rendered
by 2D Gaussian splatting with multi-view normal and texture priors. For
multi-view geometric consistency problems, DirectGaussian incorporates
curvature constraints on the generated surface during optimization process.
Through extensive experiments, we demonstrate that our framework is capable of
achieving diverse and high-fidelity 3D content creation.



---

## RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy  Reduction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-08 | Leshu Li, Jiayin Qin, Jie Peng, Zishen Wan, Huaizhi Qu, Ye Han, Pingqing Zheng, Hongsen Zhang, Yu Cao, Tianlong Chen, Yang Katie Zhao | cs.AR | [PDF](http://arxiv.org/pdf/2510.06644v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping
(SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering
efficiency and accuracy, but have not yet been adopted in resource-constrained
edge devices due to insufficient speed. Addressing this, we identify notable
redundancies across the SLAM pipeline for acceleration. While conceptually
straightforward, practical approaches are required to minimize the overhead
associated with identifying and eliminating these redundancies. In response, we
propose RTGS, an algorithm-hardware co-design framework that comprehensively
reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the
overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline.
On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to
remove the redundant Gaussians by reusing gradients computed during
backpropagation; and (2) a dynamic downsampling technique that directly reuses
the keyframe identification and alpha computing steps to eliminate redundant
pixels. On the hardware side, we propose (1) a subtile-level streaming strategy
and a pixel-level pairwise scheduling strategy that mitigates workload
imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration
information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates
the rendering backpropagation by reusing intermediate data computed during
rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory
accesses caused by atomic operations while enabling pipelined aggregation.
Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on
four datasets and three algorithms, with up to 82.5x energy efficiency over the
baseline and negligible quality loss. Code is available at
https://github.com/UMN-ZhaoLab/RTGS.

Comments:
- Accepted by MICRO2025

---

## ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head  Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-07 | Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du | cs.CV | [PDF](http://arxiv.org/pdf/2510.05488v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time
rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on
tens of thousands of 3D Gaussian points (Gaussians), with the number of
Gaussians fixed after training. However, many practical applications require
adjustable levels of detail (LOD) to balance rendering efficiency and visual
quality. In this work, we propose "ArchitectHead", the first framework for
creating 3D Gaussian head avatars that support continuous control over LOD. Our
key idea is to parameterize the Gaussians in a 2D UV feature space and propose
a UV feature field composed of multi-level learnable feature maps to encode
their latent features. A lightweight neural network-based decoder then
transforms these latent features into 3D Gaussian attributes for rendering.
ArchitectHead controls the number of Gaussians by dynamically resampling
feature maps from the UV feature field at the desired resolutions. This method
enables efficient and continuous control of LOD without retraining.
Experimental results show that ArchitectHead achieves state-of-the-art (SOTA)
quality in self and cross-identity reenactment tasks at the highest LOD, while
maintaining near SOTA performance at lower LODs. At the lowest LOD, our method
uses only 6.2\% of the Gaussians while the quality degrades moderately (L1 Loss
+7.9\%, PSNR --0.97\%, SSIM --0.6\%, LPIPS Loss +24.1\%), and the rendering
speed nearly doubles.



---

## Active Next-Best-View Optimization for Risk-Averse Path Planning


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-07 | Amirhossein Mollaei Khass, Guangyi Liu, Vivek Pandey, Wen Jiang, Boshu Lei, Kostas Daniilidis, Nader Motee | cs.RO | [PDF](http://arxiv.org/pdf/2510.06481v1){: .btn .btn-green } |

**Abstract**: Safe navigation in uncertain environments requires planning methods that
integrate risk aversion with active perception. In this work, we present a
unified framework that refines a coarse reference path by constructing
tail-sensitive risk maps from Average Value-at-Risk statistics on an
online-updated 3D Gaussian-splat Radiance Field. These maps enable the
generation of locally safe and feasible trajectories. In parallel, we formulate
Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose
manifold, where Riemannian gradient descent maximizes an expected information
gain objective to reduce uncertainty most critical for imminent motion. Our
approach advances the state-of-the-art by coupling risk-averse path refinement
with NBV planning, while introducing scalable gradient decompositions that
support efficient online updates in complex environments. We demonstrate the
effectiveness of the proposed framework through extensive computational
studies.



---

## Optimized Minimal 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-04 | Minseo Lee, Byeonghyeon Lee, Lucas Yunkyu Lee, Eunsoo Lee, Sangmin Kim, Seunghyeon Song, Joo Chan Lee, Jong Hwan Ko, Jaesik Park, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2510.03857v1){: .btn .btn-green } |

**Abstract**: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

Comments:
- 17 pages, 8 figures

---

## Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled  Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Zhiting Mei, Ola Shorinwa, Anirudha Majumdar | cs.CV | [PDF](http://arxiv.org/pdf/2510.03104v1){: .btn .btn-green } |

**Abstract**: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.



---

## GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Xinran Zhang, Hanqi Zhu, Yifan Duan, Yanyong Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2510.02884v1){: .btn .btn-green } |

**Abstract**: Constructing and sharing 3D maps is essential for many applications,
including autonomous driving and augmented reality. Recently, 3D Gaussian
splatting has emerged as a promising approach for accurate 3D reconstruction.
However, a practical map-sharing system that features high-fidelity, continuous
updates, and network efficiency remains elusive. To address these challenges,
we introduce GS-Share, a photorealistic map-sharing system with a compact
representation. The core of GS-Share includes anchor-based global map
construction, virtual-image-based map enhancement, and incremental map update.
We evaluate GS-Share against state-of-the-art methods, demonstrating that our
system achieves higher fidelity, particularly for extrapolated views, with
improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively.
Furthermore, GS-Share is significantly more compact, reducing map transmission
overhead by 36%.

Comments:
- 11 pages, 11 figures

---

## SketchPlan: Diffusion Based Drone Planning From Human Sketches

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Sixten Norelius, Aaron O. Feldman, Mac Schwager | cs.CV | [PDF](http://arxiv.org/pdf/2510.03545v1){: .btn .btn-green } |

**Abstract**: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

Comments:
- Code available at https://github.com/sixnor/SketchPlan

---

## FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Yibin Zhao, Yihan Pan, Jun Nan, Jianjun Yi | cs.CV | [PDF](http://arxiv.org/pdf/2510.02691v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.



---

## From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2510.02732v1){: .btn .btn-green } |

**Abstract**: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.



---

## ROGR: Relightable 3D Objects using Generative Relighting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-03 | Jiapeng Tang, Matthew Lavine, Dor Verbin, Stephan J. Garbin, Matthias Nie√üner, Ricardo Martin Brualla, Pratul P. Srinivasan, Philipp Henzler | cs.CV | [PDF](http://arxiv.org/pdf/2510.03163v1){: .btn .btn-green } |

**Abstract**: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

Comments:
- NeurIPS 2025 Spotlight. Project page:
  https://tangjiapeng.github.io/ROGR

---

## MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust  Physics-Based Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Changmin Lee, Jihyun Lee, Tae-Kyun Kim | cs.GR | [PDF](http://arxiv.org/pdf/2510.01619v1){: .btn .btn-green } |

**Abstract**: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

Comments:
- Accepted to NeurIPS 2025

---

## Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy  Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Georgios Kouros, Minye Wu, Tinne Tuytelaars | cs.GR | [PDF](http://arxiv.org/pdf/2510.02069v1){: .btn .btn-green } |

**Abstract**: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.



---

## GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object  Morphing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Mengtian Li, Yunshu Bai, Yimin Chu, Yijun Shen, Zhongmei Li, Weifeng Ge, Zhifeng Xie, Chaofeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2510.02034v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

Comments:
- Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

---

## SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Sung-Yeon Park, Adam Lee, Juanwu Lu, Can Cui, Luyang Jiang, Rohit Gupta, Kyungtae Han, Ahmadreza Moradipari, Ziran Wang | cs.RO | [PDF](http://arxiv.org/pdf/2510.02469v1){: .btn .btn-green } |

**Abstract**: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/



---

## ROI-GS: Interest-based Local Quality 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Quoc-Anh Bui, Gilles Rougeron, G√©raldine Morin, Simone Gasparini | cs.GR | [PDF](http://arxiv.org/pdf/2510.01978v1){: .btn .btn-green } |

**Abstract**: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

Comments:
- 4 pages, 3 figures, 2 tables

---

## StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided  Illusions

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, Wei-Chen Chiu | cs.CV | [PDF](http://arxiv.org/pdf/2510.02314v1){: .btn .btn-green } |

**Abstract**: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

Comments:
- ICCV 2025. Project page: https://hentci.github.io/stealthattack/

---

## LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for  Large-Scale Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Sheng-Hsiang Hung, Ting-Yu Yen, Wei-Fang Sun, Simon See, Shih-Hsuan Hung, Hung-Kuo Chu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01767v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.



---

## GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for  Mobile Robotics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Diram Tabaa, Gianni Di Caro | cs.RO | [PDF](http://arxiv.org/pdf/2510.01848v1){: .btn .btn-green } |

**Abstract**: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.



---

## 4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Lei Liu, Can Wang, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01991v1){: .btn .btn-green } |

**Abstract**: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.



---

## Performance-Guided Refinement for Visual Aerial Navigation using  Editable Gaussian Splatting in FalconGym 2.0

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-02 | Yan Miao, Ege Yuceel, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2510.02248v1){: .btn .btn-green } |

**Abstract**: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.



---

## Multi-level Dynamic Style Transfer for NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-01 | Zesheng Li, Shuaibo Li, Wei Ma, Jianwei Guo, Hongbin Zha | cs.CV | [PDF](http://arxiv.org/pdf/2510.00592v1){: .btn .btn-green } |

**Abstract**: As the application of neural radiance fields (NeRFs) in various 3D vision
tasks continues to expand, numerous NeRF-based style transfer techniques have
been developed. However, existing methods typically integrate style statistics
into the original NeRF pipeline, often leading to suboptimal results in both
content preservation and artistic stylization. In this paper, we present
multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that
reengineers the NeRF pipeline specifically for stylization and incorporates an
innovative dynamic style injection module. Particularly, we propose a
multi-level feature adaptor that helps generate a multi-level feature grid
representation from the content radiance field, effectively capturing the
multi-scale spatial structure of the scene. In addition, we present a dynamic
style injection module that learns to extract relevant style features and
adaptively integrates them into the content patterns. The stylized multi-level
features are then transformed into the final stylized view through our proposed
multi-level cascade decoder. Furthermore, we extend our 3D style transfer
method to support omni-view style transfer using 3D style references. Extensive
experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D
style transfer, preserving multi-scale spatial structures while effectively
transferring stylistic characteristics.

Comments:
- Accepted by Computational Visual Media Journal (CVMJ)

---

## Instant4D: 4D Gaussian Splatting in Minutes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-10-01 | Zhanpeng Luo, Haoxi Ran, Li Lu | cs.CV | [PDF](http://arxiv.org/pdf/2510.01119v1){: .btn .btn-green } |

**Abstract**: Dynamic view synthesis has seen significant advances, yet reconstructing
scenes from uncalibrated, casual video remains challenging due to slow
optimization and complex parameter estimation. In this work, we present
Instant4D, a monocular reconstruction system that leverages native 4D
representation to efficiently process casual video sequences within minutes,
without calibrated cameras or depth sensors. Our method begins with geometric
recovery through deep visual SLAM, followed by grid pruning to optimize scene
representation. Our design significantly reduces redundancy while maintaining
geometric integrity, cutting model size to under 10% of its original footprint.
To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian
representation, achieving a 30x speed-up and reducing training time to within
two minutes, while maintaining competitive performance across several
benchmarks. Our method reconstruct a single video within 10 minutes on the
Dycheck dataset or for a typical 200-frame video. We further apply our model to
in-the-wild videos, showcasing its generalizability. Our project website is
published at https://instant4d.github.io/.

Comments:
- Accepted by NeurIPS 25
