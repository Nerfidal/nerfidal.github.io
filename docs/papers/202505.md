---
layout: default
title: May 2025
parent: Papers
nav_order: 202505
---

<!---metadata--->


## UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot  Ground Vehicle Repeat

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-22 | Desiree Fisker, Alexander Krawciw, Sven Lilge, Melissa Greeff, Timothy D. Barfoot | cs.RO | [PDF](http://arxiv.org/pdf/2505.16912v1){: .btn .btn-green } |

**Abstract**: This paper presents Virtual Teach and Repeat (VirT&R): an extension of the
Teach and Repeat (T&R) framework that enables GPS-denied, zero-shot autonomous
ground vehicle navigation in untraversed environments. VirT&R leverages aerial
imagery captured for a target environment to train a Neural Radiance Field
(NeRF) model so that dense point clouds and photo-textured meshes can be
extracted. The NeRF mesh is used to create a high-fidelity simulation of the
environment for piloting an unmanned ground vehicle (UGV) to virtually define a
desired path. The mission can then be executed in the actual target environment
by using NeRF-derived point cloud submaps associated along the path and an
existing LiDAR Teach and Repeat (LT&R) framework. We benchmark the
repeatability of VirT&R on over 12 km of autonomous driving data using physical
markings that allow a sim-to-real lateral path-tracking error to be obtained
and compared with LT&R. VirT&R achieved measured root mean squared errors
(RMSE) of 19.5 cm and 18.4 cm in two different environments, which are slightly
less than one tire width (24 cm) on the robot used for testing, and respective
maximum errors were 39.4 cm and 47.6 cm. This was done using only the
NeRF-derived teach map, demonstrating that VirT&R has similar closed-loop
path-tracking performance to LT&R but does not require a human to manually
teach the path to the UGV in the actual environment.

Comments:
- 8 pages, 7 figures, submitted to IROS 2025

---

## Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-22 | Jiacong Chen, Qingyu Mao, Youneng Bao, Xiandong Meng, Fanyang Meng, Ronggang Wang, Yongsheng Liang | cs.CV | [PDF](http://arxiv.org/pdf/2505.16533v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient
paradigm for online free-viewpoint video (FVV) reconstruction, offering viewers
rapid responsiveness and immersive experiences. However, existing online
methods face challenge in prohibitive storage requirements primarily due to
point-wise modeling that fails to exploit the motion properties. To address
this limitation, we propose a novel Compact Gaussian Streaming (ComGS)
framework, leveraging the locality and consistency of motion in dynamic scene,
that models object-consistent Gaussian point motion through keypoint-driven
motion representation. By transmitting only the keypoint attributes, this
framework provides a more storage-efficient solution. Specifically, we first
identify a sparse set of motion-sensitive keypoints localized within motion
regions using a viewspace gradient difference strategy. Equipped with these
keypoints, we propose an adaptive motion-driven mechanism that predicts a
spatial influence field for propagating keypoint motion to neighboring Gaussian
points with similar motion. Moreover, ComGS adopts an error-aware correction
strategy for key frame reconstruction that selectively refines erroneous
regions and mitigates error accumulation without unnecessary overhead. Overall,
ComGS achieves a remarkable storage reduction of over 159 X compared to
3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining
competitive visual fidelity and rendering speed. Our code will be released.

Comments:
- 17 pages, 9 figures

---

## SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane  Deformation and Latent Diffusion


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-22 | Asrar Alruwayqi | cs.CV | [PDF](http://arxiv.org/pdf/2505.16535v1){: .btn .btn-green } |

**Abstract**: We present a novel framework for dynamic 3D scene reconstruction that
integrates three key components: an explicit tri-plane deformation field, a
view-conditioned canonical radiance field with spherical harmonics (SH)
attention, and a temporally-aware latent diffusion prior. Our method encodes 4D
scenes using three orthogonal 2D feature planes that evolve over time, enabling
efficient and compact spatiotemporal representation. These features are
explicitly warped into a canonical space via a deformation offset field,
eliminating the need for MLP-based motion modeling.
  In canonical space, we replace traditional MLP decoders with a structured
SH-based rendering head that synthesizes view-dependent color via attention
over learned frequency bands improving both interpretability and rendering
efficiency. To further enhance fidelity and temporal consistency, we introduce
a transformer-guided latent diffusion module that refines the tri-plane and
deformation features in a compressed latent space. This generative module
denoises scene representations under ambiguous or out-of-distribution (OOD)
motion, improving generalization.
  Our model is trained in two stages: the diffusion module is first pre-trained
independently, and then fine-tuned jointly with the full pipeline using a
combination of image reconstruction, diffusion denoising, and temporal
consistency losses. We demonstrate state-of-the-art results on synthetic
benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian
Splatting in visual quality, temporal coherence, and robustness to sparse-view
dynamic inputs.



---

## RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai | cs.CV | [PDF](http://arxiv.org/pdf/2505.15737v1){: .btn .btn-green } |

**Abstract**: Reconstructing high-fidelity underwater scenes remains a challenging task due
to light absorption, scattering, and limited visibility inherent in aquatic
environments. This paper presents an enhanced Gaussian Splatting-based
framework that improves both the visual quality and geometric accuracy of deep
underwater rendering. We propose decoupled learning for RGB channels, guided by
the physics of underwater attenuation, to enable more accurate colour
restoration. To address sparse-view limitations and improve view consistency,
we introduce a frame interpolation strategy with a novel adaptive weighting
scheme. Additionally, we introduce a new loss function aimed at reducing noise
while preserving edges, which is essential for deep-sea content. We also
release a newly collected dataset, Submerged3D, captured specifically in
deep-sea environments. Experimental results demonstrate that our framework
consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB,
delivering superior perceptual quality and robustness, and offering promising
directions for marine robotics and underwater visual analytics.

Comments:
- 10 pages, 3 figures. Submitted to BMVC 2025

---

## R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in  Unconstrained Image Collections

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, Xiangde Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.15294v1){: .btn .btn-green } |

**Abstract**: We propose R3GS, a robust reconstruction and relocalization framework
tailored for unconstrained datasets. Our method uses a hybrid representation
during training. Each anchor combines a global feature from a convolutional
neural network (CNN) with a local feature encoded by the multiresolution hash
grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict
the attributes of each Gaussians, including color, opacity, and covariance. To
mitigate the adverse effects of transient objects on the reconstruction
process, we ffne-tune a lightweight human detection network. Once ffne-tuned,
this network generates a visibility map that efffciently generalizes to other
transient objects (such as posters, banners, and cars) with minimal need for
further adaptation. Additionally, to address the challenges posed by sky
regions in outdoor scenes, we propose an effective sky-handling technique that
incorporates a depth prior as a constraint. This allows the inffnitely distant
sky to be represented on the surface of a large-radius sky sphere,
signiffcantly reducing ffoaters caused by errors in sky reconstruction.
Furthermore, we introduce a novel relocalization method that remains robust to
changes in lighting conditions while estimating the camera pose of a given
image within the reconstructed 3DGS scene. As a result, R3GS significantly
enhances rendering ffdelity, improves both training and rendering efffciency,
and reduces storage requirements. Our method achieves state-of-the-art
performance compared to baseline methods on in-the-wild datasets. The code will
be made open-source following the acceptance of the paper.

Comments:
- 7 pages, 4 figures

---

## GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Yuchen Li, Chaoran Feng, Zhenyu Tang, Kaiyuan Deng, Wangbo Yu, Yonghong Tian, Li Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2505.15287v1){: .btn .btn-green } |

**Abstract**: We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic
event dataset for high-fidelity event vision tasks, captured from real-world
sparse multi-view RGB images. Existing event datasets are often synthesized
from dense RGB videos, which typically lack viewpoint diversity and geometric
consistency, or depend on expensive, difficult-to-scale hardware setups. GS2E
overcomes these limitations by first reconstructing photorealistic static
scenes using 3D Gaussian Splatting, and subsequently employing a novel,
physically-informed event simulation pipeline. This pipeline generally
integrates adaptive trajectory interpolation with physically-consistent event
contrast threshold modeling. Such an approach yields temporally dense and
geometrically consistent event streams under diverse motion and lighting
conditions, while ensuring strong alignment with underlying scene structures.
Experimental results on event-based 3D reconstruction demonstrate GS2E's
superior generalization capabilities and its practical value as a benchmark for
advancing event vision research.

Comments:
- 21 pages, 7 figures. More details at
  http://intothemild.github.io/GS2E.github.io

---

## X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to  Computed Tomography

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Yifan Liu, Wuyang Li, Weihao Yu, Chenxin Li, Alexandre Alahi, Max Meng, Yixuan Yuan | eess.IV | [PDF](http://arxiv.org/pdf/2505.15235v1){: .btn .btn-green } |

**Abstract**: Computed Tomography serves as an indispensable tool in clinical workflows,
providing non-invasive visualization of internal anatomical structures.
Existing CT reconstruction works are limited to small-capacity model
architecture, inflexible volume representation, and small-scale training data.
In this paper, we present X-GRM (X-ray Gaussian Reconstruction Model), a large
feedforward model for reconstructing 3D CT from sparse-view 2D X-ray
projections. X-GRM employs a scalable transformer-based architecture to encode
an arbitrary number of sparse X-ray inputs, where tokens from different views
are integrated efficiently. Then, tokens are decoded into a new volume
representation, named Voxel-based Gaussian Splatting (VoxGS), which enables
efficient CT volume extraction and differentiable X-ray rendering. To support
the training of X-GRM, we collect ReconX-15K, a large-scale CT reconstruction
dataset containing around 15,000 CT/X-ray pairs across diverse organs,
including the chest, abdomen, pelvis, and tooth etc. This combination of a
high-capacity model, flexible volume representation, and large-scale training
data empowers our model to produce high-quality reconstructions from various
testing inputs, including in-domain and out-domain X-ray projections. Project
Page: https://github.com/CUHK-AIM-Group/X-GRM.



---

## GT^2-GS: Geometry-aware Texture Transfer for Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Wenjie Liu, Zhongliang Liu, Junwei Shu, Changbo Wang, Yang Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.15208v1){: .btn .btn-green } |

**Abstract**: Transferring 2D textures to 3D modalities is of great significance for
improving the efficiency of multimedia content creation. Existing approaches
have rarely focused on transferring image textures onto 3D representations. 3D
style transfer methods are capable of transferring abstract artistic styles to
3D scenes. However, these methods often overlook the geometric information of
the scene, which makes it challenging to achieve high-quality 3D texture
transfer results. In this paper, we present GT^2-GS, a geometry-aware texture
transfer framework for gaussian splitting. From the perspective of matching
texture features with geometric information in rendered views, we identify the
issue of insufficient texture features and propose a geometry-aware texture
augmentation module to expand the texture feature set. Moreover, a
geometry-consistent texture loss is proposed to optimize texture features into
the scene representation. This loss function incorporates both camera pose and
3D geometric information of the scene, enabling controllable texture-oriented
appearance editing. Finally, a geometry preservation strategy is introduced. By
alternating between the texture transfer and geometry correction stages over
multiple iterations, this strategy achieves a balance between learning texture
features and preserving geometric integrity. Extensive experiments demonstrate
the effectiveness and controllability of our method. Through geometric
awareness, our approach achieves texture transfer results that better align
with human visual perception. Our homepage is available at
https://vpx-ecnu.github.io/GT2-GS-website.

Comments:
- 15 pages, 16 figures

---

## MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth  Foundation Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Yifan Liu, Keyu Fan, Weihao Yu, Chenxin Li, Hao Lu, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2505.15185v1){: .btn .btn-green } |

**Abstract**: Recent advances in generalizable 3D Gaussian Splatting have demonstrated
promising results in real-time high-fidelity rendering without per-scene
optimization, yet existing approaches still struggle to handle unfamiliar
visual content during inference on novel scenes due to limited
generalizability. To address this challenge, we introduce MonoSplat, a novel
framework that leverages rich visual priors from pre-trained monocular depth
foundation models for robust Gaussian reconstruction. Our approach consists of
two key components: a Mono-Multi Feature Adapter that transforms monocular
features into multi-view representations, coupled with an Integrated Gaussian
Prediction module that effectively fuses both feature types for precise
Gaussian generation. Through the Adapter's lightweight attention mechanism,
features are seamlessly aligned and aggregated across views while preserving
valuable monocular priors, enabling the Prediction module to generate Gaussian
primitives with accurate geometry and appearance. Through extensive experiments
on diverse real-world datasets, we convincingly demonstrate that MonoSplat
achieves superior reconstruction quality and generalization capability compared
to existing methods while maintaining computational efficiency with minimal
trainable parameters. Codes are available at
https://github.com/CUHK-AIM-Group/MonoSplat.



---

## PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Zane K J Hartley, Lewis A G Stuart, Andrew P French, Michael P Pound | cs.CV | [PDF](http://arxiv.org/pdf/2505.15528v1){: .btn .btn-green } |

**Abstract**: Recent years have seen substantial improvements in the ability to generate
synthetic 3D objects using AI. However, generating complex 3D objects, such as
plants, remains a considerable challenge. Current generative 3D models struggle
with plant generation compared to general objects, limiting their usability in
plant analysis tools, which require fine detail and accurate geometry. We
introduce PlantDreamer, a novel approach to 3D synthetic plant generation,
which can achieve greater levels of realism for complex plant geometry and
textures than available text-to-3D models. To achieve this, our new generation
pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an
adaptable Gaussian culling algorithm, which directly improve textural realism
and geometric integrity of generated 3D plant models. Additionally,
PlantDreamer enables both purely synthetic plant generation, by leveraging
L-System-generated meshes, and the enhancement of real-world plant point clouds
by converting them into 3D Gaussian Splats. We evaluate our approach by
comparing its outputs with state-of-the-art text-to-3D models, demonstrating
that PlantDreamer outperforms existing methods in producing high-fidelity
synthetic plants. Our results indicate that our approach not only advances
synthetic plant generation, but also facilitates the upgrading of legacy point
cloud datasets, making it a valuable tool for 3D phenotyping applications.

Comments:
- 13 pages, 5 figures, 4 tables

---

## Personalize Your Gaussian: Consistent 3D Scene Personalization from a  Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-20 | Yuxuan Wang, Xuanyu Yi, Qingshan Xu, Yuan Zhou, Long Chen, Hanwang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2505.14537v1){: .btn .btn-green } |

**Abstract**: Personalizing 3D scenes from a single reference image enables intuitive
user-guided editing, which requires achieving both multi-view consistency
across perspectives and referential consistency with the input image. However,
these goals are particularly challenging due to the viewpoint bias caused by
the limited perspective provided in a single image. Lacking the mechanisms to
effectively expand reference information beyond the original view, existing
methods of image-conditioned 3DGS personalization often suffer from this
viewpoint bias and struggle to produce consistent results. Therefore, in this
paper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),
a framework that progressively propagates the single-view reference appearance
to novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D
generation and iterative LoRA fine-tuning to extract and extend the reference
appearance, and finally produces faithful multi-view guidance images and the
personalized 3DGS outputs through a view-consistent generation process guided
by geometric cues. Extensive experiments on real-world scenes show that our
CP-GS effectively mitigates the viewpoint bias, achieving high-quality
personalization that significantly outperforms existing methods. The code will
be released at https://github.com/Yuxuan-W/CP-GS.

Comments:
- 9 pages

---

## Scan, Materialize, Simulate: A Generalizable Framework for Physically  Grounded Robot Planning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-20 | Amine Elhafsi, Daniel Morton, Marco Pavone | cs.RO | [PDF](http://arxiv.org/pdf/2505.14938v1){: .btn .btn-green } |

**Abstract**: Autonomous robots must reason about the physical consequences of their
actions to operate effectively in unstructured, real-world environments. We
present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D
Gaussian Splatting for accurate scene reconstruction, visual foundation models
for semantic segmentation, vision-language models for material property
inference, and physics simulation for reliable prediction of action outcomes.
By integrating these components, SMS enables generalizable physical reasoning
and object-centric planning without the need to re-learn foundational physical
dynamics. We empirically validate SMS in a billiards-inspired manipulation task
and a challenging quadrotor landing scenario, demonstrating robust performance
on both simulated domain transfer and real-world experiments. Our results
highlight the potential of bridging differentiable rendering for scene
reconstruction, foundation models for semantic understanding, and physics-based
simulation to achieve physically grounded robot planning across diverse
settings.



---

## MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-20 | Zhenyu Bao, Qing Li, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.13839v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention in streamable
dynamic novel view synthesis (DNVS) for its photorealistic rendering capability
and computational efficiency. Despite much progress in improving rendering
quality and optimization strategies, 3DGS-based streamable dynamic scene
reconstruction still suffers from flickering artifacts and storage
inefficiency, and struggles to model the emerging objects. To tackle this, we
introduce MGStream which employs the motion-related 3D Gaussians (3DGs) to
reconstruct the dynamic and the vanilla 3DGs for the static. The motion-related
3DGs are implemented according to the motion mask and the clustering-based
convex hull algorithm. The rigid deformation is applied to the motion-related
3DGs for modeling the dynamic, and the attention-based optimization on the
motion-related 3DGs enables the reconstruction of the emerging objects. As the
deformation and optimization are only conducted on the motion-related 3DGs,
MGStream avoids flickering artifacts and improves the storage efficiency.
Extensive experiments on real-world datasets N3DV and MeetRoom demonstrate that
MGStream surpasses existing streaming 3DGS-based approaches in terms of
rendering quality, training/storage efficiency and temporal consistency. Our
code is available at: https://github.com/pcl3dv/MGStream.



---

## 3D Gaussian Adaptive Reconstruction for Fourier Light-Field Microscopy

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Chenyu Xu, Zhouyu Jin, Chengkang Shen, Hao Zhu, Zhan Ma, Bo Xiong, You Zhou, Xun Cao, Ning Gu | eess.IV | [PDF](http://arxiv.org/pdf/2505.12875v1){: .btn .btn-green } |

**Abstract**: Compared to light-field microscopy (LFM), which enables high-speed volumetric
imaging but suffers from non-uniform spatial sampling, Fourier light-field
microscopy (FLFM) introduces sub-aperture division at the pupil plane, thereby
ensuring spatially invariant sampling and enhancing spatial resolution.
Conventional FLFM reconstruction methods, such as Richardson-Lucy (RL)
deconvolution, exhibit poor axial resolution and signal degradation due to the
ill-posed nature of the inverse problem. While data-driven approaches enhance
spatial resolution by leveraging high-quality paired datasets or imposing
structural priors, Neural Radiance Fields (NeRF)-based methods employ
physics-informed self-supervised learning to overcome these limitations, yet
they are hindered by substantial computational costs and memory demands.
Therefore, we propose 3D Gaussian Adaptive Tomography (3DGAT) for FLFM, a 3D
gaussian splatting based self-supervised learning framework that significantly
improves the volumetric reconstruction quality of FLFM while maintaining
computational efficiency. Experimental results indicate that our approach
achieves higher resolution and improved reconstruction accuracy, highlighting
its potential to advance FLFM imaging and broaden its applications in 3D
optical microscopy.



---

## IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping  Extraction via NeRF-SAM2 Fusion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Wentao Song, He Huang, Youqiang Sun, Fang Qu, Jiaqi Zhang, Longhui Fang, Yuwei Hao, Chenyang Peng | cs.CV | [PDF](http://arxiv.org/pdf/2505.13633v1){: .btn .btn-green } |

**Abstract**: Advanced plant phenotyping technologies play a crucial role in targeted trait
improvement and accelerating intelligent breeding. Due to the species diversity
of plants, existing methods heavily rely on large-scale high-precision manually
annotated data. For self-occluded objects at the grain level, unsupervised
methods often prove ineffective. This study proposes IPENS, an interactive
unsupervised multi-target point cloud extraction method. The method utilizes
radiance field information to lift 2D masks, which are segmented by SAM2
(Segment Anything Model 2), into 3D space for target point cloud extraction. A
multi-target collaborative optimization strategy is designed to effectively
resolve the single-interaction multi-target segmentation challenge.
Experimental validation demonstrates that IPENS achieves a grain-level
segmentation accuracy (mIoU) of 63.72% on a rice dataset, with strong
phenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697
(RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf length
and width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On a
wheat dataset,IPENS further improves segmentation accuracy to 89.68% (mIoU),
with equally outstanding phenotypic estimation performance: spike volume
prediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00
(RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92
(RMSE = 0.23 and 0.15). This method provides a non-invasive, high-quality
phenotyping extraction solution for rice and wheat. Without requiring annotated
data, it rapidly extracts grain-level point clouds within 3 minutes through
simple single-round interactions on images for multiple targets, demonstrating
significant potential to accelerate intelligent breeding efficiency.



---

## Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Seungjun Oh, Younggeun Lee, Hyejin Jeon, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2505.13215v1){: .btn .btn-green } |

**Abstract**: Recent advancements in dynamic 3D scene reconstruction have shown promising
results, enabling high-fidelity 3D novel view synthesis with improved temporal
consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an
appealing approach due to its ability to model high-fidelity spatial and
temporal variations. However, existing methods suffer from substantial
computational and memory overhead due to the redundant allocation of 4D
Gaussians to static regions, which can also degrade image quality. In this
work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework
that adaptively represents static regions with 3D Gaussians while reserving 4D
Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian
representation and iteratively converts temporally invariant Gaussians into 3D,
significantly reducing the number of parameters and improving computational
efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,
capturing complex motions with high fidelity. Our approach achieves
significantly faster training times compared to baseline 4D Gaussian Splatting
methods while maintaining or improving the visual quality.

Comments:
- https://ohsngjun.github.io/3D-4DGS/

---

## TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D  Semantic Occupancy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Luyao Lei, Shuo Xu, Yifan Bai, Xing Wei | cs.CV | [PDF](http://arxiv.org/pdf/2505.12693v1){: .btn .btn-green } |

**Abstract**: The performance of multi-modal 3D occupancy prediction is limited by
ineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion
strategies and surface detail loss caused by sparse, noisy annotations. The
mismatch stems from the heterogeneous scale and distribution of point cloud and
image features, leading to biased matching under fixed neighborhood fusion. To
address this, we propose a target-scale adaptive, bidirectional symmetric
retrieval mechanism. It expands the neighborhood for large targets to enhance
context awareness and shrinks it for small ones to improve efficiency and
suppress noise, enabling accurate cross-modal feature alignment. This mechanism
explicitly establishes spatial correspondences and improves fusion accuracy.
For surface detail loss, sparse labels provide limited supervision, resulting
in poor predictions for small objects. We introduce an improved volume
rendering pipeline based on 3D Gaussian Splatting, which takes fused features
as input to render images, applies photometric consistency supervision, and
jointly optimizes 2D-3D consistency. This enhances surface detail
reconstruction while suppressing noise propagation. In summary, we propose
TACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy
prediction, enhanced by volume rendering supervision. Experiments on the
nuScenes and SemanticKITTI benchmarks validate its effectiveness.



---

## Recollection from Pensieve: Novel View Synthesis via Learning from  Uncalibrated Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Ruoyu Wang, Yi Ma, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2505.13440v1){: .btn .btn-green } |

**Abstract**: Currently almost all state-of-the-art novel view synthesis and reconstruction
models rely on calibrated cameras or additional geometric priors for training.
These prerequisites significantly limit their applicability to massive
uncalibrated data. To alleviate this requirement and unlock the potential for
self-supervised training on large-scale uncalibrated videos, we propose a novel
two-stage strategy to train a view synthesis model from only raw video frames
or multi-view images, without providing camera parameters or other priors. In
the first stage, we learn to reconstruct the scene implicitly in a latent space
without relying on any explicit 3D representation. Specifically, we predict
per-frame latent camera and scene context features, and employ a view synthesis
model as a proxy for explicit rendering. This pretraining stage substantially
reduces the optimization complexity and encourages the network to learn the
underlying 3D consistency in a self-supervised manner. The learned latent
camera and implicit scene representation have a large gap compared with the
real 3D world. To reduce this gap, we introduce the second stage training by
explicitly predicting 3D Gaussian primitives. We additionally apply explicit
Gaussian Splatting rendering loss and depth projection loss to align the
learned latent representations with physically grounded 3D geometry. In this
way, Stage 1 provides a strong initialization and Stage 2 enforces 3D
consistency - the two stages are complementary and mutually beneficial.
Extensive experiments demonstrate the effectiveness of our approach, achieving
high-quality novel view synthesis and accurate camera pose estimation, compared
to methods that employ supervision with calibration, pose, or depth
information. The code is available at https://github.com/Dwawayu/Pensieve.

Comments:
- 13 pages, 4 figures

---

## Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-18 | Calvin Galagain, Martyna Poreba, Fran√ßois Goulette | cs.RO | [PDF](http://arxiv.org/pdf/2505.12384v1){: .btn .btn-green } |

**Abstract**: In embedded systems, robots must perceive and interpret their environment
efficiently to operate reliably in real-world conditions. Visual Semantic SLAM
(Simultaneous Localization and Mapping) enhances standard SLAM by incorporating
semantic information into the map, enabling more informed decision-making.
However, implementing such systems on resource-limited hardware involves
trade-offs between accuracy, computing efficiency, and power usage.
  This paper provides a comparative review of recent Semantic Visual SLAM
methods with a focus on their applicability to embedded platforms. We analyze
three main types of architectures - Geometric SLAM, Neural Radiance Fields
(NeRF), and 3D Gaussian Splatting - and evaluate their performance on
constrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their
accuracy, segmentation quality, memory usage, and energy consumption.
  Our results show that methods based on NeRF and Gaussian Splatting achieve
high semantic detail but demand substantial computing resources, limiting their
use on embedded devices. In contrast, Semantic Geometric SLAM offers a more
practical balance between computational cost and accuracy. The review
highlights a need for SLAM algorithms that are better adapted to embedded
environments, and it discusses key directions for improving their efficiency
through algorithm-hardware co-design.



---

## GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects  Based on Appearance and Geometric Complexity

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-17 | Takuya Ikeda, Sergey Zakharov, Muhammad Zubair Irshad, Istvan Balazs Opra, Shun Iwase, Dian Chen, Mark Tjersland, Robert Lee, Alexandre Dilly, Rares Ambrus, Koichi Nishiwaki | cs.CV | [PDF](http://arxiv.org/pdf/2505.11905v1){: .btn .btn-green } |

**Abstract**: We present a novel method for 6-DoF object tracking and high-quality 3D
reconstruction from monocular RGBD video. Existing methods, while achieving
impressive results, often struggle with complex objects, particularly those
exhibiting symmetry, intricate geometry or complex appearance. To bridge these
gaps, we introduce an adaptive method that combines 3D Gaussian Splatting,
hybrid geometry/appearance tracking, and key frame selection to achieve robust
tracking and accurate reconstructions across a diverse range of objects.
Additionally, we present a benchmark covering these challenging object classes,
providing high-quality annotations for evaluating both tracking and
reconstruction performance. Our approach demonstrates strong capabilities in
recovering high-fidelity object meshes, setting a new standard for
single-sensor 3D reconstruction in open-world environments.

Comments:
- main contains 10 pages, 9 figures. And supplementary material
  contains 10 pages, 27 figures

---

## MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-17 | Hongyi Zhou, Xiaogang Wang, Yulan Guo, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.11868v1){: .btn .btn-green } |

**Abstract**: Accurately analyzing the motion parts and their motion attributes in dynamic
environments is crucial for advancing key areas such as embodied intelligence.
Addressing the limitations of existing methods that rely on dense multi-view
images or detailed part-level annotations, we propose an innovative framework
that can analyze 3D mobility from monocular videos in a zero-shot manner. This
framework can precisely parse motion parts and motion attributes only using a
monocular video, completely eliminating the need for annotated training data.
Specifically, our method first constructs the scene geometry and roughly
analyzes the motion parts and their initial motion attributes combining depth
estimation, optical flow analysis and point cloud registration method, then
employs 2D Gaussian splatting for scene representation. Building on this, we
introduce an end-to-end dynamic scene optimization algorithm specifically
designed for articulated objects, refining the initial analysis results to
ensure the system can handle 'rotation', 'translation', and even complex
movements ('rotation+translation'), demonstrating high flexibility and
versatility. To validate the robustness and wide applicability of our method,
we created a comprehensive dataset comprising both simulated and real-world
scenarios. Experimental results show that our framework can effectively analyze
articulated object motions in an annotation-free manner, showcasing its
significant potential in future embodied intelligence applications.



---

## Gaussian Splatting as a Unified Representation for Autonomy in  Unstructured Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-17 | Dexter Ong, Yuezhan Tao, Varun Murali, Igor Spasojevic, Vijay Kumar, Pratik Chaudhari | cs.RO | [PDF](http://arxiv.org/pdf/2505.11794v1){: .btn .btn-green } |

**Abstract**: In this work, we argue that Gaussian splatting is a suitable unified
representation for autonomous robot navigation in large-scale unstructured
outdoor environments. Such environments require representations that can
capture complex structures while remaining computationally tractable for
real-time navigation. We demonstrate that the dense geometric and photometric
information provided by a Gaussian splatting representation is useful for
navigation in unstructured environments. Additionally, semantic information can
be embedded in the Gaussian map to enable large-scale task-driven navigation.
From the lessons learned through our experiments, we highlight several
challenges and opportunities arising from the use of such a representation for
robot autonomy.



---

## EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced  Quality for outdoor scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Jianlin Guo, Haihong Xiao, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2505.10787v1){: .btn .btn-green } |

**Abstract**: Efficient scene representations are essential for many real-world
applications, especially those involving spatial measurement. Although current
NeRF-based methods have achieved impressive results in reconstructing
building-scale scenes, they still suffer from slow training and inference
speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian
Splatting (3DGS) has demonstrated excellent performance with its high-quality
rendering and real-time speed, especially for objects and small-scale scenes.
However, in outdoor scenes, its point-based explicit representation lacks an
effective adjustment mechanism, and the millions of Gaussian points required
often lead to memory constraints during training. To address these challenges,
we propose EA-3DGS, a high-quality real-time rendering method designed for
outdoor scenes. First, we introduce a mesh structure to regulate the
initialization of Gaussian components by leveraging an adaptive tetrahedral
mesh that partitions the grid and initializes Gaussian components on each face,
effectively capturing geometric structures in low-texture regions. Second, we
propose an efficient Gaussian pruning strategy that evaluates each 3D
Gaussian's contribution to the view and prunes accordingly. To retain
geometry-critical Gaussian points, we also present a structure-aware
densification strategy that densifies Gaussian points in low-curvature regions.
Additionally, we employ vector quantization for parameter quantization of
Gaussian components, significantly reducing disk space requirements with only a
minimal impact on rendering quality. Extensive experiments on 13 scenes,
including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \&
Temples, WHU) and five self-collected scenes acquired through UAV
photogrammetry measurement from SCUT-CA and plateau regions, further
demonstrate the superiority of our method.



---

## GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian  Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Simeon Adebola, Shuangyu Xie, Chung Min Kim, Justin Kerr, Bart M. van Marrewijk, Mieke van Vlaardingen, Tim van Daalen, Robert van Loo, Jose Luis Susa Rincon, Eugen Solowjow, Rick van de Zedde, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2505.10923v1){: .btn .btn-green } |

**Abstract**: Accurate temporal reconstructions of plant growth are essential for plant
phenotyping and breeding, yet remain challenging due to complex geometries,
occlusions, and non-rigid deformations of plants. We present a novel framework
for building temporal digital twins of plants by combining 3D Gaussian
Splatting with a robust sample alignment pipeline. Our method begins by
reconstructing Gaussian Splats from multi-view camera data, then leverages a
two-stage registration approach: coarse alignment through feature-based
matching and Fast Global Registration, followed by fine alignment with
Iterative Closest Point. This pipeline yields a consistent 4D model of plant
development in discrete time steps. We evaluate the approach on data from the
Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal
reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at
https://berkeleyautomation.github.io/GrowSplat/



---

## Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Abhishek Kashyap, Henrik Andreasson, Todor Stoyanov | cs.RO | [PDF](http://arxiv.org/pdf/2505.11467v1){: .btn .btn-green } |

**Abstract**: Vision based robot manipulation uses cameras to capture one or more images of
a scene containing the objects to be manipulated. Taking multiple images can
help if any object is occluded from one viewpoint but more visible from another
viewpoint. However, the camera has to be moved to a sequence of suitable
positions for capturing multiple images, which requires time and may not always
be possible, due to reachability constraints. So while additional images can
produce more accurate grasp poses due to the extra information available, the
time-cost goes up with the number of additional views sampled. Scene
representations like Gaussian Splatting are capable of rendering accurate
photorealistic virtual images from user-specified novel viewpoints. In this
work, we show initial results which indicate that novel view synthesis can
provide additional context in generating grasp poses. Our experiments on the
Graspnet-1billion dataset show that novel views contributed force-closure
grasps in addition to the force-closure grasps obtained from sparsely sampled
real views while also improving grasp coverage. In the future we hope this work
can be extended to improve grasp extraction from radiance fields constructed
with a single input image, using for example diffusion models or generalizable
radiance fields.

Comments:
- 6 pages

---

## MutualNeRF: Improve the Performance of NeRF under Limited Samples with  Mutual Information Theory

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Zifan Wang, Jingwei Li, Yitang Li, Yunze Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.11386v1){: .btn .btn-green } |

**Abstract**: This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field
(NeRF) performance under limited samples using Mutual Information Theory. While
NeRF excels in 3D scene synthesis, challenges arise with limited data and
existing methods that aim to introduce prior knowledge lack theoretical support
in a unified framework. We introduce a simple but theoretically robust concept,
Mutual Information, as a metric to uniformly measure the correlation between
images, considering both macro (semantic) and micro (pixel) levels.
  For sparse view sampling, we strategically select additional viewpoints
containing more non-overlapping scene information by minimizing mutual
information without knowing ground truth images beforehand. Our framework
employs a greedy algorithm, offering a near-optimal solution.
  For few-shot view synthesis, we maximize the mutual information between
inferred images and ground truth, expecting inferred images to gain more
relevant information from known images. This is achieved by incorporating
efficient, plug-and-play regularization terms.
  Experiments under limited samples show consistent improvement over
state-of-the-art baselines in different settings, affirming the efficacy of our
framework.



---

## VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, Fernando de la Torre | cs.GR | [PDF](http://arxiv.org/pdf/2505.10144v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.

Comments:
- I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/

---

## Consistent Quantity-Quality Control across Scenes for Deployment-Aware  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Fengdi Zhang, Hongkun Cao, Ruqi Huang | cs.CV | [PDF](http://arxiv.org/pdf/2505.10473v2){: .btn .btn-green } |

**Abstract**: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control. Through a single training run using a
fixed setup and a user-specified hyperparameter reflecting quantity-quality
preference, ControlGS can automatically find desirable quantity-quality
trade-off points across diverse scenes, from compact objects to large outdoor
scenes. It also outperforms baselines by achieving higher rendering quality
with fewer Gaussians, and supports a broad adjustment range with stepless
control over the trade-off. Project page:
https://zhang-fengdi.github.io/ControlGS/

Comments:
- 16 pages, 7 figures, 7 tables. Project page available at
  https://zhang-fengdi.github.io/ControlGS/

---

## Advances in Radiance Field for Dynamic Scene: From Neural Field to  Gaussian Field

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Jinlong Fan, Xuepu Zeng, Jing Zhang, Mingming Gong, Yuxiang Yang, Dacheng Tao | cs.CV | [PDF](http://arxiv.org/pdf/2505.10049v2){: .btn .btn-green } |

**Abstract**: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.



---

## Large-Scale Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2505.09915v1){: .btn .btn-green } |

**Abstract**: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.



---

## Neural Video Compression using 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Lakshya Gupta, Imran N. Junejo | cs.CV | [PDF](http://arxiv.org/pdf/2505.09324v1){: .btn .btn-green } |

**Abstract**: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

Comments:
- 9 pages, 8 figures

---

## ExploreGS: a vision-based low overhead framework for 3D scene  reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Yunji Feng, Chengpu Yu, Fengrui Ran, Zhi Yang, Yinni Liu | eess.IV | [PDF](http://arxiv.org/pdf/2505.10578v1){: .btn .btn-green } |

**Abstract**: This paper proposes a low-overhead, vision-based 3D scene reconstruction
framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces
traditional lidar-based point cloud acquisition process with a vision model,
achieving a high-quality reconstruction at a lower cost. The framework
integrates scene exploration and model reconstruction, and leverags a
Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore,
the 3D Gaussian Splatting (3DGS) training can be executed on-board.
Comprehensive experiments in both simulation and real-world environments
demonstrate the efficiency and applicability of the ExploreGS framework on
resource-constrained devices, while maintaining reconstruction quality
comparable to state-of-the-art methods.



---

## Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Ma Changfeng, Bi Ran, Guo Jie, Wang Chongjun, Guo Yanwen | cs.CV | [PDF](http://arxiv.org/pdf/2505.09413v1){: .btn .btn-green } |

**Abstract**: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

Comments:
- CVPR 2025 Accepted

---

## FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous  Driving via Point-Level Dynamic-Static Decoupling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Yue Wen, Liang Song, Yijia Liu, Siting Zhu, Yanzi Miao, Lijun Han, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2505.09406v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

Comments:
- 7 pages, 9 figures, accepted by ICRA2025

---

## Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or  Robot Hardware

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2505.09601v1){: .btn .btn-green } |

**Abstract**: Scaling robot learning requires vast and diverse datasets. Yet the prevailing
data collection paradigm-human teleoperation-remains costly and constrained by
manual effort and physical robot access. We introduce Real2Render2Real (R2R2R),
a novel approach for generating robot training data without relying on object
dynamics simulation or teleoperation of robot hardware. The input is a
smartphone-captured scan of one or more objects and a single video of a human
demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic
demonstrations by reconstructing detailed 3D object geometry and appearance,
and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to
enable flexible asset generation and trajectory synthesis for both rigid and
articulated objects, converting these representations to meshes to maintain
compatibility with scalable rendering engines like IsaacLab but with collision
modeling off. Robot demonstration data generated by R2R2R integrates directly
with models that operate on robot proprioceptive states and image observations,
such as vision-language-action models (VLA) and imitation learning policies.
Physical experiments suggest that models trained on R2R2R data from a single
human demonstration can match the performance of models trained on 150 human
teleoperation demonstrations. Project page: https://real2render2real.com



---

## FOCI: Trajectory Optimization on Gaussian Splats

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Mario Gomez Andreu, Maximum Wilder-Smith, Victor Klemm, Vaishakh Patil, Jesus Tordesillas, Marco Hutter | cs.RO | [PDF](http://arxiv.org/pdf/2505.08510v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently gained popularity as a faster
alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view
synthesis methods. Leveraging the spatial information encoded in 3DGS, this
work proposes FOCI (Field Overlap Collision Integral), an algorithm that is
able to optimize trajectories directly on the Gaussians themselves. FOCI
leverages a novel and interpretable collision formulation for 3DGS using the
notion of the overlap integral between Gaussians. Contrary to other approaches,
which represent the robot with conservative bounding boxes that underestimate
the traversability of the environment, we propose to represent the environment
and the robot as Gaussian Splats. This not only has desirable computational
properties, but also allows for orientation-aware planning, allowing the robot
to pass through very tight and narrow spaces. We extensively test our algorithm
in both synthetic and real Gaussian Splats, showcasing that collision-free
trajectories for the ANYmal legged robot that can be computed in a few seconds,
even with hundreds of thousands of Gaussians making up the environment. The
project page and code are available at
https://rffr.leggedrobotics.com/works/foci/

Comments:
- 7 pages, 8 figures, Mario Gomez Andreu and Maximum Wilder-Smith
  contributed equally

---

## A Survey of 3D Reconstruction with Event Cameras: From Event-based  Geometry to Neural 3D Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Ying Zhou, Vera Chung, Qiang Qu | cs.CV | [PDF](http://arxiv.org/pdf/2505.08438v1){: .btn .btn-green } |

**Abstract**: Event cameras have emerged as promising sensors for 3D reconstruction due to
their ability to capture per-pixel brightness changes asynchronously. Unlike
conventional frame-based cameras, they produce sparse and temporally rich data
streams, which enable more accurate 3D reconstruction and open up the
possibility of performing reconstruction in extreme environments such as
high-speed motion, low light, or high dynamic range scenes. In this survey, we
provide the first comprehensive review focused exclusively on 3D reconstruction
using event cameras. The survey categorises existing works into three major
types based on input modality - stereo, monocular, and multimodal systems, and
further classifies them by reconstruction approach, including geometry-based,
deep learning-based, and recent neural rendering techniques such as Neural
Radiance Fields and 3D Gaussian Splatting. Methods with a similar research
focus were organised chronologically into the most subdivided groups. We also
summarise public datasets relevant to event-based 3D reconstruction. Finally,
we highlight current research limitations in data availability, evaluation,
representation, and dynamic scene handling, and outline promising future
research directions. This survey aims to serve as a comprehensive reference and
a roadmap for future developments in event-driven 3D reconstruction.

Comments:
- 35 pages, 12 figures, 11 tables

---

## ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for  Dynamic Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | He Huang, Qi Yang, Mufan Liu, Yiling Xu, Zhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.08196v1){: .btn .btn-green } |

**Abstract**: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from
a canonical space to target frames, which overlooks redundancy among adjacent
Gaussian primitives and results in suboptimal performance. To address this
limitation, we propose Anchor-Driven Deformable and Compressed Gaussian
Splatting (ADC-GS), a compact and efficient representation for dynamic scene
reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an
anchor-based structure within the canonical space, enhanced by a temporal
significance-based anchor refinement strategy. To reduce deformation
redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that
captures motions at varying granularities. Moreover, a rate-distortion
optimization is adopted to achieve an optimal balance between bitrate
consumption and representation fidelity. Experimental results demonstrate that
ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed
by 300%-800% while achieving state-of-the-art storage efficiency without
compromising rendering quality. The code is released at
https://github.com/H-Huang774/ADC-GS.git.



---

## NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged  Information Guidance


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Wenzhe Cai, Jiaqi Peng, Yuqiang Yang, Yujian Zhang, Meng Wei, Hanqing Wang, Yilun Chen, Tai Wang, Jiangmiao Pang | cs.RO | [PDF](http://arxiv.org/pdf/2505.08712v2){: .btn .btn-green } |

**Abstract**: Learning navigation in dynamic open-world environments is an important yet
challenging skill for robots. Most previous methods rely on precise
localization and mapping or learn from expensive real-world demonstrations. In
this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end
framework trained solely in simulation and can zero-shot transfer to different
embodiments in diverse real-world environments. The key ingredient of NavDP's
network is the combination of diffusion-based trajectory generation and a
critic function for trajectory selection, which are conditioned on only local
observation tokens encoded from a shared policy transformer. Given the
privileged information of the global environment in simulation, we scale up the
demonstrations of good quality to train the diffusion policy and formulate the
critic value function targets with contrastive negative samples. Our
demonstration generation approach achieves about 2,500 trajectories/GPU per
day, 20$\times$ more efficient than real-world data collection, and results in
a large-scale navigation dataset with 363.2km trajectories across 1244 scenes.
Trained with this simulation dataset, NavDP achieves state-of-the-art
performance and consistently outstanding generalization capability on
quadruped, wheeled, and humanoid robots in diverse indoor and outdoor
environments. In addition, we present a preliminary attempt at using Gaussian
Splatting to make in-domain real-to-sim fine-tuning to further bridge the
sim-to-real gap. Experiments show that adding such real-to-sim data can improve
the success rate by 30\% without hurting its generalization capability.

Comments:
- Project Page:
  https://wzcai99.github.io/navigation-diffusion-policy.github.io/

---

## DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Holly Dinkel, Marcel B√ºsching, Alberta Longhini, Brian Coltin, Trey Smith, Danica Kragic, M√•rten Bj√∂rkman, Timothy Bretl | cs.CV | [PDF](http://arxiv.org/pdf/2505.08644v1){: .btn .btn-green } |

**Abstract**: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of
Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state
information through prediction-update filtering. The DLO-Splatting algorithm
uses a position-based dynamics model with shape smoothness and rigidity
dampening corrections to predict the object shape. Optimization with a 3D
Gaussian Splatting-based rendering loss iteratively renders and refines the
prediction to align it with the visual observations in the update step. Initial
experiments demonstrate promising results in a knot tying scenario, which is
challenging for existing vision-only methods.

Comments:
- 5 pages, 2 figures, presented at the 2025 5th Workshop: Reflections
  on Representations and Manipulating Deformable Objects at the IEEE
  International Conference on Robotics and Automation. RMDO workshop
  (https://deformable-workshop.github.io/icra2025/)

---

## TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin  Benchmark Dataset

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nie√üner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2505.07396v2){: .btn .btn-green } |

**Abstract**: Urban Digital Twins (UDTs) have become essential for managing cities and
integrating complex, heterogeneous data from diverse sources. Creating UDTs
involves challenges at multiple process stages, including acquiring accurate 3D
source data, reconstructing high-fidelity 3D models, maintaining models'
updates, and ensuring seamless interoperability to downstream tasks. Current
datasets are usually limited to one part of the processing chain, hampering
comprehensive UDTs validation. To address these challenges, we introduce the
first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.
This dataset includes georeferenced, semantically aligned 3D models and
networks along with various terrestrial, mobile, aerial, and satellite
observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently
767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high
accuracy, and multimodal data integration, the benchmark supports robust
analysis of sensors and the development of advanced reconstruction methods.
Additionally, we explore downstream tasks demonstrating the potential of
TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar
potential analysis, point cloud semantic segmentation, and LoD3 building
reconstruction. We are convinced this contribution lays a foundation for
overcoming current limitations in UDT creation, fostering new research
directions and practical solutions for smarter, data-driven urban environments.
The project is available under: https://tum2t.win

Comments:
- Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing

---

## SLAG: Scalable Language-Augmented Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Laszlo Szilagyi, Francis Engelmann, Jeannette Bohg | cs.CV | [PDF](http://arxiv.org/pdf/2505.08124v1){: .btn .btn-green } |

**Abstract**: Language-augmented scene representations hold great promise for large-scale
robotics applications such as search-and-rescue, smart cities, and mining. Many
of these scenarios are time-sensitive, requiring rapid scene encoding while
also being data-intensive, necessitating scalable solutions. Deploying these
representations on robots with limited computational resources further adds to
the challenge. To address this, we introduce SLAG, a multi-GPU framework for
language-augmented Gaussian splatting that enhances the speed and scalability
of embedding large scenes. Our method integrates 2D visual-language model
features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG
eliminates the need for a loss function to compute per-Gaussian language
embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters
via a normalized weighted average, enabling highly parallelized scene encoding.
Additionally, we introduce a vector database for efficient embedding storage
and retrieval. Our experiments show that SLAG achieves an 18 times speedup in
embedding computation on a 16-GPU setup compared to OpenGaussian, while
preserving embedding quality on the ScanNet and LERF datasets. For more
details, visit our project website: https://slag-project.github.io/.



---

## TUGS: Physics-based Compact Representation of Underwater Scenes by  Tensorized Gaussian

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Shijie Lian, Ziyi Zhang, Laurence Tianruo Yang and, Mengyu Ren, Debin Liu, Hua Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.08811v1){: .btn .btn-green } |

**Abstract**: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications



---

## GIFStream: 4D Gaussian-based Immersive Video with Feature Stream

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Hao Li, Sicheng Li, Xiang Gao, Abudouaihati Batuer, Lu Yu, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2505.07539v1){: .btn .btn-green } |

**Abstract**: Immersive video offers a 6-Dof-free viewing experience, potentially playing a
key role in future video technology. Recently, 4D Gaussian Splatting has gained
attention as an effective approach for immersive video due to its high
rendering efficiency and quality, though maintaining quality with manageable
storage remains challenging. To address this, we introduce GIFStream, a novel
4D Gaussian representation using a canonical space and a deformation field
enhanced with time-dependent feature streams. These feature streams enable
complex motion modeling and allow efficient compression by leveraging temporal
correspondence and motion-aware pruning. Additionally, we incorporate both
temporal and spatial compression networks for end-to-end compression.
Experimental results show that GIFStream delivers high-quality immersive video
at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project
page: https://xdimlab.github.io/GIFStream

Comments:
- 14 pages, 10 figures

---

## Geometric Prior-Guided Neural Implicit Surface Reconstruction in the  Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Lintao Xiang, Hongpei Zheng, Bailin Deng, Hujun Yin | cs.CV | [PDF](http://arxiv.org/pdf/2505.07373v1){: .btn .btn-green } |

**Abstract**: Neural implicit surface reconstruction using volume rendering techniques has
recently achieved significant advancements in creating high-fidelity surfaces
from multiple 2D images. However, current methods primarily target scenes with
consistent illumination and struggle to accurately reconstruct 3D geometry in
uncontrolled environments with transient occlusions or varying appearances.
While some neural radiance field (NeRF)-based variants can better manage
photometric variations and transient objects in complex scenes, they are
designed for novel view synthesis rather than precise surface reconstruction
due to limited surface constraints. To overcome this limitation, we introduce a
novel approach that applies multiple geometric constraints to the implicit
surface optimization process, enabling more accurate reconstructions from
unconstrained image collections. First, we utilize sparse 3D points from
structure-from-motion (SfM) to refine the signed distance function estimation
for the reconstructed surface, with a displacement compensation to accommodate
noise in the sparse points. Additionally, we employ robust normal priors
derived from a normal predictor, enhanced by edge prior filtering and
multi-view consistency constraints, to improve alignment with the actual
surface geometry. Extensive testing on the Heritage-Recon benchmark and other
datasets has shown that the proposed method can accurately reconstruct surfaces
from in-the-wild images, yielding geometries with superior accuracy and
granularity compared to existing techniques. Our approach enables high-quality
3D reconstruction of various landmarks, making it applicable to diverse
scenarios such as digital preservation of cultural heritage sites.



---

## NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain  Generalization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-11 | Ahmed Qazi, Abdul Basit, Asim Iqbal | cs.CV | [PDF](http://arxiv.org/pdf/2505.06894v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have significantly advanced the field of novel
view synthesis, yet their generalization across diverse scenes and conditions
remains challenging. Addressing this, we propose the integration of a novel
brain-inspired normalization technique Neural Generalization (NeuGen) into
leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts
the domain-invariant features, thereby enhancing the models' generalization
capabilities. It can be seamlessly integrated into NeRF architectures and
cultivates a comprehensive feature set that significantly improves accuracy and
robustness in image rendering. Through this integration, NeuGen shows improved
performance on benchmarks on diverse datasets across state-of-the-art NeRF
architectures, enabling them to generalize better across varied scenes. Our
comprehensive evaluations, both quantitative and qualitative, confirm that our
approach not only surpasses existing models in generalizability but also
markedly improves rendering quality. Our work exemplifies the potential of
merging neuroscientific principles with deep learning frameworks, setting a new
precedent for enhanced generalizability and efficiency in novel view synthesis.
A demo of our study is available at https://neugennerf.github.io.

Comments:
- 18 pages, 6 figures

---

## 3D Characterization of Smoke Plume Dispersion Using Multi-View Drone  Swarm

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Nikil Krishnakumar, Shashank Sharma, Srijan Kumar Pal, Jiarong Hong | cs.RO | [PDF](http://arxiv.org/pdf/2505.06638v1){: .btn .btn-green } |

**Abstract**: This study presents an advanced multi-view drone swarm imaging system for the
three-dimensional characterization of smoke plume dispersion dynamics. The
system comprises a manager drone and four worker drones, each equipped with
high-resolution cameras and precise GPS modules. The manager drone uses image
feedback to autonomously detect and position itself above the plume, then
commands the worker drones to orbit the area in a synchronized circular flight
pattern, capturing multi-angle images. The camera poses of these images are
first estimated, then the images are grouped in batches and processed using
Neural Radiance Fields (NeRF) to generate high-resolution 3D reconstructions of
plume dynamics over time. Field tests demonstrated the ability of the system to
capture critical plume characteristics including volume dynamics, wind-driven
directional shifts, and lofting behavior at a temporal resolution of about 1 s.
The 3D reconstructions generated by this system provide unique field data for
enhancing the predictive models of smoke plume dispersion and fire spread.
Broadly, the drone swarm system offers a versatile platform for high resolution
measurements of pollutant emissions and transport in wildfires, volcanic
eruptions, prescribed burns, and industrial processes, ultimately supporting
more effective fire control decisions and mitigating wildfire risks.

Comments:
- 10 pages, 8 figures

---

## FlexNeRFer: A Multi-Dataflow, Adaptive Sparsity-Aware Accelerator for  On-Device NeRF Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Seock-Hwan Noh, Banseok Shin, Jeik Choi, Seungpyo Lee, Jaeha Kung, Yeseong Kim | cs.AR | [PDF](http://arxiv.org/pdf/2505.06504v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF), an AI-driven approach for 3D view
reconstruction, has demonstrated impressive performance, sparking active
research across fields. As a result, a range of advanced NeRF models has
emerged, leading on-device applications to increasingly adopt NeRF for highly
realistic scene reconstructions. With the advent of diverse NeRF models,
NeRF-based applications leverage a variety of NeRF frameworks, creating the
need for hardware capable of efficiently supporting these models. However, GPUs
fail to meet the performance, power, and area (PPA) cost demanded by these
on-device applications, or are specialized for specific NeRF algorithms,
resulting in lower efficiency when applied to other NeRF models. To address
this limitation, in this work, we introduce FlexNeRFer, an energy-efficient
versatile NeRF accelerator. The key components enabling the enhancement of
FlexNeRFer include: i) a flexible network-on-chip (NoC) supporting
multi-dataflow and sparsity on precision-scalable MAC array, and ii) efficient
data storage using an optimal sparsity format based on the sparsity ratio and
precision modes. To evaluate the effectiveness of FlexNeRFer, we performed a
layout implementation using 28nm CMOS technology. Our evaluation shows that
FlexNeRFer achieves 8.2~243.3x speedup and 24.1~520.3x improvement in energy
efficiency over a GPU (i.e., NVIDIA RTX 2080 Ti), while demonstrating 4.2~86.9x
speedup and 2.3~47.5x improvement in energy efficiency compared to a
state-of-the-art NeRF accelerator (i.e., NeuRex).

Comments:
- Accepted for publication at the 52nd IEEE/ACM International Symposium
  on Computer Architecture (ISCA-52), 2025

---

## Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System  for Real-Time Rendering of Composed Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Xijie Yang, Linning Xu, Lihan Jiang, Dahua Lin, Bo Dai | cs.GR | [PDF](http://arxiv.org/pdf/2505.06523v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital
3D assets from multi-view images by leveraging a set of 3D Gaussian primitives
for rendering. Its explicit and discrete representation facilitates the
seamless composition of complex digital worlds, offering significant advantages
over previous neural implicit methods. However, when applied to large-scale
compositions, such as crowd-level scenes, it can encompass numerous 3D
Gaussians, posing substantial challenges for real-time rendering. To address
this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D
Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D
Gaussian clusters and dynamically selects only the necessary ones to accelerate
rendering speed. Our approach consists of two stages: (1) Offline Build, where
hierarchical clusters are generated using a local splatting method to minimize
visual differences across granularities, and (2) Online Selection, where
footprint evaluation determines perceptible clusters for efficient
rasterization during rendering. We curate a dataset of synthetic and real-world
scenes, including objects, trees, people, and buildings, each requiring 0.1
billion 3D Gaussians to capture fine details. Experiments show that our
solution balances rendering efficiency and visual quality across user-defined
tolerances, facilitating downstream interactive applications that compose
extensive 3DGS assets for consistent rendering performance.

Comments:
- project page: https://xijie-yang.github.io/V3DG/

---

## QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian  Initialization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Yueh-Cheng Liu, Lukas H√∂llein, Matthias Nie√üner, Angela Dai | cs.CV | [PDF](http://arxiv.org/pdf/2505.05591v1){: .btn .btn-green } |

**Abstract**: Surface reconstruction is fundamental to computer vision and graphics,
enabling applications in 3D modeling, mixed reality, robotics, and more.
Existing approaches based on volumetric rendering obtain promising results, but
optimize on a per-scene basis, resulting in a slow optimization that can
struggle to model under-observed or textureless regions. We introduce
QuickSplat, which learns data-driven priors to generate dense initializations
for 2D gaussian splatting optimization of large-scale indoor scenes. This
provides a strong starting point for the reconstruction, which accelerates the
convergence of the optimization and improves the geometry of flat wall
structures. We further learn to jointly estimate the densification and update
of the scene parameters during each iteration; our proposed densifier network
predicts new Gaussians based on the rendering gradients of existing ones,
removing the needs of heuristics for densification. Extensive experiments on
large-scale indoor scene reconstruction demonstrate the superiority of our
data-driven optimization. Concretely, we accelerate runtime by 8x, while
decreasing depth errors by up to 48% in comparison to state of the art methods.

Comments:
- Project page: https://liu115.github.io/quicksplat, Video:
  https://youtu.be/2IA_gnFvFG8

---

## 3D Scene Generation: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.05474v1){: .btn .btn-green } |

**Abstract**: 3D scene generation seeks to synthesize spatially structured, semantically
meaningful, and photorealistic environments for applications such as immersive
media, robotics, autonomous driving, and embodied AI. Early methods based on
procedural rules offered scalability but limited diversity. Recent advances in
deep generative models (e.g., GANs, diffusion models) and 3D representations
(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene
distributions, improving fidelity, diversity, and view consistency. Recent
advances like diffusion models bridge 3D scene synthesis and photorealism by
reframing generation as image or video synthesis problems. This survey provides
a systematic overview of state-of-the-art approaches, organizing them into four
paradigms: procedural generation, neural 3D-based generation, image-based
generation, and video-based generation. We analyze their technical foundations,
trade-offs, and representative results, and review commonly used datasets,
evaluation protocols, and downstream applications. We conclude by discussing
key challenges in generation capacity, 3D representation, data and annotations,
and evaluation, and outline promising directions including higher fidelity,
physics-aware and interactive generation, and unified perception-generation
models. This review organizes recent advances in 3D scene generation and
highlights promising directions at the intersection of generative AI, 3D
vision, and embodied intelligence. To track ongoing developments, we maintain
an up-to-date project page:
https://github.com/hzxie/Awesome-3D-Scene-Generation.

Comments:
- Project Page: https://github.com/hzxie/Awesome-3D-Scene-Generation

---

## Steepest Descent Density Control for Compact 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Peihao Wang, Yuehao Wang, Dilin Wang, Sreyas Mohan, Zhiwen Fan, Lemeng Wu, Ruisi Cai, Yu-Ying Yeh, Zhangyang Wang, Qiang Liu, Rakesh Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2505.05587v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for
real-time, high-resolution novel view synthesis. By representing scenes as a
mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for
efficient rendering and reconstruction. To optimize scene coverage and capture
fine details, 3DGS employs a densification algorithm to generate additional
points. However, this process often leads to redundant point clouds, resulting
in excessive memory usage, slower performance, and substantial storage demands
- posing significant challenges for deployment on resource-constrained devices.
To address this limitation, we propose a theoretical framework that demystifies
and improves density control in 3DGS. Our analysis reveals that splitting is
crucial for escaping saddle points. Through an optimization-theoretic approach,
we establish the necessary conditions for densification, determine the minimal
number of offspring Gaussians, identify the optimal parameter update direction,
and provide an analytical solution for normalizing off-spring opacity. Building
on these insights, we introduce SteepGS, incorporating steepest density
control, a principled strategy that minimizes loss while maintaining a compact
point cloud. SteepGS achieves a ~50% reduction in Gaussian points without
compromising rendering quality, significantly enhancing both efficiency and
scalability.

Comments:
- CVPR 2025, Project page: https://vita-group.github.io/SteepGS/

---

## UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Mark C. Eid, Ana I. L. Namburete, Jo√£o F. Henriques | eess.IV | [PDF](http://arxiv.org/pdf/2505.05643v1){: .btn .btn-green } |

**Abstract**: Ultrasound imaging is widely used due to its safety, affordability, and
real-time capabilities, but its 2D interpretation is highly operator-dependent,
leading to variability and increased cognitive demand. 2D-to-3D reconstruction
mitigates these challenges by providing standardized volumetric views, yet
existing methods are often computationally expensive, memory-intensive, or
incompatible with ultrasound physics. We introduce UltraGauss: the first
ultrasound-specific Gaussian Splatting framework, extending view synthesis
techniques to ultrasound wave propagation. Unlike conventional
perspective-based splatting, UltraGauss models probe-plane intersections in 3D,
aligning with acoustic image formation. We derive an efficient rasterization
boundary formulation for GPU parallelization and introduce a numerically stable
covariance parametrization, improving computational efficiency and
reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves
state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20
minutes on a single GPU. A survey of expert clinicians confirms UltraGauss'
reconstructions are the most realistic among competing methods. Our CUDA
implementation will be released upon publication.



---

## TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Gengyan Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2505.05672v1){: .btn .btn-green } |

**Abstract**: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have
recently enabled animatable 3D head avatars that are rendered under arbitrary
viewpoints with impressive photorealism. Today, such photoreal avatars are seen
as a key component in emerging applications in telepresence, extended reality,
and entertainment. Building a photoreal avatar requires estimating the complex
non-rigid motion of different facial components as seen in input video images;
due to inaccurate motion estimation, animatable models typically present a loss
of fidelity and detail when compared to their non-animatable counterparts,
built from an individual facial expression. Also, recent state-of-the-art
models are often affected by memory limitations that reduce the number of 3D
Gaussians used for modeling, leading to lower detail and quality. To address
these problems, we present a new high-detail 3D head avatar model that improves
upon the state of the art, largely increasing the number of 3D Gaussians and
modeling quality for rendering at 4K resolution. Our high-quality model is
reconstructed from multiview input video and builds on top of a mesh-based 3D
morphable model, which provides a coarse deformation layer for the head.
Photoreal appearance is modelled by 3D Gaussians embedded within the continuous
UVD tangent space of this mesh, allowing for more effective densification where
most needed. Additionally, these Gaussians are warped by a novel UVD
deformation field to capture subtle, localized motion. Our key contribution is
the novel deformable Gaussian encoding and overall fitting procedure that
allows our head model to preserve appearance detail, while capturing facial
motion and other transient high-frequency features such as skin wrinkling.

Comments:
- 10 pages, 9 figures, supplementary results found at:
  https://syntec-research.github.io/UVGA/, to be published in SIGGRAPH 2025

---

## SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with  Video Diffusion and Data Augmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Yonwoo Choi | cs.CV | [PDF](http://arxiv.org/pdf/2505.05475v1){: .btn .btn-green } |

**Abstract**: Creating high-quality animatable 3D human avatars from a single image remains
a significant challenge in computer vision due to the inherent difficulty of
reconstructing complete 3D information from a single viewpoint. Current
approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods
produce high-quality results but require multiple views or video sequences,
while video diffusion models can generate animations from single images but
struggle with consistency and identity preservation. We present SVAD, a novel
approach that addresses these limitations by leveraging complementary strengths
of existing techniques. Our method generates synthetic training data through
video diffusion, enhances it with identity preservation and image restoration
modules, and utilizes this refined data to train 3DGS avatars. Comprehensive
evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)
single-image methods in maintaining identity consistency and fine details
across novel poses and viewpoints, while enabling real-time rendering
capabilities. Through our data augmentation pipeline, we overcome the
dependency on dense monocular or multi-view training data typically required by
traditional 3DGS approaches. Extensive quantitative, qualitative comparisons
show our method achieves superior performance across multiple metrics against
baseline models. By effectively combining the generative power of diffusion
models with both the high-quality results and rendering efficiency of 3DGS, our
work establishes a new approach for high-fidelity avatar generation from a
single image input.

Comments:
- Accepted by CVPR 2025 SyntaGen Workshop, Project Page:
  https://yc4ny.github.io/SVAD/

---

## Time of the Flight of the Gaussians: Optimizing Depth Indirectly in  Dynamic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Runfeng Li, Mikhail Okunev, Zixuan Guo, Anh Ha Duong, Christian Richardt, Matthew O'Toole, James Tompkin | cs.GR | [PDF](http://arxiv.org/pdf/2505.05356v1){: .btn .btn-green } |

**Abstract**: We present a method to reconstruct dynamic scenes from monocular
continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that
achieves similar or better accuracy than neural volumetric approaches and is
100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a
single viewpoint is a significant challenge in computer vision. In C-ToF
radiance field reconstruction, the property of interest-depth-is not directly
measured, causing an additional challenge. This problem has a large and
underappreciated impact upon the optimization when using a fast primitive-based
scene representation like 3D Gaussian splatting, which is commonly used with
multi-view data to produce satisfactory results and is brittle in its
optimization otherwise. We incorporate two heuristics into the optimization to
improve the accuracy of scene geometry represented by Gaussians. Experimental
results show that our approach produces accurate reconstructions under
constrained C-ToF sensing conditions, including for fast motions like swinging
baseball bats. https://visual.cs.brown.edu/gftorf



---

## SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Xinran Yang, Donghao Ji, Yuanqi Li, Jie Guo, Yanwen Guo, Junyuan Xie | cs.GR | [PDF](http://arxiv.org/pdf/2505.04668v1){: .btn .btn-green } |

**Abstract**: Neural rendering techniques have made substantial progress in generating
photo-realistic 3D scenes. The latest 3D Gaussian Splatting technique has
achieved high quality novel view synthesis as well as fast rendering speed.
However, 3D Gaussians lack proficiency in defining accurate 3D geometric
structures despite their explicit primitive representations. This is due to the
fact that Gaussian's attributes are primarily tailored and fine-tuned for
rendering diverse 2D images by their anisotropic nature. To pave the way for
efficient 3D reconstruction, we present Spherical Gaussians, a simple and
effective representation for 3D geometric boundaries, from which we can
directly reconstruct 3D feature curves from a set of calibrated multi-view
images. Spherical Gaussians is optimized from grid initialization with a
view-based rendering loss, where a 2D edge map is rendered at a specific view
and then compared to the ground-truth edge map extracted from the corresponding
image, without the need for any 3D guidance or supervision. Given Spherical
Gaussians serve as intermedia for the robust edge representation, we further
introduce a novel optimization-based algorithm called SGCR to directly extract
accurate parametric curves from aligned Spherical Gaussians. We demonstrate
that SGCR outperforms existing state-of-the-art methods in 3D edge
reconstruction while enjoying great efficiency.

Comments:
- The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2025, 8 pages

---

## Bridging Geometry-Coherent Text-to-3D Generation with Multi-View  Diffusion Priors and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Feng Yang, Wenliang Qian, Wangmeng Zuo, Hui Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.04262v1){: .btn .btn-green } |

**Abstract**: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.



---

## GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view  Synthesis in 3D Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Feng Xiao, Hongbin Xu, Wanlin Liang, Wenxiong Kang | cs.GR | [PDF](http://arxiv.org/pdf/2505.04659v1){: .btn .btn-green } |

**Abstract**: The semantic synthesis of unseen scenes from multiple viewpoints is crucial
for research in 3D scene understanding. Current methods are capable of
rendering novel-view images and semantic maps by reconstructing generalizable
Neural Radiance Fields. However, they often suffer from limitations in speed
and segmentation performance. We propose a generalizable semantic Gaussian
Splatting method (GSsplat) for efficient novel-view synthesis. Our model
predicts the positions and attributes of scene-adaptive Gaussian distributions
from once input, replacing the densification and pruning processes of
traditional scene-specific Gaussian Splatting. In the multi-task framework, a
hybrid network is designed to extract color and semantic information and
predict Gaussian parameters. To augment the spatial perception of Gaussians for
high-quality rendering, we put forward a novel offset learning module through
group-based supervision and a point-level interaction module with spatial unit
aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat
achieves state-of-the-art performance for semantic synthesis at the fastest
speed.



---

## 3D Gaussian Splatting Data Compression with Mixture of Priors

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-06 | Lei Liu, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.03310v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.



---

## SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Shubhendu Jena, Shishir Reddy Vutukur, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2505.02175v1){: .btn .btn-green } |

**Abstract**: Recovering 3D information from scenes via multi-view stereo reconstruction
(MVS) and novel view synthesis (NVS) is inherently challenging, particularly in
scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting
(3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian
Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive
rasterization to achieve accurate geometry representation during rendering,
improving 3D scene reconstruction while maintaining real-time performance.
Recent approaches have tackled the problem of sparse real-time NVS using 3DGS
within a generalizable, MVS-based learning framework to regress 3D Gaussian
parameters. Our work extends this line of research by addressing the challenge
of generalizable sparse 3D reconstruction and NVS jointly, and manages to
perform successfully at both tasks. We propose an MVS-based learning pipeline
that regresses 2DGS surface element parameters in a feed-forward fashion to
perform 3D shape reconstruction and NVS from sparse-view images. We further
show that our generalizable pipeline can benefit from preexisting foundational
multi-view deep visual features. The resulting model attains the
state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms
of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also
demonstrates strong generalization on the BlendedMVS and Tanks and Temples
datasets. We note that our model outperforms the prior state-of-the-art in
feed-forward sparse view reconstruction based on volume rendering of implicit
representations, while offering an almost 2 orders of magnitude higher
inference speed.

Comments:
- Project page : https://shubhendu-jena.github.io/SparSplat/

---

## Sparfels: Fast Reconstruction from Sparse Unposed Imagery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2505.02178v1){: .btn .btn-green } |

**Abstract**: We present a method for Sparse view reconstruction with surface element
splatting that runs within 3 minutes on a consumer grade GPU. While few methods
address sparse radiance field learning from noisy or unposed sparse cameras,
shape recovery remains relatively underexplored in this setting. Several
radiance and shape learning test-time optimization methods address the sparse
posed setting by learning data priors or using combinations of external
monocular geometry priors. Differently, we propose an efficient and simple
pipeline harnessing a single recent 3D foundation model. We leverage its
various task heads, notably point maps and camera initializations to
instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image
correspondences to guide camera optimization midst 2DGS training. Key to our
contribution is a novel formulation of splatted color variance along rays,
which can be computed efficiently. Reducing this moment in training leads to
more accurate shape reconstructions. We demonstrate state-of-the-art
performances in the sparse uncalibrated setting in reconstruction and novel
view benchmarks based on established multi-view datasets.

Comments:
- Project page : https://shubhendu-jena.github.io/Sparfels/

---

## SignSplat: Rendering Sign Language via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2505.02108v1){: .btn .btn-green } |

**Abstract**: State-of-the-art approaches for conditional human body rendering via Gaussian
splatting typically focus on simple body motions captured from many views. This
is often in the context of dancing or walking. However, for more complex use
cases, such as sign language, we care less about large body motion and more
about subtle and complex motions of the hands and face. The problems of
building high fidelity models are compounded by the complexity of capturing
multi-view data of sign. The solution is to make better use of sequence data,
ensuring that we can overcome the limited information from only a few views by
exploiting temporal variability. Nevertheless, learning from sequence-level
data requires extremely accurate and consistent model fitting to ensure that
appearance is consistent across complex motions. We focus on how to achieve
this, constraining mesh parameters to build an accurate Gaussian splatting
framework from few views capable of modelling subtle human motion. We leverage
regularization techniques on the Gaussian parameters to mitigate overfitting
and rendering artifacts. Additionally, we propose a new adaptive control method
to densify Gaussians and prune splat points on the mesh surface. To demonstrate
the accuracy of our approach, we render novel sequences of sign language video,
building on neural machine translation approaches to sign stitching. On
benchmark datasets, our approach achieves state-of-the-art performance; and on
highly articulated and complex sign language motion, we significantly
outperform competing approaches.



---

## HandOcc: NeRF-based Hand Rendering with Occupancy Networks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2505.02079v1){: .btn .btn-green } |

**Abstract**: We propose HandOcc, a novel framework for hand rendering based upon
occupancy. Popular rendering methods such as NeRF are often combined with
parametric meshes to provide deformable hand models. However, in doing so, such
approaches present a trade-off between the fidelity of the mesh and the
complexity and dimensionality of the parametric model. The simplicity of
parametric mesh structures is appealing, but the underlying issue is that it
binds methods to mesh initialization, making it unable to generalize to objects
where a parametric model does not exist. It also means that estimation is tied
to mesh resolution and the accuracy of mesh fitting. This paper presents a
pipeline for meshless 3D rendering, which we apply to the hands. By providing
only a 3D skeleton, the desired appearance is extracted via a convolutional
model. We do this by exploiting a NeRF renderer conditioned upon an
occupancy-based representation. The approach uses the hand occupancy to resolve
hand-to-hand interactions further improving results, allowing fast rendering,
and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,
we achieved state-of-the-art results.



---

## Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.02005v1){: .btn .btn-green } |

**Abstract**: Recent NeRF methods on large-scale scenes have underlined the importance of
scene decomposition for scalable NeRFs. Although achieving reasonable
scalability, there are several critical problems remaining unexplored, i.e.,
learnable decomposition, modeling scene heterogeneity, and modeling efficiency.
In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash
Experts (HMoHE) network that addresses these challenges within a unified
framework. It is a highly scalable NeRF that learns heterogeneous decomposition
and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end
manner. In our framework, a gating network learns to decomposes scenes and
allocates 3D points to specialized NeRF experts. This gating network is
co-optimized with the experts, by our proposed Sparsely Gated Mixture of
Experts (MoE) NeRF framework. We incorporate a hash-based gating network and
distinct heterogeneous hash experts. The hash-based gating efficiently learns
the decomposition of the large-scale scene. The distinct heterogeneous hash
experts consist of hash grids of different resolution ranges, enabling
effective learning of the heterogeneous representation of different scene
parts. These design choices make our framework an end-to-end and highly
scalable NeRF solution for real-world large-scale scene modeling to achieve
both quality and efficiency. We evaluate our accuracy and scalability on
existing large-scale NeRF datasets and a new dataset with very large-scale
scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our
approach can be easily scaled to various large-scale scenes and achieve
state-of-the-art scene rendering accuracy. Furthermore, our method exhibits
significant efficiency, with an 8x acceleration in training and a 16x
acceleration in rendering compared to Switch-NeRF. Codes will be released in
https://github.com/MiZhenxing/Switch-NeRF.

Comments:
- 15 pages, 9 figures

---

## GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity  Non-Watertight 3D Garment Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2505.02126v1){: .btn .btn-green } |

**Abstract**: Traditional 3D garment creation requires extensive manual operations,
resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved
breakthrough progress in 3D scene reconstruction and rendering, attracting
widespread attention and opening new pathways for 3D garment reconstruction.
However, due to the unstructured and irregular nature of Gaussian primitives,
it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In
this paper, we present GarmentGS, a dense point cloud-guided method that can
reconstruct high-fidelity garment surfaces with high geometric accuracy and
generate non-watertight, single-layer meshes. Our method introduces a fast
dense point cloud reconstruction module that can complete garment point cloud
reconstruction in 10 minutes, compared to traditional methods that require
several hours. Furthermore, we use dense point clouds to guide the movement,
flattening, and rotation of Gaussian primitives, enabling better distribution
on the garment surface to achieve superior rendering effects and geometric
accuracy. Through numerical and visual comparisons, our method achieves fast
training and real-time rendering while maintaining competitive quality.



---

## GenSync: A Generalized Talking Head Framework for Audio-driven  Multi-Subject Lip-Sync using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Anushka Agarwal, Muhammad Yusuf Hassan, Talha Chafekar | cs.CV | [PDF](http://arxiv.org/pdf/2505.01928v1){: .btn .btn-green } |

**Abstract**: We introduce GenSync, a novel framework for multi-identity lip-synced video
synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that
require training a new model for each identity , GenSync learns a unified
network that synthesizes lip-synced videos for multiple speakers. By
incorporating a Disentanglement Module, our approach separates
identity-specific features from audio representations, enabling efficient
multi-identity video synthesis. This design reduces computational overhead and
achieves 6.8x faster training compared to state-of-the-art models, while
maintaining high lip-sync accuracy and visual quality.



---

## AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Junhao Shi, Jisheng Xu, Jianping He, Zhiliang Lin | cs.CV | [PDF](http://arxiv.org/pdf/2505.01799v1){: .btn .btn-green } |

**Abstract**: Underwater scene reconstruction is a critical tech-nology for underwater
operations, enabling the generation of 3D models from images captured by
underwater platforms. However, the quality of underwater images is often
degraded due to medium interference, which limits the effectiveness of
Structure-from-Motion (SfM) pose estimation, leading to subsequent
reconstruction failures. Additionally, SfM methods typically operate at slower
speeds, further hindering their applicability in real-time scenarios. In this
paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model
based on the SeaThru algorithm, which facilitates rapid and accurate separation
of scene details and medium features. Our approach initializes Gaussians by
integrating state-of-the-art multi-view stereo (MVS) technology, employs
implicit Neural Radiance Fields (NeRF) for rendering translucent media and
utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render
object surfaces, which effectively addresses the limitations of traditional
methods and accurately simulates underwater optical phenomena. Experimental
results on the data set and the robot platform show that our model can complete
high-precision reconstruction in 30 seconds with only 3 image inputs,
significantly enhancing the practical application of the algorithm in robotic
platforms.



---

## Unified Steganography via Implicit Neural Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Qi Song, Ziyuan Luo, Xiufeng Huang, Sheng Li, Renjie Wan | cs.CR | [PDF](http://arxiv.org/pdf/2505.01749v1){: .btn .btn-green } |

**Abstract**: Digital steganography is the practice of concealing for encrypted data
transmission. Typically, steganography methods embed secret data into cover
data to create stega data that incorporates hidden secret data. However,
steganography techniques often require designing specific frameworks for each
data type, which restricts their generalizability. In this paper, we present
U-INR, a novel method for steganography via Implicit Neural Representation
(INR). Rather than using the specific framework for each data format, we
directly use the neurons of the INR network to represent the secret data and
cover data across different data types. To achieve this idea, a private key is
shared between the data sender and receivers. Such a private key can be used to
determine the position of secret data in INR networks. To effectively leverage
this key, we further introduce a key-based selection strategy that can be used
to determine the position within the INRs for data storage. Comprehensive
experiments across multiple data types, including images, videos, audio, and
SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR,
emphasizing its potential for improving data security and privacy in various
applications.



---

## Visual enhancement and 3D representation for underwater scenes: a review


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Guoxi Huang, Haoran Wang, Brett Seymour, Evan Kovacs, John Ellerbrock, Dave Blackham, Nantheera Anantrasirichai | cs.CV | [PDF](http://arxiv.org/pdf/2505.01869v1){: .btn .btn-green } |

**Abstract**: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose
significant challenges in
  computer vision and AI-based tasks due to complex imaging conditions in
aquatic environments. Despite
  the development of numerous enhancement algorithms, a comprehensive and
systematic review covering both
  UVE and underwater 3D reconstruction remains absent. To advance research in
these areas, we present an
  in-depth review from multiple perspectives. First, we introduce the
fundamental physical models, highlighting the
  peculiarities that challenge conventional techniques. We survey advanced
methods for visual enhancement and
  3D reconstruction specifically designed for underwater scenarios. The paper
assesses various approaches from
  non-learning methods to advanced data-driven techniques, including Neural
Radiance Fields and 3D Gaussian
  Splatting, discussing their effectiveness in handling underwater distortions.
Finally, we conduct both quantitative
  and qualitative evaluations of state-of-the-art UVE and underwater 3D
reconstruction algorithms across multiple
  benchmark datasets. Finally, we highlight key research directions for future
advancements in underwater vision.



---

## HybridGS: High-Efficiency Gaussian Splatting Data Compression using  Dual-Channel Sparse Representation and Point Cloud Encoder

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Qi Yang, Le Yang, Geert Van Der Auwera, Zhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.01938v1){: .btn .btn-green } |

**Abstract**: Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on
producing compact 3DGS representation via implicit data embedding. They have
long coding times and highly customized data format, making it difficult for
widespread deployment. This paper presents a new 3DGS compression framework
called HybridGS, which takes advantage of both compact generation and
standardized point cloud data encoding. HybridGS first generates compact and
explicit 3DGS data. A dual-channel sparse representation is introduced to
supervise the primitive position and feature bit depth. It then utilizes a
canonical point cloud encoder to perform further data compression and form
standard output bitstreams. A simple and effective rate control scheme is
proposed to pivot the interpretable data compression scheme. At the current
stage, HybridGS does not include any modules aimed at improving 3DGS quality
during generation. But experiment results show that it still provides
comparable reconstruction performance against state-of-the-art methods, with
evidently higher encoding and decoding speed. The code is publicly available at
https://github.com/Qi-Yangsjtu/HybridGS.

Comments:
- Accepted by ICML2025

---

## FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft  Research

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-02 | Yan Miao, Will Shen, Hang Cui, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2505.01383v1){: .btn .btn-green } |

**Abstract**: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.



---

## Compensating Spatiotemporally Inconsistent Observations for Online  Dynamic 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-02 | Youngsik Yun, Jeongmin Bae, Hyunseung Son, Seoha Kim, Hahyun Lee, Gun Bang, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2505.01235v1){: .btn .btn-green } |

**Abstract**: Online reconstruction of dynamic scenes is significant as it enables learning
scenes from live-streaming video inputs, while existing offline dynamic
reconstruction methods rely on recorded video inputs. However, previous online
reconstruction approaches have primarily focused on efficiency and rendering
quality, overlooking the temporal consistency of their results, which often
contain noticeable artifacts in static regions. This paper identifies that
errors such as noise in real-world recordings affect temporal inconsistency in
online reconstruction. We propose a method that enhances temporal consistency
in online reconstruction from observations with temporal inconsistency which is
inevitable in cameras. We show that our method restores the ideal observation
by subtracting the learned error. We demonstrate that applying our method to
various baselines significantly enhances both temporal consistency and
rendering quality across datasets. Code, video results, and checkpoints are
available at https://bbangsik13.github.io/OR2.

Comments:
- SIGGRAPH 2025, Project page: https://bbangsik13.github.io/OR2

---

## Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique  Instances in Open-Vocabulary 3D Panoptic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Feng Xue, Wenzhuang Xu, Guofeng Zhong, Anlong Minga, Nicu Sebe | cs.CV | [PDF](http://arxiv.org/pdf/2505.00378v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D panoptic segmentation has recently emerged as a
significant trend. Top-performing methods currently integrate 2D segmentation
with geometry-aware 3D primitives. However, the advantage would be lost without
high-fidelity 3D point clouds, such as methods based on Neural Radiance Field
(NeRF). These methods are limited by the insufficient capacity to maintain
consistency across partial observations. To address this, recent works have
utilized contrastive loss or cross-view association pre-processing for view
consensus. In contrast to them, we present Cues3D, a compact approach that
relies solely on NeRF instead of pre-associations. The core idea is that NeRF's
implicit 3D field inherently establishes a globally consistent geometry,
enabling effective object distinction without explicit cross-view supervision.
We propose a three-phase training framework for NeRF,
initialization-disambiguation-refinement, whereby the instance IDs are
corrected using the initially-learned knowledge. Additionally, an instance
disambiguation method is proposed to match NeRF-rendered 3D masks and ensure
globally unique 3D instance identities. With the aid of Cues3D, we obtain
highly consistent and unique 3D instance ID for each object across views with a
balanced version of NeRF. Our experiments are conducted on ScanNet v2,
ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and
semantic segmentation tasks. Cues3D outperforms other 2D image-based methods
and competes with the latest 2D-3D merging based methods, while even surpassing
them when using additional 3D point clouds. The code link could be found in the
appendix and will be released on
\href{https://github.com/mRobotit/Cues3D}{github}

Comments:
- Accepted by Information Fusion

---

## Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing | cs.CV | [PDF](http://arxiv.org/pdf/2505.00421v1){: .btn .btn-green } |

**Abstract**: High-quality, animatable 3D human avatar reconstruction from monocular videos
offers significant potential for reducing reliance on complex hardware, making
it highly practical for applications in game development, augmented reality,
and social media. However, existing methods still face substantial challenges
in capturing fine geometric details and maintaining animation stability,
particularly under dynamic or complex poses. To address these issues, we
propose a novel real-time framework for animatable human avatar reconstruction
based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose
parameters, our framework not only aligns positional and rotational
discrepancies but also enables robust and natural pose-driven animation of the
reconstructed avatars. Furthermore, we introduce a Rotation Compensation
Network (RCN) that learns rotation residuals by integrating local geometric
features with global pose parameters. This network significantly improves the
handling of non-rigid deformations and ensures smooth, artifact-free pose
transitions during animation. Experimental results demonstrate that our method
successfully reconstructs realistic and highly animatable human avatars from
monocular videos, effectively preserving fine-grained details while ensuring
stable and natural pose variation. Our approach surpasses current
state-of-the-art methods in both reconstruction quality and animation
robustness on public benchmarks.


