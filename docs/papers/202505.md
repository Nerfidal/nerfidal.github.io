---
layout: default
title: May 2025
parent: Papers
nav_order: 202505
---

<!---metadata--->


## TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Gengyan Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2505.05672v1){: .btn .btn-green } |

**Abstract**: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have
recently enabled animatable 3D head avatars that are rendered under arbitrary
viewpoints with impressive photorealism. Today, such photoreal avatars are seen
as a key component in emerging applications in telepresence, extended reality,
and entertainment. Building a photoreal avatar requires estimating the complex
non-rigid motion of different facial components as seen in input video images;
due to inaccurate motion estimation, animatable models typically present a loss
of fidelity and detail when compared to their non-animatable counterparts,
built from an individual facial expression. Also, recent state-of-the-art
models are often affected by memory limitations that reduce the number of 3D
Gaussians used for modeling, leading to lower detail and quality. To address
these problems, we present a new high-detail 3D head avatar model that improves
upon the state of the art, largely increasing the number of 3D Gaussians and
modeling quality for rendering at 4K resolution. Our high-quality model is
reconstructed from multiview input video and builds on top of a mesh-based 3D
morphable model, which provides a coarse deformation layer for the head.
Photoreal appearance is modelled by 3D Gaussians embedded within the continuous
UVD tangent space of this mesh, allowing for more effective densification where
most needed. Additionally, these Gaussians are warped by a novel UVD
deformation field to capture subtle, localized motion. Our key contribution is
the novel deformable Gaussian encoding and overall fitting procedure that
allows our head model to preserve appearance detail, while capturing facial
motion and other transient high-frequency features such as skin wrinkling.

Comments:
- 10 pages, 9 figures, supplementary results found at:
  https://syntec-research.github.io/UVGA/, to be published in SIGGRAPH 2025

---

## SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with  Video Diffusion and Data Augmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Yonwoo Choi | cs.CV | [PDF](http://arxiv.org/pdf/2505.05475v1){: .btn .btn-green } |

**Abstract**: Creating high-quality animatable 3D human avatars from a single image remains
a significant challenge in computer vision due to the inherent difficulty of
reconstructing complete 3D information from a single viewpoint. Current
approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods
produce high-quality results but require multiple views or video sequences,
while video diffusion models can generate animations from single images but
struggle with consistency and identity preservation. We present SVAD, a novel
approach that addresses these limitations by leveraging complementary strengths
of existing techniques. Our method generates synthetic training data through
video diffusion, enhances it with identity preservation and image restoration
modules, and utilizes this refined data to train 3DGS avatars. Comprehensive
evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)
single-image methods in maintaining identity consistency and fine details
across novel poses and viewpoints, while enabling real-time rendering
capabilities. Through our data augmentation pipeline, we overcome the
dependency on dense monocular or multi-view training data typically required by
traditional 3DGS approaches. Extensive quantitative, qualitative comparisons
show our method achieves superior performance across multiple metrics against
baseline models. By effectively combining the generative power of diffusion
models with both the high-quality results and rendering efficiency of 3DGS, our
work establishes a new approach for high-fidelity avatar generation from a
single image input.

Comments:
- Accepted by CVPR 2025 SyntaGen Workshop, Project Page:
  https://yc4ny.github.io/SVAD/

---

## 3D Scene Generation: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.05474v1){: .btn .btn-green } |

**Abstract**: 3D scene generation seeks to synthesize spatially structured, semantically
meaningful, and photorealistic environments for applications such as immersive
media, robotics, autonomous driving, and embodied AI. Early methods based on
procedural rules offered scalability but limited diversity. Recent advances in
deep generative models (e.g., GANs, diffusion models) and 3D representations
(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene
distributions, improving fidelity, diversity, and view consistency. Recent
advances like diffusion models bridge 3D scene synthesis and photorealism by
reframing generation as image or video synthesis problems. This survey provides
a systematic overview of state-of-the-art approaches, organizing them into four
paradigms: procedural generation, neural 3D-based generation, image-based
generation, and video-based generation. We analyze their technical foundations,
trade-offs, and representative results, and review commonly used datasets,
evaluation protocols, and downstream applications. We conclude by discussing
key challenges in generation capacity, 3D representation, data and annotations,
and evaluation, and outline promising directions including higher fidelity,
physics-aware and interactive generation, and unified perception-generation
models. This review organizes recent advances in 3D scene generation and
highlights promising directions at the intersection of generative AI, 3D
vision, and embodied intelligence. To track ongoing developments, we maintain
an up-to-date project page:
https://github.com/hzxie/Awesome-3D-Scene-Generation.

Comments:
- Project Page: https://github.com/hzxie/Awesome-3D-Scene-Generation

---

## Time of the Flight of the Gaussians: Optimizing Depth Indirectly in  Dynamic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Runfeng Li, Mikhail Okunev, Zixuan Guo, Anh Ha Duong, Christian Richardt, Matthew O'Toole, James Tompkin | cs.GR | [PDF](http://arxiv.org/pdf/2505.05356v1){: .btn .btn-green } |

**Abstract**: We present a method to reconstruct dynamic scenes from monocular
continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that
achieves similar or better accuracy than neural volumetric approaches and is
100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a
single viewpoint is a significant challenge in computer vision. In C-ToF
radiance field reconstruction, the property of interest-depth-is not directly
measured, causing an additional challenge. This problem has a large and
underappreciated impact upon the optimization when using a fast primitive-based
scene representation like 3D Gaussian splatting, which is commonly used with
multi-view data to produce satisfactory results and is brittle in its
optimization otherwise. We incorporate two heuristics into the optimization to
improve the accuracy of scene geometry represented by Gaussians. Experimental
results show that our approach produces accurate reconstructions under
constrained C-ToF sensing conditions, including for fast motions like swinging
baseball bats. https://visual.cs.brown.edu/gftorf



---

## QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian  Initialization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Yueh-Cheng Liu, Lukas Höllein, Matthias Nießner, Angela Dai | cs.CV | [PDF](http://arxiv.org/pdf/2505.05591v1){: .btn .btn-green } |

**Abstract**: Surface reconstruction is fundamental to computer vision and graphics,
enabling applications in 3D modeling, mixed reality, robotics, and more.
Existing approaches based on volumetric rendering obtain promising results, but
optimize on a per-scene basis, resulting in a slow optimization that can
struggle to model under-observed or textureless regions. We introduce
QuickSplat, which learns data-driven priors to generate dense initializations
for 2D gaussian splatting optimization of large-scale indoor scenes. This
provides a strong starting point for the reconstruction, which accelerates the
convergence of the optimization and improves the geometry of flat wall
structures. We further learn to jointly estimate the densification and update
of the scene parameters during each iteration; our proposed densifier network
predicts new Gaussians based on the rendering gradients of existing ones,
removing the needs of heuristics for densification. Extensive experiments on
large-scale indoor scene reconstruction demonstrate the superiority of our
data-driven optimization. Concretely, we accelerate runtime by 8x, while
decreasing depth errors by up to 48% in comparison to state of the art methods.

Comments:
- Project page: https://liu115.github.io/quicksplat, Video:
  https://youtu.be/2IA_gnFvFG8

---

## UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Mark C. Eid, Ana I. L. Namburete, João F. Henriques | eess.IV | [PDF](http://arxiv.org/pdf/2505.05643v1){: .btn .btn-green } |

**Abstract**: Ultrasound imaging is widely used due to its safety, affordability, and
real-time capabilities, but its 2D interpretation is highly operator-dependent,
leading to variability and increased cognitive demand. 2D-to-3D reconstruction
mitigates these challenges by providing standardized volumetric views, yet
existing methods are often computationally expensive, memory-intensive, or
incompatible with ultrasound physics. We introduce UltraGauss: the first
ultrasound-specific Gaussian Splatting framework, extending view synthesis
techniques to ultrasound wave propagation. Unlike conventional
perspective-based splatting, UltraGauss models probe-plane intersections in 3D,
aligning with acoustic image formation. We derive an efficient rasterization
boundary formulation for GPU parallelization and introduce a numerically stable
covariance parametrization, improving computational efficiency and
reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves
state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20
minutes on a single GPU. A survey of expert clinicians confirms UltraGauss'
reconstructions are the most realistic among competing methods. Our CUDA
implementation will be released upon publication.



---

## Steepest Descent Density Control for Compact 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Peihao Wang, Yuehao Wang, Dilin Wang, Sreyas Mohan, Zhiwen Fan, Lemeng Wu, Ruisi Cai, Yu-Ying Yeh, Zhangyang Wang, Qiang Liu, Rakesh Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2505.05587v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for
real-time, high-resolution novel view synthesis. By representing scenes as a
mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for
efficient rendering and reconstruction. To optimize scene coverage and capture
fine details, 3DGS employs a densification algorithm to generate additional
points. However, this process often leads to redundant point clouds, resulting
in excessive memory usage, slower performance, and substantial storage demands
- posing significant challenges for deployment on resource-constrained devices.
To address this limitation, we propose a theoretical framework that demystifies
and improves density control in 3DGS. Our analysis reveals that splitting is
crucial for escaping saddle points. Through an optimization-theoretic approach,
we establish the necessary conditions for densification, determine the minimal
number of offspring Gaussians, identify the optimal parameter update direction,
and provide an analytical solution for normalizing off-spring opacity. Building
on these insights, we introduce SteepGS, incorporating steepest density
control, a principled strategy that minimizes loss while maintaining a compact
point cloud. SteepGS achieves a ~50% reduction in Gaussian points without
compromising rendering quality, significantly enhancing both efficiency and
scalability.

Comments:
- CVPR 2025, Project page: https://vita-group.github.io/SteepGS/

---

## SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Xinran Yang, Donghao Ji, Yuanqi Li, Jie Guo, Yanwen Guo, Junyuan Xie | cs.GR | [PDF](http://arxiv.org/pdf/2505.04668v1){: .btn .btn-green } |

**Abstract**: Neural rendering techniques have made substantial progress in generating
photo-realistic 3D scenes. The latest 3D Gaussian Splatting technique has
achieved high quality novel view synthesis as well as fast rendering speed.
However, 3D Gaussians lack proficiency in defining accurate 3D geometric
structures despite their explicit primitive representations. This is due to the
fact that Gaussian's attributes are primarily tailored and fine-tuned for
rendering diverse 2D images by their anisotropic nature. To pave the way for
efficient 3D reconstruction, we present Spherical Gaussians, a simple and
effective representation for 3D geometric boundaries, from which we can
directly reconstruct 3D feature curves from a set of calibrated multi-view
images. Spherical Gaussians is optimized from grid initialization with a
view-based rendering loss, where a 2D edge map is rendered at a specific view
and then compared to the ground-truth edge map extracted from the corresponding
image, without the need for any 3D guidance or supervision. Given Spherical
Gaussians serve as intermedia for the robust edge representation, we further
introduce a novel optimization-based algorithm called SGCR to directly extract
accurate parametric curves from aligned Spherical Gaussians. We demonstrate
that SGCR outperforms existing state-of-the-art methods in 3D edge
reconstruction while enjoying great efficiency.

Comments:
- The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2025, 8 pages

---

## GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view  Synthesis in 3D Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Feng Xiao, Hongbin Xu, Wanlin Liang, Wenxiong Kang | cs.GR | [PDF](http://arxiv.org/pdf/2505.04659v1){: .btn .btn-green } |

**Abstract**: The semantic synthesis of unseen scenes from multiple viewpoints is crucial
for research in 3D scene understanding. Current methods are capable of
rendering novel-view images and semantic maps by reconstructing generalizable
Neural Radiance Fields. However, they often suffer from limitations in speed
and segmentation performance. We propose a generalizable semantic Gaussian
Splatting method (GSsplat) for efficient novel-view synthesis. Our model
predicts the positions and attributes of scene-adaptive Gaussian distributions
from once input, replacing the densification and pruning processes of
traditional scene-specific Gaussian Splatting. In the multi-task framework, a
hybrid network is designed to extract color and semantic information and
predict Gaussian parameters. To augment the spatial perception of Gaussians for
high-quality rendering, we put forward a novel offset learning module through
group-based supervision and a point-level interaction module with spatial unit
aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat
achieves state-of-the-art performance for semantic synthesis at the fastest
speed.



---

## Bridging Geometry-Coherent Text-to-3D Generation with Multi-View  Diffusion Priors and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Feng Yang, Wenliang Qian, Wangmeng Zuo, Hui Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.04262v1){: .btn .btn-green } |

**Abstract**: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.



---

## 3D Gaussian Splatting Data Compression with Mixture of Priors

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-06 | Lei Liu, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.03310v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.



---

## Sparfels: Fast Reconstruction from Sparse Unposed Imagery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2505.02178v1){: .btn .btn-green } |

**Abstract**: We present a method for Sparse view reconstruction with surface element
splatting that runs within 3 minutes on a consumer grade GPU. While few methods
address sparse radiance field learning from noisy or unposed sparse cameras,
shape recovery remains relatively underexplored in this setting. Several
radiance and shape learning test-time optimization methods address the sparse
posed setting by learning data priors or using combinations of external
monocular geometry priors. Differently, we propose an efficient and simple
pipeline harnessing a single recent 3D foundation model. We leverage its
various task heads, notably point maps and camera initializations to
instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image
correspondences to guide camera optimization midst 2DGS training. Key to our
contribution is a novel formulation of splatted color variance along rays,
which can be computed efficiently. Reducing this moment in training leads to
more accurate shape reconstructions. We demonstrate state-of-the-art
performances in the sparse uncalibrated setting in reconstruction and novel
view benchmarks based on established multi-view datasets.

Comments:
- Project page : https://shubhendu-jena.github.io/Sparfels/

---

## SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Shubhendu Jena, Shishir Reddy Vutukur, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2505.02175v1){: .btn .btn-green } |

**Abstract**: Recovering 3D information from scenes via multi-view stereo reconstruction
(MVS) and novel view synthesis (NVS) is inherently challenging, particularly in
scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting
(3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian
Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive
rasterization to achieve accurate geometry representation during rendering,
improving 3D scene reconstruction while maintaining real-time performance.
Recent approaches have tackled the problem of sparse real-time NVS using 3DGS
within a generalizable, MVS-based learning framework to regress 3D Gaussian
parameters. Our work extends this line of research by addressing the challenge
of generalizable sparse 3D reconstruction and NVS jointly, and manages to
perform successfully at both tasks. We propose an MVS-based learning pipeline
that regresses 2DGS surface element parameters in a feed-forward fashion to
perform 3D shape reconstruction and NVS from sparse-view images. We further
show that our generalizable pipeline can benefit from preexisting foundational
multi-view deep visual features. The resulting model attains the
state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms
of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also
demonstrates strong generalization on the BlendedMVS and Tanks and Temples
datasets. We note that our model outperforms the prior state-of-the-art in
feed-forward sparse view reconstruction based on volume rendering of implicit
representations, while offering an almost 2 orders of magnitude higher
inference speed.

Comments:
- Project page : https://shubhendu-jena.github.io/SparSplat/

---

## SignSplat: Rendering Sign Language via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2505.02108v1){: .btn .btn-green } |

**Abstract**: State-of-the-art approaches for conditional human body rendering via Gaussian
splatting typically focus on simple body motions captured from many views. This
is often in the context of dancing or walking. However, for more complex use
cases, such as sign language, we care less about large body motion and more
about subtle and complex motions of the hands and face. The problems of
building high fidelity models are compounded by the complexity of capturing
multi-view data of sign. The solution is to make better use of sequence data,
ensuring that we can overcome the limited information from only a few views by
exploiting temporal variability. Nevertheless, learning from sequence-level
data requires extremely accurate and consistent model fitting to ensure that
appearance is consistent across complex motions. We focus on how to achieve
this, constraining mesh parameters to build an accurate Gaussian splatting
framework from few views capable of modelling subtle human motion. We leverage
regularization techniques on the Gaussian parameters to mitigate overfitting
and rendering artifacts. Additionally, we propose a new adaptive control method
to densify Gaussians and prune splat points on the mesh surface. To demonstrate
the accuracy of our approach, we render novel sequences of sign language video,
building on neural machine translation approaches to sign stitching. On
benchmark datasets, our approach achieves state-of-the-art performance; and on
highly articulated and complex sign language motion, we significantly
outperform competing approaches.



---

## HandOcc: NeRF-based Hand Rendering with Occupancy Networks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2505.02079v1){: .btn .btn-green } |

**Abstract**: We propose HandOcc, a novel framework for hand rendering based upon
occupancy. Popular rendering methods such as NeRF are often combined with
parametric meshes to provide deformable hand models. However, in doing so, such
approaches present a trade-off between the fidelity of the mesh and the
complexity and dimensionality of the parametric model. The simplicity of
parametric mesh structures is appealing, but the underlying issue is that it
binds methods to mesh initialization, making it unable to generalize to objects
where a parametric model does not exist. It also means that estimation is tied
to mesh resolution and the accuracy of mesh fitting. This paper presents a
pipeline for meshless 3D rendering, which we apply to the hands. By providing
only a 3D skeleton, the desired appearance is extracted via a convolutional
model. We do this by exploiting a NeRF renderer conditioned upon an
occupancy-based representation. The approach uses the hand occupancy to resolve
hand-to-hand interactions further improving results, allowing fast rendering,
and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,
we achieved state-of-the-art results.



---

## GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity  Non-Watertight 3D Garment Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2505.02126v1){: .btn .btn-green } |

**Abstract**: Traditional 3D garment creation requires extensive manual operations,
resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved
breakthrough progress in 3D scene reconstruction and rendering, attracting
widespread attention and opening new pathways for 3D garment reconstruction.
However, due to the unstructured and irregular nature of Gaussian primitives,
it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In
this paper, we present GarmentGS, a dense point cloud-guided method that can
reconstruct high-fidelity garment surfaces with high geometric accuracy and
generate non-watertight, single-layer meshes. Our method introduces a fast
dense point cloud reconstruction module that can complete garment point cloud
reconstruction in 10 minutes, compared to traditional methods that require
several hours. Furthermore, we use dense point clouds to guide the movement,
flattening, and rotation of Gaussian primitives, enabling better distribution
on the garment surface to achieve superior rendering effects and geometric
accuracy. Through numerical and visual comparisons, our method achieves fast
training and real-time rendering while maintaining competitive quality.



---

## Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.02005v1){: .btn .btn-green } |

**Abstract**: Recent NeRF methods on large-scale scenes have underlined the importance of
scene decomposition for scalable NeRFs. Although achieving reasonable
scalability, there are several critical problems remaining unexplored, i.e.,
learnable decomposition, modeling scene heterogeneity, and modeling efficiency.
In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash
Experts (HMoHE) network that addresses these challenges within a unified
framework. It is a highly scalable NeRF that learns heterogeneous decomposition
and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end
manner. In our framework, a gating network learns to decomposes scenes and
allocates 3D points to specialized NeRF experts. This gating network is
co-optimized with the experts, by our proposed Sparsely Gated Mixture of
Experts (MoE) NeRF framework. We incorporate a hash-based gating network and
distinct heterogeneous hash experts. The hash-based gating efficiently learns
the decomposition of the large-scale scene. The distinct heterogeneous hash
experts consist of hash grids of different resolution ranges, enabling
effective learning of the heterogeneous representation of different scene
parts. These design choices make our framework an end-to-end and highly
scalable NeRF solution for real-world large-scale scene modeling to achieve
both quality and efficiency. We evaluate our accuracy and scalability on
existing large-scale NeRF datasets and a new dataset with very large-scale
scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our
approach can be easily scaled to various large-scale scenes and achieve
state-of-the-art scene rendering accuracy. Furthermore, our method exhibits
significant efficiency, with an 8x acceleration in training and a 16x
acceleration in rendering compared to Switch-NeRF. Codes will be released in
https://github.com/MiZhenxing/Switch-NeRF.

Comments:
- 15 pages, 9 figures

---

## Unified Steganography via Implicit Neural Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Qi Song, Ziyuan Luo, Xiufeng Huang, Sheng Li, Renjie Wan | cs.CR | [PDF](http://arxiv.org/pdf/2505.01749v1){: .btn .btn-green } |

**Abstract**: Digital steganography is the practice of concealing for encrypted data
transmission. Typically, steganography methods embed secret data into cover
data to create stega data that incorporates hidden secret data. However,
steganography techniques often require designing specific frameworks for each
data type, which restricts their generalizability. In this paper, we present
U-INR, a novel method for steganography via Implicit Neural Representation
(INR). Rather than using the specific framework for each data format, we
directly use the neurons of the INR network to represent the secret data and
cover data across different data types. To achieve this idea, a private key is
shared between the data sender and receivers. Such a private key can be used to
determine the position of secret data in INR networks. To effectively leverage
this key, we further introduce a key-based selection strategy that can be used
to determine the position within the INRs for data storage. Comprehensive
experiments across multiple data types, including images, videos, audio, and
SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR,
emphasizing its potential for improving data security and privacy in various
applications.



---

## HybridGS: High-Efficiency Gaussian Splatting Data Compression using  Dual-Channel Sparse Representation and Point Cloud Encoder

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Qi Yang, Le Yang, Geert Van Der Auwera, Zhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.01938v1){: .btn .btn-green } |

**Abstract**: Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on
producing compact 3DGS representation via implicit data embedding. They have
long coding times and highly customized data format, making it difficult for
widespread deployment. This paper presents a new 3DGS compression framework
called HybridGS, which takes advantage of both compact generation and
standardized point cloud data encoding. HybridGS first generates compact and
explicit 3DGS data. A dual-channel sparse representation is introduced to
supervise the primitive position and feature bit depth. It then utilizes a
canonical point cloud encoder to perform further data compression and form
standard output bitstreams. A simple and effective rate control scheme is
proposed to pivot the interpretable data compression scheme. At the current
stage, HybridGS does not include any modules aimed at improving 3DGS quality
during generation. But experiment results show that it still provides
comparable reconstruction performance against state-of-the-art methods, with
evidently higher encoding and decoding speed. The code is publicly available at
https://github.com/Qi-Yangsjtu/HybridGS.

Comments:
- Accepted by ICML2025

---

## AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Junhao Shi, Jisheng Xu, Jianping He, Zhiliang Lin | cs.CV | [PDF](http://arxiv.org/pdf/2505.01799v1){: .btn .btn-green } |

**Abstract**: Underwater scene reconstruction is a critical tech-nology for underwater
operations, enabling the generation of 3D models from images captured by
underwater platforms. However, the quality of underwater images is often
degraded due to medium interference, which limits the effectiveness of
Structure-from-Motion (SfM) pose estimation, leading to subsequent
reconstruction failures. Additionally, SfM methods typically operate at slower
speeds, further hindering their applicability in real-time scenarios. In this
paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model
based on the SeaThru algorithm, which facilitates rapid and accurate separation
of scene details and medium features. Our approach initializes Gaussians by
integrating state-of-the-art multi-view stereo (MVS) technology, employs
implicit Neural Radiance Fields (NeRF) for rendering translucent media and
utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render
object surfaces, which effectively addresses the limitations of traditional
methods and accurately simulates underwater optical phenomena. Experimental
results on the data set and the robot platform show that our model can complete
high-precision reconstruction in 30 seconds with only 3 image inputs,
significantly enhancing the practical application of the algorithm in robotic
platforms.



---

## Visual enhancement and 3D representation for underwater scenes: a review


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Guoxi Huang, Haoran Wang, Brett Seymour, Evan Kovacs, John Ellerbrock, Dave Blackham, Nantheera Anantrasirichai | cs.CV | [PDF](http://arxiv.org/pdf/2505.01869v1){: .btn .btn-green } |

**Abstract**: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose
significant challenges in
  computer vision and AI-based tasks due to complex imaging conditions in
aquatic environments. Despite
  the development of numerous enhancement algorithms, a comprehensive and
systematic review covering both
  UVE and underwater 3D reconstruction remains absent. To advance research in
these areas, we present an
  in-depth review from multiple perspectives. First, we introduce the
fundamental physical models, highlighting the
  peculiarities that challenge conventional techniques. We survey advanced
methods for visual enhancement and
  3D reconstruction specifically designed for underwater scenarios. The paper
assesses various approaches from
  non-learning methods to advanced data-driven techniques, including Neural
Radiance Fields and 3D Gaussian
  Splatting, discussing their effectiveness in handling underwater distortions.
Finally, we conduct both quantitative
  and qualitative evaluations of state-of-the-art UVE and underwater 3D
reconstruction algorithms across multiple
  benchmark datasets. Finally, we highlight key research directions for future
advancements in underwater vision.



---

## GenSync: A Generalized Talking Head Framework for Audio-driven  Multi-Subject Lip-Sync using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Anushka Agarwal, Muhammad Yusuf Hassan, Talha Chafekar | cs.CV | [PDF](http://arxiv.org/pdf/2505.01928v1){: .btn .btn-green } |

**Abstract**: We introduce GenSync, a novel framework for multi-identity lip-synced video
synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that
require training a new model for each identity , GenSync learns a unified
network that synthesizes lip-synced videos for multiple speakers. By
incorporating a Disentanglement Module, our approach separates
identity-specific features from audio representations, enabling efficient
multi-identity video synthesis. This design reduces computational overhead and
achieves 6.8x faster training compared to state-of-the-art models, while
maintaining high lip-sync accuracy and visual quality.



---

## FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft  Research

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-02 | Yan Miao, Will Shen, Hang Cui, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2505.01383v1){: .btn .btn-green } |

**Abstract**: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.



---

## Compensating Spatiotemporally Inconsistent Observations for Online  Dynamic 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-02 | Youngsik Yun, Jeongmin Bae, Hyunseung Son, Seoha Kim, Hahyun Lee, Gun Bang, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2505.01235v1){: .btn .btn-green } |

**Abstract**: Online reconstruction of dynamic scenes is significant as it enables learning
scenes from live-streaming video inputs, while existing offline dynamic
reconstruction methods rely on recorded video inputs. However, previous online
reconstruction approaches have primarily focused on efficiency and rendering
quality, overlooking the temporal consistency of their results, which often
contain noticeable artifacts in static regions. This paper identifies that
errors such as noise in real-world recordings affect temporal inconsistency in
online reconstruction. We propose a method that enhances temporal consistency
in online reconstruction from observations with temporal inconsistency which is
inevitable in cameras. We show that our method restores the ideal observation
by subtracting the learned error. We demonstrate that applying our method to
various baselines significantly enhances both temporal consistency and
rendering quality across datasets. Code, video results, and checkpoints are
available at https://bbangsik13.github.io/OR2.

Comments:
- SIGGRAPH 2025, Project page: https://bbangsik13.github.io/OR2

---

## Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique  Instances in Open-Vocabulary 3D Panoptic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Feng Xue, Wenzhuang Xu, Guofeng Zhong, Anlong Minga, Nicu Sebe | cs.CV | [PDF](http://arxiv.org/pdf/2505.00378v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D panoptic segmentation has recently emerged as a
significant trend. Top-performing methods currently integrate 2D segmentation
with geometry-aware 3D primitives. However, the advantage would be lost without
high-fidelity 3D point clouds, such as methods based on Neural Radiance Field
(NeRF). These methods are limited by the insufficient capacity to maintain
consistency across partial observations. To address this, recent works have
utilized contrastive loss or cross-view association pre-processing for view
consensus. In contrast to them, we present Cues3D, a compact approach that
relies solely on NeRF instead of pre-associations. The core idea is that NeRF's
implicit 3D field inherently establishes a globally consistent geometry,
enabling effective object distinction without explicit cross-view supervision.
We propose a three-phase training framework for NeRF,
initialization-disambiguation-refinement, whereby the instance IDs are
corrected using the initially-learned knowledge. Additionally, an instance
disambiguation method is proposed to match NeRF-rendered 3D masks and ensure
globally unique 3D instance identities. With the aid of Cues3D, we obtain
highly consistent and unique 3D instance ID for each object across views with a
balanced version of NeRF. Our experiments are conducted on ScanNet v2,
ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and
semantic segmentation tasks. Cues3D outperforms other 2D image-based methods
and competes with the latest 2D-3D merging based methods, while even surpassing
them when using additional 3D point clouds. The code link could be found in the
appendix and will be released on
\href{https://github.com/mRobotit/Cues3D}{github}

Comments:
- Accepted by Information Fusion

---

## Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing | cs.CV | [PDF](http://arxiv.org/pdf/2505.00421v1){: .btn .btn-green } |

**Abstract**: High-quality, animatable 3D human avatar reconstruction from monocular videos
offers significant potential for reducing reliance on complex hardware, making
it highly practical for applications in game development, augmented reality,
and social media. However, existing methods still face substantial challenges
in capturing fine geometric details and maintaining animation stability,
particularly under dynamic or complex poses. To address these issues, we
propose a novel real-time framework for animatable human avatar reconstruction
based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose
parameters, our framework not only aligns positional and rotational
discrepancies but also enables robust and natural pose-driven animation of the
reconstructed avatars. Furthermore, we introduce a Rotation Compensation
Network (RCN) that learns rotation residuals by integrating local geometric
features with global pose parameters. This network significantly improves the
handling of non-rigid deformations and ensures smooth, artifact-free pose
transitions during animation. Experimental results demonstrate that our method
successfully reconstructs realistic and highly animatable human avatars from
monocular videos, effectively preserving fine-grained details while ensuring
stable and natural pose variation. Our approach surpasses current
state-of-the-art methods in both reconstruction quality and animation
robustness on public benchmarks.


