---
layout: default
title: May 2025
parent: Papers
nav_order: 202505
---

<!---metadata--->


## ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-29 | Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang | cs.CV | [PDF](http://arxiv.org/pdf/2505.23734v1){: .btn .btn-green } |

**Abstract**: Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a
promising solution for novel view synthesis, enabling one-pass inference
without the need for per-scene 3DGS optimization. However, their scalability is
fundamentally constrained by the limited capacity of their encoders, leading to
degraded performance or excessive memory consumption as the number of input
views increases. In this work, we analyze feed-forward 3DGS frameworks through
the lens of the Information Bottleneck principle and introduce ZPressor, a
lightweight architecture-agnostic module that enables efficient compression of
multi-view inputs into a compact latent state $Z$ that retains essential scene
information while discarding redundancy. Concretely, ZPressor enables existing
feed-forward 3DGS models to scale to over 100 input views at 480P resolution on
an 80GB GPU, by partitioning the views into anchor and support sets and using
cross attention to compress the information from the support views into anchor
views, forming the compressed latent state $Z$. We show that integrating
ZPressor into several state-of-the-art feed-forward 3DGS models consistently
improves performance under moderate input views and enhances robustness under
dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.
The video results, code and trained models are available on our project page:
https://lhmd.top/zpressor.

Comments:
- Project Page: https://lhmd.top/zpressor, Code:
  https://github.com/ziplab/ZPressor

---

## LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient  Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-29 | Jonas Kulhanek, Marie-Julie Rakotosaona, Fabian Manhardt, Christina Tsalicoglou, Michael Niemeyer, Torsten Sattler, Songyou Peng, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2505.23158v1){: .btn .btn-green } |

**Abstract**: In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian
Splatting that enables real-time rendering of large-scale scenes on
memory-constrained devices. Our approach introduces a hierarchical LOD
representation that iteratively selects optimal subsets of Gaussians based on
camera distance, thus largely reducing both rendering time and GPU memory
usage. We construct each LOD level by applying a depth-aware 3D smoothing
filter, followed by importance-based pruning and fine-tuning to maintain visual
fidelity. To further reduce memory overhead, we partition the scene into
spatial chunks and dynamically load only relevant Gaussians during rendering,
employing an opacity-blending mechanism to avoid visual artifacts at chunk
boundaries. Our method achieves state-of-the-art performance on both outdoor
(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality
renderings with reduced latency and memory requirements.

Comments:
- Web: https://lodge-gs.github.io/

---

## AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-29 | Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, Dahua Lin, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2505.23716v1){: .btn .btn-green } |

**Abstract**: We introduce AnySplat, a feed forward network for novel view synthesis from
uncalibrated image collections. In contrast to traditional neural rendering
pipelines that demand known camera poses and per scene optimization, or recent
feed forward methods that buckle under the computational weight of dense views,
our model predicts everything in one shot. A single forward pass yields a set
of 3D Gaussian primitives encoding both scene geometry and appearance, and the
corresponding camera intrinsics and extrinsics for each input image. This
unified design scales effortlessly to casually captured, multi view datasets
without any pose annotations. In extensive zero shot evaluations, AnySplat
matches the quality of pose aware baselines in both sparse and dense view
scenarios while surpassing existing pose free approaches. Moreover, it greatly
reduce rendering latency compared to optimization based neural fields, bringing
real time novel view synthesis within reach for unconstrained capture
settings.Project page: https://city-super.github.io/anysplat/

Comments:
- Project page: https://city-super.github.io/anysplat/

---

## Pose-free 3D Gaussian splatting via shape-ray estimation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-29 | Youngju Na, Taeyeon Kim, Jumin Lee, Kyu Beom Han, Woo Jae Kim, Sung-eui Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2505.22978v1){: .btn .btn-green } |

**Abstract**: While generalizable 3D Gaussian splatting enables efficient, high-quality
rendering of unseen scenes, it heavily depends on precise camera poses for
accurate geometry. In real-world scenarios, obtaining accurate poses is
challenging, leading to noisy pose estimates and geometric misalignments. To
address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting
framework that overcomes these ambiguities by joint shape and camera rays
estimation. Instead of relying on explicit 3D transformations, SHARE builds a
pose-aware canonical volume representation that seamlessly integrates
multi-view information, reducing misalignment caused by inaccurate pose
estimates. Additionally, anchor-aligned Gaussian prediction enhances scene
reconstruction by refining local geometry around coarse anchors, allowing for
more precise Gaussian placement. Extensive experiments on diverse real-world
datasets show that our method achieves robust performance in pose-free
generalizable Gaussian splatting.

Comments:
- ICIP 2025

---

## Mobi-$Ï€$: Mobilizing Your Robot Learning Policy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-29 | Jingyun Yang, Isabella Huang, Brandon Vu, Max Bajracharya, Rika Antonova, Jeannette Bohg | cs.RO | [PDF](http://arxiv.org/pdf/2505.23692v1){: .btn .btn-green } |

**Abstract**: Learned visuomotor policies are capable of performing increasingly complex
manipulation tasks. However, most of these policies are trained on data
collected from limited robot positions and camera viewpoints. This leads to
poor generalization to novel robot positions, which limits the use of these
policies on mobile platforms, especially for precise tasks like pressing
buttons or turning faucets. In this work, we formulate the policy mobilization
problem: find a mobile robot base pose in a novel environment that is in
distribution with respect to a manipulation policy trained on a limited set of
camera viewpoints. Compared to retraining the policy itself to be more robust
to unseen robot base pose initializations, policy mobilization decouples
navigation from manipulation and thus does not require additional
demonstrations. Crucially, this problem formulation complements existing
efforts to improve manipulation policy robustness to novel viewpoints and
remains compatible with them. To study policy mobilization, we introduce the
Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty
of mobilizing a given policy, (2) a suite of simulated mobile manipulation
tasks based on RoboCasa to evaluate policy mobilization, (3) visualization
tools for analysis, and (4) several baseline methods. We also propose a novel
approach that bridges navigation and manipulation by optimizing the robot's
base pose to align with an in-distribution base pose for a learned policy. Our
approach utilizes 3D Gaussian Splatting for novel view synthesis, a score
function to evaluate pose suitability, and sampling-based optimization to
identify optimal robot poses. We show that our approach outperforms baselines
in both simulation and real-world environments, demonstrating its effectiveness
for policy mobilization.

Comments:
- Project website: https://mobipi.github.io/

---

## Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-29 | Chuandong Liu, Huijiao Wang, Lei Yu, Gui-Song Xia | cs.CV | [PDF](http://arxiv.org/pdf/2505.23280v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting have shown remarkable potential for
novel view synthesis. However, most existing large-scale scene reconstruction
methods rely on the divide-and-conquer paradigm, which often leads to the loss
of global scene information and requires complex parameter tuning due to scene
partitioning and local optimization. To address these limitations, we propose
MixGS, a novel holistic optimization framework for large-scale 3D scene
reconstruction. MixGS models the entire scene holistically by integrating
camera pose and Gaussian attributes into a view-aware representation, which is
decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation
combines decoded and original Gaussians to jointly preserve global coherence
and local fidelity. Extensive experiments on large-scale scenes demonstrate
that MixGS achieves state-of-the-art rendering quality and competitive speed,
while significantly reducing computational requirements, enabling large-scale
scene reconstruction training on a single 24GB VRAM GPU. The code will be
released at https://github.com/azhuantou/MixGS.



---

## Radiant Triangle Soup with Soft Connectivity Forces for 3D  Reconstruction and Novel View Synthesis


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-29 | Nathaniel Burgdorfer, Philippos Mordohai | cs.CV | [PDF](http://arxiv.org/pdf/2505.23642v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce an inference-time optimization framework utilizing
triangles to represent the geometry and appearance of the scene. More
specifically, we develop a scene optimization algorithm for triangle soup, a
collection of disconnected semi-transparent triangle primitives. Compared to
the current most-widely used primitives for 3D scene representation, namely
Gaussian splats, triangles allow for more expressive color interpolation, and
benefit from a large algorithmic infrastructure for downstream tasks.
Triangles, unlike full-rank Gaussian kernels, naturally combine to form
surfaces. We formulate connectivity forces between triangles during
optimization, encouraging explicit, but soft, surface continuity in 3D. We
perform experiments on a representative 3D reconstruction dataset and show
competitive photometric and geometric results.



---

## PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-29 | Mohamed Rayan Barhdadi, Hasan Kurban, Hussein Alnuweiri | cs.CV | [PDF](http://arxiv.org/pdf/2505.23481v1){: .btn .btn-green } |

**Abstract**: PhysicsNeRF is a physically grounded framework for 3D reconstruction from
sparse views, extending Neural Radiance Fields with four complementary
constraints: depth ranking, RegNeRF-style consistency, sparsity priors, and
cross-view alignment. While standard NeRFs fail under sparse supervision,
PhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB
average PSNR using only 8 views, outperforming prior methods. A generalization
gap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental
limitations of sparse-view reconstruction. PhysicsNeRF enables physically
consistent, generalizable 3D representations for agent interaction and
simulation, and clarifies the expressiveness-generalization trade-off in
constrained NeRF models.

Comments:
- 4 pages, 2 figures, 2 tables. Preliminary work. Under review by the
  Building Physically Plausible World Models Workshop at the 42nd International
  Conference on Machine Learning (ICML 2025), Vancouver, Canada

---

## CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-28 | Kornel Howil, Joanna WaczyÅ„ska, Piotr Borycki, Tadeusz Dziarmaga, Marcin Mazur, PrzemysÅ‚aw Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2505.22854v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has recently emerged as an efficient representation
for rendering 3D scenes from 2D images and has been extended to images, videos,
and dynamic 4D content. However, applying style transfer to GS-based
representations, especially beyond simple color changes, remains challenging.
In this work, we introduce CLIPGaussians, the first unified style transfer
framework that supports text- and image-guided stylization across multiple
modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates
directly on Gaussian primitives and integrates into existing GS pipelines as a
plug-in module, without requiring large generative models or retraining from
scratch. CLIPGaussians approach enables joint optimization of color and
geometry in 3D and 4D settings, and achieves temporal coherence in videos,
while preserving a model size. We demonstrate superior style fidelity and
consistency across all tasks, validating CLIPGaussians as a universal and
efficient solution for multimodal style transfer.



---

## 3DGS Compression with Sparsity-guided Hierarchical Transform Coding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-28 | Hao Xu, Xiaolin Wu, Xi Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2505.22908v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained popularity for its fast and
high-quality rendering, but it has a very large memory footprint incurring high
transmission and storage overhead. Recently, some neural compression methods,
such as Scaffold-GS, were proposed for 3DGS but they did not adopt the approach
of end-to-end optimized analysis-synthesis transforms which has been proven
highly effective in neural signal compression. Without an appropriate analysis
transform, signal correlations cannot be removed by sparse representation.
Without such transforms the only way to remove signal redundancies is through
entropy coding driven by a complex and expensive context modeling, which
results in slower speed and suboptimal rate-distortion (R-D) performance. To
overcome this weakness, we propose Sparsity-guided Hierarchical Transform
Coding (SHTC), the first end-to-end optimized transform coding framework for
3DGS compression. SHTC jointly optimizes the 3DGS, transforms and a lightweight
context model. This joint optimization enables the transform to produce
representations that approach the best R-D performance possible. The SHTC
framework consists of a base layer using KLT for data decorrelation, and a
sparsity-coded enhancement layer that compresses the KLT residuals to refine
the representation. The enhancement encoder learns a linear transform to
project high-dimensional inputs into a low-dimensional space, while the decoder
unfolds the Iterative Shrinkage-Thresholding Algorithm (ISTA) to reconstruct
the residuals. All components are designed to be interpretable, allowing the
incorporation of signal priors and fewer parameters than black-box transforms.
This novel design significantly improves R-D performance with minimal
additional parameters and computational overhead.



---

## Can NeRFs See without Cameras?

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-28 | Chaitanya Amballa, Sattwik Basu, Yu-Lin Wei, Zhijian Yang, Mehmet Ergezer, Romit Roy Choudhury | cs.CV | [PDF](http://arxiv.org/pdf/2505.22441v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.



---

## Hyperspectral Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-28 | Sunil Kumar Narayanan, Lingjun Zhao, Lu Gan, Yongsheng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2505.21890v1){: .btn .btn-green } |

**Abstract**: Hyperspectral imaging (HSI) has been widely used in agricultural applications
for non-destructive estimation of plant nutrient composition and precise
determination of nutritional elements in samples. Recently, 3D reconstruction
methods have been used to create implicit neural representations of HSI scenes,
which can help localize the target object's nutrient composition spatially and
spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit
representation that can render hyperspectral channel compositions of each
spatial location from any viewing direction. However, it faces limitations in
training time and rendering speed. In this paper, we propose Hyperspectral
Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian
Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of
the hyperspectral scenes and novel view synthesis for the entire spectral
range. To enhance the model's ability to capture fine-grained reflectance
variations across the light spectrum and leverage correlations between adjacent
wavelengths for denoising, we introduce a wavelength encoder to generate
wavelength-specific spherical harmonics offsets. We also introduce a novel
Kullback--Leibler divergence-based loss to mitigate the spectral distribution
gap between the rendered image and the ground truth. A diffusion model is
further applied for denoising the rendered images and generating photorealistic
hyperspectral images. We present extensive evaluations on five diverse
hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of
our proposed HS-GS framework. The results demonstrate that HS-GS achieves new
state-of-the-art performance among all previously published methods. Code will
be released upon publication.



---

## STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-28 | Zehao Li, Hao Jiang, Yujun Cai, Jianing Chen, Baolong Bi, Shuqin Gao, Honglong Zhao, Yiwei Wang, Tianlu Mao, Zhaoqi Wang | cs.GR | [PDF](http://arxiv.org/pdf/2505.22400v1){: .btn .btn-green } |

**Abstract**: Although dynamic scene reconstruction has long been a fundamental challenge
in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a
promising direction by enabling high-quality, real-time rendering through
explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic
reconstruction often suffer from \textit{spatio-temporal incoherence} during
initialization, where canonical Gaussians are constructed by aggregating
observations from multiple frames without temporal distinction. This results in
spatio-temporally entangled representations, making it difficult to model
dynamic motion accurately. To overcome this limitation, we propose
\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a
plug-and-play module that learns spatio-temporal probability distributions for
each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation
field, and a consistency regularization to jointly disentangle spatial and
temporal patterns. Extensive experiments demonstrate that incorporating our
module into existing 3DGS-based dynamic scene reconstruction frameworks leads
to notable improvements in both reconstruction quality and spatio-temporal
consistency across synthetic and real-world benchmarks.



---

## UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction  in Dynamic Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-28 | Wancai Zheng, Linlin Ou, Jiajie He, Libo Zhou, Xinyi Yu, Yan Wei | cs.RO | [PDF](http://arxiv.org/pdf/2505.22335v1){: .btn .btn-green } |

**Abstract**: Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous
Localization and Mapping (SLAM) have significantly progressed in tracking and
high-fidelity mapping. However, their sequential optimization framework and
sensitivity to dynamic objects limit real-time performance and robustness in
real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for
dynamic environments that decouples tracking and mapping through a parallelized
framework. A probabilistic octree is employed to manage Gaussian primitives
adaptively, enabling efficient initialization and pruning without hand-crafted
thresholds. To robustly filter dynamic regions during tracking, we propose a
training-free uncertainty estimator that fuses multi-modal residuals to
estimate per-pixel motion uncertainty, achieving open-set dynamic object
handling without reliance on semantic labels. Furthermore, a temporal encoder
is designed to enhance rendering quality. Concurrently, low-dimensional
features are efficiently transformed via a shallow multilayer perceptron to
construct DINO features, which are then employed to enrich the Gaussian field
and improve the robustness of uncertainty prediction. Extensive experiments on
multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art
methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57
dB PSNR), while maintaining real-time performance and producing reusable,
artifact-free static maps in dynamic environments.The project:
https://aczheng-cai.github.io/up_slam.github.io/



---

## Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade  Depth Loss

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-28 | Wenjun Lu, Haodong Chen, Anqi Yi, Yuk Ying Chung, Zhiyong Wang, Kun Hu | cs.CV | [PDF](http://arxiv.org/pdf/2505.22279v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis is a fundamental task in 3D computer vision that aims to
reconstruct realistic images from a set of posed input views. However,
reconstruction quality degrades significantly under sparse-view conditions due
to limited geometric cues. Existing methods, such as Neural Radiance Fields
(NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from
blurred details and structural artifacts when trained with insufficient views.
Recent works have identified the quality of rendered depth as a key factor in
mitigating these artifacts, as it directly affects geometric accuracy and view
consistency. In this paper, we address these challenges by introducing
Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that
progressively refines geometry from coarse to fine levels. Central to HDGS is a
novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and
estimated monocular depths across multiple spatial scales. By enforcing
multi-scale depth consistency, our method substantially improves structural
fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU
benchmarks demonstrate that HDGS achieves state-of-the-art performance under
sparse-view settings while maintaining efficient and high-quality rendering



---

## Empowering Vector Graphics with Consistently Arbitrary Viewing and  View-dependent Visibility


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-27 | Yidi Li, Jun Xiao, Zhengda Lu, Yiqun Wang, Haiyong Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2505.21377v1){: .btn .btn-green } |

**Abstract**: This work presents a novel text-to-vector graphics generation approach,
Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail
optimization, and view-dependent occlusion awareness. Our approach is a
dual-branch optimization framework, consisting of an auxiliary 3D Gaussian
Splatting optimization branch and a 3D vector graphics optimization branch. The
introduced 3DGS branch can bridge the domain gaps between text prompts and
vector graphics with more consistent guidance. Moreover, 3DGS allows for
progressive detail control by scheduling classifier-free guidance, facilitating
guiding vector graphics with coarse shapes at the initial stages and finer
details at later stages. We also improve the view-dependent occlusions by
devising a visibility-awareness rendering module. Extensive results on 3D
sketches and 3D iconographies, demonstrate the superiority of the method on
different abstraction levels of details, cross-view consistency, and
occlusion-aware stroke culling.

Comments:
- CVPR 2025

---

## CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings  and Residual Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-27 | Weihang Liu, Yuhui Zhong, Yuke Li, Xi Chen, Jiadi Cui, Honglong Zhang, Lan Xu, Xin Lou, Yujiao Shi, Jingyi Yu, Yingliang Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2505.21041v2){: .btn .btn-green } |

**Abstract**: Accurate and efficient modeling of large-scale urban scenes is critical for
applications such as AR navigation, UAV based inspection, and smart city
digital twins. While aerial imagery offers broad coverage and complements
limitations of ground-based data, reconstructing city-scale environments from
such views remains challenging due to occlusions, incomplete geometry, and high
memory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve
scalability and visual quality but remain limited by dense primitive usage,
long training times, and poor suit ability for edge devices. We propose CityGo,
a hybrid framework that combines textured proxy geometry with residual and
surrounding 3D Gaussians for lightweight, photorealistic rendering of urban
scenes from aerial perspectives. Our approach first extracts compact building
proxy meshes from MVS point clouds, then uses zero order SH Gaussians to
generate occlusion-free textures via image-based rendering and back-projection.
To capture high-frequency details, we introduce residual Gaussians placed based
on proxy-photo discrepancies and guided by depth priors. Broader urban context
is represented by surrounding Gaussians, with importance-aware downsampling
applied to non-critical regions to reduce redundancy. A tailored optimization
strategy jointly refines proxy textures and Gaussian parameters, enabling
real-time rendering of complex urban scenes on mobile GPUs with significantly
reduced training and memory requirements. Extensive experiments on real-world
aerial datasets demonstrate that our hybrid representation significantly
reduces training time, achieving on average 1.4x speedup, while delivering
comparable visual fidelity to pure 3D Gaussian Splatting approaches.
Furthermore, CityGo enables real-time rendering of large-scale urban scenes on
mobile consumer GPUs, with substantially reduced memory usage and energy
consumption.



---

## Structure from Collision

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-27 | Takuhiro Kaneko | cs.GR | [PDF](http://arxiv.org/pdf/2505.21335v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural 3D representations, such as neural radiance
fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate
estimation of 3D structures from multiview images. However, this capability is
limited to estimating the visible external structure, and identifying the
invisible internal structure hidden behind the surface is difficult. To
overcome this limitation, we address a new task called Structure from Collision
(SfC), which aims to estimate the structure (including the invisible internal
structure) of an object from appearance changes during collision. To solve this
problem, we propose a novel model called SfC-NeRF that optimizes the invisible
internal structure of an object through a video sequence under physical,
appearance (i.e., visible external structure)-preserving, and keyframe
constraints. In particular, to avoid falling into undesirable local optima
owing to its ill-posed nature, we propose volume annealing; that is, searching
for global optima by repeatedly reducing and expanding the volume. Extensive
experiments on 115 objects involving diverse structures (i.e., various cavity
shapes, locations, and sizes) and material properties revealed the properties
of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.

Comments:
- Accepted to CVPR 2025 (Highlight). Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/sfc/

---

## Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-27 | Zechen Li, Lanqing Yang, Yiheng Bian, Hao Pan, Yongjian Fu, Yezhou Wang, Yi-Chao Chen, Guangtao Xue, Ju Ren | cs.NI | [PDF](http://arxiv.org/pdf/2505.20714v1){: .btn .btn-green } |

**Abstract**: This paper presents an innovative frequency-embedded 3D Gaussian splatting
(3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling,
offering an advancement over the existing works limited to single-frequency
modeling. Grounded in fundamental physics, we uncover the complex relationship
between EM wave propagation behaviors and RF frequencies. Inspired by this, we
design an EM feature network with attenuation and radiance modules to learn the
complex relationships between RF frequencies and the key properties of each 3D
Gaussian, specifically the attenuation factor and RF signal intensity. By
training the frequency-embedded 3DGS model, we can efficiently reconstruct RF
radiance fields at arbitrary unknown frequencies within a given 3D environment.
Finally, we propose a large-scale power angular spectrum (PAS) dataset
containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments,
and conduct extensive experiments to verify the effectiveness of our method.
Our approach achieves an average Structural Similarity Index Measure (SSIM) up
to 0.72, and a significant improvement up to 17.8% compared to the current
state-of-the-art (SOTA) methods trained on individual test frequencies.
Additionally, our method achieves an SSIM of 0.70 without prior training on
these frequencies, which represents only a 2.8% performance drop compared to
models trained with full PAS data. This demonstrates our model's capability to
estimate PAS at unknown frequencies. For related code and datasets, please
refer to https://github.com/sim-2-real/Wideband3DGS.



---

## Generalizable and Relightable Gaussian Splatting for Human Novel View  Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-27 | Yipengjing Sun, Chenyang Wang, Shunyuan Zheng, Zonglin Li, Shengping Zhang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2505.21502v1){: .btn .btn-green } |

**Abstract**: We propose GRGS, a generalizable and relightable 3D Gaussian framework for
high-fidelity human novel view synthesis under diverse lighting conditions.
Unlike existing methods that rely on per-character optimization or ignore
physical constraints, GRGS adopts a feed-forward, fully supervised strategy
that projects geometry, material, and illumination cues from multi-view 2D
observations into 3D Gaussian representations. Specifically, to reconstruct
lighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement
(LGR) module trained on synthetically relit data to predict accurate depth and
surface normals. Based on the high-quality geometry, a Physically Grounded
Neural Rendering (PGNR) module is further proposed to integrate neural
prediction with physics-based shading, supporting editable relighting with
shadows and indirect illumination. Besides, we design a 2D-to-3D projection
training scheme that leverages differentiable supervision from ambient
occlusion, direct, and indirect lighting maps, which alleviates the
computational cost of explicit ray tracing. Extensive experiments demonstrate
that GRGS achieves superior visual quality, geometric consistency, and
generalization across characters and lighting conditions.

Comments:
- Project Webpage: https://sypj-98.github.io/grgs/

---

## Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-27 | Xiangyu Sun, Runnan Chen, Mingming Gong, Dong Xu, Tongliang Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.20729v1){: .btn .btn-green } |

**Abstract**: Sparse-view scene reconstruction often faces significant challenges due to
the constraints imposed by limited observational data. These limitations result
in incomplete information, leading to suboptimal reconstructions using existing
methodologies. To address this, we present Intern-GS, a novel approach that
effectively leverages rich prior knowledge from vision foundation models to
enhance the process of sparse-view Gaussian Splatting, thereby enabling
high-quality scene reconstruction. Specifically, Intern-GS utilizes vision
foundation models to guide both the initialization and the optimization process
of 3D Gaussian splatting, effectively addressing the limitations of sparse
inputs. In the initialization process, our method employs DUSt3R to generate a
dense and non-redundant gaussian point cloud. This approach significantly
alleviates the limitations encountered by traditional structure-from-motion
(SfM) methods, which often struggle under sparse-view constraints. During the
optimization process, vision foundation models predict depth and appearance for
unobserved views, refining the 3D Gaussians to compensate for missing
information in unseen regions. Extensive experiments demonstrate that Intern-GS
achieves state-of-the-art rendering quality across diverse datasets, including
both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and
Temples.



---

## 3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics  Based Appearance-Medium Decoupling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-27 | Jieyu Yuan, Yujun Li, Yuanlin Zhang, Chunle Guo, Xiongxin Tang, Ruixing Wang, Chongyi Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.21238v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis for underwater scene reconstruction presents unique
challenges due to complex light-media interactions. Optical scattering and
absorption in water body bring inhomogeneous medium attenuation interference
that disrupts conventional volume rendering assumptions of uniform propagation
medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering
capabilities, it struggles with underwater inhomogeneous environments where
scattering media introduce artifacts and inconsistent appearance. In this
study, we propose a physics-based framework that disentangles object appearance
from water medium effects through tailored Gaussian modeling. Our approach
introduces appearance embeddings, which are explicit medium representations for
backscatter and attenuation, enhancing scene consistency. In addition, we
propose a distance-guided optimization strategy that leverages pseudo-depth
maps as supervision with depth regularization and scale penalty terms to
improve geometric fidelity. By integrating the proposed appearance and medium
modeling components via an underwater imaging model, our approach achieves both
high-quality novel view synthesis and physically accurate scene restoration.
Experiments demonstrate our significant improvements in rendering quality and
restoration accuracy over existing methods. The project page is available at
https://bilityniu.github.io/3D-UIR.



---

## Weather-Magician: Reconstruction and Rendering Framework for 4D Weather  Synthesis In Real Time

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Chen Sang, Yeqiang Qian, Jiale Zhang, Chunxiang Wang, Ming Yang | cs.CV | [PDF](http://arxiv.org/pdf/2505.19919v1){: .btn .btn-green } |

**Abstract**: For tasks such as urban digital twins, VR/AR/game scene design, or creating
synthetic films, the traditional industrial approach often involves manually
modeling scenes and using various rendering engines to complete the rendering
process. This approach typically requires high labor costs and hardware
demands, and can result in poor quality when replicating complex real-world
scenes. A more efficient approach is to use data from captured real-world
scenes, then apply reconstruction and rendering algorithms to quickly recreate
the authentic scene. However, current algorithms are unable to effectively
reconstruct and render real-world weather effects. To address this, we propose
a framework based on gaussian splatting, that can reconstruct real scenes and
render them under synthesized 4D weather effects. Our work can simulate various
common weather effects by applying Gaussians modeling and rendering techniques.
It supports continuous dynamic weather changes and can easily control the
details of the effects. Additionally, our work has low hardware requirements
and achieves real-time rendering performance. The result demos can be accessed
on our project homepage: weathermagician.github.io

Comments:
- Project homepage: https://weathermagician.github.io

---

## GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot  View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | You Wang, Li Fang, Hao Zhu, Fei Hu, Long Ye, Zhan Ma | cs.CV | [PDF](http://arxiv.org/pdf/2505.19813v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have transformed novel view synthesis by
modeling scene-specific volumetric representations directly from images. While
generalizable NeRF models can generate novel views across unknown scenes by
learning latent ray representations, their performance heavily depends on a
large number of multi-view observations. However, with limited input views,
these methods experience significant degradation in rendering quality. To
address this limitation, we propose GoLF-NRT: a Global and Local feature
Fusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable
neural rendering from few input views by leveraging a 3D transformer with
efficient sparse attention to capture global scene context. In parallel, it
integrates local geometric features extracted along the epipolar line, enabling
high-quality scene reconstruction from as few as 1 to 3 input views.
Furthermore, we introduce an adaptive sampling strategy based on attention
weights and kernel regression, improving the accuracy of transformer-based
neural rendering. Extensive experiments on public datasets show that GoLF-NRT
achieves state-of-the-art performance across varying numbers of input views,
highlighting the effectiveness and superiority of our approach. Code is
available at https://github.com/KLMAV-CUC/GoLF-NRT.

Comments:
- CVPR 2025

---

## ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian  Regularization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Shintaro Ito, Natsuki Takama, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki | cs.CV | [PDF](http://arxiv.org/pdf/2505.19883v1){: .btn .btn-green } |

**Abstract**: The use of multi-view images acquired by a 360-degree camera can reconstruct
a 3D space with a wide area. There are 3D reconstruction methods from
equirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis
(NVS) methods. On the other hand, it is necessary to overcome the large
distortion caused by the projection model of a 360-degree camera when
equirectangular images are used. In 3DGS-based methods, the large distortion of
the 360-degree camera model generates extremely large 3D Gaussians, resulting
in poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based
on 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering
accuracy improvement techniques: geometric regularization, scale
regularization, and distortion-aware weights and a mask to suppress the effects
of obstacles in equirectangular images. Through experiments on public datasets,
we demonstrate that ErpGS can render novel view images more accurately than
conventional methods.

Comments:
- Accepted to ICIP2025

---

## Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance  Field Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Li Fang, Hao Zhu, Longlong Chen, Fei Hu, Long Ye, Zhan Ma | cs.CV | [PDF](http://arxiv.org/pdf/2505.19793v1){: .btn .btn-green } |

**Abstract**: Recent advancements in generalizable novel view synthesis have achieved
impressive quality through interpolation between nearby views. However,
rendering high-resolution images remains computationally intensive due to the
need for dense sampling of all rays. Recognizing that natural scenes are
typically piecewise smooth and sampling all rays is often redundant, we propose
a novel depth-guided bundle sampling strategy to accelerate rendering. By
grouping adjacent rays into a bundle and sampling them collectively, a shared
representation is generated for decoding all rays within the bundle. To further
optimize efficiency, our adaptive sampling strategy dynamically allocates
samples based on depth confidence, concentrating more samples in complex
regions while reducing them in smoother areas. When applied to ENeRF, our
method achieves up to a 1.27 dB PSNR improvement and a 47% increase in FPS on
the DTU dataset. Extensive experiments on synthetic and real-world datasets
demonstrate state-of-the-art rendering quality and up to 2x faster rendering
compared to existing generalizable methods. Code is available at
https://github.com/KLMAV-CUC/GDB-NeRF.

Comments:
- CVPR 2025

---

## HaloGS: Loose Coupling of Compact Geometry and Gaussian Splats for 3D  Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Changjian Jiang, Kerui Ren, Linning Xu, Jiong Chen, Jiangmiao Pang, Yu Zhang, Bo Dai, Mulin Yu | cs.CV | [PDF](http://arxiv.org/pdf/2505.20267v1){: .btn .btn-green } |

**Abstract**: High fidelity 3D reconstruction and rendering hinge on capturing precise
geometry while preserving photo realistic detail. Most existing methods either
fuse these goals into a single cumbersome model or adopt hybrid schemes whose
uniform primitives lead to a trade off between efficiency and fidelity. In this
paper, we introduce HaloGS, a dual representation that loosely couples coarse
triangles for geometry with Gaussian primitives for appearance, motivated by
the lightweight classic geometry representations and their proven efficiency in
real world applications. Our design yields a compact yet expressive model
capable of photo realistic rendering across both indoor and outdoor
environments, seamlessly adapting to varying levels of scene complexity.
Experiments on multiple benchmark datasets demonstrate that our method yields
both compact, accurate geometry and high fidelity renderings, especially in
challenging scenarios where robust geometric structure make a clear difference.



---

## OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction  Using Blender

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Shintaro Ito, Natsuki Takama, Toshiki Watanabe, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki | cs.CV | [PDF](http://arxiv.org/pdf/2505.20126v1){: .btn .btn-green } |

**Abstract**: Recent advancements in radiance field rendering, exemplified by Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly
progressed 3D modeling and reconstruction. The use of multiple 360-degree
omnidirectional images for these tasks is increasingly favored due to
advantages in data acquisition and comprehensive scene capture. However, the
inherent geometric distortions in common omnidirectional representations, such
as equirectangular projection (particularly severe in polar regions and varying
with latitude), pose substantial challenges to achieving high-fidelity 3D
reconstructions. Current datasets, while valuable, often lack the specific
focus, scene composition, and ground truth granularity required to
systematically benchmark and drive progress in overcoming these
omnidirectional-specific challenges. To address this critical gap, we introduce
Omnidirectional Blender 3D (OB3D), a new synthetic dataset curated for
advancing 3D reconstruction from multiple omnidirectional images. OB3D features
diverse and complex 3D scenes generated from Blender 3D projects, with a
deliberate emphasis on challenging scenarios. The dataset provides
comprehensive ground truth, including omnidirectional RGB images, precise
omnidirectional camera parameters, and pixel-aligned equirectangular maps for
depth and normals, alongside evaluation metrics. By offering a controlled yet
challenging environment, OB3Daims to facilitate the rigorous evaluation of
existing methods and prompt the development of new techniques to enhance the
accuracy and reliability of 3D reconstruction from omnidirectional images.



---

## Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian  Splatting with Dense Point Cloud

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Natsuki Takama, Shintaro Ito, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki | cs.CV | [PDF](http://arxiv.org/pdf/2505.19854v2){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has gained attention as a fast and effective method
for novel view synthesis. It has also been applied to 3D reconstruction using
multi-view images and can achieve fast and accurate 3D reconstruction. However,
GS assumes that the input contains a large number of multi-view images, and
therefore, the reconstruction accuracy significantly decreases when only a
limited number of input images are available. One of the main reasons is the
insufficient number of 3D points in the sparse point cloud obtained through
Structure from Motion (SfM), which results in a poor initialization for
optimizing the Gaussian primitives. We propose a new 3D reconstruction method,
called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three
images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along
with COLMAP MVS to generate highly accurate and dense 3D point clouds, which
are then used to initialize 2D Gaussians. Through experiments on the DTU
dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of
objects using just three images. The project page is available at
https://gsisaoki.github.io/SPARSE2DGS/

Comments:
- Accepted to ICIP 2025

---

## CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Lei Tian, Xiaomin Li, Liqian Ma, Hefei Huang, Zirui Zheng, Hao Yin, Taiqing Li, Huchuan Lu, Xu Jia | cs.CV | [PDF](http://arxiv.org/pdf/2505.20469v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D reconstruction techniques and vision-language models
have fueled significant progress in 3D semantic understanding, a capability
critical to robotics, autonomous driving, and virtual/augmented reality.
However, methods that rely on 2D priors are prone to a critical challenge:
cross-view semantic inconsistencies induced by occlusion, image blur, and
view-dependent variations. These inconsistencies, when propagated via
projection supervision, deteriorate the quality of 3D Gaussian semantic fields
and introduce artifacts in the rendered outputs. To mitigate this limitation,
we propose CCL-LGS, a novel framework that enforces view-consistent semantic
supervision by integrating multi-view semantic cues. Specifically, our approach
first employs a zero-shot tracker to align a set of SAM-generated 2D masks and
reliably identify their corresponding categories. Next, we utilize CLIP to
extract robust semantic encodings across views. Finally, our Contrastive
Codebook Learning (CCL) module distills discriminative semantic features by
enforcing intra-class compactness and inter-class distinctiveness. In contrast
to previous methods that directly apply CLIP to imperfect masks, our framework
explicitly resolves semantic conflicts while preserving category
discriminability. Extensive experiments demonstrate that CCL-LGS outperforms
previous state-of-the-art methods. Our project page is available at
https://epsilontl.github.io/CCL-LGS/.



---

## ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Wenhua Wu, Chenpeng Su, Siting Zhu, Tianchen Deng, Zhe Liu, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2505.19420v1){: .btn .btn-green } |

**Abstract**: Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based
Simultaneous Localization and Mapping (SLAM) methods have demonstrated
exceptional localization precision and remarkable dense mapping performance.
However, dynamic objects introduce critical challenges by disrupting scene
consistency, leading to tracking drift and mapping artifacts. Existing methods
that employ semantic segmentation or object detection for dynamic
identification and filtering typically rely on predefined categorical priors,
while discarding dynamic scene information crucial for robotic applications
such as dynamic obstacle avoidance and environmental interaction. To overcome
these challenges, we propose ADD-SLAM: an Adaptive Dynamic Dense SLAM framework
based on Gaussian splitting. We design an adaptive dynamic identification
mechanism grounded in scene consistency analysis, comparing geometric and
textural discrepancies between real-time observations and historical maps. Ours
requires no predefined semantic category priors and adaptively discovers scene
dynamics. Precise dynamic object recognition effectively mitigates interference
from moving targets during localization. Furthermore, we propose a
dynamic-static separation mapping strategy that constructs a temporal Gaussian
model to achieve online incremental dynamic modeling. Experiments conducted on
multiple dynamic datasets demonstrate our method's flexible and accurate
dynamic segmentation capabilities, along with state-of-the-art performance in
both localization and mapping.



---

## K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple  Buffers

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Haofan Ren, Zunjie Zhu, Xiang Chen, Ming Lu, Rongfeng Lu, Chenggang Yan | cs.CV | [PDF](http://arxiv.org/pdf/2505.19564v1){: .btn .btn-green } |

**Abstract**: Neural fields are now the central focus of research in 3D vision and computer
graphics. Existing methods mainly focus on various scene representations, such
as neural points and 3D Gaussians. However, few works have studied the
rendering process to enhance the neural fields. In this work, we propose a
plug-in method named K-Buffers that leverages multiple buffers to improve the
rendering performance. Our method first renders K buffers from scene
representations and constructs K pixel-wise feature maps. Then, We introduce a
K-Feature Fusion Network (KFN) to merge the K pixel-wise feature maps. Finally,
we adopt a feature decoder to generate the rendering image. We also introduce
an acceleration strategy to improve rendering speed and quality. We apply our
method to well-known radiance field baselines, including neural point fields
and 3D Gaussian Splatting (3DGS). Extensive experiments demonstrate that our
method effectively enhances the rendering performance of neural point fields
and 3DGS.

Comments:
- 15 pages, 9 figures, IJCAI 2025

---

## ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for  Prior-free Motion Extrapolation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-26 | Jinsheng Quan, Chunshi Wang, Yawei Luo | cs.CV | [PDF](http://arxiv.org/pdf/2505.20270v1){: .btn .btn-green } |

**Abstract**: This paper aims to model the dynamics of 3D Gaussians from visual
observations to support temporal extrapolation. Existing dynamic 3D
reconstruction methods often struggle to effectively learn underlying dynamics
or rely heavily on manually defined physical priors, which limits their
extrapolation capabilities. To address this issue, we propose a novel dynamic
3D Gaussian Splatting prior-free motion extrapolation framework based on
particle dynamics systems. The core advantage of our method lies in its ability
to learn differential equations that describe the dynamics of 3D Gaussians, and
follow them during future frame extrapolation. Instead of simply fitting to the
observed visual frame sequence, we aim to more effectively model the gaussian
particle dynamics system. To this end, we introduce a dynamics latent state
vector into the standard Gaussian kernel and design a dynamics latent space
encoder to extract initial state. Subsequently, we introduce a Neural
ODEs-based dynamics module that models the temporal evolution of Gaussian in
dynamics latent space. Finally, a Gaussian kernel space decoder is used to
decode latent state at the specific time step into the deformation.
Experimental results demonstrate that the proposed method achieves comparable
rendering quality with existing approaches in reconstruction tasks, and
significantly outperforms them in future frame extrapolation. Our code is
available at https://github.com/QuanJinSheng/ParticleGS.



---

## VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale  Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-25 | Tianchen Deng, Wenhua Wu, Junjie He, Yue Pan, Xirui Jiang, Shenghai Yuan, Danwei Wang, Hesheng Wang, Weidong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2505.18992v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently shown promising results in dense visual
SLAM. However, existing 3DGS-based SLAM methods are all constrained to
small-room scenarios and struggle with memory explosion in large-scale scenes
and long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based
large-scale RGBD SLAM framework for both indoor and outdoor scenarios. We
design a novel voxel-based progressive 3D Gaussian mapping method with multiple
submaps for compact and accurate scene representation in large-scale and
long-sequence scenes. This allows us to scale up to arbitrary scenes and
improves robustness (even under pose drifts). In addition, we propose a 2D-3D
fusion camera tracking method to achieve robust and accurate camera tracking in
both indoor and outdoor large-scale scenes. Furthermore, we design a 2D-3D
Gaussian loop closure method to eliminate pose drift. We further propose a
submap fusion method with online distillation to achieve global consistency in
large-scale scenes when detecting a loop. Experiments on various indoor and
outdoor datasets demonstrate the superiority and generalizability of the
proposed framework. The code will be open source on
https://github.com/dtc111111/vpgs-slam.



---

## Improving Novel view synthesis of 360$^\circ$ Scenes in Extremely Sparse  Views by Jointly Training Hemisphere Sampled Synthetic Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-25 | Guangan Chen, Anh Minh Truong, Hanhe Lin, Michiel Vlaminck, Wilfried Philips, Hiep Luong | cs.CV | [PDF](http://arxiv.org/pdf/2505.19264v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis in 360$^\circ$ scenes from extremely sparse input views
is essential for applications like virtual reality and augmented reality. This
paper presents a novel framework for novel view synthesis in extremely
sparse-view cases. As typical structure-from-motion methods are unable to
estimate camera poses in extremely sparse-view cases, we apply DUSt3R to
estimate camera poses and generate a dense point cloud. Using the poses of
estimated cameras, we densely sample additional views from the upper hemisphere
space of the scenes, from which we render synthetic images together with the
point cloud. Training 3D Gaussian Splatting model on a combination of reference
images from sparse views and densely sampled synthetic images allows a larger
scene coverage in 3D space, addressing the overfitting challenge due to the
limited input in sparse-view cases. Retraining a diffusion-based image
enhancement model on our created dataset, we further improve the quality of the
point-cloud-rendered images by removing artifacts. We compare our framework
with benchmark methods in cases of only four input views, demonstrating
significant improvement in novel view synthesis under extremely sparse-view
conditions for 360$^\circ$ scenes.



---

## FHGS: Feature-Homogenized Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-25 | Q. G. Duan, Benyun Zhao, Mingqiao Han Yijun Huang, Ben M. Chen | cs.CV | [PDF](http://arxiv.org/pdf/2505.19154v1){: .btn .btn-green } |

**Abstract**: Scene understanding based on 3D Gaussian Splatting (3DGS) has recently
achieved notable advances. Although 3DGS related methods have efficient
rendering capabilities, they fail to address the inherent contradiction between
the anisotropic color representation of gaussian primitives and the isotropic
requirements of semantic features, leading to insufficient cross-view feature
consistency. To overcome the limitation, we proposes $\textit{FHGS}$
(Feature-Homogenized Gaussian Splatting), a novel 3D feature fusion framework
inspired by physical models, which can achieve high-precision mapping of
arbitrary 2D features from pre-trained models to 3D scenes while preserving the
real-time rendering efficiency of 3DGS. Specifically, our $\textit{FHGS}$
introduces the following innovations: Firstly, a universal feature fusion
architecture is proposed, enabling robust embedding of large-scale pre-trained
models' semantic features (e.g., SAM, CLIP) into sparse 3D structures.
Secondly, a non-differentiable feature fusion mechanism is introduced, which
enables semantic features to exhibit viewpoint independent isotropic
distributions. This fundamentally balances the anisotropic rendering of
gaussian primitives and the isotropic expression of features; Thirdly, a
dual-driven optimization strategy inspired by electric potential fields is
proposed, which combines external supervision from semantic feature fields with
internal primitive clustering guidance. This mechanism enables synergistic
optimization of global semantic alignment and local structural consistency.
More interactive results can be accessed on: https://fhgs.cuastro.org/.



---

## Triangle Splatting for Real-Time Radiance Field Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-25 | Jan Held, Renaud Vandeghen, Adrien Deliege, Abdullah Hamdi, Silvio Giancola, Anthony Cioppa, Andrea Vedaldi, Bernard Ghanem, Andrea Tagliasacchi, Marc Van Droogenbroeck | cs.CV | [PDF](http://arxiv.org/pdf/2505.19175v1){: .btn .btn-green } |

**Abstract**: The field of computer graphics was revolutionized by models such as Neural
Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant
representation for photogrammetry. In this paper, we argue for a triangle
comeback. We develop a differentiable renderer that directly optimizes
triangles via end-to-end gradients. We achieve this by rendering each triangle
as differentiable splats, combining the efficiency of triangles with the
adaptive density of representations based on independent primitives. Compared
to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher
visual fidelity, faster convergence, and increased rendering throughput. On the
Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric
primitives in visual fidelity and achieves higher perceptual quality than the
state-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible
with standard graphics stacks and GPU hardware, and highly efficient: for the
\textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using
an off-the-shelf mesh renderer. These results highlight the efficiency and
effectiveness of triangle-based representations for high-quality novel view
synthesis. Triangles bring us closer to mesh-based optimization by combining
classical computer graphics with modern differentiable rendering frameworks.
The project page is https://trianglesplatting.github.io/

Comments:
- 18 pages, 13 figures, 10 tables

---

## Veta-GS: View-dependent deformable 3D Gaussian Splatting for thermal  infrared Novel-view Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-25 | Myeongseok Nam, Wongi Park, Minsol Kim, Hyejin Hur, Soomok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2505.19138v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3D-GS) based on Thermal Infrared (TIR)
imaging has gained attention in novel-view synthesis, showing real-time
rendering. However, novel-view synthesis with thermal infrared images suffers
from transmission effects, emissivity, and low resolution, leading to floaters
and blur effects in rendered images. To address these problems, we introduce
Veta-GS, which leverages a view-dependent deformation field and a Thermal
Feature Extractor (TFE) to precisely capture subtle thermal variations and
maintain robustness. Specifically, we design view-dependent deformation field
that leverages camera position and viewing direction, which capture thermal
variations. Furthermore, we introduce the Thermal Feature Extractor (TFE) and
MonoSSIM loss, which consider appearance, edge, and frequency to maintain
robustness. Extensive experiments on the TI-NSD benchmark show that our method
achieves better performance over existing methods.



---

## Efficient Differentiable Hardware Rasterization for 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-24 | Yitian Yuan, Qianyue He | cs.GR | [PDF](http://arxiv.org/pdf/2505.18764v1){: .btn .btn-green } |

**Abstract**: Recent works demonstrate the advantages of hardware rasterization for 3D
Gaussian Splatting (3DGS) in forward-pass rendering through fast GPU-optimized
graphics and fixed memory footprint. However, extending these benefits to
backward-pass gradient computation remains challenging due to graphics pipeline
constraints. We present a differentiable hardware rasterizer for 3DGS that
overcomes the memory and performance limitations of tile-based software
rasterization. Our solution employs programmable blending for per-pixel
gradient computation combined with a hybrid gradient reduction strategy
(quad-level + subgroup) in fragment shaders, achieving over 10x faster backward
rasterization versus naive atomic operations and 3x speedup over the canonical
tile-based rasterizer. Systematic evaluation reveals 16-bit render targets
(float16 and unorm16) as the optimal accuracy-efficiency trade-off, achieving
higher gradient accuracy among mixed-precision rendering formats with execution
speeds second only to unorm8, while float32 texture incurs severe forward pass
performance degradation due to suboptimal hardware optimizations. Our method
with float16 formats demonstrates 3.07x acceleration in full pipeline execution
(forward + backward passes) on RTX4080 GPUs with the MipNeRF dataset,
outperforming the baseline tile-based renderer while preserving hardware
rasterization's memory efficiency advantages -- incurring merely 2.67% of the
memory overhead required for splat sorting operations. This work presents a
unified differentiable hardware rasterization method that simultaneously
optimizes runtime and memory usage for 3DGS, making it particularly suitable
for resource-constrained devices with limited memory capacity.

Comments:
- 8 pages,2 figures

---

## SuperGS: Consistent and Detailed 3D Super-Resolution Scene  Reconstruction via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-24 | Shiyun Xie, Zhiru Wang, Yinghao Zhu, Xu Wang, Chengwei Pan, Xiwang Dong | cs.CV | [PDF](http://arxiv.org/pdf/2505.18649v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has excelled in novel view synthesis
(NVS) with its real-time rendering capabilities and superior quality. However,
it encounters challenges for high-resolution novel view synthesis (HRNVS) due
to the coarse nature of primitives derived from low-resolution input views. To
address this issue, we propose SuperGS, an expansion of Scaffold-GS designed
with a two-stage coarse-to-fine training framework. In the low-resolution
stage, we introduce a latent feature field to represent the low-resolution
scene, which serves as both the initialization and foundational information for
super-resolution optimization. In the high-resolution stage, we propose a
multi-view consistent densification strategy that backprojects high-resolution
depth maps based on error maps and employs a multi-view voting mechanism,
mitigating ambiguities caused by multi-view inconsistencies in the pseudo
labels provided by 2D prior models while avoiding Gaussian redundancy.
Furthermore, we model uncertainty through variational feature learning and use
it to guide further scene representation refinement and adjust the supervisory
effect of pseudo-labels, ensuring consistent and detailed scene reconstruction.
Extensive experiments demonstrate that SuperGS outperforms state-of-the-art
HRNVS methods on both forward-facing and 360-degree datasets.



---

## Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose  and Appearance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-23 | Jack Goffinet, Youngjo Min, Carlo Tomasi, David E. Carlson | cs.CV | [PDF](http://arxiv.org/pdf/2505.18342v1){: .btn .btn-green } |

**Abstract**: Accurate and scalable quantification of animal pose and appearance is crucial
for studying behavior. Current 3D pose estimation techniques, such as keypoint-
and mesh-based techniques, often face challenges including limited
representational detail, labor-intensive annotation requirements, and expensive
per-frame optimization. These limitations hinder the study of subtle movements
and can make large-scale analyses impractical. We propose Pose Splatter, a
novel framework leveraging shape carving and 3D Gaussian splatting to model the
complete pose and appearance of laboratory animals without prior knowledge of
animal geometry, per-frame optimization, or manual annotations. We also propose
a novel rotation-invariant visual embedding technique for encoding pose and
appearance, designed to be a plug-in replacement for 3D keypoint data in
downstream behavioral analyses. Experiments on datasets of mice, rats, and
zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,
Pose Splatter represents subtle variations in pose, provides better
low-dimensional pose embeddings over state-of-the-art as evaluated by humans,
and generalizes to unseen data. By eliminating annotation and per-frame
optimization bottlenecks, Pose Splatter enables analysis of large-scale,
longitudinal behavior needed to map genotype, neural activity, and
micro-behavior at unprecedented resolution.

Comments:
- 19 pages, 13 figures

---

## CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human  Head Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-23 | Florian Barthel, Wieland Morgenstern, Paul Hinzer, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2505.17590v1){: .btn .btn-green } |

**Abstract**: Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high
quality synthesis of human heads. However, existing methods stabilize training
and enhance rendering quality from steep viewpoints by conditioning the random
latent vector on the current camera position. This compromises 3D consistency,
as we observe significant identity changes when re-synthesizing the 3D head
with each camera shift. Conversely, fixing the camera to a single viewpoint
yields high-quality renderings for that perspective but results in poor
performance for novel views. Removing view-conditioning typically destabilizes
GAN training, often causing the training to collapse. In response to these
challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework
that enables stable training and high-quality 3D-consistent synthesis of human
heads without relying on view-conditioning. To ensure training stability, we
introduce a multi-view regularization technique that enhances generator
convergence with minimal computational overhead. Additionally, we adapt the
conditional loss used in existing 3D Gaussian splatting GANs and propose a
generator architecture designed to not only stabilize training but also
facilitate efficient rendering and straightforward scaling, enabling output
resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate
a new dataset derived from FFHQ. This dataset enables very high resolutions,
focuses on larger portions of the human head, reduces view-dependent artifacts
for improved 3D consistency, and excludes images where subjects are obscured by
hands or other objects. As a result, our approach achieves very high rendering
quality, supported by competitive FID scores, while ensuring consistent 3D
scene generation. Check our our project page here:
https://fraunhoferhhi.github.io/cgs-gan/

Comments:
- Main paper 12 pages, supplementary materials 8 pages

---

## From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection  via Gaussian Splatting and Language-Guided Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-23 | Mahmoud Chick Zaouali, Todd Charter, Homayoun Najjaran | cs.GR | [PDF](http://arxiv.org/pdf/2505.17402v1){: .btn .btn-green } |

**Abstract**: High-fidelity 3D reconstruction is critical for aerial inspection tasks such
as infrastructure monitoring, structural assessment, and environmental
surveying. While traditional photogrammetry techniques enable geometric
modeling, they lack semantic interpretability, limiting their effectiveness for
automated inspection workflows. Recent advances in neural rendering and 3D
Gaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but
similarly lack scene-level understanding.
  In this work, we present a UAV-based pipeline that extends Feature-3DGS for
language-guided 3D segmentation. We leverage LSeg-based feature fields with
CLIP embeddings to generate heatmaps in response to language prompts. These are
thresholded to produce rough segmentations, and the highest-scoring point is
then used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view
renderings. Our results highlight the strengths and limitations of various
feature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful
structure in large-scale outdoor environments. We demonstrate that this hybrid
approach enables flexible, language-driven interaction with photorealistic 3D
reconstructions, opening new possibilities for semantic aerial inspection and
scene understanding.



---

## SplatCo: Structure-View Collaborative Gaussian Splatting for  Detail-Preserving Rendering of Large-Scale Unbounded Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-23 | Haihong Xiao, Jianan Zou, Yuxin Zhou, Ying He, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2505.17951v1){: .btn .btn-green } |

**Abstract**: We present SplatCo, a structure-view collaborative Gaussian splatting
framework for high-fidelity rendering of complex outdoor environments. SplatCo
builds upon two novel components: (1) a cross-structure collaboration module
that combines global tri-plane representations, which capture coarse scene
layouts, with local context grid features that represent fine surface details.
This fusion is achieved through a novel hierarchical compensation strategy,
ensuring both global consistency and local detail preservation; and (2) a
cross-view assisted training strategy that enhances multi-view consistency by
synchronizing gradient updates across viewpoints, applying visibility-aware
densification, and pruning overfitted or inaccurate Gaussians based on
structural consistency. Through joint optimization of structural representation
and multi-view coherence, SplatCo effectively reconstructs fine-grained
geometric structures and complex textures in large-scale scenes. Comprehensive
evaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity,
Tanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo
consistently achieves higher reconstruction quality than state-of-the-art
methods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These
results establish a new benchmark for high-fidelity rendering of large-scale
unbounded scenes. Code and additional information are available at
https://github.com/SCUT-BIP-Lab/SplatCo.



---

## CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-23 | Karly Hou, Wanhua Li, Hanspeter Pfister | cs.CV | [PDF](http://arxiv.org/pdf/2505.18306v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian Splatting methods have emerged as a desirable substitute
for prior Radiance Field methods for novel-view synthesis of scenes captured
with multi-view images or videos. In this work, we propose a novel extension to
4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual
learning, we hierarchically decompose the dynamic scene into a
"video-segment-frame" structure, with segments dynamically adjusted by optical
flow. Then, instead of directly predicting the time-dependent signals, we model
the signal as the sum of video-constant values, segment-constant values, and
frame-specific residuals, as inspired by the success of residual learning. This
approach allows more flexible models that adapt to highly variable scenes. We
demonstrate state-of-the-art visual quality and real-time rendering on several
established datasets, with the greatest improvements on complex scenes with
large movements, occlusions, and fine details, where current methods degrade
most.

Comments:
- Accepted to 4D Vision Workshop @ CVPR 2025

---

## UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot  Ground Vehicle Repeat

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-22 | Desiree Fisker, Alexander Krawciw, Sven Lilge, Melissa Greeff, Timothy D. Barfoot | cs.RO | [PDF](http://arxiv.org/pdf/2505.16912v1){: .btn .btn-green } |

**Abstract**: This paper presents Virtual Teach and Repeat (VirT&R): an extension of the
Teach and Repeat (T&R) framework that enables GPS-denied, zero-shot autonomous
ground vehicle navigation in untraversed environments. VirT&R leverages aerial
imagery captured for a target environment to train a Neural Radiance Field
(NeRF) model so that dense point clouds and photo-textured meshes can be
extracted. The NeRF mesh is used to create a high-fidelity simulation of the
environment for piloting an unmanned ground vehicle (UGV) to virtually define a
desired path. The mission can then be executed in the actual target environment
by using NeRF-derived point cloud submaps associated along the path and an
existing LiDAR Teach and Repeat (LT&R) framework. We benchmark the
repeatability of VirT&R on over 12 km of autonomous driving data using physical
markings that allow a sim-to-real lateral path-tracking error to be obtained
and compared with LT&R. VirT&R achieved measured root mean squared errors
(RMSE) of 19.5 cm and 18.4 cm in two different environments, which are slightly
less than one tire width (24 cm) on the robot used for testing, and respective
maximum errors were 39.4 cm and 47.6 cm. This was done using only the
NeRF-derived teach map, demonstrating that VirT&R has similar closed-loop
path-tracking performance to LT&R but does not require a human to manually
teach the path to the UGV in the actual environment.

Comments:
- 8 pages, 7 figures, submitted to IROS 2025

---

## Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-22 | Jiacong Chen, Qingyu Mao, Youneng Bao, Xiandong Meng, Fanyang Meng, Ronggang Wang, Yongsheng Liang | cs.CV | [PDF](http://arxiv.org/pdf/2505.16533v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient
paradigm for online free-viewpoint video (FVV) reconstruction, offering viewers
rapid responsiveness and immersive experiences. However, existing online
methods face challenge in prohibitive storage requirements primarily due to
point-wise modeling that fails to exploit the motion properties. To address
this limitation, we propose a novel Compact Gaussian Streaming (ComGS)
framework, leveraging the locality and consistency of motion in dynamic scene,
that models object-consistent Gaussian point motion through keypoint-driven
motion representation. By transmitting only the keypoint attributes, this
framework provides a more storage-efficient solution. Specifically, we first
identify a sparse set of motion-sensitive keypoints localized within motion
regions using a viewspace gradient difference strategy. Equipped with these
keypoints, we propose an adaptive motion-driven mechanism that predicts a
spatial influence field for propagating keypoint motion to neighboring Gaussian
points with similar motion. Moreover, ComGS adopts an error-aware correction
strategy for key frame reconstruction that selectively refines erroneous
regions and mitigates error accumulation without unnecessary overhead. Overall,
ComGS achieves a remarkable storage reduction of over 159 X compared to
3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining
competitive visual fidelity and rendering speed. Our code will be released.

Comments:
- 17 pages, 9 figures

---

## SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane  Deformation and Latent Diffusion


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-22 | Asrar Alruwayqi | cs.CV | [PDF](http://arxiv.org/pdf/2505.16535v1){: .btn .btn-green } |

**Abstract**: We present a novel framework for dynamic 3D scene reconstruction that
integrates three key components: an explicit tri-plane deformation field, a
view-conditioned canonical radiance field with spherical harmonics (SH)
attention, and a temporally-aware latent diffusion prior. Our method encodes 4D
scenes using three orthogonal 2D feature planes that evolve over time, enabling
efficient and compact spatiotemporal representation. These features are
explicitly warped into a canonical space via a deformation offset field,
eliminating the need for MLP-based motion modeling.
  In canonical space, we replace traditional MLP decoders with a structured
SH-based rendering head that synthesizes view-dependent color via attention
over learned frequency bands improving both interpretability and rendering
efficiency. To further enhance fidelity and temporal consistency, we introduce
a transformer-guided latent diffusion module that refines the tri-plane and
deformation features in a compressed latent space. This generative module
denoises scene representations under ambiguous or out-of-distribution (OOD)
motion, improving generalization.
  Our model is trained in two stages: the diffusion module is first pre-trained
independently, and then fine-tuned jointly with the full pipeline using a
combination of image reconstruction, diffusion denoising, and temporal
consistency losses. We demonstrate state-of-the-art results on synthetic
benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian
Splatting in visual quality, temporal coherence, and robustness to sparse-view
dynamic inputs.



---

## Render-FM: A Foundation Model for Real-time Photorealistic Volumetric  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-22 | Zhongpai Gao, Meng Zheng, Benjamin Planche, Anwesa Choudhuri, Terrence Chen, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2505.17338v1){: .btn .btn-green } |

**Abstract**: Volumetric rendering of Computed Tomography (CT) scans is crucial for
visualizing complex 3D anatomical structures in medical imaging. Current
high-fidelity approaches, especially neural rendering techniques, require
time-consuming per-scene optimization, limiting clinical applicability due to
computational demands and poor generalizability. We propose Render-FM, a novel
foundation model for direct, real-time volumetric rendering of CT scans.
Render-FM employs an encoder-decoder architecture that directly regresses 6D
Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan
optimization through large-scale pre-training on diverse medical data. By
integrating robust feature extraction with the expressive power of 6DGS, our
approach efficiently generates high-quality, real-time interactive 3D
visualizations across diverse clinical CT data. Experiments demonstrate that
Render-FM achieves visual fidelity comparable or superior to specialized
per-scan methods while drastically reducing preparation time from nearly an
hour to seconds for a single inference step. This advancement enables seamless
integration into real-time surgical planning and diagnostic workflows. The
project page is: https://gaozhongpai.github.io/renderfm/.



---

## X-GRM: Large Gaussian Reconstruction Model for Sparse-view X-rays to  Computed Tomography

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Yifan Liu, Wuyang Li, Weihao Yu, Chenxin Li, Alexandre Alahi, Max Meng, Yixuan Yuan | eess.IV | [PDF](http://arxiv.org/pdf/2505.15235v2){: .btn .btn-green } |

**Abstract**: Computed Tomography serves as an indispensable tool in clinical workflows,
providing non-invasive visualization of internal anatomical structures.
Existing CT reconstruction works are limited to small-capacity model
architecture and inflexible volume representation. In this work, we present
X-GRM (X-ray Gaussian Reconstruction Model), a large feedforward model for
reconstructing 3D CT volumes from sparse-view 2D X-ray projections. X-GRM
employs a scalable transformer-based architecture to encode sparse-view X-ray
inputs, where tokens from different views are integrated efficiently. Then,
these tokens are decoded into a novel volume representation, named Voxel-based
Gaussian Splatting (VoxGS), which enables efficient CT volume extraction and
differentiable X-ray rendering. This combination of a high-capacity model and
flexible volume representation, empowers our model to produce high-quality
reconstructions from various testing inputs, including in-domain and out-domain
X-ray projections. Our codes are available at:
https://github.com/CUHK-AIM-Group/X-GRM.



---

## GT^2-GS: Geometry-aware Texture Transfer for Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Wenjie Liu, Zhongliang Liu, Junwei Shu, Changbo Wang, Yang Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.15208v1){: .btn .btn-green } |

**Abstract**: Transferring 2D textures to 3D modalities is of great significance for
improving the efficiency of multimedia content creation. Existing approaches
have rarely focused on transferring image textures onto 3D representations. 3D
style transfer methods are capable of transferring abstract artistic styles to
3D scenes. However, these methods often overlook the geometric information of
the scene, which makes it challenging to achieve high-quality 3D texture
transfer results. In this paper, we present GT^2-GS, a geometry-aware texture
transfer framework for gaussian splitting. From the perspective of matching
texture features with geometric information in rendered views, we identify the
issue of insufficient texture features and propose a geometry-aware texture
augmentation module to expand the texture feature set. Moreover, a
geometry-consistent texture loss is proposed to optimize texture features into
the scene representation. This loss function incorporates both camera pose and
3D geometric information of the scene, enabling controllable texture-oriented
appearance editing. Finally, a geometry preservation strategy is introduced. By
alternating between the texture transfer and geometry correction stages over
multiple iterations, this strategy achieves a balance between learning texture
features and preserving geometric integrity. Extensive experiments demonstrate
the effectiveness and controllability of our method. Through geometric
awareness, our approach achieves texture transfer results that better align
with human visual perception. Our homepage is available at
https://vpx-ecnu.github.io/GT2-GS-website.

Comments:
- 15 pages, 16 figures

---

## R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in  Unconstrained Image Collections

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, Xiangde Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.15294v1){: .btn .btn-green } |

**Abstract**: We propose R3GS, a robust reconstruction and relocalization framework
tailored for unconstrained datasets. Our method uses a hybrid representation
during training. Each anchor combines a global feature from a convolutional
neural network (CNN) with a local feature encoded by the multiresolution hash
grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict
the attributes of each Gaussians, including color, opacity, and covariance. To
mitigate the adverse effects of transient objects on the reconstruction
process, we ffne-tune a lightweight human detection network. Once ffne-tuned,
this network generates a visibility map that efffciently generalizes to other
transient objects (such as posters, banners, and cars) with minimal need for
further adaptation. Additionally, to address the challenges posed by sky
regions in outdoor scenes, we propose an effective sky-handling technique that
incorporates a depth prior as a constraint. This allows the inffnitely distant
sky to be represented on the surface of a large-radius sky sphere,
signiffcantly reducing ffoaters caused by errors in sky reconstruction.
Furthermore, we introduce a novel relocalization method that remains robust to
changes in lighting conditions while estimating the camera pose of a given
image within the reconstructed 3DGS scene. As a result, R3GS significantly
enhances rendering ffdelity, improves both training and rendering efffciency,
and reduces storage requirements. Our method achieves state-of-the-art
performance compared to baseline methods on in-the-wild datasets. The code will
be made open-source following the acceptance of the paper.

Comments:
- 7 pages, 4 figures

---

## RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai | cs.CV | [PDF](http://arxiv.org/pdf/2505.15737v1){: .btn .btn-green } |

**Abstract**: Reconstructing high-fidelity underwater scenes remains a challenging task due
to light absorption, scattering, and limited visibility inherent in aquatic
environments. This paper presents an enhanced Gaussian Splatting-based
framework that improves both the visual quality and geometric accuracy of deep
underwater rendering. We propose decoupled learning for RGB channels, guided by
the physics of underwater attenuation, to enable more accurate colour
restoration. To address sparse-view limitations and improve view consistency,
we introduce a frame interpolation strategy with a novel adaptive weighting
scheme. Additionally, we introduce a new loss function aimed at reducing noise
while preserving edges, which is essential for deep-sea content. We also
release a newly collected dataset, Submerged3D, captured specifically in
deep-sea environments. Experimental results demonstrate that our framework
consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB,
delivering superior perceptual quality and robustness, and offering promising
directions for marine robotics and underwater visual analytics.

Comments:
- 10 pages, 3 figures. Submitted to BMVC 2025

---

## GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Yuchen Li, Chaoran Feng, Zhenyu Tang, Kaiyuan Deng, Wangbo Yu, Yonghong Tian, Li Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2505.15287v1){: .btn .btn-green } |

**Abstract**: We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic
event dataset for high-fidelity event vision tasks, captured from real-world
sparse multi-view RGB images. Existing event datasets are often synthesized
from dense RGB videos, which typically lack viewpoint diversity and geometric
consistency, or depend on expensive, difficult-to-scale hardware setups. GS2E
overcomes these limitations by first reconstructing photorealistic static
scenes using 3D Gaussian Splatting, and subsequently employing a novel,
physically-informed event simulation pipeline. This pipeline generally
integrates adaptive trajectory interpolation with physically-consistent event
contrast threshold modeling. Such an approach yields temporally dense and
geometrically consistent event streams under diverse motion and lighting
conditions, while ensuring strong alignment with underlying scene structures.
Experimental results on event-based 3D reconstruction demonstrate GS2E's
superior generalization capabilities and its practical value as a benchmark for
advancing event vision research.

Comments:
- 21 pages, 7 figures. More details at
  http://intothemild.github.io/GS2E.github.io

---

## MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth  Foundation Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Yifan Liu, Keyu Fan, Weihao Yu, Chenxin Li, Hao Lu, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2505.15185v1){: .btn .btn-green } |

**Abstract**: Recent advances in generalizable 3D Gaussian Splatting have demonstrated
promising results in real-time high-fidelity rendering without per-scene
optimization, yet existing approaches still struggle to handle unfamiliar
visual content during inference on novel scenes due to limited
generalizability. To address this challenge, we introduce MonoSplat, a novel
framework that leverages rich visual priors from pre-trained monocular depth
foundation models for robust Gaussian reconstruction. Our approach consists of
two key components: a Mono-Multi Feature Adapter that transforms monocular
features into multi-view representations, coupled with an Integrated Gaussian
Prediction module that effectively fuses both feature types for precise
Gaussian generation. Through the Adapter's lightweight attention mechanism,
features are seamlessly aligned and aggregated across views while preserving
valuable monocular priors, enabling the Prediction module to generate Gaussian
primitives with accurate geometry and appearance. Through extensive experiments
on diverse real-world datasets, we convincingly demonstrate that MonoSplat
achieves superior reconstruction quality and generalization capability compared
to existing methods while maintaining computational efficiency with minimal
trainable parameters. Codes are available at
https://github.com/CUHK-AIM-Group/MonoSplat.



---

## PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-21 | Zane K J Hartley, Lewis A G Stuart, Andrew P French, Michael P Pound | cs.CV | [PDF](http://arxiv.org/pdf/2505.15528v1){: .btn .btn-green } |

**Abstract**: Recent years have seen substantial improvements in the ability to generate
synthetic 3D objects using AI. However, generating complex 3D objects, such as
plants, remains a considerable challenge. Current generative 3D models struggle
with plant generation compared to general objects, limiting their usability in
plant analysis tools, which require fine detail and accurate geometry. We
introduce PlantDreamer, a novel approach to 3D synthetic plant generation,
which can achieve greater levels of realism for complex plant geometry and
textures than available text-to-3D models. To achieve this, our new generation
pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an
adaptable Gaussian culling algorithm, which directly improve textural realism
and geometric integrity of generated 3D plant models. Additionally,
PlantDreamer enables both purely synthetic plant generation, by leveraging
L-System-generated meshes, and the enhancement of real-world plant point clouds
by converting them into 3D Gaussian Splats. We evaluate our approach by
comparing its outputs with state-of-the-art text-to-3D models, demonstrating
that PlantDreamer outperforms existing methods in producing high-fidelity
synthetic plants. Our results indicate that our approach not only advances
synthetic plant generation, but also facilitates the upgrading of legacy point
cloud datasets, making it a valuable tool for 3D phenotyping applications.

Comments:
- 13 pages, 5 figures, 4 tables

---

## Personalize Your Gaussian: Consistent 3D Scene Personalization from a  Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-20 | Yuxuan Wang, Xuanyu Yi, Qingshan Xu, Yuan Zhou, Long Chen, Hanwang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2505.14537v1){: .btn .btn-green } |

**Abstract**: Personalizing 3D scenes from a single reference image enables intuitive
user-guided editing, which requires achieving both multi-view consistency
across perspectives and referential consistency with the input image. However,
these goals are particularly challenging due to the viewpoint bias caused by
the limited perspective provided in a single image. Lacking the mechanisms to
effectively expand reference information beyond the original view, existing
methods of image-conditioned 3DGS personalization often suffer from this
viewpoint bias and struggle to produce consistent results. Therefore, in this
paper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),
a framework that progressively propagates the single-view reference appearance
to novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D
generation and iterative LoRA fine-tuning to extract and extend the reference
appearance, and finally produces faithful multi-view guidance images and the
personalized 3DGS outputs through a view-consistent generation process guided
by geometric cues. Extensive experiments on real-world scenes show that our
CP-GS effectively mitigates the viewpoint bias, achieving high-quality
personalization that significantly outperforms existing methods. The code will
be released at https://github.com/Yuxuan-W/CP-GS.

Comments:
- 9 pages

---

## Scan, Materialize, Simulate: A Generalizable Framework for Physically  Grounded Robot Planning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-20 | Amine Elhafsi, Daniel Morton, Marco Pavone | cs.RO | [PDF](http://arxiv.org/pdf/2505.14938v1){: .btn .btn-green } |

**Abstract**: Autonomous robots must reason about the physical consequences of their
actions to operate effectively in unstructured, real-world environments. We
present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D
Gaussian Splatting for accurate scene reconstruction, visual foundation models
for semantic segmentation, vision-language models for material property
inference, and physics simulation for reliable prediction of action outcomes.
By integrating these components, SMS enables generalizable physical reasoning
and object-centric planning without the need to re-learn foundational physical
dynamics. We empirically validate SMS in a billiards-inspired manipulation task
and a challenging quadrotor landing scenario, demonstrating robust performance
on both simulated domain transfer and real-world experiments. Our results
highlight the potential of bridging differentiable rendering for scene
reconstruction, foundation models for semantic understanding, and physics-based
simulation to achieve physically grounded robot planning across diverse
settings.



---

## MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-20 | Zhenyu Bao, Qing Li, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.13839v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention in streamable
dynamic novel view synthesis (DNVS) for its photorealistic rendering capability
and computational efficiency. Despite much progress in improving rendering
quality and optimization strategies, 3DGS-based streamable dynamic scene
reconstruction still suffers from flickering artifacts and storage
inefficiency, and struggles to model the emerging objects. To tackle this, we
introduce MGStream which employs the motion-related 3D Gaussians (3DGs) to
reconstruct the dynamic and the vanilla 3DGs for the static. The motion-related
3DGs are implemented according to the motion mask and the clustering-based
convex hull algorithm. The rigid deformation is applied to the motion-related
3DGs for modeling the dynamic, and the attention-based optimization on the
motion-related 3DGs enables the reconstruction of the emerging objects. As the
deformation and optimization are only conducted on the motion-related 3DGs,
MGStream avoids flickering artifacts and improves the storage efficiency.
Extensive experiments on real-world datasets N3DV and MeetRoom demonstrate that
MGStream surpasses existing streaming 3DGS-based approaches in terms of
rendering quality, training/storage efficiency and temporal consistency. Our
code is available at: https://github.com/pcl3dv/MGStream.



---

## Recollection from Pensieve: Novel View Synthesis via Learning from  Uncalibrated Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Ruoyu Wang, Yi Ma, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2505.13440v1){: .btn .btn-green } |

**Abstract**: Currently almost all state-of-the-art novel view synthesis and reconstruction
models rely on calibrated cameras or additional geometric priors for training.
These prerequisites significantly limit their applicability to massive
uncalibrated data. To alleviate this requirement and unlock the potential for
self-supervised training on large-scale uncalibrated videos, we propose a novel
two-stage strategy to train a view synthesis model from only raw video frames
or multi-view images, without providing camera parameters or other priors. In
the first stage, we learn to reconstruct the scene implicitly in a latent space
without relying on any explicit 3D representation. Specifically, we predict
per-frame latent camera and scene context features, and employ a view synthesis
model as a proxy for explicit rendering. This pretraining stage substantially
reduces the optimization complexity and encourages the network to learn the
underlying 3D consistency in a self-supervised manner. The learned latent
camera and implicit scene representation have a large gap compared with the
real 3D world. To reduce this gap, we introduce the second stage training by
explicitly predicting 3D Gaussian primitives. We additionally apply explicit
Gaussian Splatting rendering loss and depth projection loss to align the
learned latent representations with physically grounded 3D geometry. In this
way, Stage 1 provides a strong initialization and Stage 2 enforces 3D
consistency - the two stages are complementary and mutually beneficial.
Extensive experiments demonstrate the effectiveness of our approach, achieving
high-quality novel view synthesis and accurate camera pose estimation, compared
to methods that employ supervision with calibration, pose, or depth
information. The code is available at https://github.com/Dwawayu/Pensieve.

Comments:
- 13 pages, 4 figures

---

## 3D Gaussian Adaptive Reconstruction for Fourier Light-Field Microscopy

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Chenyu Xu, Zhouyu Jin, Chengkang Shen, Hao Zhu, Zhan Ma, Bo Xiong, You Zhou, Xun Cao, Ning Gu | eess.IV | [PDF](http://arxiv.org/pdf/2505.12875v1){: .btn .btn-green } |

**Abstract**: Compared to light-field microscopy (LFM), which enables high-speed volumetric
imaging but suffers from non-uniform spatial sampling, Fourier light-field
microscopy (FLFM) introduces sub-aperture division at the pupil plane, thereby
ensuring spatially invariant sampling and enhancing spatial resolution.
Conventional FLFM reconstruction methods, such as Richardson-Lucy (RL)
deconvolution, exhibit poor axial resolution and signal degradation due to the
ill-posed nature of the inverse problem. While data-driven approaches enhance
spatial resolution by leveraging high-quality paired datasets or imposing
structural priors, Neural Radiance Fields (NeRF)-based methods employ
physics-informed self-supervised learning to overcome these limitations, yet
they are hindered by substantial computational costs and memory demands.
Therefore, we propose 3D Gaussian Adaptive Tomography (3DGAT) for FLFM, a 3D
gaussian splatting based self-supervised learning framework that significantly
improves the volumetric reconstruction quality of FLFM while maintaining
computational efficiency. Experimental results indicate that our approach
achieves higher resolution and improved reconstruction accuracy, highlighting
its potential to advance FLFM imaging and broaden its applications in 3D
optical microscopy.



---

## IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping  Extraction via NeRF-SAM2 Fusion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Wentao Song, He Huang, Youqiang Sun, Fang Qu, Jiaqi Zhang, Longhui Fang, Yuwei Hao, Chenyang Peng | cs.CV | [PDF](http://arxiv.org/pdf/2505.13633v1){: .btn .btn-green } |

**Abstract**: Advanced plant phenotyping technologies play a crucial role in targeted trait
improvement and accelerating intelligent breeding. Due to the species diversity
of plants, existing methods heavily rely on large-scale high-precision manually
annotated data. For self-occluded objects at the grain level, unsupervised
methods often prove ineffective. This study proposes IPENS, an interactive
unsupervised multi-target point cloud extraction method. The method utilizes
radiance field information to lift 2D masks, which are segmented by SAM2
(Segment Anything Model 2), into 3D space for target point cloud extraction. A
multi-target collaborative optimization strategy is designed to effectively
resolve the single-interaction multi-target segmentation challenge.
Experimental validation demonstrates that IPENS achieves a grain-level
segmentation accuracy (mIoU) of 63.72% on a rice dataset, with strong
phenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697
(RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf length
and width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On a
wheat dataset,IPENS further improves segmentation accuracy to 89.68% (mIoU),
with equally outstanding phenotypic estimation performance: spike volume
prediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00
(RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92
(RMSE = 0.23 and 0.15). This method provides a non-invasive, high-quality
phenotyping extraction solution for rice and wheat. Without requiring annotated
data, it rapidly extracts grain-level point clouds within 3 minutes through
simple single-round interactions on images for multiple targets, demonstrating
significant potential to accelerate intelligent breeding efficiency.



---

## TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D  Semantic Occupancy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Luyao Lei, Shuo Xu, Yifan Bai, Xing Wei | cs.CV | [PDF](http://arxiv.org/pdf/2505.12693v1){: .btn .btn-green } |

**Abstract**: The performance of multi-modal 3D occupancy prediction is limited by
ineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion
strategies and surface detail loss caused by sparse, noisy annotations. The
mismatch stems from the heterogeneous scale and distribution of point cloud and
image features, leading to biased matching under fixed neighborhood fusion. To
address this, we propose a target-scale adaptive, bidirectional symmetric
retrieval mechanism. It expands the neighborhood for large targets to enhance
context awareness and shrinks it for small ones to improve efficiency and
suppress noise, enabling accurate cross-modal feature alignment. This mechanism
explicitly establishes spatial correspondences and improves fusion accuracy.
For surface detail loss, sparse labels provide limited supervision, resulting
in poor predictions for small objects. We introduce an improved volume
rendering pipeline based on 3D Gaussian Splatting, which takes fused features
as input to render images, applies photometric consistency supervision, and
jointly optimizes 2D-3D consistency. This enhances surface detail
reconstruction while suppressing noise propagation. In summary, we propose
TACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy
prediction, enhanced by volume rendering supervision. Experiments on the
nuScenes and SemanticKITTI benchmarks validate its effectiveness.



---

## Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-19 | Seungjun Oh, Younggeun Lee, Hyejin Jeon, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2505.13215v1){: .btn .btn-green } |

**Abstract**: Recent advancements in dynamic 3D scene reconstruction have shown promising
results, enabling high-fidelity 3D novel view synthesis with improved temporal
consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an
appealing approach due to its ability to model high-fidelity spatial and
temporal variations. However, existing methods suffer from substantial
computational and memory overhead due to the redundant allocation of 4D
Gaussians to static regions, which can also degrade image quality. In this
work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework
that adaptively represents static regions with 3D Gaussians while reserving 4D
Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian
representation and iteratively converts temporally invariant Gaussians into 3D,
significantly reducing the number of parameters and improving computational
efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,
capturing complex motions with high fidelity. Our approach achieves
significantly faster training times compared to baseline 4D Gaussian Splatting
methods while maintaining or improving the visual quality.

Comments:
- https://ohsngjun.github.io/3D-4DGS/

---

## Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-18 | Calvin Galagain, Martyna Poreba, FranÃ§ois Goulette | cs.RO | [PDF](http://arxiv.org/pdf/2505.12384v1){: .btn .btn-green } |

**Abstract**: In embedded systems, robots must perceive and interpret their environment
efficiently to operate reliably in real-world conditions. Visual Semantic SLAM
(Simultaneous Localization and Mapping) enhances standard SLAM by incorporating
semantic information into the map, enabling more informed decision-making.
However, implementing such systems on resource-limited hardware involves
trade-offs between accuracy, computing efficiency, and power usage.
  This paper provides a comparative review of recent Semantic Visual SLAM
methods with a focus on their applicability to embedded platforms. We analyze
three main types of architectures - Geometric SLAM, Neural Radiance Fields
(NeRF), and 3D Gaussian Splatting - and evaluate their performance on
constrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their
accuracy, segmentation quality, memory usage, and energy consumption.
  Our results show that methods based on NeRF and Gaussian Splatting achieve
high semantic detail but demand substantial computing resources, limiting their
use on embedded devices. In contrast, Semantic Geometric SLAM offers a more
practical balance between computational cost and accuracy. The review
highlights a need for SLAM algorithms that are better adapted to embedded
environments, and it discusses key directions for improving their efficiency
through algorithm-hardware co-design.



---

## Gaussian Splatting as a Unified Representation for Autonomy in  Unstructured Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-17 | Dexter Ong, Yuezhan Tao, Varun Murali, Igor Spasojevic, Vijay Kumar, Pratik Chaudhari | cs.RO | [PDF](http://arxiv.org/pdf/2505.11794v1){: .btn .btn-green } |

**Abstract**: In this work, we argue that Gaussian splatting is a suitable unified
representation for autonomous robot navigation in large-scale unstructured
outdoor environments. Such environments require representations that can
capture complex structures while remaining computationally tractable for
real-time navigation. We demonstrate that the dense geometric and photometric
information provided by a Gaussian splatting representation is useful for
navigation in unstructured environments. Additionally, semantic information can
be embedded in the Gaussian map to enable large-scale task-driven navigation.
From the lessons learned through our experiments, we highlight several
challenges and opportunities arising from the use of such a representation for
robot autonomy.



---

## MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-17 | Hongyi Zhou, Xiaogang Wang, Yulan Guo, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.11868v1){: .btn .btn-green } |

**Abstract**: Accurately analyzing the motion parts and their motion attributes in dynamic
environments is crucial for advancing key areas such as embodied intelligence.
Addressing the limitations of existing methods that rely on dense multi-view
images or detailed part-level annotations, we propose an innovative framework
that can analyze 3D mobility from monocular videos in a zero-shot manner. This
framework can precisely parse motion parts and motion attributes only using a
monocular video, completely eliminating the need for annotated training data.
Specifically, our method first constructs the scene geometry and roughly
analyzes the motion parts and their initial motion attributes combining depth
estimation, optical flow analysis and point cloud registration method, then
employs 2D Gaussian splatting for scene representation. Building on this, we
introduce an end-to-end dynamic scene optimization algorithm specifically
designed for articulated objects, refining the initial analysis results to
ensure the system can handle 'rotation', 'translation', and even complex
movements ('rotation+translation'), demonstrating high flexibility and
versatility. To validate the robustness and wide applicability of our method,
we created a comprehensive dataset comprising both simulated and real-world
scenarios. Experimental results show that our framework can effectively analyze
articulated object motions in an annotation-free manner, showcasing its
significant potential in future embodied intelligence applications.



---

## GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects  Based on Appearance and Geometric Complexity

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-17 | Takuya Ikeda, Sergey Zakharov, Muhammad Zubair Irshad, Istvan Balazs Opra, Shun Iwase, Dian Chen, Mark Tjersland, Robert Lee, Alexandre Dilly, Rares Ambrus, Koichi Nishiwaki | cs.CV | [PDF](http://arxiv.org/pdf/2505.11905v1){: .btn .btn-green } |

**Abstract**: We present a novel method for 6-DoF object tracking and high-quality 3D
reconstruction from monocular RGBD video. Existing methods, while achieving
impressive results, often struggle with complex objects, particularly those
exhibiting symmetry, intricate geometry or complex appearance. To bridge these
gaps, we introduce an adaptive method that combines 3D Gaussian Splatting,
hybrid geometry/appearance tracking, and key frame selection to achieve robust
tracking and accurate reconstructions across a diverse range of objects.
Additionally, we present a benchmark covering these challenging object classes,
providing high-quality annotations for evaluating both tracking and
reconstruction performance. Our approach demonstrates strong capabilities in
recovering high-fidelity object meshes, setting a new standard for
single-sensor 3D reconstruction in open-world environments.

Comments:
- main contains 10 pages, 9 figures. And supplementary material
  contains 10 pages, 27 figures

---

## Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Abhishek Kashyap, Henrik Andreasson, Todor Stoyanov | cs.RO | [PDF](http://arxiv.org/pdf/2505.11467v1){: .btn .btn-green } |

**Abstract**: Vision based robot manipulation uses cameras to capture one or more images of
a scene containing the objects to be manipulated. Taking multiple images can
help if any object is occluded from one viewpoint but more visible from another
viewpoint. However, the camera has to be moved to a sequence of suitable
positions for capturing multiple images, which requires time and may not always
be possible, due to reachability constraints. So while additional images can
produce more accurate grasp poses due to the extra information available, the
time-cost goes up with the number of additional views sampled. Scene
representations like Gaussian Splatting are capable of rendering accurate
photorealistic virtual images from user-specified novel viewpoints. In this
work, we show initial results which indicate that novel view synthesis can
provide additional context in generating grasp poses. Our experiments on the
Graspnet-1billion dataset show that novel views contributed force-closure
grasps in addition to the force-closure grasps obtained from sparsely sampled
real views while also improving grasp coverage. In the future we hope this work
can be extended to improve grasp extraction from radiance fields constructed
with a single input image, using for example diffusion models or generalizable
radiance fields.

Comments:
- 6 pages

---

## MutualNeRF: Improve the Performance of NeRF under Limited Samples with  Mutual Information Theory

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Zifan Wang, Jingwei Li, Yitang Li, Yunze Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.11386v1){: .btn .btn-green } |

**Abstract**: This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field
(NeRF) performance under limited samples using Mutual Information Theory. While
NeRF excels in 3D scene synthesis, challenges arise with limited data and
existing methods that aim to introduce prior knowledge lack theoretical support
in a unified framework. We introduce a simple but theoretically robust concept,
Mutual Information, as a metric to uniformly measure the correlation between
images, considering both macro (semantic) and micro (pixel) levels.
  For sparse view sampling, we strategically select additional viewpoints
containing more non-overlapping scene information by minimizing mutual
information without knowing ground truth images beforehand. Our framework
employs a greedy algorithm, offering a near-optimal solution.
  For few-shot view synthesis, we maximize the mutual information between
inferred images and ground truth, expecting inferred images to gain more
relevant information from known images. This is achieved by incorporating
efficient, plug-and-play regularization terms.
  Experiments under limited samples show consistent improvement over
state-of-the-art baselines in different settings, affirming the efficacy of our
framework.



---

## GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian  Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Simeon Adebola, Shuangyu Xie, Chung Min Kim, Justin Kerr, Bart M. van Marrewijk, Mieke van Vlaardingen, Tim van Daalen, Robert van Loo, Jose Luis Susa Rincon, Eugen Solowjow, Rick van de Zedde, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2505.10923v1){: .btn .btn-green } |

**Abstract**: Accurate temporal reconstructions of plant growth are essential for plant
phenotyping and breeding, yet remain challenging due to complex geometries,
occlusions, and non-rigid deformations of plants. We present a novel framework
for building temporal digital twins of plants by combining 3D Gaussian
Splatting with a robust sample alignment pipeline. Our method begins by
reconstructing Gaussian Splats from multi-view camera data, then leverages a
two-stage registration approach: coarse alignment through feature-based
matching and Fast Global Registration, followed by fine alignment with
Iterative Closest Point. This pipeline yields a consistent 4D model of plant
development in discrete time steps. We evaluate the approach on data from the
Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal
reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at
https://berkeleyautomation.github.io/GrowSplat/



---

## EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced  Quality for outdoor scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Jianlin Guo, Haihong Xiao, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2505.10787v1){: .btn .btn-green } |

**Abstract**: Efficient scene representations are essential for many real-world
applications, especially those involving spatial measurement. Although current
NeRF-based methods have achieved impressive results in reconstructing
building-scale scenes, they still suffer from slow training and inference
speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian
Splatting (3DGS) has demonstrated excellent performance with its high-quality
rendering and real-time speed, especially for objects and small-scale scenes.
However, in outdoor scenes, its point-based explicit representation lacks an
effective adjustment mechanism, and the millions of Gaussian points required
often lead to memory constraints during training. To address these challenges,
we propose EA-3DGS, a high-quality real-time rendering method designed for
outdoor scenes. First, we introduce a mesh structure to regulate the
initialization of Gaussian components by leveraging an adaptive tetrahedral
mesh that partitions the grid and initializes Gaussian components on each face,
effectively capturing geometric structures in low-texture regions. Second, we
propose an efficient Gaussian pruning strategy that evaluates each 3D
Gaussian's contribution to the view and prunes accordingly. To retain
geometry-critical Gaussian points, we also present a structure-aware
densification strategy that densifies Gaussian points in low-curvature regions.
Additionally, we employ vector quantization for parameter quantization of
Gaussian components, significantly reducing disk space requirements with only a
minimal impact on rendering quality. Extensive experiments on 13 scenes,
including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \&
Temples, WHU) and five self-collected scenes acquired through UAV
photogrammetry measurement from SCUT-CA and plateau regions, further
demonstrate the superiority of our method.



---

## Advances in Radiance Field for Dynamic Scene: From Neural Field to  Gaussian Field

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Jinlong Fan, Xuepu Zeng, Jing Zhang, Mingming Gong, Yuxiang Yang, Dacheng Tao | cs.CV | [PDF](http://arxiv.org/pdf/2505.10049v2){: .btn .btn-green } |

**Abstract**: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.



---

## Large-Scale Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2505.09915v1){: .btn .btn-green } |

**Abstract**: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.



---

## VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, Fernando de la Torre | cs.GR | [PDF](http://arxiv.org/pdf/2505.10144v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.

Comments:
- I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/

---

## Consistent Quantity-Quality Control across Scenes for Deployment-Aware  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Fengdi Zhang, Hongkun Cao, Ruqi Huang | cs.CV | [PDF](http://arxiv.org/pdf/2505.10473v2){: .btn .btn-green } |

**Abstract**: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control. Through a single training run using a
fixed setup and a user-specified hyperparameter reflecting quantity-quality
preference, ControlGS can automatically find desirable quantity-quality
trade-off points across diverse scenes, from compact objects to large outdoor
scenes. It also outperforms baselines by achieving higher rendering quality
with fewer Gaussians, and supports a broad adjustment range with stepless
control over the trade-off. Project page:
https://zhang-fengdi.github.io/ControlGS/

Comments:
- 16 pages, 7 figures, 7 tables. Project page available at
  https://zhang-fengdi.github.io/ControlGS/

---

## ExploreGS: a vision-based low overhead framework for 3D scene  reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Yunji Feng, Chengpu Yu, Fengrui Ran, Zhi Yang, Yinni Liu | eess.IV | [PDF](http://arxiv.org/pdf/2505.10578v1){: .btn .btn-green } |

**Abstract**: This paper proposes a low-overhead, vision-based 3D scene reconstruction
framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces
traditional lidar-based point cloud acquisition process with a vision model,
achieving a high-quality reconstruction at a lower cost. The framework
integrates scene exploration and model reconstruction, and leverags a
Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore,
the 3D Gaussian Splatting (3DGS) training can be executed on-board.
Comprehensive experiments in both simulation and real-world environments
demonstrate the efficiency and applicability of the ExploreGS framework on
resource-constrained devices, while maintaining reconstruction quality
comparable to state-of-the-art methods.



---

## Neural Video Compression using 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Lakshya Gupta, Imran N. Junejo | cs.CV | [PDF](http://arxiv.org/pdf/2505.09324v1){: .btn .btn-green } |

**Abstract**: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

Comments:
- 9 pages, 8 figures

---

## FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous  Driving via Point-Level Dynamic-Static Decoupling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Yue Wen, Liang Song, Yijia Liu, Siting Zhu, Yanzi Miao, Lijun Han, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2505.09406v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

Comments:
- 7 pages, 9 figures, accepted by ICRA2025

---

## Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or  Robot Hardware

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2505.09601v1){: .btn .btn-green } |

**Abstract**: Scaling robot learning requires vast and diverse datasets. Yet the prevailing
data collection paradigm-human teleoperation-remains costly and constrained by
manual effort and physical robot access. We introduce Real2Render2Real (R2R2R),
a novel approach for generating robot training data without relying on object
dynamics simulation or teleoperation of robot hardware. The input is a
smartphone-captured scan of one or more objects and a single video of a human
demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic
demonstrations by reconstructing detailed 3D object geometry and appearance,
and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to
enable flexible asset generation and trajectory synthesis for both rigid and
articulated objects, converting these representations to meshes to maintain
compatibility with scalable rendering engines like IsaacLab but with collision
modeling off. Robot demonstration data generated by R2R2R integrates directly
with models that operate on robot proprioceptive states and image observations,
such as vision-language-action models (VLA) and imitation learning policies.
Physical experiments suggest that models trained on R2R2R data from a single
human demonstration can match the performance of models trained on 150 human
teleoperation demonstrations. Project page: https://real2render2real.com



---

## Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Ma Changfeng, Bi Ran, Guo Jie, Wang Chongjun, Guo Yanwen | cs.CV | [PDF](http://arxiv.org/pdf/2505.09413v1){: .btn .btn-green } |

**Abstract**: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

Comments:
- CVPR 2025 Accepted

---

## NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged  Information Guidance


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Wenzhe Cai, Jiaqi Peng, Yuqiang Yang, Yujian Zhang, Meng Wei, Hanqing Wang, Yilun Chen, Tai Wang, Jiangmiao Pang | cs.RO | [PDF](http://arxiv.org/pdf/2505.08712v2){: .btn .btn-green } |

**Abstract**: Learning navigation in dynamic open-world environments is an important yet
challenging skill for robots. Most previous methods rely on precise
localization and mapping or learn from expensive real-world demonstrations. In
this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end
framework trained solely in simulation and can zero-shot transfer to different
embodiments in diverse real-world environments. The key ingredient of NavDP's
network is the combination of diffusion-based trajectory generation and a
critic function for trajectory selection, which are conditioned on only local
observation tokens encoded from a shared policy transformer. Given the
privileged information of the global environment in simulation, we scale up the
demonstrations of good quality to train the diffusion policy and formulate the
critic value function targets with contrastive negative samples. Our
demonstration generation approach achieves about 2,500 trajectories/GPU per
day, 20$\times$ more efficient than real-world data collection, and results in
a large-scale navigation dataset with 363.2km trajectories across 1244 scenes.
Trained with this simulation dataset, NavDP achieves state-of-the-art
performance and consistently outstanding generalization capability on
quadruped, wheeled, and humanoid robots in diverse indoor and outdoor
environments. In addition, we present a preliminary attempt at using Gaussian
Splatting to make in-domain real-to-sim fine-tuning to further bridge the
sim-to-real gap. Experiments show that adding such real-to-sim data can improve
the success rate by 30\% without hurting its generalization capability.

Comments:
- Project Page:
  https://wzcai99.github.io/navigation-diffusion-policy.github.io/

---

## DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Holly Dinkel, Marcel BÃ¼sching, Alberta Longhini, Brian Coltin, Trey Smith, Danica Kragic, MÃ¥rten BjÃ¶rkman, Timothy Bretl | cs.CV | [PDF](http://arxiv.org/pdf/2505.08644v1){: .btn .btn-green } |

**Abstract**: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of
Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state
information through prediction-update filtering. The DLO-Splatting algorithm
uses a position-based dynamics model with shape smoothness and rigidity
dampening corrections to predict the object shape. Optimization with a 3D
Gaussian Splatting-based rendering loss iteratively renders and refines the
prediction to align it with the visual observations in the update step. Initial
experiments demonstrate promising results in a knot tying scenario, which is
challenging for existing vision-only methods.

Comments:
- 5 pages, 2 figures, presented at the 2025 5th Workshop: Reflections
  on Representations and Manipulating Deformable Objects at the IEEE
  International Conference on Robotics and Automation. RMDO workshop
  (https://deformable-workshop.github.io/icra2025/)

---

## ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for  Dynamic Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | He Huang, Qi Yang, Mufan Liu, Yiling Xu, Zhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.08196v1){: .btn .btn-green } |

**Abstract**: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from
a canonical space to target frames, which overlooks redundancy among adjacent
Gaussian primitives and results in suboptimal performance. To address this
limitation, we propose Anchor-Driven Deformable and Compressed Gaussian
Splatting (ADC-GS), a compact and efficient representation for dynamic scene
reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an
anchor-based structure within the canonical space, enhanced by a temporal
significance-based anchor refinement strategy. To reduce deformation
redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that
captures motions at varying granularities. Moreover, a rate-distortion
optimization is adopted to achieve an optimal balance between bitrate
consumption and representation fidelity. Experimental results demonstrate that
ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed
by 300%-800% while achieving state-of-the-art storage efficiency without
compromising rendering quality. The code is released at
https://github.com/H-Huang774/ADC-GS.git.



---

## FOCI: Trajectory Optimization on Gaussian Splats

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Mario Gomez Andreu, Maximum Wilder-Smith, Victor Klemm, Vaishakh Patil, Jesus Tordesillas, Marco Hutter | cs.RO | [PDF](http://arxiv.org/pdf/2505.08510v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently gained popularity as a faster
alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view
synthesis methods. Leveraging the spatial information encoded in 3DGS, this
work proposes FOCI (Field Overlap Collision Integral), an algorithm that is
able to optimize trajectories directly on the Gaussians themselves. FOCI
leverages a novel and interpretable collision formulation for 3DGS using the
notion of the overlap integral between Gaussians. Contrary to other approaches,
which represent the robot with conservative bounding boxes that underestimate
the traversability of the environment, we propose to represent the environment
and the robot as Gaussian Splats. This not only has desirable computational
properties, but also allows for orientation-aware planning, allowing the robot
to pass through very tight and narrow spaces. We extensively test our algorithm
in both synthetic and real Gaussian Splats, showcasing that collision-free
trajectories for the ANYmal legged robot that can be computed in a few seconds,
even with hundreds of thousands of Gaussians making up the environment. The
project page and code are available at
https://rffr.leggedrobotics.com/works/foci/

Comments:
- 7 pages, 8 figures, Mario Gomez Andreu and Maximum Wilder-Smith
  contributed equally

---

## A Survey of 3D Reconstruction with Event Cameras: From Event-based  Geometry to Neural 3D Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Ying Zhou, Vera Chung, Qiang Qu | cs.CV | [PDF](http://arxiv.org/pdf/2505.08438v1){: .btn .btn-green } |

**Abstract**: Event cameras have emerged as promising sensors for 3D reconstruction due to
their ability to capture per-pixel brightness changes asynchronously. Unlike
conventional frame-based cameras, they produce sparse and temporally rich data
streams, which enable more accurate 3D reconstruction and open up the
possibility of performing reconstruction in extreme environments such as
high-speed motion, low light, or high dynamic range scenes. In this survey, we
provide the first comprehensive review focused exclusively on 3D reconstruction
using event cameras. The survey categorises existing works into three major
types based on input modality - stereo, monocular, and multimodal systems, and
further classifies them by reconstruction approach, including geometry-based,
deep learning-based, and recent neural rendering techniques such as Neural
Radiance Fields and 3D Gaussian Splatting. Methods with a similar research
focus were organised chronologically into the most subdivided groups. We also
summarise public datasets relevant to event-based 3D reconstruction. Finally,
we highlight current research limitations in data availability, evaluation,
representation, and dynamic scene handling, and outline promising future
research directions. This survey aims to serve as a comprehensive reference and
a roadmap for future developments in event-driven 3D reconstruction.

Comments:
- 35 pages, 12 figures, 11 tables

---

## TUGS: Physics-based Compact Representation of Underwater Scenes by  Tensorized Gaussian

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Shijie Lian, Ziyi Zhang, Laurence Tianruo Yang and, Mengyu Ren, Debin Liu, Hua Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.08811v1){: .btn .btn-green } |

**Abstract**: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications



---

## TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin  Benchmark Dataset

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias NieÃŸner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2505.07396v2){: .btn .btn-green } |

**Abstract**: Urban Digital Twins (UDTs) have become essential for managing cities and
integrating complex, heterogeneous data from diverse sources. Creating UDTs
involves challenges at multiple process stages, including acquiring accurate 3D
source data, reconstructing high-fidelity 3D models, maintaining models'
updates, and ensuring seamless interoperability to downstream tasks. Current
datasets are usually limited to one part of the processing chain, hampering
comprehensive UDTs validation. To address these challenges, we introduce the
first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.
This dataset includes georeferenced, semantically aligned 3D models and
networks along with various terrestrial, mobile, aerial, and satellite
observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently
767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high
accuracy, and multimodal data integration, the benchmark supports robust
analysis of sensors and the development of advanced reconstruction methods.
Additionally, we explore downstream tasks demonstrating the potential of
TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar
potential analysis, point cloud semantic segmentation, and LoD3 building
reconstruction. We are convinced this contribution lays a foundation for
overcoming current limitations in UDT creation, fostering new research
directions and practical solutions for smarter, data-driven urban environments.
The project is available under: https://tum2t.win

Comments:
- Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing

---

## SLAG: Scalable Language-Augmented Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Laszlo Szilagyi, Francis Engelmann, Jeannette Bohg | cs.CV | [PDF](http://arxiv.org/pdf/2505.08124v1){: .btn .btn-green } |

**Abstract**: Language-augmented scene representations hold great promise for large-scale
robotics applications such as search-and-rescue, smart cities, and mining. Many
of these scenarios are time-sensitive, requiring rapid scene encoding while
also being data-intensive, necessitating scalable solutions. Deploying these
representations on robots with limited computational resources further adds to
the challenge. To address this, we introduce SLAG, a multi-GPU framework for
language-augmented Gaussian splatting that enhances the speed and scalability
of embedding large scenes. Our method integrates 2D visual-language model
features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG
eliminates the need for a loss function to compute per-Gaussian language
embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters
via a normalized weighted average, enabling highly parallelized scene encoding.
Additionally, we introduce a vector database for efficient embedding storage
and retrieval. Our experiments show that SLAG achieves an 18 times speedup in
embedding computation on a 16-GPU setup compared to OpenGaussian, while
preserving embedding quality on the ScanNet and LERF datasets. For more
details, visit our project website: https://slag-project.github.io/.



---

## Geometric Prior-Guided Neural Implicit Surface Reconstruction in the  Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Lintao Xiang, Hongpei Zheng, Bailin Deng, Hujun Yin | cs.CV | [PDF](http://arxiv.org/pdf/2505.07373v1){: .btn .btn-green } |

**Abstract**: Neural implicit surface reconstruction using volume rendering techniques has
recently achieved significant advancements in creating high-fidelity surfaces
from multiple 2D images. However, current methods primarily target scenes with
consistent illumination and struggle to accurately reconstruct 3D geometry in
uncontrolled environments with transient occlusions or varying appearances.
While some neural radiance field (NeRF)-based variants can better manage
photometric variations and transient objects in complex scenes, they are
designed for novel view synthesis rather than precise surface reconstruction
due to limited surface constraints. To overcome this limitation, we introduce a
novel approach that applies multiple geometric constraints to the implicit
surface optimization process, enabling more accurate reconstructions from
unconstrained image collections. First, we utilize sparse 3D points from
structure-from-motion (SfM) to refine the signed distance function estimation
for the reconstructed surface, with a displacement compensation to accommodate
noise in the sparse points. Additionally, we employ robust normal priors
derived from a normal predictor, enhanced by edge prior filtering and
multi-view consistency constraints, to improve alignment with the actual
surface geometry. Extensive testing on the Heritage-Recon benchmark and other
datasets has shown that the proposed method can accurately reconstruct surfaces
from in-the-wild images, yielding geometries with superior accuracy and
granularity compared to existing techniques. Our approach enables high-quality
3D reconstruction of various landmarks, making it applicable to diverse
scenarios such as digital preservation of cultural heritage sites.



---

## GIFStream: 4D Gaussian-based Immersive Video with Feature Stream

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Hao Li, Sicheng Li, Xiang Gao, Abudouaihati Batuer, Lu Yu, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2505.07539v1){: .btn .btn-green } |

**Abstract**: Immersive video offers a 6-Dof-free viewing experience, potentially playing a
key role in future video technology. Recently, 4D Gaussian Splatting has gained
attention as an effective approach for immersive video due to its high
rendering efficiency and quality, though maintaining quality with manageable
storage remains challenging. To address this, we introduce GIFStream, a novel
4D Gaussian representation using a canonical space and a deformation field
enhanced with time-dependent feature streams. These feature streams enable
complex motion modeling and allow efficient compression by leveraging temporal
correspondence and motion-aware pruning. Additionally, we incorporate both
temporal and spatial compression networks for end-to-end compression.
Experimental results show that GIFStream delivers high-quality immersive video
at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project
page: https://xdimlab.github.io/GIFStream

Comments:
- 14 pages, 10 figures

---

## NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain  Generalization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-11 | Ahmed Qazi, Abdul Basit, Asim Iqbal | cs.CV | [PDF](http://arxiv.org/pdf/2505.06894v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have significantly advanced the field of novel
view synthesis, yet their generalization across diverse scenes and conditions
remains challenging. Addressing this, we propose the integration of a novel
brain-inspired normalization technique Neural Generalization (NeuGen) into
leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts
the domain-invariant features, thereby enhancing the models' generalization
capabilities. It can be seamlessly integrated into NeRF architectures and
cultivates a comprehensive feature set that significantly improves accuracy and
robustness in image rendering. Through this integration, NeuGen shows improved
performance on benchmarks on diverse datasets across state-of-the-art NeRF
architectures, enabling them to generalize better across varied scenes. Our
comprehensive evaluations, both quantitative and qualitative, confirm that our
approach not only surpasses existing models in generalizability but also
markedly improves rendering quality. Our work exemplifies the potential of
merging neuroscientific principles with deep learning frameworks, setting a new
precedent for enhanced generalizability and efficiency in novel view synthesis.
A demo of our study is available at https://neugennerf.github.io.

Comments:
- 18 pages, 6 figures

---

## Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System  for Real-Time Rendering of Composed Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Xijie Yang, Linning Xu, Lihan Jiang, Dahua Lin, Bo Dai | cs.GR | [PDF](http://arxiv.org/pdf/2505.06523v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital
3D assets from multi-view images by leveraging a set of 3D Gaussian primitives
for rendering. Its explicit and discrete representation facilitates the
seamless composition of complex digital worlds, offering significant advantages
over previous neural implicit methods. However, when applied to large-scale
compositions, such as crowd-level scenes, it can encompass numerous 3D
Gaussians, posing substantial challenges for real-time rendering. To address
this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D
Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D
Gaussian clusters and dynamically selects only the necessary ones to accelerate
rendering speed. Our approach consists of two stages: (1) Offline Build, where
hierarchical clusters are generated using a local splatting method to minimize
visual differences across granularities, and (2) Online Selection, where
footprint evaluation determines perceptible clusters for efficient
rasterization during rendering. We curate a dataset of synthetic and real-world
scenes, including objects, trees, people, and buildings, each requiring 0.1
billion 3D Gaussians to capture fine details. Experiments show that our
solution balances rendering efficiency and visual quality across user-defined
tolerances, facilitating downstream interactive applications that compose
extensive 3DGS assets for consistent rendering performance.

Comments:
- project page: https://xijie-yang.github.io/V3DG/

---

## FlexNeRFer: A Multi-Dataflow, Adaptive Sparsity-Aware Accelerator for  On-Device NeRF Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Seock-Hwan Noh, Banseok Shin, Jeik Choi, Seungpyo Lee, Jaeha Kung, Yeseong Kim | cs.AR | [PDF](http://arxiv.org/pdf/2505.06504v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF), an AI-driven approach for 3D view
reconstruction, has demonstrated impressive performance, sparking active
research across fields. As a result, a range of advanced NeRF models has
emerged, leading on-device applications to increasingly adopt NeRF for highly
realistic scene reconstructions. With the advent of diverse NeRF models,
NeRF-based applications leverage a variety of NeRF frameworks, creating the
need for hardware capable of efficiently supporting these models. However, GPUs
fail to meet the performance, power, and area (PPA) cost demanded by these
on-device applications, or are specialized for specific NeRF algorithms,
resulting in lower efficiency when applied to other NeRF models. To address
this limitation, in this work, we introduce FlexNeRFer, an energy-efficient
versatile NeRF accelerator. The key components enabling the enhancement of
FlexNeRFer include: i) a flexible network-on-chip (NoC) supporting
multi-dataflow and sparsity on precision-scalable MAC array, and ii) efficient
data storage using an optimal sparsity format based on the sparsity ratio and
precision modes. To evaluate the effectiveness of FlexNeRFer, we performed a
layout implementation using 28nm CMOS technology. Our evaluation shows that
FlexNeRFer achieves 8.2~243.3x speedup and 24.1~520.3x improvement in energy
efficiency over a GPU (i.e., NVIDIA RTX 2080 Ti), while demonstrating 4.2~86.9x
speedup and 2.3~47.5x improvement in energy efficiency compared to a
state-of-the-art NeRF accelerator (i.e., NeuRex).

Comments:
- Accepted for publication at the 52nd IEEE/ACM International Symposium
  on Computer Architecture (ISCA-52), 2025

---

## 3D Characterization of Smoke Plume Dispersion Using Multi-View Drone  Swarm

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Nikil Krishnakumar, Shashank Sharma, Srijan Kumar Pal, Jiarong Hong | cs.RO | [PDF](http://arxiv.org/pdf/2505.06638v1){: .btn .btn-green } |

**Abstract**: This study presents an advanced multi-view drone swarm imaging system for the
three-dimensional characterization of smoke plume dispersion dynamics. The
system comprises a manager drone and four worker drones, each equipped with
high-resolution cameras and precise GPS modules. The manager drone uses image
feedback to autonomously detect and position itself above the plume, then
commands the worker drones to orbit the area in a synchronized circular flight
pattern, capturing multi-angle images. The camera poses of these images are
first estimated, then the images are grouped in batches and processed using
Neural Radiance Fields (NeRF) to generate high-resolution 3D reconstructions of
plume dynamics over time. Field tests demonstrated the ability of the system to
capture critical plume characteristics including volume dynamics, wind-driven
directional shifts, and lofting behavior at a temporal resolution of about 1 s.
The 3D reconstructions generated by this system provide unique field data for
enhancing the predictive models of smoke plume dispersion and fire spread.
Broadly, the drone swarm system offers a versatile platform for high resolution
measurements of pollutant emissions and transport in wildfires, volcanic
eruptions, prescribed burns, and industrial processes, ultimately supporting
more effective fire control decisions and mitigating wildfire risks.

Comments:
- 10 pages, 8 figures

---

## 3D Scene Generation: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.05474v1){: .btn .btn-green } |

**Abstract**: 3D scene generation seeks to synthesize spatially structured, semantically
meaningful, and photorealistic environments for applications such as immersive
media, robotics, autonomous driving, and embodied AI. Early methods based on
procedural rules offered scalability but limited diversity. Recent advances in
deep generative models (e.g., GANs, diffusion models) and 3D representations
(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene
distributions, improving fidelity, diversity, and view consistency. Recent
advances like diffusion models bridge 3D scene synthesis and photorealism by
reframing generation as image or video synthesis problems. This survey provides
a systematic overview of state-of-the-art approaches, organizing them into four
paradigms: procedural generation, neural 3D-based generation, image-based
generation, and video-based generation. We analyze their technical foundations,
trade-offs, and representative results, and review commonly used datasets,
evaluation protocols, and downstream applications. We conclude by discussing
key challenges in generation capacity, 3D representation, data and annotations,
and evaluation, and outline promising directions including higher fidelity,
physics-aware and interactive generation, and unified perception-generation
models. This review organizes recent advances in 3D scene generation and
highlights promising directions at the intersection of generative AI, 3D
vision, and embodied intelligence. To track ongoing developments, we maintain
an up-to-date project page:
https://github.com/hzxie/Awesome-3D-Scene-Generation.

Comments:
- Project Page: https://github.com/hzxie/Awesome-3D-Scene-Generation

---

## Steepest Descent Density Control for Compact 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Peihao Wang, Yuehao Wang, Dilin Wang, Sreyas Mohan, Zhiwen Fan, Lemeng Wu, Ruisi Cai, Yu-Ying Yeh, Zhangyang Wang, Qiang Liu, Rakesh Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2505.05587v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for
real-time, high-resolution novel view synthesis. By representing scenes as a
mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for
efficient rendering and reconstruction. To optimize scene coverage and capture
fine details, 3DGS employs a densification algorithm to generate additional
points. However, this process often leads to redundant point clouds, resulting
in excessive memory usage, slower performance, and substantial storage demands
- posing significant challenges for deployment on resource-constrained devices.
To address this limitation, we propose a theoretical framework that demystifies
and improves density control in 3DGS. Our analysis reveals that splitting is
crucial for escaping saddle points. Through an optimization-theoretic approach,
we establish the necessary conditions for densification, determine the minimal
number of offspring Gaussians, identify the optimal parameter update direction,
and provide an analytical solution for normalizing off-spring opacity. Building
on these insights, we introduce SteepGS, incorporating steepest density
control, a principled strategy that minimizes loss while maintaining a compact
point cloud. SteepGS achieves a ~50% reduction in Gaussian points without
compromising rendering quality, significantly enhancing both efficiency and
scalability.

Comments:
- CVPR 2025, Project page: https://vita-group.github.io/SteepGS/

---

## QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian  Initialization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Yueh-Cheng Liu, Lukas HÃ¶llein, Matthias NieÃŸner, Angela Dai | cs.CV | [PDF](http://arxiv.org/pdf/2505.05591v1){: .btn .btn-green } |

**Abstract**: Surface reconstruction is fundamental to computer vision and graphics,
enabling applications in 3D modeling, mixed reality, robotics, and more.
Existing approaches based on volumetric rendering obtain promising results, but
optimize on a per-scene basis, resulting in a slow optimization that can
struggle to model under-observed or textureless regions. We introduce
QuickSplat, which learns data-driven priors to generate dense initializations
for 2D gaussian splatting optimization of large-scale indoor scenes. This
provides a strong starting point for the reconstruction, which accelerates the
convergence of the optimization and improves the geometry of flat wall
structures. We further learn to jointly estimate the densification and update
of the scene parameters during each iteration; our proposed densifier network
predicts new Gaussians based on the rendering gradients of existing ones,
removing the needs of heuristics for densification. Extensive experiments on
large-scale indoor scene reconstruction demonstrate the superiority of our
data-driven optimization. Concretely, we accelerate runtime by 8x, while
decreasing depth errors by up to 48% in comparison to state of the art methods.

Comments:
- Project page: https://liu115.github.io/quicksplat, Video:
  https://youtu.be/2IA_gnFvFG8

---

## UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Mark C. Eid, Ana I. L. Namburete, JoÃ£o F. Henriques | eess.IV | [PDF](http://arxiv.org/pdf/2505.05643v1){: .btn .btn-green } |

**Abstract**: Ultrasound imaging is widely used due to its safety, affordability, and
real-time capabilities, but its 2D interpretation is highly operator-dependent,
leading to variability and increased cognitive demand. 2D-to-3D reconstruction
mitigates these challenges by providing standardized volumetric views, yet
existing methods are often computationally expensive, memory-intensive, or
incompatible with ultrasound physics. We introduce UltraGauss: the first
ultrasound-specific Gaussian Splatting framework, extending view synthesis
techniques to ultrasound wave propagation. Unlike conventional
perspective-based splatting, UltraGauss models probe-plane intersections in 3D,
aligning with acoustic image formation. We derive an efficient rasterization
boundary formulation for GPU parallelization and introduce a numerically stable
covariance parametrization, improving computational efficiency and
reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves
state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20
minutes on a single GPU. A survey of expert clinicians confirms UltraGauss'
reconstructions are the most realistic among competing methods. Our CUDA
implementation will be released upon publication.



---

## SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with  Video Diffusion and Data Augmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Yonwoo Choi | cs.CV | [PDF](http://arxiv.org/pdf/2505.05475v1){: .btn .btn-green } |

**Abstract**: Creating high-quality animatable 3D human avatars from a single image remains
a significant challenge in computer vision due to the inherent difficulty of
reconstructing complete 3D information from a single viewpoint. Current
approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods
produce high-quality results but require multiple views or video sequences,
while video diffusion models can generate animations from single images but
struggle with consistency and identity preservation. We present SVAD, a novel
approach that addresses these limitations by leveraging complementary strengths
of existing techniques. Our method generates synthetic training data through
video diffusion, enhances it with identity preservation and image restoration
modules, and utilizes this refined data to train 3DGS avatars. Comprehensive
evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)
single-image methods in maintaining identity consistency and fine details
across novel poses and viewpoints, while enabling real-time rendering
capabilities. Through our data augmentation pipeline, we overcome the
dependency on dense monocular or multi-view training data typically required by
traditional 3DGS approaches. Extensive quantitative, qualitative comparisons
show our method achieves superior performance across multiple metrics against
baseline models. By effectively combining the generative power of diffusion
models with both the high-quality results and rendering efficiency of 3DGS, our
work establishes a new approach for high-fidelity avatar generation from a
single image input.

Comments:
- Accepted by CVPR 2025 SyntaGen Workshop, Project Page:
  https://yc4ny.github.io/SVAD/

---

## Time of the Flight of the Gaussians: Optimizing Depth Indirectly in  Dynamic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Runfeng Li, Mikhail Okunev, Zixuan Guo, Anh Ha Duong, Christian Richardt, Matthew O'Toole, James Tompkin | cs.GR | [PDF](http://arxiv.org/pdf/2505.05356v1){: .btn .btn-green } |

**Abstract**: We present a method to reconstruct dynamic scenes from monocular
continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that
achieves similar or better accuracy than neural volumetric approaches and is
100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a
single viewpoint is a significant challenge in computer vision. In C-ToF
radiance field reconstruction, the property of interest-depth-is not directly
measured, causing an additional challenge. This problem has a large and
underappreciated impact upon the optimization when using a fast primitive-based
scene representation like 3D Gaussian splatting, which is commonly used with
multi-view data to produce satisfactory results and is brittle in its
optimization otherwise. We incorporate two heuristics into the optimization to
improve the accuracy of scene geometry represented by Gaussians. Experimental
results show that our approach produces accurate reconstructions under
constrained C-ToF sensing conditions, including for fast motions like swinging
baseball bats. https://visual.cs.brown.edu/gftorf



---

## TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Gengyan Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2505.05672v1){: .btn .btn-green } |

**Abstract**: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have
recently enabled animatable 3D head avatars that are rendered under arbitrary
viewpoints with impressive photorealism. Today, such photoreal avatars are seen
as a key component in emerging applications in telepresence, extended reality,
and entertainment. Building a photoreal avatar requires estimating the complex
non-rigid motion of different facial components as seen in input video images;
due to inaccurate motion estimation, animatable models typically present a loss
of fidelity and detail when compared to their non-animatable counterparts,
built from an individual facial expression. Also, recent state-of-the-art
models are often affected by memory limitations that reduce the number of 3D
Gaussians used for modeling, leading to lower detail and quality. To address
these problems, we present a new high-detail 3D head avatar model that improves
upon the state of the art, largely increasing the number of 3D Gaussians and
modeling quality for rendering at 4K resolution. Our high-quality model is
reconstructed from multiview input video and builds on top of a mesh-based 3D
morphable model, which provides a coarse deformation layer for the head.
Photoreal appearance is modelled by 3D Gaussians embedded within the continuous
UVD tangent space of this mesh, allowing for more effective densification where
most needed. Additionally, these Gaussians are warped by a novel UVD
deformation field to capture subtle, localized motion. Our key contribution is
the novel deformable Gaussian encoding and overall fitting procedure that
allows our head model to preserve appearance detail, while capturing facial
motion and other transient high-frequency features such as skin wrinkling.

Comments:
- 10 pages, 9 figures, supplementary results found at:
  https://syntec-research.github.io/UVGA/, to be published in SIGGRAPH 2025

---

## GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view  Synthesis in 3D Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Feng Xiao, Hongbin Xu, Wanlin Liang, Wenxiong Kang | cs.GR | [PDF](http://arxiv.org/pdf/2505.04659v1){: .btn .btn-green } |

**Abstract**: The semantic synthesis of unseen scenes from multiple viewpoints is crucial
for research in 3D scene understanding. Current methods are capable of
rendering novel-view images and semantic maps by reconstructing generalizable
Neural Radiance Fields. However, they often suffer from limitations in speed
and segmentation performance. We propose a generalizable semantic Gaussian
Splatting method (GSsplat) for efficient novel-view synthesis. Our model
predicts the positions and attributes of scene-adaptive Gaussian distributions
from once input, replacing the densification and pruning processes of
traditional scene-specific Gaussian Splatting. In the multi-task framework, a
hybrid network is designed to extract color and semantic information and
predict Gaussian parameters. To augment the spatial perception of Gaussians for
high-quality rendering, we put forward a novel offset learning module through
group-based supervision and a point-level interaction module with spatial unit
aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat
achieves state-of-the-art performance for semantic synthesis at the fastest
speed.



---

## SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Xinran Yang, Donghao Ji, Yuanqi Li, Jie Guo, Yanwen Guo, Junyuan Xie | cs.GR | [PDF](http://arxiv.org/pdf/2505.04668v1){: .btn .btn-green } |

**Abstract**: Neural rendering techniques have made substantial progress in generating
photo-realistic 3D scenes. The latest 3D Gaussian Splatting technique has
achieved high quality novel view synthesis as well as fast rendering speed.
However, 3D Gaussians lack proficiency in defining accurate 3D geometric
structures despite their explicit primitive representations. This is due to the
fact that Gaussian's attributes are primarily tailored and fine-tuned for
rendering diverse 2D images by their anisotropic nature. To pave the way for
efficient 3D reconstruction, we present Spherical Gaussians, a simple and
effective representation for 3D geometric boundaries, from which we can
directly reconstruct 3D feature curves from a set of calibrated multi-view
images. Spherical Gaussians is optimized from grid initialization with a
view-based rendering loss, where a 2D edge map is rendered at a specific view
and then compared to the ground-truth edge map extracted from the corresponding
image, without the need for any 3D guidance or supervision. Given Spherical
Gaussians serve as intermedia for the robust edge representation, we further
introduce a novel optimization-based algorithm called SGCR to directly extract
accurate parametric curves from aligned Spherical Gaussians. We demonstrate
that SGCR outperforms existing state-of-the-art methods in 3D edge
reconstruction while enjoying great efficiency.

Comments:
- The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2025, 8 pages

---

## Bridging Geometry-Coherent Text-to-3D Generation with Multi-View  Diffusion Priors and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Feng Yang, Wenliang Qian, Wangmeng Zuo, Hui Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.04262v1){: .btn .btn-green } |

**Abstract**: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.



---

## 3D Gaussian Splatting Data Compression with Mixture of Priors

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-06 | Lei Liu, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.03310v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.



---

## HandOcc: NeRF-based Hand Rendering with Occupancy Networks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2505.02079v1){: .btn .btn-green } |

**Abstract**: We propose HandOcc, a novel framework for hand rendering based upon
occupancy. Popular rendering methods such as NeRF are often combined with
parametric meshes to provide deformable hand models. However, in doing so, such
approaches present a trade-off between the fidelity of the mesh and the
complexity and dimensionality of the parametric model. The simplicity of
parametric mesh structures is appealing, but the underlying issue is that it
binds methods to mesh initialization, making it unable to generalize to objects
where a parametric model does not exist. It also means that estimation is tied
to mesh resolution and the accuracy of mesh fitting. This paper presents a
pipeline for meshless 3D rendering, which we apply to the hands. By providing
only a 3D skeleton, the desired appearance is extracted via a convolutional
model. We do this by exploiting a NeRF renderer conditioned upon an
occupancy-based representation. The approach uses the hand occupancy to resolve
hand-to-hand interactions further improving results, allowing fast rendering,
and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,
we achieved state-of-the-art results.



---

## GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity  Non-Watertight 3D Garment Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2505.02126v1){: .btn .btn-green } |

**Abstract**: Traditional 3D garment creation requires extensive manual operations,
resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved
breakthrough progress in 3D scene reconstruction and rendering, attracting
widespread attention and opening new pathways for 3D garment reconstruction.
However, due to the unstructured and irregular nature of Gaussian primitives,
it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In
this paper, we present GarmentGS, a dense point cloud-guided method that can
reconstruct high-fidelity garment surfaces with high geometric accuracy and
generate non-watertight, single-layer meshes. Our method introduces a fast
dense point cloud reconstruction module that can complete garment point cloud
reconstruction in 10 minutes, compared to traditional methods that require
several hours. Furthermore, we use dense point clouds to guide the movement,
flattening, and rotation of Gaussian primitives, enabling better distribution
on the garment surface to achieve superior rendering effects and geometric
accuracy. Through numerical and visual comparisons, our method achieves fast
training and real-time rendering while maintaining competitive quality.



---

## Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.02005v1){: .btn .btn-green } |

**Abstract**: Recent NeRF methods on large-scale scenes have underlined the importance of
scene decomposition for scalable NeRFs. Although achieving reasonable
scalability, there are several critical problems remaining unexplored, i.e.,
learnable decomposition, modeling scene heterogeneity, and modeling efficiency.
In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash
Experts (HMoHE) network that addresses these challenges within a unified
framework. It is a highly scalable NeRF that learns heterogeneous decomposition
and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end
manner. In our framework, a gating network learns to decomposes scenes and
allocates 3D points to specialized NeRF experts. This gating network is
co-optimized with the experts, by our proposed Sparsely Gated Mixture of
Experts (MoE) NeRF framework. We incorporate a hash-based gating network and
distinct heterogeneous hash experts. The hash-based gating efficiently learns
the decomposition of the large-scale scene. The distinct heterogeneous hash
experts consist of hash grids of different resolution ranges, enabling
effective learning of the heterogeneous representation of different scene
parts. These design choices make our framework an end-to-end and highly
scalable NeRF solution for real-world large-scale scene modeling to achieve
both quality and efficiency. We evaluate our accuracy and scalability on
existing large-scale NeRF datasets and a new dataset with very large-scale
scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our
approach can be easily scaled to various large-scale scenes and achieve
state-of-the-art scene rendering accuracy. Furthermore, our method exhibits
significant efficiency, with an 8x acceleration in training and a 16x
acceleration in rendering compared to Switch-NeRF. Codes will be released in
https://github.com/MiZhenxing/Switch-NeRF.

Comments:
- 15 pages, 9 figures

---

## Sparfels: Fast Reconstruction from Sparse Unposed Imagery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2505.02178v1){: .btn .btn-green } |

**Abstract**: We present a method for Sparse view reconstruction with surface element
splatting that runs within 3 minutes on a consumer grade GPU. While few methods
address sparse radiance field learning from noisy or unposed sparse cameras,
shape recovery remains relatively underexplored in this setting. Several
radiance and shape learning test-time optimization methods address the sparse
posed setting by learning data priors or using combinations of external
monocular geometry priors. Differently, we propose an efficient and simple
pipeline harnessing a single recent 3D foundation model. We leverage its
various task heads, notably point maps and camera initializations to
instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image
correspondences to guide camera optimization midst 2DGS training. Key to our
contribution is a novel formulation of splatted color variance along rays,
which can be computed efficiently. Reducing this moment in training leads to
more accurate shape reconstructions. We demonstrate state-of-the-art
performances in the sparse uncalibrated setting in reconstruction and novel
view benchmarks based on established multi-view datasets.

Comments:
- Project page : https://shubhendu-jena.github.io/Sparfels/

---

## SignSplat: Rendering Sign Language via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2505.02108v1){: .btn .btn-green } |

**Abstract**: State-of-the-art approaches for conditional human body rendering via Gaussian
splatting typically focus on simple body motions captured from many views. This
is often in the context of dancing or walking. However, for more complex use
cases, such as sign language, we care less about large body motion and more
about subtle and complex motions of the hands and face. The problems of
building high fidelity models are compounded by the complexity of capturing
multi-view data of sign. The solution is to make better use of sequence data,
ensuring that we can overcome the limited information from only a few views by
exploiting temporal variability. Nevertheless, learning from sequence-level
data requires extremely accurate and consistent model fitting to ensure that
appearance is consistent across complex motions. We focus on how to achieve
this, constraining mesh parameters to build an accurate Gaussian splatting
framework from few views capable of modelling subtle human motion. We leverage
regularization techniques on the Gaussian parameters to mitigate overfitting
and rendering artifacts. Additionally, we propose a new adaptive control method
to densify Gaussians and prune splat points on the mesh surface. To demonstrate
the accuracy of our approach, we render novel sequences of sign language video,
building on neural machine translation approaches to sign stitching. On
benchmark datasets, our approach achieves state-of-the-art performance; and on
highly articulated and complex sign language motion, we significantly
outperform competing approaches.



---

## SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Shubhendu Jena, Shishir Reddy Vutukur, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2505.02175v1){: .btn .btn-green } |

**Abstract**: Recovering 3D information from scenes via multi-view stereo reconstruction
(MVS) and novel view synthesis (NVS) is inherently challenging, particularly in
scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting
(3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian
Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive
rasterization to achieve accurate geometry representation during rendering,
improving 3D scene reconstruction while maintaining real-time performance.
Recent approaches have tackled the problem of sparse real-time NVS using 3DGS
within a generalizable, MVS-based learning framework to regress 3D Gaussian
parameters. Our work extends this line of research by addressing the challenge
of generalizable sparse 3D reconstruction and NVS jointly, and manages to
perform successfully at both tasks. We propose an MVS-based learning pipeline
that regresses 2DGS surface element parameters in a feed-forward fashion to
perform 3D shape reconstruction and NVS from sparse-view images. We further
show that our generalizable pipeline can benefit from preexisting foundational
multi-view deep visual features. The resulting model attains the
state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms
of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also
demonstrates strong generalization on the BlendedMVS and Tanks and Temples
datasets. We note that our model outperforms the prior state-of-the-art in
feed-forward sparse view reconstruction based on volume rendering of implicit
representations, while offering an almost 2 orders of magnitude higher
inference speed.

Comments:
- Project page : https://shubhendu-jena.github.io/SparSplat/

---

## HybridGS: High-Efficiency Gaussian Splatting Data Compression using  Dual-Channel Sparse Representation and Point Cloud Encoder

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Qi Yang, Le Yang, Geert Van Der Auwera, Zhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.01938v1){: .btn .btn-green } |

**Abstract**: Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on
producing compact 3DGS representation via implicit data embedding. They have
long coding times and highly customized data format, making it difficult for
widespread deployment. This paper presents a new 3DGS compression framework
called HybridGS, which takes advantage of both compact generation and
standardized point cloud data encoding. HybridGS first generates compact and
explicit 3DGS data. A dual-channel sparse representation is introduced to
supervise the primitive position and feature bit depth. It then utilizes a
canonical point cloud encoder to perform further data compression and form
standard output bitstreams. A simple and effective rate control scheme is
proposed to pivot the interpretable data compression scheme. At the current
stage, HybridGS does not include any modules aimed at improving 3DGS quality
during generation. But experiment results show that it still provides
comparable reconstruction performance against state-of-the-art methods, with
evidently higher encoding and decoding speed. The code is publicly available at
https://github.com/Qi-Yangsjtu/HybridGS.

Comments:
- Accepted by ICML2025

---

## Visual enhancement and 3D representation for underwater scenes: a review


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Guoxi Huang, Haoran Wang, Brett Seymour, Evan Kovacs, John Ellerbrock, Dave Blackham, Nantheera Anantrasirichai | cs.CV | [PDF](http://arxiv.org/pdf/2505.01869v1){: .btn .btn-green } |

**Abstract**: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose
significant challenges in
  computer vision and AI-based tasks due to complex imaging conditions in
aquatic environments. Despite
  the development of numerous enhancement algorithms, a comprehensive and
systematic review covering both
  UVE and underwater 3D reconstruction remains absent. To advance research in
these areas, we present an
  in-depth review from multiple perspectives. First, we introduce the
fundamental physical models, highlighting the
  peculiarities that challenge conventional techniques. We survey advanced
methods for visual enhancement and
  3D reconstruction specifically designed for underwater scenarios. The paper
assesses various approaches from
  non-learning methods to advanced data-driven techniques, including Neural
Radiance Fields and 3D Gaussian
  Splatting, discussing their effectiveness in handling underwater distortions.
Finally, we conduct both quantitative
  and qualitative evaluations of state-of-the-art UVE and underwater 3D
reconstruction algorithms across multiple
  benchmark datasets. Finally, we highlight key research directions for future
advancements in underwater vision.



---

## AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Junhao Shi, Jisheng Xu, Jianping He, Zhiliang Lin | cs.CV | [PDF](http://arxiv.org/pdf/2505.01799v1){: .btn .btn-green } |

**Abstract**: Underwater scene reconstruction is a critical tech-nology for underwater
operations, enabling the generation of 3D models from images captured by
underwater platforms. However, the quality of underwater images is often
degraded due to medium interference, which limits the effectiveness of
Structure-from-Motion (SfM) pose estimation, leading to subsequent
reconstruction failures. Additionally, SfM methods typically operate at slower
speeds, further hindering their applicability in real-time scenarios. In this
paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model
based on the SeaThru algorithm, which facilitates rapid and accurate separation
of scene details and medium features. Our approach initializes Gaussians by
integrating state-of-the-art multi-view stereo (MVS) technology, employs
implicit Neural Radiance Fields (NeRF) for rendering translucent media and
utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render
object surfaces, which effectively addresses the limitations of traditional
methods and accurately simulates underwater optical phenomena. Experimental
results on the data set and the robot platform show that our model can complete
high-precision reconstruction in 30 seconds with only 3 image inputs,
significantly enhancing the practical application of the algorithm in robotic
platforms.



---

## GenSync: A Generalized Talking Head Framework for Audio-driven  Multi-Subject Lip-Sync using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Anushka Agarwal, Muhammad Yusuf Hassan, Talha Chafekar | cs.CV | [PDF](http://arxiv.org/pdf/2505.01928v1){: .btn .btn-green } |

**Abstract**: We introduce GenSync, a novel framework for multi-identity lip-synced video
synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that
require training a new model for each identity , GenSync learns a unified
network that synthesizes lip-synced videos for multiple speakers. By
incorporating a Disentanglement Module, our approach separates
identity-specific features from audio representations, enabling efficient
multi-identity video synthesis. This design reduces computational overhead and
achieves 6.8x faster training compared to state-of-the-art models, while
maintaining high lip-sync accuracy and visual quality.



---

## Unified Steganography via Implicit Neural Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Qi Song, Ziyuan Luo, Xiufeng Huang, Sheng Li, Renjie Wan | cs.CR | [PDF](http://arxiv.org/pdf/2505.01749v1){: .btn .btn-green } |

**Abstract**: Digital steganography is the practice of concealing for encrypted data
transmission. Typically, steganography methods embed secret data into cover
data to create stega data that incorporates hidden secret data. However,
steganography techniques often require designing specific frameworks for each
data type, which restricts their generalizability. In this paper, we present
U-INR, a novel method for steganography via Implicit Neural Representation
(INR). Rather than using the specific framework for each data format, we
directly use the neurons of the INR network to represent the secret data and
cover data across different data types. To achieve this idea, a private key is
shared between the data sender and receivers. Such a private key can be used to
determine the position of secret data in INR networks. To effectively leverage
this key, we further introduce a key-based selection strategy that can be used
to determine the position within the INRs for data storage. Comprehensive
experiments across multiple data types, including images, videos, audio, and
SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR,
emphasizing its potential for improving data security and privacy in various
applications.



---

## Compensating Spatiotemporally Inconsistent Observations for Online  Dynamic 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-02 | Youngsik Yun, Jeongmin Bae, Hyunseung Son, Seoha Kim, Hahyun Lee, Gun Bang, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2505.01235v1){: .btn .btn-green } |

**Abstract**: Online reconstruction of dynamic scenes is significant as it enables learning
scenes from live-streaming video inputs, while existing offline dynamic
reconstruction methods rely on recorded video inputs. However, previous online
reconstruction approaches have primarily focused on efficiency and rendering
quality, overlooking the temporal consistency of their results, which often
contain noticeable artifacts in static regions. This paper identifies that
errors such as noise in real-world recordings affect temporal inconsistency in
online reconstruction. We propose a method that enhances temporal consistency
in online reconstruction from observations with temporal inconsistency which is
inevitable in cameras. We show that our method restores the ideal observation
by subtracting the learned error. We demonstrate that applying our method to
various baselines significantly enhances both temporal consistency and
rendering quality across datasets. Code, video results, and checkpoints are
available at https://bbangsik13.github.io/OR2.

Comments:
- SIGGRAPH 2025, Project page: https://bbangsik13.github.io/OR2

---

## FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft  Research

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-02 | Yan Miao, Will Shen, Hang Cui, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2505.01383v1){: .btn .btn-green } |

**Abstract**: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.



---

## Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique  Instances in Open-Vocabulary 3D Panoptic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Feng Xue, Wenzhuang Xu, Guofeng Zhong, Anlong Minga, Nicu Sebe | cs.CV | [PDF](http://arxiv.org/pdf/2505.00378v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D panoptic segmentation has recently emerged as a
significant trend. Top-performing methods currently integrate 2D segmentation
with geometry-aware 3D primitives. However, the advantage would be lost without
high-fidelity 3D point clouds, such as methods based on Neural Radiance Field
(NeRF). These methods are limited by the insufficient capacity to maintain
consistency across partial observations. To address this, recent works have
utilized contrastive loss or cross-view association pre-processing for view
consensus. In contrast to them, we present Cues3D, a compact approach that
relies solely on NeRF instead of pre-associations. The core idea is that NeRF's
implicit 3D field inherently establishes a globally consistent geometry,
enabling effective object distinction without explicit cross-view supervision.
We propose a three-phase training framework for NeRF,
initialization-disambiguation-refinement, whereby the instance IDs are
corrected using the initially-learned knowledge. Additionally, an instance
disambiguation method is proposed to match NeRF-rendered 3D masks and ensure
globally unique 3D instance identities. With the aid of Cues3D, we obtain
highly consistent and unique 3D instance ID for each object across views with a
balanced version of NeRF. Our experiments are conducted on ScanNet v2,
ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and
semantic segmentation tasks. Cues3D outperforms other 2D image-based methods
and competes with the latest 2D-3D merging based methods, while even surpassing
them when using additional 3D point clouds. The code link could be found in the
appendix and will be released on
\href{https://github.com/mRobotit/Cues3D}{github}

Comments:
- Accepted by Information Fusion

---

## Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing | cs.CV | [PDF](http://arxiv.org/pdf/2505.00421v1){: .btn .btn-green } |

**Abstract**: High-quality, animatable 3D human avatar reconstruction from monocular videos
offers significant potential for reducing reliance on complex hardware, making
it highly practical for applications in game development, augmented reality,
and social media. However, existing methods still face substantial challenges
in capturing fine geometric details and maintaining animation stability,
particularly under dynamic or complex poses. To address these issues, we
propose a novel real-time framework for animatable human avatar reconstruction
based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose
parameters, our framework not only aligns positional and rotational
discrepancies but also enables robust and natural pose-driven animation of the
reconstructed avatars. Furthermore, we introduce a Rotation Compensation
Network (RCN) that learns rotation residuals by integrating local geometric
features with global pose parameters. This network significantly improves the
handling of non-rigid deformations and ensures smooth, artifact-free pose
transitions during animation. Experimental results demonstrate that our method
successfully reconstructs realistic and highly animatable human avatars from
monocular videos, effectively preserving fine-grained details while ensuring
stable and natural pose variation. Our approach surpasses current
state-of-the-art methods in both reconstruction quality and animation
robustness on public benchmarks.


