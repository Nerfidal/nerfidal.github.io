---
layout: default
title: May 2025
parent: Papers
nav_order: 202505
---

<!---metadata--->


## Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Abhishek Kashyap, Henrik Andreasson, Todor Stoyanov | cs.RO | [PDF](http://arxiv.org/pdf/2505.11467v1){: .btn .btn-green } |

**Abstract**: Vision based robot manipulation uses cameras to capture one or more images of
a scene containing the objects to be manipulated. Taking multiple images can
help if any object is occluded from one viewpoint but more visible from another
viewpoint. However, the camera has to be moved to a sequence of suitable
positions for capturing multiple images, which requires time and may not always
be possible, due to reachability constraints. So while additional images can
produce more accurate grasp poses due to the extra information available, the
time-cost goes up with the number of additional views sampled. Scene
representations like Gaussian Splatting are capable of rendering accurate
photorealistic virtual images from user-specified novel viewpoints. In this
work, we show initial results which indicate that novel view synthesis can
provide additional context in generating grasp poses. Our experiments on the
Graspnet-1billion dataset show that novel views contributed force-closure
grasps in addition to the force-closure grasps obtained from sparsely sampled
real views while also improving grasp coverage. In the future we hope this work
can be extended to improve grasp extraction from radiance fields constructed
with a single input image, using for example diffusion models or generalizable
radiance fields.

Comments:
- 6 pages

---

## EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced  Quality for outdoor scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Jianlin Guo, Haihong Xiao, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2505.10787v1){: .btn .btn-green } |

**Abstract**: Efficient scene representations are essential for many real-world
applications, especially those involving spatial measurement. Although current
NeRF-based methods have achieved impressive results in reconstructing
building-scale scenes, they still suffer from slow training and inference
speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian
Splatting (3DGS) has demonstrated excellent performance with its high-quality
rendering and real-time speed, especially for objects and small-scale scenes.
However, in outdoor scenes, its point-based explicit representation lacks an
effective adjustment mechanism, and the millions of Gaussian points required
often lead to memory constraints during training. To address these challenges,
we propose EA-3DGS, a high-quality real-time rendering method designed for
outdoor scenes. First, we introduce a mesh structure to regulate the
initialization of Gaussian components by leveraging an adaptive tetrahedral
mesh that partitions the grid and initializes Gaussian components on each face,
effectively capturing geometric structures in low-texture regions. Second, we
propose an efficient Gaussian pruning strategy that evaluates each 3D
Gaussian's contribution to the view and prunes accordingly. To retain
geometry-critical Gaussian points, we also present a structure-aware
densification strategy that densifies Gaussian points in low-curvature regions.
Additionally, we employ vector quantization for parameter quantization of
Gaussian components, significantly reducing disk space requirements with only a
minimal impact on rendering quality. Extensive experiments on 13 scenes,
including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \&
Temples, WHU) and five self-collected scenes acquired through UAV
photogrammetry measurement from SCUT-CA and plateau regions, further
demonstrate the superiority of our method.



---

## MutualNeRF: Improve the Performance of NeRF under Limited Samples with  Mutual Information Theory

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Zifan Wang, Jingwei Li, Yitang Li, Yunze Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.11386v1){: .btn .btn-green } |

**Abstract**: This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field
(NeRF) performance under limited samples using Mutual Information Theory. While
NeRF excels in 3D scene synthesis, challenges arise with limited data and
existing methods that aim to introduce prior knowledge lack theoretical support
in a unified framework. We introduce a simple but theoretically robust concept,
Mutual Information, as a metric to uniformly measure the correlation between
images, considering both macro (semantic) and micro (pixel) levels.
  For sparse view sampling, we strategically select additional viewpoints
containing more non-overlapping scene information by minimizing mutual
information without knowing ground truth images beforehand. Our framework
employs a greedy algorithm, offering a near-optimal solution.
  For few-shot view synthesis, we maximize the mutual information between
inferred images and ground truth, expecting inferred images to gain more
relevant information from known images. This is achieved by incorporating
efficient, plug-and-play regularization terms.
  Experiments under limited samples show consistent improvement over
state-of-the-art baselines in different settings, affirming the efficacy of our
framework.



---

## GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian  Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-16 | Simeon Adebola, Shuangyu Xie, Chung Min Kim, Justin Kerr, Bart M. van Marrewijk, Mieke van Vlaardingen, Tim van Daalen, Robert van Loo, Jose Luis Susa Rincon, Eugen Solowjow, Rick van de Zedde, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2505.10923v1){: .btn .btn-green } |

**Abstract**: Accurate temporal reconstructions of plant growth are essential for plant
phenotyping and breeding, yet remain challenging due to complex geometries,
occlusions, and non-rigid deformations of plants. We present a novel framework
for building temporal digital twins of plants by combining 3D Gaussian
Splatting with a robust sample alignment pipeline. Our method begins by
reconstructing Gaussian Splats from multi-view camera data, then leverages a
two-stage registration approach: coarse alignment through feature-based
matching and Fast Global Registration, followed by fine alignment with
Iterative Closest Point. This pipeline yields a consistent 4D model of plant
development in discrete time steps. We evaluate the approach on data from the
Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal
reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at
https://berkeleyautomation.github.io/GrowSplat/



---

## Consistent Quantity-Quality Control across Scenes for Deployment-Aware  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Fengdi Zhang, Hongkun Cao, Ruqi Huang | cs.CV | [PDF](http://arxiv.org/pdf/2505.10473v1){: .btn .btn-green } |

**Abstract**: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.



---

## Large-Scale Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2505.09915v1){: .btn .btn-green } |

**Abstract**: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.



---

## VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, Fernando de la Torre | cs.GR | [PDF](http://arxiv.org/pdf/2505.10144v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has rapidly become a leading technique for
novel-view synthesis, providing exceptional performance through efficient
software-based GPU rasterization. Its versatility enables real-time
applications, including on mobile and lower-powered devices. However, 3DGS
faces key challenges in virtual reality (VR): (1) temporal artifacts, such as
popping during head movements, (2) projection-based distortions that result in
disturbing and view-inconsistent floaters, and (3) reduced framerates when
rendering large numbers of Gaussians, falling below the critical threshold for
VR. Compared to desktop environments, these issues are drastically amplified by
large field-of-view, constant head movements, and high resolution of
head-mounted displays (HMDs). In this work, we introduce VRSplat: we combine
and extend several recent advancements in 3DGS to address challenges of VR
holistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal
Projection can complement each other, by modifying the individual techniques
and core 3DGS rasterizer. Additionally, we propose an efficient foveated
rasterizer that handles focus and peripheral areas in a single GPU launch,
avoiding redundant computations and improving GPU utilization. Our method also
incorporates a fine-tuning step that optimizes Gaussian parameters based on
StopThePop depth evaluations and Optimal Projection. We validate our method
through a controlled user study with 25 participants, showing a strong
preference for VRSplat over other configurations of Mini-Splatting. VRSplat is
the first, systematically evaluated 3DGS approach capable of supporting modern
VR applications, achieving 72+ FPS while eliminating popping and
stereo-disrupting floaters.

Comments:
- I3D'25 (PACMCGIT); Project Page: https://cekavis.site/VRSplat/

---

## Advances in Radiance Field for Dynamic Scene: From Neural Field to  Gaussian Field

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-15 | Jinlong Fan, Xuepu Zeng, Jing Zhang, Mingming Gong, Yuxiang Yang, Dacheng Tao | cs.CV | [PDF](http://arxiv.org/pdf/2505.10049v1){: .btn .btn-green } |

**Abstract**: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.



---

## FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous  Driving via Point-Level Dynamic-Static Decoupling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Yue Wen, Liang Song, Yijia Liu, Siting Zhu, Yanzi Miao, Lijun Han, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2505.09406v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction for autonomous driving enables vehicles to
perceive and interpret complex scene changes more precisely. Dynamic Neural
Radiance Fields (NeRFs) have recently shown promising capability in scene
modeling. However, many existing methods rely heavily on accurate poses inputs
and multi-sensor data, leading to increased system complexity. To address this,
we propose FreeDriveRF, which reconstructs dynamic driving scenes using only
sequential RGB images without requiring poses inputs. We innovatively decouple
dynamic and static parts at the early sampling level using semantic
supervision, mitigating image blurring and artifacts. To overcome the
challenges posed by object motion and occlusion in monocular camera, we
introduce a warped ray-guided dynamic object rendering consistency loss,
utilizing optical flow to better constrain the dynamic modeling process.
Additionally, we incorporate estimated dynamic flow to constrain the pose
optimization process, improving the stability and accuracy of unbounded scene
reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets
demonstrate the superior performance of our method in dynamic scene modeling
for autonomous driving.

Comments:
- 7 pages, 9 figures, accepted by ICRA2025

---

## Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or  Robot Hardware

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Justin Yu, Letian Fu, Huang Huang, Karim El-Refai, Rares Andrei Ambrus, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2505.09601v1){: .btn .btn-green } |

**Abstract**: Scaling robot learning requires vast and diverse datasets. Yet the prevailing
data collection paradigm-human teleoperation-remains costly and constrained by
manual effort and physical robot access. We introduce Real2Render2Real (R2R2R),
a novel approach for generating robot training data without relying on object
dynamics simulation or teleoperation of robot hardware. The input is a
smartphone-captured scan of one or more objects and a single video of a human
demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic
demonstrations by reconstructing detailed 3D object geometry and appearance,
and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to
enable flexible asset generation and trajectory synthesis for both rigid and
articulated objects, converting these representations to meshes to maintain
compatibility with scalable rendering engines like IsaacLab but with collision
modeling off. Robot demonstration data generated by R2R2R integrates directly
with models that operate on robot proprioceptive states and image observations,
such as vision-language-action models (VLA) and imitation learning policies.
Physical experiments suggest that models trained on R2R2R data from a single
human demonstration can match the performance of models trained on 150 human
teleoperation demonstrations. Project page: https://real2render2real.com



---

## Neural Video Compression using 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Lakshya Gupta, Imran N. Junejo | cs.CV | [PDF](http://arxiv.org/pdf/2505.09324v1){: .btn .btn-green } |

**Abstract**: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

Comments:
- 9 pages, 8 figures

---

## ExploreGS: a vision-based low overhead framework for 3D scene  reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Yunji Feng, Chengpu Yu, Fengrui Ran, Zhi Yang, Yinni Liu | eess.IV | [PDF](http://arxiv.org/pdf/2505.10578v1){: .btn .btn-green } |

**Abstract**: This paper proposes a low-overhead, vision-based 3D scene reconstruction
framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces
traditional lidar-based point cloud acquisition process with a vision model,
achieving a high-quality reconstruction at a lower cost. The framework
integrates scene exploration and model reconstruction, and leverags a
Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore,
the 3D Gaussian Splatting (3DGS) training can be executed on-board.
Comprehensive experiments in both simulation and real-world environments
demonstrate the efficiency and applicability of the ExploreGS framework on
resource-constrained devices, while maintaining reconstruction quality
comparable to state-of-the-art methods.



---

## Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-14 | Ma Changfeng, Bi Ran, Guo Jie, Wang Chongjun, Guo Yanwen | cs.CV | [PDF](http://arxiv.org/pdf/2505.09413v1){: .btn .btn-green } |

**Abstract**: Current learning-based methods predict NeRF or 3D Gaussians from point clouds
to achieve photo-realistic rendering but still depend on categorical priors,
dense point clouds, or additional refinements. Hence, we introduce a novel
point cloud rendering method by predicting 2D Gaussians from point clouds. Our
method incorporates two identical modules with an entire-patch architecture
enabling the network to be generalized to multiple datasets. The module
normalizes and initializes the Gaussians utilizing the point cloud information
including normals, colors and distances. Then, splitting decoders are employed
to refine the initial Gaussians by duplicating them and predicting more
accurate results, making our methodology effectively accommodate sparse point
clouds as well. Once trained, our approach exhibits direct generalization to
point clouds across different categories. The predicted Gaussians are employed
directly for rendering without additional refinement on the rendered images,
retaining the benefits of 2D Gaussians. We conduct extensive experiments on
various datasets, and the results demonstrate the superiority and
generalization of our method, which achieves SOTA performance. The code is
available at
https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.

Comments:
- CVPR 2025 Accepted

---

## NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged  Information Guidance


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Wenzhe Cai, Jiaqi Peng, Yuqiang Yang, Yujian Zhang, Meng Wei, Hanqing Wang, Yilun Chen, Tai Wang, Jiangmiao Pang | cs.RO | [PDF](http://arxiv.org/pdf/2505.08712v2){: .btn .btn-green } |

**Abstract**: Learning navigation in dynamic open-world environments is an important yet
challenging skill for robots. Most previous methods rely on precise
localization and mapping or learn from expensive real-world demonstrations. In
this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end
framework trained solely in simulation and can zero-shot transfer to different
embodiments in diverse real-world environments. The key ingredient of NavDP's
network is the combination of diffusion-based trajectory generation and a
critic function for trajectory selection, which are conditioned on only local
observation tokens encoded from a shared policy transformer. Given the
privileged information of the global environment in simulation, we scale up the
demonstrations of good quality to train the diffusion policy and formulate the
critic value function targets with contrastive negative samples. Our
demonstration generation approach achieves about 2,500 trajectories/GPU per
day, 20$\times$ more efficient than real-world data collection, and results in
a large-scale navigation dataset with 363.2km trajectories across 1244 scenes.
Trained with this simulation dataset, NavDP achieves state-of-the-art
performance and consistently outstanding generalization capability on
quadruped, wheeled, and humanoid robots in diverse indoor and outdoor
environments. In addition, we present a preliminary attempt at using Gaussian
Splatting to make in-domain real-to-sim fine-tuning to further bridge the
sim-to-real gap. Experiments show that adding such real-to-sim data can improve
the success rate by 30\% without hurting its generalization capability.

Comments:
- Project Page:
  https://wzcai99.github.io/navigation-diffusion-policy.github.io/

---

## FOCI: Trajectory Optimization on Gaussian Splats

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Mario Gomez Andreu, Maximum Wilder-Smith, Victor Klemm, Vaishakh Patil, Jesus Tordesillas, Marco Hutter | cs.RO | [PDF](http://arxiv.org/pdf/2505.08510v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently gained popularity as a faster
alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view
synthesis methods. Leveraging the spatial information encoded in 3DGS, this
work proposes FOCI (Field Overlap Collision Integral), an algorithm that is
able to optimize trajectories directly on the Gaussians themselves. FOCI
leverages a novel and interpretable collision formulation for 3DGS using the
notion of the overlap integral between Gaussians. Contrary to other approaches,
which represent the robot with conservative bounding boxes that underestimate
the traversability of the environment, we propose to represent the environment
and the robot as Gaussian Splats. This not only has desirable computational
properties, but also allows for orientation-aware planning, allowing the robot
to pass through very tight and narrow spaces. We extensively test our algorithm
in both synthetic and real Gaussian Splats, showcasing that collision-free
trajectories for the ANYmal legged robot that can be computed in a few seconds,
even with hundreds of thousands of Gaussians making up the environment. The
project page and code are available at
https://rffr.leggedrobotics.com/works/foci/

Comments:
- 7 pages, 8 figures, Mario Gomez Andreu and Maximum Wilder-Smith
  contributed equally

---

## A Survey of 3D Reconstruction with Event Cameras: From Event-based  Geometry to Neural 3D Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Ying Zhou, Vera Chung, Qiang Qu | cs.CV | [PDF](http://arxiv.org/pdf/2505.08438v1){: .btn .btn-green } |

**Abstract**: Event cameras have emerged as promising sensors for 3D reconstruction due to
their ability to capture per-pixel brightness changes asynchronously. Unlike
conventional frame-based cameras, they produce sparse and temporally rich data
streams, which enable more accurate 3D reconstruction and open up the
possibility of performing reconstruction in extreme environments such as
high-speed motion, low light, or high dynamic range scenes. In this survey, we
provide the first comprehensive review focused exclusively on 3D reconstruction
using event cameras. The survey categorises existing works into three major
types based on input modality - stereo, monocular, and multimodal systems, and
further classifies them by reconstruction approach, including geometry-based,
deep learning-based, and recent neural rendering techniques such as Neural
Radiance Fields and 3D Gaussian Splatting. Methods with a similar research
focus were organised chronologically into the most subdivided groups. We also
summarise public datasets relevant to event-based 3D reconstruction. Finally,
we highlight current research limitations in data availability, evaluation,
representation, and dynamic scene handling, and outline promising future
research directions. This survey aims to serve as a comprehensive reference and
a roadmap for future developments in event-driven 3D reconstruction.

Comments:
- 35 pages, 12 figures, 11 tables

---

## ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for  Dynamic Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | He Huang, Qi Yang, Mufan Liu, Yiling Xu, Zhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.08196v1){: .btn .btn-green } |

**Abstract**: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from
a canonical space to target frames, which overlooks redundancy among adjacent
Gaussian primitives and results in suboptimal performance. To address this
limitation, we propose Anchor-Driven Deformable and Compressed Gaussian
Splatting (ADC-GS), a compact and efficient representation for dynamic scene
reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an
anchor-based structure within the canonical space, enhanced by a temporal
significance-based anchor refinement strategy. To reduce deformation
redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that
captures motions at varying granularities. Moreover, a rate-distortion
optimization is adopted to achieve an optimal balance between bitrate
consumption and representation fidelity. Experimental results demonstrate that
ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed
by 300%-800% while achieving state-of-the-art storage efficiency without
compromising rendering quality. The code is released at
https://github.com/H-Huang774/ADC-GS.git.



---

## DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-13 | Holly Dinkel, Marcel Büsching, Alberta Longhini, Brian Coltin, Trey Smith, Danica Kragic, Mårten Björkman, Timothy Bretl | cs.CV | [PDF](http://arxiv.org/pdf/2505.08644v1){: .btn .btn-green } |

**Abstract**: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of
Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state
information through prediction-update filtering. The DLO-Splatting algorithm
uses a position-based dynamics model with shape smoothness and rigidity
dampening corrections to predict the object shape. Optimization with a 3D
Gaussian Splatting-based rendering loss iteratively renders and refines the
prediction to align it with the visual observations in the update step. Initial
experiments demonstrate promising results in a knot tying scenario, which is
challenging for existing vision-only methods.

Comments:
- 5 pages, 2 figures, presented at the 2025 5th Workshop: Reflections
  on Representations and Manipulating Deformable Objects at the IEEE
  International Conference on Robotics and Automation. RMDO workshop
  (https://deformable-workshop.github.io/icra2025/)

---

## SLAG: Scalable Language-Augmented Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Laszlo Szilagyi, Francis Engelmann, Jeannette Bohg | cs.CV | [PDF](http://arxiv.org/pdf/2505.08124v1){: .btn .btn-green } |

**Abstract**: Language-augmented scene representations hold great promise for large-scale
robotics applications such as search-and-rescue, smart cities, and mining. Many
of these scenarios are time-sensitive, requiring rapid scene encoding while
also being data-intensive, necessitating scalable solutions. Deploying these
representations on robots with limited computational resources further adds to
the challenge. To address this, we introduce SLAG, a multi-GPU framework for
language-augmented Gaussian splatting that enhances the speed and scalability
of embedding large scenes. Our method integrates 2D visual-language model
features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG
eliminates the need for a loss function to compute per-Gaussian language
embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters
via a normalized weighted average, enabling highly parallelized scene encoding.
Additionally, we introduce a vector database for efficient embedding storage
and retrieval. Our experiments show that SLAG achieves an 18 times speedup in
embedding computation on a 16-GPU setup compared to OpenGaussian, while
preserving embedding quality on the ScanNet and LERF datasets. For more
details, visit our project website: https://slag-project.github.io/.



---

## TUGS: Physics-based Compact Representation of Underwater Scenes by  Tensorized Gaussian

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Shijie Lian, Ziyi Zhang, Laurence Tianruo Yang and, Mengyu Ren, Debin Liu, Hua Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.08811v1){: .btn .btn-green } |

**Abstract**: Underwater 3D scene reconstruction is crucial for undewater robotic
perception and navigation. However, the task is significantly challenged by the
complex interplay between light propagation, water medium, and object surfaces,
with existing methods unable to model their interactions accurately.
Additionally, expensive training and rendering costs limit their practical
application in underwater robotic systems. Therefore, we propose Tensorized
Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling
challenges of the complex interactions between object geometries and water
media while achieving significant parameter reduction. TUGS employs lightweight
tensorized higher-order Gaussians with a physics-based underwater Adaptive
Medium Estimation (AME) module, enabling accurate simulation of both light
attenuation and backscatter effects in underwater environments. Compared to
other NeRF-based and GS-based methods designed for underwater, TUGS is able to
render high-quality underwater images with faster rendering speeds and less
memory usage. Extensive experiments on real-world underwater datasets have
demonstrated that TUGS can efficiently achieve superior reconstruction quality
using a limited number of parameters, making it particularly suitable for
memory-constrained underwater UAV applications



---

## Geometric Prior-Guided Neural Implicit Surface Reconstruction in the  Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Lintao Xiang, Hongpei Zheng, Bailin Deng, Hujun Yin | cs.CV | [PDF](http://arxiv.org/pdf/2505.07373v1){: .btn .btn-green } |

**Abstract**: Neural implicit surface reconstruction using volume rendering techniques has
recently achieved significant advancements in creating high-fidelity surfaces
from multiple 2D images. However, current methods primarily target scenes with
consistent illumination and struggle to accurately reconstruct 3D geometry in
uncontrolled environments with transient occlusions or varying appearances.
While some neural radiance field (NeRF)-based variants can better manage
photometric variations and transient objects in complex scenes, they are
designed for novel view synthesis rather than precise surface reconstruction
due to limited surface constraints. To overcome this limitation, we introduce a
novel approach that applies multiple geometric constraints to the implicit
surface optimization process, enabling more accurate reconstructions from
unconstrained image collections. First, we utilize sparse 3D points from
structure-from-motion (SfM) to refine the signed distance function estimation
for the reconstructed surface, with a displacement compensation to accommodate
noise in the sparse points. Additionally, we employ robust normal priors
derived from a normal predictor, enhanced by edge prior filtering and
multi-view consistency constraints, to improve alignment with the actual
surface geometry. Extensive testing on the Heritage-Recon benchmark and other
datasets has shown that the proposed method can accurately reconstruct surfaces
from in-the-wild images, yielding geometries with superior accuracy and
granularity compared to existing techniques. Our approach enables high-quality
3D reconstruction of various landmarks, making it applicable to diverse
scenarios such as digital preservation of cultural heritage sites.



---

## GIFStream: 4D Gaussian-based Immersive Video with Feature Stream

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Hao Li, Sicheng Li, Xiang Gao, Abudouaihati Batuer, Lu Yu, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2505.07539v1){: .btn .btn-green } |

**Abstract**: Immersive video offers a 6-Dof-free viewing experience, potentially playing a
key role in future video technology. Recently, 4D Gaussian Splatting has gained
attention as an effective approach for immersive video due to its high
rendering efficiency and quality, though maintaining quality with manageable
storage remains challenging. To address this, we introduce GIFStream, a novel
4D Gaussian representation using a canonical space and a deformation field
enhanced with time-dependent feature streams. These feature streams enable
complex motion modeling and allow efficient compression by leveraging temporal
correspondence and motion-aware pruning. Additionally, we incorporate both
temporal and spatial compression networks for end-to-end compression.
Experimental results show that GIFStream delivers high-quality immersive video
at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project
page: https://xdimlab.github.io/GIFStream

Comments:
- 14 pages, 10 figures

---

## TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin  Benchmark Dataset

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-12 | Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2505.07396v2){: .btn .btn-green } |

**Abstract**: Urban Digital Twins (UDTs) have become essential for managing cities and
integrating complex, heterogeneous data from diverse sources. Creating UDTs
involves challenges at multiple process stages, including acquiring accurate 3D
source data, reconstructing high-fidelity 3D models, maintaining models'
updates, and ensuring seamless interoperability to downstream tasks. Current
datasets are usually limited to one part of the processing chain, hampering
comprehensive UDTs validation. To address these challenges, we introduce the
first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.
This dataset includes georeferenced, semantically aligned 3D models and
networks along with various terrestrial, mobile, aerial, and satellite
observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently
767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high
accuracy, and multimodal data integration, the benchmark supports robust
analysis of sensors and the development of advanced reconstruction methods.
Additionally, we explore downstream tasks demonstrating the potential of
TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar
potential analysis, point cloud semantic segmentation, and LoD3 building
reconstruction. We are convinced this contribution lays a foundation for
overcoming current limitations in UDT creation, fostering new research
directions and practical solutions for smarter, data-driven urban environments.
The project is available under: https://tum2t.win

Comments:
- Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing

---

## NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain  Generalization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-11 | Ahmed Qazi, Abdul Basit, Asim Iqbal | cs.CV | [PDF](http://arxiv.org/pdf/2505.06894v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have significantly advanced the field of novel
view synthesis, yet their generalization across diverse scenes and conditions
remains challenging. Addressing this, we propose the integration of a novel
brain-inspired normalization technique Neural Generalization (NeuGen) into
leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts
the domain-invariant features, thereby enhancing the models' generalization
capabilities. It can be seamlessly integrated into NeRF architectures and
cultivates a comprehensive feature set that significantly improves accuracy and
robustness in image rendering. Through this integration, NeuGen shows improved
performance on benchmarks on diverse datasets across state-of-the-art NeRF
architectures, enabling them to generalize better across varied scenes. Our
comprehensive evaluations, both quantitative and qualitative, confirm that our
approach not only surpasses existing models in generalizability but also
markedly improves rendering quality. Our work exemplifies the potential of
merging neuroscientific principles with deep learning frameworks, setting a new
precedent for enhanced generalizability and efficiency in novel view synthesis.
A demo of our study is available at https://neugennerf.github.io.

Comments:
- 18 pages, 6 figures

---

## 3D Characterization of Smoke Plume Dispersion Using Multi-View Drone  Swarm

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Nikil Krishnakumar, Shashank Sharma, Srijan Kumar Pal, Jiarong Hong | cs.RO | [PDF](http://arxiv.org/pdf/2505.06638v1){: .btn .btn-green } |

**Abstract**: This study presents an advanced multi-view drone swarm imaging system for the
three-dimensional characterization of smoke plume dispersion dynamics. The
system comprises a manager drone and four worker drones, each equipped with
high-resolution cameras and precise GPS modules. The manager drone uses image
feedback to autonomously detect and position itself above the plume, then
commands the worker drones to orbit the area in a synchronized circular flight
pattern, capturing multi-angle images. The camera poses of these images are
first estimated, then the images are grouped in batches and processed using
Neural Radiance Fields (NeRF) to generate high-resolution 3D reconstructions of
plume dynamics over time. Field tests demonstrated the ability of the system to
capture critical plume characteristics including volume dynamics, wind-driven
directional shifts, and lofting behavior at a temporal resolution of about 1 s.
The 3D reconstructions generated by this system provide unique field data for
enhancing the predictive models of smoke plume dispersion and fire spread.
Broadly, the drone swarm system offers a versatile platform for high resolution
measurements of pollutant emissions and transport in wildfires, volcanic
eruptions, prescribed burns, and industrial processes, ultimately supporting
more effective fire control decisions and mitigating wildfire risks.

Comments:
- 10 pages, 8 figures

---

## FlexNeRFer: A Multi-Dataflow, Adaptive Sparsity-Aware Accelerator for  On-Device NeRF Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Seock-Hwan Noh, Banseok Shin, Jeik Choi, Seungpyo Lee, Jaeha Kung, Yeseong Kim | cs.AR | [PDF](http://arxiv.org/pdf/2505.06504v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF), an AI-driven approach for 3D view
reconstruction, has demonstrated impressive performance, sparking active
research across fields. As a result, a range of advanced NeRF models has
emerged, leading on-device applications to increasingly adopt NeRF for highly
realistic scene reconstructions. With the advent of diverse NeRF models,
NeRF-based applications leverage a variety of NeRF frameworks, creating the
need for hardware capable of efficiently supporting these models. However, GPUs
fail to meet the performance, power, and area (PPA) cost demanded by these
on-device applications, or are specialized for specific NeRF algorithms,
resulting in lower efficiency when applied to other NeRF models. To address
this limitation, in this work, we introduce FlexNeRFer, an energy-efficient
versatile NeRF accelerator. The key components enabling the enhancement of
FlexNeRFer include: i) a flexible network-on-chip (NoC) supporting
multi-dataflow and sparsity on precision-scalable MAC array, and ii) efficient
data storage using an optimal sparsity format based on the sparsity ratio and
precision modes. To evaluate the effectiveness of FlexNeRFer, we performed a
layout implementation using 28nm CMOS technology. Our evaluation shows that
FlexNeRFer achieves 8.2~243.3x speedup and 24.1~520.3x improvement in energy
efficiency over a GPU (i.e., NVIDIA RTX 2080 Ti), while demonstrating 4.2~86.9x
speedup and 2.3~47.5x improvement in energy efficiency compared to a
state-of-the-art NeRF accelerator (i.e., NeuRex).

Comments:
- Accepted for publication at the 52nd IEEE/ACM International Symposium
  on Computer Architecture (ISCA-52), 2025

---

## Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System  for Real-Time Rendering of Composed Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-10 | Xijie Yang, Linning Xu, Lihan Jiang, Dahua Lin, Bo Dai | cs.GR | [PDF](http://arxiv.org/pdf/2505.06523v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital
3D assets from multi-view images by leveraging a set of 3D Gaussian primitives
for rendering. Its explicit and discrete representation facilitates the
seamless composition of complex digital worlds, offering significant advantages
over previous neural implicit methods. However, when applied to large-scale
compositions, such as crowd-level scenes, it can encompass numerous 3D
Gaussians, posing substantial challenges for real-time rendering. To address
this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D
Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D
Gaussian clusters and dynamically selects only the necessary ones to accelerate
rendering speed. Our approach consists of two stages: (1) Offline Build, where
hierarchical clusters are generated using a local splatting method to minimize
visual differences across granularities, and (2) Online Selection, where
footprint evaluation determines perceptible clusters for efficient
rasterization during rendering. We curate a dataset of synthetic and real-world
scenes, including objects, trees, people, and buildings, each requiring 0.1
billion 3D Gaussians to capture fine details. Experiments show that our
solution balances rendering efficiency and visual quality across user-defined
tolerances, facilitating downstream interactive applications that compose
extensive 3DGS assets for consistent rendering performance.

Comments:
- project page: https://xijie-yang.github.io/V3DG/

---

## 3D Scene Generation: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2505.05474v1){: .btn .btn-green } |

**Abstract**: 3D scene generation seeks to synthesize spatially structured, semantically
meaningful, and photorealistic environments for applications such as immersive
media, robotics, autonomous driving, and embodied AI. Early methods based on
procedural rules offered scalability but limited diversity. Recent advances in
deep generative models (e.g., GANs, diffusion models) and 3D representations
(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene
distributions, improving fidelity, diversity, and view consistency. Recent
advances like diffusion models bridge 3D scene synthesis and photorealism by
reframing generation as image or video synthesis problems. This survey provides
a systematic overview of state-of-the-art approaches, organizing them into four
paradigms: procedural generation, neural 3D-based generation, image-based
generation, and video-based generation. We analyze their technical foundations,
trade-offs, and representative results, and review commonly used datasets,
evaluation protocols, and downstream applications. We conclude by discussing
key challenges in generation capacity, 3D representation, data and annotations,
and evaluation, and outline promising directions including higher fidelity,
physics-aware and interactive generation, and unified perception-generation
models. This review organizes recent advances in 3D scene generation and
highlights promising directions at the intersection of generative AI, 3D
vision, and embodied intelligence. To track ongoing developments, we maintain
an up-to-date project page:
https://github.com/hzxie/Awesome-3D-Scene-Generation.

Comments:
- Project Page: https://github.com/hzxie/Awesome-3D-Scene-Generation

---

## TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Gengyan Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2505.05672v1){: .btn .btn-green } |

**Abstract**: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have
recently enabled animatable 3D head avatars that are rendered under arbitrary
viewpoints with impressive photorealism. Today, such photoreal avatars are seen
as a key component in emerging applications in telepresence, extended reality,
and entertainment. Building a photoreal avatar requires estimating the complex
non-rigid motion of different facial components as seen in input video images;
due to inaccurate motion estimation, animatable models typically present a loss
of fidelity and detail when compared to their non-animatable counterparts,
built from an individual facial expression. Also, recent state-of-the-art
models are often affected by memory limitations that reduce the number of 3D
Gaussians used for modeling, leading to lower detail and quality. To address
these problems, we present a new high-detail 3D head avatar model that improves
upon the state of the art, largely increasing the number of 3D Gaussians and
modeling quality for rendering at 4K resolution. Our high-quality model is
reconstructed from multiview input video and builds on top of a mesh-based 3D
morphable model, which provides a coarse deformation layer for the head.
Photoreal appearance is modelled by 3D Gaussians embedded within the continuous
UVD tangent space of this mesh, allowing for more effective densification where
most needed. Additionally, these Gaussians are warped by a novel UVD
deformation field to capture subtle, localized motion. Our key contribution is
the novel deformable Gaussian encoding and overall fitting procedure that
allows our head model to preserve appearance detail, while capturing facial
motion and other transient high-frequency features such as skin wrinkling.

Comments:
- 10 pages, 9 figures, supplementary results found at:
  https://syntec-research.github.io/UVGA/, to be published in SIGGRAPH 2025

---

## QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian  Initialization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Yueh-Cheng Liu, Lukas Höllein, Matthias Nießner, Angela Dai | cs.CV | [PDF](http://arxiv.org/pdf/2505.05591v1){: .btn .btn-green } |

**Abstract**: Surface reconstruction is fundamental to computer vision and graphics,
enabling applications in 3D modeling, mixed reality, robotics, and more.
Existing approaches based on volumetric rendering obtain promising results, but
optimize on a per-scene basis, resulting in a slow optimization that can
struggle to model under-observed or textureless regions. We introduce
QuickSplat, which learns data-driven priors to generate dense initializations
for 2D gaussian splatting optimization of large-scale indoor scenes. This
provides a strong starting point for the reconstruction, which accelerates the
convergence of the optimization and improves the geometry of flat wall
structures. We further learn to jointly estimate the densification and update
of the scene parameters during each iteration; our proposed densifier network
predicts new Gaussians based on the rendering gradients of existing ones,
removing the needs of heuristics for densification. Extensive experiments on
large-scale indoor scene reconstruction demonstrate the superiority of our
data-driven optimization. Concretely, we accelerate runtime by 8x, while
decreasing depth errors by up to 48% in comparison to state of the art methods.

Comments:
- Project page: https://liu115.github.io/quicksplat, Video:
  https://youtu.be/2IA_gnFvFG8

---

## UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Mark C. Eid, Ana I. L. Namburete, João F. Henriques | eess.IV | [PDF](http://arxiv.org/pdf/2505.05643v1){: .btn .btn-green } |

**Abstract**: Ultrasound imaging is widely used due to its safety, affordability, and
real-time capabilities, but its 2D interpretation is highly operator-dependent,
leading to variability and increased cognitive demand. 2D-to-3D reconstruction
mitigates these challenges by providing standardized volumetric views, yet
existing methods are often computationally expensive, memory-intensive, or
incompatible with ultrasound physics. We introduce UltraGauss: the first
ultrasound-specific Gaussian Splatting framework, extending view synthesis
techniques to ultrasound wave propagation. Unlike conventional
perspective-based splatting, UltraGauss models probe-plane intersections in 3D,
aligning with acoustic image formation. We derive an efficient rasterization
boundary formulation for GPU parallelization and introduce a numerically stable
covariance parametrization, improving computational efficiency and
reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves
state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20
minutes on a single GPU. A survey of expert clinicians confirms UltraGauss'
reconstructions are the most realistic among competing methods. Our CUDA
implementation will be released upon publication.



---

## SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with  Video Diffusion and Data Augmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Yonwoo Choi | cs.CV | [PDF](http://arxiv.org/pdf/2505.05475v1){: .btn .btn-green } |

**Abstract**: Creating high-quality animatable 3D human avatars from a single image remains
a significant challenge in computer vision due to the inherent difficulty of
reconstructing complete 3D information from a single viewpoint. Current
approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods
produce high-quality results but require multiple views or video sequences,
while video diffusion models can generate animations from single images but
struggle with consistency and identity preservation. We present SVAD, a novel
approach that addresses these limitations by leveraging complementary strengths
of existing techniques. Our method generates synthetic training data through
video diffusion, enhances it with identity preservation and image restoration
modules, and utilizes this refined data to train 3DGS avatars. Comprehensive
evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)
single-image methods in maintaining identity consistency and fine details
across novel poses and viewpoints, while enabling real-time rendering
capabilities. Through our data augmentation pipeline, we overcome the
dependency on dense monocular or multi-view training data typically required by
traditional 3DGS approaches. Extensive quantitative, qualitative comparisons
show our method achieves superior performance across multiple metrics against
baseline models. By effectively combining the generative power of diffusion
models with both the high-quality results and rendering efficiency of 3DGS, our
work establishes a new approach for high-fidelity avatar generation from a
single image input.

Comments:
- Accepted by CVPR 2025 SyntaGen Workshop, Project Page:
  https://yc4ny.github.io/SVAD/

---

## Time of the Flight of the Gaussians: Optimizing Depth Indirectly in  Dynamic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Runfeng Li, Mikhail Okunev, Zixuan Guo, Anh Ha Duong, Christian Richardt, Matthew O'Toole, James Tompkin | cs.GR | [PDF](http://arxiv.org/pdf/2505.05356v1){: .btn .btn-green } |

**Abstract**: We present a method to reconstruct dynamic scenes from monocular
continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that
achieves similar or better accuracy than neural volumetric approaches and is
100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a
single viewpoint is a significant challenge in computer vision. In C-ToF
radiance field reconstruction, the property of interest-depth-is not directly
measured, causing an additional challenge. This problem has a large and
underappreciated impact upon the optimization when using a fast primitive-based
scene representation like 3D Gaussian splatting, which is commonly used with
multi-view data to produce satisfactory results and is brittle in its
optimization otherwise. We incorporate two heuristics into the optimization to
improve the accuracy of scene geometry represented by Gaussians. Experimental
results show that our approach produces accurate reconstructions under
constrained C-ToF sensing conditions, including for fast motions like swinging
baseball bats. https://visual.cs.brown.edu/gftorf



---

## Steepest Descent Density Control for Compact 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-08 | Peihao Wang, Yuehao Wang, Dilin Wang, Sreyas Mohan, Zhiwen Fan, Lemeng Wu, Ruisi Cai, Yu-Ying Yeh, Zhangyang Wang, Qiang Liu, Rakesh Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2505.05587v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for
real-time, high-resolution novel view synthesis. By representing scenes as a
mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for
efficient rendering and reconstruction. To optimize scene coverage and capture
fine details, 3DGS employs a densification algorithm to generate additional
points. However, this process often leads to redundant point clouds, resulting
in excessive memory usage, slower performance, and substantial storage demands
- posing significant challenges for deployment on resource-constrained devices.
To address this limitation, we propose a theoretical framework that demystifies
and improves density control in 3DGS. Our analysis reveals that splitting is
crucial for escaping saddle points. Through an optimization-theoretic approach,
we establish the necessary conditions for densification, determine the minimal
number of offspring Gaussians, identify the optimal parameter update direction,
and provide an analytical solution for normalizing off-spring opacity. Building
on these insights, we introduce SteepGS, incorporating steepest density
control, a principled strategy that minimizes loss while maintaining a compact
point cloud. SteepGS achieves a ~50% reduction in Gaussian points without
compromising rendering quality, significantly enhancing both efficiency and
scalability.

Comments:
- CVPR 2025, Project page: https://vita-group.github.io/SteepGS/

---

## SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Xinran Yang, Donghao Ji, Yuanqi Li, Jie Guo, Yanwen Guo, Junyuan Xie | cs.GR | [PDF](http://arxiv.org/pdf/2505.04668v1){: .btn .btn-green } |

**Abstract**: Neural rendering techniques have made substantial progress in generating
photo-realistic 3D scenes. The latest 3D Gaussian Splatting technique has
achieved high quality novel view synthesis as well as fast rendering speed.
However, 3D Gaussians lack proficiency in defining accurate 3D geometric
structures despite their explicit primitive representations. This is due to the
fact that Gaussian's attributes are primarily tailored and fine-tuned for
rendering diverse 2D images by their anisotropic nature. To pave the way for
efficient 3D reconstruction, we present Spherical Gaussians, a simple and
effective representation for 3D geometric boundaries, from which we can
directly reconstruct 3D feature curves from a set of calibrated multi-view
images. Spherical Gaussians is optimized from grid initialization with a
view-based rendering loss, where a 2D edge map is rendered at a specific view
and then compared to the ground-truth edge map extracted from the corresponding
image, without the need for any 3D guidance or supervision. Given Spherical
Gaussians serve as intermedia for the robust edge representation, we further
introduce a novel optimization-based algorithm called SGCR to directly extract
accurate parametric curves from aligned Spherical Gaussians. We demonstrate
that SGCR outperforms existing state-of-the-art methods in 3D edge
reconstruction while enjoying great efficiency.

Comments:
- The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2025, 8 pages

---

## Bridging Geometry-Coherent Text-to-3D Generation with Multi-View  Diffusion Priors and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Feng Yang, Wenliang Qian, Wangmeng Zuo, Hui Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.04262v1){: .btn .btn-green } |

**Abstract**: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to
advance text-to-3D generation but neglects multi-view correlations, being prone
to geometric inconsistencies and multi-face artifacts in the generated 3D
content. In this work, we propose Coupled Score Distillation (CSD), a framework
that couples multi-view joint distribution priors to ensure geometrically
consistent 3D generation while enabling the stable and direct optimization of
3D Gaussian Splatting. Specifically, by reformulating the optimization as a
multi-view joint optimization problem, we derive an effective optimization rule
that effectively couples multi-view priors to guide optimization across
different viewpoints while preserving the diversity of generated 3D assets.
Additionally, we propose a framework that directly optimizes 3D Gaussian
Splatting (3D-GS) with random initialization to generate geometrically
consistent 3D content. We further employ a deformable tetrahedral grid,
initialized from 3D-GS and refined through CSD, to produce high-quality,
refined meshes. Quantitative and qualitative experimental results demonstrate
the efficiency and competitive quality of our approach.



---

## GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view  Synthesis in 3D Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-07 | Feng Xiao, Hongbin Xu, Wanlin Liang, Wenxiong Kang | cs.GR | [PDF](http://arxiv.org/pdf/2505.04659v1){: .btn .btn-green } |

**Abstract**: The semantic synthesis of unseen scenes from multiple viewpoints is crucial
for research in 3D scene understanding. Current methods are capable of
rendering novel-view images and semantic maps by reconstructing generalizable
Neural Radiance Fields. However, they often suffer from limitations in speed
and segmentation performance. We propose a generalizable semantic Gaussian
Splatting method (GSsplat) for efficient novel-view synthesis. Our model
predicts the positions and attributes of scene-adaptive Gaussian distributions
from once input, replacing the densification and pruning processes of
traditional scene-specific Gaussian Splatting. In the multi-task framework, a
hybrid network is designed to extract color and semantic information and
predict Gaussian parameters. To augment the spatial perception of Gaussians for
high-quality rendering, we put forward a novel offset learning module through
group-based supervision and a point-level interaction module with spatial unit
aggregation. When evaluated with varying numbers of multi-view inputs, GSsplat
achieves state-of-the-art performance for semantic synthesis at the fastest
speed.



---

## 3D Gaussian Splatting Data Compression with Mixture of Priors

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-06 | Lei Liu, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.03310v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.



---

## HandOcc: NeRF-based Hand Rendering with Occupancy Networks

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2505.02079v1){: .btn .btn-green } |

**Abstract**: We propose HandOcc, a novel framework for hand rendering based upon
occupancy. Popular rendering methods such as NeRF are often combined with
parametric meshes to provide deformable hand models. However, in doing so, such
approaches present a trade-off between the fidelity of the mesh and the
complexity and dimensionality of the parametric model. The simplicity of
parametric mesh structures is appealing, but the underlying issue is that it
binds methods to mesh initialization, making it unable to generalize to objects
where a parametric model does not exist. It also means that estimation is tied
to mesh resolution and the accuracy of mesh fitting. This paper presents a
pipeline for meshless 3D rendering, which we apply to the hands. By providing
only a 3D skeleton, the desired appearance is extracted via a convolutional
model. We do this by exploiting a NeRF renderer conditioned upon an
occupancy-based representation. The approach uses the hand occupancy to resolve
hand-to-hand interactions further improving results, allowing fast rendering,
and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,
we achieved state-of-the-art results.



---

## Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2505.02005v1){: .btn .btn-green } |

**Abstract**: Recent NeRF methods on large-scale scenes have underlined the importance of
scene decomposition for scalable NeRFs. Although achieving reasonable
scalability, there are several critical problems remaining unexplored, i.e.,
learnable decomposition, modeling scene heterogeneity, and modeling efficiency.
In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash
Experts (HMoHE) network that addresses these challenges within a unified
framework. It is a highly scalable NeRF that learns heterogeneous decomposition
and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end
manner. In our framework, a gating network learns to decomposes scenes and
allocates 3D points to specialized NeRF experts. This gating network is
co-optimized with the experts, by our proposed Sparsely Gated Mixture of
Experts (MoE) NeRF framework. We incorporate a hash-based gating network and
distinct heterogeneous hash experts. The hash-based gating efficiently learns
the decomposition of the large-scale scene. The distinct heterogeneous hash
experts consist of hash grids of different resolution ranges, enabling
effective learning of the heterogeneous representation of different scene
parts. These design choices make our framework an end-to-end and highly
scalable NeRF solution for real-world large-scale scene modeling to achieve
both quality and efficiency. We evaluate our accuracy and scalability on
existing large-scale NeRF datasets and a new dataset with very large-scale
scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our
approach can be easily scaled to various large-scale scenes and achieve
state-of-the-art scene rendering accuracy. Furthermore, our method exhibits
significant efficiency, with an 8x acceleration in training and a 16x
acceleration in rendering compared to Switch-NeRF. Codes will be released in
https://github.com/MiZhenxing/Switch-NeRF.

Comments:
- 15 pages, 9 figures

---

## GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity  Non-Watertight 3D Garment Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2505.02126v1){: .btn .btn-green } |

**Abstract**: Traditional 3D garment creation requires extensive manual operations,
resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved
breakthrough progress in 3D scene reconstruction and rendering, attracting
widespread attention and opening new pathways for 3D garment reconstruction.
However, due to the unstructured and irregular nature of Gaussian primitives,
it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In
this paper, we present GarmentGS, a dense point cloud-guided method that can
reconstruct high-fidelity garment surfaces with high geometric accuracy and
generate non-watertight, single-layer meshes. Our method introduces a fast
dense point cloud reconstruction module that can complete garment point cloud
reconstruction in 10 minutes, compared to traditional methods that require
several hours. Furthermore, we use dense point clouds to guide the movement,
flattening, and rotation of Gaussian primitives, enabling better distribution
on the garment surface to achieve superior rendering effects and geometric
accuracy. Through numerical and visual comparisons, our method achieves fast
training and real-time rendering while maintaining competitive quality.



---

## SignSplat: Rendering Sign Language via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2505.02108v1){: .btn .btn-green } |

**Abstract**: State-of-the-art approaches for conditional human body rendering via Gaussian
splatting typically focus on simple body motions captured from many views. This
is often in the context of dancing or walking. However, for more complex use
cases, such as sign language, we care less about large body motion and more
about subtle and complex motions of the hands and face. The problems of
building high fidelity models are compounded by the complexity of capturing
multi-view data of sign. The solution is to make better use of sequence data,
ensuring that we can overcome the limited information from only a few views by
exploiting temporal variability. Nevertheless, learning from sequence-level
data requires extremely accurate and consistent model fitting to ensure that
appearance is consistent across complex motions. We focus on how to achieve
this, constraining mesh parameters to build an accurate Gaussian splatting
framework from few views capable of modelling subtle human motion. We leverage
regularization techniques on the Gaussian parameters to mitigate overfitting
and rendering artifacts. Additionally, we propose a new adaptive control method
to densify Gaussians and prune splat points on the mesh surface. To demonstrate
the accuracy of our approach, we render novel sequences of sign language video,
building on neural machine translation approaches to sign stitching. On
benchmark datasets, our approach achieves state-of-the-art performance; and on
highly articulated and complex sign language motion, we significantly
outperform competing approaches.



---

## SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Shubhendu Jena, Shishir Reddy Vutukur, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2505.02175v1){: .btn .btn-green } |

**Abstract**: Recovering 3D information from scenes via multi-view stereo reconstruction
(MVS) and novel view synthesis (NVS) is inherently challenging, particularly in
scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting
(3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian
Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive
rasterization to achieve accurate geometry representation during rendering,
improving 3D scene reconstruction while maintaining real-time performance.
Recent approaches have tackled the problem of sparse real-time NVS using 3DGS
within a generalizable, MVS-based learning framework to regress 3D Gaussian
parameters. Our work extends this line of research by addressing the challenge
of generalizable sparse 3D reconstruction and NVS jointly, and manages to
perform successfully at both tasks. We propose an MVS-based learning pipeline
that regresses 2DGS surface element parameters in a feed-forward fashion to
perform 3D shape reconstruction and NVS from sparse-view images. We further
show that our generalizable pipeline can benefit from preexisting foundational
multi-view deep visual features. The resulting model attains the
state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms
of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also
demonstrates strong generalization on the BlendedMVS and Tanks and Temples
datasets. We note that our model outperforms the prior state-of-the-art in
feed-forward sparse view reconstruction based on volume rendering of implicit
representations, while offering an almost 2 orders of magnitude higher
inference speed.

Comments:
- Project page : https://shubhendu-jena.github.io/SparSplat/

---

## Sparfels: Fast Reconstruction from Sparse Unposed Imagery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-04 | Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2505.02178v1){: .btn .btn-green } |

**Abstract**: We present a method for Sparse view reconstruction with surface element
splatting that runs within 3 minutes on a consumer grade GPU. While few methods
address sparse radiance field learning from noisy or unposed sparse cameras,
shape recovery remains relatively underexplored in this setting. Several
radiance and shape learning test-time optimization methods address the sparse
posed setting by learning data priors or using combinations of external
monocular geometry priors. Differently, we propose an efficient and simple
pipeline harnessing a single recent 3D foundation model. We leverage its
various task heads, notably point maps and camera initializations to
instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image
correspondences to guide camera optimization midst 2DGS training. Key to our
contribution is a novel formulation of splatted color variance along rays,
which can be computed efficiently. Reducing this moment in training leads to
more accurate shape reconstructions. We demonstrate state-of-the-art
performances in the sparse uncalibrated setting in reconstruction and novel
view benchmarks based on established multi-view datasets.

Comments:
- Project page : https://shubhendu-jena.github.io/Sparfels/

---

## Unified Steganography via Implicit Neural Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Qi Song, Ziyuan Luo, Xiufeng Huang, Sheng Li, Renjie Wan | cs.CR | [PDF](http://arxiv.org/pdf/2505.01749v1){: .btn .btn-green } |

**Abstract**: Digital steganography is the practice of concealing for encrypted data
transmission. Typically, steganography methods embed secret data into cover
data to create stega data that incorporates hidden secret data. However,
steganography techniques often require designing specific frameworks for each
data type, which restricts their generalizability. In this paper, we present
U-INR, a novel method for steganography via Implicit Neural Representation
(INR). Rather than using the specific framework for each data format, we
directly use the neurons of the INR network to represent the secret data and
cover data across different data types. To achieve this idea, a private key is
shared between the data sender and receivers. Such a private key can be used to
determine the position of secret data in INR networks. To effectively leverage
this key, we further introduce a key-based selection strategy that can be used
to determine the position within the INRs for data storage. Comprehensive
experiments across multiple data types, including images, videos, audio, and
SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR,
emphasizing its potential for improving data security and privacy in various
applications.



---

## HybridGS: High-Efficiency Gaussian Splatting Data Compression using  Dual-Channel Sparse Representation and Point Cloud Encoder

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Qi Yang, Le Yang, Geert Van Der Auwera, Zhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2505.01938v1){: .btn .btn-green } |

**Abstract**: Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on
producing compact 3DGS representation via implicit data embedding. They have
long coding times and highly customized data format, making it difficult for
widespread deployment. This paper presents a new 3DGS compression framework
called HybridGS, which takes advantage of both compact generation and
standardized point cloud data encoding. HybridGS first generates compact and
explicit 3DGS data. A dual-channel sparse representation is introduced to
supervise the primitive position and feature bit depth. It then utilizes a
canonical point cloud encoder to perform further data compression and form
standard output bitstreams. A simple and effective rate control scheme is
proposed to pivot the interpretable data compression scheme. At the current
stage, HybridGS does not include any modules aimed at improving 3DGS quality
during generation. But experiment results show that it still provides
comparable reconstruction performance against state-of-the-art methods, with
evidently higher encoding and decoding speed. The code is publicly available at
https://github.com/Qi-Yangsjtu/HybridGS.

Comments:
- Accepted by ICML2025

---

## Visual enhancement and 3D representation for underwater scenes: a review


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Guoxi Huang, Haoran Wang, Brett Seymour, Evan Kovacs, John Ellerbrock, Dave Blackham, Nantheera Anantrasirichai | cs.CV | [PDF](http://arxiv.org/pdf/2505.01869v1){: .btn .btn-green } |

**Abstract**: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose
significant challenges in
  computer vision and AI-based tasks due to complex imaging conditions in
aquatic environments. Despite
  the development of numerous enhancement algorithms, a comprehensive and
systematic review covering both
  UVE and underwater 3D reconstruction remains absent. To advance research in
these areas, we present an
  in-depth review from multiple perspectives. First, we introduce the
fundamental physical models, highlighting the
  peculiarities that challenge conventional techniques. We survey advanced
methods for visual enhancement and
  3D reconstruction specifically designed for underwater scenarios. The paper
assesses various approaches from
  non-learning methods to advanced data-driven techniques, including Neural
Radiance Fields and 3D Gaussian
  Splatting, discussing their effectiveness in handling underwater distortions.
Finally, we conduct both quantitative
  and qualitative evaluations of state-of-the-art UVE and underwater 3D
reconstruction algorithms across multiple
  benchmark datasets. Finally, we highlight key research directions for future
advancements in underwater vision.



---

## GenSync: A Generalized Talking Head Framework for Audio-driven  Multi-Subject Lip-Sync using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Anushka Agarwal, Muhammad Yusuf Hassan, Talha Chafekar | cs.CV | [PDF](http://arxiv.org/pdf/2505.01928v1){: .btn .btn-green } |

**Abstract**: We introduce GenSync, a novel framework for multi-identity lip-synced video
synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that
require training a new model for each identity , GenSync learns a unified
network that synthesizes lip-synced videos for multiple speakers. By
incorporating a Disentanglement Module, our approach separates
identity-specific features from audio representations, enabling efficient
multi-identity video synthesis. This design reduces computational overhead and
achieves 6.8x faster training compared to state-of-the-art models, while
maintaining high lip-sync accuracy and visual quality.



---

## AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-03 | Junhao Shi, Jisheng Xu, Jianping He, Zhiliang Lin | cs.CV | [PDF](http://arxiv.org/pdf/2505.01799v1){: .btn .btn-green } |

**Abstract**: Underwater scene reconstruction is a critical tech-nology for underwater
operations, enabling the generation of 3D models from images captured by
underwater platforms. However, the quality of underwater images is often
degraded due to medium interference, which limits the effectiveness of
Structure-from-Motion (SfM) pose estimation, leading to subsequent
reconstruction failures. Additionally, SfM methods typically operate at slower
speeds, further hindering their applicability in real-time scenarios. In this
paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model
based on the SeaThru algorithm, which facilitates rapid and accurate separation
of scene details and medium features. Our approach initializes Gaussians by
integrating state-of-the-art multi-view stereo (MVS) technology, employs
implicit Neural Radiance Fields (NeRF) for rendering translucent media and
utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render
object surfaces, which effectively addresses the limitations of traditional
methods and accurately simulates underwater optical phenomena. Experimental
results on the data set and the robot platform show that our model can complete
high-precision reconstruction in 30 seconds with only 3 image inputs,
significantly enhancing the practical application of the algorithm in robotic
platforms.



---

## FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft  Research

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-02 | Yan Miao, Will Shen, Hang Cui, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2505.01383v1){: .btn .btn-green } |

**Abstract**: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.



---

## Compensating Spatiotemporally Inconsistent Observations for Online  Dynamic 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-02 | Youngsik Yun, Jeongmin Bae, Hyunseung Son, Seoha Kim, Hahyun Lee, Gun Bang, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2505.01235v1){: .btn .btn-green } |

**Abstract**: Online reconstruction of dynamic scenes is significant as it enables learning
scenes from live-streaming video inputs, while existing offline dynamic
reconstruction methods rely on recorded video inputs. However, previous online
reconstruction approaches have primarily focused on efficiency and rendering
quality, overlooking the temporal consistency of their results, which often
contain noticeable artifacts in static regions. This paper identifies that
errors such as noise in real-world recordings affect temporal inconsistency in
online reconstruction. We propose a method that enhances temporal consistency
in online reconstruction from observations with temporal inconsistency which is
inevitable in cameras. We show that our method restores the ideal observation
by subtracting the learned error. We demonstrate that applying our method to
various baselines significantly enhances both temporal consistency and
rendering quality across datasets. Code, video results, and checkpoints are
available at https://bbangsik13.github.io/OR2.

Comments:
- SIGGRAPH 2025, Project page: https://bbangsik13.github.io/OR2

---

## Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique  Instances in Open-Vocabulary 3D Panoptic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Feng Xue, Wenzhuang Xu, Guofeng Zhong, Anlong Minga, Nicu Sebe | cs.CV | [PDF](http://arxiv.org/pdf/2505.00378v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D panoptic segmentation has recently emerged as a
significant trend. Top-performing methods currently integrate 2D segmentation
with geometry-aware 3D primitives. However, the advantage would be lost without
high-fidelity 3D point clouds, such as methods based on Neural Radiance Field
(NeRF). These methods are limited by the insufficient capacity to maintain
consistency across partial observations. To address this, recent works have
utilized contrastive loss or cross-view association pre-processing for view
consensus. In contrast to them, we present Cues3D, a compact approach that
relies solely on NeRF instead of pre-associations. The core idea is that NeRF's
implicit 3D field inherently establishes a globally consistent geometry,
enabling effective object distinction without explicit cross-view supervision.
We propose a three-phase training framework for NeRF,
initialization-disambiguation-refinement, whereby the instance IDs are
corrected using the initially-learned knowledge. Additionally, an instance
disambiguation method is proposed to match NeRF-rendered 3D masks and ensure
globally unique 3D instance identities. With the aid of Cues3D, we obtain
highly consistent and unique 3D instance ID for each object across views with a
balanced version of NeRF. Our experiments are conducted on ScanNet v2,
ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and
semantic segmentation tasks. Cues3D outperforms other 2D image-based methods
and competes with the latest 2D-3D merging based methods, while even surpassing
them when using additional 3D point clouds. The code link could be found in the
appendix and will be released on
\href{https://github.com/mRobotit/Cues3D}{github}

Comments:
- Accepted by Information Fusion

---

## Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing | cs.CV | [PDF](http://arxiv.org/pdf/2505.00421v1){: .btn .btn-green } |

**Abstract**: High-quality, animatable 3D human avatar reconstruction from monocular videos
offers significant potential for reducing reliance on complex hardware, making
it highly practical for applications in game development, augmented reality,
and social media. However, existing methods still face substantial challenges
in capturing fine geometric details and maintaining animation stability,
particularly under dynamic or complex poses. To address these issues, we
propose a novel real-time framework for animatable human avatar reconstruction
based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose
parameters, our framework not only aligns positional and rotational
discrepancies but also enables robust and natural pose-driven animation of the
reconstructed avatars. Furthermore, we introduce a Rotation Compensation
Network (RCN) that learns rotation residuals by integrating local geometric
features with global pose parameters. This network significantly improves the
handling of non-rigid deformations and ensures smooth, artifact-free pose
transitions during animation. Experimental results demonstrate that our method
successfully reconstructs realistic and highly animatable human avatars from
monocular videos, effectively preserving fine-grained details while ensuring
stable and natural pose variation. Our approach surpasses current
state-of-the-art methods in both reconstruction quality and animation
robustness on public benchmarks.


