---
layout: default
title: May 2025
parent: Papers
nav_order: 202505
---

<!---metadata--->


## Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Xia Yuan, Hai Yuan, Wenyi Ge, Ying Fu, Xi Wu, Guanyu Xing | cs.CV | [PDF](http://arxiv.org/pdf/2505.00421v1){: .btn .btn-green } |

**Abstract**: High-quality, animatable 3D human avatar reconstruction from monocular videos
offers significant potential for reducing reliance on complex hardware, making
it highly practical for applications in game development, augmented reality,
and social media. However, existing methods still face substantial challenges
in capturing fine geometric details and maintaining animation stability,
particularly under dynamic or complex poses. To address these issues, we
propose a novel real-time framework for animatable human avatar reconstruction
based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose
parameters, our framework not only aligns positional and rotational
discrepancies but also enables robust and natural pose-driven animation of the
reconstructed avatars. Furthermore, we introduce a Rotation Compensation
Network (RCN) that learns rotation residuals by integrating local geometric
features with global pose parameters. This network significantly improves the
handling of non-rigid deformations and ensures smooth, artifact-free pose
transitions during animation. Experimental results demonstrate that our method
successfully reconstructs realistic and highly animatable human avatars from
monocular videos, effectively preserving fine-grained details while ensuring
stable and natural pose variation. Our approach surpasses current
state-of-the-art methods in both reconstruction quality and animation
robustness on public benchmarks.



---

## Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique  Instances in Open-Vocabulary 3D Panoptic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-05-01 | Feng Xue, Wenzhuang Xu, Guofeng Zhong, Anlong Minga, Nicu Sebe | cs.CV | [PDF](http://arxiv.org/pdf/2505.00378v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary 3D panoptic segmentation has recently emerged as a
significant trend. Top-performing methods currently integrate 2D segmentation
with geometry-aware 3D primitives. However, the advantage would be lost without
high-fidelity 3D point clouds, such as methods based on Neural Radiance Field
(NeRF). These methods are limited by the insufficient capacity to maintain
consistency across partial observations. To address this, recent works have
utilized contrastive loss or cross-view association pre-processing for view
consensus. In contrast to them, we present Cues3D, a compact approach that
relies solely on NeRF instead of pre-associations. The core idea is that NeRF's
implicit 3D field inherently establishes a globally consistent geometry,
enabling effective object distinction without explicit cross-view supervision.
We propose a three-phase training framework for NeRF,
initialization-disambiguation-refinement, whereby the instance IDs are
corrected using the initially-learned knowledge. Additionally, an instance
disambiguation method is proposed to match NeRF-rendered 3D masks and ensure
globally unique 3D instance identities. With the aid of Cues3D, we obtain
highly consistent and unique 3D instance ID for each object across views with a
balanced version of NeRF. Our experiments are conducted on ScanNet v2,
ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and
semantic segmentation tasks. Cues3D outperforms other 2D image-based methods
and competes with the latest 2D-3D merging based methods, while even surpassing
them when using additional 3D point clouds. The code link could be found in the
appendix and will be released on
\href{https://github.com/mRobotit/Cues3D}{github}

Comments:
- Accepted by Information Fusion
