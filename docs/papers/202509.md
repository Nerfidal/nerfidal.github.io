---
layout: default
title: September 2025
parent: Papers
nav_order: 202509
---

<!---metadata--->


## Gaussian splatting holography

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-25 | Shuhe Zhang, Liangcai Cao | physics.optics | [PDF](http://arxiv.org/pdf/2509.20774v1){: .btn .btn-green } |

**Abstract**: In-line holography offers high space-bandwidth product imaging with a
simplified lens-free optical system. However, in-line holographic
reconstruction is troubled by twin images arising from the Hermitian symmetry
of complex fields. Twin images disrupt the reconstruction in solving the
ill-posed phase retrieval problem. The known parameters are less than the
unknown parameters, causing phase ambiguities. State-of-the-art deep-learning
or non-learning methods face challenges in balancing data fidelity with
twin-image disturbance. We propose the Gaussian splatting holography (GSH) for
twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting
for optical field representation and compresses the number of unknown
parameters by a maximum of 15 folds, transforming the original ill-posed phase
retrieval into a well-posed one with reduced phase ambiguities. Additionally,
the Gaussian splatting tends to form sharp patterns rather than those with
noisy twin-image backgrounds as each Gaussian has a spatially slow-varying
profile. Experiments show that GSH achieves constraint-free recovery for
in-line holography with accuracy comparable to state-of-the-art
constraint-based methods, with an average peak signal-to-noise ratio equal to
26 dB, and structure similarity equal to 0.8. Combined with total variation,
GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB,
and a high compression ability of up to 15 folds.



---

## 4D Driving Scene Generation With Stereo Forcing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-24 | Hao Lu, Zhuang Ma, Guangfeng Jiang, Wenhang Ge, Bohan Li, Yuzhan Cai, Wenzhao Zheng, Yunpeng Zhang, Yingcong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2509.20251v1){: .btn .btn-green } |

**Abstract**: Current generative models struggle to synthesize dynamic 4D driving scenes
that simultaneously support temporal extrapolation and spatial novel view
synthesis (NVS) without per-scene optimization. Bridging generation and novel
view synthesis remains a major challenge. We present PhiGenesis, a unified
framework for 4D scene generation that extends video generation techniques with
geometric and temporal consistency. Given multi-view image sequences and camera
parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting
representations along target 3D trajectories. In its first stage, PhiGenesis
leverages a pre-trained video VAE with a novel range-view adapter to enable
feed-forward 4D reconstruction from multi-view images. This architecture
supports single-frame or video inputs and outputs complete 4D scenes including
geometry, semantics, and motion. In the second stage, PhiGenesis introduces a
geometric-guided video diffusion model, using rendered historical 4D scenes as
priors to generate future views conditioned on trajectories. To address
geometric exposure bias in novel views, we propose Stereo Forcing, a novel
conditioning strategy that integrates geometric uncertainty during denoising.
This method enhances temporal coherence by dynamically adjusting generative
influence based on uncertainty-aware perturbations. Our experimental results
demonstrate that our method achieves state-of-the-art performance in both
appearance and geometric reconstruction, temporal generation and novel view
synthesis (NVS) tasks, while simultaneously delivering competitive performance
in downstream evaluations. Homepage is at
\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.



---

## PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-24 | Yufei Han, Bowen Tie, Heng Guo, Youwei Lyu, Si Li, Boxin Shi, Yunpeng Jia, Zhanyu Ma | cs.CV | [PDF](http://arxiv.org/pdf/2509.19726v1){: .btn .btn-green } |

**Abstract**: Efficient shape reconstruction for surfaces with complex reflectance
properties is crucial for real-time virtual reality. While 3D Gaussian
Splatting (3DGS)-based methods offer fast novel view rendering by leveraging
their explicit surface representation, their reconstruction quality lags behind
that of implicit neural representations, particularly in the case of recovering
surfaces with complex reflective reflectance. To address these problems, we
propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective
surface reconstruction in 10 minutes. By integrating polarimetric constraints
into the 3DGS framework, PolGS effectively separates specular and diffuse
components, enhancing reconstruction quality for challenging reflective
materials. Experimental results on the synthetic and real-world dataset
validate the effectiveness of our method.

Comments:
- Accepted by ICCV 2025

---

## GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for  Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-24 | Guo Chen, Jiarun Liu, Sicong Du, Chenming Wu, Deqi Li, Shi-Sheng Huang, Guofeng Zhang, Sheng Yang | cs.CV | [PDF](http://arxiv.org/pdf/2509.19937v1){: .btn .btn-green } |

**Abstract**: This paper presents GS-RoadPatching, an inpainting method for driving scene
completion by referring to completely reconstructed regions, which are
represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting
methods that perform generative completion relying on 2D perspective-view-based
diffusion or GAN models to predict limited appearance or depth cues for missing
regions, our approach enables substitutional scene inpainting and editing
directly through the 3DGS modality, extricating it from requiring
spatial-temporal consistency of 2D cross-modals and eliminating the need for
time-intensive retraining of Gaussians. Our key insight is that the highly
repetitive patterns in driving scenes often share multi-modal similarities
within the implicit 3DGS feature space and are particularly suitable for
structural matching to enable effective 3DGS-based substitutional inpainting.
Practically, we construct feature-embedded 3DGS scenes to incorporate a patch
measurement method for abstracting local context at different scales and,
subsequently, propose a structural search method to find candidate patches in
3D space effectively. Finally, we propose a simple yet effective
substitution-and-fusion optimization for better visual harmony. We conduct
extensive experiments on multiple publicly available datasets to demonstrate
the effectiveness and efficiency of our proposed method in driving scenes, and
the results validate that our method achieves state-of-the-art performance
compared to the baseline methods in terms of both quality and interoperability.
Additional experiments in general scenes also demonstrate the applicability of
the proposed 3D inpainting strategy. The project page and code are available
at: https://shanzhaguoo.github.io/GS-RoadPatching/



---

## Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based  Intermediate View Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-24 | Jiangxue Yu, Hui Wang, San Jiang, Xing Zhang, Dejin Zhang, Qingquan Li | cs.CV | [PDF](http://arxiv.org/pdf/2509.19898v1){: .btn .btn-green } |

**Abstract**: The integration of aerial and ground images has been a promising solution in
3D modeling of complex scenes, which is seriously restricted by finding
reliable correspondences. The primary contribution of this study is a feature
matching algorithm for aerial and ground images, whose core idea is to generate
intermediate views to alleviate perspective distortions caused by the extensive
viewpoint changes. First, by using aerial images only, sparse models are
reconstructed through an incremental SfM (Structure from Motion) engine due to
their large scene coverage. Second, 3D Gaussian Splatting is then adopted for
scene rendering by taking as inputs sparse points and oriented images. For
accurate view rendering, a render viewpoint determination algorithm is designed
by using the oriented camera poses of aerial images, which is used to generate
high-quality intermediate images that can bridge the gap between aerial and
ground images. Third, with the aid of intermediate images, reliable feature
matching is conducted for match pairs from render-aerial and render-ground
images, and final matches can be generated by transmitting correspondences
through intermediate views. By using real aerial and ground datasets, the
validation of the proposed solution has been verified in terms of feature
matching and scene rendering and compared comprehensively with widely used
methods. The experimental results demonstrate that the proposed solution can
provide reliable feature matches for aerial and ground images with an obvious
increase in the number of initial and refined matches, and it can provide
enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based
scene rendering.



---

## BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth  Estimation via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-24 | Yixun Zhang, Feng Zhou, Jianqin Yin | cs.CV | [PDF](http://arxiv.org/pdf/2509.19793v1){: .btn .btn-green } |

**Abstract**: Camera-based perception is critical to autonomous driving yet remains
vulnerable to task-specific adversarial manipulations in object detection and
monocular depth estimation. Most existing 2D/3D attacks are developed in task
silos, lack mechanisms to induce controllable depth bias, and offer no
standardized protocol to quantify cross-task transfer, leaving the interaction
between detection and depth underexplored. We present BiTAA, a bi-task
adversarial attack built on 3D Gaussian Splatting that yields a single
perturbation capable of simultaneously degrading detection and biasing
monocular depth. Specifically, we introduce a dual-model attack framework that
supports both full-image and patch settings and is compatible with common
detectors and depth estimators, with optional expectation-over-transformation
(EOT) for physical reality. In addition, we design a composite loss that
couples detection suppression with a signed, magnitude-controlled log-depth
bias within regions of interest (ROIs) enabling controllable near or far
misperception while maintaining stable optimization across tasks. We also
propose a unified evaluation protocol with cross-task transfer metrics and
real-world evaluations, showing consistent cross-task degradation and a clear
asymmetry between Det to Depth and from Depth to Det transfer. The results
highlight practical risks for multi-task camera-only perception and motivate
cross-task-aware defenses in autonomous driving scenarios.

Comments:
- Intend to submit to RA-L

---

## Event-guided 3D Gaussian Splatting for Dynamic Human and Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Xiaoting Yin, Hao Shi, Kailun Yang, Jiajun Zhai, Shangwei Guo, Lin Wang, Kaiwei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.18566v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.



---

## VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with  Voxel-Aligned Prediction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang | cs.CV | [PDF](http://arxiv.org/pdf/2509.19297v1){: .btn .btn-green } |

**Abstract**: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

Comments:
- Project Page: https://lhmd.top/volsplat, Code:
  https://github.com/ziplab/VolSplat

---

## Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model  Self-Distillation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren | cs.CV | [PDF](http://arxiv.org/pdf/2509.19296v1){: .btn .btn-green } |

**Abstract**: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

Comments:
- Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/

---

## Differentiable Light Transport with Gaussian Surfels via Adapted  Radiosity for Efficient Relighting and Geometry Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Kaiwen Jiang, Jia-Mu Sun, Zilu Li, Dan Wang, Tzu-Mao Li, Ravi Ramamoorthi | cs.GR | [PDF](http://arxiv.org/pdf/2509.18497v1){: .btn .btn-green } |

**Abstract**: Radiance fields have gained tremendous success with applications ranging from
novel view synthesis to geometry reconstruction, especially with the advent of
Gaussian splatting. However, they sacrifice modeling of material reflective
properties and lighting conditions, leading to significant geometric
ambiguities and the inability to easily perform relighting. One way to address
these limitations is to incorporate physically-based rendering, but it has been
prohibitively expensive to include full global illumination within the inner
loop of the optimization. Therefore, previous works adopt simplifications that
make the whole optimization with global illumination effects efficient but less
accurate. In this work, we adopt Gaussian surfels as the primitives and build
an efficient framework for differentiable light transport, inspired from the
classic radiosity theory. The whole framework operates in the coefficient space
of spherical harmonics, enabling both diffuse and specular materials. We extend
the classic radiosity into non-binary visibility and semi-opaque primitives,
propose novel solvers to efficiently solve the light transport, and derive the
backward pass for gradient optimizations, which is more efficient than
auto-differentiation. During inference, we achieve view-independent rendering
where light transport need not be recomputed under viewpoint changes, enabling
hundreds of FPS for global illumination effects, including view-dependent
reflections using a spherical harmonics representation. Through extensive
qualitative and quantitative experiments, we demonstrate superior geometry
reconstruction, view synthesis and relighting than previous inverse rendering
baselines, or data-driven baselines given relatively sparse datasets with known
or unknown lighting conditions.



---

## BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting  for Deformable Intraoperative Surgical Navigation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Maximilian Fehrentz, Alexander Winkler, Thomas Heiliger, Nazim Haouchine, Christian Heiliger, Nassir Navab | cs.CV | [PDF](http://arxiv.org/pdf/2509.18501v1){: .btn .btn-green } |

**Abstract**: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

Comments:
- Accepted at MICCAI 2025

---

## SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian  Bracketing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Yiyu Li, Haoyuan Wang, Ke Xu, Gerhard Petrus Hancke, Rynson W. H. Lau | cs.GR | [PDF](http://arxiv.org/pdf/2509.20400v1){: .btn .btn-green } |

**Abstract**: This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting
(HDR-3DGS) approach for generating HDR novel views given multi-view LDR images.
Unlike existing methods that typically require the multi-view LDR input images
to be captured from different exposures, which are tedious to capture and more
likely to suffer from errors (e.g., object motion blurs and
calibration/alignment inaccuracies), our approach learns the HDR scene
representation from multi-view LDR images of a single exposure. Our key insight
to this ill-posed problem is that by first estimating Bracketed 3D Gaussians
(i.e., with different exposures) from single-exposure multi-view LDR images, we
may then be able to merge these bracketed 3D Gaussians into an HDR scene
representation. Specifically, SeHDR first learns base 3D Gaussians from
single-exposure LDR inputs, where the spherical harmonics parameterize colors
in a linear color space. We then estimate multiple 3D Gaussians with identical
geometry but varying linear colors conditioned on exposure manipulations.
Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to
integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view
rendering. Extensive experiments demonstrate that SeHDR outperforms existing
methods as well as carefully designed baselines.

Comments:
- ICCV 2025 accepted paper

---

## SINGER: An Onboard Generalist Vision-Language Navigation Policy for  Drones

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Maximilian Adang, JunEn Low, Ola Shorinwa, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2509.18610v1){: .btn .btn-green } |

**Abstract**: Large vision-language models have driven remarkable progress in
open-vocabulary robot policies, e.g., generalist robot manipulation policies,
that enable robots to complete complex tasks specified in natural language.
Despite these successes, open-vocabulary autonomous drone navigation remains an
unsolved challenge due to the scarcity of large-scale demonstrations, real-time
control demands of drones for stabilization, and lack of reliable external pose
estimation modules. In this work, we present SINGER for language-guided
autonomous drone navigation in the open world using only onboard sensing and
compute. To train robust, open-vocabulary navigation policies, SINGER leverages
three central components: (i) a photorealistic language-embedded flight
simulator with minimal sim-to-real gap using Gaussian Splatting for efficient
data generation, (ii) an RRT-inspired multi-trajectory generation expert for
collision-free navigation demonstrations, and these are used to train (iii) a
lightweight end-to-end visuomotor policy for real-time closed-loop control.
Through extensive hardware flight experiments, we demonstrate superior
zero-shot sim-to-real transfer of our policy to unseen environments and unseen
language-conditioned goal objects. When trained on ~700k-1M observation action
pairs of language conditioned visuomotor data and deployed on hardware, SINGER
outperforms a velocity-controlled semantic guidance baseline by reaching the
query 23.33% more on average, and maintains the query in the field of view
16.67% more on average, with 10% fewer collisions.



---

## DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust  Deblurring

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Pengteng Li, Yunfan Lu, Pinhao Song, Weiyu Guo, Huizai Yao, F. Richard Yu, Hui Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2509.18898v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.



---

## Seeing Through Reflections: Advancing 3D Scene Reconstruction in  Mirror-Containing Environments with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Zijing Guo, Yunyang Zhao, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.18956v1){: .btn .btn-green } |

**Abstract**: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.



---

## WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian  Object Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Hung Nguyen, Runfa Li, An Le, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2509.19073v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.



---

## FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score  Distillation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-23 | Zhaorui Wang, Yi Gu, Deming Zhou, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2509.18759v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.



---

## FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS  Models to LiDAR

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-22 | Junzhe Wu, Yufei Jia, Yiyi Yan, Zhixing Chen, Tiao Tan, Zifan Wang, Guangyu Wang | cs.RO | [PDF](http://arxiv.org/pdf/2509.17390v1){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic
rendering, its vast ecosystem of assets remains incompatible with
high-performance LiDAR simulation, a critical tool for robotics and autonomous
driving. We present \textbf{FGGS-LiDAR}, a framework that bridges this gap with
a truly plug-and-play approach. Our method converts \textit{any} pretrained
3DGS model into a high-fidelity, watertight mesh without requiring
LiDAR-specific supervision or architectural alterations. This conversion is
achieved through a general pipeline of volumetric discretization and Truncated
Signed Distance Field (TSDF) extraction. We pair this with a highly optimized,
GPU-accelerated ray-casting module that simulates LiDAR returns at over 500
FPS. We validate our approach on indoor and outdoor scenes, demonstrating
exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for
geometrically accurate depth sensing, our framework extends their utility
beyond visualization and unlocks new capabilities for scalable, multimodal
simulation. Our open-source implementation is available at
https://github.com/TATP-233/FGGS-LiDAR.



---

## EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian  Splats from a Mobile Device

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-22 | Gunjan Chhablani, Xiaomeng Ye, Muhammad Zubair Irshad, Zsolt Kira | cs.CV | [PDF](http://arxiv.org/pdf/2509.17430v2){: .btn .btn-green } |

**Abstract**: The field of Embodied AI predominantly relies on simulation for training and
evaluation, often using either fully synthetic environments that lack
photorealism or high-fidelity real-world reconstructions captured with
expensive hardware. As a result, sim-to-real transfer remains a major
challenge. In this paper, we introduce EmbodiedSplat, a novel approach that
personalizes policy training by efficiently capturing the deployment
environment and fine-tuning policies within the reconstructed scenes. Our
method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to
bridge the gap between realistic scene capture and effective training
environments. Using iPhone-captured deployment scenes, we reconstruct meshes
via GS, enabling training in settings that closely approximate real-world
conditions. We conduct a comprehensive analysis of training strategies,
pre-training datasets, and mesh reconstruction techniques, evaluating their
impact on sim-to-real predictivity in real-world scenarios. Experimental
results demonstrate that agents fine-tuned with EmbodiedSplat outperform both
zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and
synthetically generated datasets (HSSD), achieving absolute success rate
improvements of 20% and 40% on real-world Image Navigation task. Moreover, our
approach yields a high sim-vs-real correlation (0.87-0.97) for the
reconstructed meshes, underscoring its effectiveness in adapting policies to
diverse environments with minimal effort. Project page:
https://gchhablani.github.io/embodied-splat.

Comments:
- 16 pages, 18 figures, paper accepted at ICCV, 2025

---

## GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-22 | Jiahe Li, Jiawei Zhang, Youmin Zhang, Xiao Bai, Jin Zheng, Xiaohan Yu, Lin Gu | cs.CV | [PDF](http://arxiv.org/pdf/2509.18090v1){: .btn .btn-green } |

**Abstract**: Reconstructing accurate surfaces with radiance fields has achieved remarkable
progress in recent years. However, prevailing approaches, primarily based on
Gaussian Splatting, are increasingly constrained by representational
bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.
As strengths, sparse voxels support preserving the coverage completeness and
geometric clarity, while corresponding challenges also arise from absent scene
constraints and locality in surface refinement. To ensure correct scene
convergence, we first propose a Voxel-Uncertainty Depth Constraint that
maximizes the effect of monocular depth cues while presenting a voxel-oriented
uncertainty to avoid quality degradation, enabling effective and robust scene
constraints yet preserving highly accurate geometries. Subsequently, Sparse
Voxel Surface Regularization is designed to enhance geometric consistency for
tiny voxels and facilitate the voxel-based formation of sharp and accurate
surfaces. Extensive experiments demonstrate our superior performance compared
to existing methods across diverse challenging scenarios, excelling in
geometric accuracy, detail preservation, and reconstruction completeness while
maintaining high efficiency. Code is available at
https://github.com/Fictionarry/GeoSVR.

Comments:
- Accepted at NeurIPS 2025 (Spotlight). Project page:
  https://fictionarry.github.io/GeoSVR-project/

---

## SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-22 | Neham Jain, Andrew Jong, Sebastian Scherer, Ioannis Gkioulekas | cs.CV | [PDF](http://arxiv.org/pdf/2509.17329v1){: .btn .btn-green } |

**Abstract**: Smoke in real-world scenes can severely degrade the quality of images and
hamper visibility. Recent methods for image restoration either rely on
data-driven priors that are susceptible to hallucinations, or are limited to
static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D
scene reconstruction and smoke removal from a video capturing multiple views of
a scene. Our method uses thermal and RGB images, leveraging the fact that the
reduced scattering in thermal images enables us to see through the smoke. We
build upon 3D Gaussian splatting to fuse information from the two image
modalities, and decompose the scene explicitly into smoke and non-smoke
components. Unlike prior approaches, SmokeSeer handles a broad range of smoke
densities and can adapt to temporally varying smoke. We validate our approach
on synthetic data and introduce a real-world multi-view smoke dataset with RGB
and thermal images. We provide open-source code and data at the project
website.

Comments:
- Project website: https://imaging.cs.cmu.edu/smokeseer

---

## ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting  from Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-22 | Shi Chen, Erik Sandström, Sandro Lombardi, Siyuan Li, Martin R. Oswald | cs.CV | [PDF](http://arxiv.org/pdf/2509.17864v1){: .btn .btn-green } |

**Abstract**: Achieving truly practical dynamic 3D reconstruction requires online
operation, global pose and map consistency, detailed appearance modeling, and
the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM
methods typically merely remove the dynamic parts or require RGB-D input, while
offline methods are not scalable to long video sequences, and current
transformer-based feedforward methods lack global consistency and appearance
details. To this end, we achieve online dynamic scene reconstruction by
disentangling the static and dynamic parts within a SLAM system. The poses are
tracked robustly with a novel motion masking strategy, and dynamic parts are
reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.
Our method yields novel view renderings competitive to offline methods and
achieves on-par tracking with state-of-the-art dynamic SLAM methods.



---

## GaussianPSL: A novel framework based on Gaussian Splatting for exploring  the Pareto frontier in multi-criteria optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-22 | Phuong Mai Dinh, Van-Nam Huynh | cs.LG | [PDF](http://arxiv.org/pdf/2509.17889v1){: .btn .btn-green } |

**Abstract**: Multi-objective optimization (MOO) is essential for solving complex
real-world problems involving multiple conflicting objectives. However, many
practical applications - including engineering design, autonomous systems, and
machine learning - often yield non-convex, degenerate, or discontinuous Pareto
frontiers, which involve traditional scalarization and Pareto Set Learning
(PSL) methods that struggle to approximate accurately. Existing PSL approaches
perform well on convex fronts but tend to fail in capturing the diversity and
structure of irregular Pareto sets commonly observed in real-world scenarios.
In this paper, we propose Gaussian-PSL, a novel framework that integrates
Gaussian Splatting into PSL to address the challenges posed by non-convex
Pareto frontiers. Our method dynamically partitions the preference vector
space, enabling simple MLP networks to learn localized features within each
region, which are then integrated by an additional MLP aggregator. This
partition-aware strategy enhances both exploration and convergence, reduces
sensi- tivity to initialization, and improves robustness against local optima.
We first provide the mathematical formulation for controllable Pareto set
learning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL
architecture and evaluate its performance on synthetic and real-world
multi-objective benchmarks. Experimental results demonstrate that our approach
outperforms standard PSL models in learning irregular Pareto fronts while
maintaining computational efficiency and model simplicity. This work offers a
new direction for effective and scalable MOO under challenging frontier
geometries.



---

## Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene  Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-22 | Sitian Shen, Georgi Pramatarov, Yifu Tao, Daniele De Martini | cs.CV | [PDF](http://arxiv.org/pdf/2509.17762v1){: .btn .btn-green } |

**Abstract**: This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal
large-scale scene reconstruction that fuses multiple sensing modalities in a
per-gaussian compact, learnable embedding. While recent works focusing on
large-scale scene reconstruction have incorporated LiDAR data to provide more
accurate geometric constraints, we argue that LiDAR's rich physical properties
remain underexplored. Similarly, semantic information has been used for object
retrieval, but could provide valuable high-level context for scene
reconstruction. Traditional approaches append these properties to Gaussians as
separate parameters, increasing memory usage and limiting information exchange
across modalities. Instead, our approach fuses all modalities -- image, LiDAR,
and semantics -- into a compact, learnable embedding that implicitly encodes
optical, physical, and semantic features in each Gaussian. We then train
lightweight neural decoders to map these embeddings to Gaussian parameters,
enabling the reconstruction of each sensing modality with lower memory overhead
and improved scalability. We evaluate Neural-MMGS on the Oxford Spires and
KITTI-360 datasets. On Oxford Spires, we achieve higher-quality
reconstructions, while on KITTI-360, our method reaches competitive results
with less storage consumption compared with current approaches in LiDAR-based
novel-view synthesis.



---

## From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for  Underwater Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-22 | Guoxi Huang, Haoran Wang, Zipeng Qi, Wenjun Lu, David Bull, Nantheera Anantrasirichai | cs.CV | [PDF](http://arxiv.org/pdf/2509.17789v1){: .btn .btn-green } |

**Abstract**: Underwater image degradation poses significant challenges for 3D
reconstruction, where simplified physical models often fail in complex scenes.
We propose \textbf{R-Splatting}, a unified framework that bridges underwater
image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both
rendering quality and geometric fidelity. Our method integrates multiple
enhanced views produced by diverse UIR models into a single reconstruction
pipeline. During inference, a lightweight illumination generator samples latent
codes to support diverse yet coherent renderings, while a contrastive loss
ensures disentangled and stable illumination representations. Furthermore, we
propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models
opacity as a stochastic function to regularize training. This suppresses abrupt
gradient responses triggered by illumination variation and mitigates
overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF
and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong
baselines in both rendering quality and geometric accuracy.



---

## Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic  Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-21 | Zhenya Yang | cs.CV | [PDF](http://arxiv.org/pdf/2509.17027v1){: .btn .btn-green } |

**Abstract**: Surgical simulation is essential for medical training, enabling practitioners
to develop crucial skills in a risk-free environment while improving patient
safety and surgical outcomes. However, conventional methods for building
simulation environments are cumbersome, time-consuming, and difficult to scale,
often resulting in poor details and unrealistic simulations. In this paper, we
propose a Gaussian Splatting-based framework to directly reconstruct
interactive surgical scenes from endoscopic data while ensuring efficiency,
rendering quality, and realism. A key challenge in this data-driven simulation
paradigm is the restricted movement of endoscopic cameras, which limits
viewpoint diversity. As a result, the Gaussian Splatting representation
overfits specific perspectives, leading to reduced geometric accuracy. To
address this issue, we introduce a novel virtual camera-based regularization
method that adaptively samples virtual viewpoints around the scene and
incorporates them into the optimization process to mitigate overfitting. An
effective depth-based regularization is applied to both real and virtual views
to further refine the scene geometry. To enable fast deformation simulation, we
propose a sparse control node-based Material Point Method, which integrates
physical properties into the reconstructed scene while significantly reducing
computational costs. Experimental results on representative surgical data
demonstrate that our method can efficiently reconstruct and simulate surgical
scenes from sparse endoscopic views. Notably, our method takes only a few
minutes to reconstruct the surgical scene and is able to produce physically
plausible deformations in real-time with user-defined interactions.

Comments:
- Workshop Paper of AECAI@MICCAI 2025

---

## ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D  Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-21 | Amanuel T. Dufera, Yuan-Li Cai | cs.CV | [PDF](http://arxiv.org/pdf/2509.16863v1){: .btn .btn-green } |

**Abstract**: We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM
system for robust, highfidelity RGB-only reconstruction. Addressing geometric
inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable
depth estimation, ConfidentSplat incorporates a core innovation: a
confidence-weighted fusion mechanism. This mechanism adaptively integrates
depth cues from multiview geometry with learned monocular priors (Omnidata
ViT), dynamically weighting their contributions based on explicit reliability
estimates-derived predominantly from multi-view geometric consistency-to
generate high-fidelity proxy depth for map supervision. The resulting proxy
depth guides the optimization of a deformable 3DGS map, which efficiently
adapts online to maintain global consistency following pose updates from a
DROID-SLAM-inspired frontend and backend optimizations (loop closure, global
bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,
ScanNet) and diverse custom mobile datasets demonstrates significant
improvements in reconstruction accuracy (L1 depth error) and novel view
synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in
challenging conditions. ConfidentSplat underscores the efficacy of principled,
confidence-aware sensor fusion for advancing state-of-the-art dense visual
SLAM.



---

## PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D  Gaussian Splatting with Pixel-Aware Density Control

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-21 | Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng | cs.SD | [PDF](http://arxiv.org/pdf/2509.16922v1){: .btn .btn-green } |

**Abstract**: Audio-driven talking head generation is crucial for applications in virtual
reality, digital avatars, and film production. While NeRF-based methods enable
high-fidelity reconstruction, they suffer from low rendering efficiency and
suboptimal audio-visual synchronization. This work presents PGSTalker, a
real-time audio-driven talking head synthesis framework based on 3D Gaussian
Splatting (3DGS). To improve rendering performance, we propose a pixel-aware
density control strategy that adaptively allocates point density, enhancing
detail in dynamic facial regions while reducing redundancy elsewhere.
Additionally, we introduce a lightweight Multimodal Gated Fusion Module to
effectively fuse audio and spatial features, thereby improving the accuracy of
Gaussian deformation prediction. Extensive experiments on public datasets
demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches
in rendering quality, lip-sync precision, and inference speed. Our method
exhibits strong generalization capabilities and practical potential for
real-world deployment.

Comments:
- Main paper (15 pages). Accepted for publication by ICONIP(
  International Conference on Neural Information Processing) 2025

---

## SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting  from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-21 | Ranran Huang, Krystian Mikolajczyk | cs.CV | [PDF](http://arxiv.org/pdf/2509.17246v1){: .btn .btn-green } |

**Abstract**: We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian
splatting from sparse multi-view images, requiring no ground-truth poses during
training and inference. It employs a shared feature extraction backbone,
enabling simultaneous prediction of 3D Gaussian primitives and camera poses in
a canonical space from unposed inputs. A masked attention mechanism is
introduced to efficiently estimate target poses during training, while a
reprojection loss enforces pixel-aligned Gaussian primitives, providing
stronger geometric constraints. We further demonstrate the compatibility of our
training framework with different reconstruction architectures, resulting in
two model variants. Remarkably, despite the absence of pose supervision, our
method achieves state-of-the-art performance in both in-domain and
out-of-domain novel view synthesis, even under extreme viewpoint changes and
limited image overlap, and surpasses recent methods that rely on geometric
supervision for relative pose estimation. By eliminating dependence on
ground-truth poses, our method offers the scalability to leverage larger and
more diverse datasets. Code and pretrained models will be available on our
project page: https://ranrhuang.github.io/spfsplatv2/.



---

## DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for  Neural Radiance Fields in 3D Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-21 | Bo Liu, Runlong Li, Li Zhou, Yan Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2509.17232v1){: .btn .btn-green } |

**Abstract**: This paper proposes a Diffusion Model-Optimized Neural Radiance Field
(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency
in 3D scene reconstruction. By combining diffusion models with Transformers,
DT-NeRF effectively restores details under sparse viewpoints and maintains high
accuracy in complex geometric scenes. Experimental results demonstrate that
DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art
methods on the Matterport3D and ShapeNet datasets, particularly in metrics such
as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further
confirm the critical role of the diffusion and Transformer modules in the
model's performance, with the removal of either module leading to a decline in
performance. The design of DT-NeRF showcases the synergistic effect between
modules, providing an efficient and accurate solution for 3D scene
reconstruction. Future research may focus on further optimizing the model,
exploring more advanced generative models and network architectures to enhance
its performance in large-scale dynamic scenes.

Comments:
- 15 pages

---

## HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel  View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-21 | Zipeng Wang, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2509.17083v2){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative
to NeRF-based approaches, enabling real-time, high-quality novel view synthesis
through explicit, optimizable 3D Gaussians. However, 3DGS suffers from
significant memory overhead due to its reliance on per-Gaussian parameters to
model view-dependent effects and anisotropic shapes. While recent works propose
compressing 3DGS with neural fields, these methods struggle to capture
high-frequency spatial variations in Gaussian properties, leading to degraded
reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a
novel scene representation that combines the strengths of explicit Gaussians
and neural fields. HyRF decomposes the scene into (1) a compact set of explicit
Gaussians storing only critical high-frequency parameters and (2) grid-based
neural fields that predict remaining properties. To enhance representational
capacity, we introduce a decoupled neural field architecture, separately
modeling geometry (scale, opacity, rotation) and view-dependent color.
Additionally, we propose a hybrid rendering scheme that composites Gaussian
splatting with a neural field-predicted background, addressing limitations in
distant scene representation. Experiments demonstrate that HyRF achieves
state-of-the-art rendering quality while reducing model size by over 20 times
compared to 3DGS and maintaining real-time performance. Our project page is
available at https://wzpscott.github.io/hyrf/.

Comments:
- Accepted at NeurIPS 2025

---

## MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-20 | Kacper Marzol, Ignacy Kolton, Weronika Smolak-Dyżewska, Joanna Kaleta, Marcin Mazur, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2509.16806v1){: .btn .btn-green } |

**Abstract**: Multi-modal three-dimensional (3D) medical imaging data, derived from
ultrasound, magnetic resonance imaging (MRI), and potentially computed
tomography (CT), provide a widely adopted approach for non-invasive anatomical
visualization. Accurate modeling, registration, and visualization in this
setting depend on surface reconstruction and frame-to-frame interpolation.
Traditional methods often face limitations due to image noise and incomplete
information between frames. To address these challenges, we present MedGS, a
semi-supervised neural implicit surface reconstruction framework that employs a
Gaussian Splatting (GS)-based interpolation mechanism. In this framework,
medical imaging data are represented as consecutive two-dimensional (2D) frames
embedded in 3D space and modeled using Gaussian-based distributions. This
representation enables robust frame interpolation and high-fidelity surface
reconstruction across imaging modalities. As a result, MedGS offers more
efficient training than traditional neural implicit methods. Its explicit
GS-based representation enhances noise robustness, allows flexible editing, and
supports precise modeling of complex anatomical structures with fewer
artifacts. These features make MedGS highly suitable for scalable and practical
applications in medical imaging.



---

## ST-GS: Vision-Based 3D Semantic Occupancy Prediction with  Spatial-Temporal Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-20 | Xiaoyang Yan, Muleilan Pei, Shaojie Shen | cs.CV | [PDF](http://arxiv.org/pdf/2509.16552v1){: .btn .btn-green } |

**Abstract**: 3D occupancy prediction is critical for comprehensive scene understanding in
vision-centric autonomous driving. Recent advances have explored utilizing 3D
semantic Gaussians to model occupancy while reducing computational overhead,
but they remain constrained by insufficient multi-view spatial interaction and
limited multi-frame temporal consistency. To overcome these issues, in this
paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework
to enhance both spatial and temporal modeling in existing Gaussian-based
pipelines. Specifically, we develop a guidance-informed spatial aggregation
strategy within a dual-mode attention mechanism to strengthen spatial
interaction in Gaussian representations. Furthermore, we introduce a
geometry-aware temporal fusion scheme that effectively leverages historical
context to improve temporal continuity in scene completion. Extensive
experiments on the large-scale nuScenes occupancy prediction benchmark showcase
that our proposed approach not only achieves state-of-the-art performance but
also delivers markedly better temporal consistency compared to existing
Gaussian-based methods.



---

## RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D  Detector with 4D Automotive Radars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-19 | Weiyi Xiong, Bing Zhu, Tao Huang, Zewei Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2509.16119v1){: .btn .btn-green } |

**Abstract**: 4D automotive radars have gained increasing attention for autonomous driving
due to their low cost, robustness, and inherent velocity measurement
capability. However, existing 4D radar-based 3D detectors rely heavily on
pillar encoders for BEV feature extraction, where each point contributes to
only a single BEV grid, resulting in sparse feature maps and degraded
representation quality. In addition, they also optimize bounding box attributes
independently, leading to sub-optimal detection accuracy. Moreover, their
inference speed, while sufficient for high-end GPUs, may fail to meet the
real-time requirement on vehicle-mounted embedded devices. To overcome these
limitations, an efficient and effective Gaussian-based 3D detector, namely
RadarGaussianDet3D is introduced, leveraging Gaussian primitives and
distributions as intermediate representations for radar points and bounding
boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed
to transform each point into a Gaussian primitive after feature aggregation and
employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,
yielding denser feature maps. PGE exhibits exceptionally low latency, owing to
the optimized algorithm for point feature aggregation and fast rendering of
3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts
bounding boxes into 3D Gaussian distributions and measures their distance to
enable more comprehensive and consistent optimization. Extensive experiments on
TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves
state-of-the-art detection accuracy while delivering substantially faster
inference, highlighting its potential for real-time deployment in autonomous
driving.



---

## Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-19 | Liwei Liao, Xufeng Li, Xiaoyun Zheng, Boning Liu, Feng Gao, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.15871v1){: .btn .btn-green } |

**Abstract**: 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text
prompts, which is essential for applications such as robotics. However,
existing 3DVG methods encounter two main challenges: first, they struggle to
handle the implicit representation of spatial textures in 3D Gaussian Splatting
(3DGS), making per-scene training indispensable; second, they typically require
larges amounts of labeled data for effective training. To this end, we propose
\underline{G}rounding via \underline{V}iew \underline{R}etrieval (GVR), a novel
zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D
retrieval task that leverages object-level view retrieval to collect grounding
clues from multiple views, which not only avoids the costly process of 3D
annotation, but also eliminates the need for per-scene training. Extensive
experiments demonstrate that our method achieves state-of-the-art visual
grounding performance while avoiding per-scene training, providing a solid
foundation for zero-shot 3DVG research. Video demos can be found in
https://github.com/leviome/GVR_demos.



---

## GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host  Offloading

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-19 | Donghyun Lee, Dawoon Jeong, Jae W. Lee, Hongil Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2509.15645v1){: .btn .btn-green } |

**Abstract**: The advent of 3D Gaussian Splatting has revolutionized graphics rendering by
delivering high visual quality and fast rendering speeds. However, training
large-scale scenes at high quality remains challenging due to the substantial
memory demands required to store parameters, gradients, and optimizer states,
which can quickly overwhelm GPU memory. To address these limitations, we
propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian
Splatting. GS-Scale stores all Gaussians in host memory, transferring only a
subset to the GPU on demand for each forward and backward pass. While this
dramatically reduces GPU memory usage, it requires frustum culling and
optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's
limited compute and memory bandwidth. To mitigate this, GS-Scale employs three
system-level optimizations: (1) selective offloading of geometric parameters
for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer
updates with GPU computation, and (3) deferred optimizer update to minimize
unnecessary memory accesses for Gaussians with zero gradients. Our extensive
evaluations on large-scale datasets demonstrate that GS-Scale significantly
lowers GPU memory demands by 3.3-5.6x, while achieving training speeds
comparable to GPU without host offloading. This enables large-scale 3D Gaussian
Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the
number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU,
leading to 23-35% LPIPS (learned perceptual image patch similarity)
improvement.



---

## Camera Splatting for Continuous View Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-19 | Gahye Lee, Hyomin Kim, Gwangjin Ju, Jooeun Son, Hyejeong Yoon, Seungyong Lee | cs.CV | [PDF](http://arxiv.org/pdf/2509.15677v1){: .btn .btn-green } |

**Abstract**: We propose Camera Splatting, a novel view optimization framework for novel
view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a
camera splat, and virtual cameras, termed point cameras, are placed at 3D
points sampled near the surface to observe the distribution of camera splats.
View optimization is achieved by continuously and differentiably refining the
camera splats so that desirable target distributions are observed from the
point cameras, in a manner similar to the original 3D Gaussian splatting.
Compared to the Farthest View Sampling (FVS) approach, our optimized views
demonstrate superior performance in capturing complex view-dependent phenomena,
including intense metallic reflections and intricate textures such as text.



---

## FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation  based on 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-19 | Yuwei Jia, Yutang Lu, Zhe Cui, Fei Su | cs.CV | [PDF](http://arxiv.org/pdf/2509.15648v1){: .btn .btn-green } |

**Abstract**: Researchers have conducted many pioneer researches on contactless
fingerprints, yet the performance of contactless fingerprint recognition still
lags behind contact-based methods primary due to the insufficient contactless
fingerprint data with pose variations and lack of the usage of implicit 3D
fingerprint representations. In this paper, we introduce a novel contactless
fingerprint 3D registration, reconstruction and generation framework by
integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for
contactless fingerprint recognition that integrates 3D fingerprint
reconstruction and generation. To our knowledge, this is the first work to
apply 3D Gaussian Splatting to the field of fingerprint recognition, and the
first to achieve effective 3D registration and complete reconstruction of
contactless fingerprints with sparse input images and without requiring camera
parameters information. Experiments on 3D fingerprint registration,
reconstruction, and generation prove that our method can accurately align and
reconstruct 3D fingerprints from 2D images, and sequentially generates
high-quality contactless fingerprints from 3D model, thus increasing the
performances for contactless fingerprint recognition.



---

## MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-19 | Deming Li, Kaiwen Jiang, Yutao Tang, Ravi Ramamoorthi, Rama Chellappa, Cheng Peng | cs.CV | [PDF](http://arxiv.org/pdf/2509.15548v2){: .btn .btn-green } |

**Abstract**: In-the-wild photo collections often contain limited volumes of imagery and
exhibit multiple appearances, e.g., taken at different times of day or seasons,
posing significant challenges to scene reconstruction and novel view synthesis.
Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian
Splatting (3DGS) have improved in these areas, they tend to oversmooth and are
prone to overfitting. In this paper, we present MS-GS, a novel framework
designed with Multi-appearance capabilities in Sparse-view scenarios using
3DGS. To address the lack of support due to sparse initializations, our
approach is built on the geometric priors elicited from monocular depth
estimations. The key lies in extracting and utilizing local semantic regions
with a Structure-from-Motion (SfM) points anchored algorithm for reliable
alignment and geometry cues. Then, to introduce multi-view constraints, we
propose a series of geometry-guided supervision at virtual views in a
fine-grained and coarse scheme to encourage 3D consistency and reduce
overfitting. We also introduce a dataset and an in-the-wild experiment setting
to set up more realistic benchmarks. We demonstrate that MS-GS achieves
photorealistic renderings under various challenging sparse-view and
multi-appearance conditions and outperforms existing approaches significantly
across different datasets.



---

## RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform  for Embodied AI

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-18 | Cong Tai, Zhaoyu Zheng, Haixu Long, Hansheng Wu, Haodong Xiang, Zhengbin Long, Jun Xiong, Rong Shi, Shizhuang Zhang, Gang Qiu, He Wang, Ruifeng Li, Jun Huang, Bin Chang, Shuai Feng, Tao Shen | cs.RO | [PDF](http://arxiv.org/pdf/2509.14687v1){: .btn .btn-green } |

**Abstract**: The emerging field of Vision-Language-Action (VLA) for humanoid robots faces
several fundamental challenges, including the high cost of data acquisition,
the lack of a standardized benchmark, and the significant gap between
simulation and the real world. To overcome these obstacles, we propose
RealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror
builds an efficient, low-cost data collection, model training, and inference
system that enables end-to-end VLA research without requiring a real robot. To
facilitate model evolution and fair comparison, we also introduce a dedicated
VLA benchmark for humanoid robots, featuring multiple scenarios, extensive
trajectories, and various VLA models. Furthermore, by integrating generative
models and 3D Gaussian Splatting to reconstruct realistic environments and
robot models, we successfully demonstrate zero-shot Sim2Real transfer, where
models trained exclusively on simulation data can perform tasks on a real robot
seamlessly, without any fine-tuning. In conclusion, with the unification of
these critical components, RealMirror provides a robust framework that
significantly accelerates the development of VLA models for humanoid robots.
Project page: https://terminators2025.github.io/RealMirror.github.io



---

## Causal Reasoning Elicits Controllable 3D Scene Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-18 | Shen Chen, Ruiyu Zhao, Jiale Zhou, Zongkai Wu, Jenq-Neng Hwang, Lei Li | cs.GR | [PDF](http://arxiv.org/pdf/2509.15249v1){: .btn .btn-green } |

**Abstract**: Existing 3D scene generation methods often struggle to model the complex
logical dependencies and physical constraints between objects, limiting their
ability to adapt to dynamic and realistic environments. We propose
CausalStruct, a novel framework that embeds causal reasoning into 3D scene
generation. Utilizing large language models (LLMs), We construct causal graphs
where nodes represent objects and attributes, while edges encode causal
dependencies and physical constraints. CausalStruct iteratively refines the
scene layout by enforcing causal order to determine the placement order of
objects and applies causal intervention to adjust the spatial configuration
according to physics-driven constraints, ensuring consistency with textual
descriptions and real-world dynamics. The refined scene causal graph informs
subsequent optimization steps, employing a
Proportional-Integral-Derivative(PID) controller to iteratively tune object
scales and positions. Our method uses text or images to guide object placement
and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation
Sampling improving shape accuracy and rendering stability. Extensive
experiments show that CausalStruct generates 3D scenes with enhanced logical
coherence, realistic spatial interactions, and robust adaptability.



---

## RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-18 | Fang Li, Hao Zhang, Narendra Ahuja | cs.CV | [PDF](http://arxiv.org/pdf/2509.15123v2){: .btn .btn-green } |

**Abstract**: Although COLMAP has long remained the predominant method for camera parameter
optimization in static scenes, it is constrained by its lengthy runtime and
reliance on ground truth (GT) motion masks for application to dynamic scenes.
Many efforts attempted to improve it by incorporating more priors as
supervision such as GT focal length, motion masks, 3D point clouds, camera
poses, and metric depth, which, however, are typically unavailable in casually
captured RGB videos. In this paper, we propose a novel method for more accurate
and efficient camera parameter optimization in dynamic scenes solely supervised
by a single RGB video, dubbed ROS-Cam. Our method consists of three key
components: (1) Patch-wise Tracking Filters, to establish robust and maximally
sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint
Optimization, for efficient camera parameter optimization by adaptive
down-weighting of moving outliers, without reliance on motion priors. (3) A
Two-stage Optimization Strategy, to enhance stability and optimization speed by
a trade-off between the Softplus limits and convex minima in losses. We
visually and numerically evaluate our camera estimates. To further validate
accuracy, we feed the camera estimates into a 4D reconstruction method and
assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform
experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics)
and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates
camera parameters more efficiently and accurately with a single RGB video as
the only supervision.

Comments:
- NeurIPS 2025 Spotlight

---

## FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model  Priors for 3D Monocular Avatar Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-18 | Jinlong Fan, Bingyu Hu, Xingguang Li, Yuxiang Yang, Jing Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2509.14739v1){: .btn .btn-green } |

**Abstract**: Reconstructing high-fidelity animatable human avatars from monocular videos
remains challenging due to insufficient geometric information in single-view
observations. While recent 3D Gaussian Splatting methods have shown promise,
they struggle with surface detail preservation due to the free-form nature of
3D Gaussian primitives. To address both the representation limitations and
information scarcity, we propose a novel method, \textbf{FMGS-Avatar}, that
integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian
Splatting, where 2D Gaussian primitives are attached directly to template mesh
faces with constrained position, rotation, and movement, enabling superior
surface alignment and geometric detail preservation. Second, we leverage
foundation models trained on large-scale datasets, such as Sapiens, to
complement the limited visual cues from monocular videos. However, when
distilling multi-modal prior knowledge from foundation models, conflicting
optimization objectives can emerge as different modalities exhibit distinct
parameter sensitivities. We address this through a coordinated training
strategy with selective gradient isolation, enabling each loss component to
optimize its relevant parameters without interference. Through this combination
of enhanced representation and coordinated information distillation, our
approach significantly advances 3D monocular human avatar reconstruction.
Experimental evaluation demonstrates superior reconstruction quality compared
to existing methods, with notable gains in geometric accuracy and appearance
fidelity while providing rich semantic information. Additionally, the distilled
prior knowledge within a shared canonical space naturally enables spatially and
temporally consistent rendering under novel views and poses.



---

## NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft  Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-18 | Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer | cs.CV | [PDF](http://arxiv.org/pdf/2509.14890v1){: .btn .btn-green } |

**Abstract**: On-orbit operations require the estimation of the relative 6D pose, i.e.,
position and orientation, between a chaser spacecraft and its target. While
data-driven spacecraft pose estimation methods have been developed, their
adoption in real missions is hampered by the lack of understanding of their
decision process. This paper presents a method to visualize the 3D visual cues
on which a given pose estimator relies. For this purpose, we train a NeRF-based
image generator using the gradients back-propagated through the pose estimation
network. This enforces the generator to render the main 3D features exploited
by the spacecraft pose estimation network. Experiments demonstrate that our
method recovers the relevant 3D cues. Furthermore, they offer additional
insights on the relationship between the pose estimation network supervision
and its implicit representation of the target spacecraft.

Comments:
- under review (8 pages, 2 figures)

---

## LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray  Laminography Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-17 | Chu Chen, Ander Biguri, Jean-Michel Morel, Raymond H. Chan, Carola-Bibiane Schönlieb, Jizhou Li | cs.CV | [PDF](http://arxiv.org/pdf/2509.13863v1){: .btn .btn-green } |

**Abstract**: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.



---

## MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for  High-Fidelity Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-17 | Zhihao Cao, Hanyu Wu, Li Wa Tang, Zizhou Luo, Zihan Zhu, Wei Zhang, Marc Pollefeys, Martin R. Oswald | cs.RO | [PDF](http://arxiv.org/pdf/2509.14191v1){: .btn .btn-green } |

**Abstract**: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.



---

## Perception-Integrated Safety Critical Control via Analytic Collision  Cone Barrier Functions on 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-17 | Dario Tscholl, Yashwanth Nakka, Brian Gunter | cs.RO | [PDF](http://arxiv.org/pdf/2509.14421v1){: .btn .btn-green } |

**Abstract**: We present a perception-driven safety filter that converts each 3D Gaussian
Splat (3DGS) into a closed-form forward collision cone, which in turn yields a
first-order control barrier function (CBF) embedded within a quadratic program
(QP). By exploiting the analytic geometry of splats, our formulation provides a
continuous, closed-form representation of collision constraints that is both
simple and computationally efficient. Unlike distance-based CBFs, which tend to
activate reactively only when an obstacle is already close, our collision-cone
CBF activates proactively, allowing the robot to adjust earlier and thereby
produce smoother and safer avoidance maneuvers at lower computational cost. We
validate the method on a large synthetic scene with approximately 170k splats,
where our filter reduces planning time by a factor of 3 and significantly
decreased trajectory jerk compared to a state-of-the-art 3DGS planner, while
maintaining the same level of safety. The approach is entirely analytic,
requires no high-order CBF extensions (HOCBFs), and generalizes naturally to
robots with physical extent through a principled Minkowski-sum inflation of the
splats. These properties make the method broadly applicable to real-time
navigation in cluttered, perception-derived extreme environments, including
space robotics and satellite systems.

Comments:
- Preprint for IEEE L-CSS/ACC

---

## Plug-and-Play PDE Optimization for 3D Gaussian Splatting: Toward  High-Quality Rendering and Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-17 | Yifan Mo, Youcheng Cai, Ligang Liu | cs.GR | [PDF](http://arxiv.org/pdf/2509.13938v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction
by achieving high-quality novel view synthesis with fast rendering speed,
introducing 3D Gaussian primitives to represent the scene. However, 3DGS
encounters blurring and floaters when applied to complex scenes, caused by the
reconstruction of redundant and ambiguous geometric structures. We attribute
this issue to the unstable optimization of the Gaussians. To address this
limitation, we present a plug-and-play PDE-based optimization method that
overcomes the optimization constraints of 3DGS-based approaches in various
tasks, such as novel view synthesis and surface reconstruction. Firstly, we
theoretically derive that the 3DGS optimization procedure can be modeled as a
PDE, and introduce a viscous term to ensure stable optimization. Secondly, we
use the Material Point Method (MPM) to obtain a stable numerical solution of
the PDE, which enhances both global and local constraints. Additionally, an
effective Gaussian densification strategy and particle constraints are
introduced to ensure fine-grained details. Extensive qualitative and
quantitative experiments confirm that our method achieves state-of-the-art
rendering and reconstruction quality.



---

## ProFusion: 3D Reconstruction of Protein Complex Structures from  Multi-view AFM Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-17 | Jaydeep Rade, Md Hasibul Hasan Hasib, Meric Ozturk, Baboucarr Faal, Sheng Yang, Dipali G. Sashital, Vincenzo Venditti, Baoyu Chen, Soumik Sarkar, Adarsh Krishnamurthy, Anwesha Sarkar | cs.CV | [PDF](http://arxiv.org/pdf/2509.15242v1){: .btn .btn-green } |

**Abstract**: AI-based in silico methods have improved protein structure prediction but
often struggle with large protein complexes (PCs) involving multiple
interacting proteins due to missing 3D spatial cues. Experimental techniques
like Cryo-EM are accurate but costly and time-consuming. We present ProFusion,
a hybrid framework that integrates a deep learning model with Atomic Force
Microscopy (AFM), which provides high-resolution height maps from random
orientations, naturally yielding multi-view data for 3D reconstruction.
However, generating a large-scale AFM imaging data set sufficient to train deep
learning models is impractical. Therefore, we developed a virtual AFM framework
that simulates the imaging process and generated a dataset of ~542,000 proteins
with multi-view synthetic AFM images. We train a conditional diffusion model to
synthesize novel views from unposed inputs and an instance-specific Neural
Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D
protein structures achieve an average Chamfer Distance within the AFM imaging
resolution, reflecting high structural fidelity. Our method is extensively
validated on experimental AFM images of various PCs, demonstrating strong
potential for accurate, cost-effective protein complex structure prediction and
rapid iterative validation using AFM experiments.



---

## MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Yinlong Bai, Hongxin Zhang, Sheng Zhong, Junkai Niu, Hai Li, Yijia He, Yi Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2509.13536v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.



---

## SuNeRF-CME: Physics-Informed Neural Radiance Fields for Tomographic  Reconstruction of Coronal Mass Ejections

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Robert Jarolim, Martin Sanner, Chia-Man Hung, Emma Stevenson, Hala Lamdouar, Josh Veitch-Michaelis, Ioanna Bouri, Anna Malanushenko, Elena Provornikova, Vít Růžička, Carlos Urbina-Ortega | astro-ph.SR | [PDF](http://arxiv.org/pdf/2509.13571v1){: .btn .btn-green } |

**Abstract**: Coronagraphic observations enable direct monitoring of coronal mass ejections
(CMEs) through scattered light from free electrons, but determining the 3D
plasma distribution from 2D imaging data is challenging due to the
optically-thin plasma and the complex image formation processes. We introduce
SuNeRF-CME, a framework for 3D tomographic reconstructions of the heliosphere
using multi-viewpoint coronagraphic observations. The method leverages Neural
Radiance Fields (NeRFs) to estimate the electron density in the heliosphere
through a ray-tracing approach, while accounting for the underlying Thomson
scattering of image formation. The model is optimized by iteratively fitting
the time-dependent observational data. In addition, we apply physical
constraints in terms of continuity, propagation direction, and speed of the
heliospheric plasma to overcome limitations imposed by the sparse number of
viewpoints. We utilize synthetic observations of a CME simulation to fully
quantify the model's performance for different viewpoint configurations. The
results demonstrate that our method can reliably estimate the CME parameters
from only two viewpoints, with a mean velocity error of $3.01\pm1.94\%$ and
propagation direction errors of $3.39\pm1.94^\circ$ in latitude and
$1.76\pm0.79^\circ$ in longitude. We further show that our approach can achieve
a full 3D reconstruction of the simulated CME from two viewpoints, where we
correctly model the three-part structure, deformed CME front, and internal
plasma variations. Additional viewpoints can be seamlessly integrated, directly
enhancing the reconstruction of the plasma distribution in the heliosphere.
This study underscores the value of physics-informed methods for reconstructing
the heliospheric plasma distribution, paving the way for unraveling the dynamic
3D structure of CMEs and enabling advanced space weather monitoring.



---

## Exploring Metric Fusion for Evaluation of NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Shreyas Shivakumara, Gabriel Eilertsen, Karljohan Lundin Palmerius | cs.CV | [PDF](http://arxiv.org/pdf/2509.12836v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have demonstrated significant potential in
synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however,
remains a challenge due to the unique artifacts they exhibit, and no individual
metric performs well across all datasets. We hypothesize that combining two
successful metrics, Deep Image Structure and Texture Similarity (DISTS) and
Video Multi-Method Assessment Fusion (VMAF), based on different perceptual
methods, can overcome the limitations of individual metrics and achieve
improved correlation with subjective quality scores. We experiment with two
normalization strategies for the individual metrics and two fusion strategies
to evaluate their impact on the resulting correlation with the subjective
scores. The proposed pipeline is tested on two distinct datasets, Synthetic and
Outdoor, and its performance is evaluated across three different
configurations. We present a detailed analysis comparing the correlation
coefficients of fusion methods and individual scores with subjective scores to
demonstrate the robustness and generalizability of the fusion metrics.

Comments:
- Accepted for 17th International Conference on Quality of Multimedia
  Experience (QoMEX 25)

---

## Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single  Image


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Gaofeng Liu, Hengsen Li, Ruoyu Gao, Xuetong Li, Zhiyuan Ma, Tao Fang | cs.CV | [PDF](http://arxiv.org/pdf/2509.13013v1){: .btn .btn-green } |

**Abstract**: With the rapid advancement of 3D representation techniques and generative
models, substantial progress has been made in reconstructing full-body 3D
avatars from a single image. However, this task remains fundamentally
ill-posedness due to the limited information available from monocular input,
making it difficult to control the geometry and texture of occluded regions
during generation. To address these challenges, we redesign the reconstruction
pipeline and propose Dream3DAvatar, an efficient and text-controllable
two-stage framework for 3D avatar generation. In the first stage, we develop a
lightweight, adapter-enhanced multi-view generation model. Specifically, we
introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information
into SDXL, enforcing geometric and pose consistency across views. To preserve
facial identity, we incorporate ID-Adapter-G, which injects high-resolution
facial features into the generation process. Additionally, we leverage BLIP2 to
generate high-quality textual descriptions of the multi-view images, enhancing
text-driven controllability in occluded regions. In the second stage, we design
a feedforward Transformer model equipped with a multi-view feature fusion
module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS)
from the generated images. Furthermore, we introduce ID-Adapter-R, which
utilizes a gating mechanism to effectively fuse facial features into the
reconstruction process, improving high-frequency detail recovery. Extensive
experiments demonstrate that our method can generate realistic, animation-ready
3D avatars without any post-processing and consistently outperforms existing
baselines across multiple evaluation metrics.



---

## Effective Gaussian Management for High-fidelity Object Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Jiateng Liu, Hao Gao, Jiu-Cheng Xie, Chi-Man Pun, Jian Xiong, Haolun Li, Feng Xu | cs.CV | [PDF](http://arxiv.org/pdf/2509.12742v1){: .btn .btn-green } |

**Abstract**: This paper proposes an effective Gaussian management approach for
high-fidelity object reconstruction. Departing from recent Gaussian Splatting
(GS) methods that employ indiscriminate attribute assignment, our approach
introduces a novel densification strategy that dynamically activates spherical
harmonics (SHs) or normals under the supervision of a surface reconstruction
module, which effectively mitigates the gradient conflicts caused by dual
supervision and achieves superior reconstruction results. To further improve
representation efficiency, we develop a lightweight Gaussian representation
that adaptively adjusts the SH orders of each Gaussian based on gradient
magnitudes and performs task-decoupled pruning to remove Gaussian with minimal
impact on a reconstruction task without sacrificing others, which balances the
representational capacity with parameter quantity. Notably, our management
approach is model-agnostic and can be seamlessly integrated into other
frameworks, enhancing performance while reducing model size. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art approaches in both reconstruction quality and efficiency,
achieving superior performance with significantly fewer parameters.



---

## Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian  Splatting and Bag of Embeddings

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Abdalla Arafa, Didier Stricker | cs.CV | [PDF](http://arxiv.org/pdf/2509.12938v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis has seen significant advancements with 3D Gaussian
Splatting (3DGS), enabling real-time photorealistic rendering. However, the
inherent fuzziness of Gaussian Splatting presents challenges for 3D scene
understanding, restricting its broader applications in AR/VR and robotics.
While recent works attempt to learn semantics via 2D foundation model
distillation, they inherit fundamental limitations: alpha blending averages
semantics across objects, making 3D-level understanding impossible. We propose
a paradigm-shifting alternative that bypasses differentiable rendering for
semantics entirely. Our key insight is to leverage predecomposed object-level
Gaussians and represent each object through multiview CLIP feature aggregation,
creating comprehensive "bags of embeddings" that holistically describe objects.
This allows: (1) accurate open-vocabulary object retrieval by comparing text
queries to object-level (not Gaussian-level) embeddings, and (2) seamless task
adaptation: propagating object IDs to pixels for 2D segmentation or to
Gaussians for 3D extraction. Experiments demonstrate that our method
effectively overcomes the challenges of 3D open-vocabulary object extraction
while remaining comparable to state-of-the-art performance in 2D
open-vocabulary segmentation, ensuring minimal compromise.



---

## Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice  Vector Quantization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Hao Xu, Xiaolin Wu, Xi Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2509.13482v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

Comments:
- Code available at https://github.com/hxu160/SALVQ

---

## Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial  Vehicles

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-15 | Àlmos Veres-Vitàlyos, Genis Castillo Gomez-Raya, Filip Lemic, Daniel Johannes Bugelnig, Bernhard Rinner, Sergi Abadal, Xavier Costa-Pérez | cs.RO | [PDF](http://arxiv.org/pdf/2509.12458v1){: .btn .btn-green } |

**Abstract**: Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for
navigating indoor and hard-to-reach areas, yet their significant constraints in
payload and autonomy have largely prevented their use for complex tasks like
high-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we
introduce a novel system architecture that enables fully autonomous,
high-fidelity 3D scanning of static objects using UAVs weighing under 100
grams. Our core innovation lies in a dual-reconstruction pipeline that creates
a real-time feedback loop between data capture and flight control. A
near-real-time (near-RT) process uses Structure from Motion (SfM) to generate
an instantaneous pointcloud of the object. The system analyzes the model
quality on the fly and dynamically adapts the UAV's trajectory to intelligently
capture new images of poorly covered areas. This ensures comprehensive data
acquisition. For the final, detailed output, a non-real-time (non-RT) pipeline
employs a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR)
approach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB)
location data to achieve superior accuracy. We implemented and validated this
architecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both
single- and multi-UAV configurations, conclusively show that dynamic trajectory
adaptation consistently improves reconstruction quality over static flight
paths. This work demonstrates a scalable and autonomous solution that unlocks
the potential of miniaturized UAVs for fine-grained 3D reconstruction in
constrained environments, a capability previously limited to much larger
platforms.

Comments:
- 13 pages, 16 figures, 3 tables, 45 references

---

## Distributed 3D Gaussian Splatting for High-Resolution Isosurface  Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-15 | Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi | cs.DC | [PDF](http://arxiv.org/pdf/2509.12138v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique
for real-time, photorealistic rendering by optimizing anisotropic Gaussian
primitives from view-dependent images. While 3D-GS has been extended to
scientific visualization, prior work remains limited to single-GPU settings,
restricting scalability for large datasets on high-performance computing (HPC)
systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach
partitions data across nodes, trains Gaussian splats in parallel using
multi-nodes and multi-GPUs, and merges splats for global rendering. To
eliminate artifacts, we add ghost cells at partition boundaries and apply
background masks to remove irrelevant pixels. Benchmarks on the
Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup
across 8 nodes on Polaris while preserving image quality. These results
demonstrate that distributed 3D-GS enables scalable visualization of
large-scale scientific data and provide a foundation for future in situ
applications.



---

## A Controllable 3D Deepfake Generation Framework with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-15 | Wending Liu, Siyun Liang, Huy H. Nguyen, Isao Echizen | cs.CV | [PDF](http://arxiv.org/pdf/2509.11624v1){: .btn .btn-green } |

**Abstract**: We propose a novel 3D deepfake generation framework based on 3D Gaussian
Splatting that enables realistic, identity-preserving face swapping and
reenactment in a fully controllable 3D space. Compared to conventional 2D
deepfake approaches that suffer from geometric inconsistencies and limited
generalization to novel view, our method combines a parametric head model with
dynamic Gaussian representations to support multi-view consistent rendering,
precise expression control, and seamless background integration. To address
editing challenges in point-based representations, we explicitly separate the
head and background Gaussians and use pre-trained 2D guidance to optimize the
facial region across views. We further introduce a repair module to enhance
visual consistency under extreme poses and expressions. Experiments on
NeRSemble and additional evaluation videos demonstrate that our method achieves
comparable performance to state-of-the-art 2D approaches in identity
preservation, as well as pose and expression consistency, while significantly
outperforming them in multi-view rendering quality and 3D consistency. Our
approach bridges the gap between 3D modeling and deepfake synthesis, enabling
new directions for scene-aware, controllable, and immersive visual forgeries,
revealing the threat that emerging 3D Gaussian Splatting technique could be
used for manipulation attacks.



---

## Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-15 | Yi-Hsin Li, Thomas Sikora, Sebastian Knorr, Måarten Sjöström | cs.CV | [PDF](http://arxiv.org/pdf/2509.11853v1){: .btn .btn-green } |

**Abstract**: Sparse-view synthesis remains a challenging problem due to the difficulty of
recovering accurate geometry and appearance from limited observations. While
recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time
rendering with competitive quality, existing pipelines often rely on
Structure-from-Motion (SfM) for camera pose estimation, an approach that
struggles in genuinely sparse-view settings. Moreover, several SfM-free methods
replace SfM with multi-view stereo (MVS) models, but generate massive numbers
of 3D Gaussians by back-projecting every pixel into 3D space, leading to high
memory costs. We propose Segmentation-Driven Initialization for Gaussian
Splatting (SDI-GS), a method that mitigates inefficiency by leveraging
region-based segmentation to identify and retain only structurally significant
regions. This enables selective downsampling of the dense point cloud,
preserving scene fidelity while substantially reducing Gaussian count.
Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count
by up to 50% and achieves comparable or superior rendering quality in PSNR and
SSIM, with only marginal degradation in LPIPS. It further enables faster
training and lower memory footprint, advancing the practicality of 3DGS for
constrained-view scenarios.



---

## SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Ashkan Taghipour, Vahid Naghshin, Benjamin Southwell, Farid Boussaid, Hamid Laga, Mohammed Bennamoun | cs.CV | [PDF](http://arxiv.org/pdf/2509.11116v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.



---

## ROSGS: Relightable Outdoor Scenes With Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Lianjun Liao, Chunhui Zhang, Tong Wu, Henglei Lv, Bailin Deng, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2509.11275v1){: .btn .btn-green } |

**Abstract**: Image data captured outdoors often exhibit unbounded scenes and
unconstrained, varying lighting conditions, making it challenging to decompose
them into geometry, reflectance, and illumination. Recent works have focused on
achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D
Gaussian Splatting (3DGS) representation but remain hindered by two key
limitations: the high computational overhead associated with neural networks of
NeRF and the use of low-frequency lighting representations, which often result
in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS,
a two-stage pipeline designed to efficiently reconstruct relightable outdoor
scenes using the Gaussian Splatting representation. By leveraging monocular
normal priors, ROSGS first reconstructs the scene's geometry with the compact
2D Gaussian Splatting (2DGS) representation, providing an efficient and
accurate geometric foundation. Building upon this reconstructed geometry, ROSGS
then decomposes the scene's texture and lighting through a hybrid lighting
model. This model effectively represents typical outdoor lighting by employing
a spherical Gaussian function to capture the directional, high-frequency
components of sunlight, while learning a radiance transfer function via
Spherical Harmonic coefficients to model the remaining low-frequency skylight
comprehensively. Both quantitative metrics and qualitative comparisons
demonstrate that ROSGS achieves state-of-the-art performance in relighting
outdoor scenes and highlight its ability to deliver superior relighting
accuracy and rendering efficiency.



---

## Multispectral-NeRF:a multispectral modeling approach based on neural  radiance fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Hong Zhang, Fei Guo, Zihan Xie, Dizhao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2509.11169v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.



---

## On the Skinning of Gaussian Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Nikolaos Zioulis, Nikolaos Kotarelas, Georgios Albanis, Spyridon Thermos, Anargyros Chatzitofis | cs.CV | [PDF](http://arxiv.org/pdf/2509.11411v1){: .btn .btn-green } |

**Abstract**: Radiance field-based methods have recently been used to reconstruct human
avatars, showing that we can significantly downscale the systems needed for
creating animated human avatars. Although this progress has been initiated by
neural radiance fields, their slow rendering and backward mapping from the
observation space to the canonical space have been the main challenges. With
Gaussian splatting overcoming both challenges, a new family of approaches has
emerged that are faster to train and render, while also straightforward to
implement using forward skinning from the canonical to the observation space.
However, the linear blend skinning required for the deformation of the
Gaussians does not provide valid results for their non-linear rotation
properties. To address such artifacts, recent works use mesh properties to
rotate the non-linear Gaussian properties or train models to predict corrective
offsets. Instead, we propose a weighted rotation blending approach that
leverages quaternion averaging. This leads to simpler vertex-based Gaussians
that can be efficiently animated and integrated in any engine by only modifying
the linear blend skinning technique, and using any Gaussian rasterizer.



---

## SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene  Completion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Zhiwen Yang, Yuxin Peng | cs.CV | [PDF](http://arxiv.org/pdf/2509.11171v1){: .btn .btn-green } |

**Abstract**: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

Comments:
- 10 pages, 6 figures

---

## Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing  for Physics-based Camera Effect Data Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-13 | Yi-Ruei Liu, You-Zhe Xie, Yu-Hsiang Hsu, I-Sheng Fang, Yu-Lun Liu, Jun-Cheng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2509.10759v1){: .btn .btn-green } |

**Abstract**: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.



---

## AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-13 | Gurutva Patle, Nilay Girgaonkar, Nagabhushan Somraj, Rajiv Soundararajan | cs.GR | [PDF](http://arxiv.org/pdf/2509.11003v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel
view synthesis. However, it often struggles under sparse-view settings,
producing undesirable artifacts such as floaters, inaccurate geometry, and
overfitting due to limited observations. We find that a key contributing factor
is uncontrolled densification, where adding Gaussian primitives rapidly without
guidance can harm geometry and cause artifacts. We propose AD-GS, a novel
alternating densification framework that interleaves high and low densification
phases. During high densification, the model densifies aggressively, followed
by photometric loss based training to capture fine-grained scene details. Low
densification then primarily involves aggressive opacity pruning of Gaussians
followed by regularizing their geometry through pseudo-view consistency and
edge-aware depth smoothness. This alternating approach helps reduce overfitting
by carefully controlling model capacity growth while progressively refining the
scene representation. Extensive experiments on challenging datasets demonstrate
that AD-GS significantly improves rendering quality and geometric consistency
compared to existing methods.

Comments:
- SIGGRAPH Asia 2025

---

## T2Bs: Text-to-Character Blendshapes via Video Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-12 | Jiahao Luo, Chaoyang Wang, Michael Vasilkovsky, Vladislav Shakhrai, Di Liu, Peiye Zhuang, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee, James Davis, Jian Wang | cs.GR | [PDF](http://arxiv.org/pdf/2509.10678v1){: .btn .btn-green } |

**Abstract**: We present T2Bs, a framework for generating high-quality, animatable
character head morphable models from text by combining static text-to-3D
generation with video diffusion. Text-to-3D models produce detailed static
geometry but lack motion synthesis, while video diffusion models generate
motion with temporal and multi-view geometric inconsistencies. T2Bs bridges
this gap by leveraging deformable 3D Gaussian splatting to align static 3D
assets with video outputs. By constraining motion with static geometry and
employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D
generation methods in accuracy and expressiveness while reducing video
artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent,
fully registered 3D geometries designed to scale for building morphable models
with diverse, realistic facial motions. This enables synthesizing expressive,
animatable character heads that surpass current 4D generation techniques.



---

## On the Geometric Accuracy of Implicit and Primitive-based  Representations Derived from View Rendering Constraints

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-12 | Elias De Smijter, Renaud Detry, Christophe De Vleeschouwer | cs.CV | [PDF](http://arxiv.org/pdf/2509.10241v2){: .btn .btn-green } |

**Abstract**: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

Comments:
- 9 pages, 3 figures, to be presented at ASTRA25,

---

## DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via  Direct SDF Learning

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Wenzhi Guo, Bing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.07493v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for
photorealistic view synthesis, representing scenes with spatially distributed
Gaussian primitives. While highly effective for rendering, achieving accurate
and complete surface reconstruction remains challenging due to the unstructured
nature of the representation and the absence of explicit geometric supervision.
In this work, we propose DiGS, a unified framework that embeds Signed Distance
Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong
and interpretable surface priors. By associating each Gaussian with a learnable
SDF value, DiGS explicitly aligns primitives with underlying geometry and
improves cross-view consistency. To further ensure dense and coherent coverage,
we design a geometry-guided grid growth strategy that adaptively distributes
Gaussians along geometry-consistent regions under a multi-scale hierarchy.
Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and
Tanks& Temples, demonstrate that DiGS consistently improves reconstruction
accuracy and completeness while retaining high rendering fidelity.



---

## DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Ze-Xin Yin, Jiaxiong Qiu, Liu Liu, Xinjie Wang, Wei Sui, Zhizhong Su, Jian Yang, Jin Xie | cs.CV | [PDF](http://arxiv.org/pdf/2509.07435v1){: .btn .btn-green } |

**Abstract**: The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.

Comments:
- 14 pages, 7 figures, project page:
  https://zx-yin.github.io/dreamlifting/

---

## SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Mahtab Dahaghin, Milind G. Padalkar, Matteo Toso, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2509.07809v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D
scene representations from sets of multi-view images. However, inpainting
missing regions, whether due to occlusion or scene editing, remains a
challenging task, often leading to blurry details, artifacts, and inconsistent
geometry. In this work, we introduce SplatFill, a novel depth-guided approach
for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and
improved efficiency. Our method combines two key ideas: (1) joint depth-based
and object-based supervision to ensure inpainted Gaussians are accurately
placed in 3D space and aligned with surrounding geometry, and (2) we propose a
consistency-aware refinement scheme that selectively identifies and corrects
inconsistent regions without disrupting the rest of the scene. Evaluations on
the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing
NeRF-based and 3DGS-based inpainting methods in visual fidelity but also
reduces training time by 24.5%. Qualitative results show our method delivers
sharper details, fewer artifacts, and greater coherence across challenging
viewpoints.



---

## HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Yimin Pan, Matthias Nießner, Tobias Kirschstein | cs.CV | [PDF](http://arxiv.org/pdf/2509.07774v1){: .btn .btn-green } |

**Abstract**: Human hair reconstruction is a challenging problem in computer vision, with
growing importance for applications in virtual reality and digital human
modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient
and explicit scene representations that naturally align with the structure of
hair strands. In this work, we extend the 3DGS framework to enable strand-level
hair geometry reconstruction from multi-view images. Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.
  While existing methods typically evaluate reconstruction quality at the
geometric level, they often neglect the connectivity and topology of hair
strands. To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction. Extensive
experiments on both synthetic and real-world datasets demonstrate that our
method robustly handles a wide range of hairstyles and achieves efficient
reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/

Comments:
- This is the arXiv preprint of the paper "Hair Strand Reconstruction
  based on 3D Gaussian Splatting" published at BMVC 2025. Project website:
  https://yimin-pan.github.io/hair-gs/

---

## 3DOF+Quantization: 3DGS quantization for large scenes with limited  Degrees of Freedom

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Matthieu Gendrin, Stéphane Pateux, Théo Ladune | cs.CV | [PDF](http://arxiv.org/pdf/2509.06400v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene
reconstruction. With a number of views of a given object or scene, the
algorithm trains a model composed of 3D gaussians, which enables the production
of novel views from arbitrary points of view. This freedom of movement is
referred to as 6DoF for 6 degrees of freedom: a view is produced for any
position (3 degrees), orientation of camera (3 other degrees). On large scenes,
though, the input views are acquired from a limited zone in space, and the
reconstruction is valuable for novel views from the same zone, even if the
scene itself is almost unlimited in size. We refer to this particular case as
3DoF+, meaning that the 3 degrees of freedom of camera position are limited to
small offsets around the central position. Considering the problem of
coordinate quantization, the impact of position error on the projection error
in pixels is studied. It is shown that the projection error is proportional to
the squared inverse distance of the point being projected. Consequently, a new
quantization scheme based on spherical coordinates is proposed. Rate-distortion
performance of the proposed method are illustrated on the well-known Garden
scene.



---

## VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level  Guidance in Large Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2509.06685v3){: .btn .btn-green } |

**Abstract**: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for
novel-view synthesis (NVS) in large scenes. GS typically requires accurate
depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited
depth sensing range makes it difficult for GS to work in large scenes.
Monocular images, however, lack depth to guide the learning and lead to
inferior NVS results. Although large foundation models (LFMs) for monocular
depth estimation are available, they suffer from cross-frame inconsistency,
inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This
paper aims to generate dense, accurate depth images from monocular RGB inputs
for high-definite GS rendering. The key idea is to leverage the accurate but
sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the
dense but coarse depth from LFMs. To bridge the sparse input and dense output,
we propose an object-segmented depth propagation algorithm that renders the
depth of pixels of structured objects. Then we develop a dynamic depth
refinement module to handle the crippled SfM depth of dynamic objects and
refine the coarse LFM depth. Experiments using public and customized datasets
demonstrate the superior rendering quality of VIM-GS in large scenes.

Comments:
- Withdrawn due to an error in the author list & incomplete
  experimental results

---

## Real-time Photorealistic Mapping for Situational Awareness in Robot  Teleoperation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Ian Page, Pierre Susbielle, Olivier Aycard, Pierre-Brice Wieber | cs.RO | [PDF](http://arxiv.org/pdf/2509.06433v2){: .btn .btn-green } |

**Abstract**: Achieving efficient remote teleoperation is particularly challenging in
unknown environments, as the teleoperator must rapidly build an understanding
of the site's layout. Online 3D mapping is a proven strategy to tackle this
challenge, as it enables the teleoperator to progressively explore the site
from multiple perspectives. However, traditional online map-based teleoperation
systems struggle to generate visually accurate 3D maps in real-time due to the
high computational cost involved, leading to poor teleoperation performances.
In this work, we propose a solution to improve teleoperation efficiency in
unknown environments. Our approach proposes a novel, modular and efficient
GPU-based integration between recent advancement in gaussian splatting SLAM and
existing online map-based teleoperation systems. We compare the proposed
solution against state-of-the-art teleoperation systems and validate its
performances through real-world experiments using an aerial vehicle. The
results show significant improvements in decision-making speed and more
accurate interaction with the environment, leading to greater teleoperation
efficiency. In doing so, our system enhances remote teleoperation by seamlessly
integrating photorealistic mapping generation with real-time performances,
enabling effective teleoperation in unfamiliar environments.



---

## MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians  and Unified Pruning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-07 | Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.07021v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis
technique, but its high memory consumption severely limits its applicability on
edge devices. A growing number of 3DGS compression methods have been proposed
to make 3DGS more efficient, yet most only focus on storage compression and
fail to address the critical bottleneck of rendering memory. To address this
problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that
tackles this challenge by jointly optimizing two key factors: the total
primitive number and the parameters per primitive, achieving unprecedented
memory compression. Specifically, we replace the memory-intensive spherical
harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our
color representations. More importantly, we propose a unified soft pruning
framework that models primitive-number and lobe-number pruning as a single
constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a
50% static VRAM reduction and a 40% rendering VRAM reduction compared to
existing methods, while maintaining comparable rendering quality.

Comments:
- 14 pages, 4 figures

---

## CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Hannah Schieber, Dominik Frischmann, Victor Schaack, Simon Boche, Angela Schoellig, Stefan Leutenegger, Daniel Roth | cs.CV | [PDF](http://arxiv.org/pdf/2509.04859v2){: .btn .btn-green } |

**Abstract**: Mobile reconstruction has the potential to support time-critical tasks such
as tele-guidance and disaster response, where operators must quickly gain an
accurate understanding of the environment. Full high-fidelity scene
reconstruction is computationally expensive and often unnecessary when only
specific points of interest (POIs) matter for timely decision making. We
address this challenge with CoRe-GS, a semantic POI-focused extension of
Gaussian Splatting (GS). Instead of optimizing every scene element uniformly,
CoRe-GS first produces a fast segmentation-ready GS representation and then
selectively refines splats belonging to semantically relevant POIs detected
during data acquisition. This targeted refinement reduces training time to 25\%
compared to full semantic GS while improving novel view synthesis quality in
the areas that matter most. We validate CoRe-GS on both real-world (SCRREAM)
and synthetic (NeRDS 360) datasets, demonstrating that prioritizing POIs
enables faster and higher-quality mobile reconstruction tailored to operational
needs.



---

## Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation  in 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Sen Wang, Kunyi Li, Siyun Liang, Elena Alegret, Jing Ma, Nassir Navab, Stefano Gasperini | cs.CV | [PDF](http://arxiv.org/pdf/2509.05515v1){: .btn .btn-green } |

**Abstract**: Recently, distilling open-vocabulary language features from 2D images into 3D
Gaussians has attracted significant attention. Although existing methods
achieve impressive language-based interactions of 3D scenes, we observe two
fundamental issues: background Gaussians contributing negligibly to a rendered
pixel get the same feature as the dominant foreground ones, and multi-view
inconsistencies due to view-specific noise in language embeddings. We introduce
Visibility-Aware Language Aggregation (VALA), a lightweight yet effective
method that computes marginal contributions for each ray and applies a
visibility-aware gate to retain only visible Gaussians. Moreover, we propose a
streaming weighted geometric median in cosine space to merge noisy multi-view
features. Our method yields a robust, view-consistent language feature
embedding in a fast and memory-efficient manner. VALA improves open-vocabulary
localization and segmentation across reference datasets, consistently
surpassing existing works.



---

## GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Yangming Li, Chaoyu Liu, Lihao Liu, Simon Masnou, Carola-Bibian Schönlieb | cs.CV | [PDF](http://arxiv.org/pdf/2509.05075v1){: .btn .btn-green } |

**Abstract**: A few recent works explored incorporating geometric priors to regularize the
optimization of Gaussian splatting, further improving its performance. However,
those early studies mainly focused on the use of low-order geometric priors
(e.g., normal vector), and they are also unreliably estimated by
noise-sensitive methods, like local principal component analysis. To address
their limitations, we first present GeoSplat, a general geometry-constrained
optimization framework that exploits both first-order and second-order
geometric quantities to improve the entire training pipeline of Gaussian
splatting, including Gaussian initialization, gradient update, and
densification. As an example, we initialize the scales of 3D Gaussian
primitives in terms of principal curvatures, leading to a better coverage of
the object surface than random initialization. Secondly, based on certain
geometric structures (e.g., local manifold), we introduce efficient and
noise-robust estimation methods that provide dynamic geometric priors for our
framework. We conduct extensive experiments on multiple datasets for novel view
synthesis, showing that our framework: GeoSplat, significantly improves the
performance of Gaussian splatting and outperforms previous baselines.



---

## Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface  Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi | cs.DC | [PDF](http://arxiv.org/pdf/2509.05216v1){: .btn .btn-green } |

**Abstract**: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.



---

## SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-04 | Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu | cs.CV | [PDF](http://arxiv.org/pdf/2509.04379v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.



---

## ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast,  Memory-Efficient Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-03 | Sankeerth Durvasula, Sharanshangar Muhunthan, Zain Moustafa, Richard Chen, Ruofan Liang, Yushi Guan, Nilesh Ahuja, Nilesh Jain, Selvakumar Panneer, Nandita Vijaykumar | cs.GR | [PDF](http://arxiv.org/pdf/2509.03775v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world
scenes with high quality and real-time rendering. Typically, a higher quality
representation can be achieved by using a large number of 3D Gaussians.
However, using large 3D Gaussian counts significantly increases the GPU device
memory for storing model parameters. A large model thus requires powerful GPUs
with high memory capacities for training and has slower training/rendering
latencies due to the inefficiencies of memory access and data movement. In this
work, we introduce ContraGS, a method to enable training directly on compressed
3DGS representations without reducing the Gaussian Counts, and thus with a
little loss in model quality. ContraGS leverages codebooks to compactly store a
set of Gaussian parameter vectors throughout the training process, thereby
significantly reducing memory consumption. While codebooks have been
demonstrated to be highly effective at compressing fully trained 3DGS models,
directly training using codebook representations is an unsolved challenge.
ContraGS solves the problem of learning non-differentiable parameters in
codebook-compressed representations by posing parameter estimation as a
Bayesian inference problem. To this end, ContraGS provides a framework that
effectively uses MCMC sampling to sample over a posterior distribution of these
compressed representations. With ContraGS, we demonstrate that ContraGS
significantly reduces the peak memory during training (on average 3.49X) and
accelerated training and rendering (1.36X and 1.88X on average, respectively),
while retraining close to state-of-art quality.



---

## GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned  Residuals

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Mohit Mendiratta, Mayur Deshmukh, Kartik Teotia, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt | cs.GR | [PDF](http://arxiv.org/pdf/2509.02141v1){: .btn .btn-green } |

**Abstract**: 3D Morphable Models (3DMMs) enable controllable facial geometry and
expression editing for reconstruction, animation, and AR/VR, but traditional
PCA-based mesh models are limited in resolution, detail, and photorealism.
Neural volumetric methods improve realism but remain too slow for interactive
use. Recent Gaussian Splatting (3DGS) based facial models achieve fast,
high-quality rendering but still depend solely on a mesh-based 3DMM prior for
expression control, limiting their ability to capture fine-grained geometry,
expressions, and full-head coverage. We introduce GRMM, the first full-head
Gaussian 3D morphable model that augments a base 3DMM with residual geometry
and appearance components, additive refinements that recover high-frequency
details such as wrinkles, fine skin texture, and hairline variations. GRMM
provides disentangled control through low-dimensional, interpretable parameters
(e.g., identity shape, facial expressions) while separately modelling residuals
that capture subject- and expression-specific detail beyond the base model's
capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders
represent per-Gaussian appearance, and a lightweight CNN refines rasterised
images for enhanced realism, all while maintaining 75 FPS real-time rendering.
To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first
dataset with 60 aligned expressions across 50 identities, enabling robust
disentanglement of identity and expression in Gaussian-based 3DMMs. Across
monocular 3D face reconstruction, novel-view synthesis, and expression
transfer, GRMM surpasses state-of-the-art methods in fidelity and expression
accuracy while delivering interactive real-time performance.

Comments:
- Project page: https://mohitm1994.github.io/GRMM/

---

## Efficient Geometry Compression and Communication for 3D Gaussian  Splatting Point Clouds


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Liang Xie, Yanting Li, Luyang Tang, Wei Gao | cs.MM | [PDF](http://arxiv.org/pdf/2509.02232v1){: .btn .btn-green } |

**Abstract**: Storage and transmission challenges in dynamic 3D scene representation based
on the i3DV platform, With increasing scene complexity, the explosive growth of
3D Gaussian data volume causes excessive storage space occupancy. To address
this issue, we propose adopting the AVS PCRM reference software for efficient
compression of Gaussian point cloud geometry data. The strategy deeply
integrates the advanced encoding capabilities of AVS PCRM into the i3DV
platform, forming technical complementarity with the original rate-distortion
optimization mechanism based on binary hash tables. On one hand, the hash table
efficiently caches inter-frame Gaussian point transformation relationships,
which allows for high-fidelity transmission within a 40 Mbps bandwidth
constraint. On the other hand, AVS PCRM performs precise compression on
geometry data. Experimental results demonstrate that the joint framework
maintains the advantages of fast rendering and high-quality synthesis in 3D
Gaussian technology while achieving significant 10\%-25\% bitrate savings on
universal test sets. It provides a superior rate-distortion tradeoff solution
for the storage, transmission, and interaction of 3D volumetric video.

Comments:
- 8 pages,5 figures

---

## 2D Gaussian Splatting with Semantic Alignment for Image Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Hongyu Li, Chaofeng Chen, Xiaoming Li, Guangming Lu | cs.CV | [PDF](http://arxiv.org/pdf/2509.01964v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS), a recent technique for converting discrete points
into continuous spatial representations, has shown promising results in 3D
scene modeling and 2D image super-resolution. In this paper, we explore its
untapped potential for image inpainting, which demands both locally coherent
pixel synthesis and globally consistent semantic restoration. We propose the
first image inpainting framework based on 2D Gaussian Splatting, which encodes
incomplete images into a continuous field of 2D Gaussian splat coefficients and
reconstructs the final image via a differentiable rasterization process. The
continuous rendering paradigm of GS inherently promotes pixel-level coherence
in the inpainted results. To improve efficiency and scalability, we introduce a
patch-wise rasterization strategy that reduces memory overhead and accelerates
inference. For global semantic consistency, we incorporate features from a
pretrained DINO model. We observe that DINO's global features are naturally
robust to small missing regions and can be effectively adapted to guide
semantic alignment in large-mask scenarios, ensuring that the inpainted content
remains contextually consistent with the surrounding scene. Extensive
experiments on standard benchmarks demonstrate that our method achieves
competitive performance in both quantitative metrics and perceptual quality,
establishing a new direction for applying Gaussian Splatting to 2D image
processing.



---

## Im2Haircut: Single-view Strand-based Hair Reconstruction for Human  Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-01 | Vanessa Sklyarova, Egor Zakharov, Malte Prinzler, Giorgio Becherini, Michael J. Black, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2509.01469v1){: .btn .btn-green } |

**Abstract**: We present a novel approach for 3D hair reconstruction from single
photographs based on a global hair prior combined with local optimization.
Capturing strand-based hair geometry from single photographs is challenging due
to the variety and geometric complexity of hairstyles and the lack of ground
truth training data. Classical reconstruction methods like multi-view stereo
only reconstruct the visible hair strands, missing the inner structure of
hairstyles and hampering realistic hair simulation. To address this, existing
methods leverage hairstyle priors trained on synthetic data. Such data,
however, is limited in both quantity and quality since it requires manual work
from skilled artists to model the 3D hairstyles and create near-photorealistic
renderings. To address this, we propose a novel approach that uses both, real
and synthetic data to learn an effective hairstyle prior. Specifically, we
train a transformer-based prior model on synthetic data to obtain knowledge of
the internal hairstyle geometry and introduce real data in the learning process
to model the outer structure. This training scheme is able to model the visible
hair strands depicted in an input image, while preserving the general 3D
structure of hairstyles. We exploit this prior to create a
Gaussian-splatting-based reconstruction method that creates hairstyles from one
or more images. Qualitative and quantitative comparisons with existing
reconstruction pipelines demonstrate the effectiveness and superior performance
of our method for capturing detailed hair orientation, overall silhouette, and
backside consistency. For additional results and code, please refer to
https://im2haircut.is.tue.mpg.de.

Comments:
- For more results please refer to the project page
  https://im2haircut.is.tue.mpg.de

---

## GaussianGAN: Real-Time Photorealistic controllable Human Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-01 | Mohamed Ilyes Lakhal, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2509.01681v1){: .btn .btn-green } |

**Abstract**: Photorealistic and controllable human avatars have gained popularity in the
research community thanks to rapid advances in neural rendering, providing fast
and realistic synthesis tools. However, a limitation of current solutions is
the presence of noticeable blurring. To solve this problem, we propose
GaussianGAN, an animatable avatar approach developed for photorealistic
rendering of people in real-time. We introduce a novel Gaussian splatting
densification strategy to build Gaussian points from the surface of cylindrical
structures around estimated skeletal limbs. Given the camera calibration, we
render an accurate semantic segmentation with our novel view segmentation
module. Finally, a UNet generator uses the rendered Gaussian splatting features
and the segmentation maps to create photorealistic digital avatars. Our method
runs in real-time with a rendering speed of 79 FPS. It outperforms previous
methods regarding visual perception and quality, achieving a state-of-the-art
results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and
33.39db on the Thuman4 dataset.

Comments:
- IEEE conference series on Automatic Face and Gesture Recognition 2025
