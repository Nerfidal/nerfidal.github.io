---
layout: default
title: September 2025
parent: Papers
nav_order: 202509
---

<!---metadata--->


## SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Mahtab Dahaghin, Milind G. Padalkar, Matteo Toso, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2509.07809v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D
scene representations from sets of multi-view images. However, inpainting
missing regions, whether due to occlusion or scene editing, remains a
challenging task, often leading to blurry details, artifacts, and inconsistent
geometry. In this work, we introduce SplatFill, a novel depth-guided approach
for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and
improved efficiency. Our method combines two key ideas: (1) joint depth-based
and object-based supervision to ensure inpainted Gaussians are accurately
placed in 3D space and aligned with surrounding geometry, and (2) we propose a
consistency-aware refinement scheme that selectively identifies and corrects
inconsistent regions without disrupting the rest of the scene. Evaluations on
the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing
NeRF-based and 3DGS-based inpainting methods in visual fidelity but also
reduces training time by 24.5%. Qualitative results show our method delivers
sharper details, fewer artifacts, and greater coherence across challenging
viewpoints.



---

## DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Ze-Xin Yin, Jiaxiong Qiu, Liu Liu, Xinjie Wang, Wei Sui, Zhizhong Su, Jian Yang, Jin Xie | cs.CV | [PDF](http://arxiv.org/pdf/2509.07435v1){: .btn .btn-green } |

**Abstract**: The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.

Comments:
- 14 pages, 7 figures, project page:
  https://zx-yin.github.io/dreamlifting/

---

## HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Yimin Pan, Matthias Nie√üner, Tobias Kirschstein | cs.CV | [PDF](http://arxiv.org/pdf/2509.07774v1){: .btn .btn-green } |

**Abstract**: Human hair reconstruction is a challenging problem in computer vision, with
growing importance for applications in virtual reality and digital human
modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient
and explicit scene representations that naturally align with the structure of
hair strands. In this work, we extend the 3DGS framework to enable strand-level
hair geometry reconstruction from multi-view images. Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.
  While existing methods typically evaluate reconstruction quality at the
geometric level, they often neglect the connectivity and topology of hair
strands. To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction. Extensive
experiments on both synthetic and real-world datasets demonstrate that our
method robustly handles a wide range of hairstyles and achieves efficient
reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/

Comments:
- This is the arXiv preprint of the paper "Hair Strand Reconstruction
  based on 3D Gaussian Splatting" published at BMVC 2025. Project website:
  https://yimin-pan.github.io/hair-gs/

---

## DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via  Direct SDF Learning

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Wenzhi Guo, Bing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.07493v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for
photorealistic view synthesis, representing scenes with spatially distributed
Gaussian primitives. While highly effective for rendering, achieving accurate
and complete surface reconstruction remains challenging due to the unstructured
nature of the representation and the absence of explicit geometric supervision.
In this work, we propose DiGS, a unified framework that embeds Signed Distance
Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong
and interpretable surface priors. By associating each Gaussian with a learnable
SDF value, DiGS explicitly aligns primitives with underlying geometry and
improves cross-view consistency. To further ensure dense and coherent coverage,
we design a geometry-guided grid growth strategy that adaptively distributes
Gaussians along geometry-consistent regions under a multi-scale hierarchy.
Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and
Tanks& Temples, demonstrate that DiGS consistently improves reconstruction
accuracy and completeness while retaining high rendering fidelity.



---

## Real-time Photorealistic Mapping for Situational Awareness in Robot  Teleoperation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Ian Page, Pierre Susbielle, Olivier Aycard, Pierre-Brice Wieber | cs.RO | [PDF](http://arxiv.org/pdf/2509.06433v1){: .btn .btn-green } |

**Abstract**: Achieving efficient remote teleoperation is particularly challenging in
unknown environments, as the teleoperator must rapidly build an understanding
of the site's layout. Online 3D mapping is a proven strategy to tackle this
challenge, as it enables the teleoperator to progressively explore the site
from multiple perspectives. However, traditional online map-based teleoperation
systems struggle to generate visually accurate 3D maps in real-time due to the
high computational cost involved, leading to poor teleoperation performances.
In this work, we propose a solution to improve teleoperation efficiency in
unknown environments. Our approach proposes a novel, modular and efficient
GPU-based integration between recent advancement in gaussian splatting SLAM and
existing online map-based teleoperation systems. We compare the proposed
solution against state-of-the-art teleoperation systems and validate its
performances through real-world experiments using an aerial vehicle. The
results show significant improvements in decision-making speed and more
accurate interaction with the environment, leading to greater teleoperation
efficiency. In doing so, our system enhances remote teleoperation by seamlessly
integrating photorealistic mapping generation with real-time performances,
enabling effective teleoperation in unfamiliar environments.



---

## VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level  Guidance in Large Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2509.06685v3){: .btn .btn-green } |

**Abstract**: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for
novel-view synthesis (NVS) in large scenes. GS typically requires accurate
depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited
depth sensing range makes it difficult for GS to work in large scenes.
Monocular images, however, lack depth to guide the learning and lead to
inferior NVS results. Although large foundation models (LFMs) for monocular
depth estimation are available, they suffer from cross-frame inconsistency,
inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This
paper aims to generate dense, accurate depth images from monocular RGB inputs
for high-definite GS rendering. The key idea is to leverage the accurate but
sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the
dense but coarse depth from LFMs. To bridge the sparse input and dense output,
we propose an object-segmented depth propagation algorithm that renders the
depth of pixels of structured objects. Then we develop a dynamic depth
refinement module to handle the crippled SfM depth of dynamic objects and
refine the coarse LFM depth. Experiments using public and customized datasets
demonstrate the superior rendering quality of VIM-GS in large scenes.

Comments:
- Withdrawn due to an error in the author list & incomplete
  experimental results

---

## 3DOF+Quantization: 3DGS quantization for large scenes with limited  Degrees of Freedom

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Matthieu Gendrin, St√©phane Pateux, Th√©o Ladune | cs.CV | [PDF](http://arxiv.org/pdf/2509.06400v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene
reconstruction. With a number of views of a given object or scene, the
algorithm trains a model composed of 3D gaussians, which enables the production
of novel views from arbitrary points of view. This freedom of movement is
referred to as 6DoF for 6 degrees of freedom: a view is produced for any
position (3 degrees), orientation of camera (3 other degrees). On large scenes,
though, the input views are acquired from a limited zone in space, and the
reconstruction is valuable for novel views from the same zone, even if the
scene itself is almost unlimited in size. We refer to this particular case as
3DoF+, meaning that the 3 degrees of freedom of camera position are limited to
small offsets around the central position. Considering the problem of
coordinate quantization, the impact of position error on the projection error
in pixels is studied. It is shown that the projection error is proportional to
the squared inverse distance of the point being projected. Consequently, a new
quantization scheme based on spherical coordinates is proposed. Rate-distortion
performance of the proposed method are illustrated on the well-known Garden
scene.



---

## MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians  and Unified Pruning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-07 | Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.07021v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis
technique, but its high memory consumption severely limits its applicability on
edge devices. A growing number of 3DGS compression methods have been proposed
to make 3DGS more efficient, yet most only focus on storage compression and
fail to address the critical bottleneck of rendering memory. To address this
problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that
tackles this challenge by jointly optimizing two key factors: the total
primitive number and the parameters per primitive, achieving unprecedented
memory compression. Specifically, we replace the memory-intensive spherical
harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our
color representations. More importantly, we propose a unified soft pruning
framework that models primitive-number and lobe-number pruning as a single
constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a
50% static VRAM reduction and a 40% rendering VRAM reduction compared to
existing methods, while maintaining comparable rendering quality.

Comments:
- 14 pages, 4 figures

---

## Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation  in 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Sen Wang, Kunyi Li, Siyun Liang, Elena Alegret, Jing Ma, Nassir Navab, Stefano Gasperini | cs.CV | [PDF](http://arxiv.org/pdf/2509.05515v1){: .btn .btn-green } |

**Abstract**: Recently, distilling open-vocabulary language features from 2D images into 3D
Gaussians has attracted significant attention. Although existing methods
achieve impressive language-based interactions of 3D scenes, we observe two
fundamental issues: background Gaussians contributing negligibly to a rendered
pixel get the same feature as the dominant foreground ones, and multi-view
inconsistencies due to view-specific noise in language embeddings. We introduce
Visibility-Aware Language Aggregation (VALA), a lightweight yet effective
method that computes marginal contributions for each ray and applies a
visibility-aware gate to retain only visible Gaussians. Moreover, we propose a
streaming weighted geometric median in cosine space to merge noisy multi-view
features. Our method yields a robust, view-consistent language feature
embedding in a fast and memory-efficient manner. VALA improves open-vocabulary
localization and segmentation across reference datasets, consistently
surpassing existing works.



---

## GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Yangming Li, Chaoyu Liu, Lihao Liu, Simon Masnou, Carola-Bibian Sch√∂nlieb | cs.CV | [PDF](http://arxiv.org/pdf/2509.05075v1){: .btn .btn-green } |

**Abstract**: A few recent works explored incorporating geometric priors to regularize the
optimization of Gaussian splatting, further improving its performance. However,
those early studies mainly focused on the use of low-order geometric priors
(e.g., normal vector), and they are also unreliably estimated by
noise-sensitive methods, like local principal component analysis. To address
their limitations, we first present GeoSplat, a general geometry-constrained
optimization framework that exploits both first-order and second-order
geometric quantities to improve the entire training pipeline of Gaussian
splatting, including Gaussian initialization, gradient update, and
densification. As an example, we initialize the scales of 3D Gaussian
primitives in terms of principal curvatures, leading to a better coverage of
the object surface than random initialization. Secondly, based on certain
geometric structures (e.g., local manifold), we introduce efficient and
noise-robust estimation methods that provide dynamic geometric priors for our
framework. We conduct extensive experiments on multiple datasets for novel view
synthesis, showing that our framework: GeoSplat, significantly improves the
performance of Gaussian splatting and outperforms previous baselines.



---

## CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Hannah Schieber, Dominik Frischmann, Simon Boche, Victor Schaack, Angela Schoellig, Stefan Leutenegger, Daniel Roth | cs.CV | [PDF](http://arxiv.org/pdf/2509.04859v1){: .btn .btn-green } |

**Abstract**: Mobile reconstruction for autonomous aerial robotics holds strong potential
for critical applications such as tele-guidance and disaster response. These
tasks demand both accurate 3D reconstruction and fast scene processing. Instead
of reconstructing the entire scene in detail, it is often more efficient to
focus on specific objects, i.e., points of interest (PoIs). Mobile robots
equipped with advanced sensing can usually detect these early during data
acquisition or preliminary analysis, reducing the need for full-scene
optimization. Gaussian Splatting (GS) has recently shown promise in delivering
high-quality novel view synthesis and 3D representation by an incremental
learning process. Extending GS with scene editing, semantics adds useful
per-splat features to isolate objects effectively.
  Semantic 3D Gaussian editing can already be achieved before the full training
cycle is completed, reducing the overall training time. Moreover, the
semantically relevant area, the PoI, is usually already known during capturing.
To balance high-quality reconstruction with reduced training time, we propose
CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS
and then refine it for the semantic object using our novel color-based
effective filtering for effective object isolation. This is speeding up the
training process to be about a quarter less than a full training cycle for
semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world,
outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher
novel-view-synthesis quality.



---

## Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface  Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi | cs.DC | [PDF](http://arxiv.org/pdf/2509.05216v1){: .btn .btn-green } |

**Abstract**: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.



---

## SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-04 | Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu | cs.CV | [PDF](http://arxiv.org/pdf/2509.04379v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.



---

## ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast,  Memory-Efficient Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-03 | Sankeerth Durvasula, Sharanshangar Muhunthan, Zain Moustafa, Richard Chen, Ruofan Liang, Yushi Guan, Nilesh Ahuja, Nilesh Jain, Selvakumar Panneer, Nandita Vijaykumar | cs.GR | [PDF](http://arxiv.org/pdf/2509.03775v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world
scenes with high quality and real-time rendering. Typically, a higher quality
representation can be achieved by using a large number of 3D Gaussians.
However, using large 3D Gaussian counts significantly increases the GPU device
memory for storing model parameters. A large model thus requires powerful GPUs
with high memory capacities for training and has slower training/rendering
latencies due to the inefficiencies of memory access and data movement. In this
work, we introduce ContraGS, a method to enable training directly on compressed
3DGS representations without reducing the Gaussian Counts, and thus with a
little loss in model quality. ContraGS leverages codebooks to compactly store a
set of Gaussian parameter vectors throughout the training process, thereby
significantly reducing memory consumption. While codebooks have been
demonstrated to be highly effective at compressing fully trained 3DGS models,
directly training using codebook representations is an unsolved challenge.
ContraGS solves the problem of learning non-differentiable parameters in
codebook-compressed representations by posing parameter estimation as a
Bayesian inference problem. To this end, ContraGS provides a framework that
effectively uses MCMC sampling to sample over a posterior distribution of these
compressed representations. With ContraGS, we demonstrate that ContraGS
significantly reduces the peak memory during training (on average 3.49X) and
accelerated training and rendering (1.36X and 1.88X on average, respectively),
while retraining close to state-of-art quality.



---

## GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned  Residuals

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Mohit Mendiratta, Mayur Deshmukh, Kartik Teotia, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt | cs.GR | [PDF](http://arxiv.org/pdf/2509.02141v1){: .btn .btn-green } |

**Abstract**: 3D Morphable Models (3DMMs) enable controllable facial geometry and
expression editing for reconstruction, animation, and AR/VR, but traditional
PCA-based mesh models are limited in resolution, detail, and photorealism.
Neural volumetric methods improve realism but remain too slow for interactive
use. Recent Gaussian Splatting (3DGS) based facial models achieve fast,
high-quality rendering but still depend solely on a mesh-based 3DMM prior for
expression control, limiting their ability to capture fine-grained geometry,
expressions, and full-head coverage. We introduce GRMM, the first full-head
Gaussian 3D morphable model that augments a base 3DMM with residual geometry
and appearance components, additive refinements that recover high-frequency
details such as wrinkles, fine skin texture, and hairline variations. GRMM
provides disentangled control through low-dimensional, interpretable parameters
(e.g., identity shape, facial expressions) while separately modelling residuals
that capture subject- and expression-specific detail beyond the base model's
capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders
represent per-Gaussian appearance, and a lightweight CNN refines rasterised
images for enhanced realism, all while maintaining 75 FPS real-time rendering.
To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first
dataset with 60 aligned expressions across 50 identities, enabling robust
disentanglement of identity and expression in Gaussian-based 3DMMs. Across
monocular 3D face reconstruction, novel-view synthesis, and expression
transfer, GRMM surpasses state-of-the-art methods in fidelity and expression
accuracy while delivering interactive real-time performance.

Comments:
- Project page: https://mohitm1994.github.io/GRMM/

---

## Efficient Geometry Compression and Communication for 3D Gaussian  Splatting Point Clouds


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Liang Xie, Yanting Li, Luyang Tang, Wei Gao | cs.MM | [PDF](http://arxiv.org/pdf/2509.02232v1){: .btn .btn-green } |

**Abstract**: Storage and transmission challenges in dynamic 3D scene representation based
on the i3DV platform, With increasing scene complexity, the explosive growth of
3D Gaussian data volume causes excessive storage space occupancy. To address
this issue, we propose adopting the AVS PCRM reference software for efficient
compression of Gaussian point cloud geometry data. The strategy deeply
integrates the advanced encoding capabilities of AVS PCRM into the i3DV
platform, forming technical complementarity with the original rate-distortion
optimization mechanism based on binary hash tables. On one hand, the hash table
efficiently caches inter-frame Gaussian point transformation relationships,
which allows for high-fidelity transmission within a 40 Mbps bandwidth
constraint. On the other hand, AVS PCRM performs precise compression on
geometry data. Experimental results demonstrate that the joint framework
maintains the advantages of fast rendering and high-quality synthesis in 3D
Gaussian technology while achieving significant 10\%-25\% bitrate savings on
universal test sets. It provides a superior rate-distortion tradeoff solution
for the storage, transmission, and interaction of 3D volumetric video.

Comments:
- 8 pages,5 figures

---

## 2D Gaussian Splatting with Semantic Alignment for Image Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Hongyu Li, Chaofeng Chen, Xiaoming Li, Guangming Lu | cs.CV | [PDF](http://arxiv.org/pdf/2509.01964v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS), a recent technique for converting discrete points
into continuous spatial representations, has shown promising results in 3D
scene modeling and 2D image super-resolution. In this paper, we explore its
untapped potential for image inpainting, which demands both locally coherent
pixel synthesis and globally consistent semantic restoration. We propose the
first image inpainting framework based on 2D Gaussian Splatting, which encodes
incomplete images into a continuous field of 2D Gaussian splat coefficients and
reconstructs the final image via a differentiable rasterization process. The
continuous rendering paradigm of GS inherently promotes pixel-level coherence
in the inpainted results. To improve efficiency and scalability, we introduce a
patch-wise rasterization strategy that reduces memory overhead and accelerates
inference. For global semantic consistency, we incorporate features from a
pretrained DINO model. We observe that DINO's global features are naturally
robust to small missing regions and can be effectively adapted to guide
semantic alignment in large-mask scenarios, ensuring that the inpainted content
remains contextually consistent with the surrounding scene. Extensive
experiments on standard benchmarks demonstrate that our method achieves
competitive performance in both quantitative metrics and perceptual quality,
establishing a new direction for applying Gaussian Splatting to 2D image
processing.



---

## Im2Haircut: Single-view Strand-based Hair Reconstruction for Human  Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-01 | Vanessa Sklyarova, Egor Zakharov, Malte Prinzler, Giorgio Becherini, Michael J. Black, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2509.01469v1){: .btn .btn-green } |

**Abstract**: We present a novel approach for 3D hair reconstruction from single
photographs based on a global hair prior combined with local optimization.
Capturing strand-based hair geometry from single photographs is challenging due
to the variety and geometric complexity of hairstyles and the lack of ground
truth training data. Classical reconstruction methods like multi-view stereo
only reconstruct the visible hair strands, missing the inner structure of
hairstyles and hampering realistic hair simulation. To address this, existing
methods leverage hairstyle priors trained on synthetic data. Such data,
however, is limited in both quantity and quality since it requires manual work
from skilled artists to model the 3D hairstyles and create near-photorealistic
renderings. To address this, we propose a novel approach that uses both, real
and synthetic data to learn an effective hairstyle prior. Specifically, we
train a transformer-based prior model on synthetic data to obtain knowledge of
the internal hairstyle geometry and introduce real data in the learning process
to model the outer structure. This training scheme is able to model the visible
hair strands depicted in an input image, while preserving the general 3D
structure of hairstyles. We exploit this prior to create a
Gaussian-splatting-based reconstruction method that creates hairstyles from one
or more images. Qualitative and quantitative comparisons with existing
reconstruction pipelines demonstrate the effectiveness and superior performance
of our method for capturing detailed hair orientation, overall silhouette, and
backside consistency. For additional results and code, please refer to
https://im2haircut.is.tue.mpg.de.

Comments:
- For more results please refer to the project page
  https://im2haircut.is.tue.mpg.de

---

## GaussianGAN: Real-Time Photorealistic controllable Human Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-01 | Mohamed Ilyes Lakhal, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2509.01681v1){: .btn .btn-green } |

**Abstract**: Photorealistic and controllable human avatars have gained popularity in the
research community thanks to rapid advances in neural rendering, providing fast
and realistic synthesis tools. However, a limitation of current solutions is
the presence of noticeable blurring. To solve this problem, we propose
GaussianGAN, an animatable avatar approach developed for photorealistic
rendering of people in real-time. We introduce a novel Gaussian splatting
densification strategy to build Gaussian points from the surface of cylindrical
structures around estimated skeletal limbs. Given the camera calibration, we
render an accurate semantic segmentation with our novel view segmentation
module. Finally, a UNet generator uses the rendered Gaussian splatting features
and the segmentation maps to create photorealistic digital avatars. Our method
runs in real-time with a rendering speed of 79 FPS. It outperforms previous
methods regarding visual perception and quality, achieving a state-of-the-art
results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and
33.39db on the Thuman4 dataset.

Comments:
- IEEE conference series on Automatic Face and Gesture Recognition 2025
