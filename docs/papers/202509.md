---
layout: default
title: September 2025
parent: Papers
nav_order: 202509
---

<!---metadata--->


## Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single  Image


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Gaofeng Liu, Hengsen Li, Ruoyu Gao, Xuetong Li, Zhiyuan Ma, Tao Fang | cs.CV | [PDF](http://arxiv.org/pdf/2509.13013v1){: .btn .btn-green } |

**Abstract**: With the rapid advancement of 3D representation techniques and generative
models, substantial progress has been made in reconstructing full-body 3D
avatars from a single image. However, this task remains fundamentally
ill-posedness due to the limited information available from monocular input,
making it difficult to control the geometry and texture of occluded regions
during generation. To address these challenges, we redesign the reconstruction
pipeline and propose Dream3DAvatar, an efficient and text-controllable
two-stage framework for 3D avatar generation. In the first stage, we develop a
lightweight, adapter-enhanced multi-view generation model. Specifically, we
introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information
into SDXL, enforcing geometric and pose consistency across views. To preserve
facial identity, we incorporate ID-Adapter-G, which injects high-resolution
facial features into the generation process. Additionally, we leverage BLIP2 to
generate high-quality textual descriptions of the multi-view images, enhancing
text-driven controllability in occluded regions. In the second stage, we design
a feedforward Transformer model equipped with a multi-view feature fusion
module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS)
from the generated images. Furthermore, we introduce ID-Adapter-R, which
utilizes a gating mechanism to effectively fuse facial features into the
reconstruction process, improving high-frequency detail recovery. Extensive
experiments demonstrate that our method can generate realistic, animation-ready
3D avatars without any post-processing and consistently outperforms existing
baselines across multiple evaluation metrics.



---

## Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian  Splatting and Bag of Embeddings

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Abdalla Arafa, Didier Stricker | cs.CV | [PDF](http://arxiv.org/pdf/2509.12938v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis has seen significant advancements with 3D Gaussian
Splatting (3DGS), enabling real-time photorealistic rendering. However, the
inherent fuzziness of Gaussian Splatting presents challenges for 3D scene
understanding, restricting its broader applications in AR/VR and robotics.
While recent works attempt to learn semantics via 2D foundation model
distillation, they inherit fundamental limitations: alpha blending averages
semantics across objects, making 3D-level understanding impossible. We propose
a paradigm-shifting alternative that bypasses differentiable rendering for
semantics entirely. Our key insight is to leverage predecomposed object-level
Gaussians and represent each object through multiview CLIP feature aggregation,
creating comprehensive "bags of embeddings" that holistically describe objects.
This allows: (1) accurate open-vocabulary object retrieval by comparing text
queries to object-level (not Gaussian-level) embeddings, and (2) seamless task
adaptation: propagating object IDs to pixels for 2D segmentation or to
Gaussians for 3D extraction. Experiments demonstrate that our method
effectively overcomes the challenges of 3D open-vocabulary object extraction
while remaining comparable to state-of-the-art performance in 2D
open-vocabulary segmentation, ensuring minimal compromise.



---

## Exploring Metric Fusion for Evaluation of NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Shreyas Shivakumara, Gabriel Eilertsen, Karljohan Lundin Palmerius | cs.CV | [PDF](http://arxiv.org/pdf/2509.12836v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have demonstrated significant potential in
synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however,
remains a challenge due to the unique artifacts they exhibit, and no individual
metric performs well across all datasets. We hypothesize that combining two
successful metrics, Deep Image Structure and Texture Similarity (DISTS) and
Video Multi-Method Assessment Fusion (VMAF), based on different perceptual
methods, can overcome the limitations of individual metrics and achieve
improved correlation with subjective quality scores. We experiment with two
normalization strategies for the individual metrics and two fusion strategies
to evaluate their impact on the resulting correlation with the subjective
scores. The proposed pipeline is tested on two distinct datasets, Synthetic and
Outdoor, and its performance is evaluated across three different
configurations. We present a detailed analysis comparing the correlation
coefficients of fusion methods and individual scores with subjective scores to
demonstrate the robustness and generalizability of the fusion metrics.

Comments:
- Accepted for 17th International Conference on Quality of Multimedia
  Experience (QoMEX 25)

---

## Effective Gaussian Management for High-fidelity Object Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-16 | Jiateng Liu, Hao Gao, Jiu-Cheng Xie, Chi-Man Pun, Jian Xiong, Haolun Li, Feng Xu | cs.CV | [PDF](http://arxiv.org/pdf/2509.12742v1){: .btn .btn-green } |

**Abstract**: This paper proposes an effective Gaussian management approach for
high-fidelity object reconstruction. Departing from recent Gaussian Splatting
(GS) methods that employ indiscriminate attribute assignment, our approach
introduces a novel densification strategy that dynamically activates spherical
harmonics (SHs) or normals under the supervision of a surface reconstruction
module, which effectively mitigates the gradient conflicts caused by dual
supervision and achieves superior reconstruction results. To further improve
representation efficiency, we develop a lightweight Gaussian representation
that adaptively adjusts the SH orders of each Gaussian based on gradient
magnitudes and performs task-decoupled pruning to remove Gaussian with minimal
impact on a reconstruction task without sacrificing others, which balances the
representational capacity with parameter quantity. Notably, our management
approach is model-agnostic and can be seamlessly integrated into other
frameworks, enhancing performance while reducing model size. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art approaches in both reconstruction quality and efficiency,
achieving superior performance with significantly fewer parameters.



---

## Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-15 | Yi-Hsin Li, Thomas Sikora, Sebastian Knorr, Måarten Sjöström | cs.CV | [PDF](http://arxiv.org/pdf/2509.11853v1){: .btn .btn-green } |

**Abstract**: Sparse-view synthesis remains a challenging problem due to the difficulty of
recovering accurate geometry and appearance from limited observations. While
recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time
rendering with competitive quality, existing pipelines often rely on
Structure-from-Motion (SfM) for camera pose estimation, an approach that
struggles in genuinely sparse-view settings. Moreover, several SfM-free methods
replace SfM with multi-view stereo (MVS) models, but generate massive numbers
of 3D Gaussians by back-projecting every pixel into 3D space, leading to high
memory costs. We propose Segmentation-Driven Initialization for Gaussian
Splatting (SDI-GS), a method that mitigates inefficiency by leveraging
region-based segmentation to identify and retain only structurally significant
regions. This enables selective downsampling of the dense point cloud,
preserving scene fidelity while substantially reducing Gaussian count.
Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count
by up to 50% and achieves comparable or superior rendering quality in PSNR and
SSIM, with only marginal degradation in LPIPS. It further enables faster
training and lower memory footprint, advancing the practicality of 3DGS for
constrained-view scenarios.



---

## A Controllable 3D Deepfake Generation Framework with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-15 | Wending Liu, Siyun Liang, Huy H. Nguyen, Isao Echizen | cs.CV | [PDF](http://arxiv.org/pdf/2509.11624v1){: .btn .btn-green } |

**Abstract**: We propose a novel 3D deepfake generation framework based on 3D Gaussian
Splatting that enables realistic, identity-preserving face swapping and
reenactment in a fully controllable 3D space. Compared to conventional 2D
deepfake approaches that suffer from geometric inconsistencies and limited
generalization to novel view, our method combines a parametric head model with
dynamic Gaussian representations to support multi-view consistent rendering,
precise expression control, and seamless background integration. To address
editing challenges in point-based representations, we explicitly separate the
head and background Gaussians and use pre-trained 2D guidance to optimize the
facial region across views. We further introduce a repair module to enhance
visual consistency under extreme poses and expressions. Experiments on
NeRSemble and additional evaluation videos demonstrate that our method achieves
comparable performance to state-of-the-art 2D approaches in identity
preservation, as well as pose and expression consistency, while significantly
outperforming them in multi-view rendering quality and 3D consistency. Our
approach bridges the gap between 3D modeling and deepfake synthesis, enabling
new directions for scene-aware, controllable, and immersive visual forgeries,
revealing the threat that emerging 3D Gaussian Splatting technique could be
used for manipulation attacks.



---

## Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial  Vehicles

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-15 | Àlmos Veres-Vitàlyos, Genis Castillo Gomez-Raya, Filip Lemic, Daniel Johannes Bugelnig, Bernhard Rinner, Sergi Abadal, Xavier Costa-Pérez | cs.RO | [PDF](http://arxiv.org/pdf/2509.12458v1){: .btn .btn-green } |

**Abstract**: Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for
navigating indoor and hard-to-reach areas, yet their significant constraints in
payload and autonomy have largely prevented their use for complex tasks like
high-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we
introduce a novel system architecture that enables fully autonomous,
high-fidelity 3D scanning of static objects using UAVs weighing under 100
grams. Our core innovation lies in a dual-reconstruction pipeline that creates
a real-time feedback loop between data capture and flight control. A
near-real-time (near-RT) process uses Structure from Motion (SfM) to generate
an instantaneous pointcloud of the object. The system analyzes the model
quality on the fly and dynamically adapts the UAV's trajectory to intelligently
capture new images of poorly covered areas. This ensures comprehensive data
acquisition. For the final, detailed output, a non-real-time (non-RT) pipeline
employs a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR)
approach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB)
location data to achieve superior accuracy. We implemented and validated this
architecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both
single- and multi-UAV configurations, conclusively show that dynamic trajectory
adaptation consistently improves reconstruction quality over static flight
paths. This work demonstrates a scalable and autonomous solution that unlocks
the potential of miniaturized UAVs for fine-grained 3D reconstruction in
constrained environments, a capability previously limited to much larger
platforms.

Comments:
- 13 pages, 16 figures, 3 tables, 45 references

---

## Distributed 3D Gaussian Splatting for High-Resolution Isosurface  Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-15 | Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi | cs.DC | [PDF](http://arxiv.org/pdf/2509.12138v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique
for real-time, photorealistic rendering by optimizing anisotropic Gaussian
primitives from view-dependent images. While 3D-GS has been extended to
scientific visualization, prior work remains limited to single-GPU settings,
restricting scalability for large datasets on high-performance computing (HPC)
systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach
partitions data across nodes, trains Gaussian splats in parallel using
multi-nodes and multi-GPUs, and merges splats for global rendering. To
eliminate artifacts, we add ghost cells at partition boundaries and apply
background masks to remove irrelevant pixels. Benchmarks on the
Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup
across 8 nodes on Polaris while preserving image quality. These results
demonstrate that distributed 3D-GS enables scalable visualization of
large-scale scientific data and provide a foundation for future in situ
applications.



---

## On the Skinning of Gaussian Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Nikolaos Zioulis, Nikolaos Kotarelas, Georgios Albanis, Spyridon Thermos, Anargyros Chatzitofis | cs.CV | [PDF](http://arxiv.org/pdf/2509.11411v1){: .btn .btn-green } |

**Abstract**: Radiance field-based methods have recently been used to reconstruct human
avatars, showing that we can significantly downscale the systems needed for
creating animated human avatars. Although this progress has been initiated by
neural radiance fields, their slow rendering and backward mapping from the
observation space to the canonical space have been the main challenges. With
Gaussian splatting overcoming both challenges, a new family of approaches has
emerged that are faster to train and render, while also straightforward to
implement using forward skinning from the canonical to the observation space.
However, the linear blend skinning required for the deformation of the
Gaussians does not provide valid results for their non-linear rotation
properties. To address such artifacts, recent works use mesh properties to
rotate the non-linear Gaussian properties or train models to predict corrective
offsets. Instead, we propose a weighted rotation blending approach that
leverages quaternion averaging. This leads to simpler vertex-based Gaussians
that can be efficiently animated and integrated in any engine by only modifying
the linear blend skinning technique, and using any Gaussian rasterizer.



---

## SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene  Completion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Zhiwen Yang, Yuxin Peng | cs.CV | [PDF](http://arxiv.org/pdf/2509.11171v1){: .btn .btn-green } |

**Abstract**: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

Comments:
- 10 pages, 6 figures

---

## Multispectral-NeRF:a multispectral modeling approach based on neural  radiance fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Hong Zhang, Fei Guo, Zihan Xie, Dizhao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2509.11169v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.



---

## SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Ashkan Taghipour, Vahid Naghshin, Benjamin Southwell, Farid Boussaid, Hamid Laga, Mohammed Bennamoun | cs.CV | [PDF](http://arxiv.org/pdf/2509.11116v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.



---

## ROSGS: Relightable Outdoor Scenes With Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-14 | Lianjun Liao, Chunhui Zhang, Tong Wu, Henglei Lv, Bailin Deng, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2509.11275v1){: .btn .btn-green } |

**Abstract**: Image data captured outdoors often exhibit unbounded scenes and
unconstrained, varying lighting conditions, making it challenging to decompose
them into geometry, reflectance, and illumination. Recent works have focused on
achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D
Gaussian Splatting (3DGS) representation but remain hindered by two key
limitations: the high computational overhead associated with neural networks of
NeRF and the use of low-frequency lighting representations, which often result
in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS,
a two-stage pipeline designed to efficiently reconstruct relightable outdoor
scenes using the Gaussian Splatting representation. By leveraging monocular
normal priors, ROSGS first reconstructs the scene's geometry with the compact
2D Gaussian Splatting (2DGS) representation, providing an efficient and
accurate geometric foundation. Building upon this reconstructed geometry, ROSGS
then decomposes the scene's texture and lighting through a hybrid lighting
model. This model effectively represents typical outdoor lighting by employing
a spherical Gaussian function to capture the directional, high-frequency
components of sunlight, while learning a radiance transfer function via
Spherical Harmonic coefficients to model the remaining low-frequency skylight
comprehensively. Both quantitative metrics and qualitative comparisons
demonstrate that ROSGS achieves state-of-the-art performance in relighting
outdoor scenes and highlight its ability to deliver superior relighting
accuracy and rendering efficiency.



---

## AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-13 | Gurutva Patle, Nilay Girgaonkar, Nagabhushan Somraj, Rajiv Soundararajan | cs.GR | [PDF](http://arxiv.org/pdf/2509.11003v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel
view synthesis. However, it often struggles under sparse-view settings,
producing undesirable artifacts such as floaters, inaccurate geometry, and
overfitting due to limited observations. We find that a key contributing factor
is uncontrolled densification, where adding Gaussian primitives rapidly without
guidance can harm geometry and cause artifacts. We propose AD-GS, a novel
alternating densification framework that interleaves high and low densification
phases. During high densification, the model densifies aggressively, followed
by photometric loss based training to capture fine-grained scene details. Low
densification then primarily involves aggressive opacity pruning of Gaussians
followed by regularizing their geometry through pseudo-view consistency and
edge-aware depth smoothness. This alternating approach helps reduce overfitting
by carefully controlling model capacity growth while progressively refining the
scene representation. Extensive experiments on challenging datasets demonstrate
that AD-GS significantly improves rendering quality and geometric consistency
compared to existing methods.

Comments:
- SIGGRAPH Asia 2025

---

## Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing  for Physics-based Camera Effect Data Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-13 | Yi-Ruei Liu, You-Zhe Xie, Yu-Hsiang Hsu, I-Sheng Fang, Yu-Lun Liu, Jun-Cheng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2509.10759v1){: .btn .btn-green } |

**Abstract**: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.



---

## T2Bs: Text-to-Character Blendshapes via Video Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-12 | Jiahao Luo, Chaoyang Wang, Michael Vasilkovsky, Vladislav Shakhrai, Di Liu, Peiye Zhuang, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee, James Davis, Jian Wang | cs.GR | [PDF](http://arxiv.org/pdf/2509.10678v1){: .btn .btn-green } |

**Abstract**: We present T2Bs, a framework for generating high-quality, animatable
character head morphable models from text by combining static text-to-3D
generation with video diffusion. Text-to-3D models produce detailed static
geometry but lack motion synthesis, while video diffusion models generate
motion with temporal and multi-view geometric inconsistencies. T2Bs bridges
this gap by leveraging deformable 3D Gaussian splatting to align static 3D
assets with video outputs. By constraining motion with static geometry and
employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D
generation methods in accuracy and expressiveness while reducing video
artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent,
fully registered 3D geometries designed to scale for building morphable models
with diverse, realistic facial motions. This enables synthesizing expressive,
animatable character heads that surpass current 4D generation techniques.



---

## On the Geometric Accuracy of Implicit and Primitive-based  Representations Derived from View Rendering Constraints

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-12 | Elias De Smijter, Renaud Detry, Christophe De Vleeschouwer | cs.CV | [PDF](http://arxiv.org/pdf/2509.10241v2){: .btn .btn-green } |

**Abstract**: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

Comments:
- 9 pages, 3 figures, to be presented at ASTRA25,

---

## DiGS: Accurate and Complete Surface Reconstruction from 3D Gaussians via  Direct SDF Learning

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Wenzhi Guo, Bing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.07493v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful paradigm for
photorealistic view synthesis, representing scenes with spatially distributed
Gaussian primitives. While highly effective for rendering, achieving accurate
and complete surface reconstruction remains challenging due to the unstructured
nature of the representation and the absence of explicit geometric supervision.
In this work, we propose DiGS, a unified framework that embeds Signed Distance
Field (SDF) learning directly into the 3DGS pipeline, thereby enforcing strong
and interpretable surface priors. By associating each Gaussian with a learnable
SDF value, DiGS explicitly aligns primitives with underlying geometry and
improves cross-view consistency. To further ensure dense and coherent coverage,
we design a geometry-guided grid growth strategy that adaptively distributes
Gaussians along geometry-consistent regions under a multi-scale hierarchy.
Extensive experiments on standard benchmarks, including DTU, Mip-NeRF 360, and
Tanks& Temples, demonstrate that DiGS consistently improves reconstruction
accuracy and completeness while retaining high rendering fidelity.



---

## DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Ze-Xin Yin, Jiaxiong Qiu, Liu Liu, Xinjie Wang, Wei Sui, Zhizhong Su, Jian Yang, Jin Xie | cs.CV | [PDF](http://arxiv.org/pdf/2509.07435v1){: .btn .btn-green } |

**Abstract**: The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.

Comments:
- 14 pages, 7 figures, project page:
  https://zx-yin.github.io/dreamlifting/

---

## SplatFill: 3D Scene Inpainting via Depth-Guided Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Mahtab Dahaghin, Milind G. Padalkar, Matteo Toso, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2509.07809v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has enabled the creation of highly realistic 3D
scene representations from sets of multi-view images. However, inpainting
missing regions, whether due to occlusion or scene editing, remains a
challenging task, often leading to blurry details, artifacts, and inconsistent
geometry. In this work, we introduce SplatFill, a novel depth-guided approach
for 3DGS scene inpainting that achieves state-of-the-art perceptual quality and
improved efficiency. Our method combines two key ideas: (1) joint depth-based
and object-based supervision to ensure inpainted Gaussians are accurately
placed in 3D space and aligned with surrounding geometry, and (2) we propose a
consistency-aware refinement scheme that selectively identifies and corrects
inconsistent regions without disrupting the rest of the scene. Evaluations on
the SPIn-NeRF dataset demonstrate that SplatFill not only surpasses existing
NeRF-based and 3DGS-based inpainting methods in visual fidelity but also
reduces training time by 24.5%. Qualitative results show our method delivers
sharper details, fewer artifacts, and greater coherence across challenging
viewpoints.



---

## HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-09 | Yimin Pan, Matthias Nießner, Tobias Kirschstein | cs.CV | [PDF](http://arxiv.org/pdf/2509.07774v1){: .btn .btn-green } |

**Abstract**: Human hair reconstruction is a challenging problem in computer vision, with
growing importance for applications in virtual reality and digital human
modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient
and explicit scene representations that naturally align with the structure of
hair strands. In this work, we extend the 3DGS framework to enable strand-level
hair geometry reconstruction from multi-view images. Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.
  While existing methods typically evaluate reconstruction quality at the
geometric level, they often neglect the connectivity and topology of hair
strands. To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction. Extensive
experiments on both synthetic and real-world datasets demonstrate that our
method robustly handles a wide range of hairstyles and achieves efficient
reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/

Comments:
- This is the arXiv preprint of the paper "Hair Strand Reconstruction
  based on 3D Gaussian Splatting" published at BMVC 2025. Project website:
  https://yimin-pan.github.io/hair-gs/

---

## Real-time Photorealistic Mapping for Situational Awareness in Robot  Teleoperation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Ian Page, Pierre Susbielle, Olivier Aycard, Pierre-Brice Wieber | cs.RO | [PDF](http://arxiv.org/pdf/2509.06433v2){: .btn .btn-green } |

**Abstract**: Achieving efficient remote teleoperation is particularly challenging in
unknown environments, as the teleoperator must rapidly build an understanding
of the site's layout. Online 3D mapping is a proven strategy to tackle this
challenge, as it enables the teleoperator to progressively explore the site
from multiple perspectives. However, traditional online map-based teleoperation
systems struggle to generate visually accurate 3D maps in real-time due to the
high computational cost involved, leading to poor teleoperation performances.
In this work, we propose a solution to improve teleoperation efficiency in
unknown environments. Our approach proposes a novel, modular and efficient
GPU-based integration between recent advancement in gaussian splatting SLAM and
existing online map-based teleoperation systems. We compare the proposed
solution against state-of-the-art teleoperation systems and validate its
performances through real-world experiments using an aerial vehicle. The
results show significant improvements in decision-making speed and more
accurate interaction with the environment, leading to greater teleoperation
efficiency. In doing so, our system enhances remote teleoperation by seamlessly
integrating photorealistic mapping generation with real-time performances,
enabling effective teleoperation in unfamiliar environments.



---

## VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level  Guidance in Large Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2509.06685v3){: .btn .btn-green } |

**Abstract**: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for
novel-view synthesis (NVS) in large scenes. GS typically requires accurate
depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited
depth sensing range makes it difficult for GS to work in large scenes.
Monocular images, however, lack depth to guide the learning and lead to
inferior NVS results. Although large foundation models (LFMs) for monocular
depth estimation are available, they suffer from cross-frame inconsistency,
inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This
paper aims to generate dense, accurate depth images from monocular RGB inputs
for high-definite GS rendering. The key idea is to leverage the accurate but
sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the
dense but coarse depth from LFMs. To bridge the sparse input and dense output,
we propose an object-segmented depth propagation algorithm that renders the
depth of pixels of structured objects. Then we develop a dynamic depth
refinement module to handle the crippled SfM depth of dynamic objects and
refine the coarse LFM depth. Experiments using public and customized datasets
demonstrate the superior rendering quality of VIM-GS in large scenes.

Comments:
- Withdrawn due to an error in the author list & incomplete
  experimental results

---

## 3DOF+Quantization: 3DGS quantization for large scenes with limited  Degrees of Freedom

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-08 | Matthieu Gendrin, Stéphane Pateux, Théo Ladune | cs.CV | [PDF](http://arxiv.org/pdf/2509.06400v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene
reconstruction. With a number of views of a given object or scene, the
algorithm trains a model composed of 3D gaussians, which enables the production
of novel views from arbitrary points of view. This freedom of movement is
referred to as 6DoF for 6 degrees of freedom: a view is produced for any
position (3 degrees), orientation of camera (3 other degrees). On large scenes,
though, the input views are acquired from a limited zone in space, and the
reconstruction is valuable for novel views from the same zone, even if the
scene itself is almost unlimited in size. We refer to this particular case as
3DoF+, meaning that the 3 degrees of freedom of camera position are limited to
small offsets around the central position. Considering the problem of
coordinate quantization, the impact of position error on the projection error
in pixels is studied. It is shown that the projection error is proportional to
the squared inverse distance of the point being projected. Consequently, a new
quantization scheme based on spherical coordinates is proposed. Rate-distortion
performance of the proposed method are illustrated on the well-known Garden
scene.



---

## MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians  and Unified Pruning

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-07 | Jiarui Chen, Yikeng Chen, Yingshuang Zou, Ye Huang, Peng Wang, Yuan Liu, Yujing Sun, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2509.07021v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis
technique, but its high memory consumption severely limits its applicability on
edge devices. A growing number of 3DGS compression methods have been proposed
to make 3DGS more efficient, yet most only focus on storage compression and
fail to address the critical bottleneck of rendering memory. To address this
problem, we introduce MEGS$^{2}$, a novel memory-efficient framework that
tackles this challenge by jointly optimizing two key factors: the total
primitive number and the parameters per primitive, achieving unprecedented
memory compression. Specifically, we replace the memory-intensive spherical
harmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our
color representations. More importantly, we propose a unified soft pruning
framework that models primitive-number and lobe-number pruning as a single
constrained optimization problem. Experiments show that MEGS$^{2}$ achieves a
50% static VRAM reduction and a 40% rendering VRAM reduction compared to
existing methods, while maintaining comparable rendering quality.

Comments:
- 14 pages, 4 figures

---

## GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Yangming Li, Chaoyu Liu, Lihao Liu, Simon Masnou, Carola-Bibian Schönlieb | cs.CV | [PDF](http://arxiv.org/pdf/2509.05075v1){: .btn .btn-green } |

**Abstract**: A few recent works explored incorporating geometric priors to regularize the
optimization of Gaussian splatting, further improving its performance. However,
those early studies mainly focused on the use of low-order geometric priors
(e.g., normal vector), and they are also unreliably estimated by
noise-sensitive methods, like local principal component analysis. To address
their limitations, we first present GeoSplat, a general geometry-constrained
optimization framework that exploits both first-order and second-order
geometric quantities to improve the entire training pipeline of Gaussian
splatting, including Gaussian initialization, gradient update, and
densification. As an example, we initialize the scales of 3D Gaussian
primitives in terms of principal curvatures, leading to a better coverage of
the object surface than random initialization. Secondly, based on certain
geometric structures (e.g., local manifold), we introduce efficient and
noise-robust estimation methods that provide dynamic geometric priors for our
framework. We conduct extensive experiments on multiple datasets for novel view
synthesis, showing that our framework: GeoSplat, significantly improves the
performance of Gaussian splatting and outperforms previous baselines.



---

## Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface  Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Mengjiao Han, Andres Sewell, Joseph Insley, Janet Knowles, Victor A. Mateevitsi, Michael E. Papka, Steve Petruzza, Silvio Rizzi | cs.DC | [PDF](http://arxiv.org/pdf/2509.05216v1){: .btn .btn-green } |

**Abstract**: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.



---

## CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Hannah Schieber, Dominik Frischmann, Victor Schaack, Simon Boche, Angela Schoellig, Stefan Leutenegger, Daniel Roth | cs.CV | [PDF](http://arxiv.org/pdf/2509.04859v2){: .btn .btn-green } |

**Abstract**: Mobile reconstruction has the potential to support time-critical tasks such
as tele-guidance and disaster response, where operators must quickly gain an
accurate understanding of the environment. Full high-fidelity scene
reconstruction is computationally expensive and often unnecessary when only
specific points of interest (POIs) matter for timely decision making. We
address this challenge with CoRe-GS, a semantic POI-focused extension of
Gaussian Splatting (GS). Instead of optimizing every scene element uniformly,
CoRe-GS first produces a fast segmentation-ready GS representation and then
selectively refines splats belonging to semantically relevant POIs detected
during data acquisition. This targeted refinement reduces training time to 25\%
compared to full semantic GS while improving novel view synthesis quality in
the areas that matter most. We validate CoRe-GS on both real-world (SCRREAM)
and synthetic (NeRDS 360) datasets, demonstrating that prioritizing POIs
enables faster and higher-quality mobile reconstruction tailored to operational
needs.



---

## Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation  in 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-05 | Sen Wang, Kunyi Li, Siyun Liang, Elena Alegret, Jing Ma, Nassir Navab, Stefano Gasperini | cs.CV | [PDF](http://arxiv.org/pdf/2509.05515v1){: .btn .btn-green } |

**Abstract**: Recently, distilling open-vocabulary language features from 2D images into 3D
Gaussians has attracted significant attention. Although existing methods
achieve impressive language-based interactions of 3D scenes, we observe two
fundamental issues: background Gaussians contributing negligibly to a rendered
pixel get the same feature as the dominant foreground ones, and multi-view
inconsistencies due to view-specific noise in language embeddings. We introduce
Visibility-Aware Language Aggregation (VALA), a lightweight yet effective
method that computes marginal contributions for each ray and applies a
visibility-aware gate to retain only visible Gaussians. Moreover, we propose a
streaming weighted geometric median in cosine space to merge noisy multi-view
features. Our method yields a robust, view-consistent language feature
embedding in a fast and memory-efficient manner. VALA improves open-vocabulary
localization and segmentation across reference datasets, consistently
surpassing existing works.



---

## SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-04 | Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu | cs.CV | [PDF](http://arxiv.org/pdf/2509.04379v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.



---

## ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast,  Memory-Efficient Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-03 | Sankeerth Durvasula, Sharanshangar Muhunthan, Zain Moustafa, Richard Chen, Ruofan Liang, Yushi Guan, Nilesh Ahuja, Nilesh Jain, Selvakumar Panneer, Nandita Vijaykumar | cs.GR | [PDF](http://arxiv.org/pdf/2509.03775v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world
scenes with high quality and real-time rendering. Typically, a higher quality
representation can be achieved by using a large number of 3D Gaussians.
However, using large 3D Gaussian counts significantly increases the GPU device
memory for storing model parameters. A large model thus requires powerful GPUs
with high memory capacities for training and has slower training/rendering
latencies due to the inefficiencies of memory access and data movement. In this
work, we introduce ContraGS, a method to enable training directly on compressed
3DGS representations without reducing the Gaussian Counts, and thus with a
little loss in model quality. ContraGS leverages codebooks to compactly store a
set of Gaussian parameter vectors throughout the training process, thereby
significantly reducing memory consumption. While codebooks have been
demonstrated to be highly effective at compressing fully trained 3DGS models,
directly training using codebook representations is an unsolved challenge.
ContraGS solves the problem of learning non-differentiable parameters in
codebook-compressed representations by posing parameter estimation as a
Bayesian inference problem. To this end, ContraGS provides a framework that
effectively uses MCMC sampling to sample over a posterior distribution of these
compressed representations. With ContraGS, we demonstrate that ContraGS
significantly reduces the peak memory during training (on average 3.49X) and
accelerated training and rendering (1.36X and 1.88X on average, respectively),
while retraining close to state-of-art quality.



---

## Efficient Geometry Compression and Communication for 3D Gaussian  Splatting Point Clouds


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Liang Xie, Yanting Li, Luyang Tang, Wei Gao | cs.MM | [PDF](http://arxiv.org/pdf/2509.02232v1){: .btn .btn-green } |

**Abstract**: Storage and transmission challenges in dynamic 3D scene representation based
on the i3DV platform, With increasing scene complexity, the explosive growth of
3D Gaussian data volume causes excessive storage space occupancy. To address
this issue, we propose adopting the AVS PCRM reference software for efficient
compression of Gaussian point cloud geometry data. The strategy deeply
integrates the advanced encoding capabilities of AVS PCRM into the i3DV
platform, forming technical complementarity with the original rate-distortion
optimization mechanism based on binary hash tables. On one hand, the hash table
efficiently caches inter-frame Gaussian point transformation relationships,
which allows for high-fidelity transmission within a 40 Mbps bandwidth
constraint. On the other hand, AVS PCRM performs precise compression on
geometry data. Experimental results demonstrate that the joint framework
maintains the advantages of fast rendering and high-quality synthesis in 3D
Gaussian technology while achieving significant 10\%-25\% bitrate savings on
universal test sets. It provides a superior rate-distortion tradeoff solution
for the storage, transmission, and interaction of 3D volumetric video.

Comments:
- 8 pages,5 figures

---

## 2D Gaussian Splatting with Semantic Alignment for Image Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Hongyu Li, Chaofeng Chen, Xiaoming Li, Guangming Lu | cs.CV | [PDF](http://arxiv.org/pdf/2509.01964v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS), a recent technique for converting discrete points
into continuous spatial representations, has shown promising results in 3D
scene modeling and 2D image super-resolution. In this paper, we explore its
untapped potential for image inpainting, which demands both locally coherent
pixel synthesis and globally consistent semantic restoration. We propose the
first image inpainting framework based on 2D Gaussian Splatting, which encodes
incomplete images into a continuous field of 2D Gaussian splat coefficients and
reconstructs the final image via a differentiable rasterization process. The
continuous rendering paradigm of GS inherently promotes pixel-level coherence
in the inpainted results. To improve efficiency and scalability, we introduce a
patch-wise rasterization strategy that reduces memory overhead and accelerates
inference. For global semantic consistency, we incorporate features from a
pretrained DINO model. We observe that DINO's global features are naturally
robust to small missing regions and can be effectively adapted to guide
semantic alignment in large-mask scenarios, ensuring that the inpainted content
remains contextually consistent with the surrounding scene. Extensive
experiments on standard benchmarks demonstrate that our method achieves
competitive performance in both quantitative metrics and perceptual quality,
establishing a new direction for applying Gaussian Splatting to 2D image
processing.



---

## GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned  Residuals

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-02 | Mohit Mendiratta, Mayur Deshmukh, Kartik Teotia, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt | cs.GR | [PDF](http://arxiv.org/pdf/2509.02141v1){: .btn .btn-green } |

**Abstract**: 3D Morphable Models (3DMMs) enable controllable facial geometry and
expression editing for reconstruction, animation, and AR/VR, but traditional
PCA-based mesh models are limited in resolution, detail, and photorealism.
Neural volumetric methods improve realism but remain too slow for interactive
use. Recent Gaussian Splatting (3DGS) based facial models achieve fast,
high-quality rendering but still depend solely on a mesh-based 3DMM prior for
expression control, limiting their ability to capture fine-grained geometry,
expressions, and full-head coverage. We introduce GRMM, the first full-head
Gaussian 3D morphable model that augments a base 3DMM with residual geometry
and appearance components, additive refinements that recover high-frequency
details such as wrinkles, fine skin texture, and hairline variations. GRMM
provides disentangled control through low-dimensional, interpretable parameters
(e.g., identity shape, facial expressions) while separately modelling residuals
that capture subject- and expression-specific detail beyond the base model's
capacity. Coarse decoders produce vertex-level mesh deformations, fine decoders
represent per-Gaussian appearance, and a lightweight CNN refines rasterised
images for enhanced realism, all while maintaining 75 FPS real-time rendering.
To learn consistent, high-fidelity residuals, we present EXPRESS-50, the first
dataset with 60 aligned expressions across 50 identities, enabling robust
disentanglement of identity and expression in Gaussian-based 3DMMs. Across
monocular 3D face reconstruction, novel-view synthesis, and expression
transfer, GRMM surpasses state-of-the-art methods in fidelity and expression
accuracy while delivering interactive real-time performance.

Comments:
- Project page: https://mohitm1994.github.io/GRMM/

---

## Im2Haircut: Single-view Strand-based Hair Reconstruction for Human  Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-01 | Vanessa Sklyarova, Egor Zakharov, Malte Prinzler, Giorgio Becherini, Michael J. Black, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2509.01469v1){: .btn .btn-green } |

**Abstract**: We present a novel approach for 3D hair reconstruction from single
photographs based on a global hair prior combined with local optimization.
Capturing strand-based hair geometry from single photographs is challenging due
to the variety and geometric complexity of hairstyles and the lack of ground
truth training data. Classical reconstruction methods like multi-view stereo
only reconstruct the visible hair strands, missing the inner structure of
hairstyles and hampering realistic hair simulation. To address this, existing
methods leverage hairstyle priors trained on synthetic data. Such data,
however, is limited in both quantity and quality since it requires manual work
from skilled artists to model the 3D hairstyles and create near-photorealistic
renderings. To address this, we propose a novel approach that uses both, real
and synthetic data to learn an effective hairstyle prior. Specifically, we
train a transformer-based prior model on synthetic data to obtain knowledge of
the internal hairstyle geometry and introduce real data in the learning process
to model the outer structure. This training scheme is able to model the visible
hair strands depicted in an input image, while preserving the general 3D
structure of hairstyles. We exploit this prior to create a
Gaussian-splatting-based reconstruction method that creates hairstyles from one
or more images. Qualitative and quantitative comparisons with existing
reconstruction pipelines demonstrate the effectiveness and superior performance
of our method for capturing detailed hair orientation, overall silhouette, and
backside consistency. For additional results and code, please refer to
https://im2haircut.is.tue.mpg.de.

Comments:
- For more results please refer to the project page
  https://im2haircut.is.tue.mpg.de

---

## GaussianGAN: Real-Time Photorealistic controllable Human Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-09-01 | Mohamed Ilyes Lakhal, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2509.01681v1){: .btn .btn-green } |

**Abstract**: Photorealistic and controllable human avatars have gained popularity in the
research community thanks to rapid advances in neural rendering, providing fast
and realistic synthesis tools. However, a limitation of current solutions is
the presence of noticeable blurring. To solve this problem, we propose
GaussianGAN, an animatable avatar approach developed for photorealistic
rendering of people in real-time. We introduce a novel Gaussian splatting
densification strategy to build Gaussian points from the surface of cylindrical
structures around estimated skeletal limbs. Given the camera calibration, we
render an accurate semantic segmentation with our novel view segmentation
module. Finally, a UNet generator uses the rendered Gaussian splatting features
and the segmentation maps to create photorealistic digital avatars. Our method
runs in real-time with a rendering speed of 79 FPS. It outperforms previous
methods regarding visual perception and quality, achieving a state-of-the-art
results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and
33.39db on the Thuman4 dataset.

Comments:
- IEEE conference series on Automatic Face and Gesture Recognition 2025
