---
layout: default
title: January 2025
parent: Papers
nav_order: 202501
---

<!---metadata--->


## UV-Attack: Physical-World Adversarial Attacks for Person Detection via  Dynamic-NeRF-based UV Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-10 | Yanjie Li, Wenxuan Zhang, Kaisheng Liang, Bin Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2501.05783v1){: .btn .btn-green } |

**Abstract**: In recent research, adversarial attacks on person detectors using patches or
static 3D model-based texture modifications have struggled with low success
rates due to the flexible nature of human movement. Modeling the 3D
deformations caused by various actions has been a major challenge. Fortunately,
advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer
new possibilities. In this paper, we introduce UV-Attack, a groundbreaking
approach that achieves high success rates even with extensive and unseen human
actions. We address the challenge above by leveraging dynamic-NeRF-based UV
mapping. UV-Attack can generate human images across diverse actions and
viewpoints, and even create novel actions by sampling from the SMPL parameter
space. While dynamic NeRF models are capable of modeling human bodies,
modifying clothing textures is challenging because they are embedded in neural
network parameters. To tackle this, UV-Attack generates UV maps instead of RGB
images and modifies the texture stacks. This approach enables real-time texture
edits and makes the attack more practical. We also propose a novel Expectation
over Pose Transformation loss (EoPT) to improve the evasion success rate on
unseen poses and views. Our experiments show that UV-Attack achieves a 92.75%
attack success rate against the FastRCNN model across varied poses in dynamic
video settings, significantly outperforming the state-of-the-art AdvCamou
attack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the
latest YOLOv8 detector in black-box settings. This work highlights the
potential of dynamic NeRF-based UV mapping for creating more effective
adversarial attacks on person detectors, addressing key challenges in modeling
human movement and texture modification.

Comments:
- 23 pages, 22 figures, submitted to ICLR2025

---

## Locality-aware Gaussian Compression for Fast and High-quality Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-10 | Seungjoo Shin, Jaesik Park, Sunghyun Cho | cs.CV | [PDF](http://arxiv.org/pdf/2501.05757v1){: .btn .btn-green } |

**Abstract**: We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework
that exploits the spatial coherence of 3D Gaussians for compact modeling of
volumetric scenes. To this end, we first analyze the local coherence of 3D
Gaussian attributes, and propose a novel locality-aware 3D Gaussian
representation that effectively encodes locally-coherent Gaussian attributes
using a neural field representation with a minimal storage requirement. On top
of the novel representation, LocoGS is carefully designed with additional
components such as dense initialization, an adaptive spherical harmonics
bandwidth scheme and different encoding schemes for different Gaussian
attributes to maximize compression performance. Experimental results
demonstrate that our approach outperforms the rendering quality of existing
compact Gaussian representations for representative real-world 3D datasets
while achieving from 54.6$\times$ to 96.6$\times$ compressed storage size and
from 2.1$\times$ to 2.4$\times$ rendering speed than 3DGS. Even our approach
also demonstrates an averaged 2.4$\times$ higher rendering speed than the
state-of-the-art compression method with comparable compression performance.

Comments:
- 28 pages, 15 figures, and 14 tables

---

## Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D  Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, Lingjie Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.05427v1){: .btn .btn-green } |

**Abstract**: Recent advances in 2D image generation have achieved remarkable
quality,largely driven by the capacity of diffusion models and the availability
of large-scale datasets. However, direct 3D generation is still constrained by
the scarcity and lower fidelity of 3D datasets. In this paper, we introduce
Zero-1-to-G, a novel approach that addresses this problem by enabling direct
single-view generation on Gaussian splats using pretrained 2D diffusion models.
Our key insight is that Gaussian splats, a 3D representation, can be decomposed
into multi-view images encoding different attributes. This reframes the
challenging task of direct 3D generation within a 2D diffusion framework,
allowing us to leverage the rich priors of pretrained 2D diffusion models. To
incorporate 3D awareness, we introduce cross-view and cross-attribute attention
layers, which capture complex correlations and enforce 3D consistency across
generated splats. This makes Zero-1-to-G the first direct image-to-3D
generative model to effectively utilize pretrained 2D diffusion priors,
enabling efficient training and improved generalization to unseen objects.
Extensive experiments on both synthetic and in-the-wild datasets demonstrate
superior performance in 3D object generation, offering a new approach to
high-quality 3D generation.



---

## Light Transport-aware Diffusion Posterior Sampling for Single-View  Reconstruction of 3D Volumes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Ludwic Leonard, Nils Thuerey, Ruediger Westermann | cs.CV | [PDF](http://arxiv.org/pdf/2501.05226v1){: .btn .btn-green } |

**Abstract**: We introduce a single-view reconstruction technique of volumetric fields in
which multiple light scattering effects are omnipresent, such as in clouds. We
model the unknown distribution of volumetric fields using an unconditional
diffusion model trained on a novel benchmark dataset comprising 1,000
synthetically simulated volumetric density fields. The neural diffusion model
is trained on the latent codes of a novel, diffusion-friendly, monoplanar
representation. The generative model is used to incorporate a tailored
parametric diffusion posterior sampling technique into different reconstruction
tasks. A physically-based differentiable volume renderer is employed to provide
gradients with respect to light transport in the latent space. This stands in
contrast to classic NeRF approaches and makes the reconstructions better
aligned with observed data. Through various experiments, we demonstrate
single-view reconstruction of volumetric clouds at a previously unattainable
quality.



---

## Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID  Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou | cs.CV | [PDF](http://arxiv.org/pdf/2501.05379v1){: .btn .btn-green } |

**Abstract**: Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in
reconstructing detailed 3D scenes within multi-view setups and the emergence of
large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based
method utilizing a human face foundation model as guidance with just a single
image as input. To achieve that, we extend such a model for diverse-view human
head generation by fine-tuning on synthetic data and modifying its
conditioning. Our avatars maintain a dense correspondence with a human face
mesh template, allowing blendshape-based expression generation. This is
achieved through a modified 3DGS approach, connectivity regularizers, and a
strategic initialization tailored for our task. Additionally, we propose an
optional efficient SDS-based correction step to refine the blendshape
expressions, enhancing realism and diversity. Experiments demonstrate that
Arc2Avatar achieves state-of-the-art realism and identity preservation,
effectively addressing color issues by allowing the use of very low guidance,
enabled by our strong identity prior and initialization strategy, without
compromising detail.



---

## Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and  Photorealistic Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun | cs.CV | [PDF](http://arxiv.org/pdf/2501.05242v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis
in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM
methods utilizing 3DGS have failed to provide high-quality novel view rendering
for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods
perform well for RGB-D cameras but suffer significant degradation in rendering
quality for monocular cameras. In this paper, we present Scaffold-SLAM, which
delivers simultaneous localization and high-quality photorealistic mapping
across monocular, stereo, and RGB-D cameras. We introduce two key innovations
to achieve this state-of-the-art visual quality. First, we propose
Appearance-from-Motion embedding, enabling 3D Gaussians to better model image
appearance variations across different camera poses. Second, we introduce a
frequency regularization pyramid to guide the distribution of Gaussians,
allowing the model to effectively capture finer details in the scene. Extensive
experiments on monocular, stereo, and RGB-D datasets demonstrate that
Scaffold-SLAM significantly outperforms state-of-the-art methods in
photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D
datasets for monocular cameras.

Comments:
- 12 pages, 6 figures

---

## FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using  Gaussian Splatting with Depth-Feature Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-08 | Han Huang, Yulun Wu, Chao Deng, Ge Gao, Ming Gu, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.04628v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian Splatting has sparked a new trend in the field of computer
vision. Apart from novel view synthesis, it has also been extended to the area
of multi-view reconstruction. The latest methods facilitate complete, detailed
surface reconstruction while ensuring fast training speed. However, these
methods still require dense input views, and their output quality significantly
degrades with sparse views. We observed that the Gaussian primitives tend to
overfit the few training views, leading to noisy floaters and incomplete
reconstruction surfaces. In this paper, we present an innovative sparse-view
reconstruction framework that leverages intra-view depth and multi-view feature
consistency to achieve remarkably accurate surface reconstruction.
Specifically, we utilize monocular depth ranking information to supervise the
consistency of depth distribution within patches and employ a smoothness loss
to enhance the continuity of the distribution. To achieve finer surface
reconstruction, we optimize the absolute position of depth through multi-view
projection features. Extensive experiments on DTU and BlendedMVS demonstrate
that our method outperforms state-of-the-art methods with a speedup of 60x to
200x, achieving swift and fine-grained mesh reconstruction without the need for
costly pre-training.

Comments:
- Accepted by AAAI 2025. Project page:
  https://alvin528.github.io/FatesGS/

---

## GaussianVideo: Efficient Video Representation via Hierarchical Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-08 | Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem | cs.CV | [PDF](http://arxiv.org/pdf/2501.04782v1){: .btn .btn-green } |

**Abstract**: Efficient neural representations for dynamic video scenes are critical for
applications ranging from video compression to interactive simulations. Yet,
existing methods often face challenges related to high memory usage, lengthy
training times, and temporal consistency. To address these issues, we introduce
a novel neural video representation that combines 3D Gaussian splatting with
continuous camera motion modeling. By leveraging Neural ODEs, our approach
learns smooth camera trajectories while maintaining an explicit 3D scene
representation through Gaussians. Additionally, we introduce a spatiotemporal
hierarchical learning strategy, progressively refining spatial and temporal
features to enhance reconstruction quality and accelerate convergence. This
memory-efficient approach achieves high-quality rendering at impressive speeds.
Experimental results show that our hierarchical learning, combined with robust
camera motion modeling, captures complex dynamic scenes with strong temporal
consistency, achieving state-of-the-art performance across diverse video
datasets in both high- and low-motion scenarios.

Comments:
- 10 pages, 10 figures

---

## ConcealGS: Concealing Invisible Copyright Information in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Yifeng Yang, Hengyu Liu, Chenxin Li, Yining Sun, Wuyang Li, Yifan Liu, Yiyang Lin, Yixuan Yuan, Nanyang Ye | cs.CV | [PDF](http://arxiv.org/pdf/2501.03605v1){: .btn .btn-green } |

**Abstract**: With the rapid development of 3D reconstruction technology, the widespread
distribution of 3D data has become a future trend. While traditional visual
data (such as images and videos) and NeRF-based formats already have mature
techniques for copyright protection, steganographic techniques for the emerging
3D Gaussian Splatting (3D-GS) format have yet to be fully explored. To address
this, we propose ConcealGS, an innovative method for embedding implicit
information into 3D-GS. By introducing the knowledge distillation and gradient
optimization strategy based on 3D-GS, ConcealGS overcomes the limitations of
NeRF-based models and enhances the robustness of implicit information and the
quality of 3D reconstruction. We evaluate ConcealGS in various potential
application scenarios, and experimental results have demonstrated that
ConcealGS not only successfully recovers implicit information but also has
almost no impact on rendering quality, providing a new approach for embedding
invisible and recoverable information into 3D models in the future.



---

## MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval  Adjustment for Compact Dynamic 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, Munchurl Kim | cs.CV | [PDF](http://arxiv.org/pdf/2501.03714v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has made significant strides in scene
representation and neural rendering, with intense efforts focused on adapting
it for dynamic scenes. Despite delivering remarkable rendering quality and
speed, existing methods struggle with storage demands and representing complex
real-world motions. To tackle these issues, we propose MoDecGS, a
memory-efficient Gaussian splatting framework designed for reconstructing novel
views in challenging scenarios with complex motions. We introduce GlobaltoLocal
Motion Decomposition (GLMD) to effectively capture dynamic motions in a
coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global
CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold
representation to dynamic video reconstruction. For Global CS, we propose
Global Anchor Deformation (GAD) to efficiently represent global dynamics along
complex motions, by directly deforming the implicit Scaffold attributes which
are anchor position, offset, and local context features. Next, we finely adjust
local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly.
Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically
control the temporal coverage of each Local CS during training, allowing
MoDecGS to find optimal interval assignments based on the specified number of
temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an
average 70% reduction in model size over stateoftheart methods for dynamic 3D
Gaussians from realworld dynamic videos while maintaining or even improving
rendering quality.

Comments:
- The last two authors are co-corresponding authors. Please visit our
  project page at https://kaist-viclab.github.io/MoDecGS-site/

---

## ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Abhishek Saroha, Florian Hofherr, Mariia Gladkova, Cecilia Curreli, Or Litany, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2501.03875v1){: .btn .btn-green } |

**Abstract**: Stylizing a dynamic scene based on an exemplar image is critical for various
real-world applications, including gaming, filmmaking, and augmented and
virtual reality. However, achieving consistent stylization across both spatial
and temporal dimensions remains a significant challenge. Most existing methods
are designed for static scenes and often require an optimization process for
each style image, limiting their adaptability. We introduce ZDySS, a zero-shot
stylization framework for dynamic scenes, allowing our model to generalize to
previously unseen style images at inference. Our approach employs Gaussian
splatting for scene representation, linking each Gaussian to a learned feature
vector that renders a feature map for any given view and timestamp. By applying
style transfer on the learned feature vectors instead of the rendered feature
map, we enhance spatio-temporal consistency across frames. Our method
demonstrates superior performance and coherence over state-of-the-art baselines
in tests on real-world dynamic scenes, making it a robust solution for
practical applications.



---

## DehazeGS: Seeing Through Fog with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.03659v1){: .btn .btn-green } |

**Abstract**: Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and rendering quality. Although NeRF-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, NeRF's implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward rendering process. We introduce
DehazeGS, a method capable of decomposing and rendering a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy datasets demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both rendering
quality and computational efficiency.

Comments:
- 9 pages,4 figures

---

## NeuralSVG: An Implicit Representation for Text-to-Vector Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Sagi Polaczek, Yuval Alaluf, Elad Richardson, Yael Vinker, Daniel Cohen-Or | cs.CV | [PDF](http://arxiv.org/pdf/2501.03992v1){: .btn .btn-green } |

**Abstract**: Vector graphics are essential in design, providing artists with a versatile
medium for creating resolution-independent and highly editable visual content.
Recent advancements in vision-language and diffusion models have fueled
interest in text-to-vector graphics generation. However, existing approaches
often suffer from over-parameterized outputs or treat the layered structure - a
core feature of vector graphics - as a secondary goal, diminishing their
practical use. Recognizing the importance of layered SVG representations, we
propose NeuralSVG, an implicit neural representation for generating vector
graphics from text prompts. Inspired by Neural Radiance Fields (NeRFs),
NeuralSVG encodes the entire scene into the weights of a small MLP network,
optimized using Score Distillation Sampling (SDS). To encourage a layered
structure in the generated SVG, we introduce a dropout-based regularization
technique that strengthens the standalone meaning of each shape. We
additionally demonstrate that utilizing a neural representation provides an
added benefit of inference-time control, enabling users to dynamically adapt
the generated SVG based on user-provided inputs, all with a single learned
representation. Through extensive qualitative and quantitative evaluations, we
demonstrate that NeuralSVG outperforms existing methods in generating
structured and flexible SVG.

Comments:
- Project Page: https://sagipolaczek.github.io/NeuralSVG/

---

## NeRFs are Mirror Detectors: Using Structural Similarity for Multi-View  Mirror Scene Reconstruction with 3D Surface Primitives

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Leif Van Holland, Michael Weinmann, Jan U. Müller, Patrick Stotko, Reinhard Klein | cs.CV | [PDF](http://arxiv.org/pdf/2501.04074v1){: .btn .btn-green } |

**Abstract**: While neural radiance fields (NeRF) led to a breakthrough in photorealistic
novel view synthesis, handling mirroring surfaces still denotes a particular
challenge as they introduce severe inconsistencies in the scene representation.
Previous attempts either focus on reconstructing single reflective objects or
rely on strong supervision guidance in terms of additional user-provided
annotations of visible image regions of the mirrors, thereby limiting the
practical usability. In contrast, in this paper, we present NeRF-MD, a method
which shows that NeRFs can be considered as mirror detectors and which is
capable of reconstructing neural radiance fields of scenes containing mirroring
surfaces without the need for prior annotations. To this end, we first compute
an initial estimate of the scene geometry by training a standard NeRF using a
depth reprojection loss. Our key insight lies in the fact that parts of the
scene corresponding to a mirroring surface will still exhibit a significant
photometric inconsistency, whereas the remaining parts are already
reconstructed in a plausible manner. This allows us to detect mirror surfaces
by fitting geometric primitives to such inconsistent regions in this initial
stage of the training. Using this information, we then jointly optimize the
radiance field and mirror geometry in a second training stage to refine their
quality. We demonstrate the capability of our method to allow the faithful
detection of mirrors in the scene as well as the reconstruction of a single
consistent scene representation, and demonstrate its potential in comparison to
baseline and mirror-aware approaches.



---

## HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian  Splatting Based Data Augmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Wentian Qu, Jiahe Li, Jian Cheng, Jian Shi, Chenyu Meng, Cuixia Ma, Hongan Wang, Xiaoming Deng, Yinda Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.02845v1){: .btn .btn-green } |

**Abstract**: Understanding of bimanual hand-object interaction plays an important role in
robotics and virtual reality. However, due to significant occlusions between
hands and object as well as the high degree-of-freedom motions, it is
challenging to collect and annotate a high-quality, large-scale dataset, which
prevents further improvement of bimanual hand-object interaction-related
baselines. In this work, we propose a new 3D Gaussian Splatting based data
augmentation framework for bimanual hand-object interaction, which is capable
of augmenting existing dataset to large-scale photorealistic data with various
hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects
and hands, and to deal with the rendering blur problem due to multi-resolution
input images used, we design a super-resolution module. Second, we extend the
single hand grasping pose optimization module for the bimanual hand object to
generate various poses of bimanual hand-object interaction, which can
significantly expand the pose distribution of the dataset. Third, we conduct an
analysis for the impact of different aspects of the proposed data augmentation
on the understanding of the bimanual hand-object interaction. We perform our
data augmentation on two benchmarks, H2O and Arctic, and verify that our method
can improve the performance of the baselines.

Comments:
- Accepted by AAAI2025

---

## Compression of 3D Gaussian Splatting with Optimized Feature Planes and  Standard Video Codecs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge | cs.CV | [PDF](http://arxiv.org/pdf/2501.03399v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting is a recognized method for 3D scene representation,
known for its high rendering quality and speed. However, its substantial data
requirements present challenges for practical applications. In this paper, we
introduce an efficient compression technique that significantly reduces storage
overhead by using compact representation. We propose a unified architecture
that combines point cloud data and feature planes through a progressive
tri-plane structure. Our method utilizes 2D feature planes, enabling continuous
spatial representation. To further optimize these representations, we
incorporate entropy modeling in the frequency domain, specifically designed for
standard video codecs. We also propose channel-wise bit allocation to achieve a
better trade-off between bitrate consumption and feature plane representation.
Consequently, our model effectively leverages spatial correlations within the
feature planes to enhance rate-distortion performance using standard,
non-differentiable video codecs. Experimental results demonstrate that our
method outperforms existing methods in data compactness while maintaining high
rendering quality. Our project page is available at
https://fraunhoferhhi.github.io/CodecGS



---

## AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal  Conditions and Larger Scene

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Chaoran Feng, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuan, Yonghong Tian | cs.CV | [PDF](http://arxiv.org/pdf/2501.02807v2){: .btn .btn-green } |

**Abstract**: Compared to frame-based methods, computational neuromorphic imaging using
event cameras offers significant advantages, such as minimal motion blur,
enhanced temporal resolution, and high dynamic range. The multi-view
consistency of Neural Radiance Fields combined with the unique benefits of
event cameras, has spurred recent research into reconstructing NeRF from data
captured by moving event cameras. While showing impressive performance,
existing methods rely on ideal conditions with the availability of uniform and
high-quality event sequences and accurate camera poses, and mainly focus on the
object level reconstruction, thus limiting their practical applications. In
this work, we propose AE-NeRF to address the challenges of learning event-based
NeRF from non-ideal conditions, including non-uniform event sequences, noisy
poses, and various scales of scenes. Our method exploits the density of event
streams and jointly learn a pose correction module with an event-based NeRF
(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.
To generalize to larger scenes, we propose hierarchical event distillation with
a proposal e-NeRF network and a vanilla e-NeRF network to resample and refine
the reconstruction process. We further propose an event reconstruction loss and
a temporal loss to improve the view consistency of the reconstructed scene. We
established a comprehensive benchmark that includes large-scale scenes to
simulate practical non-ideal conditions, incorporating both synthetic and
challenging real-world event datasets. The experimental results show that our
method achieves a new state-of-the-art in event-based 3D reconstruction.



---

## Gaussian Masked Autoencoders

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, Shiry Ginosar | cs.CV | [PDF](http://arxiv.org/pdf/2501.03229v1){: .btn .btn-green } |

**Abstract**: This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While
reconstructive self-supervised learning frameworks such as MAE learns good
semantic abstractions, it is not trained for explicit spatial awareness. Our
approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic
abstractions and spatial understanding jointly. Like MAE, it reconstructs the
image end-to-end in the pixel space, but beyond MAE, it also introduces an
intermediate, 3D Gaussian-based representation and renders images via
splatting. We show that GMAE can enable various zero-shot learning capabilities
of spatial understanding (e.g., figure-ground segmentation, image layering,
edge detection, etc.) while preserving the high-level semantics of
self-supervised representation quality from MAE. To our knowledge, we are the
first to employ Gaussian primitives in an image representation learning
framework beyond optimization-based single-scene reconstructions. We believe
GMAE will inspire further research in this direction and contribute to
developing next-generation techniques for modeling high-fidelity visual data.
More details at https://brjathu.github.io/gmae



---

## GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields  through Efficient Dense 3D Point Tracking

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-05 | Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2501.02690v1){: .btn .btn-green } |

**Abstract**: 4D video control is essential in video generation as it enables the use of
sophisticated lens techniques, such as multi-camera shooting and dolly zoom,
which are currently unsupported by existing methods. Training a video Diffusion
Transformer (DiT) directly to control 4D content requires expensive multi-view
videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that
optimizes a 4D representation and renders videos according to different 4D
elements, such as camera pose and object motion editing, we bring pseudo 4D
Gaussian fields to video generation. Specifically, we propose a novel framework
that constructs a pseudo 4D Gaussian field with dense 3D point tracking and
renders the Gaussian field for all video frames. Then we finetune a pretrained
DiT to generate videos following the guidance of the rendered video, dubbed as
GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense
3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field
construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art
sparse 3D point tracking method, in accuracy and accelerates the inference
speed by two orders of magnitude. During the inference stage, GS-DiT can
generate videos with the same dynamic content while adhering to different
camera parameters, addressing a significant limitation of current video
generation models. GS-DiT demonstrates strong generalization capabilities and
extends the 4D controllability of Gaussian splatting to video generation beyond
just camera poses. It supports advanced cinematic effects through the
manipulation of the Gaussian field and camera intrinsics, making it a powerful
tool for creative video production. Demos are available at
https://wkbian.github.io/Projects/GS-DiT/.

Comments:
- Project Page: https://wkbian.github.io/Projects/GS-DiT/

---

## CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Chenhao Zhang, Yuanping Cao, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01695v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene
representation and reconstruction, leveraging densely distributed Gaussian
primitives to enable real-time rendering of high-resolution images. While
existing 3DGS methods perform well in scenes with minor view variation, large
view changes in cross-view scenes pose optimization challenges for these
methods. To address these issues, we propose a novel cross-view Gaussian
Splatting method for large-scale scene reconstruction, based on dual-branch
fusion. Our method independently reconstructs models from aerial and ground
views as two independent branches to establish the baselines of Gaussian
distribution, providing reliable priors for cross-view reconstruction during
both initialization and densification. Specifically, a gradient-aware
regularization strategy is introduced to mitigate smoothing issues caused by
significant view disparities. Additionally, a unique Gaussian supplementation
strategy is utilized to incorporate complementary information of dual-branch
into the cross-view model. Extensive experiments on benchmark datasets
demonstrate that our method achieves superior performance in novel view
synthesis compared to state-of-the-art methods.



---

## Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Alberta Longhini, Marcel Büsching, Bardienus P. Duisterhof, Jens Lundell, Jeffrey Ichnowski, Mårten Björkman, Danica Kragic | cs.CV | [PDF](http://arxiv.org/pdf/2501.01715v1){: .btn .btn-green } |

**Abstract**: We introduce Cloth-Splatting, a method for estimating 3D states of cloth from
RGB images through a prediction-update framework. Cloth-Splatting leverages an
action-conditioned dynamics model for predicting future states and uses 3D
Gaussian Splatting to update the predicted states. Our key insight is that
coupling a 3D mesh-based representation with Gaussian Splatting allows us to
define a differentiable map between the cloth state space and the image space.
This enables the use of gradient-based optimization techniques to refine
inaccurate state estimates using only RGB supervision. Our experiments
demonstrate that Cloth-Splatting not only improves state estimation accuracy
over current baselines but also reduces convergence time.

Comments:
- Accepted at the 8th Conference on Robot Learning (CoRL 2024). Code
  and videos available at: kth-rpl.github.io/cloth-splatting

---

## PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban  Buildings Reconstruction via Semantic-Aware Grouping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Tengfei Wang, Xin Wang, Yongmao Hou, Yiwei Xu, Wendi Zhang, Zongqian Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2501.01677v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a transformative method in the
field of real-time novel synthesis. Based on 3DGS, recent advancements cope
with large-scale scenes via spatial-based partition strategy to reduce video
memory and optimization time costs. In this work, we introduce a parallel
Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues
for both partitioning and Gaussian kernel optimization, enabling fine-grained
building surface reconstruction of large-scale urban areas without downsampling
the original image resolution. First, the Cross-modal model - Language Segment
Anything is leveraged to segment building masks. Then, the segmented building
regions is grouped into sub-regions according to the visibility check across
registered images. The Gaussian kernels for these sub-regions are optimized in
parallel with masked pixels. In addition, the normal loss is re-formulated for
the detected edges of masks to alleviate the ambiguities in normal vectors on
edges. Finally, to improve the optimization of 3D Gaussians, we introduce a
gradient-constrained balance-load loss that accounts for the complexity of the
corresponding scenes, effectively minimizing the thread waiting time in the
pixel-parallel rendering stage as well as the reconstruction lost. Extensive
experiments are tested on various urban datasets, the results demonstrated the
superior performance of our PG-SAG on building surface reconstruction, compared
to several state-of-the-art 3DGS-based methods. Project
Web:https://github.com/TFWang-9527/PG-SAG.



---

## EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren | cs.RO | [PDF](http://arxiv.org/pdf/2501.01895v1){: .btn .btn-green } |

**Abstract**: We introduce EnerVerse, a comprehensive framework for embodied future space
generation specifically designed for robotic manipulation tasks. EnerVerse
seamlessly integrates convolutional and bidirectional attention mechanisms for
inner-chunk space modeling, ensuring low-level consistency and continuity.
Recognizing the inherent redundancy in video data, we propose a sparse memory
context combined with a chunkwise unidirectional generative paradigm to enable
the generation of infinitely long sequences. To further augment robotic
capabilities, we introduce the Free Anchor View (FAV) space, which provides
flexible perspectives to enhance observation and analysis. The FAV space
mitigates motion modeling ambiguity, removes physical constraints in confined
environments, and significantly improves the robot's generalization and
adaptability across various tasks and settings. To address the prohibitive
costs and labor intensity of acquiring multi-camera observations, we present a
data engine pipeline that integrates a generative model with 4D Gaussian
Splatting (4DGS). This pipeline leverages the generative model's robust
generalization capabilities and the spatial constraints provided by 4DGS,
enabling an iterative enhancement of data quality and diversity, thus creating
a data flywheel effect that effectively narrows the sim-to-real gap. Finally,
our experiments demonstrate that the embodied future space generation prior
substantially enhances policy predictive capabilities, resulting in improved
overall performance, particularly in long-range robotic manipulation tasks.

Comments:
- Website: https://sites.google.com/view/enerverse

---

## Deformable Gaussian Splatting for Efficient and High-Fidelity  Reconstruction of Surgical Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-02 | Jiwei Shan, Zeyu Cai, Cheng-Tai Hsieh, Shing Shin Cheng, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01101v1){: .btn .btn-green } |

**Abstract**: Efficient and high-fidelity reconstruction of deformable surgical scenes is a
critical yet challenging task. Building on recent advancements in 3D Gaussian
splatting, current methods have seen significant improvements in both
reconstruction quality and rendering speed. However, two major limitations
remain: (1) difficulty in handling irreversible dynamic changes, such as tissue
shearing, which are common in surgical scenes; and (2) the lack of hierarchical
modeling for surgical scene deformation, which reduces rendering speed. To
address these challenges, we introduce EH-SurGS, an efficient and high-fidelity
reconstruction algorithm for deformable surgical scenes. We propose a
deformation modeling approach that incorporates the life cycle of 3D Gaussians,
effectively capturing both regular and irreversible deformations, thus
enhancing reconstruction quality. Additionally, we present an adaptive motion
hierarchy strategy that distinguishes between static and deformable regions
within the surgical scene. This strategy reduces the number of 3D Gaussians
passing through the deformation field, thereby improving rendering speed.
Extensive experiments demonstrate that our method surpasses existing
state-of-the-art approaches in both reconstruction quality and rendering speed.
Ablation studies further validate the effectiveness and necessity of our
proposed components. We will open-source our code upon acceptance of the paper.

Comments:
- 7 pages, 4 figures, submitted to ICRA 2025

---

## EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-02 | Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01003v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene
representation. Despite their impressive performance, they confront challenges
due to the limitation of structure-from-motion (SfM) methods on acquiring
accurate scene initialization, or the inefficiency of densification strategy.
In this paper, we introduce a novel framework EasySplat to achieve high-quality
3DGS modeling. Instead of using SfM for scene initialization, we employ a novel
method to release the power of large-scale pointmap approaches. Specifically,
we propose an efficient grouping strategy based on view similarity, and use
robust pointmap priors to obtain high-quality point clouds and camera poses for
3D scene initialization. After obtaining a reliable scene structure, we propose
a novel densification approach that adaptively splits Gaussian primitives based
on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme.
In this way, the proposed method tackles the limitation on initialization and
optimization, leading to an efficient and accurate 3DGS modeling. Extensive
experiments demonstrate that EasySplat outperforms the current state-of-the-art
(SOTA) in handling novel view synthesis.

Comments:
- 6 pages, 5figures
