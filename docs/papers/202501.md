---
layout: default
title: January 2025
parent: Papers
nav_order: 202501
---

<!---metadata--->


## LinPrim: Linear Primitives for Differentiable Volumetric Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-27 | Nicolas von Lützow, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2501.16312v1){: .btn .btn-green } |

**Abstract**: Volumetric rendering has become central to modern novel view synthesis
methods, which use differentiable rendering to optimize 3D scene
representations directly from observed views. While many recent works build on
NeRF or 3D Gaussians, we explore an alternative volumetric scene
representation. More specifically, we introduce two new scene representations
based on linear primitives-octahedra and tetrahedra-both of which define
homogeneous volumes bounded by triangular faces. This formulation aligns
naturally with standard mesh-based tools, minimizing overhead for downstream
applications. To optimize these primitives, we present a differentiable
rasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based
optimization while maintaining realtime rendering capabilities. Through
experiments on real-world datasets, we demonstrate comparable performance to
state-of-the-art volumetric methods while requiring fewer primitives to achieve
similar reconstruction fidelity. Our findings provide insights into the
geometry of volumetric rendering and suggest that adopting explicit polyhedra
can expand the design space of scene representations.



---

## GaussianToken: An Effective Image Tokenizer with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-26 | Jiajun Dong, Chengkun Wang, Wenzhao Zheng, Lei Chen, Jiwen Lu, Yansong Tang | cs.CV | [PDF](http://arxiv.org/pdf/2501.15619v1){: .btn .btn-green } |

**Abstract**: Effective image tokenization is crucial for both multi-modal understanding
and generation tasks due to the necessity of the alignment with discrete text
data. To this end, existing approaches utilize vector quantization (VQ) to
project pixels onto a discrete codebook and reconstruct images from the
discrete representation. However, compared with the continuous latent space,
the limited discrete codebook space significantly restrict the representational
ability of these image tokenizers. In this paper, we propose GaussianToken: An
Effective Image Tokenizer with 2D Gaussian Splatting as a solution. We first
represent the encoded samples as multiple flexible featured 2D Gaussians
characterized by positions, rotation angles, scaling factors, and feature
coefficients. We adopt the standard quantization for the Gaussian features and
then concatenate the quantization results with the other intrinsic Gaussian
parameters before the corresponding splatting operation and the subsequent
decoding module. In general, GaussianToken integrates the local influence of 2D
Gaussian distribution into the discrete space and thus enhances the
representation capability of the image tokenizer. Competitive reconstruction
performances on CIFAR, Mini-ImageNet, and ImageNet-1K demonstrate the
effectiveness of our framework. Our code is available at:
https://github.com/ChrisDong-THU/GaussianToken.



---

## Towards Better Robustness: Progressively Joint Pose-3DGS Learning for  Arbitrarily Long Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-25 | Zhen-Hui Dong, Sheng Ye, Yu-Hui Wen, Nannan Li, Yong-Jin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.15096v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation due to
its efficiency and high-fidelity rendering. However, 3DGS training requires a
known camera pose for each input view, typically obtained by
Structure-from-Motion (SfM) pipelines. Pioneering works have attempted to relax
this restriction but still face difficulties when handling long sequences with
complex camera trajectories. In this work, we propose Rob-GS, a robust
framework to progressively estimate camera poses and optimize 3DGS for
arbitrarily long video sequences. Leveraging the inherent continuity of videos,
we design an adjacent pose tracking method to ensure stable pose estimation
between consecutive frames. To handle arbitrarily long inputs, we adopt a
"divide and conquer" scheme that adaptively splits the video sequence into
several segments and optimizes them separately. Extensive experiments on the
Tanks and Temples dataset and our collected real-world dataset show that our
Rob-GS outperforms the state-of-the-arts.



---

## HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-25 | Yingzhi Tang, Qijian Zhang, Junhui Hou | cs.CV | [PDF](http://arxiv.org/pdf/2501.15008v1){: .btn .btn-green } |

**Abstract**: We present HuGDiffusion, a generalizable 3D Gaussian splatting (3DGS)
learning pipeline to achieve novel view synthesis (NVS) of human characters
from single-view input images. Existing approaches typically require monocular
videos or calibrated multi-view images as inputs, whose applicability could be
weakened in real-world scenarios with arbitrary and/or unknown camera poses. In
this paper, we aim to generate the set of 3DGS attributes via a diffusion-based
framework conditioned on human priors extracted from a single image.
Specifically, we begin with carefully integrated human-centric feature
extraction procedures to deduce informative conditioning signals. Based on our
empirical observations that jointly learning the whole 3DGS attributes is
challenging to optimize, we design a multi-stage generation strategy to obtain
different types of 3DGS attributes. To facilitate the training process, we
investigate constructing proxy ground-truth 3D Gaussian attributes as
high-quality attribute-level supervision signals. Through extensive
experiments, our HuGDiffusion shows significant performance improvements over
the state-of-the-art methods. Our code will be made publicly available.



---

## SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human  Pose and Talking Head Animation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-24 | Yujian Liu, Shidang Xu, Jing Guo, Dingbin Wang, Zairan Wang, Xianfeng Tan, Xiaoli Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.14646v1){: .btn .btn-green } |

**Abstract**: Generating talking avatar driven by audio remains a significant challenge.
Existing methods typically require high computational costs and often lack
sufficient facial detail and realism, making them unsuitable for applications
that demand high real-time performance and visual quality. Additionally, while
some methods can synchronize lip movement, they still face issues with
consistency between facial expressions and upper body movement, particularly
during silent periods. In this paper, we introduce SyncAnimation, the first
NeRF-based method that achieves audio-driven, stable, and real-time generation
of speaking avatar by combining generalized audio-to-pose matching and
audio-to-expression synchronization. By integrating AudioPose Syncer and
AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression
generation, progressively producing audio-synchronized upper body, head, and
lip shapes. Furthermore, the High-Synchronization Human Renderer ensures
seamless integration of the head and upper body, and achieves audio-sync lip.
The project page can be found at https://syncanimation.github.io

Comments:
- 11 pages, 7 figures

---

## HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-24 | Javier Yu, Timothy Chen, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2501.14147v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting offers expressive scene reconstruction, modeling a
broad range of visual, geometric, and semantic information. However, efficient
real-time map reconstruction with data streamed from multiple robots and
devices remains a challenge. To that end, we propose HAMMER, a server-based
collaborative Gaussian Splatting method that leverages widely available ROS
communication infrastructure to generate 3D, metric-semantic maps from
asynchronous robot data-streams with no prior knowledge of initial robot
positions and varying on-device pose estimators. HAMMER consists of (i) a frame
alignment module that transforms local SLAM poses and image data into a global
frame and requires no prior relative pose knowledge, and (ii) an online module
for training semantic 3DGS maps from streaming data. HAMMER handles mixed
perception modes, adjusts automatically for variations in image pre-processing
among different devices, and distills CLIP semantic codes into the 3D scene for
open-vocabulary language queries. In our real-world experiments, HAMMER creates
higher-fidelity maps (2x) compared to competing baselines and is useful for
downstream tasks, such as semantic goal-conditioned navigation (e.g., ``go to
the couch"). Accompanying content available at hammer-project.github.io.



---

## Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-24 | Anil Armagan, Albert Saà-Garriga, Bruno Manganelli, Mateusz Nowak, Mehmet Kerim Yucel | cs.CV | [PDF](http://arxiv.org/pdf/2501.14534v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting (GS) for 3D reconstruction has become quite popular due to
their fast training, inference speeds and high quality reconstruction. However,
GS-based reconstructions generally consist of millions of Gaussians, which
makes them hard to use on computationally constrained devices such as
smartphones. In this paper, we first propose a principled analysis of advances
in efficient GS methods. Then, we propose Trick-GS, which is a careful
combination of several strategies including (1) progressive training with
resolution, noise and Gaussian scales, (2) learning to prune and mask
primitives and SH bands by their significance, and (3) accelerated GS training
framework. Trick-GS takes a large step towards resource-constrained GS, where
faster run-time, smaller and faster-convergence of models is of paramount
concern. Our results on three datasets show that Trick-GS achieves up to 2x
faster training, 40x smaller disk size and 2x faster rendering speed compared
to vanilla GS, while having comparable accuracy.

Comments:
- Accepted at ICASSP'25

---

## Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and  3D Reconstruction from Noisy Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-24 | Xiaohao Xu, Tianyi Zhang, Shibo Zhao, Xiang Li, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Sebastian Scherer, Xiaonan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2501.14319v1){: .btn .btn-green } |

**Abstract**: We aim to redefine robust ego-motion estimation and photorealistic 3D
reconstruction by addressing a critical limitation: the reliance on noise-free
data in existing models. While such sanitized conditions simplify evaluation,
they fail to capture the unpredictable, noisy complexities of real-world
environments. Dynamic motion, sensor imperfections, and synchronization
perturbations lead to sharp performance declines when these models are deployed
in practice, revealing an urgent need for frameworks that embrace and excel
under real-world noise. To bridge this gap, we tackle three core challenges:
scalable data generation, comprehensive benchmarking, and model robustness
enhancement. First, we introduce a scalable noisy data synthesis pipeline that
generates diverse datasets simulating complex motion, sensor imperfections, and
synchronization errors. Second, we leverage this pipeline to create
Robust-Ego3D, a benchmark rigorously designed to expose noise-induced
performance degradation, highlighting the limitations of current learning-based
methods in ego-motion accuracy and 3D reconstruction quality. Third, we propose
Correspondence-guided Gaussian Splatting (CorrGS), a novel test-time adaptation
method that progressively refines an internal clean 3D representation by
aligning noisy observations with rendered RGB-D frames from clean 3D map,
enhancing geometric alignment and appearance restoration through visual
correspondence. Extensive experiments on synthetic and real-world data
demonstrate that CorrGS consistently outperforms prior state-of-the-art
methods, particularly in scenarios involving rapid motion and dynamic
illumination.

Comments:
- Accepted by ICLR 2025; 92 Pages; Project Repo:
  https://github.com/Xiaohao-Xu/SLAM-under-Perturbation. arXiv admin note:
  substantial text overlap with arXiv:2406.16850

---

## Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from  Unconstrained Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-24 | Yihui Li, Chengxin Lv, Hongyu Yang, Di Huang | cs.CV | [PDF](http://arxiv.org/pdf/2501.14231v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction from unconstrained image collections presents substantial
challenges due to varying appearances and transient occlusions. In this paper,
we introduce Micro-macro Wavelet-based Gaussian Splatting (MW-GS), a novel
approach designed to enhance 3D reconstruction by disentangling scene
representations into global, refined, and intrinsic components. The proposed
method features two key innovations: Micro-macro Projection, which allows
Gaussian points to capture details from feature maps across multiple scales
with enhanced diversity; and Wavelet-based Sampling, which leverages frequency
domain information to refine feature representations and significantly improve
the modeling of scene appearances. Additionally, we incorporate a Hierarchical
Residual Fusion Network to seamlessly integrate these features. Extensive
experiments demonstrate that MW-GS delivers state-of-the-art rendering
performance, surpassing existing methods.

Comments:
- 11 pages, 6 figures,accepted by AAAI 2025

---

## Dense-SfM: Structure from Motion with Dense Consistent Matching


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-24 | JongMin Lee, Sungjoo Yoo | cs.CV | [PDF](http://arxiv.org/pdf/2501.14277v1){: .btn .btn-green } |

**Abstract**: We present Dense-SfM, a novel Structure from Motion (SfM) framework designed
for dense and accurate 3D reconstruction from multi-view images. Sparse
keypoint matching, which traditional SfM methods often rely on, limits both
accuracy and point density, especially in texture-less areas. Dense-SfM
addresses this limitation by integrating dense matching with a Gaussian
Splatting (GS) based track extension which gives more consistent, longer
feature tracks. To further improve reconstruction accuracy, Dense-SfM is
equipped with a multi-view kernelized matching module leveraging transformer
and Gaussian Process architectures, for robust track refinement across
multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that
Dense-SfM offers significant improvements in accuracy and density over
state-of-the-art methods.



---

## GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable  Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-23 | Francesco Di Sario, Riccardo Renzulli, Marco Grangetto, Akihiro Sugimoto, Enzo Tartaglione | cs.CV | [PDF](http://arxiv.org/pdf/2501.13558v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting enhances real-time performance in novel view synthesis
by representing scenes with mixtures of Gaussians and utilizing differentiable
rasterization. However, it typically requires large storage capacity and high
VRAM, demanding the design of effective pruning and compression techniques.
Existing methods, while effective in some scenarios, struggle with scalability
and fail to adapt models based on critical factors such as computing
capabilities or bandwidth, requiring to re-train the model under different
configurations. In this work, we propose a novel, model-agnostic technique that
organizes Gaussians into several hierarchical layers, enabling progressive
Level of Detail (LoD) strategy. This method, combined with recent approach of
compression of 3DGS, allows a single model to instantly scale across several
compression ratios, with minimal to none impact to quality compared to a single
non-scalable model and without requiring re-training. We validate our approach
on typical datasets and benchmarks, showcasing low distortion and substantial
gains in terms of scalability and adaptability.



---

## VIGS SLAM: IMU-based Large-Scale 3D Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-23 | Gyuhyeon Pak, Euntai Kim | cs.RO | [PDF](http://arxiv.org/pdf/2501.13402v1){: .btn .btn-green } |

**Abstract**: Recently, map representations based on radiance fields such as 3D Gaussian
Splatting and NeRF, which excellent for realistic depiction, have attracted
considerable attention, leading to attempts to combine them with SLAM. While
these approaches can build highly realistic maps, large-scale SLAM still
remains a challenge because they require a large number of Gaussian images for
mapping and adjacent images as keyframes for tracking. We propose a novel 3D
Gaussian Splatting SLAM method, VIGS SLAM, that utilizes sensor fusion of RGB-D
and IMU sensors for large-scale indoor environments. To reduce the
computational load of 3DGS-based tracking, we adopt an ICP-based tracking
framework that combines IMU preintegration to provide a good initial guess for
accurate pose estimation. Our proposed method is the first to propose that
Gaussian Splatting-based SLAM can be effectively performed in large-scale
environments by integrating IMU sensor measurements. This proposal not only
enhances the performance of Gaussian Splatting SLAM beyond room-scale scenarios
but also achieves SLAM performance comparable to state-of-the-art methods in
large-scale indoor environments.

Comments:
- 7 pages, 5 figures

---

## Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-23 | Xianrui Luo, Juewen Peng, Zhongang Cai, Lei Yang, Fan Yang, Zhiguo Cao, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2501.13335v1){: .btn .btn-green } |

**Abstract**: We introduce Deblur-Avatar, a novel framework for modeling high-fidelity,
animatable 3D human avatars from motion-blurred monocular video inputs. Motion
blur is prevalent in real-world dynamic video capture, especially due to human
movements in 3D human avatar modeling. Existing methods either (1) assume sharp
image inputs, failing to address the detail loss introduced by motion blur, or
(2) mainly consider blur by camera movements, neglecting the human motion blur
which is more common in animatable avatars. Our proposed approach integrates a
human movement-based motion blur model into 3D Gaussian Splatting (3DGS). By
explicitly modeling human motion trajectories during exposure time, we jointly
optimize the trajectories and 3D Gaussians to reconstruct sharp, high-quality
human avatars. We employ a pose-dependent fusion mechanism to distinguish
moving body regions, optimizing both blurred and sharp areas effectively.
Extensive experiments on synthetic and real-world datasets demonstrate that
Deblur-Avatar significantly outperforms existing methods in rendering quality
and quantitative metrics, producing sharp avatar reconstructions and enabling
real-time rendering under challenging motion blur conditions.



---

## MultiDreamer3D: Multi-concept 3D Customization with Concept-Aware  Diffusion Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-23 | Wooseok Song, Seunggyu Chang, Jaejun Yoo | cs.CV | [PDF](http://arxiv.org/pdf/2501.13449v1){: .btn .btn-green } |

**Abstract**: While single-concept customization has been studied in 3D, multi-concept
customization remains largely unexplored. To address this, we propose
MultiDreamer3D that can generate coherent multi-concept 3D content in a
divide-and-conquer manner. First, we generate 3D bounding boxes using an
LLM-based layout controller. Next, a selective point cloud generator creates
coarse point clouds for each concept. These point clouds are placed in the 3D
bounding boxes and initialized into 3D Gaussian Splatting with concept labels,
enabling precise identification of concept attributions in 2D projections.
Finally, we refine 3D Gaussians via concept-aware interval score matching,
guided by concept-aware diffusion. Our experimental results show that
MultiDreamer3D not only ensures object presence and preserves the distinct
identities of each concept but also successfully handles complex cases such as
property change or interaction. To the best of our knowledge, we are the first
to address the multi-concept customization in 3D.

Comments:
- 9 pages

---

## GeomGS: LiDAR-Guided Geometry-Aware Gaussian Splatting for Robot  Localization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-23 | Jaewon Lee, Mangyu Kong, Minseong Park, Euntai Kim | cs.RO | [PDF](http://arxiv.org/pdf/2501.13417v1){: .btn .btn-green } |

**Abstract**: Mapping and localization are crucial problems in robotics and autonomous
driving. Recent advances in 3D Gaussian Splatting (3DGS) have enabled precise
3D mapping and scene understanding by rendering photo-realistic images.
However, existing 3DGS methods often struggle to accurately reconstruct a 3D
map that reflects the actual scale and geometry of the real world, which
degrades localization performance. To address these limitations, we propose a
novel 3DGS method called Geometry-Aware Gaussian Splatting (GeomGS). This
method fully integrates LiDAR data into 3D Gaussian primitives via a
probabilistic approach, as opposed to approaches that only use LiDAR as initial
points or introduce simple constraints for Gaussian points. To this end, we
introduce a Geometric Confidence Score (GCS), which identifies the structural
reliability of each Gaussian point. The GCS is optimized simultaneously with
Gaussians under probabilistic distance constraints to construct a precise
structure. Furthermore, we propose a novel localization method that fully
utilizes both the geometric and photometric properties of GeomGS. Our GeomGS
demonstrates state-of-the-art geometric and localization performance across
several benchmarks, while also improving photometric performance.

Comments:
- Preprint, Under review

---

## Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made  Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-22 | Yuang Shi, Simone Gasparini, Géraldine Morin, Chenggang Yang, Wei Tsang Ooi | cs.CV | [PDF](http://arxiv.org/pdf/2501.13045v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising representation for
photorealistic rendering of 3D scenes. However, its high storage requirements
pose significant challenges for practical applications. We observe that
Gaussians exhibit distinct roles and characteristics that are analogous to
traditional artistic techniques -- Like how artists first sketch outlines
before filling in broader areas with color, some Gaussians capture
high-frequency features like edges and contours; While other Gaussians
represent broader, smoother regions, that are analogous to broader brush
strokes that add volume and depth to a painting. Based on this observation, we
propose a novel hybrid representation that categorizes Gaussians into (i)
Sketch Gaussians, which define scene boundaries, and (ii) Patch Gaussians,
which cover smooth regions. Sketch Gaussians are efficiently encoded using
parametric models, leveraging their geometric coherence, while Patch Gaussians
undergo optimized pruning, retraining, and vector quantization to maintain
volumetric consistency and storage efficiency. Our comprehensive evaluation
across diverse indoor and outdoor scenes demonstrates that this structure-aware
approach achieves up to 32.62% improvement in PSNR, 19.12% in SSIM, and 45.41%
in LPIPS at equivalent model sizes, and correspondingly, for an indoor scene,
our model maintains the visual quality with 2.3% of the original model size.



---

## DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet  Transform

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-22 | Hung Nguyen, Blark Runfa Li, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2501.12637v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) has achieved superior performance in novel view
synthesis and 3D scene representation, but its practical applications are
hindered by slow convergence and reliance on dense training views. To this end,
we present DWTNeRF, a unified framework based on Instant-NGP's fast-training
hash encoding. It is coupled with regularization terms designed for few-shot
NeRF, which operates on sparse training views. Our DWTNeRF includes a novel
Discrete Wavelet loss that allows explicit prioritization of low frequencies
directly in the training objective, reducing few-shot NeRF's overfitting on
high frequencies in earlier training stages. We additionally introduce a
model-based approach, based on multi-head attention, that is compatible with
INGP-based models, which are sensitive to architectural changes. On the 3-shot
LLFF benchmark, DWTNeRF outperforms Vanilla NeRF by 15.07% in PSNR, 24.45% in
SSIM and 36.30% in LPIPS. Our approach encourages a re-thinking of current
few-shot approaches for INGP-based models.

Comments:
- 10 pages, 6 figures

---

## 3DGS$^2$: Near Second-order Converging 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-22 | Lei Lan, Tianjia Shao, Zixuan Lu, Yu Zhang, Chenfanfu Jiang, Yin Yang | cs.CV | [PDF](http://arxiv.org/pdf/2501.13975v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a mainstream solution for novel
view synthesis and 3D reconstruction. By explicitly encoding a 3D scene using a
collection of Gaussian kernels, 3DGS achieves high-quality rendering with
superior efficiency. As a learning-based approach, 3DGS training has been dealt
with the standard stochastic gradient descent (SGD) method, which offers at
most linear convergence. Consequently, training often requires tens of minutes,
even with GPU acceleration. This paper introduces a (near) second-order
convergent training algorithm for 3DGS, leveraging its unique properties. Our
approach is inspired by two key observations. First, the attributes of a
Gaussian kernel contribute independently to the image-space loss, which
endorses isolated and local optimization algorithms. We exploit this by
splitting the optimization at the level of individual kernel attributes,
analytically constructing small-size Newton systems for each parameter group,
and efficiently solving these systems on GPU threads. This achieves Newton-like
convergence per training image without relying on the global Hessian. Second,
kernels exhibit sparse and structured coupling across input images. This
property allows us to effectively utilize spatial information to mitigate
overshoot during stochastic training. Our method converges an order faster than
standard GPU-based 3DGS training, requiring over $10\times$ fewer iterations
while maintaining or surpassing the quality of the compared with the SGD-based
3DGS reconstructions.

Comments:
- 11 pages

---

## Neural Radiance Fields for the Real World: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-22 | Wenhui Xiao, Remi Chierchia, Rodrigo Santa Cruz, Xuesong Li, David Ahmedt-Aristizabal, Olivier Salvado, Clinton Fookes, Leo Lebrat | cs.CV | [PDF](http://arxiv.org/pdf/2501.13104v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since
release. NeRFs can effectively reconstruct complex 3D scenes from 2D images,
advancing different fields and applications such as scene understanding, 3D
content generation, and robotics. Despite significant research progress, a
thorough review of recent innovations, applications, and challenges is lacking.
This survey compiles key theoretical advancements and alternative
representations and investigates emerging challenges. It further explores
applications on reconstruction, highlights NeRFs' impact on computer vision and
robotics, and reviews essential datasets and toolkits. By identifying gaps in
the literature, this survey discusses open challenges and offers directions for
future research.



---

## GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-22 | Junzhe Jiang, Chun Gu, Yurui Chen, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.13971v1){: .btn .btn-green } |

**Abstract**: LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR
simulation, offering valuable simulated point cloud data from novel viewpoints
to aid in autonomous driving systems. However, existing LiDAR NVS methods
typically rely on neural radiance fields (NeRF) as their 3D representation,
which incurs significant computational costs in both training and rendering.
Moreover, NeRF and its variants are designed for symmetrical scenes, making
them ill-suited for driving scenarios. To address these challenges, we propose
GS-LiDAR, a novel framework for generating realistic LiDAR point clouds with
panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with
periodic vibration properties, allowing for precise geometric reconstruction of
both static and dynamic elements in driving scenarios. We further introduce a
novel panoramic rendering technique with explicit ray-splat intersection,
guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop
spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance
the realism of the rendered point clouds. Extensive experiments on KITTI-360
and nuScenes demonstrate the superiority of our method in terms of quantitative
metrics, visual quality, as well as training and rendering efficiency.



---

## HAC++: Towards 100X Compression of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-21 | Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2501.12255v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel
view synthesis, boasting rapid rendering speed with high fidelity. However, the
substantial Gaussians and their associated attributes necessitate effective
compression techniques. Nevertheless, the sparse and unorganized nature of the
point cloud of Gaussians (or anchors in our paper) presents challenges for
compression. To achieve a compact size, we propose HAC++, which leverages the
relationships between unorganized anchors and a structured hash grid, utilizing
their mutual information for context modeling. Additionally, HAC++ captures
intra-anchor contextual relationships to further enhance compression
performance. To facilitate entropy coding, we utilize Gaussian distributions to
precisely estimate the probability of each quantized attribute, where an
adaptive quantization module is proposed to enable high-precision quantization
of these attributes for improved fidelity restoration. Moreover, we incorporate
an adaptive masking strategy to eliminate invalid Gaussians and anchors.
Overall, HAC++ achieves a remarkable size reduction of over 100X compared to
vanilla 3DGS when averaged on all datasets, while simultaneously improving
fidelity. It also delivers more than 20X size reduction compared to
Scaffold-GS. Our code is available at
https://github.com/YihangChen-ee/HAC-plus.

Comments:
- Project Page: https://yihangchen-ee.github.io/project_hac++/ Code:
  https://github.com/YihangChen-ee/HAC-plus. This paper is a journal extension
  of HAC at arXiv:2403.14530 (ECCV 2024)

---

## GSVC: Efficient Video Representation and Compression Through 2D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-21 | Longan Wang, Yuang Shi, Wei Tsang Ooi | cs.CV | [PDF](http://arxiv.org/pdf/2501.12060v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian splats have emerged as a revolutionary, effective, learned
representation for static 3D scenes. In this work, we explore using 2D Gaussian
splats as a new primitive for representing videos. We propose GSVC, an approach
to learning a set of 2D Gaussian splats that can effectively represent and
compress video frames. GSVC incorporates the following techniques: (i) To
exploit temporal redundancy among adjacent frames, which can speed up training
and improve the compression efficiency, we predict the Gaussian splats of a
frame based on its previous frame; (ii) To control the trade-offs between file
size and quality, we remove Gaussian splats with low contribution to the video
quality; (iii) To capture dynamics in videos, we randomly add Gaussian splats
to fit content with large motion or newly-appeared objects; (iv) To handle
significant changes in the scene, we detect key frames based on loss
differences during the learning process. Experiment results show that GSVC
achieves good rate-distortion trade-offs, comparable to state-of-the-art video
codecs such as AV1 and VVC, and a rendering speed of 1500 fps for a 1920x1080
video.



---

## DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial  Basis Functions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-21 | Vishagar Arunan, Saeedha Nazar, Hashiru Pramuditha, Vinasirajan Viruthshaan, Sameera Ramasinghe, Simon Lucey, Ranga Rodrigo | cs.CV | [PDF](http://arxiv.org/pdf/2501.12369v1){: .btn .btn-green } |

**Abstract**: Splatting-based 3D reconstruction methods have gained popularity with the
advent of 3D Gaussian Splatting, efficiently synthesizing high-quality novel
views. These methods commonly resort to using exponential family functions,
such as the Gaussian function, as reconstruction kernels due to their
anisotropic nature, ease of projection, and differentiability in rasterization.
However, the field remains restricted to variations within the exponential
family, leaving generalized reconstruction kernels largely underexplored,
partly due to the lack of easy integrability in 3D to 2D projections. In this
light, we show that a class of decaying anisotropic radial basis functions
(DARBFs), which are non-negative functions of the Mahalanobis distance,
supports splatting by approximating the Gaussian function's closed-form
integration advantage. With this fresh perspective, we demonstrate up to 34%
faster convergence during training and a 15% reduction in memory consumption
across various DARB reconstruction kernels, while maintaining comparable PSNR,
SSIM, and LPIPS results. We will make the code available.

Comments:
- Link to the project page:
  https://randomnerds.github.io/darbs.github.io/

---

## Fast Underwater Scene Reconstruction using Multi-View Stereo and  Physical Imaging

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-21 | Shuyi Hu, Qi Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.11884v1){: .btn .btn-green } |

**Abstract**: Underwater scene reconstruction poses a substantial challenge because of the
intricate interplay between light and the medium, resulting in scattering and
absorption effects that make both depth estimation and rendering more complex.
While recent Neural Radiance Fields (NeRF) based methods for underwater scenes
achieve high-quality results by modeling and separating the scattering medium,
they still suffer from slow training and rendering speeds. To address these
limitations, we propose a novel method that integrates Multi-View Stereo (MVS)
with a physics-based underwater image formation model. Our approach consists of
two branches: one for depth estimation using the traditional cost volume
pipeline of MVS, and the other for rendering based on the physics-based image
formation model. The depth branch improves scene geometry, while the medium
branch determines the scattering parameters to achieve precise scene rendering.
Unlike traditional MVSNet methods that rely on ground-truth depth, our method
does not necessitate the use of depth truth, thus allowing for expedited
training and rendering processes. By leveraging the medium subnet to estimate
the medium parameters and combining this with a color MLP for rendering, we
restore the true colors of underwater scenes and achieve higher-fidelity
geometric representations. Experimental results show that our method enables
high-quality synthesis of novel views in scattering media, clear views
restoration by removing the medium, and outperforms existing methods in
rendering quality and training efficiency.



---

## DNRSelect: Active Best View Selection for Deferred Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-21 | Dongli Wu, Haochen Li, Xiaobao Wei | cs.CV | [PDF](http://arxiv.org/pdf/2501.12150v1){: .btn .btn-green } |

**Abstract**: Deferred neural rendering (DNR) is an emerging computer graphics pipeline
designed for high-fidelity rendering and robotic perception. However, DNR
heavily relies on datasets composed of numerous ray-traced images and demands
substantial computational resources. It remains under-explored how to reduce
the reliance on high-quality ray-traced images while maintaining the rendering
fidelity. In this paper, we propose DNRSelect, which integrates a reinforcement
learning-based view selector and a 3D texture aggregator for deferred neural
rendering. We first propose a novel view selector for deferred neural rendering
based on reinforcement learning, which is trained on easily obtained rasterized
images to identify the optimal views. By acquiring only a few ray-traced images
for these selected views, the selector enables DNR to achieve high-quality
rendering. To further enhance spatial awareness and geometric consistency in
DNR, we introduce a 3D texture aggregator that fuses pyramid features from
depth maps and normal maps with UV maps. Given that acquiring ray-traced images
is more time-consuming than generating rasterized images, DNRSelect minimizes
the need for ray-traced data by using only a few selected views while still
achieving high-fidelity rendering results. We conduct detailed experiments and
ablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness
of DNRSelect. The code will be released.

Comments:
- 7 pages, 8 figures, submitted to ICRA 2025

---

## See In Detail: Enhancing Sparse-view 3D Gaussian Splatting with Local  Depth and Semantic Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-20 | Zongqi He, Zhe Xiao, Kin-Chung Chan, Yushen Zuo, Jun Xiao, Kin-Man Lam | cs.CV | [PDF](http://arxiv.org/pdf/2501.11508v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown remarkable performance in novel view
synthesis. However, its rendering quality deteriorates with sparse inphut
views, leading to distorted content and reduced details. This limitation
hinders its practical application. To address this issue, we propose a
sparse-view 3DGS method. Given the inherently ill-posed nature of sparse-view
rendering, incorporating prior information is crucial. We propose a semantic
regularization technique, using features extracted from the pretrained DINO-ViT
model, to ensure multi-view semantic consistency. Additionally, we propose
local depth regularization, which constrains depth values to improve
generalization on unseen views. Our method outperforms state-of-the-art novel
view synthesis approaches, achieving up to 0.4dB improvement in terms of PSNR
on the LLFF dataset, with reduced distortion and enhanced visual quality.

Comments:
- 5 pages, 5 figures, has been accepted by the ICASSP 2025

---

## RDG-GS: Relative Depth Guidance with Gaussian Splatting for Real-time  Sparse-View 3D Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-19 | Chenlu Zhan, Yufei Zhang, Yu Lin, Gaoang Wang, Hongwei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2501.11102v1){: .btn .btn-green } |

**Abstract**: Efficiently synthesizing novel views from sparse inputs while maintaining
accuracy remains a critical challenge in 3D reconstruction. While advanced
techniques like radiance fields and 3D Gaussian Splatting achieve rendering
quality and impressive efficiency with dense view inputs, they suffer from
significant geometric reconstruction errors when applied to sparse input views.
Moreover, although recent methods leverage monocular depth estimation to
enhance geometric learning, their dependence on single-view estimated depth
often leads to view inconsistency issues across different viewpoints.
Consequently, this reliance on absolute depth can introduce inaccuracies in
geometric information, ultimately compromising the quality of scene
reconstruction with Gaussian splats. In this paper, we present RDG-GS, a novel
sparse-view 3D rendering framework with Relative Depth Guidance based on 3D
Gaussian Splatting. The core innovation lies in utilizing relative depth
guidance to refine the Gaussian field, steering it towards view-consistent
spatial geometric representations, thereby enabling the reconstruction of
accurate geometric structures and capturing intricate textures. First, we
devise refined depth priors to rectify the coarse estimated depth and insert
global and fine-grained scene information to regular Gaussians. Building on
this, to address spatial geometric inaccuracies from absolute depth, we propose
relative depth guidance by optimizing the similarity between spatially
correlated patches of depth and images. Additionally, we also directly deal
with the sparse areas challenging to converge by the adaptive sampling for
quick densification. Across extensive experiments on Mip-NeRF360, LLFF, DTU,
and Blender, RDG-GS demonstrates state-of-the-art rendering quality and
efficiency, making a significant advancement for real-world application.

Comments:
- 24 pages, 12 figures

---

## Decoupling Appearance Variations with 3D Consistent Features in Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-18 | Jiaqi Lin, Zhihao Li, Binxiao Huang, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Xiaofei Wu, Fenglong Song, Wenming Yang | cs.CV | [PDF](http://arxiv.org/pdf/2501.10788v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has emerged as a prominent 3D representation in novel view
synthesis, but it still suffers from appearance variations, which are caused by
various factors, such as modern camera ISPs, different time of day, weather
conditions, and local light changes. These variations can lead to floaters and
color distortions in the rendered images/videos. Recent appearance modeling
approaches in Gaussian Splatting are either tightly coupled with the rendering
process, hindering real-time rendering, or they only account for mild global
variations, performing poorly in scenes with local light changes. In this
paper, we propose DAVIGS, a method that decouples appearance variations in a
plug-and-play and efficient manner. By transforming the rendering results at
the image level instead of the Gaussian level, our approach can model
appearance variations with minimal optimization time and memory overhead.
Furthermore, our method gathers appearance-related information in 3D space to
transform the rendered images, thus building 3D consistency across views
implicitly. We validate our method on several appearance-variant scenes, and
demonstrate that it achieves state-of-the-art rendering quality with minimal
training time and memory usage, without compromising rendering speeds.
Additionally, it provides performance improvements for different Gaussian
Splatting baselines in a plug-and-play manner.

Comments:
- Accepted to AAAI 2025. Project website:
  https://davi-gaussian.github.io

---

## GSTAR: Gaussian Surface Tracking and Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-17 | Chengwei Zheng, Lixin Xue, Juan Zarate, Jie Song | cs.CV | [PDF](http://arxiv.org/pdf/2501.10283v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting techniques have enabled efficient photo-realistic
rendering of static scenes. Recent works have extended these approaches to
support surface reconstruction and tracking. However, tracking dynamic surfaces
with 3D Gaussians remains challenging due to complex topology changes, such as
surfaces appearing, disappearing, or splitting. To address these challenges, we
propose GSTAR, a novel method that achieves photo-realistic rendering, accurate
surface reconstruction, and reliable 3D tracking for general dynamic scenes
with changing topology. Given multi-view captures as input, GSTAR binds
Gaussians to mesh faces to represent dynamic objects. For surfaces with
consistent topology, GSTAR maintains the mesh topology and tracks the meshes
using Gaussians. In regions where topology changes, GSTAR adaptively unbinds
Gaussians from the mesh, enabling accurate registration and the generation of
new surfaces based on these optimized Gaussians. Additionally, we introduce a
surface-based scene flow method that provides robust initialization for
tracking between frames. Experiments demonstrate that our method effectively
tracks and reconstructs dynamic surfaces, enabling a range of applications. Our
project page with the code release is available at
https://eth-ait.github.io/GSTAR/.



---

## Surface-SOS: Self-Supervised Object Segmentation via Neural Surface  Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-17 | Xiaoyun Zheng, Liwei Liao, Jianbo Jiao, Feng Gao, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2501.09947v1){: .btn .btn-green } |

**Abstract**: Self-supervised Object Segmentation (SOS) aims to segment objects without any
annotations. Under conditions of multi-camera inputs, the structural, textural
and geometrical consistency among each view can be leveraged to achieve
fine-grained object segmentation. To make better use of the above information,
we propose Surface representation based Self-supervised Object Segmentation
(Surface-SOS), a new framework to segment objects for each view by 3D surface
representation from multi-view images of a scene. To model high-quality
geometry surfaces for complex scenes, we design a novel scene representation
scheme, which decomposes the scene into two complementary neural representation
modules respectively with a Signed Distance Function (SDF). Moreover,
Surface-SOS is able to refine single-view segmentation with multi-view
unlabeled images, by introducing coarse segmentation masks as additional input.
To the best of our knowledge, Surface-SOS is the first self-supervised approach
that leverages neural surface representation to break the dependence on large
amounts of annotated data and strong constraints. These constraints typically
involve observing target objects against a static background or relying on
temporal supervision in videos. Extensive experiments on standard benchmarks
including LLFF, CO3D, BlendedMVS, TUM and several real-world scenes show that
Surface-SOS always yields finer object masks than its NeRF-based counterparts
and surpasses supervised single-view baselines remarkably. Code is available
at: https://github.com/zhengxyun/Surface-SOS.

Comments:
- Accepted by TIP

---

## Poxel: Voxel Reconstruction for 3D Printing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-16 | Ruixiang Cao, Satoshi Yagi, Satoshi Yamamori, Jun Morimoto | cs.GR | [PDF](http://arxiv.org/pdf/2501.10474v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D reconstruction, especially through neural rendering
approaches like Neural Radiance Fields (NeRF) and Plenoxel, have led to
high-quality 3D visualizations. However, these methods are optimized for
digital environments and employ view-dependent color models (RGB) and 2D
splatting techniques, which do not translate well to physical 3D printing. This
paper introduces "Poxel", which stands for Printable-Voxel, a voxel-based 3D
reconstruction framework optimized for photopolymer jetting 3D printing, which
allows for high-resolution, full-color 3D models using a CMYKWCl color model.
Our framework directly outputs printable voxel grids by removing
view-dependency and converting the digital RGB color space to a physical
CMYKWCl color space suitable for multi-material jetting. The proposed system
achieves better fidelity and quality in printed models, aligning with the
requirements of physical 3D objects.



---

## Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective  Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-16 | Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, Wenzhen Yue | cs.CV | [PDF](http://arxiv.org/pdf/2501.09460v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) often struggle with reconstructing and
rendering highly reflective scenes. Recent advancements have developed various
reflection-aware appearance models to enhance NeRF's capability to render
specular reflections. However, the robust reconstruction of highly reflective
scenes is still hindered by the inherent shape ambiguity on specular surfaces.
Existing methods typically rely on additional geometry priors to regularize the
shape prediction, but this can lead to oversmoothed geometry in complex scenes.
Observing the critical role of surface normals in parameterizing reflections,
we introduce a transmittance-gradient-based normal estimation technique that
remains robust even under ambiguous shape conditions. Furthermore, we propose a
dual activated densities module that effectively bridges the gap between smooth
surface normals and sharp object boundaries. Combined with a reflection-aware
appearance model, our proposed method achieves robust reconstruction and
high-fidelity rendering of scenes featuring both highly specular reflections
and intricate geometric structures. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art methods on various datasets.

Comments:
- AAAI 2025, code available at https://github.com/sjj118/Normal-NeRF

---

## Creating Virtual Environments with 3D Gaussian Splatting: A Comparative  Study

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-16 | Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng | cs.CV | [PDF](http://arxiv.org/pdf/2501.09302v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as an innovative and
efficient 3D representation technique. While its potential for extended reality
(XR) applications is frequently highlighted, its practical effectiveness
remains underexplored. In this work, we examine three distinct 3DGS-based
approaches for virtual environment (VE) creation, leveraging their unique
strengths for efficient and visually compelling scene representation. By
conducting a comparable study, we evaluate the feasibility of 3DGS in creating
immersive VEs, identify its limitations in XR applications, and discuss future
research and development opportunities.

Comments:
- IEEE VR 2025 Posters

---

## GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused  Odometry with Gaussian Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-15 | Sheng Hong, Chunran Zheng, Yishu Shen, Changze Li, Fu Zhang, Tong Qin, Shaojie Shen | cs.RO | [PDF](http://arxiv.org/pdf/2501.08672v1){: .btn .btn-green } |

**Abstract**: In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene
representation approach. However, existing vision-only 3D-GS methods often rely
on hand-crafted heuristics for point-cloud densification and face challenges in
handling occlusions and high GPU memory and computation consumption.
LiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior
performance in localization and dense mapping by leveraging complementary
sensing characteristics: rich texture information from cameras, precise
geometric measurements from LiDAR, and high-frequency motion data from IMU.
Inspired by this, we propose a novel real-time Gaussian-based simultaneous
localization and mapping (SLAM) system. Our map system comprises a global
Gaussian map and a sliding window of Gaussians, along with an IESKF-based
odometry. The global Gaussian map consists of hash-indexed voxels organized in
a recursive octree, effectively covering sparse spatial volumes while adapting
to different levels of detail and scales. The Gaussian map is initialized
through multi-sensor fusion and optimized with photometric gradients. Our
system incrementally maintains a sliding window of Gaussians, significantly
reducing GPU computation and memory consumption by only optimizing the map
within the sliding window. Moreover, we implement a tightly coupled
multi-sensor fusion odometry with an iterative error state Kalman filter
(IESKF), leveraging real-time updating and rendering of the Gaussian map. Our
system represents the first real-time Gaussian-based SLAM framework deployable
on resource-constrained embedded systems, demonstrated on the NVIDIA Jetson
Orin NX platform. The framework achieves real-time performance while
maintaining robust multi-sensor fusion capabilities. All implementation
algorithms, hardware designs, and CAD models will be publicly available.



---

## SLC$^2$-SLAM: Semantic-guided Loop Closure with Shared Latent Code for  NeRF SLAM

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-15 | Yuhang Ming, Di Ma, Weichen Dai, Han Yang, Rui Fan, Guofeng Zhang, Wanzeng Kong | cs.RO | [PDF](http://arxiv.org/pdf/2501.08880v1){: .btn .btn-green } |

**Abstract**: Targeting the notorious cumulative drift errors in NeRF SLAM, we propose a
Semantic-guided Loop Closure with Shared Latent Code, dubbed SLC$^2$-SLAM.
Especially, we argue that latent codes stored in many NeRF SLAM systems are not
fully exploited, as they are only used for better reconstruction. In this
paper, we propose a simple yet effective way to detect potential loops using
the same latent codes as local features. To further improve the loop detection
performance, we use the semantic information, which are also decoded from the
same latent codes to guide the aggregation of local features. Finally, with the
potential loops detected, we close them with a graph optimization followed by
bundle adjustment to refine both the estimated poses and the reconstructed
scene. To evaluate the performance of our SLC$^2$-SLAM, we conduct extensive
experiments on Replica and ScanNet datasets. Our proposed semantic-guided loop
closure significantly outperforms the pre-trained NetVLAD and ORB combined with
Bag-of-Words, which are used in all the other NeRF SLAM with loop closure. As a
result, our SLC$^2$-SLAM also demonstrated better tracking and reconstruction
performance, especially in larger scenes with more loops, like ScanNet.

Comments:
- 8 pages, 5 figures, 4 tables

---

## CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes  with Gaussian Representation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-15 | Qi Ma, Runyi Yang, Bin Ren, Ender Konukoglu, Luc Van Gool, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2501.08982v1){: .btn .btn-green } |

**Abstract**: Localizing text descriptions in large-scale 3D scenes is inherently an
ambiguous task. This nonetheless arises while describing general concepts, e.g.
all traffic lights in a city.
  To facilitate reasoning based on such concepts, text localization in the form
of distribution is required. In this paper, we generate the distribution of the
camera poses conditioned upon the textual description.
  To facilitate such generation, we propose a diffusion-based architecture that
conditionally diffuses the noisy 6DoF camera poses to their plausible
locations.
  The conditional signals are derived from the text descriptions, using the
pre-trained text encoders. The connection between text descriptions and pose
distribution is established through pretrained Vision-Language-Model, i.e.
CLIP. Furthermore, we demonstrate that the candidate poses for the distribution
can be further refined by rendering potential poses using 3D Gaussian
splatting, guiding incorrectly posed samples towards locations that better
align with the textual description, through visual reasoning.
  We demonstrate the effectiveness of our method by comparing it with both
standard retrieval methods and learning-based approaches. Our proposed method
consistently outperforms these baselines across all five large-scale datasets.
Our source code and dataset will be made publicly available.



---

## BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal  Scene Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-15 | Xiaolu Hou, Mingcheng Li, Dingkang Yang, Jiawei Chen, Ziyun Qian, Xiao Zhao, Yue Jiang, Jinjie Wei, Qingyao Xu, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.10462v1){: .btn .btn-green } |

**Abstract**: With the widespread use of virtual reality applications, 3D scene generation
has become a new challenging research frontier. 3D scenes have highly complex
structures and need to ensure that the output is dense, coherent, and contains
all necessary structures. Many current 3D scene generation methods rely on
pre-trained text-to-image diffusion models and monocular depth estimators.
However, the generated scenes occupy large amounts of storage space and often
lack effective regularisation methods, leading to geometric distortions. To
this end, we propose BloomScene, a lightweight structured 3D Gaussian splatting
for crossmodal scene generation, which creates diverse and high-quality 3D
scenes from text or image inputs. Specifically, a crossmodal progressive scene
generation framework is proposed to generate coherent scenes utilizing
incremental point cloud reconstruction and 3D Gaussian splatting. Additionally,
we propose a hierarchical depth prior-based regularization mechanism that
utilizes multi-level constraints on depth accuracy and smoothness to enhance
the realism and continuity of the generated scenes. Ultimately, we propose a
structured context-guided compression mechanism that exploits structured hash
grids to model the context of unorganized anchor attributes, which
significantly eliminates structural redundancy and reduces storage overhead.
Comprehensive experiments across multiple scenes demonstrate the significant
potential and advantages of our framework compared with several baselines.



---

## Object-Centric 2D Gaussian Splatting: Background Removal and  Occlusion-Aware Pruning for Compact Object Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-14 | Marcel Rogge, Didier Stricker | cs.CV | [PDF](http://arxiv.org/pdf/2501.08174v1){: .btn .btn-green } |

**Abstract**: Current Gaussian Splatting approaches are effective for reconstructing entire
scenes but lack the option to target specific objects, making them
computationally expensive and unsuitable for object-specific applications. We
propose a novel approach that leverages object masks to enable targeted
reconstruction, resulting in object-centric models. Additionally, we introduce
an occlusion-aware pruning strategy to minimize the number of Gaussians without
compromising quality. Our method reconstructs compact object models, yielding
object-centric Gaussian and mesh representations that are up to 96\% smaller
and up to 71\% faster to train compared to the baseline while retaining
competitive quality. These representations are immediately usable for
downstream applications such as appearance editing and physics simulation
without additional processing.

Comments:
- Accepted at ICPRAM 2025 (https://icpram.scitevents.org/Home.aspx)

---

## VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large  Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-14 | Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding | cs.RO | [PDF](http://arxiv.org/pdf/2501.08286v1){: .btn .btn-green } |

**Abstract**: VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework
designed for large scenes. The framework comprises four main components: VIO
Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO
Front End, RGB frames are processed through dense bundle adjustment and
uncertainty estimation to extract scene geometry and poses. Based on this
output, the mapping module incrementally constructs and maintains a 2D Gaussian
map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,
Score Manager, and Pose Refinement, which collectively improve mapping speed
and localization accuracy. This enables the SLAM system to handle large-scale
urban environments with up to 50 million Gaussian ellipsoids. To ensure global
consistency in large-scale scenes, we design a Loop Closure module, which
innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian
Splatting for loop closure detection and correction of the Gaussian map.
Additionally, we propose a Dynamic Eraser to address the inevitable presence of
dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor
and outdoor environments demonstrate that our approach achieves localization
performance on par with Visual-Inertial Odometry while surpassing recent
GS/NeRF SLAM methods. It also significantly outperforms all existing methods in
terms of mapping and rendering quality. Furthermore, we developed a mobile app
and verified that our framework can generate high-quality Gaussian maps in real
time using only a smartphone camera and a low-frequency IMU sensor. To the best
of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method
capable of operating in outdoor environments and supporting kilometer-scale
large scenes.



---

## 3D Gaussian Splatting with Normal Information for Mesh Extraction and  Improved Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-14 | Meenakshi Krishnan, Liam Fowl, Ramani Duraiswami | cs.GR | [PDF](http://arxiv.org/pdf/2501.08370v1){: .btn .btn-green } |

**Abstract**: Differentiable 3D Gaussian splatting has emerged as an efficient and flexible
rendering technique for representing complex scenes from a collection of 2D
views and enabling high-quality real-time novel-view synthesis. However, its
reliance on photometric losses can lead to imprecisely reconstructed geometry
and extracted meshes, especially in regions with high curvature or fine detail.
We propose a novel regularization method using the gradients of a signed
distance function estimated from the Gaussians, to improve the quality of
rendering while also extracting a surface mesh. The regularizing normal
supervision facilitates better rendering and mesh reconstruction, which is
crucial for downstream applications in video generation, animation, AR-VR and
gaming. We demonstrate the effectiveness of our approach on datasets such as
Mip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on
photorealism metrics compared to other mesh extracting rendering methods
without compromising mesh quality.

Comments:
- ICASSP 2025: Workshop on Generative Data Augmentation for Real-World
  Signal Processing Applications

---

## UnCommon Objects in 3D


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Y. Zhang, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny | cs.CV | [PDF](http://arxiv.org/pdf/2501.07574v1){: .btn .btn-green } |

**Abstract**: We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for
3D deep learning and 3D generative AI. uCO3D is the largest publicly-available
collection of high-resolution videos of objects with 3D annotations that
ensures full-360$^{\circ}$ coverage. uCO3D is significantly more diverse than
MVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of
higher quality, due to extensive quality checks of both the collected videos
and the 3D annotations. Similar to analogous datasets, uCO3D contains
annotations for 3D camera poses, depth maps and sparse point clouds. In
addition, each object is equipped with a caption and a 3D Gaussian Splat
reconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D
and obtain superior results using the latter, showing that uCO3D is better for
learning applications.



---

## RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular  Video Based on Rectified Mesh-embedded Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong | cs.CV | [PDF](http://arxiv.org/pdf/2501.07104v1){: .btn .btn-green } |

**Abstract**: We introduce RMAvatar, a novel human avatar representation with Gaussian
splatting embedded on mesh to learn clothed avatar from a monocular video. We
utilize the explicit mesh geometry to represent motion and shape of a virtual
human and implicit appearance rendering with Gaussian Splatting. Our method
consists of two main modules: Gaussian initialization module and Gaussian
rectification module. We embed Gaussians into triangular faces and control
their motion through the mesh, which ensures low-frequency motion and surface
deformation of the avatar. Due to the limitations of LBS formula, the human
skeleton is hard to control complex non-rigid transformations. We then design a
pose-related Gaussian rectification module to learn fine-detailed non-rigid
deformations, further improving the realism and expressiveness of the avatar.
We conduct extensive experiments on public datasets, RMAvatar shows
state-of-the-art performance on both rendering quality and quantitative
evaluations. Please see our project page at https://rm-avatar.github.io.

Comments:
- CVM2025

---

## 3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point  Cloud or Mesh

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Lewis A G Stuart, Michael P Pound | cs.GR | [PDF](http://arxiv.org/pdf/2501.07478v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D
reconstructions, but these scenes often require specialised renderers for
effective visualisation. In contrast, point clouds are a widely used 3D
representation and are compatible with most popular 3D processing software, yet
converting 3DGS scenes into point clouds is a complex challenge. In this work
we introduce 3DGS-to-PC, a flexible and highly customisable framework that is
capable of transforming 3DGS scenes into dense, high-accuracy point clouds. We
sample points probabilistically from each Gaussian as a 3D density function. We
additionally threshold new points using the Mahalanobis distance to the
Gaussian centre, preventing extreme outliers. The result is a point cloud that
closely represents the shape encoded into the 3D Gaussian scene. Individual
Gaussians use spherical harmonics to adapt colours depending on view, and each
point may contribute only subtle colour hints to the resulting rendered scene.
To avoid spurious or incorrect colours that do not fit with the final point
cloud, we recalculate Gaussian colours via a customised image rendering
approach, assigning each Gaussian the colour of the pixel to which it
contributes most across all views. 3DGS-to-PC also supports mesh generation
through Poisson Surface Reconstruction, applied to points sampled from
predicted surface Gaussians. This allows coloured meshes to be generated from
3DGS scenes without the need for re-training. This package is highly
customisable and capability of simple integration into existing 3DGS pipelines.
3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud
and surface-based formats.



---

## Evaluating Human Perception of Novel View Synthesis: Subjective Quality  Assessment of Gaussian Splatting and NeRF in Dynamic Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Yuhang Zhang, Joshua Maraval, Zhengyu Zhang, Nicolas Ramin, Shishun Tian, Lu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.08072v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) and Neural Radiance Fields (NeRF) are two
groundbreaking technologies that have revolutionized the field of Novel View
Synthesis (NVS), enabling immersive photorealistic rendering and user
experiences by synthesizing multiple viewpoints from a set of images of sparse
views. The potential applications of NVS, such as high-quality virtual and
augmented reality, detailed 3D modeling, and realistic medical organ imaging,
underscore the importance of quality assessment of NVS methods from the
perspective of human perception. Although some previous studies have explored
subjective quality assessments for NVS technology, they still face several
challenges, especially in NVS methods selection, scenario coverage, and
evaluation methodology. To address these challenges, we conducted two
subjective experiments for the quality assessment of NVS technologies
containing both GS-based and NeRF-based methods, focusing on dynamic and
real-world scenes. This study covers 360{\deg}, front-facing, and
single-viewpoint videos while providing a richer and greater number of real
scenes. Meanwhile, it's the first time to explore the impact of NVS methods in
dynamic scenes with moving objects. The two types of subjective experiments
help to fully comprehend the influences of different viewing paths from a human
perception perspective and pave the way for future development of
full-reference and no-reference quality metrics. In addition, we established a
comprehensive benchmark of various state-of-the-art objective metrics on the
proposed database, highlighting that existing methods still struggle to
accurately capture subjective quality. The results give us some insights into
the limitations of existing NVS methods and may promote the development of new
NVS methods.



---

## SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Yue Hu, Rong Liu, Meida Chen, Peter Beerel, Andrew Feng | cs.CV | [PDF](http://arxiv.org/pdf/2501.07015v2){: .btn .btn-green } |

**Abstract**: Achieving high-fidelity 3D reconstruction from monocular video remains
challenging due to the inherent limitations of traditional methods like
Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene
details. While differentiable rendering techniques such as Neural Radiance
Fields (NeRF) address some of these challenges, their high computational costs
make them unsuitable for real-time applications. Additionally, existing 3D
Gaussian Splatting (3DGS) methods often focus on photometric consistency,
neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and
pose updates for scene refinement. We propose a framework integrating dense
SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach
introduces SLAM-Informed Adaptive Densification, which dynamically updates and
densifies the Gaussian model by leveraging dense point clouds from SLAM.
Additionally, we incorporate Geometry-Guided Optimization, which combines
edge-aware geometric constraints and photometric consistency to jointly
optimize the appearance and geometry of the 3DGS scene representation, enabling
detailed and accurate SLAM mapping reconstruction. Experiments on the Replica
and TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving
state-of-the-art results among monocular systems. Specifically, our method
achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,
representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the
previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by
10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the
potential of our framework in bridging the gap between photometric and
geometric dense 3D scene representations, paving the way for practical and
efficient monocular dense reconstruction.



---

## CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Xinyi Zheng, Steve Zhang, Weizhe Lin, Aaron Zhang, Walterio W. Mayol-Cuevas, Junxiao Shen | cs.CV | [PDF](http://arxiv.org/pdf/2501.06927v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a large-scale fine-grained dataset using
high-resolution images captured from locations worldwide. Compared to existing
datasets, our dataset offers a significantly larger size and includes a higher
level of detail, making it uniquely suited for fine-grained 3D applications.
Notably, our dataset is built using drone-captured aerial imagery, which
provides a more accurate perspective for capturing real-world site layouts and
architectural structures. By reconstructing environments with these detailed
images, our dataset supports applications such as the COLMAP format for
Gaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible
with widely-used techniques including SLAM, Multi-View Stereo, and Neural
Radiance Fields (NeRF), enabling accurate 3D reconstructions and point clouds.
This makes it a benchmark for reconstruction and segmentation tasks. The
dataset enables seamless integration with multi-modal data, supporting a range
of 3D applications, from architectural reconstruction to virtual tourism. Its
flexibility promotes innovation, facilitating breakthroughs in 3D modeling and
analysis.



---

## Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale  Super-Resolution

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Du Chen, Liyi Chen, Zhengqiang Zhang, Lei Zhang | eess.IV | [PDF](http://arxiv.org/pdf/2501.06838v2){: .btn .btn-green } |

**Abstract**: Equipped with the continuous representation capability of Multi-Layer
Perceptron (MLP), Implicit Neural Representation (INR) has been successfully
employed for Arbitrary-scale Super-Resolution (ASR). However, the limited
receptive field of the linear layers in MLP restricts the representation
capability of INR, while it is computationally expensive to query the MLP
numerous times to render each pixel. Recently, Gaussian Splatting (GS) has
shown its advantages over INR in both visual quality and rendering speed in 3D
tasks, which motivates us to explore whether GS can be employed for the ASR
task. However, directly applying GS to ASR is exceptionally challenging because
the original GS is an optimization-based method through overfitting each single
scene, while in ASR we aim to learn a single model that can generalize to
different images and scaling factors. We overcome these challenges by
developing two novel techniques. Firstly, to generalize GS for ASR, we
elaborately design an architecture to predict the corresponding
image-conditioned Gaussians of the input low-resolution image in a feed-forward
manner. Secondly, we implement an efficient differentiable 2D GPU/CUDA-based
scale-aware rasterization to render super-resolved images by sampling discrete
RGB values from the predicted contiguous Gaussians. Via end-to-end training,
our optimized network, namely GSASR, can perform ASR for any image and unseen
scaling factors. Extensive experiments validate the effectiveness of our
proposed method. The project page can be found at
\url{https://mt-cly.github.io/GSASR.github.io/}.



---

## Synthetic Prior for Few-Shot Drivable Head Avatar Inversion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart | cs.CV | [PDF](http://arxiv.org/pdf/2501.06903v1){: .btn .btn-green } |

**Abstract**: We present SynShot, a novel method for the few-shot inversion of a drivable
head avatar based on a synthetic prior. We tackle two major challenges. First,
training a controllable 3D generative network requires a large number of
diverse sequences, for which pairs of images and high-quality tracked meshes
are not always available. Second, state-of-the-art monocular avatar models
struggle to generalize to new views and expressions, lacking a strong prior and
often overfitting to a specific viewpoint distribution. Inspired by machine
learning models trained solely on synthetic data, we propose a method that
learns a prior model from a large dataset of synthetic heads with diverse
identities, expressions, and viewpoints. With few input images, SynShot
fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a
photorealistic head avatar that generalizes to novel expressions and
viewpoints. We model the head avatar using 3D Gaussian splatting and a
convolutional encoder-decoder that outputs Gaussian parameters in UV texture
space. To account for the different modeling complexities over parts of the
head (e.g., skin vs hair), we embed the prior with explicit control for
upsampling the number of per-part primitives. Compared to state-of-the-art
monocular methods that require thousands of real training images, SynShot
significantly improves novel view and expression synthesis.

Comments:
- Website https://zielon.github.io/synshot/

---

## ActiveGAMER: Active GAussian Mapping through Efficient Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Liyan Chen, Huangying Zhan, Kevin Chen, Xiangyu Xu, Qingan Yan, Changjiang Cai, Yi Xu | cs.CV | [PDF](http://arxiv.org/pdf/2501.06897v1){: .btn .btn-green } |

**Abstract**: We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian
Splatting (3DGS) to achieve high-quality, real-time scene mapping and
exploration. Unlike traditional NeRF-based methods, which are computationally
demanding and restrict active mapping performance, our approach leverages the
efficient rendering capabilities of 3DGS, allowing effective and efficient
exploration in complex environments. The core of our system is a
rendering-based information gain module that dynamically identifies the most
informative viewpoints for next-best-view planning, enhancing both geometric
and photometric reconstruction accuracy. ActiveGAMER also integrates a
carefully balanced framework, combining coarse-to-fine exploration,
post-refinement, and a global-local keyframe selection strategy to maximize
reconstruction completeness and fidelity. Our system autonomously explores and
reconstructs environments with state-of-the-art geometric and photometric
accuracy and completeness, significantly surpassing existing approaches in both
aspects. Extensive evaluations on benchmark datasets such as Replica and MP3D
highlight ActiveGAMER's effectiveness in active mapping tasks.



---

## F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with  Cycle-Consistent Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Yuxin Wang, Qianyi Wu, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2501.06714v1){: .btn .btn-green } |

**Abstract**: This paper tackles the problem of generalizable 3D-aware generation from
monocular datasets, e.g., ImageNet. The key challenge of this task is learning
a robust 3D-aware representation without multi-view or dynamic data, while
ensuring consistent texture and geometry across different viewpoints. Although
some baseline methods are capable of 3D-aware generation, the quality of the
generated images still lags behind state-of-the-art 2D generation approaches,
which excel in producing high-quality, detailed images. To address this severe
limitation, we propose a novel feed-forward pipeline based on pixel-aligned
Gaussian Splatting, coined as F3D-Gaus, which can produce more realistic and
reliable 3D renderings from monocular inputs. In addition, we introduce a
self-supervised cycle-consistent constraint to enforce cross-view consistency
in the learned 3D representation. This training strategy naturally allows
aggregation of multiple aligned Gaussian primitives and significantly
alleviates the interpolation limitations inherent in single-view pixel-aligned
Gaussian Splatting. Furthermore, we incorporate video model priors to perform
geometry-aware refinement, enhancing the generation of fine details in
wide-viewpoint scenarios and improving the model's capability to capture
intricate 3D textures. Extensive experiments demonstrate that our approach not
only achieves high-quality, multi-view consistent 3D-aware generation from
monocular datasets, but also significantly improves training and inference
efficiency.

Comments:
- Project Page: https://w-ted.github.io/publications/F3D-Gaus

---

## SuperNeRF-GAN: A Universal 3D-Consistent Super-Resolution Framework for  Efficient and Enhanced 3D-Aware Image Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Peng Zheng, Linzhi Huang, Yizhou Yu, Yi Chang, Yilin Wang, Rui Ma | cs.CV | [PDF](http://arxiv.org/pdf/2501.06770v2){: .btn .btn-green } |

**Abstract**: Neural volume rendering techniques, such as NeRF, have revolutionized
3D-aware image synthesis by enabling the generation of images of a single scene
or object from various camera poses. However, the high computational cost of
NeRF presents challenges for synthesizing high-resolution (HR) images. Most
existing methods address this issue by leveraging 2D super-resolution, which
compromise 3D-consistency. Other methods propose radiance manifolds or
two-stage generation to achieve 3D-consistent HR synthesis, yet they are
limited to specific synthesis tasks, reducing their universality. To tackle
these challenges, we propose SuperNeRF-GAN, a universal framework for
3D-consistent super-resolution. A key highlight of SuperNeRF-GAN is its
seamless integration with NeRF-based 3D-aware image synthesis methods and it
can simultaneously enhance the resolution of generated images while preserving
3D-consistency and reducing computational cost. Specifically, given a
pre-trained generator capable of producing a NeRF representation such as
tri-plane, we first perform volume rendering to obtain a low-resolution image
with corresponding depth and normal map. Then, we employ a NeRF
Super-Resolution module which learns a network to obtain a high-resolution
NeRF. Next, we propose a novel Depth-Guided Rendering process which contains
three simple yet effective steps, including the construction of a
boundary-correct multi-depth map through depth aggregation, a normal-guided
depth super-resolution and a depth-guided NeRF rendering. Experimental results
demonstrate the superior efficiency, 3D-consistency, and quality of our
approach. Additionally, ablation studies confirm the effectiveness of our
proposed components.



---

## NVS-SQA: Exploring Self-Supervised Quality Representation Learning for  Neurally Synthesized Scenes without References

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-11 | Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Weidong Cai, Tongliang Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.06488v1){: .btn .btn-green } |

**Abstract**: Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,
effectively creates photorealistic scenes from sparse viewpoints, typically
evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,
these full-reference methods, which compare synthesized views to reference
views, may not fully capture the perceptual quality of neurally synthesized
scenes (NSS), particularly due to the limited availability of dense reference
views. Furthermore, the challenges in acquiring human perceptual labels hinder
the creation of extensive labeled datasets, risking model overfitting and
reduced generalizability. To address these issues, we propose NVS-SQA, a NSS
quality assessment method to learn no-reference quality representations through
self-supervision without reliance on human labels. Traditional self-supervised
learning predominantly relies on the "same instance, similar representation"
assumption and extensive datasets. However, given that these conditions do not
apply in NSS quality assessment, we employ heuristic cues and quality scores as
learning objectives, along with a specialized contrastive pair preparation
process to improve the effectiveness and efficiency of learning. The results
show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,
on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second
best) and even exceeds 16 full-reference methods across all evaluation metrics
(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).



---

## MapGS: Generalizable Pretraining and Data Augmentation for Online  Mapping via Novel View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-11 | Hengyuan Zhang, David Paz, Yuliang Guo, Xinyu Huang, Henrik I. Christensen, Liu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2501.06660v1){: .btn .btn-green } |

**Abstract**: Online mapping reduces the reliance of autonomous vehicles on high-definition
(HD) maps, significantly enhancing scalability. However, recent advancements
often overlook cross-sensor configuration generalization, leading to
performance degradation when models are deployed on vehicles with different
camera intrinsics and extrinsics. With the rapid evolution of novel view
synthesis methods, we investigate the extent to which these techniques can be
leveraged to address the sensor configuration generalization challenge. We
propose a novel framework leveraging Gaussian splatting to reconstruct scenes
and render camera images in target sensor configurations. The target config
sensor data, along with labels mapped to the target config, are used to train
online mapping models. Our proposed framework on the nuScenes and Argoverse 2
datasets demonstrates a performance improvement of 18% through effective
dataset augmentation, achieves faster convergence and efficient training, and
exceeds state-of-the-art performance when using only 25% of the original
training data. This enables data reuse and reduces the need for laborious data
labeling. Project page at https://henryzhangzhy.github.io/mapgs.



---

## UV-Attack: Physical-World Adversarial Attacks for Person Detection via  Dynamic-NeRF-based UV Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-10 | Yanjie Li, Wenxuan Zhang, Kaisheng Liang, Bin Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2501.05783v1){: .btn .btn-green } |

**Abstract**: In recent research, adversarial attacks on person detectors using patches or
static 3D model-based texture modifications have struggled with low success
rates due to the flexible nature of human movement. Modeling the 3D
deformations caused by various actions has been a major challenge. Fortunately,
advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer
new possibilities. In this paper, we introduce UV-Attack, a groundbreaking
approach that achieves high success rates even with extensive and unseen human
actions. We address the challenge above by leveraging dynamic-NeRF-based UV
mapping. UV-Attack can generate human images across diverse actions and
viewpoints, and even create novel actions by sampling from the SMPL parameter
space. While dynamic NeRF models are capable of modeling human bodies,
modifying clothing textures is challenging because they are embedded in neural
network parameters. To tackle this, UV-Attack generates UV maps instead of RGB
images and modifies the texture stacks. This approach enables real-time texture
edits and makes the attack more practical. We also propose a novel Expectation
over Pose Transformation loss (EoPT) to improve the evasion success rate on
unseen poses and views. Our experiments show that UV-Attack achieves a 92.75%
attack success rate against the FastRCNN model across varied poses in dynamic
video settings, significantly outperforming the state-of-the-art AdvCamou
attack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the
latest YOLOv8 detector in black-box settings. This work highlights the
potential of dynamic NeRF-based UV mapping for creating more effective
adversarial attacks on person detectors, addressing key challenges in modeling
human movement and texture modification.

Comments:
- 23 pages, 22 figures, submitted to ICLR2025

---

## Locality-aware Gaussian Compression for Fast and High-quality Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-10 | Seungjoo Shin, Jaesik Park, Sunghyun Cho | cs.CV | [PDF](http://arxiv.org/pdf/2501.05757v1){: .btn .btn-green } |

**Abstract**: We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework
that exploits the spatial coherence of 3D Gaussians for compact modeling of
volumetric scenes. To this end, we first analyze the local coherence of 3D
Gaussian attributes, and propose a novel locality-aware 3D Gaussian
representation that effectively encodes locally-coherent Gaussian attributes
using a neural field representation with a minimal storage requirement. On top
of the novel representation, LocoGS is carefully designed with additional
components such as dense initialization, an adaptive spherical harmonics
bandwidth scheme and different encoding schemes for different Gaussian
attributes to maximize compression performance. Experimental results
demonstrate that our approach outperforms the rendering quality of existing
compact Gaussian representations for representative real-world 3D datasets
while achieving from 54.6$\times$ to 96.6$\times$ compressed storage size and
from 2.1$\times$ to 2.4$\times$ rendering speed than 3DGS. Even our approach
also demonstrates an averaged 2.4$\times$ higher rendering speed than the
state-of-the-art compression method with comparable compression performance.

Comments:
- 28 pages, 15 figures, and 14 tables

---

## Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and  Photorealistic Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun | cs.CV | [PDF](http://arxiv.org/pdf/2501.05242v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis
in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM
methods utilizing 3DGS have failed to provide high-quality novel view rendering
for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods
perform well for RGB-D cameras but suffer significant degradation in rendering
quality for monocular cameras. In this paper, we present Scaffold-SLAM, which
delivers simultaneous localization and high-quality photorealistic mapping
across monocular, stereo, and RGB-D cameras. We introduce two key innovations
to achieve this state-of-the-art visual quality. First, we propose
Appearance-from-Motion embedding, enabling 3D Gaussians to better model image
appearance variations across different camera poses. Second, we introduce a
frequency regularization pyramid to guide the distribution of Gaussians,
allowing the model to effectively capture finer details in the scene. Extensive
experiments on monocular, stereo, and RGB-D datasets demonstrate that
Scaffold-SLAM significantly outperforms state-of-the-art methods in
photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D
datasets for monocular cameras.

Comments:
- 12 pages, 6 figures

---

## Light Transport-aware Diffusion Posterior Sampling for Single-View  Reconstruction of 3D Volumes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Ludwic Leonard, Nils Thuerey, Ruediger Westermann | cs.CV | [PDF](http://arxiv.org/pdf/2501.05226v2){: .btn .btn-green } |

**Abstract**: We introduce a single-view reconstruction technique of volumetric fields in
which multiple light scattering effects are omnipresent, such as in clouds. We
model the unknown distribution of volumetric fields using an unconditional
diffusion model trained on a novel benchmark dataset comprising 1,000
synthetically simulated volumetric density fields. The neural diffusion model
is trained on the latent codes of a novel, diffusion-friendly, monoplanar
representation. The generative model is used to incorporate a tailored
parametric diffusion posterior sampling technique into different reconstruction
tasks. A physically-based differentiable volume renderer is employed to provide
gradients with respect to light transport in the latent space. This stands in
contrast to classic NeRF approaches and makes the reconstructions better
aligned with observed data. Through various experiments, we demonstrate
single-view reconstruction of volumetric clouds at a previously unattainable
quality.



---

## Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D  Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, Lingjie Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.05427v1){: .btn .btn-green } |

**Abstract**: Recent advances in 2D image generation have achieved remarkable
quality,largely driven by the capacity of diffusion models and the availability
of large-scale datasets. However, direct 3D generation is still constrained by
the scarcity and lower fidelity of 3D datasets. In this paper, we introduce
Zero-1-to-G, a novel approach that addresses this problem by enabling direct
single-view generation on Gaussian splats using pretrained 2D diffusion models.
Our key insight is that Gaussian splats, a 3D representation, can be decomposed
into multi-view images encoding different attributes. This reframes the
challenging task of direct 3D generation within a 2D diffusion framework,
allowing us to leverage the rich priors of pretrained 2D diffusion models. To
incorporate 3D awareness, we introduce cross-view and cross-attribute attention
layers, which capture complex correlations and enforce 3D consistency across
generated splats. This makes Zero-1-to-G the first direct image-to-3D
generative model to effectively utilize pretrained 2D diffusion priors,
enabling efficient training and improved generalization to unseen objects.
Extensive experiments on both synthetic and in-the-wild datasets demonstrate
superior performance in 3D object generation, offering a new approach to
high-quality 3D generation.



---

## Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID  Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou | cs.CV | [PDF](http://arxiv.org/pdf/2501.05379v2){: .btn .btn-green } |

**Abstract**: Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in
reconstructing detailed 3D scenes within multi-view setups and the emergence of
large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based
method utilizing a human face foundation model as guidance with just a single
image as input. To achieve that, we extend such a model for diverse-view human
head generation by fine-tuning on synthetic data and modifying its
conditioning. Our avatars maintain a dense correspondence with a human face
mesh template, allowing blendshape-based expression generation. This is
achieved through a modified 3DGS approach, connectivity regularizers, and a
strategic initialization tailored for our task. Additionally, we propose an
optional efficient SDS-based correction step to refine the blendshape
expressions, enhancing realism and diversity. Experiments demonstrate that
Arc2Avatar achieves state-of-the-art realism and identity preservation,
effectively addressing color issues by allowing the use of very low guidance,
enabled by our strong identity prior and initialization strategy, without
compromising detail. Please visit https://arc2avatar.github.io for more
resources.

Comments:
- Project Page https://arc2avatar.github.io

---

## GaussianVideo: Efficient Video Representation via Hierarchical Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-08 | Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem | cs.CV | [PDF](http://arxiv.org/pdf/2501.04782v1){: .btn .btn-green } |

**Abstract**: Efficient neural representations for dynamic video scenes are critical for
applications ranging from video compression to interactive simulations. Yet,
existing methods often face challenges related to high memory usage, lengthy
training times, and temporal consistency. To address these issues, we introduce
a novel neural video representation that combines 3D Gaussian splatting with
continuous camera motion modeling. By leveraging Neural ODEs, our approach
learns smooth camera trajectories while maintaining an explicit 3D scene
representation through Gaussians. Additionally, we introduce a spatiotemporal
hierarchical learning strategy, progressively refining spatial and temporal
features to enhance reconstruction quality and accelerate convergence. This
memory-efficient approach achieves high-quality rendering at impressive speeds.
Experimental results show that our hierarchical learning, combined with robust
camera motion modeling, captures complex dynamic scenes with strong temporal
consistency, achieving state-of-the-art performance across diverse video
datasets in both high- and low-motion scenarios.

Comments:
- 10 pages, 10 figures

---

## FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using  Gaussian Splatting with Depth-Feature Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-08 | Han Huang, Yulun Wu, Chao Deng, Ge Gao, Ming Gu, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.04628v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian Splatting has sparked a new trend in the field of computer
vision. Apart from novel view synthesis, it has also been extended to the area
of multi-view reconstruction. The latest methods facilitate complete, detailed
surface reconstruction while ensuring fast training speed. However, these
methods still require dense input views, and their output quality significantly
degrades with sparse views. We observed that the Gaussian primitives tend to
overfit the few training views, leading to noisy floaters and incomplete
reconstruction surfaces. In this paper, we present an innovative sparse-view
reconstruction framework that leverages intra-view depth and multi-view feature
consistency to achieve remarkably accurate surface reconstruction.
Specifically, we utilize monocular depth ranking information to supervise the
consistency of depth distribution within patches and employ a smoothness loss
to enhance the continuity of the distribution. To achieve finer surface
reconstruction, we optimize the absolute position of depth through multi-view
projection features. Extensive experiments on DTU and BlendedMVS demonstrate
that our method outperforms state-of-the-art methods with a speedup of 60x to
200x, achieving swift and fine-grained mesh reconstruction without the need for
costly pre-training.

Comments:
- Accepted by AAAI 2025. Project page:
  https://alvin528.github.io/FatesGS/

---

## NeuralSVG: An Implicit Representation for Text-to-Vector Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Sagi Polaczek, Yuval Alaluf, Elad Richardson, Yael Vinker, Daniel Cohen-Or | cs.CV | [PDF](http://arxiv.org/pdf/2501.03992v1){: .btn .btn-green } |

**Abstract**: Vector graphics are essential in design, providing artists with a versatile
medium for creating resolution-independent and highly editable visual content.
Recent advancements in vision-language and diffusion models have fueled
interest in text-to-vector graphics generation. However, existing approaches
often suffer from over-parameterized outputs or treat the layered structure - a
core feature of vector graphics - as a secondary goal, diminishing their
practical use. Recognizing the importance of layered SVG representations, we
propose NeuralSVG, an implicit neural representation for generating vector
graphics from text prompts. Inspired by Neural Radiance Fields (NeRFs),
NeuralSVG encodes the entire scene into the weights of a small MLP network,
optimized using Score Distillation Sampling (SDS). To encourage a layered
structure in the generated SVG, we introduce a dropout-based regularization
technique that strengthens the standalone meaning of each shape. We
additionally demonstrate that utilizing a neural representation provides an
added benefit of inference-time control, enabling users to dynamically adapt
the generated SVG based on user-provided inputs, all with a single learned
representation. Through extensive qualitative and quantitative evaluations, we
demonstrate that NeuralSVG outperforms existing methods in generating
structured and flexible SVG.

Comments:
- Project Page: https://sagipolaczek.github.io/NeuralSVG/

---

## NeRFs are Mirror Detectors: Using Structural Similarity for Multi-View  Mirror Scene Reconstruction with 3D Surface Primitives

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Leif Van Holland, Michael Weinmann, Jan U. Müller, Patrick Stotko, Reinhard Klein | cs.CV | [PDF](http://arxiv.org/pdf/2501.04074v1){: .btn .btn-green } |

**Abstract**: While neural radiance fields (NeRF) led to a breakthrough in photorealistic
novel view synthesis, handling mirroring surfaces still denotes a particular
challenge as they introduce severe inconsistencies in the scene representation.
Previous attempts either focus on reconstructing single reflective objects or
rely on strong supervision guidance in terms of additional user-provided
annotations of visible image regions of the mirrors, thereby limiting the
practical usability. In contrast, in this paper, we present NeRF-MD, a method
which shows that NeRFs can be considered as mirror detectors and which is
capable of reconstructing neural radiance fields of scenes containing mirroring
surfaces without the need for prior annotations. To this end, we first compute
an initial estimate of the scene geometry by training a standard NeRF using a
depth reprojection loss. Our key insight lies in the fact that parts of the
scene corresponding to a mirroring surface will still exhibit a significant
photometric inconsistency, whereas the remaining parts are already
reconstructed in a plausible manner. This allows us to detect mirror surfaces
by fitting geometric primitives to such inconsistent regions in this initial
stage of the training. Using this information, we then jointly optimize the
radiance field and mirror geometry in a second training stage to refine their
quality. We demonstrate the capability of our method to allow the faithful
detection of mirrors in the scene as well as the reconstruction of a single
consistent scene representation, and demonstrate its potential in comparison to
baseline and mirror-aware approaches.



---

## DehazeGS: Seeing Through Fog with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.03659v2){: .btn .btn-green } |

**Abstract**: Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and rendering quality. Although NeRF-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, NeRF's implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward rendering process. We introduce
DehazeGS, a method capable of decomposing and rendering a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy datasets demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both rendering
quality and computational efficiency.

Comments:
- 9 pages,4 figures

---

## ConcealGS: Concealing Invisible Copyright Information in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Yifeng Yang, Hengyu Liu, Chenxin Li, Yining Sun, Wuyang Li, Yifan Liu, Yiyang Lin, Yixuan Yuan, Nanyang Ye | cs.CV | [PDF](http://arxiv.org/pdf/2501.03605v1){: .btn .btn-green } |

**Abstract**: With the rapid development of 3D reconstruction technology, the widespread
distribution of 3D data has become a future trend. While traditional visual
data (such as images and videos) and NeRF-based formats already have mature
techniques for copyright protection, steganographic techniques for the emerging
3D Gaussian Splatting (3D-GS) format have yet to be fully explored. To address
this, we propose ConcealGS, an innovative method for embedding implicit
information into 3D-GS. By introducing the knowledge distillation and gradient
optimization strategy based on 3D-GS, ConcealGS overcomes the limitations of
NeRF-based models and enhances the robustness of implicit information and the
quality of 3D reconstruction. We evaluate ConcealGS in various potential
application scenarios, and experimental results have demonstrated that
ConcealGS not only successfully recovers implicit information but also has
almost no impact on rendering quality, providing a new approach for embedding
invisible and recoverable information into 3D models in the future.



---

## MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval  Adjustment for Compact Dynamic 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, Munchurl Kim | cs.CV | [PDF](http://arxiv.org/pdf/2501.03714v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has made significant strides in scene
representation and neural rendering, with intense efforts focused on adapting
it for dynamic scenes. Despite delivering remarkable rendering quality and
speed, existing methods struggle with storage demands and representing complex
real-world motions. To tackle these issues, we propose MoDecGS, a
memory-efficient Gaussian splatting framework designed for reconstructing novel
views in challenging scenarios with complex motions. We introduce GlobaltoLocal
Motion Decomposition (GLMD) to effectively capture dynamic motions in a
coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global
CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold
representation to dynamic video reconstruction. For Global CS, we propose
Global Anchor Deformation (GAD) to efficiently represent global dynamics along
complex motions, by directly deforming the implicit Scaffold attributes which
are anchor position, offset, and local context features. Next, we finely adjust
local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly.
Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically
control the temporal coverage of each Local CS during training, allowing
MoDecGS to find optimal interval assignments based on the specified number of
temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an
average 70% reduction in model size over stateoftheart methods for dynamic 3D
Gaussians from realworld dynamic videos while maintaining or even improving
rendering quality.

Comments:
- The last two authors are co-corresponding authors. Please visit our
  project page at https://kaist-viclab.github.io/MoDecGS-site/

---

## ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Abhishek Saroha, Florian Hofherr, Mariia Gladkova, Cecilia Curreli, Or Litany, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2501.03875v1){: .btn .btn-green } |

**Abstract**: Stylizing a dynamic scene based on an exemplar image is critical for various
real-world applications, including gaming, filmmaking, and augmented and
virtual reality. However, achieving consistent stylization across both spatial
and temporal dimensions remains a significant challenge. Most existing methods
are designed for static scenes and often require an optimization process for
each style image, limiting their adaptability. We introduce ZDySS, a zero-shot
stylization framework for dynamic scenes, allowing our model to generalize to
previously unseen style images at inference. Our approach employs Gaussian
splatting for scene representation, linking each Gaussian to a learned feature
vector that renders a feature map for any given view and timestamp. By applying
style transfer on the learned feature vectors instead of the rendered feature
map, we enhance spatio-temporal consistency across frames. Our method
demonstrates superior performance and coherence over state-of-the-art baselines
in tests on real-world dynamic scenes, making it a robust solution for
practical applications.



---

## HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian  Splatting Based Data Augmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Wentian Qu, Jiahe Li, Jian Cheng, Jian Shi, Chenyu Meng, Cuixia Ma, Hongan Wang, Xiaoming Deng, Yinda Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.02845v1){: .btn .btn-green } |

**Abstract**: Understanding of bimanual hand-object interaction plays an important role in
robotics and virtual reality. However, due to significant occlusions between
hands and object as well as the high degree-of-freedom motions, it is
challenging to collect and annotate a high-quality, large-scale dataset, which
prevents further improvement of bimanual hand-object interaction-related
baselines. In this work, we propose a new 3D Gaussian Splatting based data
augmentation framework for bimanual hand-object interaction, which is capable
of augmenting existing dataset to large-scale photorealistic data with various
hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects
and hands, and to deal with the rendering blur problem due to multi-resolution
input images used, we design a super-resolution module. Second, we extend the
single hand grasping pose optimization module for the bimanual hand object to
generate various poses of bimanual hand-object interaction, which can
significantly expand the pose distribution of the dataset. Third, we conduct an
analysis for the impact of different aspects of the proposed data augmentation
on the understanding of the bimanual hand-object interaction. We perform our
data augmentation on two benchmarks, H2O and Arctic, and verify that our method
can improve the performance of the baselines.

Comments:
- Accepted by AAAI2025

---

## AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal  Conditions and Larger Scene

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Chaoran Feng, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuan, Yonghong Tian | cs.CV | [PDF](http://arxiv.org/pdf/2501.02807v2){: .btn .btn-green } |

**Abstract**: Compared to frame-based methods, computational neuromorphic imaging using
event cameras offers significant advantages, such as minimal motion blur,
enhanced temporal resolution, and high dynamic range. The multi-view
consistency of Neural Radiance Fields combined with the unique benefits of
event cameras, has spurred recent research into reconstructing NeRF from data
captured by moving event cameras. While showing impressive performance,
existing methods rely on ideal conditions with the availability of uniform and
high-quality event sequences and accurate camera poses, and mainly focus on the
object level reconstruction, thus limiting their practical applications. In
this work, we propose AE-NeRF to address the challenges of learning event-based
NeRF from non-ideal conditions, including non-uniform event sequences, noisy
poses, and various scales of scenes. Our method exploits the density of event
streams and jointly learn a pose correction module with an event-based NeRF
(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.
To generalize to larger scenes, we propose hierarchical event distillation with
a proposal e-NeRF network and a vanilla e-NeRF network to resample and refine
the reconstruction process. We further propose an event reconstruction loss and
a temporal loss to improve the view consistency of the reconstructed scene. We
established a comprehensive benchmark that includes large-scale scenes to
simulate practical non-ideal conditions, incorporating both synthetic and
challenging real-world event datasets. The experimental results show that our
method achieves a new state-of-the-art in event-based 3D reconstruction.



---

## Gaussian Masked Autoencoders

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, Shiry Ginosar | cs.CV | [PDF](http://arxiv.org/pdf/2501.03229v1){: .btn .btn-green } |

**Abstract**: This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While
reconstructive self-supervised learning frameworks such as MAE learns good
semantic abstractions, it is not trained for explicit spatial awareness. Our
approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic
abstractions and spatial understanding jointly. Like MAE, it reconstructs the
image end-to-end in the pixel space, but beyond MAE, it also introduces an
intermediate, 3D Gaussian-based representation and renders images via
splatting. We show that GMAE can enable various zero-shot learning capabilities
of spatial understanding (e.g., figure-ground segmentation, image layering,
edge detection, etc.) while preserving the high-level semantics of
self-supervised representation quality from MAE. To our knowledge, we are the
first to employ Gaussian primitives in an image representation learning
framework beyond optimization-based single-scene reconstructions. We believe
GMAE will inspire further research in this direction and contribute to
developing next-generation techniques for modeling high-fidelity visual data.
More details at https://brjathu.github.io/gmae



---

## Compression of 3D Gaussian Splatting with Optimized Feature Planes and  Standard Video Codecs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge | cs.CV | [PDF](http://arxiv.org/pdf/2501.03399v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting is a recognized method for 3D scene representation,
known for its high rendering quality and speed. However, its substantial data
requirements present challenges for practical applications. In this paper, we
introduce an efficient compression technique that significantly reduces storage
overhead by using compact representation. We propose a unified architecture
that combines point cloud data and feature planes through a progressive
tri-plane structure. Our method utilizes 2D feature planes, enabling continuous
spatial representation. To further optimize these representations, we
incorporate entropy modeling in the frequency domain, specifically designed for
standard video codecs. We also propose channel-wise bit allocation to achieve a
better trade-off between bitrate consumption and feature plane representation.
Consequently, our model effectively leverages spatial correlations within the
feature planes to enhance rate-distortion performance using standard,
non-differentiable video codecs. Experimental results demonstrate that our
method outperforms existing methods in data compactness while maintaining high
rendering quality. Our project page is available at
https://fraunhoferhhi.github.io/CodecGS



---

## GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields  through Efficient Dense 3D Point Tracking

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-05 | Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2501.02690v1){: .btn .btn-green } |

**Abstract**: 4D video control is essential in video generation as it enables the use of
sophisticated lens techniques, such as multi-camera shooting and dolly zoom,
which are currently unsupported by existing methods. Training a video Diffusion
Transformer (DiT) directly to control 4D content requires expensive multi-view
videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that
optimizes a 4D representation and renders videos according to different 4D
elements, such as camera pose and object motion editing, we bring pseudo 4D
Gaussian fields to video generation. Specifically, we propose a novel framework
that constructs a pseudo 4D Gaussian field with dense 3D point tracking and
renders the Gaussian field for all video frames. Then we finetune a pretrained
DiT to generate videos following the guidance of the rendered video, dubbed as
GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense
3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field
construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art
sparse 3D point tracking method, in accuracy and accelerates the inference
speed by two orders of magnitude. During the inference stage, GS-DiT can
generate videos with the same dynamic content while adhering to different
camera parameters, addressing a significant limitation of current video
generation models. GS-DiT demonstrates strong generalization capabilities and
extends the 4D controllability of Gaussian splatting to video generation beyond
just camera poses. It supports advanced cinematic effects through the
manipulation of the Gaussian field and camera intrinsics, making it a powerful
tool for creative video production. Demos are available at
https://wkbian.github.io/Projects/GS-DiT/.

Comments:
- Project Page: https://wkbian.github.io/Projects/GS-DiT/

---

## CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Chenhao Zhang, Yuanping Cao, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01695v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene
representation and reconstruction, leveraging densely distributed Gaussian
primitives to enable real-time rendering of high-resolution images. While
existing 3DGS methods perform well in scenes with minor view variation, large
view changes in cross-view scenes pose optimization challenges for these
methods. To address these issues, we propose a novel cross-view Gaussian
Splatting method for large-scale scene reconstruction, based on dual-branch
fusion. Our method independently reconstructs models from aerial and ground
views as two independent branches to establish the baselines of Gaussian
distribution, providing reliable priors for cross-view reconstruction during
both initialization and densification. Specifically, a gradient-aware
regularization strategy is introduced to mitigate smoothing issues caused by
significant view disparities. Additionally, a unique Gaussian supplementation
strategy is utilized to incorporate complementary information of dual-branch
into the cross-view model. Extensive experiments on benchmark datasets
demonstrate that our method achieves superior performance in novel view
synthesis compared to state-of-the-art methods.



---

## Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Alberta Longhini, Marcel Büsching, Bardienus P. Duisterhof, Jens Lundell, Jeffrey Ichnowski, Mårten Björkman, Danica Kragic | cs.CV | [PDF](http://arxiv.org/pdf/2501.01715v1){: .btn .btn-green } |

**Abstract**: We introduce Cloth-Splatting, a method for estimating 3D states of cloth from
RGB images through a prediction-update framework. Cloth-Splatting leverages an
action-conditioned dynamics model for predicting future states and uses 3D
Gaussian Splatting to update the predicted states. Our key insight is that
coupling a 3D mesh-based representation with Gaussian Splatting allows us to
define a differentiable map between the cloth state space and the image space.
This enables the use of gradient-based optimization techniques to refine
inaccurate state estimates using only RGB supervision. Our experiments
demonstrate that Cloth-Splatting not only improves state estimation accuracy
over current baselines but also reduces convergence time.

Comments:
- Accepted at the 8th Conference on Robot Learning (CoRL 2024). Code
  and videos available at: kth-rpl.github.io/cloth-splatting

---

## EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren | cs.RO | [PDF](http://arxiv.org/pdf/2501.01895v1){: .btn .btn-green } |

**Abstract**: We introduce EnerVerse, a comprehensive framework for embodied future space
generation specifically designed for robotic manipulation tasks. EnerVerse
seamlessly integrates convolutional and bidirectional attention mechanisms for
inner-chunk space modeling, ensuring low-level consistency and continuity.
Recognizing the inherent redundancy in video data, we propose a sparse memory
context combined with a chunkwise unidirectional generative paradigm to enable
the generation of infinitely long sequences. To further augment robotic
capabilities, we introduce the Free Anchor View (FAV) space, which provides
flexible perspectives to enhance observation and analysis. The FAV space
mitigates motion modeling ambiguity, removes physical constraints in confined
environments, and significantly improves the robot's generalization and
adaptability across various tasks and settings. To address the prohibitive
costs and labor intensity of acquiring multi-camera observations, we present a
data engine pipeline that integrates a generative model with 4D Gaussian
Splatting (4DGS). This pipeline leverages the generative model's robust
generalization capabilities and the spatial constraints provided by 4DGS,
enabling an iterative enhancement of data quality and diversity, thus creating
a data flywheel effect that effectively narrows the sim-to-real gap. Finally,
our experiments demonstrate that the embodied future space generation prior
substantially enhances policy predictive capabilities, resulting in improved
overall performance, particularly in long-range robotic manipulation tasks.

Comments:
- Website: https://sites.google.com/view/enerverse

---

## PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban  Buildings Reconstruction via Semantic-Aware Grouping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Tengfei Wang, Xin Wang, Yongmao Hou, Yiwei Xu, Wendi Zhang, Zongqian Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2501.01677v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a transformative method in the
field of real-time novel synthesis. Based on 3DGS, recent advancements cope
with large-scale scenes via spatial-based partition strategy to reduce video
memory and optimization time costs. In this work, we introduce a parallel
Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues
for both partitioning and Gaussian kernel optimization, enabling fine-grained
building surface reconstruction of large-scale urban areas without downsampling
the original image resolution. First, the Cross-modal model - Language Segment
Anything is leveraged to segment building masks. Then, the segmented building
regions is grouped into sub-regions according to the visibility check across
registered images. The Gaussian kernels for these sub-regions are optimized in
parallel with masked pixels. In addition, the normal loss is re-formulated for
the detected edges of masks to alleviate the ambiguities in normal vectors on
edges. Finally, to improve the optimization of 3D Gaussians, we introduce a
gradient-constrained balance-load loss that accounts for the complexity of the
corresponding scenes, effectively minimizing the thread waiting time in the
pixel-parallel rendering stage as well as the reconstruction lost. Extensive
experiments are tested on various urban datasets, the results demonstrated the
superior performance of our PG-SAG on building surface reconstruction, compared
to several state-of-the-art 3DGS-based methods. Project
Web:https://github.com/TFWang-9527/PG-SAG.



---

## EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-02 | Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01003v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene
representation. Despite their impressive performance, they confront challenges
due to the limitation of structure-from-motion (SfM) methods on acquiring
accurate scene initialization, or the inefficiency of densification strategy.
In this paper, we introduce a novel framework EasySplat to achieve high-quality
3DGS modeling. Instead of using SfM for scene initialization, we employ a novel
method to release the power of large-scale pointmap approaches. Specifically,
we propose an efficient grouping strategy based on view similarity, and use
robust pointmap priors to obtain high-quality point clouds and camera poses for
3D scene initialization. After obtaining a reliable scene structure, we propose
a novel densification approach that adaptively splits Gaussian primitives based
on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme.
In this way, the proposed method tackles the limitation on initialization and
optimization, leading to an efficient and accurate 3DGS modeling. Extensive
experiments demonstrate that EasySplat outperforms the current state-of-the-art
(SOTA) in handling novel view synthesis.

Comments:
- 6 pages, 5figures

---

## Deformable Gaussian Splatting for Efficient and High-Fidelity  Reconstruction of Surgical Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-02 | Jiwei Shan, Zeyu Cai, Cheng-Tai Hsieh, Shing Shin Cheng, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01101v1){: .btn .btn-green } |

**Abstract**: Efficient and high-fidelity reconstruction of deformable surgical scenes is a
critical yet challenging task. Building on recent advancements in 3D Gaussian
splatting, current methods have seen significant improvements in both
reconstruction quality and rendering speed. However, two major limitations
remain: (1) difficulty in handling irreversible dynamic changes, such as tissue
shearing, which are common in surgical scenes; and (2) the lack of hierarchical
modeling for surgical scene deformation, which reduces rendering speed. To
address these challenges, we introduce EH-SurGS, an efficient and high-fidelity
reconstruction algorithm for deformable surgical scenes. We propose a
deformation modeling approach that incorporates the life cycle of 3D Gaussians,
effectively capturing both regular and irreversible deformations, thus
enhancing reconstruction quality. Additionally, we present an adaptive motion
hierarchy strategy that distinguishes between static and deformable regions
within the surgical scene. This strategy reduces the number of 3D Gaussians
passing through the deformation field, thereby improving rendering speed.
Extensive experiments demonstrate that our method surpasses existing
state-of-the-art approaches in both reconstruction quality and rendering speed.
Ablation studies further validate the effectiveness and necessity of our
proposed components. We will open-source our code upon acceptance of the paper.

Comments:
- 7 pages, 4 figures, submitted to ICRA 2025
