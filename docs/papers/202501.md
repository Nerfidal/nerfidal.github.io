---
layout: default
title: January 2025
parent: Papers
nav_order: 202501
---

<!---metadata--->


## VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large  Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-14 | Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding | cs.RO | [PDF](http://arxiv.org/pdf/2501.08286v1){: .btn .btn-green } |

**Abstract**: VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework
designed for large scenes. The framework comprises four main components: VIO
Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO
Front End, RGB frames are processed through dense bundle adjustment and
uncertainty estimation to extract scene geometry and poses. Based on this
output, the mapping module incrementally constructs and maintains a 2D Gaussian
map. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,
Score Manager, and Pose Refinement, which collectively improve mapping speed
and localization accuracy. This enables the SLAM system to handle large-scale
urban environments with up to 50 million Gaussian ellipsoids. To ensure global
consistency in large-scale scenes, we design a Loop Closure module, which
innovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian
Splatting for loop closure detection and correction of the Gaussian map.
Additionally, we propose a Dynamic Eraser to address the inevitable presence of
dynamic objects in real-world outdoor scenes. Extensive evaluations in indoor
and outdoor environments demonstrate that our approach achieves localization
performance on par with Visual-Inertial Odometry while surpassing recent
GS/NeRF SLAM methods. It also significantly outperforms all existing methods in
terms of mapping and rendering quality. Furthermore, we developed a mobile app
and verified that our framework can generate high-quality Gaussian maps in real
time using only a smartphone camera and a low-frequency IMU sensor. To the best
of our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method
capable of operating in outdoor environments and supporting kilometer-scale
large scenes.



---

## Object-Centric 2D Gaussian Splatting: Background Removal and  Occlusion-Aware Pruning for Compact Object Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-14 | Marcel Rogge, Didier Stricker | cs.CV | [PDF](http://arxiv.org/pdf/2501.08174v1){: .btn .btn-green } |

**Abstract**: Current Gaussian Splatting approaches are effective for reconstructing entire
scenes but lack the option to target specific objects, making them
computationally expensive and unsuitable for object-specific applications. We
propose a novel approach that leverages object masks to enable targeted
reconstruction, resulting in object-centric models. Additionally, we introduce
an occlusion-aware pruning strategy to minimize the number of Gaussians without
compromising quality. Our method reconstructs compact object models, yielding
object-centric Gaussian and mesh representations that are up to 96\% smaller
and up to 71\% faster to train compared to the baseline while retaining
competitive quality. These representations are immediately usable for
downstream applications such as appearance editing and physics simulation
without additional processing.

Comments:
- Accepted at ICPRAM 2025 (https://icpram.scitevents.org/Home.aspx)

---

## Evaluating Human Perception of Novel View Synthesis: Subjective Quality  Assessment of Gaussian Splatting and NeRF in Dynamic Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Yuhang Zhang, Joshua Maraval, Zhengyu Zhang, Nicolas Ramin, Shishun Tian, Lu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.08072v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) and Neural Radiance Fields (NeRF) are two
groundbreaking technologies that have revolutionized the field of Novel View
Synthesis (NVS), enabling immersive photorealistic rendering and user
experiences by synthesizing multiple viewpoints from a set of images of sparse
views. The potential applications of NVS, such as high-quality virtual and
augmented reality, detailed 3D modeling, and realistic medical organ imaging,
underscore the importance of quality assessment of NVS methods from the
perspective of human perception. Although some previous studies have explored
subjective quality assessments for NVS technology, they still face several
challenges, especially in NVS methods selection, scenario coverage, and
evaluation methodology. To address these challenges, we conducted two
subjective experiments for the quality assessment of NVS technologies
containing both GS-based and NeRF-based methods, focusing on dynamic and
real-world scenes. This study covers 360{\deg}, front-facing, and
single-viewpoint videos while providing a richer and greater number of real
scenes. Meanwhile, it's the first time to explore the impact of NVS methods in
dynamic scenes with moving objects. The two types of subjective experiments
help to fully comprehend the influences of different viewing paths from a human
perception perspective and pave the way for future development of
full-reference and no-reference quality metrics. In addition, we established a
comprehensive benchmark of various state-of-the-art objective metrics on the
proposed database, highlighting that existing methods still struggle to
accurately capture subjective quality. The results give us some insights into
the limitations of existing NVS methods and may promote the development of new
NVS methods.



---

## UnCommon Objects in 3D


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Xingchen Liu, Piyush Tayal, Jianyuan Wang, Jesus Zarzar, Tom Monnier, Konstantinos Tertikas, Jiali Duan, Antoine Toisoul, Jason Y. Zhang, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny | cs.CV | [PDF](http://arxiv.org/pdf/2501.07574v1){: .btn .btn-green } |

**Abstract**: We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for
3D deep learning and 3D generative AI. uCO3D is the largest publicly-available
collection of high-resolution videos of objects with 3D annotations that
ensures full-360$^{\circ}$ coverage. uCO3D is significantly more diverse than
MVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of
higher quality, due to extensive quality checks of both the collected videos
and the 3D annotations. Similar to analogous datasets, uCO3D contains
annotations for 3D camera poses, depth maps and sparse point clouds. In
addition, each object is equipped with a caption and a 3D Gaussian Splat
reconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D
and obtain superior results using the latter, showing that uCO3D is better for
learning applications.



---

## 3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point  Cloud or Mesh

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Lewis A G Stuart, Michael P Pound | cs.GR | [PDF](http://arxiv.org/pdf/2501.07478v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D
reconstructions, but these scenes often require specialised renderers for
effective visualisation. In contrast, point clouds are a widely used 3D
representation and are compatible with most popular 3D processing software, yet
converting 3DGS scenes into point clouds is a complex challenge. In this work
we introduce 3DGS-to-PC, a flexible and highly customisable framework that is
capable of transforming 3DGS scenes into dense, high-accuracy point clouds. We
sample points probabilistically from each Gaussian as a 3D density function. We
additionally threshold new points using the Mahalanobis distance to the
Gaussian centre, preventing extreme outliers. The result is a point cloud that
closely represents the shape encoded into the 3D Gaussian scene. Individual
Gaussians use spherical harmonics to adapt colours depending on view, and each
point may contribute only subtle colour hints to the resulting rendered scene.
To avoid spurious or incorrect colours that do not fit with the final point
cloud, we recalculate Gaussian colours via a customised image rendering
approach, assigning each Gaussian the colour of the pixel to which it
contributes most across all views. 3DGS-to-PC also supports mesh generation
through Poisson Surface Reconstruction, applied to points sampled from
predicted surface Gaussians. This allows coloured meshes to be generated from
3DGS scenes without the need for re-training. This package is highly
customisable and capability of simple integration into existing 3DGS pipelines.
3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud
and surface-based formats.



---

## RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular  Video Based on Rectified Mesh-embedded Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Sen Peng, Weixing Xie, Zilong Wang, Xiaohu Guo, Zhonggui Chen, Baorong Yang, Xiao Dong | cs.CV | [PDF](http://arxiv.org/pdf/2501.07104v1){: .btn .btn-green } |

**Abstract**: We introduce RMAvatar, a novel human avatar representation with Gaussian
splatting embedded on mesh to learn clothed avatar from a monocular video. We
utilize the explicit mesh geometry to represent motion and shape of a virtual
human and implicit appearance rendering with Gaussian Splatting. Our method
consists of two main modules: Gaussian initialization module and Gaussian
rectification module. We embed Gaussians into triangular faces and control
their motion through the mesh, which ensures low-frequency motion and surface
deformation of the avatar. Due to the limitations of LBS formula, the human
skeleton is hard to control complex non-rigid transformations. We then design a
pose-related Gaussian rectification module to learn fine-detailed non-rigid
deformations, further improving the realism and expressiveness of the avatar.
We conduct extensive experiments on public datasets, RMAvatar shows
state-of-the-art performance on both rendering quality and quantitative
evaluations. Please see our project page at https://rm-avatar.github.io.

Comments:
- CVM2025

---

## SplatMAP: Online Dense Monocular SLAM with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-13 | Yue Hu, Rong Liu, Meida Chen, Andrew Feng, Peter Beerel | cs.CV | [PDF](http://arxiv.org/pdf/2501.07015v1){: .btn .btn-green } |

**Abstract**: Achieving high-fidelity 3D reconstruction from monocular video remains
challenging due to the inherent limitations of traditional methods like
Structure-from-Motion (SfM) and monocular SLAM in accurately capturing scene
details. While differentiable rendering techniques such as Neural Radiance
Fields (NeRF) address some of these challenges, their high computational costs
make them unsuitable for real-time applications. Additionally, existing 3D
Gaussian Splatting (3DGS) methods often focus on photometric consistency,
neglecting geometric accuracy and failing to exploit SLAM's dynamic depth and
pose updates for scene refinement. We propose a framework integrating dense
SLAM with 3DGS for real-time, high-fidelity dense reconstruction. Our approach
introduces SLAM-Informed Adaptive Densification, which dynamically updates and
densifies the Gaussian model by leveraging dense point clouds from SLAM.
Additionally, we incorporate Geometry-Guided Optimization, which combines
edge-aware geometric constraints and photometric consistency to jointly
optimize the appearance and geometry of the 3DGS scene representation, enabling
detailed and accurate SLAM mapping reconstruction. Experiments on the Replica
and TUM-RGBD datasets demonstrate the effectiveness of our approach, achieving
state-of-the-art results among monocular systems. Specifically, our method
achieves a PSNR of 36.864, SSIM of 0.985, and LPIPS of 0.040 on Replica,
representing improvements of 10.7%, 6.4%, and 49.4%, respectively, over the
previous SOTA. On TUM-RGBD, our method outperforms the closest baseline by
10.2%, 6.6%, and 34.7% in the same metrics. These results highlight the
potential of our framework in bridging the gap between photometric and
geometric dense 3D scene representations, paving the way for practical and
efficient monocular dense reconstruction.



---

## ActiveGAMER: Active GAussian Mapping through Efficient Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Liyan Chen, Huangying Zhan, Kevin Chen, Xiangyu Xu, Qingan Yan, Changjiang Cai, Yi Xu | cs.CV | [PDF](http://arxiv.org/pdf/2501.06897v1){: .btn .btn-green } |

**Abstract**: We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian
Splatting (3DGS) to achieve high-quality, real-time scene mapping and
exploration. Unlike traditional NeRF-based methods, which are computationally
demanding and restrict active mapping performance, our approach leverages the
efficient rendering capabilities of 3DGS, allowing effective and efficient
exploration in complex environments. The core of our system is a
rendering-based information gain module that dynamically identifies the most
informative viewpoints for next-best-view planning, enhancing both geometric
and photometric reconstruction accuracy. ActiveGAMER also integrates a
carefully balanced framework, combining coarse-to-fine exploration,
post-refinement, and a global-local keyframe selection strategy to maximize
reconstruction completeness and fidelity. Our system autonomously explores and
reconstructs environments with state-of-the-art geometric and photometric
accuracy and completeness, significantly surpassing existing approaches in both
aspects. Extensive evaluations on benchmark datasets such as Replica and MP3D
highlight ActiveGAMER's effectiveness in active mapping tasks.



---

## F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with  Cycle-Consistent Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Yuxin Wang, Qianyi Wu, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2501.06714v1){: .btn .btn-green } |

**Abstract**: This paper tackles the problem of generalizable 3D-aware generation from
monocular datasets, e.g., ImageNet. The key challenge of this task is learning
a robust 3D-aware representation without multi-view or dynamic data, while
ensuring consistent texture and geometry across different viewpoints. Although
some baseline methods are capable of 3D-aware generation, the quality of the
generated images still lags behind state-of-the-art 2D generation approaches,
which excel in producing high-quality, detailed images. To address this severe
limitation, we propose a novel feed-forward pipeline based on pixel-aligned
Gaussian Splatting, coined as F3D-Gaus, which can produce more realistic and
reliable 3D renderings from monocular inputs. In addition, we introduce a
self-supervised cycle-consistent constraint to enforce cross-view consistency
in the learned 3D representation. This training strategy naturally allows
aggregation of multiple aligned Gaussian primitives and significantly
alleviates the interpolation limitations inherent in single-view pixel-aligned
Gaussian Splatting. Furthermore, we incorporate video model priors to perform
geometry-aware refinement, enhancing the generation of fine details in
wide-viewpoint scenarios and improving the model's capability to capture
intricate 3D textures. Extensive experiments demonstrate that our approach not
only achieves high-quality, multi-view consistent 3D-aware generation from
monocular datasets, but also significantly improves training and inference
efficiency.

Comments:
- Project Page: https://w-ted.github.io/publications/F3D-Gaus

---

## SuperNeRF-GAN: A Universal 3D-Consistent Super-Resolution Framework for  Efficient and Enhanced 3D-Aware Image Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Peng Zheng, Linzhi Huang, Yizhou Yu, Yi Chang, Yilin Wang, Rui Ma | cs.CV | [PDF](http://arxiv.org/pdf/2501.06770v1){: .btn .btn-green } |

**Abstract**: Neural volume rendering techniques, such as NeRF, have revolutionized
3D-aware image synthesis by enabling the generation of images of a single scene
or object from various camera poses. However, the high computational cost of
NeRF presents challenges for synthesizing high-resolution (HR) images. Most
existing methods address this issue by leveraging 2D super-resolution, which
compromise 3D-consistency. Other methods propose radiance manifolds or
two-stage generation to achieve 3D-consistent HR synthesis, yet they are
limited to specific synthesis tasks, reducing their universality. To tackle
these challenges, we propose SuperNeRF-GAN, a universal framework for
3D-consistent super-resolution. A key highlight of SuperNeRF-GAN is its
seamless integration with NeRF-based 3D-aware image synthesis methods and it
can simultaneously enhance the resolution of generated images while preserving
3D-consistency and reducing computational cost. Specifically, given a
pre-trained generator capable of producing a NeRF representation such as
tri-plane, we first perform volume rendering to obtain a low-resolution image
with corresponding depth and normal map. Then, we employ a NeRF
Super-Resolution module which learns a network to obtain a high-resolution
NeRF. Next, we propose a novel Depth-Guided Rendering process which contains
three simple yet effective steps, including the construction of a
boundary-correct multi-depth map through depth aggregation, a normal-guided
depth super-resolution and a depth-guided NeRF rendering. Experimental results
demonstrate the superior efficiency, 3D-consistency, and quality of our
approach. Additionally, ablation studies confirm the effectiveness of our
proposed components.



---

## Generalized and Efficient 2D Gaussian Splatting for Arbitrary-scale  Super-Resolution

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Du Chen, Liyi Chen, Zhengqiang Zhang, Lei Zhang | eess.IV | [PDF](http://arxiv.org/pdf/2501.06838v2){: .btn .btn-green } |

**Abstract**: Equipped with the continuous representation capability of Multi-Layer
Perceptron (MLP), Implicit Neural Representation (INR) has been successfully
employed for Arbitrary-scale Super-Resolution (ASR). However, the limited
receptive field of the linear layers in MLP restricts the representation
capability of INR, while it is computationally expensive to query the MLP
numerous times to render each pixel. Recently, Gaussian Splatting (GS) has
shown its advantages over INR in both visual quality and rendering speed in 3D
tasks, which motivates us to explore whether GS can be employed for the ASR
task. However, directly applying GS to ASR is exceptionally challenging because
the original GS is an optimization-based method through overfitting each single
scene, while in ASR we aim to learn a single model that can generalize to
different images and scaling factors. We overcome these challenges by
developing two novel techniques. Firstly, to generalize GS for ASR, we
elaborately design an architecture to predict the corresponding
image-conditioned Gaussians of the input low-resolution image in a feed-forward
manner. Secondly, we implement an efficient differentiable 2D GPU/CUDA-based
scale-aware rasterization to render super-resolved images by sampling discrete
RGB values from the predicted contiguous Gaussians. Via end-to-end training,
our optimized network, namely GSASR, can perform ASR for any image and unseen
scaling factors. Extensive experiments validate the effectiveness of our
proposed method. The project page can be found at
\url{https://mt-cly.github.io/GSASR.github.io/}.



---

## Synthetic Prior for Few-Shot Drivable Head Avatar Inversion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart | cs.CV | [PDF](http://arxiv.org/pdf/2501.06903v1){: .btn .btn-green } |

**Abstract**: We present SynShot, a novel method for the few-shot inversion of a drivable
head avatar based on a synthetic prior. We tackle two major challenges. First,
training a controllable 3D generative network requires a large number of
diverse sequences, for which pairs of images and high-quality tracked meshes
are not always available. Second, state-of-the-art monocular avatar models
struggle to generalize to new views and expressions, lacking a strong prior and
often overfitting to a specific viewpoint distribution. Inspired by machine
learning models trained solely on synthetic data, we propose a method that
learns a prior model from a large dataset of synthetic heads with diverse
identities, expressions, and viewpoints. With few input images, SynShot
fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a
photorealistic head avatar that generalizes to novel expressions and
viewpoints. We model the head avatar using 3D Gaussian splatting and a
convolutional encoder-decoder that outputs Gaussian parameters in UV texture
space. To account for the different modeling complexities over parts of the
head (e.g., skin vs hair), we embed the prior with explicit control for
upsampling the number of per-part primitives. Compared to state-of-the-art
monocular methods that require thousands of real training images, SynShot
significantly improves novel view and expression synthesis.

Comments:
- Website https://zielon.github.io/synshot/

---

## CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-12 | Xinyi Zheng, Steve Zhang, Weizhe Lin, Aaron Zhang, Walterio W. Mayol-Cuevas, Junxiao Shen | cs.CV | [PDF](http://arxiv.org/pdf/2501.06927v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a large-scale fine-grained dataset using
high-resolution images captured from locations worldwide. Compared to existing
datasets, our dataset offers a significantly larger size and includes a higher
level of detail, making it uniquely suited for fine-grained 3D applications.
Notably, our dataset is built using drone-captured aerial imagery, which
provides a more accurate perspective for capturing real-world site layouts and
architectural structures. By reconstructing environments with these detailed
images, our dataset supports applications such as the COLMAP format for
Gaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible
with widely-used techniques including SLAM, Multi-View Stereo, and Neural
Radiance Fields (NeRF), enabling accurate 3D reconstructions and point clouds.
This makes it a benchmark for reconstruction and segmentation tasks. The
dataset enables seamless integration with multi-modal data, supporting a range
of 3D applications, from architectural reconstruction to virtual tourism. Its
flexibility promotes innovation, facilitating breakthroughs in 3D modeling and
analysis.



---

## MapGS: Generalizable Pretraining and Data Augmentation for Online  Mapping via Novel View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-11 | Hengyuan Zhang, David Paz, Yuliang Guo, Xinyu Huang, Henrik I. Christensen, Liu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2501.06660v1){: .btn .btn-green } |

**Abstract**: Online mapping reduces the reliance of autonomous vehicles on high-definition
(HD) maps, significantly enhancing scalability. However, recent advancements
often overlook cross-sensor configuration generalization, leading to
performance degradation when models are deployed on vehicles with different
camera intrinsics and extrinsics. With the rapid evolution of novel view
synthesis methods, we investigate the extent to which these techniques can be
leveraged to address the sensor configuration generalization challenge. We
propose a novel framework leveraging Gaussian splatting to reconstruct scenes
and render camera images in target sensor configurations. The target config
sensor data, along with labels mapped to the target config, are used to train
online mapping models. Our proposed framework on the nuScenes and Argoverse 2
datasets demonstrates a performance improvement of 18% through effective
dataset augmentation, achieves faster convergence and efficient training, and
exceeds state-of-the-art performance when using only 25% of the original
training data. This enables data reuse and reduces the need for laborious data
labeling. Project page at https://henryzhangzhy.github.io/mapgs.



---

## NVS-SQA: Exploring Self-Supervised Quality Representation Learning for  Neurally Synthesized Scenes without References

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-11 | Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Weidong Cai, Tongliang Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.06488v1){: .btn .btn-green } |

**Abstract**: Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,
effectively creates photorealistic scenes from sparse viewpoints, typically
evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,
these full-reference methods, which compare synthesized views to reference
views, may not fully capture the perceptual quality of neurally synthesized
scenes (NSS), particularly due to the limited availability of dense reference
views. Furthermore, the challenges in acquiring human perceptual labels hinder
the creation of extensive labeled datasets, risking model overfitting and
reduced generalizability. To address these issues, we propose NVS-SQA, a NSS
quality assessment method to learn no-reference quality representations through
self-supervision without reliance on human labels. Traditional self-supervised
learning predominantly relies on the "same instance, similar representation"
assumption and extensive datasets. However, given that these conditions do not
apply in NSS quality assessment, we employ heuristic cues and quality scores as
learning objectives, along with a specialized contrastive pair preparation
process to improve the effectiveness and efficiency of learning. The results
show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,
on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second
best) and even exceeds 16 full-reference methods across all evaluation metrics
(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).



---

## UV-Attack: Physical-World Adversarial Attacks for Person Detection via  Dynamic-NeRF-based UV Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-10 | Yanjie Li, Wenxuan Zhang, Kaisheng Liang, Bin Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2501.05783v1){: .btn .btn-green } |

**Abstract**: In recent research, adversarial attacks on person detectors using patches or
static 3D model-based texture modifications have struggled with low success
rates due to the flexible nature of human movement. Modeling the 3D
deformations caused by various actions has been a major challenge. Fortunately,
advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer
new possibilities. In this paper, we introduce UV-Attack, a groundbreaking
approach that achieves high success rates even with extensive and unseen human
actions. We address the challenge above by leveraging dynamic-NeRF-based UV
mapping. UV-Attack can generate human images across diverse actions and
viewpoints, and even create novel actions by sampling from the SMPL parameter
space. While dynamic NeRF models are capable of modeling human bodies,
modifying clothing textures is challenging because they are embedded in neural
network parameters. To tackle this, UV-Attack generates UV maps instead of RGB
images and modifies the texture stacks. This approach enables real-time texture
edits and makes the attack more practical. We also propose a novel Expectation
over Pose Transformation loss (EoPT) to improve the evasion success rate on
unseen poses and views. Our experiments show that UV-Attack achieves a 92.75%
attack success rate against the FastRCNN model across varied poses in dynamic
video settings, significantly outperforming the state-of-the-art AdvCamou
attack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the
latest YOLOv8 detector in black-box settings. This work highlights the
potential of dynamic NeRF-based UV mapping for creating more effective
adversarial attacks on person detectors, addressing key challenges in modeling
human movement and texture modification.

Comments:
- 23 pages, 22 figures, submitted to ICLR2025

---

## Locality-aware Gaussian Compression for Fast and High-quality Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-10 | Seungjoo Shin, Jaesik Park, Sunghyun Cho | cs.CV | [PDF](http://arxiv.org/pdf/2501.05757v1){: .btn .btn-green } |

**Abstract**: We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework
that exploits the spatial coherence of 3D Gaussians for compact modeling of
volumetric scenes. To this end, we first analyze the local coherence of 3D
Gaussian attributes, and propose a novel locality-aware 3D Gaussian
representation that effectively encodes locally-coherent Gaussian attributes
using a neural field representation with a minimal storage requirement. On top
of the novel representation, LocoGS is carefully designed with additional
components such as dense initialization, an adaptive spherical harmonics
bandwidth scheme and different encoding schemes for different Gaussian
attributes to maximize compression performance. Experimental results
demonstrate that our approach outperforms the rendering quality of existing
compact Gaussian representations for representative real-world 3D datasets
while achieving from 54.6$\times$ to 96.6$\times$ compressed storage size and
from 2.1$\times$ to 2.4$\times$ rendering speed than 3DGS. Even our approach
also demonstrates an averaged 2.4$\times$ higher rendering speed than the
state-of-the-art compression method with comparable compression performance.

Comments:
- 28 pages, 15 figures, and 14 tables

---

## Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID  Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stefanos Zafeiriou | cs.CV | [PDF](http://arxiv.org/pdf/2501.05379v2){: .btn .btn-green } |

**Abstract**: Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in
reconstructing detailed 3D scenes within multi-view setups and the emergence of
large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based
method utilizing a human face foundation model as guidance with just a single
image as input. To achieve that, we extend such a model for diverse-view human
head generation by fine-tuning on synthetic data and modifying its
conditioning. Our avatars maintain a dense correspondence with a human face
mesh template, allowing blendshape-based expression generation. This is
achieved through a modified 3DGS approach, connectivity regularizers, and a
strategic initialization tailored for our task. Additionally, we propose an
optional efficient SDS-based correction step to refine the blendshape
expressions, enhancing realism and diversity. Experiments demonstrate that
Arc2Avatar achieves state-of-the-art realism and identity preservation,
effectively addressing color issues by allowing the use of very low guidance,
enabled by our strong identity prior and initialization strategy, without
compromising detail. Please visit https://arc2avatar.github.io for more
resources.

Comments:
- Project Page https://arc2avatar.github.io

---

## Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D  Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Xuyi Meng, Chen Wang, Jiahui Lei, Kostas Daniilidis, Jiatao Gu, Lingjie Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.05427v1){: .btn .btn-green } |

**Abstract**: Recent advances in 2D image generation have achieved remarkable
quality,largely driven by the capacity of diffusion models and the availability
of large-scale datasets. However, direct 3D generation is still constrained by
the scarcity and lower fidelity of 3D datasets. In this paper, we introduce
Zero-1-to-G, a novel approach that addresses this problem by enabling direct
single-view generation on Gaussian splats using pretrained 2D diffusion models.
Our key insight is that Gaussian splats, a 3D representation, can be decomposed
into multi-view images encoding different attributes. This reframes the
challenging task of direct 3D generation within a 2D diffusion framework,
allowing us to leverage the rich priors of pretrained 2D diffusion models. To
incorporate 3D awareness, we introduce cross-view and cross-attribute attention
layers, which capture complex correlations and enforce 3D consistency across
generated splats. This makes Zero-1-to-G the first direct image-to-3D
generative model to effectively utilize pretrained 2D diffusion priors,
enabling efficient training and improved generalization to unseen objects.
Extensive experiments on both synthetic and in-the-wild datasets demonstrate
superior performance in 3D object generation, offering a new approach to
high-quality 3D generation.



---

## Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and  Photorealistic Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun | cs.CV | [PDF](http://arxiv.org/pdf/2501.05242v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis
in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM
methods utilizing 3DGS have failed to provide high-quality novel view rendering
for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods
perform well for RGB-D cameras but suffer significant degradation in rendering
quality for monocular cameras. In this paper, we present Scaffold-SLAM, which
delivers simultaneous localization and high-quality photorealistic mapping
across monocular, stereo, and RGB-D cameras. We introduce two key innovations
to achieve this state-of-the-art visual quality. First, we propose
Appearance-from-Motion embedding, enabling 3D Gaussians to better model image
appearance variations across different camera poses. Second, we introduce a
frequency regularization pyramid to guide the distribution of Gaussians,
allowing the model to effectively capture finer details in the scene. Extensive
experiments on monocular, stereo, and RGB-D datasets demonstrate that
Scaffold-SLAM significantly outperforms state-of-the-art methods in
photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D
datasets for monocular cameras.

Comments:
- 12 pages, 6 figures

---

## Light Transport-aware Diffusion Posterior Sampling for Single-View  Reconstruction of 3D Volumes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-09 | Ludwic Leonard, Nils Thuerey, Ruediger Westermann | cs.CV | [PDF](http://arxiv.org/pdf/2501.05226v2){: .btn .btn-green } |

**Abstract**: We introduce a single-view reconstruction technique of volumetric fields in
which multiple light scattering effects are omnipresent, such as in clouds. We
model the unknown distribution of volumetric fields using an unconditional
diffusion model trained on a novel benchmark dataset comprising 1,000
synthetically simulated volumetric density fields. The neural diffusion model
is trained on the latent codes of a novel, diffusion-friendly, monoplanar
representation. The generative model is used to incorporate a tailored
parametric diffusion posterior sampling technique into different reconstruction
tasks. A physically-based differentiable volume renderer is employed to provide
gradients with respect to light transport in the latent space. This stands in
contrast to classic NeRF approaches and makes the reconstructions better
aligned with observed data. Through various experiments, we demonstrate
single-view reconstruction of volumetric clouds at a previously unattainable
quality.



---

## GaussianVideo: Efficient Video Representation via Hierarchical Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-08 | Andrew Bond, Jui-Hsien Wang, Long Mai, Erkut Erdem, Aykut Erdem | cs.CV | [PDF](http://arxiv.org/pdf/2501.04782v1){: .btn .btn-green } |

**Abstract**: Efficient neural representations for dynamic video scenes are critical for
applications ranging from video compression to interactive simulations. Yet,
existing methods often face challenges related to high memory usage, lengthy
training times, and temporal consistency. To address these issues, we introduce
a novel neural video representation that combines 3D Gaussian splatting with
continuous camera motion modeling. By leveraging Neural ODEs, our approach
learns smooth camera trajectories while maintaining an explicit 3D scene
representation through Gaussians. Additionally, we introduce a spatiotemporal
hierarchical learning strategy, progressively refining spatial and temporal
features to enhance reconstruction quality and accelerate convergence. This
memory-efficient approach achieves high-quality rendering at impressive speeds.
Experimental results show that our hierarchical learning, combined with robust
camera motion modeling, captures complex dynamic scenes with strong temporal
consistency, achieving state-of-the-art performance across diverse video
datasets in both high- and low-motion scenarios.

Comments:
- 10 pages, 10 figures

---

## FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using  Gaussian Splatting with Depth-Feature Consistency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-08 | Han Huang, Yulun Wu, Chao Deng, Ge Gao, Ming Gu, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2501.04628v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian Splatting has sparked a new trend in the field of computer
vision. Apart from novel view synthesis, it has also been extended to the area
of multi-view reconstruction. The latest methods facilitate complete, detailed
surface reconstruction while ensuring fast training speed. However, these
methods still require dense input views, and their output quality significantly
degrades with sparse views. We observed that the Gaussian primitives tend to
overfit the few training views, leading to noisy floaters and incomplete
reconstruction surfaces. In this paper, we present an innovative sparse-view
reconstruction framework that leverages intra-view depth and multi-view feature
consistency to achieve remarkably accurate surface reconstruction.
Specifically, we utilize monocular depth ranking information to supervise the
consistency of depth distribution within patches and employ a smoothness loss
to enhance the continuity of the distribution. To achieve finer surface
reconstruction, we optimize the absolute position of depth through multi-view
projection features. Extensive experiments on DTU and BlendedMVS demonstrate
that our method outperforms state-of-the-art methods with a speedup of 60x to
200x, achieving swift and fine-grained mesh reconstruction without the need for
costly pre-training.

Comments:
- Accepted by AAAI 2025. Project page:
  https://alvin528.github.io/FatesGS/

---

## DehazeGS: Seeing Through Fog with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.03659v2){: .btn .btn-green } |

**Abstract**: Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and rendering quality. Although NeRF-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, NeRF's implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward rendering process. We introduce
DehazeGS, a method capable of decomposing and rendering a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy datasets demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both rendering
quality and computational efficiency.

Comments:
- 9 pages,4 figures

---

## NeRFs are Mirror Detectors: Using Structural Similarity for Multi-View  Mirror Scene Reconstruction with 3D Surface Primitives

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Leif Van Holland, Michael Weinmann, Jan U. MÃ¼ller, Patrick Stotko, Reinhard Klein | cs.CV | [PDF](http://arxiv.org/pdf/2501.04074v1){: .btn .btn-green } |

**Abstract**: While neural radiance fields (NeRF) led to a breakthrough in photorealistic
novel view synthesis, handling mirroring surfaces still denotes a particular
challenge as they introduce severe inconsistencies in the scene representation.
Previous attempts either focus on reconstructing single reflective objects or
rely on strong supervision guidance in terms of additional user-provided
annotations of visible image regions of the mirrors, thereby limiting the
practical usability. In contrast, in this paper, we present NeRF-MD, a method
which shows that NeRFs can be considered as mirror detectors and which is
capable of reconstructing neural radiance fields of scenes containing mirroring
surfaces without the need for prior annotations. To this end, we first compute
an initial estimate of the scene geometry by training a standard NeRF using a
depth reprojection loss. Our key insight lies in the fact that parts of the
scene corresponding to a mirroring surface will still exhibit a significant
photometric inconsistency, whereas the remaining parts are already
reconstructed in a plausible manner. This allows us to detect mirror surfaces
by fitting geometric primitives to such inconsistent regions in this initial
stage of the training. Using this information, we then jointly optimize the
radiance field and mirror geometry in a second training stage to refine their
quality. We demonstrate the capability of our method to allow the faithful
detection of mirrors in the scene as well as the reconstruction of a single
consistent scene representation, and demonstrate its potential in comparison to
baseline and mirror-aware approaches.



---

## NeuralSVG: An Implicit Representation for Text-to-Vector Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Sagi Polaczek, Yuval Alaluf, Elad Richardson, Yael Vinker, Daniel Cohen-Or | cs.CV | [PDF](http://arxiv.org/pdf/2501.03992v1){: .btn .btn-green } |

**Abstract**: Vector graphics are essential in design, providing artists with a versatile
medium for creating resolution-independent and highly editable visual content.
Recent advancements in vision-language and diffusion models have fueled
interest in text-to-vector graphics generation. However, existing approaches
often suffer from over-parameterized outputs or treat the layered structure - a
core feature of vector graphics - as a secondary goal, diminishing their
practical use. Recognizing the importance of layered SVG representations, we
propose NeuralSVG, an implicit neural representation for generating vector
graphics from text prompts. Inspired by Neural Radiance Fields (NeRFs),
NeuralSVG encodes the entire scene into the weights of a small MLP network,
optimized using Score Distillation Sampling (SDS). To encourage a layered
structure in the generated SVG, we introduce a dropout-based regularization
technique that strengthens the standalone meaning of each shape. We
additionally demonstrate that utilizing a neural representation provides an
added benefit of inference-time control, enabling users to dynamically adapt
the generated SVG based on user-provided inputs, all with a single learned
representation. Through extensive qualitative and quantitative evaluations, we
demonstrate that NeuralSVG outperforms existing methods in generating
structured and flexible SVG.

Comments:
- Project Page: https://sagipolaczek.github.io/NeuralSVG/

---

## ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Abhishek Saroha, Florian Hofherr, Mariia Gladkova, Cecilia Curreli, Or Litany, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2501.03875v1){: .btn .btn-green } |

**Abstract**: Stylizing a dynamic scene based on an exemplar image is critical for various
real-world applications, including gaming, filmmaking, and augmented and
virtual reality. However, achieving consistent stylization across both spatial
and temporal dimensions remains a significant challenge. Most existing methods
are designed for static scenes and often require an optimization process for
each style image, limiting their adaptability. We introduce ZDySS, a zero-shot
stylization framework for dynamic scenes, allowing our model to generalize to
previously unseen style images at inference. Our approach employs Gaussian
splatting for scene representation, linking each Gaussian to a learned feature
vector that renders a feature map for any given view and timestamp. By applying
style transfer on the learned feature vectors instead of the rendered feature
map, we enhance spatio-temporal consistency across frames. Our method
demonstrates superior performance and coherence over state-of-the-art baselines
in tests on real-world dynamic scenes, making it a robust solution for
practical applications.



---

## MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval  Adjustment for Compact Dynamic 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, Munchurl Kim | cs.CV | [PDF](http://arxiv.org/pdf/2501.03714v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has made significant strides in scene
representation and neural rendering, with intense efforts focused on adapting
it for dynamic scenes. Despite delivering remarkable rendering quality and
speed, existing methods struggle with storage demands and representing complex
real-world motions. To tackle these issues, we propose MoDecGS, a
memory-efficient Gaussian splatting framework designed for reconstructing novel
views in challenging scenarios with complex motions. We introduce GlobaltoLocal
Motion Decomposition (GLMD) to effectively capture dynamic motions in a
coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global
CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold
representation to dynamic video reconstruction. For Global CS, we propose
Global Anchor Deformation (GAD) to efficiently represent global dynamics along
complex motions, by directly deforming the implicit Scaffold attributes which
are anchor position, offset, and local context features. Next, we finely adjust
local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly.
Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically
control the temporal coverage of each Local CS during training, allowing
MoDecGS to find optimal interval assignments based on the specified number of
temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an
average 70% reduction in model size over stateoftheart methods for dynamic 3D
Gaussians from realworld dynamic videos while maintaining or even improving
rendering quality.

Comments:
- The last two authors are co-corresponding authors. Please visit our
  project page at https://kaist-viclab.github.io/MoDecGS-site/

---

## ConcealGS: Concealing Invisible Copyright Information in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-07 | Yifeng Yang, Hengyu Liu, Chenxin Li, Yining Sun, Wuyang Li, Yifan Liu, Yiyang Lin, Yixuan Yuan, Nanyang Ye | cs.CV | [PDF](http://arxiv.org/pdf/2501.03605v1){: .btn .btn-green } |

**Abstract**: With the rapid development of 3D reconstruction technology, the widespread
distribution of 3D data has become a future trend. While traditional visual
data (such as images and videos) and NeRF-based formats already have mature
techniques for copyright protection, steganographic techniques for the emerging
3D Gaussian Splatting (3D-GS) format have yet to be fully explored. To address
this, we propose ConcealGS, an innovative method for embedding implicit
information into 3D-GS. By introducing the knowledge distillation and gradient
optimization strategy based on 3D-GS, ConcealGS overcomes the limitations of
NeRF-based models and enhances the robustness of implicit information and the
quality of 3D reconstruction. We evaluate ConcealGS in various potential
application scenarios, and experimental results have demonstrated that
ConcealGS not only successfully recovers implicit information but also has
almost no impact on rendering quality, providing a new approach for embedding
invisible and recoverable information into 3D models in the future.



---

## Gaussian Masked Autoencoders

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, Shiry Ginosar | cs.CV | [PDF](http://arxiv.org/pdf/2501.03229v1){: .btn .btn-green } |

**Abstract**: This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While
reconstructive self-supervised learning frameworks such as MAE learns good
semantic abstractions, it is not trained for explicit spatial awareness. Our
approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic
abstractions and spatial understanding jointly. Like MAE, it reconstructs the
image end-to-end in the pixel space, but beyond MAE, it also introduces an
intermediate, 3D Gaussian-based representation and renders images via
splatting. We show that GMAE can enable various zero-shot learning capabilities
of spatial understanding (e.g., figure-ground segmentation, image layering,
edge detection, etc.) while preserving the high-level semantics of
self-supervised representation quality from MAE. To our knowledge, we are the
first to employ Gaussian primitives in an image representation learning
framework beyond optimization-based single-scene reconstructions. We believe
GMAE will inspire further research in this direction and contribute to
developing next-generation techniques for modeling high-fidelity visual data.
More details at https://brjathu.github.io/gmae



---

## Compression of 3D Gaussian Splatting with Optimized Feature Planes and  Standard Video Codecs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Soonbin Lee, Fangwen Shu, Yago Sanchez, Thomas Schierl, Cornelius Hellge | cs.CV | [PDF](http://arxiv.org/pdf/2501.03399v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting is a recognized method for 3D scene representation,
known for its high rendering quality and speed. However, its substantial data
requirements present challenges for practical applications. In this paper, we
introduce an efficient compression technique that significantly reduces storage
overhead by using compact representation. We propose a unified architecture
that combines point cloud data and feature planes through a progressive
tri-plane structure. Our method utilizes 2D feature planes, enabling continuous
spatial representation. To further optimize these representations, we
incorporate entropy modeling in the frequency domain, specifically designed for
standard video codecs. We also propose channel-wise bit allocation to achieve a
better trade-off between bitrate consumption and feature plane representation.
Consequently, our model effectively leverages spatial correlations within the
feature planes to enhance rate-distortion performance using standard,
non-differentiable video codecs. Experimental results demonstrate that our
method outperforms existing methods in data compactness while maintaining high
rendering quality. Our project page is available at
https://fraunhoferhhi.github.io/CodecGS



---

## HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian  Splatting Based Data Augmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Wentian Qu, Jiahe Li, Jian Cheng, Jian Shi, Chenyu Meng, Cuixia Ma, Hongan Wang, Xiaoming Deng, Yinda Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.02845v1){: .btn .btn-green } |

**Abstract**: Understanding of bimanual hand-object interaction plays an important role in
robotics and virtual reality. However, due to significant occlusions between
hands and object as well as the high degree-of-freedom motions, it is
challenging to collect and annotate a high-quality, large-scale dataset, which
prevents further improvement of bimanual hand-object interaction-related
baselines. In this work, we propose a new 3D Gaussian Splatting based data
augmentation framework for bimanual hand-object interaction, which is capable
of augmenting existing dataset to large-scale photorealistic data with various
hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects
and hands, and to deal with the rendering blur problem due to multi-resolution
input images used, we design a super-resolution module. Second, we extend the
single hand grasping pose optimization module for the bimanual hand object to
generate various poses of bimanual hand-object interaction, which can
significantly expand the pose distribution of the dataset. Third, we conduct an
analysis for the impact of different aspects of the proposed data augmentation
on the understanding of the bimanual hand-object interaction. We perform our
data augmentation on two benchmarks, H2O and Arctic, and verify that our method
can improve the performance of the baselines.

Comments:
- Accepted by AAAI2025

---

## AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal  Conditions and Larger Scene

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-06 | Chaoran Feng, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuan, Yonghong Tian | cs.CV | [PDF](http://arxiv.org/pdf/2501.02807v2){: .btn .btn-green } |

**Abstract**: Compared to frame-based methods, computational neuromorphic imaging using
event cameras offers significant advantages, such as minimal motion blur,
enhanced temporal resolution, and high dynamic range. The multi-view
consistency of Neural Radiance Fields combined with the unique benefits of
event cameras, has spurred recent research into reconstructing NeRF from data
captured by moving event cameras. While showing impressive performance,
existing methods rely on ideal conditions with the availability of uniform and
high-quality event sequences and accurate camera poses, and mainly focus on the
object level reconstruction, thus limiting their practical applications. In
this work, we propose AE-NeRF to address the challenges of learning event-based
NeRF from non-ideal conditions, including non-uniform event sequences, noisy
poses, and various scales of scenes. Our method exploits the density of event
streams and jointly learn a pose correction module with an event-based NeRF
(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.
To generalize to larger scenes, we propose hierarchical event distillation with
a proposal e-NeRF network and a vanilla e-NeRF network to resample and refine
the reconstruction process. We further propose an event reconstruction loss and
a temporal loss to improve the view consistency of the reconstructed scene. We
established a comprehensive benchmark that includes large-scale scenes to
simulate practical non-ideal conditions, incorporating both synthetic and
challenging real-world event datasets. The experimental results show that our
method achieves a new state-of-the-art in event-based 3D reconstruction.



---

## GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields  through Efficient Dense 3D Point Tracking

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-05 | Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2501.02690v1){: .btn .btn-green } |

**Abstract**: 4D video control is essential in video generation as it enables the use of
sophisticated lens techniques, such as multi-camera shooting and dolly zoom,
which are currently unsupported by existing methods. Training a video Diffusion
Transformer (DiT) directly to control 4D content requires expensive multi-view
videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that
optimizes a 4D representation and renders videos according to different 4D
elements, such as camera pose and object motion editing, we bring pseudo 4D
Gaussian fields to video generation. Specifically, we propose a novel framework
that constructs a pseudo 4D Gaussian field with dense 3D point tracking and
renders the Gaussian field for all video frames. Then we finetune a pretrained
DiT to generate videos following the guidance of the rendered video, dubbed as
GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense
3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field
construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art
sparse 3D point tracking method, in accuracy and accelerates the inference
speed by two orders of magnitude. During the inference stage, GS-DiT can
generate videos with the same dynamic content while adhering to different
camera parameters, addressing a significant limitation of current video
generation models. GS-DiT demonstrates strong generalization capabilities and
extends the 4D controllability of Gaussian splatting to video generation beyond
just camera poses. It supports advanced cinematic effects through the
manipulation of the Gaussian field and camera intrinsics, making it a powerful
tool for creative video production. Demos are available at
https://wkbian.github.io/Projects/GS-DiT/.

Comments:
- Project Page: https://wkbian.github.io/Projects/GS-DiT/

---

## CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Chenhao Zhang, Yuanping Cao, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01695v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene
representation and reconstruction, leveraging densely distributed Gaussian
primitives to enable real-time rendering of high-resolution images. While
existing 3DGS methods perform well in scenes with minor view variation, large
view changes in cross-view scenes pose optimization challenges for these
methods. To address these issues, we propose a novel cross-view Gaussian
Splatting method for large-scale scene reconstruction, based on dual-branch
fusion. Our method independently reconstructs models from aerial and ground
views as two independent branches to establish the baselines of Gaussian
distribution, providing reliable priors for cross-view reconstruction during
both initialization and densification. Specifically, a gradient-aware
regularization strategy is introduced to mitigate smoothing issues caused by
significant view disparities. Additionally, a unique Gaussian supplementation
strategy is utilized to incorporate complementary information of dual-branch
into the cross-view model. Extensive experiments on benchmark datasets
demonstrate that our method achieves superior performance in novel view
synthesis compared to state-of-the-art methods.



---

## Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Alberta Longhini, Marcel BÃ¼sching, Bardienus P. Duisterhof, Jens Lundell, Jeffrey Ichnowski, MÃ¥rten BjÃ¶rkman, Danica Kragic | cs.CV | [PDF](http://arxiv.org/pdf/2501.01715v1){: .btn .btn-green } |

**Abstract**: We introduce Cloth-Splatting, a method for estimating 3D states of cloth from
RGB images through a prediction-update framework. Cloth-Splatting leverages an
action-conditioned dynamics model for predicting future states and uses 3D
Gaussian Splatting to update the predicted states. Our key insight is that
coupling a 3D mesh-based representation with Gaussian Splatting allows us to
define a differentiable map between the cloth state space and the image space.
This enables the use of gradient-based optimization techniques to refine
inaccurate state estimates using only RGB supervision. Our experiments
demonstrate that Cloth-Splatting not only improves state estimation accuracy
over current baselines but also reduces convergence time.

Comments:
- Accepted at the 8th Conference on Robot Learning (CoRL 2024). Code
  and videos available at: kth-rpl.github.io/cloth-splatting

---

## PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban  Buildings Reconstruction via Semantic-Aware Grouping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Tengfei Wang, Xin Wang, Yongmao Hou, Yiwei Xu, Wendi Zhang, Zongqian Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2501.01677v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a transformative method in the
field of real-time novel synthesis. Based on 3DGS, recent advancements cope
with large-scale scenes via spatial-based partition strategy to reduce video
memory and optimization time costs. In this work, we introduce a parallel
Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues
for both partitioning and Gaussian kernel optimization, enabling fine-grained
building surface reconstruction of large-scale urban areas without downsampling
the original image resolution. First, the Cross-modal model - Language Segment
Anything is leveraged to segment building masks. Then, the segmented building
regions is grouped into sub-regions according to the visibility check across
registered images. The Gaussian kernels for these sub-regions are optimized in
parallel with masked pixels. In addition, the normal loss is re-formulated for
the detected edges of masks to alleviate the ambiguities in normal vectors on
edges. Finally, to improve the optimization of 3D Gaussians, we introduce a
gradient-constrained balance-load loss that accounts for the complexity of the
corresponding scenes, effectively minimizing the thread waiting time in the
pixel-parallel rendering stage as well as the reconstruction lost. Extensive
experiments are tested on various urban datasets, the results demonstrated the
superior performance of our PG-SAG on building surface reconstruction, compared
to several state-of-the-art 3DGS-based methods. Project
Web:https://github.com/TFWang-9527/PG-SAG.



---

## EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-03 | Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren | cs.RO | [PDF](http://arxiv.org/pdf/2501.01895v1){: .btn .btn-green } |

**Abstract**: We introduce EnerVerse, a comprehensive framework for embodied future space
generation specifically designed for robotic manipulation tasks. EnerVerse
seamlessly integrates convolutional and bidirectional attention mechanisms for
inner-chunk space modeling, ensuring low-level consistency and continuity.
Recognizing the inherent redundancy in video data, we propose a sparse memory
context combined with a chunkwise unidirectional generative paradigm to enable
the generation of infinitely long sequences. To further augment robotic
capabilities, we introduce the Free Anchor View (FAV) space, which provides
flexible perspectives to enhance observation and analysis. The FAV space
mitigates motion modeling ambiguity, removes physical constraints in confined
environments, and significantly improves the robot's generalization and
adaptability across various tasks and settings. To address the prohibitive
costs and labor intensity of acquiring multi-camera observations, we present a
data engine pipeline that integrates a generative model with 4D Gaussian
Splatting (4DGS). This pipeline leverages the generative model's robust
generalization capabilities and the spatial constraints provided by 4DGS,
enabling an iterative enhancement of data quality and diversity, thus creating
a data flywheel effect that effectively narrows the sim-to-real gap. Finally,
our experiments demonstrate that the embodied future space generation prior
substantially enhances policy predictive capabilities, resulting in improved
overall performance, particularly in long-range robotic manipulation tasks.

Comments:
- Website: https://sites.google.com/view/enerverse

---

## Deformable Gaussian Splatting for Efficient and High-Fidelity  Reconstruction of Surgical Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-02 | Jiwei Shan, Zeyu Cai, Cheng-Tai Hsieh, Shing Shin Cheng, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01101v1){: .btn .btn-green } |

**Abstract**: Efficient and high-fidelity reconstruction of deformable surgical scenes is a
critical yet challenging task. Building on recent advancements in 3D Gaussian
splatting, current methods have seen significant improvements in both
reconstruction quality and rendering speed. However, two major limitations
remain: (1) difficulty in handling irreversible dynamic changes, such as tissue
shearing, which are common in surgical scenes; and (2) the lack of hierarchical
modeling for surgical scene deformation, which reduces rendering speed. To
address these challenges, we introduce EH-SurGS, an efficient and high-fidelity
reconstruction algorithm for deformable surgical scenes. We propose a
deformation modeling approach that incorporates the life cycle of 3D Gaussians,
effectively capturing both regular and irreversible deformations, thus
enhancing reconstruction quality. Additionally, we present an adaptive motion
hierarchy strategy that distinguishes between static and deformable regions
within the surgical scene. This strategy reduces the number of 3D Gaussians
passing through the deformation field, thereby improving rendering speed.
Extensive experiments demonstrate that our method surpasses existing
state-of-the-art approaches in both reconstruction quality and rendering speed.
Ablation studies further validate the effectiveness and necessity of our
proposed components. We will open-source our code upon acceptance of the paper.

Comments:
- 7 pages, 4 figures, submitted to ICRA 2025

---

## EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-01-02 | Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2501.01003v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene
representation. Despite their impressive performance, they confront challenges
due to the limitation of structure-from-motion (SfM) methods on acquiring
accurate scene initialization, or the inefficiency of densification strategy.
In this paper, we introduce a novel framework EasySplat to achieve high-quality
3DGS modeling. Instead of using SfM for scene initialization, we employ a novel
method to release the power of large-scale pointmap approaches. Specifically,
we propose an efficient grouping strategy based on view similarity, and use
robust pointmap priors to obtain high-quality point clouds and camera poses for
3D scene initialization. After obtaining a reliable scene structure, we propose
a novel densification approach that adaptively splits Gaussian primitives based
on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme.
In this way, the proposed method tackles the limitation on initialization and
optimization, leading to an efficient and accurate 3DGS modeling. Extensive
experiments demonstrate that EasySplat outperforms the current state-of-the-art
(SOTA) in handling novel view synthesis.

Comments:
- 6 pages, 5figures
