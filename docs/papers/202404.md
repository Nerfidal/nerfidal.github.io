---
layout: default
title: April 2024
parent: Papers
nav_order: 202404
---

<!---metadata--->


## 3D Gaussian Blendshapes for Head Avatar Animation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-30 | Shengjie Ma, Yanlin Weng, Tianjia Shao, Kun Zhou | cs.GR | [PDF](http://arxiv.org/pdf/2404.19398v2){: .btn .btn-green } |

**Abstract**: We introduce 3D Gaussian blendshapes for modeling photorealistic head
avatars. Taking a monocular video as input, we learn a base head model of
neutral expression, along with a group of expression blendshapes, each of which
corresponds to a basis expression in classical parametric face models. Both the
neutral model and expression blendshapes are represented as 3D Gaussians, which
contain a few properties to depict the avatar appearance. The avatar model of
an arbitrary expression can be effectively generated by combining the neutral
model and expression blendshapes through linear blending of Gaussians with the
expression coefficients. High-fidelity head avatar animations can be
synthesized in real time using Gaussian splatting. Compared to state-of-the-art
methods, our Gaussian blendshape representation better captures high-frequency
details exhibited in input video, and achieves superior rendering performance.

Comments:
- ACM SIGGRAPH Conference Proceedings 2024

---

## RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-30 | Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2404.19706v3){: .btn .btn-green } |

**Abstract**: We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction
system with an RGBD camera for large-scale environments using Gaussian
splatting. The system features a compact Gaussian representation and a highly
efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be
either opaque or nearly transparent, with the opaque ones fitting the surface
and dominant colors, and transparent ones fitting residual colors. By rendering
depth in a different way from color rendering, we let a single opaque Gaussian
well fit a local surface region without the need of multiple overlapping
Gaussians, hence largely reducing the memory and computation cost. For
on-the-fly Gaussian optimization, we explicitly add Gaussians for three types
of pixels per frame: newly observed, with large color errors, and with large
depth errors. We also categorize all Gaussians into stable and unstable ones,
where the stable Gaussians are expected to well fit previously observed RGBD
images and otherwise unstable. We only optimize the unstable Gaussians and only
render the pixels occupied by unstable Gaussians. In this way, both the number
of Gaussians to be optimized and pixels to be rendered are largely reduced, and
the optimization can be done in real time. We show real-time reconstructions of
a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD
SLAM, our system achieves comparable high-quality reconstruction but with
around twice the speed and half the memory cost, and shows superior performance
in the realism of novel view synthesis and camera tracking accuracy.

Comments:
- To be published in ACM SIGGRAPH 2024

---

## NeRF-Insert: 3D Local Editing with Multimodal Control Signals

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-30 | Benet Oriol Sabat, Alessandro Achille, Matthew Trager, Stefano Soatto | cs.CV | [PDF](http://arxiv.org/pdf/2404.19204v1){: .btn .btn-green } |

**Abstract**: We propose NeRF-Insert, a NeRF editing framework that allows users to make
high-quality local edits with a flexible level of control. Unlike previous work
that relied on image-to-image models, we cast scene editing as an in-painting
problem, which encourages the global structure of the scene to be preserved.
Moreover, while most existing methods use only textual prompts to condition
edits, our framework accepts a combination of inputs of different modalities as
reference. More precisely, a user may provide a combination of textual and
visual inputs including images, CAD models, and binary image masks for
specifying a 3D region. We use generic image generation models to in-paint the
scene from multiple viewpoints, and lift the local edits to a 3D-consistent
NeRF edit. Compared to previous methods, our results show better visual quality
and also maintain stronger consistency with the original NeRF.



---

## GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-30 | Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, Zexiang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.19702v1){: .btn .btn-green } |

**Abstract**: We propose GS-LRM, a scalable large reconstruction model that can predict
high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23
seconds on single A100 GPU. Our model features a very simple transformer-based
architecture; we patchify input posed images, pass the concatenated multi-view
image tokens through a sequence of transformer blocks, and decode final
per-pixel Gaussian parameters directly from these tokens for differentiable
rendering. In contrast to previous LRMs that can only reconstruct objects, by
predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large
variations in scale and complexity. We show that our model can work on both
object and scene captures by training it on Objaverse and RealEstate10K
respectively. In both scenarios, the models outperform state-of-the-art
baselines by a wide margin. We also demonstrate applications of our model in
downstream 3D generation tasks. Our project webpage is available at:
https://sai-bi.github.io/project/gs-lrm/ .

Comments:
- Project webpage: https://sai-bi.github.io/project/gs-lrm/

---

## 3D Gaussian Splatting with Deferred Reflection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Keyang Ye, Qiming Hou, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2404.18454v1){: .btn .btn-green } |

**Abstract**: The advent of neural and Gaussian-based radiance field methods have achieved
great success in the field of novel view synthesis. However, specular
reflection remains non-trivial, as the high frequency radiance field is
notoriously difficult to fit stably and accurately. We present a deferred
shading method to effectively render specular reflection with Gaussian
splatting. The key challenge comes from the environment map reflection model,
which requires accurate surface normal while simultaneously bottlenecks normal
estimation with discontinuous gradients. We leverage the per-pixel reflection
gradients generated by deferred shading to bridge the optimization process of
neighboring Gaussians, allowing nearly correct normal estimations to gradually
propagate and eventually spread over all reflective objects. Our method
significantly outperforms state-of-the-art techniques and concurrent work in
synthesizing high-quality specular reflection effects, demonstrating a
consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic
and real-world scenes, while running at a frame rate almost identical to
vanilla Gaussian splatting.



---

## Reconstructing Satellites in 3D from Amateur Telescope Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Zhiming Chang, Boyang Liu, Yifei Xia, Youming Guo, Boxin Shi, He Sun | cs.CV | [PDF](http://arxiv.org/pdf/2404.18394v1){: .btn .btn-green } |

**Abstract**: This paper proposes a framework for the 3D reconstruction of satellites in
low-Earth orbit, utilizing videos captured by small amateur telescopes. The
video data obtained from these telescopes differ significantly from data for
standard 3D reconstruction tasks, characterized by intense motion blur,
atmospheric turbulence, pervasive background light pollution, extended focal
length and constrained observational perspectives. To address these challenges,
our approach begins with a comprehensive pre-processing workflow that
encompasses deep learning-based image restoration, feature point extraction and
camera pose initialization. We proceed with the application of an improved 3D
Gaussian splatting algorithm for reconstructing the 3D model. Our technique
supports simultaneous 3D Gaussian training and pose estimation, enabling the
robust generation of intricate 3D point clouds from sparse, noisy data. The
procedure is further bolstered by a post-editing phase designed to eliminate
noise points inconsistent with our prior knowledge of a satellite's geometric
constraints. We validate our approach using both synthetic datasets and actual
observations of China's Space Station, showcasing its significant advantages
over existing methods in reconstructing 3D space objects from ground-based
observations.



---

## SAGS: Structure-Aware 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Evangelos Ververas, Rolandos Alexandros Potamias, Jifei Song, Jiankang Deng, Stefanos Zafeiriou | cs.CV | [PDF](http://arxiv.org/pdf/2404.19149v1){: .btn .btn-green } |

**Abstract**: Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the
way to real-time neural rendering overcoming the computational burden of
volumetric methods. Following the pioneering work of 3D-GS, several methods
have attempted to achieve compressible and high-fidelity performance
alternatives. However, by employing a geometry-agnostic optimization scheme,
these methods neglect the inherent 3D structure of the scene, thereby
restricting the expressivity and the quality of the representation, resulting
in various floating points and artifacts. In this work, we propose a
structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the
geometry of the scene, which reflects to state-of-the-art rendering performance
and reduced storage requirements on benchmark novel-view synthesis datasets.
SAGS is founded on a local-global graph representation that facilitates the
learning of complex scenes and enforces meaningful point displacements that
preserve the scene's geometry. Additionally, we introduce a lightweight version
of SAGS, using a simple yet effective mid-point interpolation scheme, which
showcases a compact representation of the scene with up to 24$\times$ size
reduction without the reliance on any compression strategies. Extensive
experiments across multiple benchmark datasets demonstrate the superiority of
SAGS compared to state-of-the-art 3D-GS methods under both rendering quality
and model size. Besides, we demonstrate that our structure-aware method can
effectively mitigate floating artifacts and irregular distortions of previous
methods while obtaining precise depth maps. Project page
https://eververas.github.io/SAGS/.

Comments:
- 15 pages, 8 figures, 3 tables

---

## Bootstrap 3D Reconstructed Scenes from 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Yifei Gao, Jie Ou, Lei Wang, Jun Cheng | cs.GR | [PDF](http://arxiv.org/pdf/2404.18669v1){: .btn .btn-green } |

**Abstract**: Recent developments in neural rendering techniques have greatly enhanced the
rendering of photo-realistic 3D scenes across both academic and commercial
fields. The latest method, known as 3D Gaussian Splatting (3D-GS), has set new
benchmarks for rendering quality and speed. Nevertheless, the limitations of
3D-GS become pronounced in synthesizing new viewpoints, especially for views
that greatly deviate from those seen during training. Additionally, issues such
as dilation and aliasing arise when zooming in or out. These challenges can all
be traced back to a single underlying issue: insufficient sampling. In our
paper, we present a bootstrapping method that significantly addresses this
problem. This approach employs a diffusion model to enhance the rendering of
novel views using trained 3D-GS, thereby streamlining the training process. Our
results indicate that bootstrapping effectively reduces artifacts, as well as
clear enhancements on the evaluation metrics. Furthermore, we show that our
method is versatile and can be easily integrated, allowing various 3D
reconstruction projects to benefit from our approach.



---

## Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler  Solutions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan | cs.CV | [PDF](http://arxiv.org/pdf/2404.19015v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) show impressive performance in photo-realistic
free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF
and ZipNeRF employ explicit models for faster optimization and rendering, as
compared to the NeRF that employs an implicit representation. However, both
implicit and explicit radiance fields require dense sampling of images in the
given scene. Their performance degrades significantly when only a sparse set of
views is available. Researchers find that supervising the depth estimated by a
radiance field helps train it effectively with fewer views. The depth
supervision is obtained either using classical approaches or neural networks
pre-trained on a large dataset. While the former may provide only sparse
supervision, the latter may suffer from generalization issues. As opposed to
the earlier approaches, we seek to learn the depth supervision by designing
augmented models and training them along with the main radiance field. Further,
we aim to design a framework of regularizations that can work across different
implicit and explicit radiance fields. We observe that certain features of
these radiance field models overfit to the observed images in the sparse-input
scenario. Our key finding is that reducing the capability of the radiance
fields with respect to positional encoding, the number of decomposed tensor
components or the size of the hash table, constrains the model to learn simpler
solutions, which estimate better depth in certain regions. By designing
augmented models based on such reduced capabilities, we obtain better depth
supervision for the main radiance field. We achieve state-of-the-art
view-synthesis performance with sparse input views on popular datasets
containing forward-facing and 360$^\circ$ scenes by employing the above
regularizations.

Comments:
- The source code for our model can be found on our project page:
  https://nagabhushansn95.github.io/publications/2024/Simple-RF.html. arXiv
  admin note: substantial text overlap with arXiv:2309.03955

---

## GSTalker: Real-time Audio-Driven Talking Face Generation via Deformable  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Bo Chen, Shoukang Hu, Qi Chen, Chenpeng Du, Ran Yi, Yanmin Qian, Xie Chen | cs.CV | [PDF](http://arxiv.org/pdf/2404.19040v1){: .btn .btn-green } |

**Abstract**: We present GStalker, a 3D audio-driven talking face generation model with
Gaussian Splatting for both fast training (40 minutes) and real-time rendering
(125 FPS) with a 3$\sim$5 minute video for training material, in comparison
with previous 2D and 3D NeRF-based modeling frameworks which require hours of
training and seconds of rendering per frame. Specifically, GSTalker learns an
audio-driven Gaussian deformation field to translate and transform 3D Gaussians
to synchronize with audio information, in which multi-resolution hashing
grid-based tri-plane and temporal smooth module are incorporated to learn
accurate deformation for fine-grained facial details. In addition, a
pose-conditioned deformation field is designed to model the stabilized torso.
To enable efficient optimization of the condition Gaussian deformation field,
we initialize 3D Gaussians by learning a coarse static Gaussian representation.
Extensive experiments in person-specific videos with audio tracks validate that
GSTalker can generate high-fidelity and audio-lips synchronized results with
fast training and real-time rendering speed.



---

## MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and  Head Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.19026v1){: .btn .btn-green } |

**Abstract**: Creating high-fidelity head avatars from multi-view videos is a core issue
for many AR/VR applications. However, existing methods usually struggle to
obtain high-quality renderings for all different head components simultaneously
since they use one single representation to model components with drastically
different characteristics (e.g., skin vs. hair). In this paper, we propose a
Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components
with more suitable representations. Specifically, we select an enhanced FLAME
mesh as our facial representation and predict a UV displacement map to provide
per-vertex offsets for improved personalized geometric details. To achieve
photorealistic renderings, we obtain facial colors using deferred neural
rendering and disentangle neural textures into three meaningful parts. For hair
modeling, we first build a static canonical hair using 3D Gaussian Splatting. A
rigid transformation and an MLP-based deformation field are further applied to
handle complex dynamic expressions. Combined with our occlusion-aware blending,
MeGA generates higher-fidelity renderings for the whole head and naturally
supports more downstream tasks. Experiments on the NeRSemble dataset
demonstrate the effectiveness of our designs, outperforming previous
state-of-the-art methods and supporting various editing functionalities,
including hairstyle alteration and texture editing.

Comments:
- Project page: https://conallwang.github.io/MeGA_Pages/

---

## Embedded Representation Learning Network for Animating Styled Video  Portrait

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Tianyong Wang, Xiangyu Liang, Wangguandong Zheng, Dan Niu, Haifeng Xia, Siyu Xia | cs.CV | [PDF](http://arxiv.org/pdf/2404.19038v1){: .btn .btn-green } |

**Abstract**: The talking head generation recently attracted considerable attention due to
its widespread application prospects, especially for digital avatars and 3D
animation design. Inspired by this practical demand, several works explored
Neural Radiance Fields (NeRF) to synthesize the talking heads. However, these
methods based on NeRF face two challenges: (1) Difficulty in generating
style-controllable talking heads. (2) Displacement artifacts around the neck in
rendered images. To overcome these two challenges, we propose a novel
generative paradigm \textit{Embedded Representation Learning Network} (ERLNet)
with two learning stages. First, the \textit{ audio-driven FLAME} (ADF) module
is constructed to produce facial expression and head pose sequences
synchronized with content audio and style video. Second, given the sequence
deduced by the ADF, one novel \textit{dual-branch fusion NeRF} (DBF-NeRF)
explores these contents to render the final images. Extensive empirical studies
demonstrate that the collaboration of these two stages effectively facilitates
our method to render a more realistic talking head than the existing
algorithms.



---

## DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-29 | Minghao Chen, Iro Laina, Andrea Vedaldi | cs.CV | [PDF](http://arxiv.org/pdf/2404.18929v1){: .btn .btn-green } |

**Abstract**: We consider the problem of editing 3D objects and scenes based on open-ended
language instructions. The established paradigm to solve this problem is to use
a 2D image generator or editor to guide the 3D editing process. However, this
is often slow as it requires do update a computationally expensive 3D
representations such as a neural radiance field, and to do so by using
contradictory guidance from a 2D model which is inherently not multi-view
consistent. We thus introduce the Direct Gaussian Editor (DGE), a method that
addresses these issues in two ways. First, we modify a given high-quality image
editor like InstructPix2Pix to be multi-view consistent. We do so by utilizing
a training-free approach which integrates cues from the underlying 3D geometry
of the scene. Second, given a multi-view consistent edited sequence of images
of the object, we directly and efficiently optimize the 3D object
representation, which is based on 3D Gaussian Splatting. Because it does not
require to apply edits incrementally and iteratively, DGE is significantly more
efficient than existing approaches, and comes with other perks such as allowing
selective editing of parts of the scene.

Comments:
- Project Page: https://silent-chen.github.io/DGE/

---

## S3-SLAM: Sparse Tri-plane Encoding for Neural Implicit SLAM

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-28 | Zhiyao Zhang, Yunzhou Zhang, Yanmin Wu, Bin Zhao, Xingshuo Wang, Rui Tian | cs.CV | [PDF](http://arxiv.org/pdf/2404.18284v1){: .btn .btn-green } |

**Abstract**: With the emergence of Neural Radiance Fields (NeRF), neural implicit
representations have gained widespread applications across various domains,
including simultaneous localization and mapping. However, current neural
implicit SLAM faces a challenging trade-off problem between performance and the
number of parameters. To address this problem, we propose sparse tri-plane
encoding, which efficiently achieves scene reconstruction at resolutions up to
512 using only 2~4% of the commonly used tri-plane parameters (reduced from
100MB to 2~4MB). On this basis, we design S3-SLAM to achieve rapid and
high-quality tracking and mapping through sparsifying plane parameters and
integrating orthogonal features of tri-plane. Furthermore, we develop
hierarchical bundle adjustment to achieve globally consistent geometric
structures and reconstruct high-resolution appearance. Experimental results
demonstrate that our approach achieves competitive tracking and scene
reconstruction with minimal parameters on three datasets. Source code will soon
be available.



---

## DPER: Diffusion Prior Driven Neural Representation for Limited Angle and  Sparse View CT Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-27 | Chenhe Du, Xiyue Lin, Qing Wu, Xuanyu Tian, Ying Su, Zhe Luo, Hongjiang Wei, S. Kevin Zhou, Jingyi Yu, Yuyao Zhang | eess.IV | [PDF](http://arxiv.org/pdf/2404.17890v1){: .btn .btn-green } |

**Abstract**: Limited-angle and sparse-view computed tomography (LACT and SVCT) are crucial
for expanding the scope of X-ray CT applications. However, they face challenges
due to incomplete data acquisition, resulting in diverse artifacts in the
reconstructed CT images. Emerging implicit neural representation (INR)
techniques, such as NeRF, NeAT, and NeRP, have shown promise in
under-determined CT imaging reconstruction tasks. However, the unsupervised
nature of INR architecture imposes limited constraints on the solution space,
particularly for the highly ill-posed reconstruction task posed by LACT and
ultra-SVCT. In this study, we introduce the Diffusion Prior Driven Neural
Representation (DPER), an advanced unsupervised framework designed to address
the exceptionally ill-posed CT reconstruction inverse problems. DPER adopts the
Half Quadratic Splitting (HQS) algorithm to decompose the inverse problem into
data fidelity and distribution prior sub-problems. The two sub-problems are
respectively addressed by INR reconstruction scheme and pre-trained score-based
diffusion model. This combination initially preserves the implicit image local
consistency prior from INR. Additionally, it effectively augments the
feasibility of the solution space for the inverse problem through the
generative diffusion model, resulting in increased stability and precision in
the solutions. We conduct comprehensive experiments to evaluate the performance
of DPER on LACT and ultra-SVCT reconstruction with two public datasets (AAPM
and LIDC). The results show that our method outperforms the state-of-the-art
reconstruction methods on in-domain datasets, while achieving significant
performance improvements on out-of-domain datasets.

Comments:
- 15 pages, 10 figures

---

## SLAM for Indoor Mapping of Wide Area Construction Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-26 | Vincent Ress, Wei Zhang, David Skuddis, Norbert Haala, Uwe Soergel | cs.RO | [PDF](http://arxiv.org/pdf/2404.17215v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping (SLAM), i.e., the reconstruction of the
environment represented by a (3D) map and the concurrent pose estimation, has
made astonishing progress. Meanwhile, large scale applications aiming at the
data collection in complex environments like factory halls or construction
sites are becoming feasible. However, in contrast to small scale scenarios with
building interiors separated to single rooms, shop floors or construction areas
require measures at larger distances in potentially texture less areas under
difficult illumination. Pose estimation is further aggravated since no GNSS
measures are available as it is usual for such indoor applications. In our
work, we realize data collection in a large factory hall by a robot system
equipped with four stereo cameras as well as a 3D laser scanner. We apply our
state-of-the-art LiDAR and visual SLAM approaches and discuss the respective
pros and cons of the different sensor types for trajectory estimation and dense
map generation in such an environment. Additionally, dense and accurate depth
maps are generated by 3D Gaussian splatting, which we plan to use in the
context of our project aiming on the automatic construction and site
monitoring.



---

## Geometry-aware Reconstruction and Fusion-refined Rendering for  Generalizable Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-26 | Tianqi Liu, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2404.17528v1){: .btn .btn-green } |

**Abstract**: Generalizable NeRF aims to synthesize novel views for unseen scenes. Common
practices involve constructing variance-based cost volumes for geometry
reconstruction and encoding 3D descriptors for decoding novel views. However,
existing methods show limited generalization ability in challenging conditions
due to inaccurate geometry, sub-optimal descriptors, and decoding strategies.
We address these issues point by point. First, we find the variance-based cost
volume exhibits failure patterns as the features of pixels corresponding to the
same point can be inconsistent across different views due to occlusions or
reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to
amplify the contribution of consistent pixel pairs and suppress inconsistent
ones. Unlike previous methods that solely fuse 2D features into descriptors,
our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D
context into descriptors through spatial and inter-view interaction. When
decoding the descriptors, we observe the two existing decoding strategies excel
in different areas, which are complementary. A Consistency-Aware Fusion (CAF)
strategy is proposed to leverage the advantages of both. We incorporate the
above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware
Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains
state-of-the-art performance across multiple datasets. Code is available at
https://github.com/TQTQliu/GeFu .

Comments:
- Accepted by CVPR 2024. Project page: https://gefucvpr24.github.io

---

## DIG3D: Marrying Gaussian Splatting with Deformable Transformer for  Single Image 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-25 | Jiamin Wu, Kenkun Liu, Han Gao, Xiaoke Jiang, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.16323v1){: .btn .btn-green } |

**Abstract**: In this paper, we study the problem of 3D reconstruction from a single-view
RGB image and propose a novel approach called DIG3D for 3D object
reconstruction and novel view synthesis. Our method utilizes an encoder-decoder
framework which generates 3D Gaussians in decoder with the guidance of
depth-aware image features from encoder. In particular, we introduce the use of
deformable transformer, allowing efficient and effective decoding through 3D
reference point and multi-layer refinement adaptations. By harnessing the
benefits of 3D Gaussians, our approach offers an efficient and accurate
solution for 3D reconstruction from single-view images. We evaluate our method
on the ShapeNet SRN dataset, getting PSNR of 24.21 and 24.98 in car and chair
dataset, respectively. The result outperforming the recent method by around
2.25%, demonstrating the effectiveness of our method in achieving superior
results.



---

## Interactive3D: Create What You Want by Interactive 3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-25 | Shaocong Dong, Lihe Ding, Zhanpeng Huang, Zibin Wang, Tianfan Xue, Dan Xu | cs.GR | [PDF](http://arxiv.org/pdf/2404.16510v1){: .btn .btn-green } |

**Abstract**: 3D object generation has undergone significant advancements, yielding
high-quality results. However, fall short of achieving precise user control,
often yielding results that do not align with user expectations, thus limiting
their applicability. User-envisioning 3D object generation faces significant
challenges in realizing its concepts using current generative models due to
limited interaction capabilities. Existing methods mainly offer two approaches:
(i) interpreting textual instructions with constrained controllability, or (ii)
reconstructing 3D objects from 2D images. Both of them limit customization to
the confines of the 2D reference and potentially introduce undesirable
artifacts during the 3D lifting process, restricting the scope for direct and
versatile 3D modifications. In this work, we introduce Interactive3D, an
innovative framework for interactive 3D generation that grants users precise
control over the generative process through extensive 3D interaction
capabilities. Interactive3D is constructed in two cascading stages, utilizing
distinct 3D representations. The first stage employs Gaussian Splatting for
direct user interaction, allowing modifications and guidance of the generative
direction at any intermediate step through (i) Adding and Removing components,
(ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv)
Semantic Editing. Subsequently, the Gaussian splats are transformed into
InstantNGP. We introduce a novel (v) Interactive Hash Refinement module to
further add details and extract the geometry in the second stage. Our
experiments demonstrate that Interactive3D markedly improves the
controllability and quality of 3D generation. Our project webpage is available
at \url{https://interactive-3d.github.io/}.

Comments:
- project page: https://interactive-3d.github.io/

---

## Depth Supervised Neural Surface Reconstruction from Airborne Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-25 | Vincent Hackstein, Paul Fauth-Mayer, Matthias Rothermel, Norbert Haala | cs.CV | [PDF](http://arxiv.org/pdf/2404.16429v1){: .btn .btn-green } |

**Abstract**: While originally developed for novel view synthesis, Neural Radiance Fields
(NeRFs) have recently emerged as an alternative to multi-view stereo (MVS).
Triggered by a manifold of research activities, promising results have been
gained especially for texture-less, transparent, and reflecting surfaces, while
such scenarios remain challenging for traditional MVS-based approaches.
However, most of these investigations focus on close-range scenarios, with
studies for airborne scenarios still missing. For this task, NeRFs face
potential difficulties at areas of low image redundancy and weak data evidence,
as often found in street canyons, facades or building shadows. Furthermore,
training such networks is computationally expensive. Thus, the aim of our work
is twofold: First, we investigate the applicability of NeRFs for aerial image
blocks representing different characteristics like nadir-only, oblique and
high-resolution imagery. Second, during these investigations we demonstrate the
benefit of integrating depth priors from tie-point measures, which are provided
during presupposed Bundle Block Adjustment. Our work is based on the
state-of-the-art framework VolSDF, which models 3D scenes by signed distance
functions (SDFs), since this is more applicable for surface reconstruction
compared to the standard volumetric representation in vanilla NeRFs. For
evaluation, the NeRF-based reconstructions are compared to results of a
publicly available benchmark dataset for airborne images.



---

## GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with  Audio-Driven 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-24 | Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2404.16012v2){: .btn .btn-green } |

**Abstract**: We propose GaussianTalker, a novel framework for real-time generation of
pose-controllable talking heads. It leverages the fast rendering capabilities
of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly
controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS
representation of the head and deforms it in sync with the audio. A key insight
is to encode the 3D Gaussian attributes into a shared implicit feature
representation, where it is merged with audio features to manipulate each
Gaussian attribute. This design exploits the spatial-aware features and
enforces interactions between neighboring points. The feature embeddings are
then fed to a spatial-audio attention module, which predicts frame-wise offsets
for the attributes of each Gaussian. It is more stable than previous
concatenation or multiplication approaches for manipulating the numerous
Gaussians and their intricate parameters. Experimental results showcase
GaussianTalker's superiority in facial fidelity, lip synchronization accuracy,
and rendering speed compared to previous methods. Specifically, GaussianTalker
achieves a remarkable rendering speed up to 120 FPS, surpassing previous
benchmarks. Our code is made available at
https://github.com/KU-CVLAB/GaussianTalker/ .

Comments:
- Project Page: https://ku-cvlab.github.io/GaussianTalker

---

## NeRF-XL: Scaling NeRFs with Multiple GPUs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-24 | Ruilong Li, Sanja Fidler, Angjoo Kanazawa, Francis Williams | cs.CV | [PDF](http://arxiv.org/pdf/2404.16221v1){: .btn .btn-green } |

**Abstract**: We present NeRF-XL, a principled method for distributing Neural Radiance
Fields (NeRFs) across multiple GPUs, thus enabling the training and rendering
of NeRFs with an arbitrarily large capacity. We begin by revisiting existing
multi-GPU approaches, which decompose large scenes into multiple independently
trained NeRFs, and identify several fundamental issues with these methods that
hinder improvements in reconstruction quality as additional computational
resources (GPUs) are used in training. NeRF-XL remedies these issues and
enables the training and rendering of NeRFs with an arbitrary number of
parameters by simply using more hardware. At the core of our method lies a
novel distributed training and rendering formulation, which is mathematically
equivalent to the classic single-GPU case and minimizes communication between
GPUs. By unlocking NeRFs with arbitrarily large parameter counts, our approach
is the first to reveal multi-GPU scaling laws for NeRFs, showing improvements
in reconstruction quality with larger parameter counts and speed improvements
with more GPUs. We demonstrate the effectiveness of NeRF-XL on a wide variety
of datasets, including the largest open-source dataset to date, MatrixCity,
containing 258K images covering a 25km^2 city area.

Comments:
- Webpage: https://research.nvidia.com/labs/toronto-ai/nerfxl/

---

## OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian  Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-24 | Lizhi Wang, Feng Zhou, Jianqin Yin | cs.CV | [PDF](http://arxiv.org/pdf/2404.15891v2){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D reconstruction technologies have paved the way for
high-quality and real-time rendering of complex 3D scenes. Despite these
achievements, a notable challenge persists: it is difficult to precisely
reconstruct specific objects from large scenes. Current scene reconstruction
techniques frequently result in the loss of object detail textures and are
unable to reconstruct object portions that are occluded or unseen in views. To
address this challenge, we delve into the meticulous 3D reconstruction of
specific objects within large scenes and propose a framework termed OMEGAS:
Object Mesh Extraction from Large Scenes Guided by GAussian Segmentation.
OMEGAS employs a multi-step approach, grounded in several excellent
off-the-shelf methodologies. Specifically, initially, we utilize the Segment
Anything Model (SAM) to guide the segmentation of 3D Gaussian Splatting (3DGS),
thereby creating a basic 3DGS model of the target object. Then, we leverage
large-scale diffusion priors to further refine the details of the 3DGS model,
especially aimed at addressing invisible or occluded object portions from the
original scene views. Subsequently, by re-rendering the 3DGS model onto the
scene views, we achieve accurate object segmentation and effectively remove the
background. Finally, these target-only images are used to improve the 3DGS
model further and extract the definitive 3D object mesh by the SuGaR model. In
various scenarios, our experiments demonstrate that OMEGAS significantly
surpasses existing scene reconstruction methods. Our project page is at:
https://github.com/CrystalWlz/OMEGAS

Comments:
- arXiv admin note: text overlap with arXiv:2311.17061 by other authors

---

## ESR-NeRF: Emissive Source Reconstruction Using LDR Multi-view Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-24 | Jinseo Jeong, Junseo Koo, Qimeng Zhang, Gunhee Kim | cs.CV | [PDF](http://arxiv.org/pdf/2404.15707v1){: .btn .btn-green } |

**Abstract**: Existing NeRF-based inverse rendering methods suppose that scenes are
exclusively illuminated by distant light sources, neglecting the potential
influence of emissive sources within a scene. In this work, we confront this
limitation using LDR multi-view images captured with emissive sources turned on
and off. Two key issues must be addressed: 1) ambiguity arising from the
limited dynamic range along with unknown lighting details, and 2) the expensive
computational cost in volume rendering to backtrace the paths leading to final
object colors. We present a novel approach, ESR-NeRF, leveraging neural
networks as learnable functions to represent ray-traced fields. By training
networks to satisfy light transport segments, we regulate outgoing radiances,
progressively identifying emissive sources while being aware of reflection
areas. The results on scenes encompassing emissive sources with various
properties demonstrate the superiority of ESR-NeRF in qualitative and
quantitative ways. Our approach also extends its applicability to the scenes
devoid of emissive sources, achieving lower CD metrics on the DTU dataset.

Comments:
- CVPR 2024

---

## TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-23 | Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu | cs.CV | [PDF](http://arxiv.org/pdf/2404.15264v1){: .btn .btn-green } |

**Abstract**: Radiance fields have demonstrated impressive performance in synthesizing
lifelike 3D talking heads. However, due to the difficulty in fitting steep
appearance changes, the prevailing paradigm that presents facial motions by
directly modifying point appearance may lead to distortions in dynamic regions.
To tackle this challenge, we introduce TalkingGaussian, a deformation-based
radiance fields framework for high-fidelity talking head synthesis. Leveraging
the point-based Gaussian Splatting, facial motions can be represented in our
method by applying smooth and continuous deformations to persistent Gaussian
primitives, without requiring to learn the difficult appearance change like
previous methods. Due to this simplification, precise facial motions can be
synthesized while keeping a highly intact facial feature. Under such a
deformation paradigm, we further identify a face-mouth motion inconsistency
that would affect the learning of detailed speaking motions. To address this
conflict, we decompose the model into two branches separately for the face and
inside mouth areas, therefore simplifying the learning tasks to help
reconstruct more accurate motion and structure of the mouth region. Extensive
experiments demonstrate that our method renders high-quality lip-synchronized
talking head videos, with better facial fidelity and higher efficiency compared
with previous methods.

Comments:
- Project page: https://fictionarry.github.io/TalkingGaussian/

---

## FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient  Descent

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-23 | Cameron Smith, David Charatan, Ayush Tewari, Vincent Sitzmann | cs.CV | [PDF](http://arxiv.org/pdf/2404.15259v1){: .btn .btn-green } |

**Abstract**: This paper introduces FlowMap, an end-to-end differentiable method that
solves for precise camera poses, camera intrinsics, and per-frame dense depth
of a video sequence. Our method performs per-video gradient-descent
minimization of a simple least-squares objective that compares the optical flow
induced by depth, intrinsics, and poses against correspondences obtained via
off-the-shelf optical flow and point tracking. Alongside the use of point
tracks to encourage long-term geometric consistency, we introduce
differentiable re-parameterizations of depth, intrinsics, and pose that are
amenable to first-order optimization. We empirically show that camera
parameters and dense depth recovered by our method enable photo-realistic novel
view synthesis on 360-degree trajectories using Gaussian Splatting. Our method
not only far outperforms prior gradient-descent based bundle adjustment
methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM
method, on the downstream task of 360-degree novel view synthesis (even though
our method is purely gradient-descent based, fully differentiable, and presents
a complete departure from conventional SfM).

Comments:
- Project website: https://cameronosmith.github.io/flowmap/

---

## DreamCraft: Text-Guided Generation of Functional 3D Environments in  Minecraft

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-23 | Sam Earle, Filippos Kokkinos, Yuhe Nie, Julian Togelius, Roberta Raileanu | cs.GR | [PDF](http://arxiv.org/pdf/2404.15538v1){: .btn .btn-green } |

**Abstract**: Procedural Content Generation (PCG) algorithms enable the automatic
generation of complex and diverse artifacts. However, they don't provide
high-level control over the generated content and typically require domain
expertise. In contrast, text-to-3D methods allow users to specify desired
characteristics in natural language, offering a high amount of flexibility and
expressivity. But unlike PCG, such approaches cannot guarantee functionality,
which is crucial for certain applications like game design. In this paper, we
present a method for generating functional 3D artifacts from free-form text
prompts in the open-world game Minecraft. Our method, DreamCraft, trains
quantized Neural Radiance Fields (NeRFs) to represent artifacts that, when
viewed in-game, match given text descriptions. We find that DreamCraft produces
more aligned in-game artifacts than a baseline that post-processes the output
of an unconstrained NeRF. Thanks to the quantized representation of the
environment, functional constraints can be integrated using specialized loss
terms. We show how this can be leveraged to generate 3D structures that match a
target distribution or obey certain adjacency rules over the block types.
DreamCraft inherits a high degree of expressivity and controllability from the
NeRF, while still being able to incorporate functional constraints through
domain-specific objectives.

Comments:
- 16 pages, 9 figures, accepted to Foundation of Digital Games 2024

---

## Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D  Glimpses


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-22 | Inhee Lee, Byungjun Kim, Hanbyul Joo | cs.CV | [PDF](http://arxiv.org/pdf/2404.14410v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a method to reconstruct the world and multiple
dynamic humans in 3D from a monocular video input. As a key idea, we represent
both the world and multiple humans via the recently emerging 3D Gaussian
Splatting (3D-GS) representation, enabling to conveniently and efficiently
compose and render them together. In particular, we address the scenarios with
severely limited and sparse observations in 3D human reconstruction, a common
challenge encountered in the real world. To tackle this challenge, we introduce
a novel approach to optimize the 3D-GS representation in a canonical space by
fusing the sparse cues in the common space, where we leverage a pre-trained 2D
diffusion model to synthesize unseen views while keeping the consistency with
the observed 2D appearances. We demonstrate our method can reconstruct
high-quality animatable 3D humans in various challenging examples, in the
presence of occlusion, image crops, few-shot, and extremely sparse
observations. After reconstruction, our method is capable of not only rendering
the scene in any novel views at arbitrary time instances, but also editing the
3D scene by removing individual humans or applying different motions for each
human. Through various experiments, we demonstrate the quality and efficiency
of our methods over alternative existing approaches.

Comments:
- The project page is available at https://snuvclab.github.io/gtu/

---

## Neural Radiance Field in Autonomous Driving: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-22 | Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2404.13816v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has garnered significant attention from both
academia and industry due to its intrinsic advantages, particularly its
implicit representation and novel view synthesis capabilities. With the rapid
advancements in deep learning, a multitude of methods have emerged to explore
the potential applications of NeRF in the domain of Autonomous Driving (AD).
However, a conspicuous void is apparent within the current literature. To
bridge this gap, this paper conducts a comprehensive survey of NeRF's
applications in the context of AD. Our survey is structured to categorize
NeRF's applications in Autonomous Driving (AD), specifically encompassing
perception, 3D reconstruction, simultaneous localization and mapping (SLAM),
and simulation. We delve into in-depth analysis and summarize the findings for
each application category, and conclude by providing insights and discussions
on future directions in this field. We hope this paper serves as a
comprehensive reference for researchers in this domain. To the best of our
knowledge, this is the first survey specifically focused on the applications of
NeRF in the Autonomous Driving domain.



---

## CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and  View-consistent 3D Semantic Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-22 | Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.14249v1){: .btn .btn-green } |

**Abstract**: The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time
synthesis of novel views in 3D scenes. Currently, it primarily focuses on
geometry and appearance modeling, while lacking the semantic understanding of
scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from
Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to
efficiently comprehend 3D environments without annotated semantic data. In
specific, rather than straightforwardly learning and rendering high-dimensional
semantic features of 3D Gaussians, which significantly diminishes the
efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC
exploits the inherent unified semantics within objects to learn compact yet
effective semantic representations of 3D Gaussians, enabling highly efficient
rendering (>100 FPS). Additionally, to address the semantic ambiguity, caused
by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we
introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the
multi-view consistency originated from the 3D model. 3DCS imposes cross-view
semantic consistency constraints by leveraging refined, self-predicted
pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing
precise and view-consistent segmentation results. Extensive experiments
demonstrate that our method remarkably outperforms existing state-of-the-art
approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on
Replica and ScanNet datasets, respectively, while maintaining real-time
rendering speed. Furthermore, our approach exhibits superior performance even
with sparse input data, verifying the robustness of our method.

Comments:
- https://github.com/gbliao/CLIP-GS

---

## NeRF-DetS: Enhancing Multi-View 3D Object Detection with  Sampling-adaptive Network of Continuous NeRF-based Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-22 | Chi Huang, Xinyang Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji | cs.CV | [PDF](http://arxiv.org/pdf/2404.13921v1){: .btn .btn-green } |

**Abstract**: As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and
3D perception, demonstrating that perceptual tasks can benefit from novel view
synthesis methods like NeRF, significantly improving the performance of indoor
multi-view 3D object detection. Using the geometry MLP of NeRF to direct the
attention of detection head to crucial parts and incorporating self-supervised
loss from novel view rendering contribute to the achieved improvement. To
better leverage the notable advantages of the continuous representation through
neural rendering in space, we introduce a novel 3D perception network
structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level
Sampling-Adaptive Network, making the sampling process adaptively from coarse
to fine. Also, we propose a superior multi-view information fusion method,
known as Multi-head Weighted Fusion. This fusion approach efficiently addresses
the challenge of losing multi-view information when using arithmetic mean,
while keeping low computational costs. NeRF-DetS outperforms competitive
NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement
in mAP@.25 and mAP@.50, respectively.



---

## GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian  Splatting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-22 | Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.14037v2){: .btn .btn-green } |

**Abstract**: Recent works on audio-driven talking head synthesis using Neural Radiance
Fields (NeRF) have achieved impressive results. However, due to inadequate pose
and expression control caused by NeRF implicit representation, these methods
still have some limitations, such as unsynchronized or unnatural lip movements,
and visual jitter and artifacts. In this paper, we propose GaussianTalker, a
novel method for audio-driven talking head synthesis based on 3D Gaussian
Splatting. With the explicit representation property of 3D Gaussians, intuitive
control of the facial motion is achieved by binding Gaussians to 3D facial
models. GaussianTalker consists of two modules, Speaker-specific Motion
Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator
achieves accurate lip movements specific to the target speaker through
universalized audio feature extraction and customized lip motion generation.
Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance
facial detail representation via a latent pose, delivering stable and realistic
rendered videos. Extensive experimental results suggest that GaussianTalker
outperforms existing state-of-the-art methods in talking head synthesis,
delivering precise lip synchronization and exceptional visual quality. Our
method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU,
significantly exceeding the threshold for real-time rendering performance, and
can potentially be deployed on other hardware platforms.

Comments:
- https://yuhongyun777.github.io/GaussianTalker/

---

## CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with  Complex Trajectory

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-22 | Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao Sun, Jiming Chen | cs.CV | [PDF](http://arxiv.org/pdf/2404.13896v2){: .btn .btn-green } |

**Abstract**: Neural radiance field (NeRF) has achieved impressive results in high-quality
3D scene reconstruction. However, NeRF heavily relies on precise camera poses.
While recent works like BARF have introduced camera pose optimization within
NeRF, their applicability is limited to simple trajectory scenes. Existing
methods struggle while tackling complex trajectories involving large rotations.
To address this limitation, we propose CT-NeRF, an incremental reconstruction
optimization pipeline using only RGB images without pose and depth input. In
this pipeline, we first propose a local-global bundle adjustment under a pose
graph connecting neighboring frames to enforce the consistency between poses to
escape the local minima caused by only pose consistency with the scene
structure. Further, we instantiate the consistency between poses as a
reprojected geometric image distance constraint resulting from pixel-level
correspondences between input image pairs. Through the incremental
reconstruction, CT-NeRF enables the recovery of both camera poses and scene
structure and is capable of handling scenes with complex trajectories. We
evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and
Free-Dataset, which feature complex trajectories. Results show CT-NeRF
outperforms existing methods in novel view synthesis and pose estimation
accuracy.



---

## ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-21 | Zichen Tang, Hongyu Yang | cs.CV | [PDF](http://arxiv.org/pdf/2404.13711v2){: .btn .btn-green } |

**Abstract**: Recent advances in generative visual models and neural radiance fields have
greatly boosted 3D-aware image synthesis and stylization tasks. However,
previous NeRF-based work is limited to single scene stylization, training a
model to generate 3D-aware cartoon faces with arbitrary styles remains
unsolved. We propose ArtNeRF, a novel face stylization framework derived from
3D-aware GAN to tackle this problem. In this framework, we utilize an
expressive generator to synthesize stylized faces and a triple-branch
discriminator module to improve the visual quality and style consistency of the
generated faces. Specifically, a style encoder based on contrastive learning is
leveraged to extract robust low-dimensional embeddings of style images,
empowering the generator with the knowledge of various styles. To smooth the
training process of cross-domain transfer learning, we propose an adaptive
style blending module which helps inject style information and allows users to
freely tune the level of stylization. We further introduce a neural rendering
module to achieve efficient real-time rendering of images with higher
resolutions. Extensive experiments demonstrate that ArtNeRF is versatile in
generating high-quality 3D-aware cartoon faces with arbitrary styles.



---

## Generalizable Novel-View Synthesis using a Stereo Camera

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-21 | Haechan Lee, Wonjoon Jin, Seung-Hwan Baek, Sunghyun Cho | cs.CV | [PDF](http://arxiv.org/pdf/2404.13541v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose the first generalizable view synthesis approach
that specifically targets multi-view stereo-camera images. Since recent stereo
matching has demonstrated accurate geometry prediction, we introduce stereo
matching into novel-view synthesis for high-quality geometry reconstruction. To
this end, this paper proposes a novel framework, dubbed StereoNeRF, which
integrates stereo matching into a NeRF-based generalizable view synthesis
approach. StereoNeRF is equipped with three key components to effectively
exploit stereo matching in novel-view synthesis: a stereo feature extractor, a
depth-guided plane-sweeping, and a stereo depth loss. Moreover, we propose the
StereoNVS dataset, the first multi-view dataset of stereo-camera images,
encompassing a wide variety of both real and synthetic scenes. Our experimental
results demonstrate that StereoNeRF surpasses previous approaches in
generalizable view synthesis.

Comments:
- Accepted to CVPR 2024. Project page URL:
  https://jinwonjoon.github.io/stereonerf/

---

## GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting  for Object Removal

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-21 | Yuxin Wang, Qianyi Wu, Guofeng Zhang, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.13679v1){: .btn .btn-green } |

**Abstract**: This paper tackles the intricate challenge of object removal to update the
radiance field using the 3D Gaussian Splatting. The main challenges of this
task lie in the preservation of geometric consistency and the maintenance of
texture coherence in the presence of the substantial discrete nature of
Gaussian primitives. We introduce a robust framework specifically designed to
overcome these obstacles. The key insight of our approach is the enhancement of
information exchange among visible and invisible areas, facilitating content
restoration in terms of both geometry and texture. Our methodology begins with
optimizing the positioning of Gaussian primitives to improve geometric
consistency across both removed and visible areas, guided by an online
registration process informed by monocular depth estimation. Following this, we
employ a novel feature propagation mechanism to bolster texture coherence,
leveraging a cross-attention design that bridges sampling Gaussians from both
uncertain and certain areas. This innovative approach significantly refines the
texture coherence within the final radiance field. Extensive experiments
validate that our method not only elevates the quality of novel view synthesis
for scenes undergoing object removal but also showcases notable efficiency
gains in training and rendering speeds.

Comments:
- Project Page: https://w-ted.github.io/publications/gscream

---

## EC-SLAM: Real-time Dense Neural RGB-D SLAM System with Effectively  Constrained Global Bundle Adjustment

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-20 | Guanghao Li, Qi Chen, YuXiang Yan, Jian Pu | cs.RO | [PDF](http://arxiv.org/pdf/2404.13346v1){: .btn .btn-green } |

**Abstract**: We introduce EC-SLAM, a real-time dense RGB-D simultaneous localization and
mapping (SLAM) system utilizing Neural Radiance Fields (NeRF). Although recent
NeRF-based SLAM systems have demonstrated encouraging outcomes, they have yet
to completely leverage NeRF's capability to constrain pose optimization. By
employing an effectively constrained global bundle adjustment (BA) strategy,
our system makes use of NeRF's implicit loop closure correction capability.
This improves the tracking accuracy by reinforcing the constraints on the
keyframes that are most pertinent to the optimized current frame. In addition,
by implementing a feature-based and uniform sampling strategy that minimizes
the number of ineffective constraint points for pose optimization, we mitigate
the effects of random sampling in NeRF. EC-SLAM utilizes sparse parametric
encodings and the truncated signed distance field (TSDF) to represent the map
in order to facilitate efficient fusion, resulting in reduced model parameters
and accelerated convergence velocity. A comprehensive evaluation conducted on
the Replica, ScanNet, and TUM datasets showcases cutting-edge performance,
including enhanced reconstruction accuracy resulting from precise pose
estimation, 21 Hz run time, and tracking precision improvements of up to 50\%.
The source code is available at https://github.com/Lightingooo/EC-SLAM.



---

## High-fidelity Endoscopic Image Synthesis by Utilizing Depth-guided  Neural Surfaces

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-20 | Baoru Huang, Yida Wang, Anh Nguyen, Daniel Elson, Francisco Vasconcelos, Danail Stoyanov | cs.CV | [PDF](http://arxiv.org/pdf/2404.13437v1){: .btn .btn-green } |

**Abstract**: In surgical oncology, screening colonoscopy plays a pivotal role in providing
diagnostic assistance, such as biopsy, and facilitating surgical navigation,
particularly in polyp detection. Computer-assisted endoscopic surgery has
recently gained attention and amalgamated various 3D computer vision
techniques, including camera localization, depth estimation, surface
reconstruction, etc. Neural Radiance Fields (NeRFs) and Neural Implicit
Surfaces (NeuS) have emerged as promising methodologies for deriving accurate
3D surface models from sets of registered images, addressing the limitations of
existing colon reconstruction approaches stemming from constrained camera
movement.
  However, the inadequate tissue texture representation and confused scale
problem in monocular colonoscopic image reconstruction still impede the
progress of the final rendering results. In this paper, we introduce a novel
method for colon section reconstruction by leveraging NeuS applied to
endoscopic images, supplemented by a single frame of depth map. Notably, we
pioneered the exploration of utilizing only one frame depth map in
photorealistic reconstruction and neural rendering applications while this
single depth map can be easily obtainable from other monocular depth estimation
networks with an object scale. Through rigorous experimentation and validation
on phantom imagery, our approach demonstrates exceptional accuracy in
completely rendering colon sections, even capturing unseen portions of the
surface. This breakthrough opens avenues for achieving stable and consistently
scaled reconstructions, promising enhanced quality in cancer screening
procedures and treatment interventions.



---

## EfficientGS: Streamlining Gaussian Splatting for Large-Scale  High-Resolution Scene Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-19 | Wenkai Liu, Tao Guan, Bin Zhu, Lili Ju, Zikai Song, Dan Li, Yuesong Wang, Wei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2404.12777v1){: .btn .btn-green } |

**Abstract**: In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has
emerged as a pivotal technology. However, its application to large-scale,
high-resolution scenes (exceeding 4k$\times$4k pixels) is hindered by the
excessive computational requirements for managing a large number of Gaussians.
Addressing this, we introduce 'EfficientGS', an advanced approach that
optimizes 3DGS for high-resolution, large-scale scenes. We analyze the
densification process in 3DGS and identify areas of Gaussian
over-proliferation. We propose a selective strategy, limiting Gaussian increase
to key primitives, thereby enhancing the representational efficiency.
Additionally, we develop a pruning mechanism to remove redundant Gaussians,
those that are merely auxiliary to adjacent ones. For further enhancement, we
integrate a sparse order increment for Spherical Harmonics (SH), designed to
alleviate storage constraints and reduce training overhead. Our empirical
evaluations, conducted on a range of datasets including extensive 4K+ aerial
images, demonstrate that 'EfficientGS' not only expedites training and
rendering times but also achieves this with a model size approximately tenfold
smaller than conventional 3DGS while maintaining high rendering fidelity.



---

## FlyNeRF: NeRF-Based Aerial Mapping for High-Quality 3D Scene  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-19 | Maria Dronova, Vladislav Cheremnykh, Alexey Kotcov, Aleksey Fedoseev, Dzmitry Tsetserukou | cs.RO | [PDF](http://arxiv.org/pdf/2404.12970v1){: .btn .btn-green } |

**Abstract**: Current methods for 3D reconstruction and environmental mapping frequently
face challenges in achieving high precision, highlighting the need for
practical and effective solutions. In response to this issue, our study
introduces FlyNeRF, a system integrating Neural Radiance Fields (NeRF) with
drone-based data acquisition for high-quality 3D reconstruction. Utilizing
unmanned aerial vehicle (UAV) for capturing images and corresponding spatial
coordinates, the obtained data is subsequently used for the initial NeRF-based
3D reconstruction of the environment. Further evaluation of the reconstruction
render quality is accomplished by the image evaluation neural network developed
within the scope of our system. According to the results of the image
evaluation module, an autonomous algorithm determines the position for
additional image capture, thereby improving the reconstruction quality. The
neural network introduced for render quality assessment demonstrates an
accuracy of 97%. Furthermore, our adaptive methodology enhances the overall
reconstruction quality, resulting in an average improvement of 2.5 dB in Peak
Signal-to-Noise Ratio (PSNR) for the 10% quantile. The FlyNeRF demonstrates
promising results, offering advancements in such fields as environmental
monitoring, surveillance, and digital twins, where high-fidelity 3D
reconstructions are crucial.



---

## Learn2Talk: 3D Talking Face Learns from 2D Talking Face

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-19 | Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin | cs.CV | [PDF](http://arxiv.org/pdf/2404.12888v1){: .btn .btn-green } |

**Abstract**: Speech-driven facial animation methods usually contain two main classes, 3D
and 2D talking face, both of which attract considerable research attention in
recent years. However, to the best of our knowledge, the research on 3D talking
face does not go deeper as 2D talking face, in the aspect of
lip-synchronization (lip-sync) and speech perception. To mind the gap between
the two sub-fields, we propose a learning framework named Learn2Talk, which can
construct a better 3D talking face network by exploiting two expertise points
from the field of 2D talking face. Firstly, inspired by the audio-video sync
network, a 3D sync-lip expert model is devised for the pursuit of lip-sync
between audio and 3D facial motion. Secondly, a teacher model selected from 2D
talking face methods is used to guide the training of the audio-to-3D motions
regression network to yield more 3D vertex accuracy. Extensive experiments show
the advantages of the proposed framework in terms of lip-sync, vertex accuracy
and speech perception, compared with state-of-the-arts. Finally, we show two
applications of the proposed framework: audio-visual speech recognition and
speech-driven 3D Gaussian Splatting based avatar animation.



---

## Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-18 | Isabella Liu, Hao Su, Xiaolong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.12379v2){: .btn .btn-green } |

**Abstract**: Modern 3D engines and graphics pipelines require mesh as a memory-efficient
representation, which allows efficient rendering, geometry processing, texture
editing, and many other downstream operations. However, it is still highly
difficult to obtain high-quality mesh in terms of structure and detail from
monocular visual observations. The problem becomes even more challenging for
dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh
(DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh
given a single monocular video. Our work leverages the recent advancement in 3D
Gaussian Splatting to construct the mesh sequence with temporal consistency
from a video. Building on top of this representation, DG-Mesh recovers
high-quality meshes from the Gaussian points and can track the mesh vertices
over time, which enables applications such as texture editing on dynamic
objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly
distributed Gaussians, resulting better mesh reconstruction through mesh-guided
densification and pruning on the deformed Gaussians. By applying
cycle-consistent deformation between the canonical and the deformed space, we
can project the anchored Gaussian back to the canonical space and optimize
Gaussians across all time frames. During the evaluation on different datasets,
DG-Mesh provides significantly better mesh reconstruction and rendering than
baselines. Project page: https://www.liuisabella.com/DG-Mesh/

Comments:
- Project page: https://www.liuisabella.com/DG-Mesh/

---

## Cicero: Addressing Algorithmic and Architectural Bottlenecks in Neural  Rendering by Radiance Warping and Memory Optimizations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-18 | Yu Feng, Zihan Liu, Jingwen Leng, Minyi Guo, Yuhao Zhu | cs.AR | [PDF](http://arxiv.org/pdf/2404.11852v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) is widely seen as an alternative to traditional
physically-based rendering. However, NeRF has not yet seen its adoption in
resource-limited mobile systems such as Virtual and Augmented Reality (VR/AR),
because it is simply extremely slow. On a mobile Volta GPU, even the
state-of-the-art NeRF models generally execute only at 0.8 FPS. We show that
the main performance bottlenecks are both algorithmic and architectural. We
introduce, CICERO, to tame both forms of inefficiencies. We first introduce two
algorithms, one fundamentally reduces the amount of work any NeRF model has to
execute, and the other eliminates irregular DRAM accesses. We then describe an
on-chip data layout strategy that eliminates SRAM bank conflicts. A pure
software implementation of CICERO offers an 8.0x speed-up and 7.9x energy
saving over a mobile Volta GPU. When compared to a baseline with a dedicated
DNN accelerator, our speed-up and energy reduction increase to 28.2x and 37.8x,
respectively - all with minimal quality loss (less than 1.0 dB peak
signal-to-noise ratio reduction).



---

## MeshLRM: Large Reconstruction Model for High-Quality Mesh

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-18 | Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, Zexiang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.12385v1){: .btn .btn-green } |

**Abstract**: We propose MeshLRM, a novel LRM-based approach that can reconstruct a
high-quality mesh from merely four input images in less than one second.
Different from previous large reconstruction models (LRMs) that focus on
NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction
and rendering within the LRM framework. This allows for end-to-end mesh
reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.
Moreover, we improve the LRM architecture by simplifying several complex
designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained
with low- and high-resolution images; this new LRM training strategy enables
significantly faster convergence and thereby leads to better quality with less
compute. Our approach achieves state-of-the-art mesh reconstruction from
sparse-view inputs and also allows for many downstream applications, including
text-to-3D and single-image-to-3D generation. Project page:
https://sarahweiii.github.io/meshlrm/



---

## Does Gaussian Splatting need SFM Initialization?

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-18 | Yalda Foroutan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2404.12547v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently been embraced as a versatile and effective
method for scene reconstruction and novel view synthesis, owing to its
high-quality results and compatibility with hardware rasterization. Despite its
advantages, Gaussian Splatting's reliance on high-quality point cloud
initialization by Structure-from-Motion (SFM) algorithms is a significant
limitation to be overcome. To this end, we investigate various initialization
strategies for Gaussian Splatting and delve into how volumetric reconstructions
from Neural Radiance Fields (NeRF) can be utilized to bypass the dependency on
SFM data. Our findings demonstrate that random initialization can perform much
better if carefully designed and that by employing a combination of improved
initialization strategies and structure distillation from low-cost NeRF models,
it is possible to achieve equivalent results, or at times even superior, to
those obtained from SFM initialization.

Comments:
- 14 pages, 6 figures

---

## AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height  Large-scale Outdoor Scene Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-18 | Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.11897v1){: .btn .btn-green } |

**Abstract**: Existing neural radiance fields (NeRF)-based novel view synthesis methods for
large-scale outdoor scenes are mainly built on a single altitude. Moreover,
they often require a priori camera shooting height and scene scope, leading to
inefficient and impractical applications when camera altitude changes. In this
work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce
the training cost of building good reconstructions by synthesizing
free-viewpoint images based on varying altitudes of scenes. Specifically, to
tackle the detail variation problem from low altitude (drone-level) to high
altitude (satellite-level), a source image selection method and an
attention-based feature fusion approach are developed to extract and fuse the
most relevant features of target view from multi-height images for
high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF
achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only
requires a half hour of training time to reach the competitive PSNR as compared
to the latest BungeeNeRF.



---

## Novel View Synthesis for Cinematic Anatomy on Mobile and Immersive  Displays

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-17 | Simon Niedermayr, Christoph Neuhauser, Kaloian Petkov, Klaus Engel, Rdiger Westermann | cs.GR | [PDF](http://arxiv.org/pdf/2404.11285v1){: .btn .btn-green } |

**Abstract**: Interactive photorealistic visualization of 3D anatomy (i.e., Cinematic
Anatomy) is used in medical education to explain the structure of the human
body. It is currently restricted to frontal teaching scenarios, where the
demonstrator needs a powerful GPU and high-speed access to a large storage
device where the dataset is hosted. We demonstrate the use of novel view
synthesis via compressed 3D Gaussian splatting to overcome this restriction and
to enable students to perform cinematic anatomy on lightweight mobile devices
and in virtual reality environments. We present an automatic approach for
finding a set of images that captures all potentially seen structures in the
data. By mixing closeup views with images from a distance, the splat
representation can recover structures up to the voxel resolution. The use of
Mip-Splatting enables smooth transitions when the focal length is increased.
Even for GB datasets, the final renderable representation can usually be
compressed to less than 70 MB, enabling interactive rendering on low-end
devices using rasterization.



---

## DeblurGS: Gaussian Splatting for Camera Motion Blur

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-17 | Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee | cs.CV | [PDF](http://arxiv.org/pdf/2404.11358v2){: .btn .btn-green } |

**Abstract**: Although significant progress has been made in reconstructing sharp 3D scenes
from motion-blurred images, a transition to real-world applications remains
challenging. The primary obstacle stems from the severe blur which leads to
inaccuracies in the acquisition of initial camera poses through
Structure-from-Motion, a critical aspect often overlooked by previous
approaches. To address this challenge, we propose DeblurGS, a method to
optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the
noisy camera pose initialization. We restore a fine-grained sharp scene by
leveraging the remarkable reconstruction capability of 3D Gaussian Splatting.
Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry
observation and synthesizes corresponding blurry renderings for the
optimization process. Furthermore, we propose Gaussian Densification Annealing
strategy to prevent the generation of inaccurate Gaussians at erroneous
locations during the early training stages when camera motion is still
imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves
state-of-the-art performance in deblurring and novel view synthesis for
real-world and synthetic benchmark datasets, as well as field-captured blurry
smartphone videos.



---

## RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled  Neural Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-17 | Xianqiang Lyu, Hui Liu, Junhui Hou | cs.CV | [PDF](http://arxiv.org/pdf/2404.11401v1){: .btn .btn-green } |

**Abstract**: We propose RainyScape, an unsupervised framework for reconstructing clean
scenes from a collection of multi-view rainy images. RainyScape consists of two
main modules: a neural rendering module and a rain-prediction module that
incorporates a predictor network and a learnable latent embedding that captures
the rain characteristics of the scene. Specifically, based on the spectral bias
property of neural networks, we first optimize the neural rendering pipeline to
obtain a low-frequency scene representation. Subsequently, we jointly optimize
the two modules, driven by the proposed adaptive direction-sensitive
gradient-based reconstruction loss, which encourages the network to distinguish
between scene details and rain streaks, facilitating the propagation of
gradients to the relevant components. Extensive experiments on both the classic
neural radiance field and the recently proposed 3D Gaussian splatting
demonstrate the superiority of our method in effectively eliminating rain
streaks and rendering clean images, achieving state-of-the-art performance. The
constructed high-quality dataset and source code will be publicly available.



---

## SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-17 | Vincent Cartillier, Grant Schindler, Irfan Essa | cs.CV | [PDF](http://arxiv.org/pdf/2404.11419v1){: .btn .btn-green } |

**Abstract**: We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose
a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM
(NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing
NeRF-SLAM systems consistently exhibit inferior tracking performance compared
to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via
image alignment and photometric bundle-adjustment. Such optimization processes
are difficult to optimize due to the narrow basin of attraction of the
optimization loss in image space (local minima) and the lack of initial
correspondences. We mitigate these limitations by implementing a Gaussian
pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking
optimization strategy. Furthermore, NeRF systems encounter challenges in
converging to the right geometry with limited input views. While prior
approaches use a Signed-Distance Function (SDF)-based NeRF and directly
supervise SDF values by approximating ground truth SDF through depth
measurements, this often results in suboptimal geometry. In contrast, our
method employs a volume density representation and introduces a novel KL
regularizer on the ray termination distribution, constraining scene geometry to
consist of empty space and opaque surfaces. Our solution implements both local
and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate
(KL regularizer) SLAM solution. We conduct experiments on multiple datasets
(ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in
reconstruction accuracy.



---

## SRGS: Super-Resolution 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Xiang Feng, Yongbo He, Yubo Wang, Yan Yang, Zhenzhong Kuang, Yu Jun, Jianping Fan, Jiajun ding | cs.CV | [PDF](http://arxiv.org/pdf/2404.10318v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel
explicit 3D representation. This approach relies on the representation power of
Gaussian primitives to provide a high-quality rendering. However, primitives
optimized at low resolution inevitably exhibit sparsity and texture deficiency,
posing a challenge for achieving high-resolution novel view synthesis (HRNVS).
To address this problem, we propose Super-Resolution 3D Gaussian Splatting
(SRGS) to perform the optimization in a high-resolution (HR) space. The
sub-pixel constraint is introduced for the increased viewpoints in HR space,
exploiting the sub-pixel cross-view information of the multiple low-resolution
(LR) views. The gradient accumulated from more viewpoints will facilitate the
densification of primitives. Furthermore, a pre-trained 2D super-resolution
model is integrated with the sub-pixel constraint, enabling these dense
primitives to learn faithful texture features. In general, our method focuses
on densification and texture learning to effectively enhance the representation
ability of primitives. Experimentally, our method achieves high rendering
quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on
challenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes
will be released upon acceptance.

Comments:
- submit ACM MM 2024

---

## 1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Hang Du, Yaping Xue, Weidong Dai, Xuejun Yan, Jingjing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.10441v1){: .btn .btn-green } |

**Abstract**: In this report, we present the 1st place solution for ICCV 2023 OmniObject3D
Challenge: Sparse-View Reconstruction. The challenge aims to evaluate
approaches for novel view synthesis and surface reconstruction using only a few
posed images of each object. We utilize Pixel-NeRF as the basic model, and
apply depth supervision as well as coarse-to-fine positional encoding. The
experiments demonstrate the effectiveness of our approach in improving
sparse-view reconstruction quality. We ranked first in the final test with a
PSNR of 25.44614.



---

## Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in  Unbounded Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Zehao Yu, Torsten Sattler, Andreas Geiger | cs.CV | [PDF](http://arxiv.org/pdf/2404.10772v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view
synthesis results, while allowing the rendering of high-resolution images in
real-time. However, leveraging 3D Gaussians for surface reconstruction poses
significant challenges due to the explicit and disconnected nature of 3D
Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel
approach for efficient, high-quality, and compact surface reconstruction in
unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of
3D Gaussians, enabling direct geometry extraction from 3D Gaussians by
identifying its levelset, without resorting to Poisson reconstruction or TSDF
fusion as in previous work. We approximate the surface normal of Gaussians as
the normal of the ray-Gaussian intersection plane, enabling the application of
regularization that significantly enhances geometry. Furthermore, we develop an
efficient geometry extraction method utilizing marching tetrahedra, where the
tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's
complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based
methods in surface reconstruction and novel view synthesis. Further, it
compares favorably to, or even outperforms, neural implicit methods in both
quality and speed.

Comments:
- Project page:
  https://niujinshuchong.github.io/gaussian-opacity-fields

---

## Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Seungwook Kim, Kejie Li, Xueqing Deng, Yichun Shi, Minsu Cho, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.10603v1){: .btn .btn-green } |

**Abstract**: Leveraging multi-view diffusion models as priors for 3D optimization have
alleviated the problem of 3D consistency, e.g., the Janus face problem or the
content drift problem, in zero-shot text-to-3D models. However, the 3D
geometric fidelity of the output remains an unresolved issue; albeit the
rendered 2D views are realistic, the underlying geometry may contain errors
such as unreasonable concavities. In this work, we propose CorrespondentDream,
an effective method to leverage annotation-free, cross-view correspondences
yielded from the diffusion U-Net to provide additional 3D prior to the NeRF
optimization process. We find that these correspondences are strongly
consistent with human perception, and by adopting it in our loss design, we are
able to produce NeRF models with geometries that are more coherent with common
sense, e.g., more smoothed object surface, yielding higher 3D fidelity. We
demonstrate the efficacy of our approach through various comparative
qualitative results and a solid user study.

Comments:
- 25 pages, 22 figures, accepted to CVPR 2024

---

## Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using  VDB Grid and Hierarchical Ray Traversal

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Yoshio Kato, Shuhei Tarashima | cs.CV | [PDF](http://arxiv.org/pdf/2404.10272v1){: .btn .btn-green } |

**Abstract**: Transmittance estimators such as Occupancy Grid (OG) can accelerate the
training and rendering of Neural Radiance Field (NeRF) by predicting important
samples that contributes much to the generated image. However, OG manages
occupied regions in the form of the dense binary grid, in which there are many
blocks with the same values that cause redundant examination of voxels'
emptiness in ray-tracing. In our work, we introduce two techniques to improve
the efficiency of ray-tracing in trained OG without fine-tuning. First, we
replace the dense grids with VDB grids to reduce the spatial redundancy.
Second, we use hierarchical digital differential analyzer (HDDA) to efficiently
trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF
360 datasets show that our proposed method successfully accelerates rendering
NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in
average, compared to a fast implementation of OG, NerfAcc, without losing the
quality of rendered images.

Comments:
- Short paper for CVPR Neural Rendering Intelligence Workshop 2024.
  Code: https://github.com/Yosshi999/faster-occgrid

---

## Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Florian Barthel, Arian Beckmann, Wieland Morgenstern, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2404.10625v1){: .btn .btn-green } |

**Abstract**: NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or
GIRAFFE have shown very high rendering quality under large representational
variety. However, rendering with Neural Radiance Fields poses challenges for 3D
applications: First, the significant computational demands of NeRF rendering
preclude its use on low-power devices, such as mobiles and VR/AR headsets.
Second, implicit representations based on neural networks are difficult to
incorporate into explicit 3D scenes, such as VR environments or video games. 3D
Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit
3D representation that can be rendered efficiently at high frame rates. In this
work, we present a novel approach that combines the high rendering quality of
NeRF-based 3D-aware GANs with the flexibility and computational advantages of
3DGS. By training a decoder that maps implicit NeRF representations to explicit
3D Gaussian Splatting attributes, we can integrate the representational
diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting
for the first time. Additionally, our approach allows for a high resolution GAN
inversion and real-time GAN editing with 3D Gaussian Splatting scenes.

Comments:
- CVPRW

---

## AbsGS: Recovering Fine Details for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, Yong Dou | cs.CV | [PDF](http://arxiv.org/pdf/2404.10484v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with
differentiable rasterization to achieve high-quality novel view synthesis
results while providing advanced real-time rendering performance. However, due
to the flaw of its adaptive density control strategy in 3D-GS, it frequently
suffers from over-reconstruction issue in intricate scenes containing
high-frequency details, leading to blurry rendered images. The underlying
reason for the flaw has still been under-explored. In this work, we present a
comprehensive analysis of the cause of aforementioned artifacts, namely
gradient collision, which prevents large Gaussians in over-reconstructed
regions from splitting. To address this issue, we propose the novel
homodirectional view-space positional gradient as the criterion for
densification. Our strategy efficiently identifies large Gaussians in
over-reconstructed regions, and recovers fine details by splitting. We evaluate
our proposed method on various challenging datasets. The experimental results
indicate that our approach achieves the best rendering quality with reduced or
similar memory consumption. Our method is easy to implement and can be
incorporated into a wide variety of most recent Gaussian Splatting-based
methods. We will open source our codes upon formal publication. Our project
page is available at: https://ty424.github.io/AbsGS.github.io/



---

## LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted  Gaussian Primitives

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Jiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.09748v1){: .btn .btn-green } |

**Abstract**: Large garages are ubiquitous yet intricate scenes in our daily lives, posing
challenges characterized by monotonous colors, repetitive patterns, reflective
surfaces, and transparent vehicle glass. Conventional Structure from Motion
(SfM) methods for camera pose estimation and 3D reconstruction fail in these
environments due to poor correspondence construction. To address these
challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting
approach for large-scale garage modeling and rendering. We develop a handheld
scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate
accurate LiDAR and image data scanning. With this Polar device, we present a
GarageWorld dataset consisting of five expansive garage scenes with diverse
geometric structures and will release the dataset to the community for further
research. We demonstrate that the collected LiDAR point cloud by the Polar
device enhances a suite of 3D Gaussian splatting algorithms for garage scene
modeling and rendering. We also propose a novel depth regularizer for 3D
Gaussian splatting algorithm training, effectively eliminating floating
artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian
renderer for real-time viewing on web-based devices. Additionally, we explore a
hybrid representation that combines the advantages of traditional mesh in
depicting simple geometry and colors (e.g., walls and the ground) with modern
3D Gaussian representations capturing complex details and high-frequency
textures. This strategy achieves an optimal balance between memory performance
and rendering quality. Experimental results on our dataset, along with
ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering
quality and resource efficiency.

Comments:
- Project Page: https://jdtsui.github.io/letsgo/

---

## CompGS: Efficient 3D Scene Representation via Compressed Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, Sam Kwong | cs.CV | [PDF](http://arxiv.org/pdf/2404.09458v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting, renowned for its exceptional rendering quality and
efficiency, has emerged as a prominent technique in 3D scene representation.
However, the substantial data volume of Gaussian splatting impedes its
practical utility in real-world applications. Herein, we propose an efficient
3D scene representation, named Compressed Gaussian Splatting (CompGS), which
harnesses compact Gaussian primitives for faithful 3D scene modeling with a
remarkably reduced data size. To ensure the compactness of Gaussian primitives,
we devise a hybrid primitive structure that captures predictive relationships
between each other. Then, we exploit a small set of anchor primitives for
prediction, allowing the majority of primitives to be encapsulated into highly
compact residual forms. Moreover, we develop a rate-constrained optimization
scheme to eliminate redundancies within such hybrid primitives, steering our
CompGS towards an optimal trade-off between bitrate consumption and
representation efficacy. Experimental results show that the proposed CompGS
significantly outperforms existing methods, achieving superior compactness in
3D scene representation without compromising model accuracy and rendering
quality. Our code will be released on GitHub for further research.

Comments:
- Submitted to a conference

---

## Video2Game: Real-time, Interactive, Realistic and Browser-Compatible  Environment from a Single Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Hongchi Xia, Zhi-Hao Lin, Wei-Chiu Ma, Shenlong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.09833v1){: .btn .btn-green } |

**Abstract**: Creating high-quality and interactive virtual environments, such as games and
simulators, often involves complex and costly manual modeling processes. In
this paper, we present Video2Game, a novel approach that automatically converts
videos of real-world scenes into realistic and interactive game environments.
At the heart of our system are three core components:(i) a neural radiance
fields (NeRF) module that effectively captures the geometry and visual
appearance of the scene; (ii) a mesh module that distills the knowledge from
NeRF for faster rendering; and (iii) a physics module that models the
interactions and physical dynamics among the objects. By following the
carefully designed pipeline, one can construct an interactable and actionable
digital replica of the real world. We benchmark our system on both indoor and
large-scale outdoor scenes. We show that we can not only produce
highly-realistic renderings in real-time, but also build interactive games on
top.

Comments:
- CVPR 2024. Project page (with code): https://video2game.github.io/

---

## 3D Gaussian Splatting as Markov Chain Monte Carlo

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi | cs.CV | [PDF](http://arxiv.org/pdf/2404.09591v1){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting has recently become popular for neural rendering,
current methods rely on carefully engineered cloning and splitting strategies
for placing Gaussians, which does not always generalize and may lead to
poor-quality renderings. In addition, for real-world scenes, they rely on a
good initial point cloud to perform well. In this work, we rethink 3D Gaussians
as random samples drawn from an underlying probability distribution describing
the physical representation of the scene -- in other words, Markov Chain Monte
Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates are
strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update. As
with MCMC, samples are nothing but past visit locations, adding new Gaussians
under our framework can simply be realized without heuristics as placing
Gaussians at existing Gaussian locations. To encourage using fewer Gaussians
for efficiency, we introduce an L1-regularizer on the Gaussians. On various
standard evaluation scenes, we show that our method provides improved rendering
quality, easy control over the number of Gaussians, and robustness to
initialization.



---

## DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred  Shading

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Tong Wu, Jia-Mu Sun, Yu-Kun Lai, Yuewen Ma, Leif Kobbelt, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2404.09412v1){: .btn .btn-green } |

**Abstract**: Reconstructing and editing 3D objects and scenes both play crucial roles in
computer graphics and computer vision. Neural radiance fields (NeRFs) can
achieve realistic reconstruction and editing results but suffer from
inefficiency in rendering. Gaussian splatting significantly accelerates
rendering by rasterizing Gaussian ellipsoids. However, Gaussian splatting
utilizes a single Spherical Harmonic (SH) function to model both texture and
lighting, limiting independent editing capabilities of these components.
Recently, attempts have been made to decouple texture and lighting with the
Gaussian splatting representation but may fail to produce plausible geometry
and decomposition results on reflective scenes. Additionally, the forward
shading technique they employ introduces noticeable blending artifacts during
relighting, as the geometry attributes of Gaussians are optimized under the
original illumination and may not be suitable for novel lighting conditions. To
address these issues, we introduce DeferredGS, a method for decoupling and
editing the Gaussian splatting representation using deferred shading. To
achieve successful decoupling, we model the illumination with a learnable
environment map and define additional attributes such as texture parameters and
normal direction on Gaussians, where the normal is distilled from a jointly
trained signed distance function. More importantly, we apply deferred shading,
resulting in more realistic relighting effects compared to previous methods.
Both qualitative and quantitative experiments demonstrate the superior
performance of DeferredGS in novel view synthesis and editing tasks.



---

## Taming Latent Diffusion Model for Neural Radiance Field Inpainting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng | cs.CV | [PDF](http://arxiv.org/pdf/2404.09995v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) is a representation for 3D reconstruction from
multi-view images. Despite some recent work showing preliminary success in
editing a reconstructed NeRF with diffusion prior, they remain struggling to
synthesize reasonable geometry in completely uncovered regions. One major
reason is the high diversity of synthetic contents from the diffusion model,
which hinders the radiance field from converging to a crisp and deterministic
geometry. Moreover, applying latent diffusion models on real data often yields
a textural shift incoherent to the image condition due to auto-encoding errors.
These two problems are further reinforced with the use of pixel-distance
losses. To address these issues, we propose tempering the diffusion model's
stochasticity with per-scene customization and mitigating the textural shift
with masked adversarial training. During the analyses, we also found the
commonly used pixel and perceptual losses are harmful in the NeRF inpainting
task. Through rigorous experiments, our framework yields state-of-the-art NeRF
inpainting results on various real-world scenes. Project page:
https://hubert0527.github.io/MALD-NeRF

Comments:
- Project page: https://hubert0527.github.io/MALD-NeRF

---

## EGGS: Edge Guided Gaussian Splatting for Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-14 | Yuanhao Gong | cs.CV | [PDF](http://arxiv.org/pdf/2404.09105v1){: .btn .btn-green } |

**Abstract**: The Gaussian splatting methods are getting popular. However, their loss
function only contains the $\ell_1$ norm and the structural similarity between
the rendered and input images, without considering the edges in these images.
It is well-known that the edges in an image provide important information.
Therefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS)
method that leverages the edges in the input images. More specifically, we give
the edge region a higher weight than the flat region. With such edge guidance,
the resulting Gaussian particles focus more on the edges instead of the flat
regions. Moreover, such edge guidance does not crease the computation cost
during the training and rendering stage. The experiments confirm that such
simple edge-weighted loss function indeed improves about $1\sim2$ dB on several
difference data sets. With simply plugging in the edge guidance, the proposed
method can improve all Gaussian splatting methods in different scenarios, such
as human head modeling, building 3D reconstruction, etc.



---

## VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-14 | Fei Xue, Ignas Budvytis, Daniel Olmeda Reino, Roberto Cipolla | cs.CV | [PDF](http://arxiv.org/pdf/2404.09271v1){: .btn .btn-green } |

**Abstract**: Visual relocalization is a key technique to autonomous driving, robotics, and
virtual/augmented reality. After decades of explorations, absolute pose
regression (APR), scene coordinate regression (SCR), and hierarchical methods
(HMs) have become the most popular frameworks. However, in spite of high
efficiency, APRs and SCRs have limited accuracy especially in large-scale
outdoor scenes; HMs are accurate but need to store a large number of 2D
descriptors for matching, resulting in poor efficiency. In this paper, we
propose an efficient and accurate framework, called VRS-NeRF, for visual
relocalization with sparse neural radiance field. Precisely, we introduce an
explicit geometric map (EGM) for 3D map representation and an implicit learning
map (ILM) for sparse patches rendering. In this localization process, EGP
provides priors of spare 2D points and ILM utilizes these sparse points to
render patches with sparse NeRFs for matching. This allows us to discard a
large number of 2D descriptors so as to reduce the map size. Moreover,
rendering patches only for useful points rather than all pixels in the whole
image reduces the rendering time significantly. This framework inherits the
accuracy of HMs and discards their low efficiency. Experiments on 7Scenes,
CambridgeLandmarks, and Aachen datasets show that our method gives much better
accuracy than APRs and SCRs, and close performance to HMs but is much more
efficient.

Comments:
- source code https://github.com/feixue94/vrs-nerf

---

## DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-14 | Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang | cs.CV | [PDF](http://arxiv.org/pdf/2404.09227v1){: .btn .btn-green } |

**Abstract**: Recent progress in text-to-3D creation has been propelled by integrating the
potent prior of Diffusion Models from text-to-image generation into the 3D
domain. Nevertheless, generating 3D scenes characterized by multiple instances
and intricate arrangements remains challenging. In this study, we present
DreamScape, a method for creating highly consistent 3D scenes solely from
textual descriptions, leveraging the strong 3D representation capabilities of
Gaussian Splatting and the complex arrangement abilities of large language
models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene
representation, consisting of semantic primitives (objects) and their spatial
transformations and relationships derived directly from text prompts using
LLMs. This compositional representation allows for local-to-global optimization
of the entire scene. A progressive scale control is tailored during local
object generation, ensuring that objects of different sizes and densities adapt
to the scene, which addresses training instability issue arising from simple
blending in the subsequent global optimization stage. To mitigate potential
biases of LLM priors, we model collision relationships between objects at the
global level, enhancing physical correctness and overall realism. Additionally,
to generate pervasive objects like rain and snow distributed extensively across
the scene, we introduce a sparse initialization and densification strategy.
Experiments demonstrate that DreamScape offers high usability and
controllability, enabling the generation of high-fidelity 3D scenes from only
text prompts and achieving state-of-the-art performance compared to other
methods.



---

## LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via  Eulerian Motion Field

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-13 | Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He | cs.CV | [PDF](http://arxiv.org/pdf/2404.08966v2){: .btn .btn-green } |

**Abstract**: Cinemagraph is a unique form of visual media that combines elements of still
photography and subtle motion to create a captivating experience. However, the
majority of videos generated by recent works lack depth information and are
confined to the constraints of 2D image space. In this paper, inspired by
significant progress in the field of novel view synthesis (NVS) achieved by 3D
Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from
2D image space to 3D space using 3D Gaussian modeling. To achieve this, we
first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from
multi-view images of static scenes,incorporating shape regularization terms to
prevent blurring or artifacts caused by object deformation. We then adopt an
autoencoder tailored for 3D Gaussian to project it into feature space. To
maintain the local continuity of the scene, we devise SuperGaussian for
clustering based on the acquired features. By calculating the similarity
between clusters and employing a two-stage estimation method, we derive an
Eulerian motion field to describe velocities across the entire scene. The 3D
Gaussian points then move within the estimated Eulerian motion field. Through
bidirectional animation techniques, we ultimately generate a 3D Cinemagraph
that exhibits natural and seamlessly loopable dynamics. Experiment results
validate the effectiveness of our approach, demonstrating high-quality and
visually appealing scene generation. The project is available at
https://pokerlishao.github.io/LoopGaussian/.

Comments:
- 10 pages

---

## OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-12 | Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu | cs.CV | [PDF](http://arxiv.org/pdf/2404.08449v2){: .btn .btn-green } |

**Abstract**: Rendering dynamic 3D human from monocular videos is crucial for various
applications such as virtual reality and digital entertainment. Most methods
assume the people is in an unobstructed scene, while various objects may cause
the occlusion of body parts in real-life scenarios. Previous method utilizing
NeRF for surface rendering to recover the occluded areas, but it requiring more
than one day to train and several seconds to render, failing to meet the
requirements of real-time interactive applications. To address these issues, we
propose OccGaussian based on 3D Gaussian Splatting, which can be trained within
6 minutes and produces high-quality human renderings up to 160 FPS with
occluded input. OccGaussian initializes 3D Gaussian distributions in the
canonical space, and we perform occlusion feature query at occluded regions,
the aggregated pixel-align feature is extracted to compensate for the missing
information. Then we use Gaussian Feature MLP to further process the feature
along with the occlusion-aware loss functions to better perceive the occluded
area. Extensive experiments both in simulated and real-world occlusions,
demonstrate that our method achieves comparable or even superior performance
compared to the state-of-the-art method. And we improving training and
inference speeds by 250x and 800x, respectively. Our code will be available for
research purposes.



---

## GPN: Generative Point-based NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-12 | Haipeng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.08312v1){: .btn .btn-green } |

**Abstract**: Scanning real-life scenes with modern registration devices typically gives
incomplete point cloud representations, primarily due to the limitations of
partial scanning, 3D occlusions, and dynamic light conditions. Recent works on
processing incomplete point clouds have always focused on point cloud
completion. However, these approaches do not ensure consistency between the
completed point cloud and the captured images regarding color and geometry. We
propose using Generative Point-based NeRF (GPN) to reconstruct and repair a
partial cloud by fully utilizing the scanning images and the corresponding
reconstructed cloud. The repaired point cloud can achieve multi-view
consistency with the captured images at high spatial resolution. For the
finetunes of a single scene, we optimize the global latent condition by
incorporating an Auto-Decoder architecture while retaining multi-view
consistency. As a result, the generated point clouds are smooth, plausible, and
geometrically consistent with the partial scanning images. Extensive
experiments on ShapeNet demonstrate that our works achieve competitive
performances to the other state-of-the-art point cloud-based neural scene
rendering and editing performances.



---

## MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based  Monocular Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-12 | Yuqun Wu, Jae Yong Lee, Chuhang Zou, Shenlong Wang, Derek Hoiem | cs.CV | [PDF](http://arxiv.org/pdf/2404.08252v1){: .btn .btn-green } |

**Abstract**: The latest regularized Neural Radiance Field (NeRF) approaches produce poor
geometry and view extrapolation for multiview stereo (MVS) benchmarks such as
ETH3D. In this paper, we aim to create 3D models that provide accurate geometry
and view synthesis, partially closing the large geometric performance gap
between NeRF and traditional MVS methods. We propose a patch-based approach
that effectively leverages monocular surface normal and relative depth
predictions. The patch-based ray sampling also enables the appearance
regularization of normalized cross-correlation (NCC) and structural similarity
(SSIM) between randomly sampled virtual and training views. We further show
that "density restrictions" based on sparse structure-from-motion points can
help greatly improve geometric accuracy with a slight drop in novel view
synthesis metrics. Our experiments show 4x the performance of RegNeRF and 8x
that of FreeNeRF on average F1@2cm for ETH3D MVS benchmark, suggesting a
fruitful research direction to improve the geometric accuracy of NeRF-based
models, and sheds light on a potential future approach to enable NeRF-based
optimization to eventually outperform traditional MVS.

Comments:
- 26 pages, 15 figures

---

## NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous  Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | William Ljungbergh, Adam Tonderski, Joakim Johnander, Holger Caesar, Kalle strm, Michael Felsberg, Christoffer Petersson | cs.CV | [PDF](http://arxiv.org/pdf/2404.07762v2){: .btn .btn-green } |

**Abstract**: We present a versatile NeRF-based simulator for testing autonomous driving
(AD) software systems, designed with a focus on sensor-realistic closed-loop
evaluation and the creation of safety-critical scenarios. The simulator learns
from sequences of real-world driving sensor data and enables reconfigurations
and renderings of new, unseen scenarios. In this work, we use our simulator to
test the responses of AD models to safety-critical scenarios inspired by the
European New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,
while state-of-the-art end-to-end planners excel in nominal driving scenarios
in an open-loop setting, they exhibit critical flaws when navigating our
safety-critical scenarios in a closed-loop setting. This highlights the need
for advancements in the safety and real-world usability of end-to-end planners.
By publicly releasing our simulator and scenarios as an easy-to-run evaluation
suite, we invite the research community to explore, refine, and validate their
AD models in controlled, yet highly configurable and challenging
sensor-realistic environments. Code and instructions can be found at
https://github.com/wljungbergh/NeuroNCAP



---

## GoMAvatar: Efficient Animatable Human Modeling from Monocular Video  Using Gaussians-on-Mesh

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.07991v1){: .btn .btn-green } |

**Abstract**: We introduce GoMAvatar, a novel approach for real-time, memory-efficient,
high-quality animatable human modeling. GoMAvatar takes as input a single
monocular video to create a digital avatar capable of re-articulation in new
poses and real-time rendering from novel viewpoints, while seamlessly
integrating with rasterization-based graphics pipelines. Central to our method
is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering
quality and speed of Gaussian splatting with geometry modeling and
compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and
various YouTube videos. GoMAvatar matches or surpasses current monocular human
modeling algorithms in rendering quality and significantly outperforms them in
computational efficiency (43 FPS) while being memory-efficient (3.63 MB per
subject).

Comments:
- CVPR 2024; project page: https://wenj.github.io/GoMAvatar/

---

## G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | Zixiong Huang, Qi Chen, Libo Sun, Yifan Yang, Naizhou Wang, Mingkui Tan, Qi Wu | cs.CV | [PDF](http://arxiv.org/pdf/2404.07474v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis aims to generate new view images of a given view image
collection. Recent attempts address this problem relying on 3D geometry priors
(e.g., shapes, sizes, and positions) learned from multi-view images. However,
such methods encounter the following limitations: 1) they require a set of
multi-view images as training data for a specific scene (e.g., face, car or
chair), which is often unavailable in many real-world scenarios; 2) they fail
to extract the geometry priors from single-view images due to the lack of
multi-view supervision. In this paper, we propose a Geometry-enhanced NeRF
(G-NeRF), which seeks to enhance the geometry priors by a geometry-guided
multi-view synthesis approach, followed by a depth-aware training. In the
synthesis process, inspired that existing 3D GAN models can unconditionally
synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D
GAN models, such as EG3D, as a free source to provide geometry priors through
synthesizing multi-view data. Simultaneously, to further improve the geometry
quality of the synthetic data, we introduce a truncation method to effectively
sample latent codes within 3D GAN models. To tackle the absence of multi-view
supervision for single-view images, we design the depth-aware training
approach, incorporating a depth-aware discriminator to guide geometry priors
through depth maps. Experiments demonstrate the effectiveness of our method in
terms of both qualitative and quantitative results.

Comments:
- CVPR 2024 Accepted Paper

---

## Boosting Self-Supervision for Single-View Scene Completion via Knowledge  Distillation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2404.07933v1){: .btn .btn-green } |

**Abstract**: Inferring scene geometry from images via Structure from Motion is a
long-standing and fundamental problem in computer vision. While classical
approaches and, more recently, depth map predictions only focus on the visible
parts of a scene, the task of scene completion aims to reason about geometry
even in occluded regions. With the popularity of neural radiance fields
(NeRFs), implicit representations also became popular for scene completion by
predicting so-called density fields. Unlike explicit approaches. e.g.
voxel-based methods, density fields also allow for accurate depth prediction
and novel-view synthesis via image-based rendering. In this work, we propose to
fuse the scene reconstruction from multiple images and distill this knowledge
into a more accurate single-view scene reconstruction. To this end, we propose
Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed
images, trained fully self-supervised only from image data. Using knowledge
distillation, we use MVBTS to train a single-view scene completion network via
direct supervision called KDBTS. It achieves state-of-the-art performance on
occupancy prediction, especially in occluded regions.



---

## Connecting NeRFs, Images, and Text

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | Francesco Ballerini, Pierluigi Zama Ramirez, Roberto Mirabella, Samuele Salti, Luigi Di Stefano | cs.CV | [PDF](http://arxiv.org/pdf/2404.07993v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as a standard framework for
representing 3D scenes and objects, introducing a novel data type for
information exchange and storage. Concurrently, significant progress has been
made in multimodal representation learning for text and image data. This paper
explores a novel research direction that aims to connect the NeRF modality with
other modalities, similar to established methodologies for images and text. To
this end, we propose a simple framework that exploits pre-trained models for
NeRF representations alongside multimodal models for text and image processing.
Our framework learns a bidirectional mapping between NeRF embeddings and those
obtained from corresponding images and text. This mapping unlocks several novel
and useful applications, including NeRF zero-shot classification and NeRF
retrieval from images or text.

Comments:
- Accepted at CVPRW-INRV 2024

---

## SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike  Camera

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Gaole Dai, Zhenyu Wang, Qinwen Xu, Ming Lu, Wen Chen, Boxin Shi, Shanghang Zhang, Tiejun Huang | cs.CV | [PDF](http://arxiv.org/pdf/2404.06710v3){: .btn .btn-green } |

**Abstract**: One of the most critical factors in achieving sharp Novel View Synthesis
(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) is the quality of the training images. However,
Conventional RGB cameras are susceptible to motion blur. In contrast,
neuromorphic cameras like event and spike cameras inherently capture more
comprehensive temporal information, which can provide a sharp representation of
the scene as additional training data. Recent methods have explored the
integration of event cameras to improve the quality of NVS. The event-RGB
approaches have some limitations, such as high training costs and the inability
to work effectively in the background. Instead, our study introduces a new
method that uses the spike camera to overcome these limitations. By considering
texture reconstruction from spike streams as ground truth, we design the
Texture from Spike (TfS) loss. Since the spike camera relies on temporal
integration instead of temporal differentiation used by event cameras, our
proposed TfS loss maintains manageable training costs. It handles foreground
objects with backgrounds simultaneously. We also provide a real-world dataset
captured with our spike-RGB camera system to facilitate future research
endeavors. We conduct extensive experiments using synthetic and real-world
datasets to demonstrate that our design can enhance novel view synthesis across
NeRF and 3DGS. The code and dataset will be made available for public access.



---

## Zero-shot Point Cloud Completion Via 2D Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2404.06814v1){: .btn .btn-green } |

**Abstract**: 3D point cloud completion is designed to recover complete shapes from
partially observed point clouds. Conventional completion methods typically
depend on extensive point cloud data for training %, with their effectiveness
often constrained to object categories similar to those seen during training.
In contrast, we propose a zero-shot framework aimed at completing partially
observed point clouds across any unseen categories. Leveraging point rendering
via Gaussian Splatting, we develop techniques of Point Cloud Colorization and
Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion
models to infer missing regions. Experimental results on both synthetic and
real-world scanned point clouds demonstrate that our approach outperforms
existing methods in completing a variety of objects without any requirement for
specific training data.



---

## MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D  Reconstruction of Indoor Scenes from Monocular RGB Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Runfa Li, Upal Mahbub, Vasudev Bhaskaran, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2404.06753v1){: .btn .btn-green } |

**Abstract**: Current monocular 3D scene reconstruction (3DR) works are either
fully-supervised, or not generalizable, or implicit in 3D representation. We
propose a novel framework - MonoSelfRecon that for the first time achieves
explicit 3D mesh reconstruction for generalizable indoor scenes with monocular
RGB views by purely self-supervision on voxel-SDF (signed distance function).
MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and
a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF
in self-supervision. We propose novel self-supervised losses, which not only
support pure self-supervision, but can be used together with supervised signals
to further boost supervised training. Our experiments show that "MonoSelfRecon"
trained in pure self-supervision outperforms current best self-supervised
indoor depth estimation models and is comparable to 3DR models trained in fully
supervision with depth annotations. MonoSelfRecon is not restricted by specific
model design, which can be used to any models with voxel-SDF for purely
self-supervised manner.



---

## Bayesian NeRF: Quantifying Uncertainty with Volume Density in Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Sibeak Lee, Kyeongsu Kang, Hyeonwoo Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.06727v1){: .btn .btn-green } |

**Abstract**: We present the Bayesian Neural Radiance Field (NeRF), which explicitly
quantifies uncertainty in geometric volume structures without the need for
additional networks, making it adept for challenging observations and
uncontrolled images. NeRF diverges from traditional geometric methods by
offering an enriched scene representation, rendering color and density in 3D
space from various viewpoints. However, NeRF encounters limitations in relaxing
uncertainties by using geometric structure information, leading to inaccuracies
in interpretation under insufficient real-world observations. Recent research
efforts aimed at addressing this issue have primarily relied on empirical
methods or auxiliary networks. To fundamentally address this issue, we propose
a series of formulational extensions to NeRF. By introducing generalized
approximations and defining density-related uncertainty, our method seamlessly
extends to manage uncertainty not only for RGB but also for depth, without the
need for additional networks or empirical assumptions. In experiments we show
that our method significantly enhances performance on RGB and depth images in
the comprehensive dataset, demonstrating the reliability of the Bayesian NeRF
approach to quantifying uncertainty based on the geometric structure.



---

## DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi | cs.CV | [PDF](http://arxiv.org/pdf/2404.06903v1){: .btn .btn-green } |

**Abstract**: The increasing demand for virtual reality applications has highlighted the
significance of crafting immersive 3D assets. We present a text-to-3D
360$^{\circ}$ scene generation pipeline that facilitates the creation of
comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of
minutes. Our approach utilizes the generative power of a 2D diffusion model and
prompt self-refinement to create a high-quality and globally coherent panoramic
image. This image acts as a preliminary "flat" (2D) scene representation.
Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to
enable real-time exploration. To produce consistent 3D geometry, our pipeline
constructs a spatially coherent structure by aligning the 2D monocular depth
into a globally optimized point cloud. This point cloud serves as the initial
state for the centroids of 3D Gaussians. In order to address invisible issues
inherent in single-view inputs, we impose semantic and geometric constraints on
both synthesized and input camera views as regularizations. These guide the
optimization of Gaussians, aiding in the reconstruction of unseen regions. In
summary, our method offers a globally consistent 3D scene within a
360$^{\circ}$ perspective, providing an enhanced immersive experience over
existing techniques. Project website at: http://dreamscene360.github.io/



---

## RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2404.07199v1){: .btn .btn-green } |

**Abstract**: We introduce RealmDreamer, a technique for generation of general
forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D
Gaussian Splatting representation to match complex text prompts. We initialize
these splats by utilizing the state-of-the-art text-to-image generators,
lifting their samples into 3D, and computing the occlusion volume. We then
optimize this representation across multiple views as a 3D inpainting task with
image-conditional diffusion models. To learn correct geometric structure, we
incorporate a depth diffusion model by conditioning on the samples from the
inpainting model, giving rich geometric structure. Finally, we finetune the
model using sharpened samples from image generators. Notably, our technique
does not require video or multi-view data and can synthesize a variety of
high-quality 3D scenes in different styles, consisting of multiple objects. Its
generality additionally allows 3D synthesis from a single image.

Comments:
- Project Page: https://realmdreamer.github.io/

---

## SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn | cs.CV | [PDF](http://arxiv.org/pdf/2404.06832v1){: .btn .btn-green } |

**Abstract**: Detecting anomalies in images has become a well-explored problem in both
academia and industry. State-of-the-art algorithms are able to detect defects
in increasingly difficult settings and data modalities. However, most current
methods are not suited to address 3D objects captured from differing poses.
While solutions using Neural Radiance Fields (NeRFs) have been proposed, they
suffer from excessive computation requirements, which hinder real-world
usability. For this reason, we propose the novel 3D Gaussian splatting-based
framework SplatPose which, given multi-view images of a 3D object, accurately
estimates the pose of unseen views in a differentiable manner, and detects
anomalies in them. We achieve state-of-the-art results in both training and
inference speed, and detection performance, even when using less training data
than competing methods. We thoroughly evaluate our framework using the recently
proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly
detection (MAD) data set.

Comments:
- Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024

---

## Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Xiaolei Lang, Laijian Li, Hang Zhang, Feng Xiong, Mu Xu, Yong Liu, Xingxing Zuo, Jiajun Lv | cs.RO | [PDF](http://arxiv.org/pdf/2404.06926v1){: .btn .btn-green } |

**Abstract**: We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian
Splatting as the mapping backend. Leveraging robust pose estimates from our
LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic
mapping system is proposed in this paper. We initialize 3D Gaussians from
colorized LiDAR points and optimize them using differentiable rendering powered
by 3D Gaussian Splatting. Meticulously designed strategies are employed to
incrementally expand the Gaussian map and adaptively control its density,
ensuring high-quality mapping with real-time capability. Experiments conducted
in diverse scenarios demonstrate the superior performance of our method
compared to existing radiance-field-based SLAM systems.

Comments:
- Submitted to IROS 2024

---

## Hash3D: Training-free Acceleration for 3D Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Xingyi Yang, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.06091v1){: .btn .btn-green } |

**Abstract**: The evolution of 3D generative modeling has been notably propelled by the
adoption of 2D diffusion models. Despite this progress, the cumbersome
optimization process per se presents a critical hurdle to efficiency. In this
paper, we introduce Hash3D, a universal acceleration for 3D generation without
model training. Central to Hash3D is the insight that feature-map redundancy is
prevalent in images rendered from camera positions and diffusion time-steps in
close proximity. By effectively hashing and reusing these feature maps across
neighboring timesteps and camera angles, Hash3D substantially prevents
redundant calculations, thus accelerating the diffusion model's inference in 3D
generation tasks. We achieve this through an adaptive grid-based hashing.
Surprisingly, this feature-sharing mechanism not only speed up the generation
but also enhances the smoothness and view consistency of the synthesized 3D
objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models,
demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency
by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian
splatting largely speeds up 3D model creation, reducing text-to-3D processing
to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The
project page is at https://adamdad.github.io/hash3D/.

Comments:
- https://adamdad.github.io/hash3D/

---

## Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for  Realistic Endoscopic Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano | cs.CV | [PDF](http://arxiv.org/pdf/2404.06128v1){: .btn .btn-green } |

**Abstract**: Within colorectal cancer diagnostics, conventional colonoscopy techniques
face critical limitations, including a limited field of view and a lack of
depth information, which can impede the detection of precancerous lesions.
Current methods struggle to provide comprehensive and accurate 3D
reconstructions of the colonic surface which can help minimize the missing
regions and reinspection for pre-cancerous polyps. Addressing this, we
introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting
(3D GS) combined with a Recurrent Neural Network-based Simultaneous
Localization and Mapping (RNNSLAM) system. By introducing geometric and depth
regularization into the 3D GS framework, our approach ensures more accurate
alignment of Gaussians with the colon surface, resulting in smoother 3D
reconstructions with novel viewing of detailed textures and structures.
Evaluations across three diverse datasets show that Gaussian Pancakes enhances
novel view synthesis quality, surpassing current leading methods with a 18%
boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster
rendering and more than 10X shorter training times, making it a practical tool
for real-time applications. Hence, this holds promise for achieving clinical
translation for better detection and diagnosis of colorectal cancer.

Comments:
- 12 pages, 5 figures

---

## Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi Feng, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2404.06429v1){: .btn .btn-green } |

**Abstract**: Benefiting from the rapid development of 2D diffusion models, 3D content
creation has made significant progress recently. One promising solution
involves the fine-tuning of pre-trained 2D diffusion models to harness their
capacity for producing multi-view images, which are then lifted into accurate
3D models via methods like fast-NeRFs or large reconstruction models. However,
as inconsistency still exists and limited generated resolution, the generation
results of such methods still lack intricate textures and complex geometries.
To solve this problem, we propose Magic-Boost, a multi-view conditioned
diffusion model that significantly refines coarse generative results through a
brief period of SDS optimization ($\sim15$min). Compared to the previous text
or single image based diffusion models, Magic-Boost exhibits a robust
capability to generate images with high consistency from pseudo synthesized
multi-view images. It provides precise SDS guidance that well aligns with the
identity of the input images, enriching the local detail in both geometry and
texture of the initial generative results. Extensive experiments show
Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D
assets with rich geometric and textural details. (Project Page:
https://magic-research.github.io/magic-boost/)



---

## HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet | cs.CV | [PDF](http://arxiv.org/pdf/2404.06152v1){: .btn .btn-green } |

**Abstract**: In recent advancements in novel view synthesis, generalizable Neural Radiance
Fields (NeRF) based methods applied to human subjects have shown remarkable
results in generating novel views from few images. However, this generalization
ability cannot capture the underlying structural features of the skeleton
shared across all instances. Building upon this, we introduce HFNeRF: a novel
generalizable human feature NeRF aimed at generating human biomechanic features
using a pre-trained image encoder. While previous human NeRF methods have shown
promising results in the generation of photorealistic virtual avatars, such
methods lack underlying human structure or biomechanic features such as
skeleton or joint information that are crucial for downstream applications
including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D
pre-trained foundation models toward learning human features in 3D using neural
rendering, and then volume rendering towards generating 2D feature maps. We
evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as
features. The proposed method is fully differentiable, allowing to successfully
learn color, geometry, and human skeleton in a simultaneous manner. This paper
presents preliminary results of HFNeRF, illustrating its potential in
generating realistic virtual avatars with biomechanic features using NeRF.



---

## 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View  Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai | cs.CV | [PDF](http://arxiv.org/pdf/2404.06270v2){: .btn .btn-green } |

**Abstract**: In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting
method for dynamic view synthesis. Existing neural radiance fields (NeRF) based
solutions learn the deformation in an implicit manner, which cannot incorporate
3D scene geometry. Therefore, the learned deformation is not necessarily
geometrically coherent, which results in unsatisfactory dynamic view synthesis
and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new
representation of the 3D scene, building upon which the 3D geometry could be
exploited in learning the complex 3D deformation. Specifically, the scenes are
represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized
to move and rotate over time to model the deformation. To enforce the 3D scene
geometry constraint during deformation, we explicitly extract 3D geometry
features and integrate them in learning the 3D deformation. In this way, our
solution achieves 3D geometry-aware deformation modeling, which enables
improved dynamic view synthesis and 3D dynamic reconstruction. Extensive
experimental results on both synthetic and real datasets prove the superiority
of our solution, which achieves new state-of-the-art performance.
  The project is available at https://npucvr.github.io/GaGS/

Comments:
- Accepted by CVPR 2024. Project page: https://npucvr.github.io/GaGS/

---

## Revising Densification in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Samuel Rota Bul, Lorenzo Porzi, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2404.06109v1){: .btn .btn-green } |

**Abstract**: In this paper, we address the limitations of Adaptive Density Control (ADC)
in 3D Gaussian Splatting (3DGS), a scene representation method achieving
high-quality, photorealistic results for novel view synthesis. ADC has been
introduced for automatic 3D point primitive management, controlling
densification and pruning, however, with certain limitations in the
densification logic. Our main contribution is a more principled, pixel-error
driven formulation for density control in 3DGS, leveraging an auxiliary,
per-pixel error function as the criterion for densification. We further
introduce a mechanism to control the total number of primitives generated per
scene and correct a bias in the current opacity handling strategy of ADC during
cloning operations. Our approach leads to consistent quality improvements
across a variety of benchmark scenes, without sacrificing the method's
efficiency.



---

## GHNeRF: Learning Generalizable Human Features with Efficient Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Arnab Dey, Di Yang, Rohith Agaram, Antitza Dantcheva, Andrew I. Comport, Srinath Sridhar, Jean Martinet | cs.CV | [PDF](http://arxiv.org/pdf/2404.06246v1){: .btn .btn-green } |

**Abstract**: Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising
results in 3D scene representations, including 3D human representations.
However, these representations often lack crucial information on the underlying
human pose and structure, which is crucial for AR/VR applications and games. In
this paper, we introduce a novel approach, termed GHNeRF, designed to address
these limitations by learning 2D/3D joint locations of human subjects with NeRF
representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract
essential human features from 2D images, which are then incorporated into the
NeRF framework in order to encode human biomechanic features. This allows our
network to simultaneously learn biomechanic features, such as joint locations,
along with human geometry and texture. To assess the effectiveness of our
method, we conduct a comprehensive comparison with state-of-the-art human NeRF
techniques and joint estimation algorithms. Our results show that GHNeRF can
achieve state-of-the-art results in near real-time.



---

## Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Y. Wang, A. Gao, Y. Gong, Y. Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2404.05236v1){: .btn .btn-green } |

**Abstract**: Recently, a surge of 3D style transfer methods has been proposed that
leverage the scene reconstruction power of a pre-trained neural radiance field
(NeRF). To successfully stylize a scene this way, one must first reconstruct a
photo-realistic radiance field from collected images of the scene. However,
when only sparse input views are available, pre-trained few-shot NeRFs often
suffer from high-frequency artifacts, which are generated as a by-product of
high-frequency details for improving reconstruction quality. Is it possible to
generate more faithful stylized scenes from sparse inputs by directly
optimizing encoding-based scene representation with target style? In this
paper, we consider the stylization of sparse-view scenes in terms of
disentangling content semantics and style textures. We propose a coarse-to-fine
sparse-view scene stylization framework, where a novel hierarchical
encoding-based neural representation is designed to generate high-quality
stylized scenes directly from implicit scene representations. We also propose a
new optimization strategy with content strength annealing to achieve realistic
stylization and better content preservation. Extensive experiments demonstrate
that our method can achieve high-quality stylization of sparse-view scenes and
outperforms fine-tuning-based baselines in terms of stylization quality and
efficiency.



---

## Semantic Flow: Learning Semantic Field of Dynamic Scenes from Monocular  Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Fengrui Tian, Yueqi Duan, Angtian Wang, Jianfei Guo, Shaoyi Du | cs.CV | [PDF](http://arxiv.org/pdf/2404.05163v1){: .btn .btn-green } |

**Abstract**: In this work, we pioneer Semantic Flow, a neural semantic representation of
dynamic scenes from monocular videos. In contrast to previous NeRF methods that
reconstruct dynamic scenes from the colors and volume densities of individual
points, Semantic Flow learns semantics from continuous flows that contain rich
3D motion information. As there is 2D-to-3D ambiguity problem in the viewing
direction when extracting 3D flow features from 2D video frames, we consider
the volume densities as opacity priors that describe the contributions of flow
features to the semantics on the frames. More specifically, we first learn a
flow network to predict flows in the dynamic scene, and propose a flow feature
aggregation module to extract flow features from video frames. Then, we propose
a flow attention module to extract motion information from flow features, which
is followed by a semantic network to output semantic logits of flows. We
integrate the logits with volume densities in the viewing direction to
supervise the flow features with semantic labels on video frames. Experimental
results show that our model is able to learn from multiple dynamic scenes and
supports a series of new tasks such as instance-level scene editing, semantic
completions, dynamic scene tracking and semantic adaption on novel scenes.
Codes are available at https://github.com/tianfr/Semantic-Flow/.

Comments:
- Accepted by ICLR 2024, Codes are available at
  https://github.com/tianfr/Semantic-Flow/

---

## StylizedGS: Controllable Stylization for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Dingxi Zhang, Zhuoxun Chen, Yu-Jie Yuan, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2404.05220v1){: .btn .btn-green } |

**Abstract**: With the rapid development of XR, 3D generation and editing are becoming more
and more important, among which, stylization is an important tool of 3D
appearance editing. It can achieve consistent 3D artistic stylization given a
single reference style image and thus is a user-friendly editing way. However,
recent NeRF-based 3D stylization methods face efficiency issues that affect the
actual user experience and the implicit nature limits its ability to transfer
the geometric pattern styles. Additionally, the ability for artists to exert
flexible control over stylized scenes is considered highly desirable, fostering
an environment conducive to creative exploration. In this paper, we introduce
StylizedGS, a 3D neural style transfer framework with adaptable control over
perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The
3DGS brings the benefits of high efficiency. We propose a GS filter to
eliminate floaters in the reconstruction which affects the stylization effects
before stylization. Then the nearest neighbor-based style loss is introduced to
achieve stylization by fine-tuning the geometry and color parameters of 3DGS,
while a depth preservation loss with other regularizations is proposed to
prevent the tampering of geometry content. Moreover, facilitated by specially
designed losses, StylizedGS enables users to control color, stylized scale and
regions during the stylization to possess customized capabilities. Our method
can attain high-quality stylization results characterized by faithful
brushstrokes and geometric consistency with flexible controls. Extensive
experiments across various scenes and styles demonstrate the effectiveness and
efficiency of our method concerning both stylization quality and inference FPS.



---

## Dual-Camera Smooth Zoom on Mobile Phones

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2404.04908v1){: .btn .btn-green } |

**Abstract**: When zooming between dual cameras on a mobile, noticeable jumps in geometric
content and image color occur in the preview, inevitably affecting the user's
zoom experience. In this work, we introduce a new task, ie, dual-camera smooth
zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI)
technique is a potential solution but struggles with ground-truth collection.
To address the issue, we suggest a data factory solution where continuous
virtual cameras are assembled to generate DCSZ data by rendering reconstructed
3D models of the scene. In particular, we propose a novel dual-camera smooth
zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is
introduced to construct a specific 3D model for each virtual camera. With the
proposed data factory, we construct a synthetic dataset for DCSZ, and we
utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom
images without ground-truth for evaluation. Extensive experiments are conducted
with multiple FI methods. The results show that the fine-tuned FI models
achieve a significant performance improvement over the original ones on DCSZ
task. The datasets, codes, and pre-trained models will be publicly available.

Comments:
- 24

---

## GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric  Reconstruction Using Gaussian Splatting and NeRF

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Butian Xiong, Nanjun Zheng, Zhen Li | cs.CV | [PDF](http://arxiv.org/pdf/2404.04880v1){: .btn .btn-green } |

**Abstract**: We introduce a novel large-scale scene reconstruction benchmark that utilizes
newly developed 3D representation approaches: Gaussian Splatting and Neural
Radiance Fields, on our expansive GauU-Scene V2 dataset. GauU-Scene V2
encompasses over 6.5 square kilometers and features a comprehensive RGB dataset
coupled with LiDAR ground truth. This dataset offers a unique blend of urban
and academic environments for advanced spatial analysis, covering more than 6.5
km2. We also provide detailed supplementary information on data collection
protocols. Furthermore, we present an easy-to-follow pipeline to align the
COLMAP sparse point cloud with the detailed LiDAR dataset. Our evaluation of
U-Scene, which includes a detailed analysis across various novel viewpoints
using image-based metrics such as SSIM, LPIPS, and PSNR, shows contradictory
results when applying geometric-based metrics, such as Chamfer distance. This
leads to doubts about the reliability of current image-based measurement
matrices and geometric extraction methods on Gaussian Splatting. We also make
the dataset available on the following anonymous project page

Comments:
- 8 pages(No reference) 6 figures 4 tabs

---

## NeRF2Points: Large-Scale Point Cloud Generation From Street Views'  Radiance Field Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Peng Tu, Xun Zhou, Mingming Wang, Xiaojun Yang, Bo Peng, Ping Chen, Xiu Su, Yawen Huang, Yefeng Zheng, Chang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.04875v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have emerged as a paradigm-shifting methodology
for the photorealistic rendering of objects and environments, enabling the
synthesis of novel viewpoints with remarkable fidelity. This is accomplished
through the strategic utilization of object-centric camera poses characterized
by significant inter-frame overlap. This paper explores a compelling,
alternative utility of NeRF: the derivation of point clouds from aggregated
urban landscape imagery. The transmutation of street-view data into point
clouds is fraught with complexities, attributable to a nexus of interdependent
variables. First, high-quality point cloud generation hinges on precise camera
poses, yet many datasets suffer from inaccuracies in pose metadata. Also, the
standard approach of NeRF is ill-suited for the distinct characteristics of
street-view data from autonomous vehicles in vast, open settings. Autonomous
vehicle cameras often record with limited overlap, leading to blurring,
artifacts, and compromised pavement representation in NeRF-based point clouds.
In this paper, we present NeRF2Points, a tailored NeRF variant for urban point
cloud synthesis, notable for its high-quality output from RGB inputs alone. Our
paper is supported by a bespoke, high-resolution 20-kilometer urban street
dataset, designed for point cloud generation and evaluation. NeRF2Points
adeptly navigates the inherent challenges of NeRF-based point cloud synthesis
through the implementation of the following strategic innovations: (1)
Integration of Weighted Iterative Geometric Optimization (WIGO) and Structure
from Motion (SfM) for enhanced camera pose accuracy, elevating street-view data
precision. (2) Layered Perception and Integrated Modeling (LPiM) is designed
for distinct radiance field modeling in urban environments, resulting in
coherent point cloud representations.

Comments:
- 18 pages

---

## CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality  Novel-view Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Gyeongjin Kang, Younggeun Lee, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2404.04913v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have achieved huge success in effectively
capturing and representing 3D objects and scenes. However, several factors have
impeded its further proliferation as next-generation 3D media. To establish a
ubiquitous presence in everyday media formats, such as images and videos, it is
imperative to devise a solution that effectively fulfills three key objectives:
fast encoding and decoding time, compact model sizes, and high-quality
renderings. Despite significant advancements, a comprehensive algorithm that
adequately addresses all objectives has yet to be fully realized. In this work,
we present CodecNeRF, a neural codec for NeRF representations, consisting of a
novel encoder and decoder architecture that can generate a NeRF representation
in a single forward pass. Furthermore, inspired by the recent
parameter-efficient finetuning approaches, we develop a novel finetuning method
to efficiently adapt the generated NeRF representations to a new test instance,
leading to high-quality image renderings and compact code sizes. The proposed
CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF,
achieved unprecedented compression performance of more than 150x and 20x
reduction in encoding time while maintaining (or improving) the image quality
on widely used 3D object datasets, such as ShapeNet and Objaverse.

Comments:
- 34 pages, 22 figures, Project page:
  https://gynjn.github.io/Codec-NeRF/

---

## DATENeRF: Depth-Aware Text-based Editing of NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-06 | Sara Rojas, Julien Philip, Kai Zhang, Sai Bi, Fujun Luan, Bernard Ghanem, Kalyan Sunkavall | cs.CV | [PDF](http://arxiv.org/pdf/2404.04526v1){: .btn .btn-green } |

**Abstract**: Recent advancements in diffusion models have shown remarkable proficiency in
editing 2D images based on text prompts. However, extending these techniques to
edit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual
2D frames can result in inconsistencies across multiple views. Our crucial
insight is that a NeRF scene's geometry can serve as a bridge to integrate
these 2D edits. Utilizing this geometry, we employ a depth-conditioned
ControlNet to enhance the coherence of each 2D image modification. Moreover, we
introduce an inpainting approach that leverages the depth information of NeRF
scenes to distribute 2D edits across different images, ensuring robustness
against errors and resampling challenges. Our results reveal that this
methodology achieves more consistent, lifelike, and detailed edits than
existing leading methods for text-driven NeRF scene editing.

Comments:
- 14 pages, Conference paper, 3D Scene Editing, Neural Rendering,
  Diffusion Models

---

## Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-06 | Ziyuan Qu, Omkar Vengurlekar, Mohamad Qadri, Kevin Zhang, Michael Kaess, Christopher Metzler, Suren Jayasuriya, Adithya Pediredla | cs.CV | [PDF](http://arxiv.org/pdf/2404.04687v1){: .btn .btn-green } |

**Abstract**: Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent
technique in computer vision and graphics for reconstructing 3D scenes. GS
represents a scene as a set of 3D Gaussians with varying opacities and employs
a computationally efficient splatting operation along with analytical
derivatives to compute the 3D Gaussian parameters given scene images captured
from various viewpoints. Unfortunately, capturing surround view ($360^{\circ}$
viewpoint) images is impossible or impractical in many real-world imaging
scenarios, including underwater imaging, rooms inside a building, and
autonomous navigation. In these restricted baseline imaging scenarios, the GS
algorithm suffers from a well-known 'missing cone' problem, which results in
poor reconstruction along the depth axis. In this manuscript, we demonstrate
that using transient data (from sonars) allows us to address the missing cone
problem by sampling high-frequency data along the depth axis. We extend the
Gaussian splatting algorithms for two commonly used sonars and propose fusion
algorithms that simultaneously utilize RGB camera data and sonar data. Through
simulations, emulations, and hardware experiments across various imaging
scenarios, we show that the proposed fusion algorithms lead to significantly
better novel view synthesis (5 dB improvement in PSNR) and 3D geometry
reconstruction (60% lower Chamfer distance).



---

## Robust Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-05 | Franois Darmon, Lorenzo Porzi, Samuel Rota-Bul, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2404.04211v1){: .btn .btn-green } |

**Abstract**: In this paper, we address common error sources for 3D Gaussian Splatting
(3DGS) including blur, imperfect camera poses, and color inconsistencies, with
the goal of improving its robustness for practical applications like
reconstructions from handheld phone captures. Our main contribution involves
modeling motion blur as a Gaussian distribution over camera poses, allowing us
to address both camera pose refinement and motion blur correction in a unified
way. Additionally, we propose mechanisms for defocus blur compensation and for
addressing color in-consistencies caused by ambient light, shadows, or due to
camera-related factors like varying white balancing settings. Our proposed
solutions integrate in a seamless way with the 3DGS formulation while
maintaining its benefits in terms of training efficiency and rendering speed.
We experimentally validate our contributions on relevant benchmark datasets
including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and
thus consistent improvements over relevant baselines.



---

## OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field  Reconstruction using Omnidirectional Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2404.03202v2){: .btn .btn-green } |

**Abstract**: Photorealistic reconstruction relying on 3D Gaussian Splatting has shown
promising potential in robotics. However, the current 3D Gaussian Splatting
system only supports radiance field reconstruction using undistorted
perspective images. In this paper, we present OmniGS, a novel omnidirectional
Gaussian splatting system, to take advantage of omnidirectional images for fast
radiance field reconstruction. Specifically, we conduct a theoretical analysis
of spherical camera model derivatives in 3D Gaussian Splatting. According to
the derivatives, we then implement a new GPU-accelerated omnidirectional
rasterizer that directly splats 3D Gaussians onto the equirectangular screen
space for omnidirectional image rendering. As a result, we realize
differentiable optimization of the radiance field without the requirement of
cube-map rectification or tangent-plane approximation. Extensive experiments
conducted in egocentric and roaming scenarios demonstrate that our method
achieves state-of-the-art reconstruction quality and high rendering speed using
omnidirectional images. To benefit the research community, the code will be
made publicly available once the paper is published.

Comments:
- 7 pages, 4 figures

---

## GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Emmanouil Nikolakakis, Utkarsh Gupta, Jonathan Vengosh, Justin Bui, Razvan Marinescu | eess.IV | [PDF](http://arxiv.org/pdf/2404.03126v1){: .btn .btn-green } |

**Abstract**: We present GaSpCT, a novel view synthesis and 3D scene representation method
used to generate novel projection views for Computer Tomography (CT) scans. We
adapt the Gaussian Splatting framework to enable novel view synthesis in CT
based on limited sets of 2D image projections and without the need for
Structure from Motion (SfM) methodologies. Therefore, we reduce the total
scanning duration and the amount of radiation dose the patient receives during
the scan. We adapted the loss function to our use-case by encouraging a
stronger background and foreground distinction using two sparsity promoting
regularizers: a beta loss and a total variation (TV) loss. Finally, we
initialize the Gaussian locations across the 3D space using a uniform prior
distribution of where the brain's positioning would be expected to be within
the field of view. We evaluate the performance of our model using brain CT
scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and
demonstrate that the rendered novel views closely match the original projection
views of the simulated scan, and have better performance than other implicit 3D
scene representations methodologies. Furthermore, we empirically observe
reduced training time compared to neural network based image synthesis for
sparse-view CT image reconstruction. Finally, the memory requirements of the
Gaussian Splatting representations are reduced by 17% compared to the
equivalent voxel grid image representations.

Comments:
- Under Review Process for MICCAI 2024

---

## OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features  and Rendered Novel Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2404.03650v1){: .btn .btn-green } |

**Abstract**: Large visual-language models (VLMs), like CLIP, enable open-set image
segmentation to segment arbitrary concepts from an image in a zero-shot manner.
This goes beyond the traditional closed-set assumption, i.e., where models can
only segment classes from a pre-defined training set. More recently, first
works on open-set segmentation in 3D scenes have appeared in the literature.
These methods are heavily influenced by closed-set 3D convolutional approaches
that process point clouds or polygon meshes. However, these 3D scene
representations do not align well with the image-based nature of the
visual-language models. Indeed, point cloud and 3D meshes typically have a
lower resolution than images and the reconstructed 3D scene geometry might not
project well to the underlying 2D image sequences used to compute pixel-aligned
CLIP features. To address these challenges, we propose OpenNeRF which naturally
operates on posed images and directly encodes the VLM features within the NeRF.
This is similar in spirit to LERF, however our work shows that using pixel-wise
VLM features (instead of global CLIP features) results in an overall less
complex architecture without the need for additional DINO regularization. Our
OpenNeRF further leverages NeRF's ability to render novel views and extract
open-set VLM features from areas that are not well observed in the initial
posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF
outperforms recent open-vocabulary methods such as LERF and OpenScene by at
least +4.9 mIoU.

Comments:
- ICLR 2024, Project page: https://opennerf.github.io

---

## SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai | cs.CV | [PDF](http://arxiv.org/pdf/2404.03736v1){: .btn .btn-green } |

**Abstract**: Recent advances in 2D/3D generative models enable the generation of dynamic
3D objects from a single-view video. Existing approaches utilize score
distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D
Gaussians. However, these methods struggle to strike a balance among reference
view alignment, spatio-temporal consistency, and motion fidelity under
single-view conditions due to the implicit nature of NeRF or the intricate
dense Gaussian motion prediction. To address these issues, this paper proposes
an efficient, sparse-controlled video-to-4D framework named SC4D, that
decouples motion and appearance to achieve superior video-to-4D generation.
Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian
Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity
of the learned motion and shape. Comprehensive experimental results demonstrate
that our method surpasses existing methods in both quality and efficiency. In
addition, facilitated by the disentangled modeling of motion and appearance of
SC4D, we devise a novel application that seamlessly transfers the learned
motion onto a diverse array of 4D entities according to textual descriptions.

Comments:
- Project Page: https://sc4d.github.io/

---

## RaFE: Generative Radiance Fields Restoration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.03654v2){: .btn .btn-green } |

**Abstract**: NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel
view synthesis and 3D reconstruction, but its performance is sensitive to input
image quality, which struggles to achieve high-fidelity rendering when provided
with low-quality sparse input viewpoints. Previous methods for NeRF restoration
are tailored for specific degradation type, ignoring the generality of
restoration. To overcome this limitation, we propose a generic radiance fields
restoration pipeline, named RaFE, which applies to various types of
degradations, such as low resolution, blurriness, noise, compression artifacts,
or their combinations. Our approach leverages the success of off-the-shelf 2D
restoration methods to recover the multi-view images individually. Instead of
reconstructing a blurred NeRF by averaging inconsistencies, we introduce a
novel approach using Generative Adversarial Networks (GANs) for NeRF generation
to better accommodate the geometric and appearance inconsistencies present in
the multi-view images. Specifically, we adopt a two-level tri-plane
architecture, where the coarse level remains fixed to represent the low-quality
NeRF, and a fine-level residual tri-plane to be added to the coarse level is
modeled as a distribution with GAN to capture potential variations in
restoration. We validate RaFE on both synthetic and real cases for various
restoration tasks, demonstrating superior performance in both quantitative and
qualitative evaluations, surpassing other 3D restoration methods specific to
single task. Please see our project website
https://zkaiwu.github.io/RaFE-Project/.

Comments:
- Project Page: https://zkaiwu.github.io/RaFE

---

## Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2404.03613v1){: .btn .btn-green } |

**Abstract**: As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view
synthesis, it is a natural extension to deform a canonical 3DGS to multiple
frames. However, previous works fail to accurately reconstruct dynamic scenes,
especially 1) static parts moving along nearby dynamic parts, and 2) some
dynamic areas are blurry. We attribute the failure to the wrong design of the
deformation field, which is built as a coordinate-based function. This approach
is problematic because 3DGS is a mixture of multiple fields centered at the
Gaussians, not just a single coordinate-based framework. To resolve this
problem, we define the deformation as a function of per-Gaussian embeddings and
temporal embeddings. Moreover, we decompose deformations as coarse and fine
deformations to model slow and fast movements, respectively. Also, we introduce
an efficient training strategy for faster convergence and higher quality.
Project page: https://jeongminb.github.io/e-d3dgs/

Comments:
- Preprint

---

## VF-NeRF: Viewshed Fields for Rigid NeRF Registration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Leo Segre, Shai Avidan | cs.CV | [PDF](http://arxiv.org/pdf/2404.03349v1){: .btn .btn-green } |

**Abstract**: 3D scene registration is a fundamental problem in computer vision that seeks
the best 6-DoF alignment between two scenes. This problem was extensively
investigated in the case of point clouds and meshes, but there has been
relatively limited work regarding Neural Radiance Fields (NeRF). In this paper,
we consider the problem of rigid registration between two NeRFs when the
position of the original cameras is not given. Our key novelty is the
introduction of Viewshed Fields (VF), an implicit function that determines, for
each 3D point, how likely it is to be viewed by the original cameras. We
demonstrate how VF can help in the various stages of NeRF registration, with an
extensive evaluation showing that VF-NeRF achieves SOTA results on various
datasets with different capturing approaches such as LLFF and Objaverese.



---

## Neural Radiance Fields with Torch Units

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Bingnan Ni, Huanyu Wang, Dongfeng Bai, Minghe Weng, Dexin Qi, Weichao Qiu, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.02617v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction
methods widely used in industrial applications. Although prevalent methods
achieve considerable improvements in small-scale scenes, accomplishing
reconstruction in complex and large-scale scenes is still challenging. First,
the background in complex scenes shows a large variance among different views.
Second, the current inference pattern, $i.e.$, a pixel only relies on an
individual camera ray, fails to capture contextual information. To solve these
problems, we propose to enlarge the ray perception field and build up the
sample points interactions. In this paper, we design a novel inference pattern
that encourages a single camera ray possessing more contextual information, and
models the relationship among sample points on each camera ray. To hold
contextual information,a camera ray in our proposed method can render a patch
of pixels simultaneously. Moreover, we replace the MLP in neural radiance field
models with distance-aware convolutions to enhance the feature propagation
among sample points from the same camera ray. To summarize, as a torchlight, a
ray in our proposed method achieves rendering a patch of image. Thus, we call
the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF
show that the Torch-NeRF exhibits excellent performance.



---

## TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding  Autonomous Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.02410v1){: .btn .btn-green } |

**Abstract**: Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize
3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR
data capabilities but also overlooks the potential advantages of fusing LiDAR
with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera
Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both
LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and
novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D
mesh) and implicit (hierarchical octree feature) 3D representation derived from
LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D
Gaussian's properties are not only initialized in alignment with the 3D mesh
which provides more completed 3D shape and color information, but are also
endowed with broader contextual information through retrieved octree implicit
features. During the Gaussian Splatting optimization process, the 3D mesh
offers dense depth information as supervision, which enhances the training
process by learning of a robust geometry. Comprehensive evaluations conducted
on the Waymo Open Dataset and nuScenes Dataset validate our method's
state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our
method demonstrates fast training and achieves real-time RGB and depth
rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in
resolution of 1600x900 (nuScenes) in urban scenarios.



---

## LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2404.02742v1){: .btn .btn-green } |

**Abstract**: Although neural radiance fields (NeRFs) have achieved triumphs in image novel
view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS
methods employ a simple shift from image NVS methods while ignoring the dynamic
nature and the large-scale reconstruction problem of LiDAR point clouds. In
light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for
novel space-time LiDAR view synthesis. In consideration of the sparsity and
large-scale characteristics, we design a 4D hybrid representation combined with
multi-planar and grid features to achieve effective reconstruction in a
coarse-to-fine manner. Furthermore, we introduce geometric constraints derived
from point clouds to improve temporal consistency. For the realistic synthesis
of LiDAR point clouds, we incorporate the global optimization of ray-drop
probability to preserve cross-region patterns. Extensive experiments on
KITTI-360 and NuScenes datasets demonstrate the superiority of our method in
accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes
are available at https://github.com/ispc-lab/LiDAR4D.

Comments:
- Accepted by CVPR 2024. Project Page:
  https://dyfcalid.github.io/LiDAR4D

---

## Freditor: High-Fidelity and Transferable NeRF Editing by Frequency  Decomposition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang | cs.CV | [PDF](http://arxiv.org/pdf/2404.02514v1){: .btn .btn-green } |

**Abstract**: This paper enables high-fidelity, transferable NeRF editing by frequency
decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D
scenes while suffering from blurry results, and fail to capture detailed
structures caused by the inconsistency between 2D editings. Our critical
insight is that low-frequency components of images are more
multiview-consistent after editing compared with their high-frequency parts.
Moreover, the appearance style is mainly exhibited on the low-frequency
components, and the content details especially reside in high-frequency parts.
This motivates us to perform editing on low-frequency components, which results
in high-fidelity edited scenes. In addition, the editing is performed in the
low-frequency feature space, enabling stable intensity control and novel scene
transfer. Comprehensive experiments conducted on photorealistic datasets
demonstrate the superior performance of high-fidelity and transferable NeRF
editing. The project page is at \url{https://aigc3d.github.io/freditor}.



---

## GenN2N: Generative NeRF2NeRF Translation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan, Li Yi | cs.CV | [PDF](http://arxiv.org/pdf/2404.02788v1){: .btn .btn-green } |

**Abstract**: We present GenN2N, a unified NeRF-to-NeRF translation framework for various
NeRF translation tasks such as text-driven NeRF editing, colorization,
super-resolution, inpainting, etc. Unlike previous methods designed for
individual translation tasks with task-specific schemes, GenN2N achieves all
these NeRF editing tasks by employing a plug-and-play image-to-image translator
to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF
space. Since the 3D consistency of 2D edits may not be assured, we propose to
model the distribution of the underlying 3D edits through a generative model
that can cover all possible edited NeRFs. To model the distribution of 3D
edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes
images while decoding NeRFs. The latent space is trained to align with a
Gaussian distribution and the NeRFs are supervised through an adversarial loss
on its renderings. To ensure the latent code does not depend on 2D viewpoints
but truly reflects the 3D edits, we also regularize the latent code through a
contrastive learning scheme. Extensive experiments on various editing tasks
show GenN2N, as a universal framework, performs as well or better than
task-specific specialists while possessing flexible generative power. More
results on our project page: https://xiangyueliu.github.io/GenN2N/

Comments:
- Accepted to CVPR 2024. Project page:
  https://xiangyueliu.github.io/GenN2N/

---

## Uncertainty-aware Active Learning of NeRF-based Object Models for Robot  Manipulators using Visual and Re-orientation Actions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Saptarshi Dasgupta, Akshat Gupta, Shreshth Tuli, Rohan Paul | cs.RO | [PDF](http://arxiv.org/pdf/2404.01812v1){: .btn .btn-green } |

**Abstract**: Manipulating unseen objects is challenging without a 3D representation, as
objects generally have occluded surfaces. This requires physical interaction
with objects to build their internal representations. This paper presents an
approach that enables a robot to rapidly learn the complete 3D model of a given
object for manipulation in unfamiliar orientations. We use an ensemble of
partially constructed NeRF models to quantify model uncertainty to determine
the next action (a visual or re-orientation action) by optimizing
informativeness and feasibility. Further, our approach determines when and how
to grasp and re-orient an object given its partial NeRF model and re-estimates
the object pose to rectify misalignments introduced during the interaction.
Experiments with a simulated Franka Emika Robot Manipulator operating in a
tabletop environment with benchmark objects demonstrate an improvement of (i)
14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth
reconstruction of the object surface (F-score) and (iii) 71% in the task
success rate of manipulating objects a-priori unseen orientations/stable
configurations in the scene; over current methods. The project page can be
found here: https://actnerf.github.io.

Comments:
- This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible

---

## Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Joshua Ahn, Haochen Wang, Raymond A. Yeh, Greg Shakhnarovich | cs.CV | [PDF](http://arxiv.org/pdf/2404.02155v1){: .btn .btn-green } |

**Abstract**: Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of
volumetric densities in neural radiance fields, i.e., the densities double when
scene size is halved, and vice versa. We call this property alpha invariance.
For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing
both distance and volume densities in log space, and 2) a
discretization-agnostic initialization strategy to guarantee high ray
transmittance. We revisit a few popular radiance field models and find that
these systems use various heuristics to deal with issues arising from scene
scaling. We test their behaviors and show our recipe to be more robust.

Comments:
- CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance

---

## Surface Reconstruction from Gaussian Splatting via Novel Stereo Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Yaniv Wolf, Amit Bracha, Ron Kimmel | cs.CV | [PDF](http://arxiv.org/pdf/2404.01810v1){: .btn .btn-green } |

**Abstract**: The Gaussian splatting for radiance field rendering method has recently
emerged as an efficient approach for accurate scene representation. It
optimizes the location, size, color, and shape of a cloud of 3D Gaussian
elements to visually match, after projection, or splatting, a set of given
images taken from various viewing directions. And yet, despite the proximity of
Gaussian elements to the shape boundaries, direct surface reconstruction of
objects in the scene is a challenge.
  We propose a novel approach for surface reconstruction from Gaussian
splatting models. Rather than relying on the Gaussian elements' locations as a
prior for surface reconstruction, we leverage the superior novel-view synthesis
capabilities of 3DGS. To that end, we use the Gaussian splatting model to
render pairs of stereo-calibrated novel views from which we extract depth
profiles using a stereo matching method. We then combine the extracted RGB-D
images into a geometrically consistent surface. The resulting reconstruction is
more accurate and shows finer details when compared to other methods for
surface reconstruction from Gaussian splatting models, while requiring
significantly less compute time compared to other surface reconstruction
methods.
  We performed extensive testing of the proposed method on in-the-wild scenes,
taken by a smartphone, showcasing its superior reconstruction abilities.
Additionally, we tested the proposed method on the Tanks and Temples benchmark,
and it has surpassed the current leading method for surface reconstruction from
Gaussian splatting models. Project page: https://gs2mesh.github.io/.

Comments:
- Project Page: https://gs2mesh.github.io/

---

## NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for  Memory-Efficient Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Sicheng Li, Hao Li, Yiyi Liao, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.02185v1){: .btn .btn-green } |

**Abstract**: The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene
modeling and novel-view synthesis. As a kind of visual media for 3D scene
representation, compression with high rate-distortion performance is an eternal
target. Motivated by advances in neural compression and neural field
representation, we propose NeRFCodec, an end-to-end NeRF compression framework
that integrates non-linear transform, quantization, and entropy coding for
memory-efficient scene representation. Since training a non-linear transform
directly on a large scale of NeRF feature planes is impractical, we discover
that pre-trained neural 2D image codec can be utilized for compressing the
features when adding content-specific parameters. Specifically, we reuse neural
2D image codec but modify its encoder and decoder heads, while keeping the
other parts of the pre-trained decoder frozen. This allows us to train the full
pipeline via supervision of rendering loss and entropy loss, yielding the
rate-distortion balance by updating the content-specific parameters. At test
time, the bitstreams containing latent code, feature decoder head, and other
side information are transmitted for communication. Experimental results
demonstrate our method outperforms existing NeRF compression methods, enabling
high-quality novel view synthesis with a memory budget of 0.5 MB.

Comments:
- Accepted at CVPR2024. The source code will be released

---

## StructLDM: Structured Latent Diffusion for 3D Human Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Tao Hu, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.01241v2){: .btn .btn-green } |

**Abstract**: Recent 3D human generative models have achieved remarkable progress by
learning 3D-aware GANs from 2D images. However, existing 3D human generative
methods model humans in a compact 1D latent space, ignoring the articulated
structure and semantics of human body topology. In this paper, we explore more
expressive and higher-dimensional latent space for 3D human modeling and
propose StructLDM, a diffusion-based unconditional 3D human generative model,
which is learned from 2D images. StructLDM solves the challenges imposed due to
the high-dimensional growth of latent space with three key designs: 1) A
semantic structured latent space defined on the dense surface manifold of a
statistical human body template. 2) A structured 3D-aware auto-decoder that
factorizes the global latent space into several semantic body parts
parameterized by a set of conditional structured local NeRFs anchored to the
body template, which embeds the properties learned from the 2D training data
and can be decoded to render view-consistent humans under different poses and
clothing styles. 3) A structured latent diffusion model for generative human
appearance sampling. Extensive experiments validate StructLDM's
state-of-the-art generation performance and illustrate the expressiveness of
the structured latent space over the well-adopted 1D latent space. Notably,
StructLDM enables different levels of controllable 3D human generation and
editing, including pose/view/shape control, and high-level tasks including
compositional generations, part-aware clothing editing, 3D virtual try-on, etc.
Our project page is at: https://taohuumd.github.io/projects/StructLDM/.

Comments:
- Project page: https://taohuumd.github.io/projects/StructLDM/

---

## DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable  Primitive Assembly

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.00875v2){: .btn .btn-green } |

**Abstract**: We present a differentiable rendering framework to learn structured 3D
abstractions in the form of primitive assemblies from sparse RGB images
capturing a 3D object. By leveraging differentiable volume rendering, our
method does not require 3D supervision. Architecturally, our network follows
the general pipeline of an image-conditioned neural radiance field (NeRF)
exemplified by pixelNeRF for color prediction. As our core contribution, we
introduce differential primitive assembly (DPA) into NeRF to output a 3D
occupancy field in place of density prediction, where the predicted occupancies
serve as opacity values for volume rendering. Our network, coined DPA-Net,
produces a union of convexes, each as an intersection of convex quadric
primitives, to approximate the target 3D object, subject to an abstraction loss
and a masking loss, both defined in the image space upon volume rendering. With
test-time adaptation and additional sampling and loss designs aimed at
improving the accuracy and compactness of the obtained assemblies, our method
demonstrates superior performance over state-of-the-art alternatives for 3D
primitive abstraction from sparse views.

Comments:
- 14 pages

---

## CityGaussian: Real-time High-quality Large-Scale Scene Rendering with  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.01133v1){: .btn .btn-green } |

**Abstract**: The advancement of real-time 3D scene reconstruction and novel view synthesis
has been significantly propelled by 3D Gaussian Splatting (3DGS). However,
effectively training large-scale 3DGS and rendering it in real-time across
various scales remains challenging. This paper introduces CityGaussian
(CityGS), which employs a novel divide-and-conquer training approach and
Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and
rendering. Specifically, the global scene prior and adaptive training data
selection enables efficient training and seamless fusion. Based on fused
Gaussian primitives, we generate different detail levels through compression,
and realize fast rendering across various scales through the proposed
block-wise detail levels selection and aggregation strategy. Extensive
experimental results on large-scale scenes demonstrate that our approach
attains state-of-theart rendering quality, enabling consistent real-time
rendering of largescale scenes across vastly different scales. Our project page
is available at https://dekuliutesla.github.io/citygs/.

Comments:
- Project Page: https://dekuliutesla.github.io/citygs/

---

## SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency  Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yuru Xiao, Xianming Liu, Deming Zhai, Kui Jiang, Junjun Jiang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2404.00992v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) technology has made significant strides in
creating novel viewpoints. However, its effectiveness is hampered when working
with sparsely available views, often leading to performance dips due to
overfitting. FreeNeRF attempts to overcome this limitation by integrating
implicit geometry regularization, which incrementally improves both geometry
and textures. Nonetheless, an initial low positional encoding bandwidth results
in the exclusion of high-frequency elements. The quest for a holistic approach
that simultaneously addresses overfitting and the preservation of
high-frequency details remains ongoing. This study introduces a novel feature
matching based sparse geometry regularization module. This module excels in
pinpointing high-frequency keypoints, thereby safeguarding the integrity of
fine details. Through progressive refinement of geometry and textures across
NeRF iterations, we unveil an effective few-shot neural rendering architecture,
designated as SGCNeRF, for enhanced novel view synthesis. Our experiments
demonstrate that SGCNeRF not only achieves superior geometry-consistent
outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in
PSNR on the LLFF and DTU datasets, respectively.



---

## FlexiDreamer: Single Image-to-3D Generation with FlexiCubes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ruowen Zhao, Zhengyi Wang, Yikai Wang, Zihan Zhou, Jun Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00987v1){: .btn .btn-green } |

**Abstract**: 3D content generation from text prompts or single images has made remarkable
progress in quality and speed recently. One of its dominant paradigms involves
generating consistent multi-view images followed by a sparse-view
reconstruction. However, due to the challenge of directly deforming the mesh
representation to approach the target topology, most methodologies learn an
implicit representation (such as NeRF) during the sparse-view reconstruction
and acquire the target mesh by a post-processing extraction. Although the
implicit representation can effectively model rich 3D information, its training
typically entails a long convergence time. In addition, the post-extraction
operation from the implicit field also leads to undesirable visual artifacts.
In this paper, we propose FlexiDreamer, a novel single image-to-3d generation
framework that reconstructs the target mesh in an end-to-end manner. By
leveraging a flexible gradient-based extraction known as FlexiCubes, our method
circumvents the defects brought by the post-processing and facilitates a direct
acquisition of the target mesh. Furthermore, we incorporate a multi-resolution
hash grid encoding scheme that progressively activates the encoding levels into
the implicit field in FlexiCubes to help capture geometric details for per-step
optimization. Notably, FlexiDreamer recovers a dense 3D structure from a
single-view image in approximately 1 minute on a single NVIDIA A100 GPU,
outperforming previous methodologies by a large margin.

Comments:
- project page:https://flexidreamer.github.io

---

## NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus | cs.CV | [PDF](http://arxiv.org/pdf/2404.01300v1){: .btn .btn-green } |

**Abstract**: Neural fields excel in computer vision and robotics due to their ability to
understand the 3D visual world such as inferring semantics, geometry, and
dynamics. Given the capabilities of neural fields in densely representing a 3D
scene from 2D images, we ask the question: Can we scale their self-supervised
pretraining, specifically using masked autoencoders, to generate effective 3D
representations from posed RGB images. Owing to the astounding success of
extending transformers to novel data modalities, we employ standard 3D Vision
Transformers to suit the unique formulation of NeRFs. We leverage NeRF's
volumetric grid as a dense input to the transformer, contrasting it with other
3D representations such as pointclouds where the information density can be
uneven, and the representation is irregular. Due to the difficulty of applying
masked autoencoders to an implicit representation, such as NeRF, we opt for
extracting an explicit representation that canonicalizes scenes across domains
by employing the camera trajectory for sampling. Our goal is made possible by
masking random patches from NeRF's radiance and density grid and employing a
standard 3D Swin Transformer to reconstruct the masked patches. In doing so,
the model can learn the semantic and spatial structure of complete scenes. We
pretrain this representation at scale on our proposed curated posed-RGB data,
totaling over 1.6 million images. Once pretrained, the encoder is used for
effective 3D transfer learning. Our novel self-supervised pretraining for
NeRFs, NeRF-MAE, scales remarkably well and improves performance on various
challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,
NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF
scene understanding baselines on Front3D and ScanNet datasets with an absolute
performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.

Comments:
- 29 pages, 13 figures. Project Page: https://nerf-mae.github.io/

---

## DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jie Long Lee, Chen Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2404.00874v1){: .btn .btn-green } |

**Abstract**: We present DiSR-NeRF, a diffusion-guided framework for view-consistent
super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement
for high-resolution (HR) reference images by leveraging existing powerful 2D
super-resolution models. Nonetheless, independent SR 2D images are often
inconsistent across different views. We thus propose Iterative 3D
Synchronization (I3DS) to mitigate the inconsistency problem via the inherent
multi-view consistency property of NeRF. Specifically, our I3DS alternates
between upscaling low-resolution (LR) rendered images with diffusion models,
and updating the underlying 3D representation with standard NeRF training. We
further introduce Renoised Score Distillation (RSD), a novel score-distillation
objective for 2D image resolution. Our RSD combines features from ancestral
sampling and Score Distillation Sampling (SDS) to generate sharp images that
are also LR-consistent. Qualitative and quantitative results on both synthetic
and real-world datasets demonstrate that our DiSR-NeRF can achieve better
results on NeRF super-resolution compared with existing works. Code and video
results available at the project website.



---

## MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00923v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping is essential for position tracking and
scene understanding. 3D Gaussian-based map representations enable
photorealistic reconstruction and real-time rendering of scenes using multiple
posed cameras. We show for the first time that using 3D Gaussians for map
representation with unposed camera images and inertial measurements can enable
accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural
radiance field-based representations by enabling faster rendering, scale
awareness, and improved trajectory tracking. Our framework enables
keyframe-based mapping and tracking utilizing loss functions that incorporate
relative pose transformations from pre-integrated inertial measurements, depth
estimates, and measures of photometric rendering quality. We also release a
multi-modal dataset, UT-MM, collected from a mobile robot equipped with a
camera and an inertial measurement unit. Experimental evaluation on several
scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking
and 5% improvement in photometric rendering quality compared to the current
3DGS SLAM state-of-the-art, while allowing real-time rendering of a
high-resolution dense 3D map. Project Webpage:
https://vita-group.github.io/MM3DGS-SLAM

Comments:
- Project Webpage: https://vita-group.github.io/MM3DGS-SLAM

---

## NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented  Camera Pose Regressor and Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Juyeop Han, Lukas Lao Beyer, Guilherme V. Cavalheiro, Sertac Karaman | cs.RO | [PDF](http://arxiv.org/pdf/2404.01400v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful
tool for 3D reconstruction and novel view synthesis. However, the computational
cost of NeRF rendering and degradation in quality due to the presence of
artifacts pose significant challenges for its application in real-time and
robust robotic tasks, especially on embedded systems. This paper introduces a
novel framework that integrates NeRF-derived localization information with
Visual-Inertial Odometry(VIO) to provide a robust solution for robotic
navigation in a real-time. By training an absolute pose regression network with
augmented image data rendered from a NeRF and quantifying its uncertainty, our
approach effectively counters positional drift and enhances system reliability.
We also establish a mathematically sound foundation for combining visual
inertial navigation with camera localization neural networks, considering
uncertainty under a Bayesian framework. Experimental validation in the
photorealistic simulation environment demonstrates significant improvements in
accuracy compared to a conventional VIO approach.

Comments:
- 8 pages, 5 figures, 2 tables

---

## MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Armand Comas-Massagu, Di Qiu, Menglei Chai, Marcel Bhler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2404.01296v1){: .btn .btn-green } |

**Abstract**: We introduce a novel framework for 3D human avatar generation and
personalization, leveraging text prompts to enhance user engagement and
customization. Central to our approach are key innovations aimed at overcoming
the challenges in photo-realistic avatar synthesis. Firstly, we utilize a
conditional Neural Radiance Fields (NeRF) model, trained on a large-scale
unannotated multi-view dataset, to create a versatile initial solution space
that accelerates and diversifies avatar generation. Secondly, we develop a
geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models,
to ensure superior view invariance and enable direct optimization of avatar
geometry. These foundational ideas are complemented by our optimization
pipeline built on Variational Score Distillation (VSD), which mitigates texture
loss and over-saturation issues. As supported by our extensive experiments,
these strategies collectively enable the creation of custom avatars with
unparalleled visual quality and better adherence to input text prompts. You can
find more results and videos in our website:
https://syntec-research.github.io/MagicMirror



---

## Marrying NeRF with Feature Matching for One-step Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ronghan Chen, Yang Cong, Yu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.00891v1){: .btn .btn-green } |

**Abstract**: Given the image collection of an object, we aim at building a real-time
image-based pose estimation method, which requires neither its CAD model nor
hours of object-specific training. Recent NeRF-based methods provide a
promising solution by directly optimizing the pose from pixel loss between
rendered and target images. However, during inference, they require long
converging time, and suffer from local minima, making them impractical for
real-time robot applications. We aim at solving this problem by marrying image
matching with NeRF. With 2D matches and depth rendered by NeRF, we directly
solve the pose in one step by building 2D-3D correspondences between target and
initial view, thus allowing for real-time prediction. Moreover, to improve the
accuracy of 2D-3D correspondences, we propose a 3D consistent point mining
strategy, which effectively discards unfaithful points reconstruted by NeRF.
Moreover, current NeRF-based methods naively optimizing pixel loss fail at
occluded images. Thus, we further propose a 2D matches based sampling strategy
to preclude the occluded area. Experimental results on representative datasets
prove that our method outperforms state-of-the-art methods, and improves
inference efficiency by 90x, achieving real-time prediction at 6 FPS.

Comments:
- ICRA, 2024. Video https://www.youtube.com/watch?v=70fgUobOFWo

---

## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2404.01053v1){: .btn .btn-green } |

**Abstract**: We present HAHA - a novel approach for animatable human avatar generation
from monocular input videos. The proposed method relies on learning the
trade-off between the use of Gaussian splatting and a textured mesh for
efficient and high fidelity rendering. We demonstrate its efficiency to animate
and render full-body human avatars controlled via the SMPL-X parametric model.
Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh
where it is necessary, like hair and out-of-mesh clothing. This results in a
minimal number of Gaussians being used to represent the full avatar, and
reduced rendering artifacts. This allows us to handle the animation of small
body parts such as fingers that are traditionally disregarded. We demonstrate
the effectiveness of our approach on two open datasets: SnapshotPeople and
X-Humans. Our method demonstrates on par reconstruction quality to the
state-of-the-art on SnapshotPeople, while using less than a third of Gaussians.
HAHA outperforms previous state-of-the-art on novel poses from X-Humans both
quantitatively and qualitatively.



---

## Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2404.01168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the
realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much
like its predecessor Neural Radiance Fields (NeRF), struggles to accurately
model physical reflections, particularly in mirrors that are ubiquitous in
real-world scenes. This oversight mistakenly perceives reflections as separate
entities that physically exist, resulting in inaccurate reconstructions and
inconsistent reflective properties across varied viewpoints. To address this
pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework
devised to master the intricacies of mirror geometries and reflections, paving
the way for the generation of realistically depicted mirror reflections. By
ingeniously incorporating mirror attributes into the 3DGS and leveraging the
principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to
observe from behind the mirror, enriching the realism of scene renderings.
Extensive assessments, spanning both synthetic and real-world scenes, showcase
our method's ability to render novel views with enhanced fidelity in real-time,
surpassing the state-of-the-art Mirror-NeRF specifically within the challenging
mirror regions. Our code will be made publicly available for reproducible
research.

Comments:
- 22 pages, 7 figures
