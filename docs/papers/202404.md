---
layout: default
title: April 2024
parent: Papers
nav_order: 202404
---

<!---metadata--->


## GenN2N: Generative NeRF2NeRF Translation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan, Li Yi | cs.CV | [PDF](http://arxiv.org/pdf/2404.02788v1){: .btn .btn-green } |

**Abstract**: We present GenN2N, a unified NeRF-to-NeRF translation framework for various
NeRF translation tasks such as text-driven NeRF editing, colorization,
super-resolution, inpainting, etc. Unlike previous methods designed for
individual translation tasks with task-specific schemes, GenN2N achieves all
these NeRF editing tasks by employing a plug-and-play image-to-image translator
to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF
space. Since the 3D consistency of 2D edits may not be assured, we propose to
model the distribution of the underlying 3D edits through a generative model
that can cover all possible edited NeRFs. To model the distribution of 3D
edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes
images while decoding NeRFs. The latent space is trained to align with a
Gaussian distribution and the NeRFs are supervised through an adversarial loss
on its renderings. To ensure the latent code does not depend on 2D viewpoints
but truly reflects the 3D edits, we also regularize the latent code through a
contrastive learning scheme. Extensive experiments on various editing tasks
show GenN2N, as a universal framework, performs as well or better than
task-specific specialists while possessing flexible generative power. More
results on our project page: https://xiangyueliu.github.io/GenN2N/

Comments:
- Accepted to CVPR 2024. Project page:
  https://xiangyueliu.github.io/GenN2N/

---

## Freditor: High-Fidelity and Transferable NeRF Editing by Frequency  Decomposition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang | cs.CV | [PDF](http://arxiv.org/pdf/2404.02514v1){: .btn .btn-green } |

**Abstract**: This paper enables high-fidelity, transferable NeRF editing by frequency
decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D
scenes while suffering from blurry results, and fail to capture detailed
structures caused by the inconsistency between 2D editings. Our critical
insight is that low-frequency components of images are more
multiview-consistent after editing compared with their high-frequency parts.
Moreover, the appearance style is mainly exhibited on the low-frequency
components, and the content details especially reside in high-frequency parts.
This motivates us to perform editing on low-frequency components, which results
in high-fidelity edited scenes. In addition, the editing is performed in the
low-frequency feature space, enabling stable intensity control and novel scene
transfer. Comprehensive experiments conducted on photorealistic datasets
demonstrate the superior performance of high-fidelity and transferable NeRF
editing. The project page is at \url{https://aigc3d.github.io/freditor}.



---

## TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding  Autonomous Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.02410v1){: .btn .btn-green } |

**Abstract**: Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize
3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR
data capabilities but also overlooks the potential advantages of fusing LiDAR
with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera
Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both
LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and
novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D
mesh) and implicit (hierarchical octree feature) 3D representation derived from
LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D
Gaussian's properties are not only initialized in alignment with the 3D mesh
which provides more completed 3D shape and color information, but are also
endowed with broader contextual information through retrieved octree implicit
features. During the Gaussian Splatting optimization process, the 3D mesh
offers dense depth information as supervision, which enhances the training
process by learning of a robust geometry. Comprehensive evaluations conducted
on the Waymo Open Dataset and nuScenes Dataset validate our method's
state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our
method demonstrates fast training and achieves real-time RGB and depth
rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in
resolution of 1600x900 (nuScenes) in urban scenarios.



---

## LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2404.02742v1){: .btn .btn-green } |

**Abstract**: Although neural radiance fields (NeRFs) have achieved triumphs in image novel
view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS
methods employ a simple shift from image NVS methods while ignoring the dynamic
nature and the large-scale reconstruction problem of LiDAR point clouds. In
light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for
novel space-time LiDAR view synthesis. In consideration of the sparsity and
large-scale characteristics, we design a 4D hybrid representation combined with
multi-planar and grid features to achieve effective reconstruction in a
coarse-to-fine manner. Furthermore, we introduce geometric constraints derived
from point clouds to improve temporal consistency. For the realistic synthesis
of LiDAR point clouds, we incorporate the global optimization of ray-drop
probability to preserve cross-region patterns. Extensive experiments on
KITTI-360 and NuScenes datasets demonstrate the superiority of our method in
accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes
are available at https://github.com/ispc-lab/LiDAR4D.

Comments:
- Accepted by CVPR 2024. Project Page:
  https://dyfcalid.github.io/LiDAR4D

---

## Neural Radiance Fields with Torch Units

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Bingnan Ni, Huanyu Wang, Dongfeng Bai, Minghe Weng, Dexin Qi, Weichao Qiu, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.02617v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction
methods widely used in industrial applications. Although prevalent methods
achieve considerable improvements in small-scale scenes, accomplishing
reconstruction in complex and large-scale scenes is still challenging. First,
the background in complex scenes shows a large variance among different views.
Second, the current inference pattern, $i.e.$, a pixel only relies on an
individual camera ray, fails to capture contextual information. To solve these
problems, we propose to enlarge the ray perception field and build up the
sample points interactions. In this paper, we design a novel inference pattern
that encourages a single camera ray possessing more contextual information, and
models the relationship among sample points on each camera ray. To hold
contextual information,a camera ray in our proposed method can render a patch
of pixels simultaneously. Moreover, we replace the MLP in neural radiance field
models with distance-aware convolutions to enhance the feature propagation
among sample points from the same camera ray. To summarize, as a torchlight, a
ray in our proposed method achieves rendering a patch of image. Thus, we call
the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF
show that the Torch-NeRF exhibits excellent performance.



---

## Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Joshua Ahn, Haochen Wang, Raymond A. Yeh, Greg Shakhnarovich | cs.CV | [PDF](http://arxiv.org/pdf/2404.02155v1){: .btn .btn-green } |

**Abstract**: Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of
volumetric densities in neural radiance fields, i.e., the densities double when
scene size is halved, and vice versa. We call this property alpha invariance.
For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing
both distance and volume densities in log space, and 2) a
discretization-agnostic initialization strategy to guarantee high ray
transmittance. We revisit a few popular radiance field models and find that
these systems use various heuristics to deal with issues arising from scene
scaling. We test their behaviors and show our recipe to be more robust.

Comments:
- CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance

---

## Uncertainty-aware Active Learning of NeRF-based Object Models for Robot  Manipulators using Visual and Re-orientation Actions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Saptarshi Dasgupta, Akshat Gupta, Shreshth Tuli, Rohan Paul | cs.RO | [PDF](http://arxiv.org/pdf/2404.01812v1){: .btn .btn-green } |

**Abstract**: Manipulating unseen objects is challenging without a 3D representation, as
objects generally have occluded surfaces. This requires physical interaction
with objects to build their internal representations. This paper presents an
approach that enables a robot to rapidly learn the complete 3D model of a given
object for manipulation in unfamiliar orientations. We use an ensemble of
partially constructed NeRF models to quantify model uncertainty to determine
the next action (a visual or re-orientation action) by optimizing
informativeness and feasibility. Further, our approach determines when and how
to grasp and re-orient an object given its partial NeRF model and re-estimates
the object pose to rectify misalignments introduced during the interaction.
Experiments with a simulated Franka Emika Robot Manipulator operating in a
tabletop environment with benchmark objects demonstrate an improvement of (i)
14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth
reconstruction of the object surface (F-score) and (iii) 71% in the task
success rate of manipulating objects a-priori unseen orientations/stable
configurations in the scene; over current methods. The project page can be
found here: https://actnerf.github.io.

Comments:
- This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible

---

## Surface Reconstruction from Gaussian Splatting via Novel Stereo Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Yaniv Wolf, Amit Bracha, Ron Kimmel | cs.CV | [PDF](http://arxiv.org/pdf/2404.01810v1){: .btn .btn-green } |

**Abstract**: The Gaussian splatting for radiance field rendering method has recently
emerged as an efficient approach for accurate scene representation. It
optimizes the location, size, color, and shape of a cloud of 3D Gaussian
elements to visually match, after projection, or splatting, a set of given
images taken from various viewing directions. And yet, despite the proximity of
Gaussian elements to the shape boundaries, direct surface reconstruction of
objects in the scene is a challenge.
  We propose a novel approach for surface reconstruction from Gaussian
splatting models. Rather than relying on the Gaussian elements' locations as a
prior for surface reconstruction, we leverage the superior novel-view synthesis
capabilities of 3DGS. To that end, we use the Gaussian splatting model to
render pairs of stereo-calibrated novel views from which we extract depth
profiles using a stereo matching method. We then combine the extracted RGB-D
images into a geometrically consistent surface. The resulting reconstruction is
more accurate and shows finer details when compared to other methods for
surface reconstruction from Gaussian splatting models, while requiring
significantly less compute time compared to other surface reconstruction
methods.
  We performed extensive testing of the proposed method on in-the-wild scenes,
taken by a smartphone, showcasing its superior reconstruction abilities.
Additionally, we tested the proposed method on the Tanks and Temples benchmark,
and it has surpassed the current leading method for surface reconstruction from
Gaussian splatting models. Project page: https://gs2mesh.github.io/.

Comments:
- Project Page: https://gs2mesh.github.io/

---

## NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for  Memory-Efficient Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Sicheng Li, Hao Li, Yiyi Liao, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.02185v1){: .btn .btn-green } |

**Abstract**: The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene
modeling and novel-view synthesis. As a kind of visual media for 3D scene
representation, compression with high rate-distortion performance is an eternal
target. Motivated by advances in neural compression and neural field
representation, we propose NeRFCodec, an end-to-end NeRF compression framework
that integrates non-linear transform, quantization, and entropy coding for
memory-efficient scene representation. Since training a non-linear transform
directly on a large scale of NeRF feature planes is impractical, we discover
that pre-trained neural 2D image codec can be utilized for compressing the
features when adding content-specific parameters. Specifically, we reuse neural
2D image codec but modify its encoder and decoder heads, while keeping the
other parts of the pre-trained decoder frozen. This allows us to train the full
pipeline via supervision of rendering loss and entropy loss, yielding the
rate-distortion balance by updating the content-specific parameters. At test
time, the bitstreams containing latent code, feature decoder head, and other
side information are transmitted for communication. Experimental results
demonstrate our method outperforms existing NeRF compression methods, enabling
high-quality novel view synthesis with a memory budget of 0.5 MB.

Comments:
- Accepted at CVPR2024. The source code will be released

---

## DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable  Primitive Assembly

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.00875v2){: .btn .btn-green } |

**Abstract**: We present a differentiable rendering framework to learn structured 3D
abstractions in the form of primitive assemblies from sparse RGB images
capturing a 3D object. By leveraging differentiable volume rendering, our
method does not require 3D supervision. Architecturally, our network follows
the general pipeline of an image-conditioned neural radiance field (NeRF)
exemplified by pixelNeRF for color prediction. As our core contribution, we
introduce differential primitive assembly (DPA) into NeRF to output a 3D
occupancy field in place of density prediction, where the predicted occupancies
serve as opacity values for volume rendering. Our network, coined DPA-Net,
produces a union of convexes, each as an intersection of convex quadric
primitives, to approximate the target 3D object, subject to an abstraction loss
and a masking loss, both defined in the image space upon volume rendering. With
test-time adaptation and additional sampling and loss designs aimed at
improving the accuracy and compactness of the obtained assemblies, our method
demonstrates superior performance over state-of-the-art alternatives for 3D
primitive abstraction from sparse views.

Comments:
- 14 pages

---

## SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency  Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yuru Xiao, Xianming Liu, Deming Zhai, Kui Jiang, Junjun Jiang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2404.00992v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) technology has made significant strides in
creating novel viewpoints. However, its effectiveness is hampered when working
with sparsely available views, often leading to performance dips due to
overfitting. FreeNeRF attempts to overcome this limitation by integrating
implicit geometry regularization, which incrementally improves both geometry
and textures. Nonetheless, an initial low positional encoding bandwidth results
in the exclusion of high-frequency elements. The quest for a holistic approach
that simultaneously addresses overfitting and the preservation of
high-frequency details remains ongoing. This study introduces a novel feature
matching based sparse geometry regularization module. This module excels in
pinpointing high-frequency keypoints, thereby safeguarding the integrity of
fine details. Through progressive refinement of geometry and textures across
NeRF iterations, we unveil an effective few-shot neural rendering architecture,
designated as SGCNeRF, for enhanced novel view synthesis. Our experiments
demonstrate that SGCNeRF not only achieves superior geometry-consistent
outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in
PSNR on the LLFF and DTU datasets, respectively.



---

## Marrying NeRF with Feature Matching for One-step Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ronghan Chen, Yang Cong, Yu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.00891v1){: .btn .btn-green } |

**Abstract**: Given the image collection of an object, we aim at building a real-time
image-based pose estimation method, which requires neither its CAD model nor
hours of object-specific training. Recent NeRF-based methods provide a
promising solution by directly optimizing the pose from pixel loss between
rendered and target images. However, during inference, they require long
converging time, and suffer from local minima, making them impractical for
real-time robot applications. We aim at solving this problem by marrying image
matching with NeRF. With 2D matches and depth rendered by NeRF, we directly
solve the pose in one step by building 2D-3D correspondences between target and
initial view, thus allowing for real-time prediction. Moreover, to improve the
accuracy of 2D-3D correspondences, we propose a 3D consistent point mining
strategy, which effectively discards unfaithful points reconstruted by NeRF.
Moreover, current NeRF-based methods naively optimizing pixel loss fail at
occluded images. Thus, we further propose a 2D matches based sampling strategy
to preclude the occluded area. Experimental results on representative datasets
prove that our method outperforms state-of-the-art methods, and improves
inference efficiency by 90x, achieving real-time prediction at 6 FPS.

Comments:
- ICRA, 2024. Video https://www.youtube.com/watch?v=70fgUobOFWo

---

## MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00923v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping is essential for position tracking and
scene understanding. 3D Gaussian-based map representations enable
photorealistic reconstruction and real-time rendering of scenes using multiple
posed cameras. We show for the first time that using 3D Gaussians for map
representation with unposed camera images and inertial measurements can enable
accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural
radiance field-based representations by enabling faster rendering, scale
awareness, and improved trajectory tracking. Our framework enables
keyframe-based mapping and tracking utilizing loss functions that incorporate
relative pose transformations from pre-integrated inertial measurements, depth
estimates, and measures of photometric rendering quality. We also release a
multi-modal dataset, UT-MM, collected from a mobile robot equipped with a
camera and an inertial measurement unit. Experimental evaluation on several
scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking
and 5% improvement in photometric rendering quality compared to the current
3DGS SLAM state-of-the-art, while allowing real-time rendering of a
high-resolution dense 3D map. Project Webpage:
https://vita-group.github.io/MM3DGS-SLAM

Comments:
- Project Webpage: https://vita-group.github.io/MM3DGS-SLAM

---

## FlexiDreamer: Single Image-to-3D Generation with FlexiCubes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ruowen Zhao, Zhengyi Wang, Yikai Wang, Zihan Zhou, Jun Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00987v1){: .btn .btn-green } |

**Abstract**: 3D content generation from text prompts or single images has made remarkable
progress in quality and speed recently. One of its dominant paradigms involves
generating consistent multi-view images followed by a sparse-view
reconstruction. However, due to the challenge of directly deforming the mesh
representation to approach the target topology, most methodologies learn an
implicit representation (such as NeRF) during the sparse-view reconstruction
and acquire the target mesh by a post-processing extraction. Although the
implicit representation can effectively model rich 3D information, its training
typically entails a long convergence time. In addition, the post-extraction
operation from the implicit field also leads to undesirable visual artifacts.
In this paper, we propose FlexiDreamer, a novel single image-to-3d generation
framework that reconstructs the target mesh in an end-to-end manner. By
leveraging a flexible gradient-based extraction known as FlexiCubes, our method
circumvents the defects brought by the post-processing and facilitates a direct
acquisition of the target mesh. Furthermore, we incorporate a multi-resolution
hash grid encoding scheme that progressively activates the encoding levels into
the implicit field in FlexiCubes to help capture geometric details for per-step
optimization. Notably, FlexiDreamer recovers a dense 3D structure from a
single-view image in approximately 1 minute on a single NVIDIA A100 GPU,
outperforming previous methodologies by a large margin.

Comments:
- project page:https://flexidreamer.github.io

---

## DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jie Long Lee, Chen Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2404.00874v1){: .btn .btn-green } |

**Abstract**: We present DiSR-NeRF, a diffusion-guided framework for view-consistent
super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement
for high-resolution (HR) reference images by leveraging existing powerful 2D
super-resolution models. Nonetheless, independent SR 2D images are often
inconsistent across different views. We thus propose Iterative 3D
Synchronization (I3DS) to mitigate the inconsistency problem via the inherent
multi-view consistency property of NeRF. Specifically, our I3DS alternates
between upscaling low-resolution (LR) rendered images with diffusion models,
and updating the underlying 3D representation with standard NeRF training. We
further introduce Renoised Score Distillation (RSD), a novel score-distillation
objective for 2D image resolution. Our RSD combines features from ancestral
sampling and Score Distillation Sampling (SDS) to generate sharp images that
are also LR-consistent. Qualitative and quantitative results on both synthetic
and real-world datasets demonstrate that our DiSR-NeRF can achieve better
results on NeRF super-resolution compared with existing works. Code and video
results available at the project website.



---

## MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2404.01296v1){: .btn .btn-green } |

**Abstract**: We introduce a novel framework for 3D human avatar generation and
personalization, leveraging text prompts to enhance user engagement and
customization. Central to our approach are key innovations aimed at overcoming
the challenges in photo-realistic avatar synthesis. Firstly, we utilize a
conditional Neural Radiance Fields (NeRF) model, trained on a large-scale
unannotated multi-view dataset, to create a versatile initial solution space
that accelerates and diversifies avatar generation. Secondly, we develop a
geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models,
to ensure superior view invariance and enable direct optimization of avatar
geometry. These foundational ideas are complemented by our optimization
pipeline built on Variational Score Distillation (VSD), which mitigates texture
loss and over-saturation issues. As supported by our extensive experiments,
these strategies collectively enable the creation of custom avatars with
unparalleled visual quality and better adherence to input text prompts. You can
find more results and videos in our website:
https://syntec-research.github.io/MagicMirror



---

## Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2404.01168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the
realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much
like its predecessor Neural Radiance Fields (NeRF), struggles to accurately
model physical reflections, particularly in mirrors that are ubiquitous in
real-world scenes. This oversight mistakenly perceives reflections as separate
entities that physically exist, resulting in inaccurate reconstructions and
inconsistent reflective properties across varied viewpoints. To address this
pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework
devised to master the intricacies of mirror geometries and reflections, paving
the way for the generation of realistically depicted mirror reflections. By
ingeniously incorporating mirror attributes into the 3DGS and leveraging the
principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to
observe from behind the mirror, enriching the realism of scene renderings.
Extensive assessments, spanning both synthetic and real-world scenes, showcase
our method's ability to render novel views with enhanced fidelity in real-time,
surpassing the state-of-the-art Mirror-NeRF specifically within the challenging
mirror regions. Our code will be made publicly available for reproducible
research.

Comments:
- 22 pages, 7 figures

---

## StructLDM: Structured Latent Diffusion for 3D Human Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Tao Hu, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.01241v2){: .btn .btn-green } |

**Abstract**: Recent 3D human generative models have achieved remarkable progress by
learning 3D-aware GANs from 2D images. However, existing 3D human generative
methods model humans in a compact 1D latent space, ignoring the articulated
structure and semantics of human body topology. In this paper, we explore more
expressive and higher-dimensional latent space for 3D human modeling and
propose StructLDM, a diffusion-based unconditional 3D human generative model,
which is learned from 2D images. StructLDM solves the challenges imposed due to
the high-dimensional growth of latent space with three key designs: 1) A
semantic structured latent space defined on the dense surface manifold of a
statistical human body template. 2) A structured 3D-aware auto-decoder that
factorizes the global latent space into several semantic body parts
parameterized by a set of conditional structured local NeRFs anchored to the
body template, which embeds the properties learned from the 2D training data
and can be decoded to render view-consistent humans under different poses and
clothing styles. 3) A structured latent diffusion model for generative human
appearance sampling. Extensive experiments validate StructLDM's
state-of-the-art generation performance and illustrate the expressiveness of
the structured latent space over the well-adopted 1D latent space. Notably,
StructLDM enables different levels of controllable 3D human generation and
editing, including pose/view/shape control, and high-level tasks including
compositional generations, part-aware clothing editing, 3D virtual try-on, etc.
Our project page is at: https://taohuumd.github.io/projects/StructLDM/.

Comments:
- Project page: https://taohuumd.github.io/projects/StructLDM/

---

## NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus | cs.CV | [PDF](http://arxiv.org/pdf/2404.01300v1){: .btn .btn-green } |

**Abstract**: Neural fields excel in computer vision and robotics due to their ability to
understand the 3D visual world such as inferring semantics, geometry, and
dynamics. Given the capabilities of neural fields in densely representing a 3D
scene from 2D images, we ask the question: Can we scale their self-supervised
pretraining, specifically using masked autoencoders, to generate effective 3D
representations from posed RGB images. Owing to the astounding success of
extending transformers to novel data modalities, we employ standard 3D Vision
Transformers to suit the unique formulation of NeRFs. We leverage NeRF's
volumetric grid as a dense input to the transformer, contrasting it with other
3D representations such as pointclouds where the information density can be
uneven, and the representation is irregular. Due to the difficulty of applying
masked autoencoders to an implicit representation, such as NeRF, we opt for
extracting an explicit representation that canonicalizes scenes across domains
by employing the camera trajectory for sampling. Our goal is made possible by
masking random patches from NeRF's radiance and density grid and employing a
standard 3D Swin Transformer to reconstruct the masked patches. In doing so,
the model can learn the semantic and spatial structure of complete scenes. We
pretrain this representation at scale on our proposed curated posed-RGB data,
totaling over 1.6 million images. Once pretrained, the encoder is used for
effective 3D transfer learning. Our novel self-supervised pretraining for
NeRFs, NeRF-MAE, scales remarkably well and improves performance on various
challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,
NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF
scene understanding baselines on Front3D and ScanNet datasets with an absolute
performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.

Comments:
- 29 pages, 13 figures. Project Page: https://nerf-mae.github.io/

---

## CityGaussian: Real-time High-quality Large-Scale Scene Rendering with  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.01133v1){: .btn .btn-green } |

**Abstract**: The advancement of real-time 3D scene reconstruction and novel view synthesis
has been significantly propelled by 3D Gaussian Splatting (3DGS). However,
effectively training large-scale 3DGS and rendering it in real-time across
various scales remains challenging. This paper introduces CityGaussian
(CityGS), which employs a novel divide-and-conquer training approach and
Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and
rendering. Specifically, the global scene prior and adaptive training data
selection enables efficient training and seamless fusion. Based on fused
Gaussian primitives, we generate different detail levels through compression,
and realize fast rendering across various scales through the proposed
block-wise detail levels selection and aggregation strategy. Extensive
experimental results on large-scale scenes demonstrate that our approach
attains state-of-theart rendering quality, enabling consistent real-time
rendering of largescale scenes across vastly different scales. Our project page
is available at https://dekuliutesla.github.io/citygs/.

Comments:
- Project Page: https://dekuliutesla.github.io/citygs/

---

## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2404.01053v1){: .btn .btn-green } |

**Abstract**: We present HAHA - a novel approach for animatable human avatar generation
from monocular input videos. The proposed method relies on learning the
trade-off between the use of Gaussian splatting and a textured mesh for
efficient and high fidelity rendering. We demonstrate its efficiency to animate
and render full-body human avatars controlled via the SMPL-X parametric model.
Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh
where it is necessary, like hair and out-of-mesh clothing. This results in a
minimal number of Gaussians being used to represent the full avatar, and
reduced rendering artifacts. This allows us to handle the animation of small
body parts such as fingers that are traditionally disregarded. We demonstrate
the effectiveness of our approach on two open datasets: SnapshotPeople and
X-Humans. Our method demonstrates on par reconstruction quality to the
state-of-the-art on SnapshotPeople, while using less than a third of Gaussians.
HAHA outperforms previous state-of-the-art on novel poses from X-Humans both
quantitatively and qualitatively.



---

## NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented  Camera Pose Regressor and Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Juyeop Han, Lukas Lao Beyer, Guilherme V. Cavalheiro, Sertac Karaman | cs.RO | [PDF](http://arxiv.org/pdf/2404.01400v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful
tool for 3D reconstruction and novel view synthesis. However, the computational
cost of NeRF rendering and degradation in quality due to the presence of
artifacts pose significant challenges for its application in real-time and
robust robotic tasks, especially on embedded systems. This paper introduces a
novel framework that integrates NeRF-derived localization information with
Visual-Inertial Odometry(VIO) to provide a robust solution for robotic
navigation in a real-time. By training an absolute pose regression network with
augmented image data rendered from a NeRF and quantifying its uncertainty, our
approach effectively counters positional drift and enhances system reliability.
We also establish a mathematically sound foundation for combining visual
inertial navigation with camera localization neural networks, considering
uncertainty under a Bayesian framework. Experimental validation in the
photorealistic simulation environment demonstrates significant improvements in
accuracy compared to a conventional VIO approach.

Comments:
- 8 pages, 5 figures, 2 tables
