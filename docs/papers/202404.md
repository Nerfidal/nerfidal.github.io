---
layout: default
title: April 2024
parent: Papers
nav_order: 202404
---

<!---metadata--->


## Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Joshua Ahn, Haochen Wang, Raymond A. Yeh, Greg Shakhnarovich | cs.CV | [PDF](http://arxiv.org/pdf/2404.02155v1){: .btn .btn-green } |

**Abstract**: Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of
volumetric densities in neural radiance fields, i.e., the densities double when
scene size is halved, and vice versa. We call this property alpha invariance.
For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing
both distance and volume densities in log space, and 2) a
discretization-agnostic initialization strategy to guarantee high ray
transmittance. We revisit a few popular radiance field models and find that
these systems use various heuristics to deal with issues arising from scene
scaling. We test their behaviors and show our recipe to be more robust.

Comments:
- CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance

---

## Uncertainty-aware Active Learning of NeRF-based Object Models for Robot  Manipulators using Visual and Re-orientation Actions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Saptarshi Dasgupta, Akshat Gupta, Shreshth Tuli, Rohan Paul | cs.RO | [PDF](http://arxiv.org/pdf/2404.01812v1){: .btn .btn-green } |

**Abstract**: Manipulating unseen objects is challenging without a 3D representation, as
objects generally have occluded surfaces. This requires physical interaction
with objects to build their internal representations. This paper presents an
approach that enables a robot to rapidly learn the complete 3D model of a given
object for manipulation in unfamiliar orientations. We use an ensemble of
partially constructed NeRF models to quantify model uncertainty to determine
the next action (a visual or re-orientation action) by optimizing
informativeness and feasibility. Further, our approach determines when and how
to grasp and re-orient an object given its partial NeRF model and re-estimates
the object pose to rectify misalignments introduced during the interaction.
Experiments with a simulated Franka Emika Robot Manipulator operating in a
tabletop environment with benchmark objects demonstrate an improvement of (i)
14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth
reconstruction of the object surface (F-score) and (iii) 71% in the task
success rate of manipulating objects a-priori unseen orientations/stable
configurations in the scene; over current methods. The project page can be
found here: https://actnerf.github.io.

Comments:
- This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible

---

## Surface Reconstruction from Gaussian Splatting via Novel Stereo Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Yaniv Wolf, Amit Bracha, Ron Kimmel | cs.CV | [PDF](http://arxiv.org/pdf/2404.01810v1){: .btn .btn-green } |

**Abstract**: The Gaussian splatting for radiance field rendering method has recently
emerged as an efficient approach for accurate scene representation. It
optimizes the location, size, color, and shape of a cloud of 3D Gaussian
elements to visually match, after projection, or splatting, a set of given
images taken from various viewing directions. And yet, despite the proximity of
Gaussian elements to the shape boundaries, direct surface reconstruction of
objects in the scene is a challenge.
  We propose a novel approach for surface reconstruction from Gaussian
splatting models. Rather than relying on the Gaussian elements' locations as a
prior for surface reconstruction, we leverage the superior novel-view synthesis
capabilities of 3DGS. To that end, we use the Gaussian splatting model to
render pairs of stereo-calibrated novel views from which we extract depth
profiles using a stereo matching method. We then combine the extracted RGB-D
images into a geometrically consistent surface. The resulting reconstruction is
more accurate and shows finer details when compared to other methods for
surface reconstruction from Gaussian splatting models, while requiring
significantly less compute time compared to other surface reconstruction
methods.
  We performed extensive testing of the proposed method on in-the-wild scenes,
taken by a smartphone, showcasing its superior reconstruction abilities.
Additionally, we tested the proposed method on the Tanks and Temples benchmark,
and it has surpassed the current leading method for surface reconstruction from
Gaussian splatting models. Project page: https://gs2mesh.github.io/.

Comments:
- Project Page: https://gs2mesh.github.io/

---

## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2404.01053v1){: .btn .btn-green } |

**Abstract**: We present HAHA - a novel approach for animatable human avatar generation
from monocular input videos. The proposed method relies on learning the
trade-off between the use of Gaussian splatting and a textured mesh for
efficient and high fidelity rendering. We demonstrate its efficiency to animate
and render full-body human avatars controlled via the SMPL-X parametric model.
Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh
where it is necessary, like hair and out-of-mesh clothing. This results in a
minimal number of Gaussians being used to represent the full avatar, and
reduced rendering artifacts. This allows us to handle the animation of small
body parts such as fingers that are traditionally disregarded. We demonstrate
the effectiveness of our approach on two open datasets: SnapshotPeople and
X-Humans. Our method demonstrates on par reconstruction quality to the
state-of-the-art on SnapshotPeople, while using less than a third of Gaussians.
HAHA outperforms previous state-of-the-art on novel poses from X-Humans both
quantitatively and qualitatively.



---

## CityGaussian: Real-time High-quality Large-Scale Scene Rendering with  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.01133v1){: .btn .btn-green } |

**Abstract**: The advancement of real-time 3D scene reconstruction and novel view synthesis
has been significantly propelled by 3D Gaussian Splatting (3DGS). However,
effectively training large-scale 3DGS and rendering it in real-time across
various scales remains challenging. This paper introduces CityGaussian
(CityGS), which employs a novel divide-and-conquer training approach and
Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and
rendering. Specifically, the global scene prior and adaptive training data
selection enables efficient training and seamless fusion. Based on fused
Gaussian primitives, we generate different detail levels through compression,
and realize fast rendering across various scales through the proposed
block-wise detail levels selection and aggregation strategy. Extensive
experimental results on large-scale scenes demonstrate that our approach
attains state-of-theart rendering quality, enabling consistent real-time
rendering of largescale scenes across vastly different scales. Our project page
is available at https://dekuliutesla.github.io/citygs/.

Comments:
- Project Page: https://dekuliutesla.github.io/citygs/

---

## NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented  Camera Pose Regressor and Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Juyeop Han, Lukas Lao Beyer, Guilherme V. Cavalheiro, Sertac Karaman | cs.RO | [PDF](http://arxiv.org/pdf/2404.01400v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful
tool for 3D reconstruction and novel view synthesis. However, the computational
cost of NeRF rendering and degradation in quality due to the presence of
artifacts pose significant challenges for its application in real-time and
robust robotic tasks, especially on embedded systems. This paper introduces a
novel framework that integrates NeRF-derived localization information with
Visual-Inertial Odometry(VIO) to provide a robust solution for robotic
navigation in a real-time. By training an absolute pose regression network with
augmented image data rendered from a NeRF and quantifying its uncertainty, our
approach effectively counters positional drift and enhances system reliability.
We also establish a mathematically sound foundation for combining visual
inertial navigation with camera localization neural networks, considering
uncertainty under a Bayesian framework. Experimental validation in the
photorealistic simulation environment demonstrates significant improvements in
accuracy compared to a conventional VIO approach.

Comments:
- 8 pages, 5 figures, 2 tables

---

## NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus | cs.CV | [PDF](http://arxiv.org/pdf/2404.01300v1){: .btn .btn-green } |

**Abstract**: Neural fields excel in computer vision and robotics due to their ability to
understand the 3D visual world such as inferring semantics, geometry, and
dynamics. Given the capabilities of neural fields in densely representing a 3D
scene from 2D images, we ask the question: Can we scale their self-supervised
pretraining, specifically using masked autoencoders, to generate effective 3D
representations from posed RGB images. Owing to the astounding success of
extending transformers to novel data modalities, we employ standard 3D Vision
Transformers to suit the unique formulation of NeRFs. We leverage NeRF's
volumetric grid as a dense input to the transformer, contrasting it with other
3D representations such as pointclouds where the information density can be
uneven, and the representation is irregular. Due to the difficulty of applying
masked autoencoders to an implicit representation, such as NeRF, we opt for
extracting an explicit representation that canonicalizes scenes across domains
by employing the camera trajectory for sampling. Our goal is made possible by
masking random patches from NeRF's radiance and density grid and employing a
standard 3D Swin Transformer to reconstruct the masked patches. In doing so,
the model can learn the semantic and spatial structure of complete scenes. We
pretrain this representation at scale on our proposed curated posed-RGB data,
totaling over 1.6 million images. Once pretrained, the encoder is used for
effective 3D transfer learning. Our novel self-supervised pretraining for
NeRFs, NeRF-MAE, scales remarkably well and improves performance on various
challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,
NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF
scene understanding baselines on Front3D and ScanNet datasets with an absolute
performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.

Comments:
- 29 pages, 13 figures. Project Page: https://nerf-mae.github.io/

---

## MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2404.01296v1){: .btn .btn-green } |

**Abstract**: We introduce a novel framework for 3D human avatar generation and
personalization, leveraging text prompts to enhance user engagement and
customization. Central to our approach are key innovations aimed at overcoming
the challenges in photo-realistic avatar synthesis. Firstly, we utilize a
conditional Neural Radiance Fields (NeRF) model, trained on a large-scale
unannotated multi-view dataset, to create a versatile initial solution space
that accelerates and diversifies avatar generation. Secondly, we develop a
geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models,
to ensure superior view invariance and enable direct optimization of avatar
geometry. These foundational ideas are complemented by our optimization
pipeline built on Variational Score Distillation (VSD), which mitigates texture
loss and over-saturation issues. As supported by our extensive experiments,
these strategies collectively enable the creation of custom avatars with
unparalleled visual quality and better adherence to input text prompts. You can
find more results and videos in our website:
https://syntec-research.github.io/MagicMirror



---

## StructLDM: Structured Latent Diffusion for 3D Human Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Tao Hu, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.01241v2){: .btn .btn-green } |

**Abstract**: Recent 3D human generative models have achieved remarkable progress by
learning 3D-aware GANs from 2D images. However, existing 3D human generative
methods model humans in a compact 1D latent space, ignoring the articulated
structure and semantics of human body topology. In this paper, we explore more
expressive and higher-dimensional latent space for 3D human modeling and
propose StructLDM, a diffusion-based unconditional 3D human generative model,
which is learned from 2D images. StructLDM solves the challenges imposed due to
the high-dimensional growth of latent space with three key designs: 1) A
semantic structured latent space defined on the dense surface manifold of a
statistical human body template. 2) A structured 3D-aware auto-decoder that
factorizes the global latent space into several semantic body parts
parameterized by a set of conditional structured local NeRFs anchored to the
body template, which embeds the properties learned from the 2D training data
and can be decoded to render view-consistent humans under different poses and
clothing styles. 3) A structured latent diffusion model for generative human
appearance sampling. Extensive experiments validate StructLDM's
state-of-the-art generation performance and illustrate the expressiveness of
the structured latent space over the well-adopted 1D latent space. Notably,
StructLDM enables different levels of controllable 3D human generation and
editing, including pose/view/shape control, and high-level tasks including
compositional generations, part-aware clothing editing, 3D virtual try-on, etc.
Our project page is at: https://taohuumd.github.io/projects/StructLDM/.

Comments:
- Project page: https://taohuumd.github.io/projects/StructLDM/

---

## Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2404.01168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the
realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much
like its predecessor Neural Radiance Fields (NeRF), struggles to accurately
model physical reflections, particularly in mirrors that are ubiquitous in
real-world scenes. This oversight mistakenly perceives reflections as separate
entities that physically exist, resulting in inaccurate reconstructions and
inconsistent reflective properties across varied viewpoints. To address this
pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework
devised to master the intricacies of mirror geometries and reflections, paving
the way for the generation of realistically depicted mirror reflections. By
ingeniously incorporating mirror attributes into the 3DGS and leveraging the
principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to
observe from behind the mirror, enriching the realism of scene renderings.
Extensive assessments, spanning both synthetic and real-world scenes, showcase
our method's ability to render novel views with enhanced fidelity in real-time,
surpassing the state-of-the-art Mirror-NeRF specifically within the challenging
mirror regions. Our code will be made publicly available for reproducible
research.

Comments:
- 22 pages, 7 figures

---

## DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable  Primitive Assembly

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.00875v2){: .btn .btn-green } |

**Abstract**: We present a differentiable rendering framework to learn structured 3D
abstractions in the form of primitive assemblies from sparse RGB images
capturing a 3D object. By leveraging differentiable volume rendering, our
method does not require 3D supervision. Architecturally, our network follows
the general pipeline of an image-conditioned neural radiance field (NeRF)
exemplified by pixelNeRF for color prediction. As our core contribution, we
introduce differential primitive assembly (DPA) into NeRF to output a 3D
occupancy field in place of density prediction, where the predicted occupancies
serve as opacity values for volume rendering. Our network, coined DPA-Net,
produces a union of convexes, each as an intersection of convex quadric
primitives, to approximate the target 3D object, subject to an abstraction loss
and a masking loss, both defined in the image space upon volume rendering. With
test-time adaptation and additional sampling and loss designs aimed at
improving the accuracy and compactness of the obtained assemblies, our method
demonstrates superior performance over state-of-the-art alternatives for 3D
primitive abstraction from sparse views.

Comments:
- 14 pages

---

## DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jie Long Lee, Chen Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2404.00874v1){: .btn .btn-green } |

**Abstract**: We present DiSR-NeRF, a diffusion-guided framework for view-consistent
super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement
for high-resolution (HR) reference images by leveraging existing powerful 2D
super-resolution models. Nonetheless, independent SR 2D images are often
inconsistent across different views. We thus propose Iterative 3D
Synchronization (I3DS) to mitigate the inconsistency problem via the inherent
multi-view consistency property of NeRF. Specifically, our I3DS alternates
between upscaling low-resolution (LR) rendered images with diffusion models,
and updating the underlying 3D representation with standard NeRF training. We
further introduce Renoised Score Distillation (RSD), a novel score-distillation
objective for 2D image resolution. Our RSD combines features from ancestral
sampling and Score Distillation Sampling (SDS) to generate sharp images that
are also LR-consistent. Qualitative and quantitative results on both synthetic
and real-world datasets demonstrate that our DiSR-NeRF can achieve better
results on NeRF super-resolution compared with existing works. Code and video
results available at the project website.



---

## FlexiDreamer: Single Image-to-3D Generation with FlexiCubes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ruowen Zhao, Zhengyi Wang, Yikai Wang, Zihan Zhou, Jun Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00987v1){: .btn .btn-green } |

**Abstract**: 3D content generation from text prompts or single images has made remarkable
progress in quality and speed recently. One of its dominant paradigms involves
generating consistent multi-view images followed by a sparse-view
reconstruction. However, due to the challenge of directly deforming the mesh
representation to approach the target topology, most methodologies learn an
implicit representation (such as NeRF) during the sparse-view reconstruction
and acquire the target mesh by a post-processing extraction. Although the
implicit representation can effectively model rich 3D information, its training
typically entails a long convergence time. In addition, the post-extraction
operation from the implicit field also leads to undesirable visual artifacts.
In this paper, we propose FlexiDreamer, a novel single image-to-3d generation
framework that reconstructs the target mesh in an end-to-end manner. By
leveraging a flexible gradient-based extraction known as FlexiCubes, our method
circumvents the defects brought by the post-processing and facilitates a direct
acquisition of the target mesh. Furthermore, we incorporate a multi-resolution
hash grid encoding scheme that progressively activates the encoding levels into
the implicit field in FlexiCubes to help capture geometric details for per-step
optimization. Notably, FlexiDreamer recovers a dense 3D structure from a
single-view image in approximately 1 minute on a single NVIDIA A100 GPU,
outperforming previous methodologies by a large margin.

Comments:
- project page:https://flexidreamer.github.io

---

## MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00923v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping is essential for position tracking and
scene understanding. 3D Gaussian-based map representations enable
photorealistic reconstruction and real-time rendering of scenes using multiple
posed cameras. We show for the first time that using 3D Gaussians for map
representation with unposed camera images and inertial measurements can enable
accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural
radiance field-based representations by enabling faster rendering, scale
awareness, and improved trajectory tracking. Our framework enables
keyframe-based mapping and tracking utilizing loss functions that incorporate
relative pose transformations from pre-integrated inertial measurements, depth
estimates, and measures of photometric rendering quality. We also release a
multi-modal dataset, UT-MM, collected from a mobile robot equipped with a
camera and an inertial measurement unit. Experimental evaluation on several
scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking
and 5% improvement in photometric rendering quality compared to the current
3DGS SLAM state-of-the-art, while allowing real-time rendering of a
high-resolution dense 3D map. Project Webpage:
https://vita-group.github.io/MM3DGS-SLAM

Comments:
- Project Webpage: https://vita-group.github.io/MM3DGS-SLAM

---

## Marrying NeRF with Feature Matching for One-step Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ronghan Chen, Yang Cong, Yu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.00891v1){: .btn .btn-green } |

**Abstract**: Given the image collection of an object, we aim at building a real-time
image-based pose estimation method, which requires neither its CAD model nor
hours of object-specific training. Recent NeRF-based methods provide a
promising solution by directly optimizing the pose from pixel loss between
rendered and target images. However, during inference, they require long
converging time, and suffer from local minima, making them impractical for
real-time robot applications. We aim at solving this problem by marrying image
matching with NeRF. With 2D matches and depth rendered by NeRF, we directly
solve the pose in one step by building 2D-3D correspondences between target and
initial view, thus allowing for real-time prediction. Moreover, to improve the
accuracy of 2D-3D correspondences, we propose a 3D consistent point mining
strategy, which effectively discards unfaithful points reconstruted by NeRF.
Moreover, current NeRF-based methods naively optimizing pixel loss fail at
occluded images. Thus, we further propose a 2D matches based sampling strategy
to preclude the occluded area. Experimental results on representative datasets
prove that our method outperforms state-of-the-art methods, and improves
inference efficiency by 90x, achieving real-time prediction at 6 FPS.

Comments:
- ICRA, 2024. Video https://www.youtube.com/watch?v=70fgUobOFWo

---

## SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency  Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yuru Xiao, Xianming Liu, Deming Zhai, Kui Jiang, Junjun Jiang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2404.00992v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) technology has made significant strides in
creating novel viewpoints. However, its effectiveness is hampered when working
with sparsely available views, often leading to performance dips due to
overfitting. FreeNeRF attempts to overcome this limitation by integrating
implicit geometry regularization, which incrementally improves both geometry
and textures. Nonetheless, an initial low positional encoding bandwidth results
in the exclusion of high-frequency elements. The quest for a holistic approach
that simultaneously addresses overfitting and the preservation of
high-frequency details remains ongoing. This study introduces a novel feature
matching based sparse geometry regularization module. This module excels in
pinpointing high-frequency keypoints, thereby safeguarding the integrity of
fine details. Through progressive refinement of geometry and textures across
NeRF iterations, we unveil an effective few-shot neural rendering architecture,
designated as SGCNeRF, for enhanced novel view synthesis. Our experiments
demonstrate that SGCNeRF not only achieves superior geometry-consistent
outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in
PSNR on the LLFF and DTU datasets, respectively.


