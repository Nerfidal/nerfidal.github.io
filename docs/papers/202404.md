---
layout: default
title: April 2024
parent: Papers
nav_order: 202404
---

<!---metadata--->


## Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in  Unbounded Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Zehao Yu, Torsten Sattler, Andreas Geiger | cs.CV | [PDF](http://arxiv.org/pdf/2404.10772v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view
synthesis results, while allowing the rendering of high-resolution images in
real-time. However, leveraging 3D Gaussians for surface reconstruction poses
significant challenges due to the explicit and disconnected nature of 3D
Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel
approach for efficient, high-quality, and compact surface reconstruction in
unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of
3D Gaussians, enabling direct geometry extraction from 3D Gaussians by
identifying its levelset, without resorting to Poisson reconstruction or TSDF
fusion as in previous work. We approximate the surface normal of Gaussians as
the normal of the ray-Gaussian intersection plane, enabling the application of
regularization that significantly enhances geometry. Furthermore, we develop an
efficient geometry extraction method utilizing marching tetrahedra, where the
tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's
complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based
methods in surface reconstruction and novel view synthesis. Further, it
compares favorably to, or even outperforms, neural implicit methods in both
quality and speed.

Comments:
- Project page:
  https://niujinshuchong.github.io/gaussian-opacity-fields

---

## SRGS: Super-Resolution 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Xiang Feng, Yongbo He, Yubo Wang, Yan Yang, Zhenzhong Kuang, Yu Jun, Jianping Fan, Jiajun ding | cs.CV | [PDF](http://arxiv.org/pdf/2404.10318v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel
explicit 3D representation. This approach relies on the representation power of
Gaussian primitives to provide a high-quality rendering. However, primitives
optimized at low resolution inevitably exhibit sparsity and texture deficiency,
posing a challenge for achieving high-resolution novel view synthesis (HRNVS).
To address this problem, we propose Super-Resolution 3D Gaussian Splatting
(SRGS) to perform the optimization in a high-resolution (HR) space. The
sub-pixel constraint is introduced for the increased viewpoints in HR space,
exploiting the sub-pixel cross-view information of the multiple low-resolution
(LR) views. The gradient accumulated from more viewpoints will facilitate the
densification of primitives. Furthermore, a pre-trained 2D super-resolution
model is integrated with the sub-pixel constraint, enabling these dense
primitives to learn faithful texture features. In general, our method focuses
on densification and texture learning to effectively enhance the representation
ability of primitives. Experimentally, our method achieves high rendering
quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on
challenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes
will be released upon acceptance.

Comments:
- submit ACM MM 2024

---

## Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Florian Barthel, Arian Beckmann, Wieland Morgenstern, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2404.10625v1){: .btn .btn-green } |

**Abstract**: NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or
GIRAFFE have shown very high rendering quality under large representational
variety. However, rendering with Neural Radiance Fields poses challenges for 3D
applications: First, the significant computational demands of NeRF rendering
preclude its use on low-power devices, such as mobiles and VR/AR headsets.
Second, implicit representations based on neural networks are difficult to
incorporate into explicit 3D scenes, such as VR environments or video games. 3D
Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit
3D representation that can be rendered efficiently at high frame rates. In this
work, we present a novel approach that combines the high rendering quality of
NeRF-based 3D-aware GANs with the flexibility and computational advantages of
3DGS. By training a decoder that maps implicit NeRF representations to explicit
3D Gaussian Splatting attributes, we can integrate the representational
diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting
for the first time. Additionally, our approach allows for a high resolution GAN
inversion and real-time GAN editing with 3D Gaussian Splatting scenes.

Comments:
- CVPRW

---

## Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using  VDB Grid and Hierarchical Ray Traversal

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Yoshio Kato, Shuhei Tarashima | cs.CV | [PDF](http://arxiv.org/pdf/2404.10272v1){: .btn .btn-green } |

**Abstract**: Transmittance estimators such as Occupancy Grid (OG) can accelerate the
training and rendering of Neural Radiance Field (NeRF) by predicting important
samples that contributes much to the generated image. However, OG manages
occupied regions in the form of the dense binary grid, in which there are many
blocks with the same values that cause redundant examination of voxels'
emptiness in ray-tracing. In our work, we introduce two techniques to improve
the efficiency of ray-tracing in trained OG without fine-tuning. First, we
replace the dense grids with VDB grids to reduce the spatial redundancy.
Second, we use hierarchical digital differential analyzer (HDDA) to efficiently
trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF
360 datasets show that our proposed method successfully accelerates rendering
NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in
average, compared to a fast implementation of OG, NerfAcc, without losing the
quality of rendered images.

Comments:
- Short paper for CVPR Neural Rendering Intelligence Workshop 2024.
  Code: https://github.com/Yosshi999/faster-occgrid

---

## Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Seungwook Kim, Kejie Li, Xueqing Deng, Yichun Shi, Minsu Cho, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.10603v1){: .btn .btn-green } |

**Abstract**: Leveraging multi-view diffusion models as priors for 3D optimization have
alleviated the problem of 3D consistency, e.g., the Janus face problem or the
content drift problem, in zero-shot text-to-3D models. However, the 3D
geometric fidelity of the output remains an unresolved issue; albeit the
rendered 2D views are realistic, the underlying geometry may contain errors
such as unreasonable concavities. In this work, we propose CorrespondentDream,
an effective method to leverage annotation-free, cross-view correspondences
yielded from the diffusion U-Net to provide additional 3D prior to the NeRF
optimization process. We find that these correspondences are strongly
consistent with human perception, and by adopting it in our loss design, we are
able to produce NeRF models with geometries that are more coherent with common
sense, e.g., more smoothed object surface, yielding higher 3D fidelity. We
demonstrate the efficacy of our approach through various comparative
qualitative results and a solid user study.

Comments:
- 25 pages, 22 figures, accepted to CVPR 2024

---

## 1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Hang Du, Yaping Xue, Weidong Dai, Xuejun Yan, Jingjing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.10441v1){: .btn .btn-green } |

**Abstract**: In this report, we present the 1st place solution for ICCV 2023 OmniObject3D
Challenge: Sparse-View Reconstruction. The challenge aims to evaluate
approaches for novel view synthesis and surface reconstruction using only a few
posed images of each object. We utilize Pixel-NeRF as the basic model, and
apply depth supervision as well as coarse-to-fine positional encoding. The
experiments demonstrate the effectiveness of our approach in improving
sparse-view reconstruction quality. We ranked first in the final test with a
PSNR of 25.44614.



---

## AbsGS: Recovering Fine Details for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-16 | Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, Yong Dou | cs.CV | [PDF](http://arxiv.org/pdf/2404.10484v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with
differentiable rasterization to achieve high-quality novel view synthesis
results while providing advanced real-time rendering performance. However, due
to the flaw of its adaptive density control strategy in 3D-GS, it frequently
suffers from over-reconstruction issue in intricate scenes containing
high-frequency details, leading to blurry rendered images. The underlying
reason for the flaw has still been under-explored. In this work, we present a
comprehensive analysis of the cause of aforementioned artifacts, namely
gradient collision, which prevents large Gaussians in over-reconstructed
regions from splitting. To address this issue, we propose the novel
homodirectional view-space positional gradient as the criterion for
densification. Our strategy efficiently identifies large Gaussians in
over-reconstructed regions, and recovers fine details by splitting. We evaluate
our proposed method on various challenging datasets. The experimental results
indicate that our approach achieves the best rendering quality with reduced or
similar memory consumption. Our method is easy to implement and can be
incorporated into a wide variety of most recent Gaussian Splatting-based
methods. We will open source our codes upon formal publication. Our project
page is available at: https://ty424.github.io/AbsGS.github.io/



---

## Taming Latent Diffusion Model for Neural Radiance Field Inpainting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng | cs.CV | [PDF](http://arxiv.org/pdf/2404.09995v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) is a representation for 3D reconstruction from
multi-view images. Despite some recent work showing preliminary success in
editing a reconstructed NeRF with diffusion prior, they remain struggling to
synthesize reasonable geometry in completely uncovered regions. One major
reason is the high diversity of synthetic contents from the diffusion model,
which hinders the radiance field from converging to a crisp and deterministic
geometry. Moreover, applying latent diffusion models on real data often yields
a textural shift incoherent to the image condition due to auto-encoding errors.
These two problems are further reinforced with the use of pixel-distance
losses. To address these issues, we propose tempering the diffusion model's
stochasticity with per-scene customization and mitigating the textural shift
with masked adversarial training. During the analyses, we also found the
commonly used pixel and perceptual losses are harmful in the NeRF inpainting
task. Through rigorous experiments, our framework yields state-of-the-art NeRF
inpainting results on various real-world scenes. Project page:
https://hubert0527.github.io/MALD-NeRF

Comments:
- Project page: https://hubert0527.github.io/MALD-NeRF

---

## Video2Game: Real-time, Interactive, Realistic and Browser-Compatible  Environment from a Single Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Hongchi Xia, Zhi-Hao Lin, Wei-Chiu Ma, Shenlong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.09833v1){: .btn .btn-green } |

**Abstract**: Creating high-quality and interactive virtual environments, such as games and
simulators, often involves complex and costly manual modeling processes. In
this paper, we present Video2Game, a novel approach that automatically converts
videos of real-world scenes into realistic and interactive game environments.
At the heart of our system are three core components:(i) a neural radiance
fields (NeRF) module that effectively captures the geometry and visual
appearance of the scene; (ii) a mesh module that distills the knowledge from
NeRF for faster rendering; and (iii) a physics module that models the
interactions and physical dynamics among the objects. By following the
carefully designed pipeline, one can construct an interactable and actionable
digital replica of the real world. We benchmark our system on both indoor and
large-scale outdoor scenes. We show that we can not only produce
highly-realistic renderings in real-time, but also build interactive games on
top.

Comments:
- CVPR 2024. Project page (with code): https://video2game.github.io/

---

## LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted  Gaussian Primitives

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Jiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.09748v1){: .btn .btn-green } |

**Abstract**: Large garages are ubiquitous yet intricate scenes in our daily lives, posing
challenges characterized by monotonous colors, repetitive patterns, reflective
surfaces, and transparent vehicle glass. Conventional Structure from Motion
(SfM) methods for camera pose estimation and 3D reconstruction fail in these
environments due to poor correspondence construction. To address these
challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting
approach for large-scale garage modeling and rendering. We develop a handheld
scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate
accurate LiDAR and image data scanning. With this Polar device, we present a
GarageWorld dataset consisting of five expansive garage scenes with diverse
geometric structures and will release the dataset to the community for further
research. We demonstrate that the collected LiDAR point cloud by the Polar
device enhances a suite of 3D Gaussian splatting algorithms for garage scene
modeling and rendering. We also propose a novel depth regularizer for 3D
Gaussian splatting algorithm training, effectively eliminating floating
artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian
renderer for real-time viewing on web-based devices. Additionally, we explore a
hybrid representation that combines the advantages of traditional mesh in
depicting simple geometry and colors (e.g., walls and the ground) with modern
3D Gaussian representations capturing complex details and high-frequency
textures. This strategy achieves an optimal balance between memory performance
and rendering quality. Experimental results on our dataset, along with
ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering
quality and resource efficiency.

Comments:
- Project Page: https://jdtsui.github.io/letsgo/

---

## 3D Gaussian Splatting as Markov Chain Monte Carlo

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi | cs.CV | [PDF](http://arxiv.org/pdf/2404.09591v1){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting has recently become popular for neural rendering,
current methods rely on carefully engineered cloning and splitting strategies
for placing Gaussians, which does not always generalize and may lead to
poor-quality renderings. In addition, for real-world scenes, they rely on a
good initial point cloud to perform well. In this work, we rethink 3D Gaussians
as random samples drawn from an underlying probability distribution describing
the physical representation of the scene -- in other words, Markov Chain Monte
Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates are
strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update. As
with MCMC, samples are nothing but past visit locations, adding new Gaussians
under our framework can simply be realized without heuristics as placing
Gaussians at existing Gaussian locations. To encourage using fewer Gaussians
for efficiency, we introduce an L1-regularizer on the Gaussians. On various
standard evaluation scenes, we show that our method provides improved rendering
quality, easy control over the number of Gaussians, and robustness to
initialization.



---

## CompGS: Efficient 3D Scene Representation via Compressed Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, Sam Kwong | cs.CV | [PDF](http://arxiv.org/pdf/2404.09458v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting, renowned for its exceptional rendering quality and
efficiency, has emerged as a prominent technique in 3D scene representation.
However, the substantial data volume of Gaussian splatting impedes its
practical utility in real-world applications. Herein, we propose an efficient
3D scene representation, named Compressed Gaussian Splatting (CompGS), which
harnesses compact Gaussian primitives for faithful 3D scene modeling with a
remarkably reduced data size. To ensure the compactness of Gaussian primitives,
we devise a hybrid primitive structure that captures predictive relationships
between each other. Then, we exploit a small set of anchor primitives for
prediction, allowing the majority of primitives to be encapsulated into highly
compact residual forms. Moreover, we develop a rate-constrained optimization
scheme to eliminate redundancies within such hybrid primitives, steering our
CompGS towards an optimal trade-off between bitrate consumption and
representation efficacy. Experimental results show that the proposed CompGS
significantly outperforms existing methods, achieving superior compactness in
3D scene representation without compromising model accuracy and rendering
quality. Our code will be released on GitHub for further research.

Comments:
- Submitted to a conference

---

## DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred  Shading

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-15 | Tong Wu, Jia-Mu Sun, Yu-Kun Lai, Yuewen Ma, Leif Kobbelt, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2404.09412v1){: .btn .btn-green } |

**Abstract**: Reconstructing and editing 3D objects and scenes both play crucial roles in
computer graphics and computer vision. Neural radiance fields (NeRFs) can
achieve realistic reconstruction and editing results but suffer from
inefficiency in rendering. Gaussian splatting significantly accelerates
rendering by rasterizing Gaussian ellipsoids. However, Gaussian splatting
utilizes a single Spherical Harmonic (SH) function to model both texture and
lighting, limiting independent editing capabilities of these components.
Recently, attempts have been made to decouple texture and lighting with the
Gaussian splatting representation but may fail to produce plausible geometry
and decomposition results on reflective scenes. Additionally, the forward
shading technique they employ introduces noticeable blending artifacts during
relighting, as the geometry attributes of Gaussians are optimized under the
original illumination and may not be suitable for novel lighting conditions. To
address these issues, we introduce DeferredGS, a method for decoupling and
editing the Gaussian splatting representation using deferred shading. To
achieve successful decoupling, we model the illumination with a learnable
environment map and define additional attributes such as texture parameters and
normal direction on Gaussians, where the normal is distilled from a jointly
trained signed distance function. More importantly, we apply deferred shading,
resulting in more realistic relighting effects compared to previous methods.
Both qualitative and quantitative experiments demonstrate the superior
performance of DeferredGS in novel view synthesis and editing tasks.



---

## VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-14 | Fei Xue, Ignas Budvytis, Daniel Olmeda Reino, Roberto Cipolla | cs.CV | [PDF](http://arxiv.org/pdf/2404.09271v1){: .btn .btn-green } |

**Abstract**: Visual relocalization is a key technique to autonomous driving, robotics, and
virtual/augmented reality. After decades of explorations, absolute pose
regression (APR), scene coordinate regression (SCR), and hierarchical methods
(HMs) have become the most popular frameworks. However, in spite of high
efficiency, APRs and SCRs have limited accuracy especially in large-scale
outdoor scenes; HMs are accurate but need to store a large number of 2D
descriptors for matching, resulting in poor efficiency. In this paper, we
propose an efficient and accurate framework, called VRS-NeRF, for visual
relocalization with sparse neural radiance field. Precisely, we introduce an
explicit geometric map (EGM) for 3D map representation and an implicit learning
map (ILM) for sparse patches rendering. In this localization process, EGP
provides priors of spare 2D points and ILM utilizes these sparse points to
render patches with sparse NeRFs for matching. This allows us to discard a
large number of 2D descriptors so as to reduce the map size. Moreover,
rendering patches only for useful points rather than all pixels in the whole
image reduces the rendering time significantly. This framework inherits the
accuracy of HMs and discards their low efficiency. Experiments on 7Scenes,
CambridgeLandmarks, and Aachen datasets show that our method gives much better
accuracy than APRs and SCRs, and close performance to HMs but is much more
efficient.

Comments:
- source code https://github.com/feixue94/vrs-nerf

---

## DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-14 | Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang | cs.CV | [PDF](http://arxiv.org/pdf/2404.09227v1){: .btn .btn-green } |

**Abstract**: Recent progress in text-to-3D creation has been propelled by integrating the
potent prior of Diffusion Models from text-to-image generation into the 3D
domain. Nevertheless, generating 3D scenes characterized by multiple instances
and intricate arrangements remains challenging. In this study, we present
DreamScape, a method for creating highly consistent 3D scenes solely from
textual descriptions, leveraging the strong 3D representation capabilities of
Gaussian Splatting and the complex arrangement abilities of large language
models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene
representation, consisting of semantic primitives (objects) and their spatial
transformations and relationships derived directly from text prompts using
LLMs. This compositional representation allows for local-to-global optimization
of the entire scene. A progressive scale control is tailored during local
object generation, ensuring that objects of different sizes and densities adapt
to the scene, which addresses training instability issue arising from simple
blending in the subsequent global optimization stage. To mitigate potential
biases of LLM priors, we model collision relationships between objects at the
global level, enhancing physical correctness and overall realism. Additionally,
to generate pervasive objects like rain and snow distributed extensively across
the scene, we introduce a sparse initialization and densification strategy.
Experiments demonstrate that DreamScape offers high usability and
controllability, enabling the generation of high-fidelity 3D scenes from only
text prompts and achieving state-of-the-art performance compared to other
methods.



---

## EGGS: Edge Guided Gaussian Splatting for Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-14 | Yuanhao Gong | cs.CV | [PDF](http://arxiv.org/pdf/2404.09105v1){: .btn .btn-green } |

**Abstract**: The Gaussian splatting methods are getting popular. However, their loss
function only contains the $\ell_1$ norm and the structural similarity between
the rendered and input images, without considering the edges in these images.
It is well-known that the edges in an image provide important information.
Therefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS)
method that leverages the edges in the input images. More specifically, we give
the edge region a higher weight than the flat region. With such edge guidance,
the resulting Gaussian particles focus more on the edges instead of the flat
regions. Moreover, such edge guidance does not crease the computation cost
during the training and rendering stage. The experiments confirm that such
simple edge-weighted loss function indeed improves about $1\sim2$ dB on several
difference data sets. With simply plugging in the edge guidance, the proposed
method can improve all Gaussian splatting methods in different scenarios, such
as human head modeling, building 3D reconstruction, etc.



---

## LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via  Eulerian Motion Field

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-13 | Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He | cs.CV | [PDF](http://arxiv.org/pdf/2404.08966v2){: .btn .btn-green } |

**Abstract**: Cinemagraph is a unique form of visual media that combines elements of still
photography and subtle motion to create a captivating experience. However, the
majority of videos generated by recent works lack depth information and are
confined to the constraints of 2D image space. In this paper, inspired by
significant progress in the field of novel view synthesis (NVS) achieved by 3D
Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from
2D image space to 3D space using 3D Gaussian modeling. To achieve this, we
first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from
multi-view images of static scenes,incorporating shape regularization terms to
prevent blurring or artifacts caused by object deformation. We then adopt an
autoencoder tailored for 3D Gaussian to project it into feature space. To
maintain the local continuity of the scene, we devise SuperGaussian for
clustering based on the acquired features. By calculating the similarity
between clusters and employing a two-stage estimation method, we derive an
Eulerian motion field to describe velocities across the entire scene. The 3D
Gaussian points then move within the estimated Eulerian motion field. Through
bidirectional animation techniques, we ultimately generate a 3D Cinemagraph
that exhibits natural and seamlessly loopable dynamics. Experiment results
validate the effectiveness of our approach, demonstrating high-quality and
visually appealing scene generation. The project is available at
https://pokerlishao.github.io/LoopGaussian/.

Comments:
- 10 pages

---

## OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-12 | Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu | cs.CV | [PDF](http://arxiv.org/pdf/2404.08449v2){: .btn .btn-green } |

**Abstract**: Rendering dynamic 3D human from monocular videos is crucial for various
applications such as virtual reality and digital entertainment. Most methods
assume the people is in an unobstructed scene, while various objects may cause
the occlusion of body parts in real-life scenarios. Previous method utilizing
NeRF for surface rendering to recover the occluded areas, but it requiring more
than one day to train and several seconds to render, failing to meet the
requirements of real-time interactive applications. To address these issues, we
propose OccGaussian based on 3D Gaussian Splatting, which can be trained within
6 minutes and produces high-quality human renderings up to 160 FPS with
occluded input. OccGaussian initializes 3D Gaussian distributions in the
canonical space, and we perform occlusion feature query at occluded regions,
the aggregated pixel-align feature is extracted to compensate for the missing
information. Then we use Gaussian Feature MLP to further process the feature
along with the occlusion-aware loss functions to better perceive the occluded
area. Extensive experiments both in simulated and real-world occlusions,
demonstrate that our method achieves comparable or even superior performance
compared to the state-of-the-art method. And we improving training and
inference speeds by 250x and 800x, respectively. Our code will be available for
research purposes.



---

## GPN: Generative Point-based NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-12 | Haipeng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.08312v1){: .btn .btn-green } |

**Abstract**: Scanning real-life scenes with modern registration devices typically gives
incomplete point cloud representations, primarily due to the limitations of
partial scanning, 3D occlusions, and dynamic light conditions. Recent works on
processing incomplete point clouds have always focused on point cloud
completion. However, these approaches do not ensure consistency between the
completed point cloud and the captured images regarding color and geometry. We
propose using Generative Point-based NeRF (GPN) to reconstruct and repair a
partial cloud by fully utilizing the scanning images and the corresponding
reconstructed cloud. The repaired point cloud can achieve multi-view
consistency with the captured images at high spatial resolution. For the
finetunes of a single scene, we optimize the global latent condition by
incorporating an Auto-Decoder architecture while retaining multi-view
consistency. As a result, the generated point clouds are smooth, plausible, and
geometrically consistent with the partial scanning images. Extensive
experiments on ShapeNet demonstrate that our works achieve competitive
performances to the other state-of-the-art point cloud-based neural scene
rendering and editing performances.



---

## MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based  Monocular Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-12 | Yuqun Wu, Jae Yong Lee, Chuhang Zou, Shenlong Wang, Derek Hoiem | cs.CV | [PDF](http://arxiv.org/pdf/2404.08252v1){: .btn .btn-green } |

**Abstract**: The latest regularized Neural Radiance Field (NeRF) approaches produce poor
geometry and view extrapolation for multiview stereo (MVS) benchmarks such as
ETH3D. In this paper, we aim to create 3D models that provide accurate geometry
and view synthesis, partially closing the large geometric performance gap
between NeRF and traditional MVS methods. We propose a patch-based approach
that effectively leverages monocular surface normal and relative depth
predictions. The patch-based ray sampling also enables the appearance
regularization of normalized cross-correlation (NCC) and structural similarity
(SSIM) between randomly sampled virtual and training views. We further show
that "density restrictions" based on sparse structure-from-motion points can
help greatly improve geometric accuracy with a slight drop in novel view
synthesis metrics. Our experiments show 4x the performance of RegNeRF and 8x
that of FreeNeRF on average F1@2cm for ETH3D MVS benchmark, suggesting a
fruitful research direction to improve the geometric accuracy of NeRF-based
models, and sheds light on a potential future approach to enable NeRF-based
optimization to eventually outperform traditional MVS.

Comments:
- 26 pages, 15 figures

---

## NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous  Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | William Ljungbergh, Adam Tonderski, Joakim Johnander, Holger Caesar, Kalle Åström, Michael Felsberg, Christoffer Petersson | cs.CV | [PDF](http://arxiv.org/pdf/2404.07762v2){: .btn .btn-green } |

**Abstract**: We present a versatile NeRF-based simulator for testing autonomous driving
(AD) software systems, designed with a focus on sensor-realistic closed-loop
evaluation and the creation of safety-critical scenarios. The simulator learns
from sequences of real-world driving sensor data and enables reconfigurations
and renderings of new, unseen scenarios. In this work, we use our simulator to
test the responses of AD models to safety-critical scenarios inspired by the
European New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,
while state-of-the-art end-to-end planners excel in nominal driving scenarios
in an open-loop setting, they exhibit critical flaws when navigating our
safety-critical scenarios in a closed-loop setting. This highlights the need
for advancements in the safety and real-world usability of end-to-end planners.
By publicly releasing our simulator and scenarios as an easy-to-run evaluation
suite, we invite the research community to explore, refine, and validate their
AD models in controlled, yet highly configurable and challenging
sensor-realistic environments. Code and instructions can be found at
https://github.com/wljungbergh/NeuroNCAP



---

## Connecting NeRFs, Images, and Text

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | Francesco Ballerini, Pierluigi Zama Ramirez, Roberto Mirabella, Samuele Salti, Luigi Di Stefano | cs.CV | [PDF](http://arxiv.org/pdf/2404.07993v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as a standard framework for
representing 3D scenes and objects, introducing a novel data type for
information exchange and storage. Concurrently, significant progress has been
made in multimodal representation learning for text and image data. This paper
explores a novel research direction that aims to connect the NeRF modality with
other modalities, similar to established methodologies for images and text. To
this end, we propose a simple framework that exploits pre-trained models for
NeRF representations alongside multimodal models for text and image processing.
Our framework learns a bidirectional mapping between NeRF embeddings and those
obtained from corresponding images and text. This mapping unlocks several novel
and useful applications, including NeRF zero-shot classification and NeRF
retrieval from images or text.

Comments:
- Accepted at CVPRW-INRV 2024

---

## G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | Zixiong Huang, Qi Chen, Libo Sun, Yifan Yang, Naizhou Wang, Mingkui Tan, Qi Wu | cs.CV | [PDF](http://arxiv.org/pdf/2404.07474v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis aims to generate new view images of a given view image
collection. Recent attempts address this problem relying on 3D geometry priors
(e.g., shapes, sizes, and positions) learned from multi-view images. However,
such methods encounter the following limitations: 1) they require a set of
multi-view images as training data for a specific scene (e.g., face, car or
chair), which is often unavailable in many real-world scenarios; 2) they fail
to extract the geometry priors from single-view images due to the lack of
multi-view supervision. In this paper, we propose a Geometry-enhanced NeRF
(G-NeRF), which seeks to enhance the geometry priors by a geometry-guided
multi-view synthesis approach, followed by a depth-aware training. In the
synthesis process, inspired that existing 3D GAN models can unconditionally
synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D
GAN models, such as EG3D, as a free source to provide geometry priors through
synthesizing multi-view data. Simultaneously, to further improve the geometry
quality of the synthetic data, we introduce a truncation method to effectively
sample latent codes within 3D GAN models. To tackle the absence of multi-view
supervision for single-view images, we design the depth-aware training
approach, incorporating a depth-aware discriminator to guide geometry priors
through depth maps. Experiments demonstrate the effectiveness of our method in
terms of both qualitative and quantitative results.

Comments:
- CVPR 2024 Accepted Paper

---

## GoMAvatar: Efficient Animatable Human Modeling from Monocular Video  Using Gaussians-on-Mesh

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.07991v1){: .btn .btn-green } |

**Abstract**: We introduce GoMAvatar, a novel approach for real-time, memory-efficient,
high-quality animatable human modeling. GoMAvatar takes as input a single
monocular video to create a digital avatar capable of re-articulation in new
poses and real-time rendering from novel viewpoints, while seamlessly
integrating with rasterization-based graphics pipelines. Central to our method
is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering
quality and speed of Gaussian splatting with geometry modeling and
compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and
various YouTube videos. GoMAvatar matches or surpasses current monocular human
modeling algorithms in rendering quality and significantly outperforms them in
computational efficiency (43 FPS) while being memory-efficient (3.63 MB per
subject).

Comments:
- CVPR 2024; project page: https://wenj.github.io/GoMAvatar/

---

## Boosting Self-Supervision for Single-View Scene Completion via Knowledge  Distillation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-11 | Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2404.07933v1){: .btn .btn-green } |

**Abstract**: Inferring scene geometry from images via Structure from Motion is a
long-standing and fundamental problem in computer vision. While classical
approaches and, more recently, depth map predictions only focus on the visible
parts of a scene, the task of scene completion aims to reason about geometry
even in occluded regions. With the popularity of neural radiance fields
(NeRFs), implicit representations also became popular for scene completion by
predicting so-called density fields. Unlike explicit approaches. e.g.
voxel-based methods, density fields also allow for accurate depth prediction
and novel-view synthesis via image-based rendering. In this work, we propose to
fuse the scene reconstruction from multiple images and distill this knowledge
into a more accurate single-view scene reconstruction. To this end, we propose
Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed
images, trained fully self-supervised only from image data. Using knowledge
distillation, we use MVBTS to train a single-view scene completion network via
direct supervision called KDBTS. It achieves state-of-the-art performance on
occupancy prediction, especially in occluded regions.



---

## DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi | cs.CV | [PDF](http://arxiv.org/pdf/2404.06903v1){: .btn .btn-green } |

**Abstract**: The increasing demand for virtual reality applications has highlighted the
significance of crafting immersive 3D assets. We present a text-to-3D
360$^{\circ}$ scene generation pipeline that facilitates the creation of
comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of
minutes. Our approach utilizes the generative power of a 2D diffusion model and
prompt self-refinement to create a high-quality and globally coherent panoramic
image. This image acts as a preliminary "flat" (2D) scene representation.
Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to
enable real-time exploration. To produce consistent 3D geometry, our pipeline
constructs a spatially coherent structure by aligning the 2D monocular depth
into a globally optimized point cloud. This point cloud serves as the initial
state for the centroids of 3D Gaussians. In order to address invisible issues
inherent in single-view inputs, we impose semantic and geometric constraints on
both synthesized and input camera views as regularizations. These guide the
optimization of Gaussians, aiding in the reconstruction of unseen regions. In
summary, our method offers a globally consistent 3D scene within a
360$^{\circ}$ perspective, providing an enhanced immersive experience over
existing techniques. Project website at: http://dreamscene360.github.io/



---

## RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth  Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2404.07199v1){: .btn .btn-green } |

**Abstract**: We introduce RealmDreamer, a technique for generation of general
forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D
Gaussian Splatting representation to match complex text prompts. We initialize
these splats by utilizing the state-of-the-art text-to-image generators,
lifting their samples into 3D, and computing the occlusion volume. We then
optimize this representation across multiple views as a 3D inpainting task with
image-conditional diffusion models. To learn correct geometric structure, we
incorporate a depth diffusion model by conditioning on the samples from the
inpainting model, giving rich geometric structure. Finally, we finetune the
model using sharpened samples from image generators. Notably, our technique
does not require video or multi-view data and can synthesize a variety of
high-quality 3D scenes in different styles, consisting of multiple objects. Its
generality additionally allows 3D synthesis from a single image.

Comments:
- Project Page: https://realmdreamer.github.io/

---

## SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn | cs.CV | [PDF](http://arxiv.org/pdf/2404.06832v1){: .btn .btn-green } |

**Abstract**: Detecting anomalies in images has become a well-explored problem in both
academia and industry. State-of-the-art algorithms are able to detect defects
in increasingly difficult settings and data modalities. However, most current
methods are not suited to address 3D objects captured from differing poses.
While solutions using Neural Radiance Fields (NeRFs) have been proposed, they
suffer from excessive computation requirements, which hinder real-world
usability. For this reason, we propose the novel 3D Gaussian splatting-based
framework SplatPose which, given multi-view images of a 3D object, accurately
estimates the pose of unseen views in a differentiable manner, and detects
anomalies in them. We achieve state-of-the-art results in both training and
inference speed, and detection performance, even when using less training data
than competing methods. We thoroughly evaluate our framework using the recently
proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly
detection (MAD) data set.

Comments:
- Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024

---

## MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D  Reconstruction of Indoor Scenes from Monocular RGB Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Runfa Li, Upal Mahbub, Vasudev Bhaskaran, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2404.06753v1){: .btn .btn-green } |

**Abstract**: Current monocular 3D scene reconstruction (3DR) works are either
fully-supervised, or not generalizable, or implicit in 3D representation. We
propose a novel framework - MonoSelfRecon that for the first time achieves
explicit 3D mesh reconstruction for generalizable indoor scenes with monocular
RGB views by purely self-supervision on voxel-SDF (signed distance function).
MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and
a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF
in self-supervision. We propose novel self-supervised losses, which not only
support pure self-supervision, but can be used together with supervised signals
to further boost supervised training. Our experiments show that "MonoSelfRecon"
trained in pure self-supervision outperforms current best self-supervised
indoor depth estimation models and is comparable to 3DR models trained in fully
supervision with depth annotations. MonoSelfRecon is not restricted by specific
model design, which can be used to any models with voxel-SDF for purely
self-supervised manner.



---

## Bayesian NeRF: Quantifying Uncertainty with Volume Density in Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Sibeak Lee, Kyeongsu Kang, Hyeonwoo Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.06727v1){: .btn .btn-green } |

**Abstract**: We present the Bayesian Neural Radiance Field (NeRF), which explicitly
quantifies uncertainty in geometric volume structures without the need for
additional networks, making it adept for challenging observations and
uncontrolled images. NeRF diverges from traditional geometric methods by
offering an enriched scene representation, rendering color and density in 3D
space from various viewpoints. However, NeRF encounters limitations in relaxing
uncertainties by using geometric structure information, leading to inaccuracies
in interpretation under insufficient real-world observations. Recent research
efforts aimed at addressing this issue have primarily relied on empirical
methods or auxiliary networks. To fundamentally address this issue, we propose
a series of formulational extensions to NeRF. By introducing generalized
approximations and defining density-related uncertainty, our method seamlessly
extends to manage uncertainty not only for RGB but also for depth, without the
need for additional networks or empirical assumptions. In experiments we show
that our method significantly enhances performance on RGB and depth images in
the comprehensive dataset, demonstrating the reliability of the Bayesian NeRF
approach to quantifying uncertainty based on the geometric structure.



---

## Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Xiaolei Lang, Laijian Li, Hang Zhang, Feng Xiong, Mu Xu, Yong Liu, Xingxing Zuo, Jiajun Lv | cs.RO | [PDF](http://arxiv.org/pdf/2404.06926v1){: .btn .btn-green } |

**Abstract**: We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian
Splatting as the mapping backend. Leveraging robust pose estimates from our
LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic
mapping system is proposed in this paper. We initialize 3D Gaussians from
colorized LiDAR points and optimize them using differentiable rendering powered
by 3D Gaussian Splatting. Meticulously designed strategies are employed to
incrementally expand the Gaussian map and adaptively control its density,
ensuring high-quality mapping with real-time capability. Experiments conducted
in diverse scenarios demonstrate the superior performance of our method
compared to existing radiance-field-based SLAM systems.

Comments:
- Submitted to IROS 2024

---

## Zero-shot Point Cloud Completion Via 2D Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2404.06814v1){: .btn .btn-green } |

**Abstract**: 3D point cloud completion is designed to recover complete shapes from
partially observed point clouds. Conventional completion methods typically
depend on extensive point cloud data for training %, with their effectiveness
often constrained to object categories similar to those seen during training.
In contrast, we propose a zero-shot framework aimed at completing partially
observed point clouds across any unseen categories. Leveraging point rendering
via Gaussian Splatting, we develop techniques of Point Cloud Colorization and
Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion
models to infer missing regions. Experimental results on both synthetic and
real-world scanned point clouds demonstrate that our approach outperforms
existing methods in completing a variety of objects without any requirement for
specific training data.



---

## SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike  Camera

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-10 | Gaole Dai, Zhenyu Wang, Qinwen Xu, Ming Lu, Wen Chen, Boxin Shi, Shanghang Zhang, Tiejun Huang | cs.CV | [PDF](http://arxiv.org/pdf/2404.06710v3){: .btn .btn-green } |

**Abstract**: One of the most critical factors in achieving sharp Novel View Synthesis
(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) is the quality of the training images. However,
Conventional RGB cameras are susceptible to motion blur. In contrast,
neuromorphic cameras like event and spike cameras inherently capture more
comprehensive temporal information, which can provide a sharp representation of
the scene as additional training data. Recent methods have explored the
integration of event cameras to improve the quality of NVS. The event-RGB
approaches have some limitations, such as high training costs and the inability
to work effectively in the background. Instead, our study introduces a new
method that uses the spike camera to overcome these limitations. By considering
texture reconstruction from spike streams as ground truth, we design the
Texture from Spike (TfS) loss. Since the spike camera relies on temporal
integration instead of temporal differentiation used by event cameras, our
proposed TfS loss maintains manageable training costs. It handles foreground
objects with backgrounds simultaneously. We also provide a real-world dataset
captured with our spike-RGB camera system to facilitate future research
endeavors. We conduct extensive experiments using synthetic and real-world
datasets to demonstrate that our design can enhance novel view synthesis across
NeRF and 3DGS. The code and dataset will be made available for public access.



---

## Hash3D: Training-free Acceleration for 3D Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Xingyi Yang, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2404.06091v1){: .btn .btn-green } |

**Abstract**: The evolution of 3D generative modeling has been notably propelled by the
adoption of 2D diffusion models. Despite this progress, the cumbersome
optimization process per se presents a critical hurdle to efficiency. In this
paper, we introduce Hash3D, a universal acceleration for 3D generation without
model training. Central to Hash3D is the insight that feature-map redundancy is
prevalent in images rendered from camera positions and diffusion time-steps in
close proximity. By effectively hashing and reusing these feature maps across
neighboring timesteps and camera angles, Hash3D substantially prevents
redundant calculations, thus accelerating the diffusion model's inference in 3D
generation tasks. We achieve this through an adaptive grid-based hashing.
Surprisingly, this feature-sharing mechanism not only speed up the generation
but also enhances the smoothness and view consistency of the synthesized 3D
objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models,
demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency
by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian
splatting largely speeds up 3D model creation, reducing text-to-3D processing
to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The
project page is at https://adamdad.github.io/hash3D/.

Comments:
- https://adamdad.github.io/hash3D/

---

## HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet | cs.CV | [PDF](http://arxiv.org/pdf/2404.06152v1){: .btn .btn-green } |

**Abstract**: In recent advancements in novel view synthesis, generalizable Neural Radiance
Fields (NeRF) based methods applied to human subjects have shown remarkable
results in generating novel views from few images. However, this generalization
ability cannot capture the underlying structural features of the skeleton
shared across all instances. Building upon this, we introduce HFNeRF: a novel
generalizable human feature NeRF aimed at generating human biomechanic features
using a pre-trained image encoder. While previous human NeRF methods have shown
promising results in the generation of photorealistic virtual avatars, such
methods lack underlying human structure or biomechanic features such as
skeleton or joint information that are crucial for downstream applications
including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D
pre-trained foundation models toward learning human features in 3D using neural
rendering, and then volume rendering towards generating 2D feature maps. We
evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as
features. The proposed method is fully differentiable, allowing to successfully
learn color, geometry, and human skeleton in a simultaneous manner. This paper
presents preliminary results of HFNeRF, illustrating its potential in
generating realistic virtual avatars with biomechanic features using NeRF.



---

## Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi Feng, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2404.06429v1){: .btn .btn-green } |

**Abstract**: Benefiting from the rapid development of 2D diffusion models, 3D content
creation has made significant progress recently. One promising solution
involves the fine-tuning of pre-trained 2D diffusion models to harness their
capacity for producing multi-view images, which are then lifted into accurate
3D models via methods like fast-NeRFs or large reconstruction models. However,
as inconsistency still exists and limited generated resolution, the generation
results of such methods still lack intricate textures and complex geometries.
To solve this problem, we propose Magic-Boost, a multi-view conditioned
diffusion model that significantly refines coarse generative results through a
brief period of SDS optimization ($\sim15$min). Compared to the previous text
or single image based diffusion models, Magic-Boost exhibits a robust
capability to generate images with high consistency from pseudo synthesized
multi-view images. It provides precise SDS guidance that well aligns with the
identity of the input images, enriching the local detail in both geometry and
texture of the initial generative results. Extensive experiments show
Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D
assets with rich geometric and textural details. (Project Page:
https://magic-research.github.io/magic-boost/)



---

## Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for  Realistic Endoscopic Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano | cs.CV | [PDF](http://arxiv.org/pdf/2404.06128v1){: .btn .btn-green } |

**Abstract**: Within colorectal cancer diagnostics, conventional colonoscopy techniques
face critical limitations, including a limited field of view and a lack of
depth information, which can impede the detection of precancerous lesions.
Current methods struggle to provide comprehensive and accurate 3D
reconstructions of the colonic surface which can help minimize the missing
regions and reinspection for pre-cancerous polyps. Addressing this, we
introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting
(3D GS) combined with a Recurrent Neural Network-based Simultaneous
Localization and Mapping (RNNSLAM) system. By introducing geometric and depth
regularization into the 3D GS framework, our approach ensures more accurate
alignment of Gaussians with the colon surface, resulting in smoother 3D
reconstructions with novel viewing of detailed textures and structures.
Evaluations across three diverse datasets show that Gaussian Pancakes enhances
novel view synthesis quality, surpassing current leading methods with a 18%
boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster
rendering and more than 10X shorter training times, making it a practical tool
for real-time applications. Hence, this holds promise for achieving clinical
translation for better detection and diagnosis of colorectal cancer.

Comments:
- 12 pages, 5 figures

---

## 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View  Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai | cs.CV | [PDF](http://arxiv.org/pdf/2404.06270v2){: .btn .btn-green } |

**Abstract**: In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting
method for dynamic view synthesis. Existing neural radiance fields (NeRF) based
solutions learn the deformation in an implicit manner, which cannot incorporate
3D scene geometry. Therefore, the learned deformation is not necessarily
geometrically coherent, which results in unsatisfactory dynamic view synthesis
and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new
representation of the 3D scene, building upon which the 3D geometry could be
exploited in learning the complex 3D deformation. Specifically, the scenes are
represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized
to move and rotate over time to model the deformation. To enforce the 3D scene
geometry constraint during deformation, we explicitly extract 3D geometry
features and integrate them in learning the 3D deformation. In this way, our
solution achieves 3D geometry-aware deformation modeling, which enables
improved dynamic view synthesis and 3D dynamic reconstruction. Extensive
experimental results on both synthetic and real datasets prove the superiority
of our solution, which achieves new state-of-the-art performance.
  The project is available at https://npucvr.github.io/GaGS/

Comments:
- Accepted by CVPR 2024. Project page: https://npucvr.github.io/GaGS/

---

## GHNeRF: Learning Generalizable Human Features with Efficient Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Arnab Dey, Di Yang, Rohith Agaram, Antitza Dantcheva, Andrew I. Comport, Srinath Sridhar, Jean Martinet | cs.CV | [PDF](http://arxiv.org/pdf/2404.06246v1){: .btn .btn-green } |

**Abstract**: Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising
results in 3D scene representations, including 3D human representations.
However, these representations often lack crucial information on the underlying
human pose and structure, which is crucial for AR/VR applications and games. In
this paper, we introduce a novel approach, termed GHNeRF, designed to address
these limitations by learning 2D/3D joint locations of human subjects with NeRF
representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract
essential human features from 2D images, which are then incorporated into the
NeRF framework in order to encode human biomechanic features. This allows our
network to simultaneously learn biomechanic features, such as joint locations,
along with human geometry and texture. To assess the effectiveness of our
method, we conduct a comprehensive comparison with state-of-the-art human NeRF
techniques and joint estimation algorithms. Our results show that GHNeRF can
achieve state-of-the-art results in near real-time.



---

## Revising Densification in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-09 | Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2404.06109v1){: .btn .btn-green } |

**Abstract**: In this paper, we address the limitations of Adaptive Density Control (ADC)
in 3D Gaussian Splatting (3DGS), a scene representation method achieving
high-quality, photorealistic results for novel view synthesis. ADC has been
introduced for automatic 3D point primitive management, controlling
densification and pruning, however, with certain limitations in the
densification logic. Our main contribution is a more principled, pixel-error
driven formulation for density control in 3DGS, leveraging an auxiliary,
per-pixel error function as the criterion for densification. We further
introduce a mechanism to control the total number of primitives generated per
scene and correct a bias in the current opacity handling strategy of ADC during
cloning operations. Our approach leads to consistent quality improvements
across a variety of benchmark scenes, without sacrificing the method's
efficiency.



---

## Semantic Flow: Learning Semantic Field of Dynamic Scenes from Monocular  Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Fengrui Tian, Yueqi Duan, Angtian Wang, Jianfei Guo, Shaoyi Du | cs.CV | [PDF](http://arxiv.org/pdf/2404.05163v1){: .btn .btn-green } |

**Abstract**: In this work, we pioneer Semantic Flow, a neural semantic representation of
dynamic scenes from monocular videos. In contrast to previous NeRF methods that
reconstruct dynamic scenes from the colors and volume densities of individual
points, Semantic Flow learns semantics from continuous flows that contain rich
3D motion information. As there is 2D-to-3D ambiguity problem in the viewing
direction when extracting 3D flow features from 2D video frames, we consider
the volume densities as opacity priors that describe the contributions of flow
features to the semantics on the frames. More specifically, we first learn a
flow network to predict flows in the dynamic scene, and propose a flow feature
aggregation module to extract flow features from video frames. Then, we propose
a flow attention module to extract motion information from flow features, which
is followed by a semantic network to output semantic logits of flows. We
integrate the logits with volume densities in the viewing direction to
supervise the flow features with semantic labels on video frames. Experimental
results show that our model is able to learn from multiple dynamic scenes and
supports a series of new tasks such as instance-level scene editing, semantic
completions, dynamic scene tracking and semantic adaption on novel scenes.
Codes are available at https://github.com/tianfr/Semantic-Flow/.

Comments:
- Accepted by ICLR 2024, Codes are available at
  https://github.com/tianfr/Semantic-Flow/

---

## StylizedGS: Controllable Stylization for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Dingxi Zhang, Zhuoxun Chen, Yu-Jie Yuan, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2404.05220v1){: .btn .btn-green } |

**Abstract**: With the rapid development of XR, 3D generation and editing are becoming more
and more important, among which, stylization is an important tool of 3D
appearance editing. It can achieve consistent 3D artistic stylization given a
single reference style image and thus is a user-friendly editing way. However,
recent NeRF-based 3D stylization methods face efficiency issues that affect the
actual user experience and the implicit nature limits its ability to transfer
the geometric pattern styles. Additionally, the ability for artists to exert
flexible control over stylized scenes is considered highly desirable, fostering
an environment conducive to creative exploration. In this paper, we introduce
StylizedGS, a 3D neural style transfer framework with adaptable control over
perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The
3DGS brings the benefits of high efficiency. We propose a GS filter to
eliminate floaters in the reconstruction which affects the stylization effects
before stylization. Then the nearest neighbor-based style loss is introduced to
achieve stylization by fine-tuning the geometry and color parameters of 3DGS,
while a depth preservation loss with other regularizations is proposed to
prevent the tampering of geometry content. Moreover, facilitated by specially
designed losses, StylizedGS enables users to control color, stylized scale and
regions during the stylization to possess customized capabilities. Our method
can attain high-quality stylization results characterized by faithful
brushstrokes and geometric consistency with flexible controls. Extensive
experiments across various scenes and styles demonstrate the effectiveness and
efficiency of our method concerning both stylization quality and inference FPS.



---

## Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Y. Wang, A. Gao, Y. Gong, Y. Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2404.05236v1){: .btn .btn-green } |

**Abstract**: Recently, a surge of 3D style transfer methods has been proposed that
leverage the scene reconstruction power of a pre-trained neural radiance field
(NeRF). To successfully stylize a scene this way, one must first reconstruct a
photo-realistic radiance field from collected images of the scene. However,
when only sparse input views are available, pre-trained few-shot NeRFs often
suffer from high-frequency artifacts, which are generated as a by-product of
high-frequency details for improving reconstruction quality. Is it possible to
generate more faithful stylized scenes from sparse inputs by directly
optimizing encoding-based scene representation with target style? In this
paper, we consider the stylization of sparse-view scenes in terms of
disentangling content semantics and style textures. We propose a coarse-to-fine
sparse-view scene stylization framework, where a novel hierarchical
encoding-based neural representation is designed to generate high-quality
stylized scenes directly from implicit scene representations. We also propose a
new optimization strategy with content strength annealing to achieve realistic
stylization and better content preservation. Extensive experiments demonstrate
that our method can achieve high-quality stylization of sparse-view scenes and
outperforms fine-tuning-based baselines in terms of stylization quality and
efficiency.



---

## CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality  Novel-view Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Gyeongjin Kang, Younggeun Lee, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2404.04913v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have achieved huge success in effectively
capturing and representing 3D objects and scenes. However, several factors have
impeded its further proliferation as next-generation 3D media. To establish a
ubiquitous presence in everyday media formats, such as images and videos, it is
imperative to devise a solution that effectively fulfills three key objectives:
fast encoding and decoding time, compact model sizes, and high-quality
renderings. Despite significant advancements, a comprehensive algorithm that
adequately addresses all objectives has yet to be fully realized. In this work,
we present CodecNeRF, a neural codec for NeRF representations, consisting of a
novel encoder and decoder architecture that can generate a NeRF representation
in a single forward pass. Furthermore, inspired by the recent
parameter-efficient finetuning approaches, we develop a novel finetuning method
to efficiently adapt the generated NeRF representations to a new test instance,
leading to high-quality image renderings and compact code sizes. The proposed
CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF,
achieved unprecedented compression performance of more than 150x and 20x
reduction in encoding time while maintaining (or improving) the image quality
on widely used 3D object datasets, such as ShapeNet and Objaverse.

Comments:
- 34 pages, 22 figures, Project page:
  https://gynjn.github.io/Codec-NeRF/

---

## GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric  Reconstruction Using Gaussian Splatting and NeRF

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Butian Xiong, Nanjun Zheng, Zhen Li | cs.CV | [PDF](http://arxiv.org/pdf/2404.04880v1){: .btn .btn-green } |

**Abstract**: We introduce a novel large-scale scene reconstruction benchmark that utilizes
newly developed 3D representation approaches: Gaussian Splatting and Neural
Radiance Fields, on our expansive GauU-Scene V2 dataset. GauU-Scene V2
encompasses over 6.5 square kilometers and features a comprehensive RGB dataset
coupled with LiDAR ground truth. This dataset offers a unique blend of urban
and academic environments for advanced spatial analysis, covering more than 6.5
km2. We also provide detailed supplementary information on data collection
protocols. Furthermore, we present an easy-to-follow pipeline to align the
COLMAP sparse point cloud with the detailed LiDAR dataset. Our evaluation of
U-Scene, which includes a detailed analysis across various novel viewpoints
using image-based metrics such as SSIM, LPIPS, and PSNR, shows contradictory
results when applying geometric-based metrics, such as Chamfer distance. This
leads to doubts about the reliability of current image-based measurement
matrices and geometric extraction methods on Gaussian Splatting. We also make
the dataset available on the following anonymous project page

Comments:
- 8 pages(No reference) 6 figures 4 tabs

---

## Dual-Camera Smooth Zoom on Mobile Phones

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2404.04908v1){: .btn .btn-green } |

**Abstract**: When zooming between dual cameras on a mobile, noticeable jumps in geometric
content and image color occur in the preview, inevitably affecting the user's
zoom experience. In this work, we introduce a new task, ie, dual-camera smooth
zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI)
technique is a potential solution but struggles with ground-truth collection.
To address the issue, we suggest a data factory solution where continuous
virtual cameras are assembled to generate DCSZ data by rendering reconstructed
3D models of the scene. In particular, we propose a novel dual-camera smooth
zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is
introduced to construct a specific 3D model for each virtual camera. With the
proposed data factory, we construct a synthetic dataset for DCSZ, and we
utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom
images without ground-truth for evaluation. Extensive experiments are conducted
with multiple FI methods. The results show that the fine-tuned FI models
achieve a significant performance improvement over the original ones on DCSZ
task. The datasets, codes, and pre-trained models will be publicly available.

Comments:
- 24

---

## NeRF2Points: Large-Scale Point Cloud Generation From Street Views'  Radiance Field Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Peng Tu, Xun Zhou, Mingming Wang, Xiaojun Yang, Bo Peng, Ping Chen, Xiu Su, Yawen Huang, Yefeng Zheng, Chang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.04875v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have emerged as a paradigm-shifting methodology
for the photorealistic rendering of objects and environments, enabling the
synthesis of novel viewpoints with remarkable fidelity. This is accomplished
through the strategic utilization of object-centric camera poses characterized
by significant inter-frame overlap. This paper explores a compelling,
alternative utility of NeRF: the derivation of point clouds from aggregated
urban landscape imagery. The transmutation of street-view data into point
clouds is fraught with complexities, attributable to a nexus of interdependent
variables. First, high-quality point cloud generation hinges on precise camera
poses, yet many datasets suffer from inaccuracies in pose metadata. Also, the
standard approach of NeRF is ill-suited for the distinct characteristics of
street-view data from autonomous vehicles in vast, open settings. Autonomous
vehicle cameras often record with limited overlap, leading to blurring,
artifacts, and compromised pavement representation in NeRF-based point clouds.
In this paper, we present NeRF2Points, a tailored NeRF variant for urban point
cloud synthesis, notable for its high-quality output from RGB inputs alone. Our
paper is supported by a bespoke, high-resolution 20-kilometer urban street
dataset, designed for point cloud generation and evaluation. NeRF2Points
adeptly navigates the inherent challenges of NeRF-based point cloud synthesis
through the implementation of the following strategic innovations: (1)
Integration of Weighted Iterative Geometric Optimization (WIGO) and Structure
from Motion (SfM) for enhanced camera pose accuracy, elevating street-view data
precision. (2) Layered Perception and Integrated Modeling (LPiM) is designed
for distinct radiance field modeling in urban environments, resulting in
coherent point cloud representations.

Comments:
- 18 pages

---

## Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-06 | Ziyuan Qu, Omkar Vengurlekar, Mohamad Qadri, Kevin Zhang, Michael Kaess, Christopher Metzler, Suren Jayasuriya, Adithya Pediredla | cs.CV | [PDF](http://arxiv.org/pdf/2404.04687v1){: .btn .btn-green } |

**Abstract**: Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent
technique in computer vision and graphics for reconstructing 3D scenes. GS
represents a scene as a set of 3D Gaussians with varying opacities and employs
a computationally efficient splatting operation along with analytical
derivatives to compute the 3D Gaussian parameters given scene images captured
from various viewpoints. Unfortunately, capturing surround view ($360^{\circ}$
viewpoint) images is impossible or impractical in many real-world imaging
scenarios, including underwater imaging, rooms inside a building, and
autonomous navigation. In these restricted baseline imaging scenarios, the GS
algorithm suffers from a well-known 'missing cone' problem, which results in
poor reconstruction along the depth axis. In this manuscript, we demonstrate
that using transient data (from sonars) allows us to address the missing cone
problem by sampling high-frequency data along the depth axis. We extend the
Gaussian splatting algorithms for two commonly used sonars and propose fusion
algorithms that simultaneously utilize RGB camera data and sonar data. Through
simulations, emulations, and hardware experiments across various imaging
scenarios, we show that the proposed fusion algorithms lead to significantly
better novel view synthesis (5 dB improvement in PSNR) and 3D geometry
reconstruction (60% lower Chamfer distance).



---

## DATENeRF: Depth-Aware Text-based Editing of NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-06 | Sara Rojas, Julien Philip, Kai Zhang, Sai Bi, Fujun Luan, Bernard Ghanem, Kalyan Sunkavall | cs.CV | [PDF](http://arxiv.org/pdf/2404.04526v1){: .btn .btn-green } |

**Abstract**: Recent advancements in diffusion models have shown remarkable proficiency in
editing 2D images based on text prompts. However, extending these techniques to
edit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual
2D frames can result in inconsistencies across multiple views. Our crucial
insight is that a NeRF scene's geometry can serve as a bridge to integrate
these 2D edits. Utilizing this geometry, we employ a depth-conditioned
ControlNet to enhance the coherence of each 2D image modification. Moreover, we
introduce an inpainting approach that leverages the depth information of NeRF
scenes to distribute 2D edits across different images, ensuring robustness
against errors and resampling challenges. Our results reveal that this
methodology achieves more consistent, lifelike, and detailed edits than
existing leading methods for text-driven NeRF scene editing.

Comments:
- 14 pages, Conference paper, 3D Scene Editing, Neural Rendering,
  Diffusion Models

---

## Robust Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-05 | François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2404.04211v1){: .btn .btn-green } |

**Abstract**: In this paper, we address common error sources for 3D Gaussian Splatting
(3DGS) including blur, imperfect camera poses, and color inconsistencies, with
the goal of improving its robustness for practical applications like
reconstructions from handheld phone captures. Our main contribution involves
modeling motion blur as a Gaussian distribution over camera poses, allowing us
to address both camera pose refinement and motion blur correction in a unified
way. Additionally, we propose mechanisms for defocus blur compensation and for
addressing color in-consistencies caused by ambient light, shadows, or due to
camera-related factors like varying white balancing settings. Our proposed
solutions integrate in a seamless way with the 3DGS formulation while
maintaining its benefits in terms of training efficiency and rendering speed.
We experimentally validate our contributions on relevant benchmark datasets
including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and
thus consistent improvements over relevant baselines.



---

## OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field  Reconstruction using Omnidirectional Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2404.03202v2){: .btn .btn-green } |

**Abstract**: Photorealistic reconstruction relying on 3D Gaussian Splatting has shown
promising potential in robotics. However, the current 3D Gaussian Splatting
system only supports radiance field reconstruction using undistorted
perspective images. In this paper, we present OmniGS, a novel omnidirectional
Gaussian splatting system, to take advantage of omnidirectional images for fast
radiance field reconstruction. Specifically, we conduct a theoretical analysis
of spherical camera model derivatives in 3D Gaussian Splatting. According to
the derivatives, we then implement a new GPU-accelerated omnidirectional
rasterizer that directly splats 3D Gaussians onto the equirectangular screen
space for omnidirectional image rendering. As a result, we realize
differentiable optimization of the radiance field without the requirement of
cube-map rectification or tangent-plane approximation. Extensive experiments
conducted in egocentric and roaming scenarios demonstrate that our method
achieves state-of-the-art reconstruction quality and high rendering speed using
omnidirectional images. To benefit the research community, the code will be
made publicly available once the paper is published.

Comments:
- 7 pages, 4 figures

---

## VF-NeRF: Viewshed Fields for Rigid NeRF Registration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Leo Segre, Shai Avidan | cs.CV | [PDF](http://arxiv.org/pdf/2404.03349v1){: .btn .btn-green } |

**Abstract**: 3D scene registration is a fundamental problem in computer vision that seeks
the best 6-DoF alignment between two scenes. This problem was extensively
investigated in the case of point clouds and meshes, but there has been
relatively limited work regarding Neural Radiance Fields (NeRF). In this paper,
we consider the problem of rigid registration between two NeRFs when the
position of the original cameras is not given. Our key novelty is the
introduction of Viewshed Fields (VF), an implicit function that determines, for
each 3D point, how likely it is to be viewed by the original cameras. We
demonstrate how VF can help in the various stages of NeRF registration, with an
extensive evaluation showing that VF-NeRF achieves SOTA results on various
datasets with different capturing approaches such as LLFF and Objaverese.



---

## RaFE: Generative Radiance Fields Restoration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.03654v2){: .btn .btn-green } |

**Abstract**: NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel
view synthesis and 3D reconstruction, but its performance is sensitive to input
image quality, which struggles to achieve high-fidelity rendering when provided
with low-quality sparse input viewpoints. Previous methods for NeRF restoration
are tailored for specific degradation type, ignoring the generality of
restoration. To overcome this limitation, we propose a generic radiance fields
restoration pipeline, named RaFE, which applies to various types of
degradations, such as low resolution, blurriness, noise, compression artifacts,
or their combinations. Our approach leverages the success of off-the-shelf 2D
restoration methods to recover the multi-view images individually. Instead of
reconstructing a blurred NeRF by averaging inconsistencies, we introduce a
novel approach using Generative Adversarial Networks (GANs) for NeRF generation
to better accommodate the geometric and appearance inconsistencies present in
the multi-view images. Specifically, we adopt a two-level tri-plane
architecture, where the coarse level remains fixed to represent the low-quality
NeRF, and a fine-level residual tri-plane to be added to the coarse level is
modeled as a distribution with GAN to capture potential variations in
restoration. We validate RaFE on both synthetic and real cases for various
restoration tasks, demonstrating superior performance in both quantitative and
qualitative evaluations, surpassing other 3D restoration methods specific to
single task. Please see our project website
https://zkaiwu.github.io/RaFE-Project/.

Comments:
- Project Page: https://zkaiwu.github.io/RaFE

---

## OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features  and Rendered Novel Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2404.03650v1){: .btn .btn-green } |

**Abstract**: Large visual-language models (VLMs), like CLIP, enable open-set image
segmentation to segment arbitrary concepts from an image in a zero-shot manner.
This goes beyond the traditional closed-set assumption, i.e., where models can
only segment classes from a pre-defined training set. More recently, first
works on open-set segmentation in 3D scenes have appeared in the literature.
These methods are heavily influenced by closed-set 3D convolutional approaches
that process point clouds or polygon meshes. However, these 3D scene
representations do not align well with the image-based nature of the
visual-language models. Indeed, point cloud and 3D meshes typically have a
lower resolution than images and the reconstructed 3D scene geometry might not
project well to the underlying 2D image sequences used to compute pixel-aligned
CLIP features. To address these challenges, we propose OpenNeRF which naturally
operates on posed images and directly encodes the VLM features within the NeRF.
This is similar in spirit to LERF, however our work shows that using pixel-wise
VLM features (instead of global CLIP features) results in an overall less
complex architecture without the need for additional DINO regularization. Our
OpenNeRF further leverages NeRF's ability to render novel views and extract
open-set VLM features from areas that are not well observed in the initial
posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF
outperforms recent open-vocabulary methods such as LERF and OpenScene by at
least +4.9 mIoU.

Comments:
- ICLR 2024, Project page: https://opennerf.github.io

---

## Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2404.03613v1){: .btn .btn-green } |

**Abstract**: As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view
synthesis, it is a natural extension to deform a canonical 3DGS to multiple
frames. However, previous works fail to accurately reconstruct dynamic scenes,
especially 1) static parts moving along nearby dynamic parts, and 2) some
dynamic areas are blurry. We attribute the failure to the wrong design of the
deformation field, which is built as a coordinate-based function. This approach
is problematic because 3DGS is a mixture of multiple fields centered at the
Gaussians, not just a single coordinate-based framework. To resolve this
problem, we define the deformation as a function of per-Gaussian embeddings and
temporal embeddings. Moreover, we decompose deformations as coarse and fine
deformations to model slow and fast movements, respectively. Also, we introduce
an efficient training strategy for faster convergence and higher quality.
Project page: https://jeongminb.github.io/e-d3dgs/

Comments:
- Preprint

---

## GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Emmanouil Nikolakakis, Utkarsh Gupta, Jonathan Vengosh, Justin Bui, Razvan Marinescu | eess.IV | [PDF](http://arxiv.org/pdf/2404.03126v1){: .btn .btn-green } |

**Abstract**: We present GaSpCT, a novel view synthesis and 3D scene representation method
used to generate novel projection views for Computer Tomography (CT) scans. We
adapt the Gaussian Splatting framework to enable novel view synthesis in CT
based on limited sets of 2D image projections and without the need for
Structure from Motion (SfM) methodologies. Therefore, we reduce the total
scanning duration and the amount of radiation dose the patient receives during
the scan. We adapted the loss function to our use-case by encouraging a
stronger background and foreground distinction using two sparsity promoting
regularizers: a beta loss and a total variation (TV) loss. Finally, we
initialize the Gaussian locations across the 3D space using a uniform prior
distribution of where the brain's positioning would be expected to be within
the field of view. We evaluate the performance of our model using brain CT
scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and
demonstrate that the rendered novel views closely match the original projection
views of the simulated scan, and have better performance than other implicit 3D
scene representations methodologies. Furthermore, we empirically observe
reduced training time compared to neural network based image synthesis for
sparse-view CT image reconstruction. Finally, the memory requirements of the
Gaussian Splatting representations are reduced by 17% compared to the
equivalent voxel grid image representations.

Comments:
- Under Review Process for MICCAI 2024

---

## SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai | cs.CV | [PDF](http://arxiv.org/pdf/2404.03736v1){: .btn .btn-green } |

**Abstract**: Recent advances in 2D/3D generative models enable the generation of dynamic
3D objects from a single-view video. Existing approaches utilize score
distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D
Gaussians. However, these methods struggle to strike a balance among reference
view alignment, spatio-temporal consistency, and motion fidelity under
single-view conditions due to the implicit nature of NeRF or the intricate
dense Gaussian motion prediction. To address these issues, this paper proposes
an efficient, sparse-controlled video-to-4D framework named SC4D, that
decouples motion and appearance to achieve superior video-to-4D generation.
Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian
Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity
of the learned motion and shape. Comprehensive experimental results demonstrate
that our method surpasses existing methods in both quality and efficiency. In
addition, facilitated by the disentangled modeling of motion and appearance of
SC4D, we devise a novel application that seamlessly transfers the learned
motion onto a diverse array of 4D entities according to textual descriptions.

Comments:
- Project Page: https://sc4d.github.io/

---

## Neural Radiance Fields with Torch Units

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Bingnan Ni, Huanyu Wang, Dongfeng Bai, Minghe Weng, Dexin Qi, Weichao Qiu, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.02617v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction
methods widely used in industrial applications. Although prevalent methods
achieve considerable improvements in small-scale scenes, accomplishing
reconstruction in complex and large-scale scenes is still challenging. First,
the background in complex scenes shows a large variance among different views.
Second, the current inference pattern, $i.e.$, a pixel only relies on an
individual camera ray, fails to capture contextual information. To solve these
problems, we propose to enlarge the ray perception field and build up the
sample points interactions. In this paper, we design a novel inference pattern
that encourages a single camera ray possessing more contextual information, and
models the relationship among sample points on each camera ray. To hold
contextual information,a camera ray in our proposed method can render a patch
of pixels simultaneously. Moreover, we replace the MLP in neural radiance field
models with distance-aware convolutions to enhance the feature propagation
among sample points from the same camera ray. To summarize, as a torchlight, a
ray in our proposed method achieves rendering a patch of image. Thus, we call
the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF
show that the Torch-NeRF exhibits excellent performance.



---

## LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2404.02742v1){: .btn .btn-green } |

**Abstract**: Although neural radiance fields (NeRFs) have achieved triumphs in image novel
view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS
methods employ a simple shift from image NVS methods while ignoring the dynamic
nature and the large-scale reconstruction problem of LiDAR point clouds. In
light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for
novel space-time LiDAR view synthesis. In consideration of the sparsity and
large-scale characteristics, we design a 4D hybrid representation combined with
multi-planar and grid features to achieve effective reconstruction in a
coarse-to-fine manner. Furthermore, we introduce geometric constraints derived
from point clouds to improve temporal consistency. For the realistic synthesis
of LiDAR point clouds, we incorporate the global optimization of ray-drop
probability to preserve cross-region patterns. Extensive experiments on
KITTI-360 and NuScenes datasets demonstrate the superiority of our method in
accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes
are available at https://github.com/ispc-lab/LiDAR4D.

Comments:
- Accepted by CVPR 2024. Project Page:
  https://dyfcalid.github.io/LiDAR4D

---

## Freditor: High-Fidelity and Transferable NeRF Editing by Frequency  Decomposition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang | cs.CV | [PDF](http://arxiv.org/pdf/2404.02514v1){: .btn .btn-green } |

**Abstract**: This paper enables high-fidelity, transferable NeRF editing by frequency
decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D
scenes while suffering from blurry results, and fail to capture detailed
structures caused by the inconsistency between 2D editings. Our critical
insight is that low-frequency components of images are more
multiview-consistent after editing compared with their high-frequency parts.
Moreover, the appearance style is mainly exhibited on the low-frequency
components, and the content details especially reside in high-frequency parts.
This motivates us to perform editing on low-frequency components, which results
in high-fidelity edited scenes. In addition, the editing is performed in the
low-frequency feature space, enabling stable intensity control and novel scene
transfer. Comprehensive experiments conducted on photorealistic datasets
demonstrate the superior performance of high-fidelity and transferable NeRF
editing. The project page is at \url{https://aigc3d.github.io/freditor}.



---

## GenN2N: Generative NeRF2NeRF Translation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan, Li Yi | cs.CV | [PDF](http://arxiv.org/pdf/2404.02788v1){: .btn .btn-green } |

**Abstract**: We present GenN2N, a unified NeRF-to-NeRF translation framework for various
NeRF translation tasks such as text-driven NeRF editing, colorization,
super-resolution, inpainting, etc. Unlike previous methods designed for
individual translation tasks with task-specific schemes, GenN2N achieves all
these NeRF editing tasks by employing a plug-and-play image-to-image translator
to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF
space. Since the 3D consistency of 2D edits may not be assured, we propose to
model the distribution of the underlying 3D edits through a generative model
that can cover all possible edited NeRFs. To model the distribution of 3D
edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes
images while decoding NeRFs. The latent space is trained to align with a
Gaussian distribution and the NeRFs are supervised through an adversarial loss
on its renderings. To ensure the latent code does not depend on 2D viewpoints
but truly reflects the 3D edits, we also regularize the latent code through a
contrastive learning scheme. Extensive experiments on various editing tasks
show GenN2N, as a universal framework, performs as well or better than
task-specific specialists while possessing flexible generative power. More
results on our project page: https://xiangyueliu.github.io/GenN2N/

Comments:
- Accepted to CVPR 2024. Project page:
  https://xiangyueliu.github.io/GenN2N/

---

## TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding  Autonomous Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.02410v1){: .btn .btn-green } |

**Abstract**: Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize
3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR
data capabilities but also overlooks the potential advantages of fusing LiDAR
with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera
Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both
LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and
novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D
mesh) and implicit (hierarchical octree feature) 3D representation derived from
LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D
Gaussian's properties are not only initialized in alignment with the 3D mesh
which provides more completed 3D shape and color information, but are also
endowed with broader contextual information through retrieved octree implicit
features. During the Gaussian Splatting optimization process, the 3D mesh
offers dense depth information as supervision, which enhances the training
process by learning of a robust geometry. Comprehensive evaluations conducted
on the Waymo Open Dataset and nuScenes Dataset validate our method's
state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our
method demonstrates fast training and achieves real-time RGB and depth
rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in
resolution of 1600x900 (nuScenes) in urban scenarios.



---

## Surface Reconstruction from Gaussian Splatting via Novel Stereo Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Yaniv Wolf, Amit Bracha, Ron Kimmel | cs.CV | [PDF](http://arxiv.org/pdf/2404.01810v1){: .btn .btn-green } |

**Abstract**: The Gaussian splatting for radiance field rendering method has recently
emerged as an efficient approach for accurate scene representation. It
optimizes the location, size, color, and shape of a cloud of 3D Gaussian
elements to visually match, after projection, or splatting, a set of given
images taken from various viewing directions. And yet, despite the proximity of
Gaussian elements to the shape boundaries, direct surface reconstruction of
objects in the scene is a challenge.
  We propose a novel approach for surface reconstruction from Gaussian
splatting models. Rather than relying on the Gaussian elements' locations as a
prior for surface reconstruction, we leverage the superior novel-view synthesis
capabilities of 3DGS. To that end, we use the Gaussian splatting model to
render pairs of stereo-calibrated novel views from which we extract depth
profiles using a stereo matching method. We then combine the extracted RGB-D
images into a geometrically consistent surface. The resulting reconstruction is
more accurate and shows finer details when compared to other methods for
surface reconstruction from Gaussian splatting models, while requiring
significantly less compute time compared to other surface reconstruction
methods.
  We performed extensive testing of the proposed method on in-the-wild scenes,
taken by a smartphone, showcasing its superior reconstruction abilities.
Additionally, we tested the proposed method on the Tanks and Temples benchmark,
and it has surpassed the current leading method for surface reconstruction from
Gaussian splatting models. Project page: https://gs2mesh.github.io/.

Comments:
- Project Page: https://gs2mesh.github.io/

---

## NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for  Memory-Efficient Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Sicheng Li, Hao Li, Yiyi Liao, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.02185v1){: .btn .btn-green } |

**Abstract**: The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene
modeling and novel-view synthesis. As a kind of visual media for 3D scene
representation, compression with high rate-distortion performance is an eternal
target. Motivated by advances in neural compression and neural field
representation, we propose NeRFCodec, an end-to-end NeRF compression framework
that integrates non-linear transform, quantization, and entropy coding for
memory-efficient scene representation. Since training a non-linear transform
directly on a large scale of NeRF feature planes is impractical, we discover
that pre-trained neural 2D image codec can be utilized for compressing the
features when adding content-specific parameters. Specifically, we reuse neural
2D image codec but modify its encoder and decoder heads, while keeping the
other parts of the pre-trained decoder frozen. This allows us to train the full
pipeline via supervision of rendering loss and entropy loss, yielding the
rate-distortion balance by updating the content-specific parameters. At test
time, the bitstreams containing latent code, feature decoder head, and other
side information are transmitted for communication. Experimental results
demonstrate our method outperforms existing NeRF compression methods, enabling
high-quality novel view synthesis with a memory budget of 0.5 MB.

Comments:
- Accepted at CVPR2024. The source code will be released

---

## Uncertainty-aware Active Learning of NeRF-based Object Models for Robot  Manipulators using Visual and Re-orientation Actions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Saptarshi Dasgupta, Akshat Gupta, Shreshth Tuli, Rohan Paul | cs.RO | [PDF](http://arxiv.org/pdf/2404.01812v1){: .btn .btn-green } |

**Abstract**: Manipulating unseen objects is challenging without a 3D representation, as
objects generally have occluded surfaces. This requires physical interaction
with objects to build their internal representations. This paper presents an
approach that enables a robot to rapidly learn the complete 3D model of a given
object for manipulation in unfamiliar orientations. We use an ensemble of
partially constructed NeRF models to quantify model uncertainty to determine
the next action (a visual or re-orientation action) by optimizing
informativeness and feasibility. Further, our approach determines when and how
to grasp and re-orient an object given its partial NeRF model and re-estimates
the object pose to rectify misalignments introduced during the interaction.
Experiments with a simulated Franka Emika Robot Manipulator operating in a
tabletop environment with benchmark objects demonstrate an improvement of (i)
14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth
reconstruction of the object surface (F-score) and (iii) 71% in the task
success rate of manipulating objects a-priori unseen orientations/stable
configurations in the scene; over current methods. The project page can be
found here: https://actnerf.github.io.

Comments:
- This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible

---

## Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Joshua Ahn, Haochen Wang, Raymond A. Yeh, Greg Shakhnarovich | cs.CV | [PDF](http://arxiv.org/pdf/2404.02155v1){: .btn .btn-green } |

**Abstract**: Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of
volumetric densities in neural radiance fields, i.e., the densities double when
scene size is halved, and vice versa. We call this property alpha invariance.
For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing
both distance and volume densities in log space, and 2) a
discretization-agnostic initialization strategy to guarantee high ray
transmittance. We revisit a few popular radiance field models and find that
these systems use various heuristics to deal with issues arising from scene
scaling. We test their behaviors and show our recipe to be more robust.

Comments:
- CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance

---

## FlexiDreamer: Single Image-to-3D Generation with FlexiCubes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ruowen Zhao, Zhengyi Wang, Yikai Wang, Zihan Zhou, Jun Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00987v1){: .btn .btn-green } |

**Abstract**: 3D content generation from text prompts or single images has made remarkable
progress in quality and speed recently. One of its dominant paradigms involves
generating consistent multi-view images followed by a sparse-view
reconstruction. However, due to the challenge of directly deforming the mesh
representation to approach the target topology, most methodologies learn an
implicit representation (such as NeRF) during the sparse-view reconstruction
and acquire the target mesh by a post-processing extraction. Although the
implicit representation can effectively model rich 3D information, its training
typically entails a long convergence time. In addition, the post-extraction
operation from the implicit field also leads to undesirable visual artifacts.
In this paper, we propose FlexiDreamer, a novel single image-to-3d generation
framework that reconstructs the target mesh in an end-to-end manner. By
leveraging a flexible gradient-based extraction known as FlexiCubes, our method
circumvents the defects brought by the post-processing and facilitates a direct
acquisition of the target mesh. Furthermore, we incorporate a multi-resolution
hash grid encoding scheme that progressively activates the encoding levels into
the implicit field in FlexiCubes to help capture geometric details for per-step
optimization. Notably, FlexiDreamer recovers a dense 3D structure from a
single-view image in approximately 1 minute on a single NVIDIA A100 GPU,
outperforming previous methodologies by a large margin.

Comments:
- project page:https://flexidreamer.github.io

---

## CityGaussian: Real-time High-quality Large-Scale Scene Rendering with  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.01133v1){: .btn .btn-green } |

**Abstract**: The advancement of real-time 3D scene reconstruction and novel view synthesis
has been significantly propelled by 3D Gaussian Splatting (3DGS). However,
effectively training large-scale 3DGS and rendering it in real-time across
various scales remains challenging. This paper introduces CityGaussian
(CityGS), which employs a novel divide-and-conquer training approach and
Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and
rendering. Specifically, the global scene prior and adaptive training data
selection enables efficient training and seamless fusion. Based on fused
Gaussian primitives, we generate different detail levels through compression,
and realize fast rendering across various scales through the proposed
block-wise detail levels selection and aggregation strategy. Extensive
experimental results on large-scale scenes demonstrate that our approach
attains state-of-theart rendering quality, enabling consistent real-time
rendering of largescale scenes across vastly different scales. Our project page
is available at https://dekuliutesla.github.io/citygs/.

Comments:
- Project Page: https://dekuliutesla.github.io/citygs/

---

## MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2404.01296v1){: .btn .btn-green } |

**Abstract**: We introduce a novel framework for 3D human avatar generation and
personalization, leveraging text prompts to enhance user engagement and
customization. Central to our approach are key innovations aimed at overcoming
the challenges in photo-realistic avatar synthesis. Firstly, we utilize a
conditional Neural Radiance Fields (NeRF) model, trained on a large-scale
unannotated multi-view dataset, to create a versatile initial solution space
that accelerates and diversifies avatar generation. Secondly, we develop a
geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models,
to ensure superior view invariance and enable direct optimization of avatar
geometry. These foundational ideas are complemented by our optimization
pipeline built on Variational Score Distillation (VSD), which mitigates texture
loss and over-saturation issues. As supported by our extensive experiments,
these strategies collectively enable the creation of custom avatars with
unparalleled visual quality and better adherence to input text prompts. You can
find more results and videos in our website:
https://syntec-research.github.io/MagicMirror



---

## MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00923v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping is essential for position tracking and
scene understanding. 3D Gaussian-based map representations enable
photorealistic reconstruction and real-time rendering of scenes using multiple
posed cameras. We show for the first time that using 3D Gaussians for map
representation with unposed camera images and inertial measurements can enable
accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural
radiance field-based representations by enabling faster rendering, scale
awareness, and improved trajectory tracking. Our framework enables
keyframe-based mapping and tracking utilizing loss functions that incorporate
relative pose transformations from pre-integrated inertial measurements, depth
estimates, and measures of photometric rendering quality. We also release a
multi-modal dataset, UT-MM, collected from a mobile robot equipped with a
camera and an inertial measurement unit. Experimental evaluation on several
scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking
and 5% improvement in photometric rendering quality compared to the current
3DGS SLAM state-of-the-art, while allowing real-time rendering of a
high-resolution dense 3D map. Project Webpage:
https://vita-group.github.io/MM3DGS-SLAM

Comments:
- Project Webpage: https://vita-group.github.io/MM3DGS-SLAM

---

## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2404.01053v1){: .btn .btn-green } |

**Abstract**: We present HAHA - a novel approach for animatable human avatar generation
from monocular input videos. The proposed method relies on learning the
trade-off between the use of Gaussian splatting and a textured mesh for
efficient and high fidelity rendering. We demonstrate its efficiency to animate
and render full-body human avatars controlled via the SMPL-X parametric model.
Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh
where it is necessary, like hair and out-of-mesh clothing. This results in a
minimal number of Gaussians being used to represent the full avatar, and
reduced rendering artifacts. This allows us to handle the animation of small
body parts such as fingers that are traditionally disregarded. We demonstrate
the effectiveness of our approach on two open datasets: SnapshotPeople and
X-Humans. Our method demonstrates on par reconstruction quality to the
state-of-the-art on SnapshotPeople, while using less than a third of Gaussians.
HAHA outperforms previous state-of-the-art on novel poses from X-Humans both
quantitatively and qualitatively.



---

## Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2404.01168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the
realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much
like its predecessor Neural Radiance Fields (NeRF), struggles to accurately
model physical reflections, particularly in mirrors that are ubiquitous in
real-world scenes. This oversight mistakenly perceives reflections as separate
entities that physically exist, resulting in inaccurate reconstructions and
inconsistent reflective properties across varied viewpoints. To address this
pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework
devised to master the intricacies of mirror geometries and reflections, paving
the way for the generation of realistically depicted mirror reflections. By
ingeniously incorporating mirror attributes into the 3DGS and leveraging the
principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to
observe from behind the mirror, enriching the realism of scene renderings.
Extensive assessments, spanning both synthetic and real-world scenes, showcase
our method's ability to render novel views with enhanced fidelity in real-time,
surpassing the state-of-the-art Mirror-NeRF specifically within the challenging
mirror regions. Our code will be made publicly available for reproducible
research.

Comments:
- 22 pages, 7 figures

---

## StructLDM: Structured Latent Diffusion for 3D Human Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Tao Hu, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.01241v2){: .btn .btn-green } |

**Abstract**: Recent 3D human generative models have achieved remarkable progress by
learning 3D-aware GANs from 2D images. However, existing 3D human generative
methods model humans in a compact 1D latent space, ignoring the articulated
structure and semantics of human body topology. In this paper, we explore more
expressive and higher-dimensional latent space for 3D human modeling and
propose StructLDM, a diffusion-based unconditional 3D human generative model,
which is learned from 2D images. StructLDM solves the challenges imposed due to
the high-dimensional growth of latent space with three key designs: 1) A
semantic structured latent space defined on the dense surface manifold of a
statistical human body template. 2) A structured 3D-aware auto-decoder that
factorizes the global latent space into several semantic body parts
parameterized by a set of conditional structured local NeRFs anchored to the
body template, which embeds the properties learned from the 2D training data
and can be decoded to render view-consistent humans under different poses and
clothing styles. 3) A structured latent diffusion model for generative human
appearance sampling. Extensive experiments validate StructLDM's
state-of-the-art generation performance and illustrate the expressiveness of
the structured latent space over the well-adopted 1D latent space. Notably,
StructLDM enables different levels of controllable 3D human generation and
editing, including pose/view/shape control, and high-level tasks including
compositional generations, part-aware clothing editing, 3D virtual try-on, etc.
Our project page is at: https://taohuumd.github.io/projects/StructLDM/.

Comments:
- Project page: https://taohuumd.github.io/projects/StructLDM/

---

## NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus | cs.CV | [PDF](http://arxiv.org/pdf/2404.01300v1){: .btn .btn-green } |

**Abstract**: Neural fields excel in computer vision and robotics due to their ability to
understand the 3D visual world such as inferring semantics, geometry, and
dynamics. Given the capabilities of neural fields in densely representing a 3D
scene from 2D images, we ask the question: Can we scale their self-supervised
pretraining, specifically using masked autoencoders, to generate effective 3D
representations from posed RGB images. Owing to the astounding success of
extending transformers to novel data modalities, we employ standard 3D Vision
Transformers to suit the unique formulation of NeRFs. We leverage NeRF's
volumetric grid as a dense input to the transformer, contrasting it with other
3D representations such as pointclouds where the information density can be
uneven, and the representation is irregular. Due to the difficulty of applying
masked autoencoders to an implicit representation, such as NeRF, we opt for
extracting an explicit representation that canonicalizes scenes across domains
by employing the camera trajectory for sampling. Our goal is made possible by
masking random patches from NeRF's radiance and density grid and employing a
standard 3D Swin Transformer to reconstruct the masked patches. In doing so,
the model can learn the semantic and spatial structure of complete scenes. We
pretrain this representation at scale on our proposed curated posed-RGB data,
totaling over 1.6 million images. Once pretrained, the encoder is used for
effective 3D transfer learning. Our novel self-supervised pretraining for
NeRFs, NeRF-MAE, scales remarkably well and improves performance on various
challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,
NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF
scene understanding baselines on Front3D and ScanNet datasets with an absolute
performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.

Comments:
- 29 pages, 13 figures. Project Page: https://nerf-mae.github.io/

---

## DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable  Primitive Assembly

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.00875v2){: .btn .btn-green } |

**Abstract**: We present a differentiable rendering framework to learn structured 3D
abstractions in the form of primitive assemblies from sparse RGB images
capturing a 3D object. By leveraging differentiable volume rendering, our
method does not require 3D supervision. Architecturally, our network follows
the general pipeline of an image-conditioned neural radiance field (NeRF)
exemplified by pixelNeRF for color prediction. As our core contribution, we
introduce differential primitive assembly (DPA) into NeRF to output a 3D
occupancy field in place of density prediction, where the predicted occupancies
serve as opacity values for volume rendering. Our network, coined DPA-Net,
produces a union of convexes, each as an intersection of convex quadric
primitives, to approximate the target 3D object, subject to an abstraction loss
and a masking loss, both defined in the image space upon volume rendering. With
test-time adaptation and additional sampling and loss designs aimed at
improving the accuracy and compactness of the obtained assemblies, our method
demonstrates superior performance over state-of-the-art alternatives for 3D
primitive abstraction from sparse views.

Comments:
- 14 pages

---

## Marrying NeRF with Feature Matching for One-step Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ronghan Chen, Yang Cong, Yu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.00891v1){: .btn .btn-green } |

**Abstract**: Given the image collection of an object, we aim at building a real-time
image-based pose estimation method, which requires neither its CAD model nor
hours of object-specific training. Recent NeRF-based methods provide a
promising solution by directly optimizing the pose from pixel loss between
rendered and target images. However, during inference, they require long
converging time, and suffer from local minima, making them impractical for
real-time robot applications. We aim at solving this problem by marrying image
matching with NeRF. With 2D matches and depth rendered by NeRF, we directly
solve the pose in one step by building 2D-3D correspondences between target and
initial view, thus allowing for real-time prediction. Moreover, to improve the
accuracy of 2D-3D correspondences, we propose a 3D consistent point mining
strategy, which effectively discards unfaithful points reconstruted by NeRF.
Moreover, current NeRF-based methods naively optimizing pixel loss fail at
occluded images. Thus, we further propose a 2D matches based sampling strategy
to preclude the occluded area. Experimental results on representative datasets
prove that our method outperforms state-of-the-art methods, and improves
inference efficiency by 90x, achieving real-time prediction at 6 FPS.

Comments:
- ICRA, 2024. Video https://www.youtube.com/watch?v=70fgUobOFWo

---

## NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented  Camera Pose Regressor and Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Juyeop Han, Lukas Lao Beyer, Guilherme V. Cavalheiro, Sertac Karaman | cs.RO | [PDF](http://arxiv.org/pdf/2404.01400v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful
tool for 3D reconstruction and novel view synthesis. However, the computational
cost of NeRF rendering and degradation in quality due to the presence of
artifacts pose significant challenges for its application in real-time and
robust robotic tasks, especially on embedded systems. This paper introduces a
novel framework that integrates NeRF-derived localization information with
Visual-Inertial Odometry(VIO) to provide a robust solution for robotic
navigation in a real-time. By training an absolute pose regression network with
augmented image data rendered from a NeRF and quantifying its uncertainty, our
approach effectively counters positional drift and enhances system reliability.
We also establish a mathematically sound foundation for combining visual
inertial navigation with camera localization neural networks, considering
uncertainty under a Bayesian framework. Experimental validation in the
photorealistic simulation environment demonstrates significant improvements in
accuracy compared to a conventional VIO approach.

Comments:
- 8 pages, 5 figures, 2 tables

---

## DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jie Long Lee, Chen Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2404.00874v1){: .btn .btn-green } |

**Abstract**: We present DiSR-NeRF, a diffusion-guided framework for view-consistent
super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement
for high-resolution (HR) reference images by leveraging existing powerful 2D
super-resolution models. Nonetheless, independent SR 2D images are often
inconsistent across different views. We thus propose Iterative 3D
Synchronization (I3DS) to mitigate the inconsistency problem via the inherent
multi-view consistency property of NeRF. Specifically, our I3DS alternates
between upscaling low-resolution (LR) rendered images with diffusion models,
and updating the underlying 3D representation with standard NeRF training. We
further introduce Renoised Score Distillation (RSD), a novel score-distillation
objective for 2D image resolution. Our RSD combines features from ancestral
sampling and Score Distillation Sampling (SDS) to generate sharp images that
are also LR-consistent. Qualitative and quantitative results on both synthetic
and real-world datasets demonstrate that our DiSR-NeRF can achieve better
results on NeRF super-resolution compared with existing works. Code and video
results available at the project website.



---

## SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency  Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yuru Xiao, Xianming Liu, Deming Zhai, Kui Jiang, Junjun Jiang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2404.00992v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) technology has made significant strides in
creating novel viewpoints. However, its effectiveness is hampered when working
with sparsely available views, often leading to performance dips due to
overfitting. FreeNeRF attempts to overcome this limitation by integrating
implicit geometry regularization, which incrementally improves both geometry
and textures. Nonetheless, an initial low positional encoding bandwidth results
in the exclusion of high-frequency elements. The quest for a holistic approach
that simultaneously addresses overfitting and the preservation of
high-frequency details remains ongoing. This study introduces a novel feature
matching based sparse geometry regularization module. This module excels in
pinpointing high-frequency keypoints, thereby safeguarding the integrity of
fine details. Through progressive refinement of geometry and textures across
NeRF iterations, we unveil an effective few-shot neural rendering architecture,
designated as SGCNeRF, for enhanced novel view synthesis. Our experiments
demonstrate that SGCNeRF not only achieves superior geometry-consistent
outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in
PSNR on the LLFF and DTU datasets, respectively.


