---
layout: default
title: April 2024
parent: Papers
nav_order: 202404
---

<!---metadata--->


## Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Y. Wang, A. Gao, Y. Gong, Y. Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2404.05236v1){: .btn .btn-green } |

**Abstract**: Recently, a surge of 3D style transfer methods has been proposed that
leverage the scene reconstruction power of a pre-trained neural radiance field
(NeRF). To successfully stylize a scene this way, one must first reconstruct a
photo-realistic radiance field from collected images of the scene. However,
when only sparse input views are available, pre-trained few-shot NeRFs often
suffer from high-frequency artifacts, which are generated as a by-product of
high-frequency details for improving reconstruction quality. Is it possible to
generate more faithful stylized scenes from sparse inputs by directly
optimizing encoding-based scene representation with target style? In this
paper, we consider the stylization of sparse-view scenes in terms of
disentangling content semantics and style textures. We propose a coarse-to-fine
sparse-view scene stylization framework, where a novel hierarchical
encoding-based neural representation is designed to generate high-quality
stylized scenes directly from implicit scene representations. We also propose a
new optimization strategy with content strength annealing to achieve realistic
stylization and better content preservation. Extensive experiments demonstrate
that our method can achieve high-quality stylization of sparse-view scenes and
outperforms fine-tuning-based baselines in terms of stylization quality and
efficiency.



---

## StylizedGS: Controllable Stylization for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Dingxi Zhang, Zhuoxun Chen, Yu-Jie Yuan, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2404.05220v1){: .btn .btn-green } |

**Abstract**: With the rapid development of XR, 3D generation and editing are becoming more
and more important, among which, stylization is an important tool of 3D
appearance editing. It can achieve consistent 3D artistic stylization given a
single reference style image and thus is a user-friendly editing way. However,
recent NeRF-based 3D stylization methods face efficiency issues that affect the
actual user experience and the implicit nature limits its ability to transfer
the geometric pattern styles. Additionally, the ability for artists to exert
flexible control over stylized scenes is considered highly desirable, fostering
an environment conducive to creative exploration. In this paper, we introduce
StylizedGS, a 3D neural style transfer framework with adaptable control over
perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The
3DGS brings the benefits of high efficiency. We propose a GS filter to
eliminate floaters in the reconstruction which affects the stylization effects
before stylization. Then the nearest neighbor-based style loss is introduced to
achieve stylization by fine-tuning the geometry and color parameters of 3DGS,
while a depth preservation loss with other regularizations is proposed to
prevent the tampering of geometry content. Moreover, facilitated by specially
designed losses, StylizedGS enables users to control color, stylized scale and
regions during the stylization to possess customized capabilities. Our method
can attain high-quality stylization results characterized by faithful
brushstrokes and geometric consistency with flexible controls. Extensive
experiments across various scenes and styles demonstrate the effectiveness and
efficiency of our method concerning both stylization quality and inference FPS.



---

## Semantic Flow: Learning Semantic Field of Dynamic Scenes from Monocular  Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-08 | Fengrui Tian, Yueqi Duan, Angtian Wang, Jianfei Guo, Shaoyi Du | cs.CV | [PDF](http://arxiv.org/pdf/2404.05163v1){: .btn .btn-green } |

**Abstract**: In this work, we pioneer Semantic Flow, a neural semantic representation of
dynamic scenes from monocular videos. In contrast to previous NeRF methods that
reconstruct dynamic scenes from the colors and volume densities of individual
points, Semantic Flow learns semantics from continuous flows that contain rich
3D motion information. As there is 2D-to-3D ambiguity problem in the viewing
direction when extracting 3D flow features from 2D video frames, we consider
the volume densities as opacity priors that describe the contributions of flow
features to the semantics on the frames. More specifically, we first learn a
flow network to predict flows in the dynamic scene, and propose a flow feature
aggregation module to extract flow features from video frames. Then, we propose
a flow attention module to extract motion information from flow features, which
is followed by a semantic network to output semantic logits of flows. We
integrate the logits with volume densities in the viewing direction to
supervise the flow features with semantic labels on video frames. Experimental
results show that our model is able to learn from multiple dynamic scenes and
supports a series of new tasks such as instance-level scene editing, semantic
completions, dynamic scene tracking and semantic adaption on novel scenes.
Codes are available at https://github.com/tianfr/Semantic-Flow/.

Comments:
- Accepted by ICLR 2024, Codes are available at
  https://github.com/tianfr/Semantic-Flow/

---

## CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality  Novel-view Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Gyeongjin Kang, Younggeun Lee, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2404.04913v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have achieved huge success in effectively
capturing and representing 3D objects and scenes. However, several factors have
impeded its further proliferation as next-generation 3D media. To establish a
ubiquitous presence in everyday media formats, such as images and videos, it is
imperative to devise a solution that effectively fulfills three key objectives:
fast encoding and decoding time, compact model sizes, and high-quality
renderings. Despite significant advancements, a comprehensive algorithm that
adequately addresses all objectives has yet to be fully realized. In this work,
we present CodecNeRF, a neural codec for NeRF representations, consisting of a
novel encoder and decoder architecture that can generate a NeRF representation
in a single forward pass. Furthermore, inspired by the recent
parameter-efficient finetuning approaches, we develop a novel finetuning method
to efficiently adapt the generated NeRF representations to a new test instance,
leading to high-quality image renderings and compact code sizes. The proposed
CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF,
achieved unprecedented compression performance of more than 150x and 20x
reduction in encoding time while maintaining (or improving) the image quality
on widely used 3D object datasets, such as ShapeNet and Objaverse.

Comments:
- 34 pages, 22 figures, Project page:
  https://gynjn.github.io/Codec-NeRF/

---

## GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric  Reconstruction Using Gaussian Splatting and NeRF

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Butian Xiong, Nanjun Zheng, Zhen Li | cs.CV | [PDF](http://arxiv.org/pdf/2404.04880v1){: .btn .btn-green } |

**Abstract**: We introduce a novel large-scale scene reconstruction benchmark that utilizes
newly developed 3D representation approaches: Gaussian Splatting and Neural
Radiance Fields, on our expansive GauU-Scene V2 dataset. GauU-Scene V2
encompasses over 6.5 square kilometers and features a comprehensive RGB dataset
coupled with LiDAR ground truth. This dataset offers a unique blend of urban
and academic environments for advanced spatial analysis, covering more than 6.5
km2. We also provide detailed supplementary information on data collection
protocols. Furthermore, we present an easy-to-follow pipeline to align the
COLMAP sparse point cloud with the detailed LiDAR dataset. Our evaluation of
U-Scene, which includes a detailed analysis across various novel viewpoints
using image-based metrics such as SSIM, LPIPS, and PSNR, shows contradictory
results when applying geometric-based metrics, such as Chamfer distance. This
leads to doubts about the reliability of current image-based measurement
matrices and geometric extraction methods on Gaussian Splatting. We also make
the dataset available on the following anonymous project page

Comments:
- 8 pages(No reference) 6 figures 4 tabs

---

## NeRF2Points: Large-Scale Point Cloud Generation From Street Views'  Radiance Field Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Peng Tu, Xun Zhou, Mingming Wang, Xiaojun Yang, Bo Peng, Ping Chen, Xiu Su, Yawen Huang, Yefeng Zheng, Chang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.04875v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have emerged as a paradigm-shifting methodology
for the photorealistic rendering of objects and environments, enabling the
synthesis of novel viewpoints with remarkable fidelity. This is accomplished
through the strategic utilization of object-centric camera poses characterized
by significant inter-frame overlap. This paper explores a compelling,
alternative utility of NeRF: the derivation of point clouds from aggregated
urban landscape imagery. The transmutation of street-view data into point
clouds is fraught with complexities, attributable to a nexus of interdependent
variables. First, high-quality point cloud generation hinges on precise camera
poses, yet many datasets suffer from inaccuracies in pose metadata. Also, the
standard approach of NeRF is ill-suited for the distinct characteristics of
street-view data from autonomous vehicles in vast, open settings. Autonomous
vehicle cameras often record with limited overlap, leading to blurring,
artifacts, and compromised pavement representation in NeRF-based point clouds.
In this paper, we present NeRF2Points, a tailored NeRF variant for urban point
cloud synthesis, notable for its high-quality output from RGB inputs alone. Our
paper is supported by a bespoke, high-resolution 20-kilometer urban street
dataset, designed for point cloud generation and evaluation. NeRF2Points
adeptly navigates the inherent challenges of NeRF-based point cloud synthesis
through the implementation of the following strategic innovations: (1)
Integration of Weighted Iterative Geometric Optimization (WIGO) and Structure
from Motion (SfM) for enhanced camera pose accuracy, elevating street-view data
precision. (2) Layered Perception and Integrated Modeling (LPiM) is designed
for distinct radiance field modeling in urban environments, resulting in
coherent point cloud representations.

Comments:
- 18 pages

---

## Dual-Camera Smooth Zoom on Mobile Phones

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-07 | Renlong Wu, Zhilu Zhang, Yu Yang, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2404.04908v1){: .btn .btn-green } |

**Abstract**: When zooming between dual cameras on a mobile, noticeable jumps in geometric
content and image color occur in the preview, inevitably affecting the user's
zoom experience. In this work, we introduce a new task, ie, dual-camera smooth
zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI)
technique is a potential solution but struggles with ground-truth collection.
To address the issue, we suggest a data factory solution where continuous
virtual cameras are assembled to generate DCSZ data by rendering reconstructed
3D models of the scene. In particular, we propose a novel dual-camera smooth
zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is
introduced to construct a specific 3D model for each virtual camera. With the
proposed data factory, we construct a synthetic dataset for DCSZ, and we
utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom
images without ground-truth for evaluation. Extensive experiments are conducted
with multiple FI methods. The results show that the fine-tuned FI models
achieve a significant performance improvement over the original ones on DCSZ
task. The datasets, codes, and pre-trained models will be publicly available.

Comments:
- 24

---

## Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-06 | Ziyuan Qu, Omkar Vengurlekar, Mohamad Qadri, Kevin Zhang, Michael Kaess, Christopher Metzler, Suren Jayasuriya, Adithya Pediredla | cs.CV | [PDF](http://arxiv.org/pdf/2404.04687v1){: .btn .btn-green } |

**Abstract**: Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent
technique in computer vision and graphics for reconstructing 3D scenes. GS
represents a scene as a set of 3D Gaussians with varying opacities and employs
a computationally efficient splatting operation along with analytical
derivatives to compute the 3D Gaussian parameters given scene images captured
from various viewpoints. Unfortunately, capturing surround view ($360^{\circ}$
viewpoint) images is impossible or impractical in many real-world imaging
scenarios, including underwater imaging, rooms inside a building, and
autonomous navigation. In these restricted baseline imaging scenarios, the GS
algorithm suffers from a well-known 'missing cone' problem, which results in
poor reconstruction along the depth axis. In this manuscript, we demonstrate
that using transient data (from sonars) allows us to address the missing cone
problem by sampling high-frequency data along the depth axis. We extend the
Gaussian splatting algorithms for two commonly used sonars and propose fusion
algorithms that simultaneously utilize RGB camera data and sonar data. Through
simulations, emulations, and hardware experiments across various imaging
scenarios, we show that the proposed fusion algorithms lead to significantly
better novel view synthesis (5 dB improvement in PSNR) and 3D geometry
reconstruction (60% lower Chamfer distance).



---

## DATENeRF: Depth-Aware Text-based Editing of NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-06 | Sara Rojas, Julien Philip, Kai Zhang, Sai Bi, Fujun Luan, Bernard Ghanem, Kalyan Sunkavall | cs.CV | [PDF](http://arxiv.org/pdf/2404.04526v1){: .btn .btn-green } |

**Abstract**: Recent advancements in diffusion models have shown remarkable proficiency in
editing 2D images based on text prompts. However, extending these techniques to
edit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual
2D frames can result in inconsistencies across multiple views. Our crucial
insight is that a NeRF scene's geometry can serve as a bridge to integrate
these 2D edits. Utilizing this geometry, we employ a depth-conditioned
ControlNet to enhance the coherence of each 2D image modification. Moreover, we
introduce an inpainting approach that leverages the depth information of NeRF
scenes to distribute 2D edits across different images, ensuring robustness
against errors and resampling challenges. Our results reveal that this
methodology achieves more consistent, lifelike, and detailed edits than
existing leading methods for text-driven NeRF scene editing.

Comments:
- 14 pages, Conference paper, 3D Scene Editing, Neural Rendering,
  Diffusion Models

---

## Robust Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-05 | François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2404.04211v1){: .btn .btn-green } |

**Abstract**: In this paper, we address common error sources for 3D Gaussian Splatting
(3DGS) including blur, imperfect camera poses, and color inconsistencies, with
the goal of improving its robustness for practical applications like
reconstructions from handheld phone captures. Our main contribution involves
modeling motion blur as a Gaussian distribution over camera poses, allowing us
to address both camera pose refinement and motion blur correction in a unified
way. Additionally, we propose mechanisms for defocus blur compensation and for
addressing color in-consistencies caused by ambient light, shadows, or due to
camera-related factors like varying white balancing settings. Our proposed
solutions integrate in a seamless way with the 3DGS formulation while
maintaining its benefits in terms of training efficiency and rendering speed.
We experimentally validate our contributions on relevant benchmark datasets
including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and
thus consistent improvements over relevant baselines.



---

## OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field  Reconstruction using Omnidirectional Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2404.03202v2){: .btn .btn-green } |

**Abstract**: Photorealistic reconstruction relying on 3D Gaussian Splatting has shown
promising potential in robotics. However, the current 3D Gaussian Splatting
system only supports radiance field reconstruction using undistorted
perspective images. In this paper, we present OmniGS, a novel omnidirectional
Gaussian splatting system, to take advantage of omnidirectional images for fast
radiance field reconstruction. Specifically, we conduct a theoretical analysis
of spherical camera model derivatives in 3D Gaussian Splatting. According to
the derivatives, we then implement a new GPU-accelerated omnidirectional
rasterizer that directly splats 3D Gaussians onto the equirectangular screen
space for omnidirectional image rendering. As a result, we realize
differentiable optimization of the radiance field without the requirement of
cube-map rectification or tangent-plane approximation. Extensive experiments
conducted in egocentric and roaming scenarios demonstrate that our method
achieves state-of-the-art reconstruction quality and high rendering speed using
omnidirectional images. To benefit the research community, the code will be
made publicly available once the paper is published.

Comments:
- 7 pages, 4 figures

---

## RaFE: Generative Radiance Fields Restoration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2404.03654v2){: .btn .btn-green } |

**Abstract**: NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel
view synthesis and 3D reconstruction, but its performance is sensitive to input
image quality, which struggles to achieve high-fidelity rendering when provided
with low-quality sparse input viewpoints. Previous methods for NeRF restoration
are tailored for specific degradation type, ignoring the generality of
restoration. To overcome this limitation, we propose a generic radiance fields
restoration pipeline, named RaFE, which applies to various types of
degradations, such as low resolution, blurriness, noise, compression artifacts,
or their combinations. Our approach leverages the success of off-the-shelf 2D
restoration methods to recover the multi-view images individually. Instead of
reconstructing a blurred NeRF by averaging inconsistencies, we introduce a
novel approach using Generative Adversarial Networks (GANs) for NeRF generation
to better accommodate the geometric and appearance inconsistencies present in
the multi-view images. Specifically, we adopt a two-level tri-plane
architecture, where the coarse level remains fixed to represent the low-quality
NeRF, and a fine-level residual tri-plane to be added to the coarse level is
modeled as a distribution with GAN to capture potential variations in
restoration. We validate RaFE on both synthetic and real cases for various
restoration tasks, demonstrating superior performance in both quantitative and
qualitative evaluations, surpassing other 3D restoration methods specific to
single task. Please see our project website
https://zkaiwu.github.io/RaFE-Project/.

Comments:
- Project Page: https://zkaiwu.github.io/RaFE

---

## Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh | cs.CV | [PDF](http://arxiv.org/pdf/2404.03613v1){: .btn .btn-green } |

**Abstract**: As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view
synthesis, it is a natural extension to deform a canonical 3DGS to multiple
frames. However, previous works fail to accurately reconstruct dynamic scenes,
especially 1) static parts moving along nearby dynamic parts, and 2) some
dynamic areas are blurry. We attribute the failure to the wrong design of the
deformation field, which is built as a coordinate-based function. This approach
is problematic because 3DGS is a mixture of multiple fields centered at the
Gaussians, not just a single coordinate-based framework. To resolve this
problem, we define the deformation as a function of per-Gaussian embeddings and
temporal embeddings. Moreover, we decompose deformations as coarse and fine
deformations to model slow and fast movements, respectively. Also, we introduce
an efficient training strategy for faster convergence and higher quality.
Project page: https://jeongminb.github.io/e-d3dgs/

Comments:
- Preprint

---

## VF-NeRF: Viewshed Fields for Rigid NeRF Registration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Leo Segre, Shai Avidan | cs.CV | [PDF](http://arxiv.org/pdf/2404.03349v1){: .btn .btn-green } |

**Abstract**: 3D scene registration is a fundamental problem in computer vision that seeks
the best 6-DoF alignment between two scenes. This problem was extensively
investigated in the case of point clouds and meshes, but there has been
relatively limited work regarding Neural Radiance Fields (NeRF). In this paper,
we consider the problem of rigid registration between two NeRFs when the
position of the original cameras is not given. Our key novelty is the
introduction of Viewshed Fields (VF), an implicit function that determines, for
each 3D point, how likely it is to be viewed by the original cameras. We
demonstrate how VF can help in the various stages of NeRF registration, with an
extensive evaluation showing that VF-NeRF achieves SOTA results on various
datasets with different capturing approaches such as LLFF and Objaverese.



---

## SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, Xiang Bai | cs.CV | [PDF](http://arxiv.org/pdf/2404.03736v1){: .btn .btn-green } |

**Abstract**: Recent advances in 2D/3D generative models enable the generation of dynamic
3D objects from a single-view video. Existing approaches utilize score
distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D
Gaussians. However, these methods struggle to strike a balance among reference
view alignment, spatio-temporal consistency, and motion fidelity under
single-view conditions due to the implicit nature of NeRF or the intricate
dense Gaussian motion prediction. To address these issues, this paper proposes
an efficient, sparse-controlled video-to-4D framework named SC4D, that
decouples motion and appearance to achieve superior video-to-4D generation.
Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian
Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity
of the learned motion and shape. Comprehensive experimental results demonstrate
that our method surpasses existing methods in both quality and efficiency. In
addition, facilitated by the disentangled modeling of motion and appearance of
SC4D, we devise a novel application that seamlessly transfers the learned
motion onto a diverse array of 4D entities according to textual descriptions.

Comments:
- Project Page: https://sc4d.github.io/

---

## OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features  and Rendered Novel Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2404.03650v1){: .btn .btn-green } |

**Abstract**: Large visual-language models (VLMs), like CLIP, enable open-set image
segmentation to segment arbitrary concepts from an image in a zero-shot manner.
This goes beyond the traditional closed-set assumption, i.e., where models can
only segment classes from a pre-defined training set. More recently, first
works on open-set segmentation in 3D scenes have appeared in the literature.
These methods are heavily influenced by closed-set 3D convolutional approaches
that process point clouds or polygon meshes. However, these 3D scene
representations do not align well with the image-based nature of the
visual-language models. Indeed, point cloud and 3D meshes typically have a
lower resolution than images and the reconstructed 3D scene geometry might not
project well to the underlying 2D image sequences used to compute pixel-aligned
CLIP features. To address these challenges, we propose OpenNeRF which naturally
operates on posed images and directly encodes the VLM features within the NeRF.
This is similar in spirit to LERF, however our work shows that using pixel-wise
VLM features (instead of global CLIP features) results in an overall less
complex architecture without the need for additional DINO regularization. Our
OpenNeRF further leverages NeRF's ability to render novel views and extract
open-set VLM features from areas that are not well observed in the initial
posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF
outperforms recent open-vocabulary methods such as LERF and OpenScene by at
least +4.9 mIoU.

Comments:
- ICLR 2024, Project page: https://opennerf.github.io

---

## GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-04 | Emmanouil Nikolakakis, Utkarsh Gupta, Jonathan Vengosh, Justin Bui, Razvan Marinescu | eess.IV | [PDF](http://arxiv.org/pdf/2404.03126v1){: .btn .btn-green } |

**Abstract**: We present GaSpCT, a novel view synthesis and 3D scene representation method
used to generate novel projection views for Computer Tomography (CT) scans. We
adapt the Gaussian Splatting framework to enable novel view synthesis in CT
based on limited sets of 2D image projections and without the need for
Structure from Motion (SfM) methodologies. Therefore, we reduce the total
scanning duration and the amount of radiation dose the patient receives during
the scan. We adapted the loss function to our use-case by encouraging a
stronger background and foreground distinction using two sparsity promoting
regularizers: a beta loss and a total variation (TV) loss. Finally, we
initialize the Gaussian locations across the 3D space using a uniform prior
distribution of where the brain's positioning would be expected to be within
the field of view. We evaluate the performance of our model using brain CT
scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and
demonstrate that the rendered novel views closely match the original projection
views of the simulated scan, and have better performance than other implicit 3D
scene representations methodologies. Furthermore, we empirically observe
reduced training time compared to neural network based image synthesis for
sparse-view CT image reconstruction. Finally, the memory requirements of the
Gaussian Splatting representations are reduced by 17% compared to the
equivalent voxel grid image representations.

Comments:
- Under Review Process for MICCAI 2024

---

## LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2404.02742v1){: .btn .btn-green } |

**Abstract**: Although neural radiance fields (NeRFs) have achieved triumphs in image novel
view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS
methods employ a simple shift from image NVS methods while ignoring the dynamic
nature and the large-scale reconstruction problem of LiDAR point clouds. In
light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for
novel space-time LiDAR view synthesis. In consideration of the sparsity and
large-scale characteristics, we design a 4D hybrid representation combined with
multi-planar and grid features to achieve effective reconstruction in a
coarse-to-fine manner. Furthermore, we introduce geometric constraints derived
from point clouds to improve temporal consistency. For the realistic synthesis
of LiDAR point clouds, we incorporate the global optimization of ray-drop
probability to preserve cross-region patterns. Extensive experiments on
KITTI-360 and NuScenes datasets demonstrate the superiority of our method in
accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes
are available at https://github.com/ispc-lab/LiDAR4D.

Comments:
- Accepted by CVPR 2024. Project Page:
  https://dyfcalid.github.io/LiDAR4D

---

## Neural Radiance Fields with Torch Units

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Bingnan Ni, Huanyu Wang, Dongfeng Bai, Minghe Weng, Dexin Qi, Weichao Qiu, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.02617v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction
methods widely used in industrial applications. Although prevalent methods
achieve considerable improvements in small-scale scenes, accomplishing
reconstruction in complex and large-scale scenes is still challenging. First,
the background in complex scenes shows a large variance among different views.
Second, the current inference pattern, $i.e.$, a pixel only relies on an
individual camera ray, fails to capture contextual information. To solve these
problems, we propose to enlarge the ray perception field and build up the
sample points interactions. In this paper, we design a novel inference pattern
that encourages a single camera ray possessing more contextual information, and
models the relationship among sample points on each camera ray. To hold
contextual information,a camera ray in our proposed method can render a patch
of pixels simultaneously. Moreover, we replace the MLP in neural radiance field
models with distance-aware convolutions to enhance the feature propagation
among sample points from the same camera ray. To summarize, as a torchlight, a
ray in our proposed method achieves rendering a patch of image. Thus, we call
the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF
show that the Torch-NeRF exhibits excellent performance.



---

## GenN2N: Generative NeRF2NeRF Translation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan, Li Yi | cs.CV | [PDF](http://arxiv.org/pdf/2404.02788v1){: .btn .btn-green } |

**Abstract**: We present GenN2N, a unified NeRF-to-NeRF translation framework for various
NeRF translation tasks such as text-driven NeRF editing, colorization,
super-resolution, inpainting, etc. Unlike previous methods designed for
individual translation tasks with task-specific schemes, GenN2N achieves all
these NeRF editing tasks by employing a plug-and-play image-to-image translator
to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF
space. Since the 3D consistency of 2D edits may not be assured, we propose to
model the distribution of the underlying 3D edits through a generative model
that can cover all possible edited NeRFs. To model the distribution of 3D
edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes
images while decoding NeRFs. The latent space is trained to align with a
Gaussian distribution and the NeRFs are supervised through an adversarial loss
on its renderings. To ensure the latent code does not depend on 2D viewpoints
but truly reflects the 3D edits, we also regularize the latent code through a
contrastive learning scheme. Extensive experiments on various editing tasks
show GenN2N, as a universal framework, performs as well or better than
task-specific specialists while possessing flexible generative power. More
results on our project page: https://xiangyueliu.github.io/GenN2N/

Comments:
- Accepted to CVPR 2024. Project page:
  https://xiangyueliu.github.io/GenN2N/

---

## TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding  Autonomous Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.02410v1){: .btn .btn-green } |

**Abstract**: Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize
3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR
data capabilities but also overlooks the potential advantages of fusing LiDAR
with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera
Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both
LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and
novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D
mesh) and implicit (hierarchical octree feature) 3D representation derived from
LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D
Gaussian's properties are not only initialized in alignment with the 3D mesh
which provides more completed 3D shape and color information, but are also
endowed with broader contextual information through retrieved octree implicit
features. During the Gaussian Splatting optimization process, the 3D mesh
offers dense depth information as supervision, which enhances the training
process by learning of a robust geometry. Comprehensive evaluations conducted
on the Waymo Open Dataset and nuScenes Dataset validate our method's
state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our
method demonstrates fast training and achieves real-time RGB and depth
rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in
resolution of 1600x900 (nuScenes) in urban scenarios.



---

## Freditor: High-Fidelity and Transferable NeRF Editing by Frequency  Decomposition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-03 | Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang | cs.CV | [PDF](http://arxiv.org/pdf/2404.02514v1){: .btn .btn-green } |

**Abstract**: This paper enables high-fidelity, transferable NeRF editing by frequency
decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D
scenes while suffering from blurry results, and fail to capture detailed
structures caused by the inconsistency between 2D editings. Our critical
insight is that low-frequency components of images are more
multiview-consistent after editing compared with their high-frequency parts.
Moreover, the appearance style is mainly exhibited on the low-frequency
components, and the content details especially reside in high-frequency parts.
This motivates us to perform editing on low-frequency components, which results
in high-fidelity edited scenes. In addition, the editing is performed in the
low-frequency feature space, enabling stable intensity control and novel scene
transfer. Comprehensive experiments conducted on photorealistic datasets
demonstrate the superior performance of high-fidelity and transferable NeRF
editing. The project page is at \url{https://aigc3d.github.io/freditor}.



---

## Surface Reconstruction from Gaussian Splatting via Novel Stereo Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Yaniv Wolf, Amit Bracha, Ron Kimmel | cs.CV | [PDF](http://arxiv.org/pdf/2404.01810v1){: .btn .btn-green } |

**Abstract**: The Gaussian splatting for radiance field rendering method has recently
emerged as an efficient approach for accurate scene representation. It
optimizes the location, size, color, and shape of a cloud of 3D Gaussian
elements to visually match, after projection, or splatting, a set of given
images taken from various viewing directions. And yet, despite the proximity of
Gaussian elements to the shape boundaries, direct surface reconstruction of
objects in the scene is a challenge.
  We propose a novel approach for surface reconstruction from Gaussian
splatting models. Rather than relying on the Gaussian elements' locations as a
prior for surface reconstruction, we leverage the superior novel-view synthesis
capabilities of 3DGS. To that end, we use the Gaussian splatting model to
render pairs of stereo-calibrated novel views from which we extract depth
profiles using a stereo matching method. We then combine the extracted RGB-D
images into a geometrically consistent surface. The resulting reconstruction is
more accurate and shows finer details when compared to other methods for
surface reconstruction from Gaussian splatting models, while requiring
significantly less compute time compared to other surface reconstruction
methods.
  We performed extensive testing of the proposed method on in-the-wild scenes,
taken by a smartphone, showcasing its superior reconstruction abilities.
Additionally, we tested the proposed method on the Tanks and Temples benchmark,
and it has surpassed the current leading method for surface reconstruction from
Gaussian splatting models. Project page: https://gs2mesh.github.io/.

Comments:
- Project Page: https://gs2mesh.github.io/

---

## NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for  Memory-Efficient Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Sicheng Li, Hao Li, Yiyi Liao, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2404.02185v1){: .btn .btn-green } |

**Abstract**: The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene
modeling and novel-view synthesis. As a kind of visual media for 3D scene
representation, compression with high rate-distortion performance is an eternal
target. Motivated by advances in neural compression and neural field
representation, we propose NeRFCodec, an end-to-end NeRF compression framework
that integrates non-linear transform, quantization, and entropy coding for
memory-efficient scene representation. Since training a non-linear transform
directly on a large scale of NeRF feature planes is impractical, we discover
that pre-trained neural 2D image codec can be utilized for compressing the
features when adding content-specific parameters. Specifically, we reuse neural
2D image codec but modify its encoder and decoder heads, while keeping the
other parts of the pre-trained decoder frozen. This allows us to train the full
pipeline via supervision of rendering loss and entropy loss, yielding the
rate-distortion balance by updating the content-specific parameters. At test
time, the bitstreams containing latent code, feature decoder head, and other
side information are transmitted for communication. Experimental results
demonstrate our method outperforms existing NeRF compression methods, enabling
high-quality novel view synthesis with a memory budget of 0.5 MB.

Comments:
- Accepted at CVPR2024. The source code will be released

---

## Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Joshua Ahn, Haochen Wang, Raymond A. Yeh, Greg Shakhnarovich | cs.CV | [PDF](http://arxiv.org/pdf/2404.02155v1){: .btn .btn-green } |

**Abstract**: Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of
volumetric densities in neural radiance fields, i.e., the densities double when
scene size is halved, and vice versa. We call this property alpha invariance.
For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing
both distance and volume densities in log space, and 2) a
discretization-agnostic initialization strategy to guarantee high ray
transmittance. We revisit a few popular radiance field models and find that
these systems use various heuristics to deal with issues arising from scene
scaling. We test their behaviors and show our recipe to be more robust.

Comments:
- CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance

---

## Uncertainty-aware Active Learning of NeRF-based Object Models for Robot  Manipulators using Visual and Re-orientation Actions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-02 | Saptarshi Dasgupta, Akshat Gupta, Shreshth Tuli, Rohan Paul | cs.RO | [PDF](http://arxiv.org/pdf/2404.01812v1){: .btn .btn-green } |

**Abstract**: Manipulating unseen objects is challenging without a 3D representation, as
objects generally have occluded surfaces. This requires physical interaction
with objects to build their internal representations. This paper presents an
approach that enables a robot to rapidly learn the complete 3D model of a given
object for manipulation in unfamiliar orientations. We use an ensemble of
partially constructed NeRF models to quantify model uncertainty to determine
the next action (a visual or re-orientation action) by optimizing
informativeness and feasibility. Further, our approach determines when and how
to grasp and re-orient an object given its partial NeRF model and re-estimates
the object pose to rectify misalignments introduced during the interaction.
Experiments with a simulated Franka Emika Robot Manipulator operating in a
tabletop environment with benchmark objects demonstrate an improvement of (i)
14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth
reconstruction of the object surface (F-score) and (iii) 71% in the task
success rate of manipulating objects a-priori unseen orientations/stable
configurations in the scene; over current methods. The project page can be
found here: https://actnerf.github.io.

Comments:
- This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible

---

## Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2404.01168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the
realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much
like its predecessor Neural Radiance Fields (NeRF), struggles to accurately
model physical reflections, particularly in mirrors that are ubiquitous in
real-world scenes. This oversight mistakenly perceives reflections as separate
entities that physically exist, resulting in inaccurate reconstructions and
inconsistent reflective properties across varied viewpoints. To address this
pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework
devised to master the intricacies of mirror geometries and reflections, paving
the way for the generation of realistically depicted mirror reflections. By
ingeniously incorporating mirror attributes into the 3DGS and leveraging the
principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to
observe from behind the mirror, enriching the realism of scene renderings.
Extensive assessments, spanning both synthetic and real-world scenes, showcase
our method's ability to render novel views with enhanced fidelity in real-time,
surpassing the state-of-the-art Mirror-NeRF specifically within the challenging
mirror regions. Our code will be made publicly available for reproducible
research.

Comments:
- 22 pages, 7 figures

---

## MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler | cs.CV | [PDF](http://arxiv.org/pdf/2404.01296v1){: .btn .btn-green } |

**Abstract**: We introduce a novel framework for 3D human avatar generation and
personalization, leveraging text prompts to enhance user engagement and
customization. Central to our approach are key innovations aimed at overcoming
the challenges in photo-realistic avatar synthesis. Firstly, we utilize a
conditional Neural Radiance Fields (NeRF) model, trained on a large-scale
unannotated multi-view dataset, to create a versatile initial solution space
that accelerates and diversifies avatar generation. Secondly, we develop a
geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models,
to ensure superior view invariance and enable direct optimization of avatar
geometry. These foundational ideas are complemented by our optimization
pipeline built on Variational Score Distillation (VSD), which mitigates texture
loss and over-saturation issues. As supported by our extensive experiments,
these strategies collectively enable the creation of custom avatars with
unparalleled visual quality and better adherence to input text prompts. You can
find more results and videos in our website:
https://syntec-research.github.io/MagicMirror



---

## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2404.01053v1){: .btn .btn-green } |

**Abstract**: We present HAHA - a novel approach for animatable human avatar generation
from monocular input videos. The proposed method relies on learning the
trade-off between the use of Gaussian splatting and a textured mesh for
efficient and high fidelity rendering. We demonstrate its efficiency to animate
and render full-body human avatars controlled via the SMPL-X parametric model.
Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh
where it is necessary, like hair and out-of-mesh clothing. This results in a
minimal number of Gaussians being used to represent the full avatar, and
reduced rendering artifacts. This allows us to handle the animation of small
body parts such as fingers that are traditionally disregarded. We demonstrate
the effectiveness of our approach on two open datasets: SnapshotPeople and
X-Humans. Our method demonstrates on par reconstruction quality to the
state-of-the-art on SnapshotPeople, while using less than a third of Gaussians.
HAHA outperforms previous state-of-the-art on novel poses from X-Humans both
quantitatively and qualitatively.



---

## StructLDM: Structured Latent Diffusion for 3D Human Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Tao Hu, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2404.01241v2){: .btn .btn-green } |

**Abstract**: Recent 3D human generative models have achieved remarkable progress by
learning 3D-aware GANs from 2D images. However, existing 3D human generative
methods model humans in a compact 1D latent space, ignoring the articulated
structure and semantics of human body topology. In this paper, we explore more
expressive and higher-dimensional latent space for 3D human modeling and
propose StructLDM, a diffusion-based unconditional 3D human generative model,
which is learned from 2D images. StructLDM solves the challenges imposed due to
the high-dimensional growth of latent space with three key designs: 1) A
semantic structured latent space defined on the dense surface manifold of a
statistical human body template. 2) A structured 3D-aware auto-decoder that
factorizes the global latent space into several semantic body parts
parameterized by a set of conditional structured local NeRFs anchored to the
body template, which embeds the properties learned from the 2D training data
and can be decoded to render view-consistent humans under different poses and
clothing styles. 3) A structured latent diffusion model for generative human
appearance sampling. Extensive experiments validate StructLDM's
state-of-the-art generation performance and illustrate the expressiveness of
the structured latent space over the well-adopted 1D latent space. Notably,
StructLDM enables different levels of controllable 3D human generation and
editing, including pose/view/shape control, and high-level tasks including
compositional generations, part-aware clothing editing, 3D virtual try-on, etc.
Our project page is at: https://taohuumd.github.io/projects/StructLDM/.

Comments:
- Project page: https://taohuumd.github.io/projects/StructLDM/

---

## CityGaussian: Real-time High-quality Large-Scale Scene Rendering with  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.01133v1){: .btn .btn-green } |

**Abstract**: The advancement of real-time 3D scene reconstruction and novel view synthesis
has been significantly propelled by 3D Gaussian Splatting (3DGS). However,
effectively training large-scale 3DGS and rendering it in real-time across
various scales remains challenging. This paper introduces CityGaussian
(CityGS), which employs a novel divide-and-conquer training approach and
Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and
rendering. Specifically, the global scene prior and adaptive training data
selection enables efficient training and seamless fusion. Based on fused
Gaussian primitives, we generate different detail levels through compression,
and realize fast rendering across various scales through the proposed
block-wise detail levels selection and aggregation strategy. Extensive
experimental results on large-scale scenes demonstrate that our approach
attains state-of-theart rendering quality, enabling consistent real-time
rendering of largescale scenes across vastly different scales. Our project page
is available at https://dekuliutesla.github.io/citygs/.

Comments:
- Project Page: https://dekuliutesla.github.io/citygs/

---

## NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus | cs.CV | [PDF](http://arxiv.org/pdf/2404.01300v1){: .btn .btn-green } |

**Abstract**: Neural fields excel in computer vision and robotics due to their ability to
understand the 3D visual world such as inferring semantics, geometry, and
dynamics. Given the capabilities of neural fields in densely representing a 3D
scene from 2D images, we ask the question: Can we scale their self-supervised
pretraining, specifically using masked autoencoders, to generate effective 3D
representations from posed RGB images. Owing to the astounding success of
extending transformers to novel data modalities, we employ standard 3D Vision
Transformers to suit the unique formulation of NeRFs. We leverage NeRF's
volumetric grid as a dense input to the transformer, contrasting it with other
3D representations such as pointclouds where the information density can be
uneven, and the representation is irregular. Due to the difficulty of applying
masked autoencoders to an implicit representation, such as NeRF, we opt for
extracting an explicit representation that canonicalizes scenes across domains
by employing the camera trajectory for sampling. Our goal is made possible by
masking random patches from NeRF's radiance and density grid and employing a
standard 3D Swin Transformer to reconstruct the masked patches. In doing so,
the model can learn the semantic and spatial structure of complete scenes. We
pretrain this representation at scale on our proposed curated posed-RGB data,
totaling over 1.6 million images. Once pretrained, the encoder is used for
effective 3D transfer learning. Our novel self-supervised pretraining for
NeRFs, NeRF-MAE, scales remarkably well and improves performance on various
challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,
NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF
scene understanding baselines on Front3D and ScanNet datasets with an absolute
performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.

Comments:
- 29 pages, 13 figures. Project Page: https://nerf-mae.github.io/

---

## NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented  Camera Pose Regressor and Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Juyeop Han, Lukas Lao Beyer, Guilherme V. Cavalheiro, Sertac Karaman | cs.RO | [PDF](http://arxiv.org/pdf/2404.01400v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful
tool for 3D reconstruction and novel view synthesis. However, the computational
cost of NeRF rendering and degradation in quality due to the presence of
artifacts pose significant challenges for its application in real-time and
robust robotic tasks, especially on embedded systems. This paper introduces a
novel framework that integrates NeRF-derived localization information with
Visual-Inertial Odometry(VIO) to provide a robust solution for robotic
navigation in a real-time. By training an absolute pose regression network with
augmented image data rendered from a NeRF and quantifying its uncertainty, our
approach effectively counters positional drift and enhances system reliability.
We also establish a mathematically sound foundation for combining visual
inertial navigation with camera localization neural networks, considering
uncertainty under a Bayesian framework. Experimental validation in the
photorealistic simulation environment demonstrates significant improvements in
accuracy compared to a conventional VIO approach.

Comments:
- 8 pages, 5 figures, 2 tables

---

## SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency  Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Yuru Xiao, Xianming Liu, Deming Zhai, Kui Jiang, Junjun Jiang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2404.00992v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) technology has made significant strides in
creating novel viewpoints. However, its effectiveness is hampered when working
with sparsely available views, often leading to performance dips due to
overfitting. FreeNeRF attempts to overcome this limitation by integrating
implicit geometry regularization, which incrementally improves both geometry
and textures. Nonetheless, an initial low positional encoding bandwidth results
in the exclusion of high-frequency elements. The quest for a holistic approach
that simultaneously addresses overfitting and the preservation of
high-frequency details remains ongoing. This study introduces a novel feature
matching based sparse geometry regularization module. This module excels in
pinpointing high-frequency keypoints, thereby safeguarding the integrity of
fine details. Through progressive refinement of geometry and textures across
NeRF iterations, we unveil an effective few-shot neural rendering architecture,
designated as SGCNeRF, for enhanced novel view synthesis. Our experiments
demonstrate that SGCNeRF not only achieves superior geometry-consistent
outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in
PSNR on the LLFF and DTU datasets, respectively.



---

## DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Jie Long Lee, Chen Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2404.00874v1){: .btn .btn-green } |

**Abstract**: We present DiSR-NeRF, a diffusion-guided framework for view-consistent
super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement
for high-resolution (HR) reference images by leveraging existing powerful 2D
super-resolution models. Nonetheless, independent SR 2D images are often
inconsistent across different views. We thus propose Iterative 3D
Synchronization (I3DS) to mitigate the inconsistency problem via the inherent
multi-view consistency property of NeRF. Specifically, our I3DS alternates
between upscaling low-resolution (LR) rendered images with diffusion models,
and updating the underlying 3D representation with standard NeRF training. We
further introduce Renoised Score Distillation (RSD), a novel score-distillation
objective for 2D image resolution. Our RSD combines features from ancestral
sampling and Score Distillation Sampling (SDS) to generate sharp images that
are also LR-consistent. Qualitative and quantitative results on both synthetic
and real-world datasets demonstrate that our DiSR-NeRF can achieve better
results on NeRF super-resolution compared with existing works. Code and video
results available at the project website.



---

## DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable  Primitive Assembly

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2404.00875v2){: .btn .btn-green } |

**Abstract**: We present a differentiable rendering framework to learn structured 3D
abstractions in the form of primitive assemblies from sparse RGB images
capturing a 3D object. By leveraging differentiable volume rendering, our
method does not require 3D supervision. Architecturally, our network follows
the general pipeline of an image-conditioned neural radiance field (NeRF)
exemplified by pixelNeRF for color prediction. As our core contribution, we
introduce differential primitive assembly (DPA) into NeRF to output a 3D
occupancy field in place of density prediction, where the predicted occupancies
serve as opacity values for volume rendering. Our network, coined DPA-Net,
produces a union of convexes, each as an intersection of convex quadric
primitives, to approximate the target 3D object, subject to an abstraction loss
and a masking loss, both defined in the image space upon volume rendering. With
test-time adaptation and additional sampling and loss designs aimed at
improving the accuracy and compactness of the obtained assemblies, our method
demonstrates superior performance over state-of-the-art alternatives for 3D
primitive abstraction from sparse views.

Comments:
- 14 pages

---

## MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00923v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping is essential for position tracking and
scene understanding. 3D Gaussian-based map representations enable
photorealistic reconstruction and real-time rendering of scenes using multiple
posed cameras. We show for the first time that using 3D Gaussians for map
representation with unposed camera images and inertial measurements can enable
accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural
radiance field-based representations by enabling faster rendering, scale
awareness, and improved trajectory tracking. Our framework enables
keyframe-based mapping and tracking utilizing loss functions that incorporate
relative pose transformations from pre-integrated inertial measurements, depth
estimates, and measures of photometric rendering quality. We also release a
multi-modal dataset, UT-MM, collected from a mobile robot equipped with a
camera and an inertial measurement unit. Experimental evaluation on several
scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking
and 5% improvement in photometric rendering quality compared to the current
3DGS SLAM state-of-the-art, while allowing real-time rendering of a
high-resolution dense 3D map. Project Webpage:
https://vita-group.github.io/MM3DGS-SLAM

Comments:
- Project Webpage: https://vita-group.github.io/MM3DGS-SLAM

---

## Marrying NeRF with Feature Matching for One-step Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ronghan Chen, Yang Cong, Yu Ren | cs.CV | [PDF](http://arxiv.org/pdf/2404.00891v1){: .btn .btn-green } |

**Abstract**: Given the image collection of an object, we aim at building a real-time
image-based pose estimation method, which requires neither its CAD model nor
hours of object-specific training. Recent NeRF-based methods provide a
promising solution by directly optimizing the pose from pixel loss between
rendered and target images. However, during inference, they require long
converging time, and suffer from local minima, making them impractical for
real-time robot applications. We aim at solving this problem by marrying image
matching with NeRF. With 2D matches and depth rendered by NeRF, we directly
solve the pose in one step by building 2D-3D correspondences between target and
initial view, thus allowing for real-time prediction. Moreover, to improve the
accuracy of 2D-3D correspondences, we propose a 3D consistent point mining
strategy, which effectively discards unfaithful points reconstruted by NeRF.
Moreover, current NeRF-based methods naively optimizing pixel loss fail at
occluded images. Thus, we further propose a 2D matches based sampling strategy
to preclude the occluded area. Experimental results on representative datasets
prove that our method outperforms state-of-the-art methods, and improves
inference efficiency by 90x, achieving real-time prediction at 6 FPS.

Comments:
- ICRA, 2024. Video https://www.youtube.com/watch?v=70fgUobOFWo

---

## FlexiDreamer: Single Image-to-3D Generation with FlexiCubes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-04-01 | Ruowen Zhao, Zhengyi Wang, Yikai Wang, Zihan Zhou, Jun Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2404.00987v1){: .btn .btn-green } |

**Abstract**: 3D content generation from text prompts or single images has made remarkable
progress in quality and speed recently. One of its dominant paradigms involves
generating consistent multi-view images followed by a sparse-view
reconstruction. However, due to the challenge of directly deforming the mesh
representation to approach the target topology, most methodologies learn an
implicit representation (such as NeRF) during the sparse-view reconstruction
and acquire the target mesh by a post-processing extraction. Although the
implicit representation can effectively model rich 3D information, its training
typically entails a long convergence time. In addition, the post-extraction
operation from the implicit field also leads to undesirable visual artifacts.
In this paper, we propose FlexiDreamer, a novel single image-to-3d generation
framework that reconstructs the target mesh in an end-to-end manner. By
leveraging a flexible gradient-based extraction known as FlexiCubes, our method
circumvents the defects brought by the post-processing and facilitates a direct
acquisition of the target mesh. Furthermore, we incorporate a multi-resolution
hash grid encoding scheme that progressively activates the encoding levels into
the implicit field in FlexiCubes to help capture geometric details for per-step
optimization. Notably, FlexiDreamer recovers a dense 3D structure from a
single-view image in approximately 1 minute on a single NVIDIA A100 GPU,
outperforming previous methodologies by a large margin.

Comments:
- project page:https://flexidreamer.github.io
