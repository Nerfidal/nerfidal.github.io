---
layout: default
title: October 2024
parent: Papers
nav_order: 202410
---

<!---metadata--->


## DiffGS: Functional Gaussian Splatting Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-25 | Junsheng Zhou, Weiqi Zhang, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2410.19657v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown convincing performance in rendering
speed and fidelity, yet the generation of Gaussian Splatting remains a
challenge due to its discreteness and unstructured nature. In this work, we
propose DiffGS, a general Gaussian generator based on latent diffusion models.
DiffGS is a powerful and efficient 3D generative model which is capable of
generating Gaussian primitives at arbitrary numbers for high-fidelity rendering
with rasterization. The key insight is to represent Gaussian Splatting in a
disentangled manner via three novel functions to model Gaussian probabilities,
colors and transforms. Through the novel disentanglement of 3DGS, we represent
the discrete and unstructured 3DGS with continuous Gaussian Splatting
functions, where we then train a latent diffusion model with the target of
generating these Gaussian Splatting functions both unconditionally and
conditionally. Meanwhile, we introduce a discretization algorithm to extract
Gaussians at arbitrary numbers from the generated functions via octree-guided
sampling and optimization. We explore DiffGS for various tasks, including
unconditional generation, conditional generation from text, image, and partial
3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides
a new direction for flexibly modeling and generating Gaussian Splatting.

Comments:
- Accepted by NeurIPS 2024. Project page:
  https://junshengzhou.github.io/DiffGS

---

## Robotic Learning in your Backyard: A Neural Simulator from Open Source  Components

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-25 | Liyou Zhou, Oleg Sinavski, Athanasios Polydoros | cs.RO | [PDF](http://arxiv.org/pdf/2410.19564v1){: .btn .btn-green } |

**Abstract**: The emergence of 3D Gaussian Splatting for fast and high-quality novel view
synthesize has opened up the possibility to construct photo-realistic
simulations from video for robotic reinforcement learning. While the approach
has been demonstrated in several research papers, the software tools used to
build such a simulator remain unavailable or proprietary. We present SplatGym,
an open source neural simulator for training data-driven robotic control
policies. The simulator creates a photorealistic virtual environment from a
single video. It supports ego camera view generation, collision detection, and
virtual object in-painting. We demonstrate training several visual navigation
policies via reinforcement learning. SplatGym represents a notable first step
towards an open-source general-purpose neural environment for robotic learning.
It broadens the range of applications that can effectively utilise
reinforcement learning by providing convenient and unrestricted tooling, and by
eliminating the need for the manual development of conventional 3D
environments.

Comments:
- Accepted for Oral Presentation at IEEE International Conference on
  Robotic Computing (IRC)

---

## Content-Aware Radiance Fields: Aligning Model Complexity with Scene  Intricacy Through Learned Bitwidth Quantization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-25 | Weihang Liu, Xue Xian Zheng, Jingyi Yu, Xin Lou | cs.CV | [PDF](http://arxiv.org/pdf/2410.19483v1){: .btn .btn-green } |

**Abstract**: The recent popular radiance field models, exemplified by Neural Radiance
Fields (NeRF), Instant-NGP and 3D Gaussian Splat?ting, are designed to
represent 3D content by that training models for each individual scene. This
unique characteristic of scene representation and per-scene training
distinguishes radiance field models from other neural models, because complex
scenes necessitate models with higher representational capacity and vice versa.
In this paper, we propose content?aware radiance fields, aligning the model
complexity with the scene intricacies through Adversarial Content-Aware
Quantization (A-CAQ). Specifically, we make the bitwidth of parameters
differentiable and train?able, tailored to the unique characteristics of
specific scenes and requirements. The proposed framework has been assessed on
Instant-NGP, a well-known NeRF variant and evaluated using various datasets.
Experimental results demonstrate a notable reduction in computational
complexity, while preserving the requisite reconstruction and rendering
quality, making it beneficial for practical deployment of radiance fields
models. Codes are available at
https://github.com/WeihangLiu2024/Content_Aware_NeRF.

Comments:
- accepted by ECCV2024

---

## Evaluation of strategies for efficient rate-distortion NeRF streaming

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-25 | Pedro Martin, António Rodrigues, João Ascenso, Maria Paula Queluz | cs.MM | [PDF](http://arxiv.org/pdf/2410.19459v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have revolutionized the field of 3D visual
representation by enabling highly realistic and detailed scene reconstructions
from a sparse set of images. NeRF uses a volumetric functional representation
that maps 3D points to their corresponding colors and opacities, allowing for
photorealistic view synthesis from arbitrary viewpoints. Despite its
advancements, the efficient streaming of NeRF content remains a significant
challenge due to the large amount of data involved. This paper investigates the
rate-distortion performance of two NeRF streaming strategies: pixel-based and
neural network (NN) parameter-based streaming. While in the former, images are
coded and then transmitted throughout the network, in the latter, the
respective NeRF model parameters are coded and transmitted instead. This work
also highlights the trade-offs in complexity and performance, demonstrating
that the NN parameter-based strategy generally offers superior efficiency,
making it suitable for one-to-many streaming scenarios.



---

## Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-24 | Mingtong Zhang, Kaifeng Zhang, Yunzhu Li | cs.RO | [PDF](http://arxiv.org/pdf/2410.18912v1){: .btn .btn-green } |

**Abstract**: Videos of robots interacting with objects encode rich information about the
objects' dynamics. However, existing video prediction approaches typically do
not explicitly account for the 3D information from videos, such as robot
actions and objects' 3D states, limiting their use in real-world robotic
applications. In this work, we introduce a framework to learn object dynamics
directly from multi-view RGB videos by explicitly considering the robot's
action trajectories and their effects on scene dynamics. We utilize the 3D
Gaussian representation of 3D Gaussian Splatting (3DGS) to train a
particle-based dynamics model using Graph Neural Networks. This model operates
on sparse control particles downsampled from the densely tracked 3D Gaussian
reconstructions. By learning the neural dynamics model on offline robot
interaction data, our method can predict object motions under varying initial
configurations and unseen robot actions. The 3D transformations of Gaussians
can be interpolated from the motions of control particles, enabling the
rendering of predicted future object states and achieving action-conditioned
video prediction. The dynamics model can also be applied to model-based
planning frameworks for object manipulation tasks. We conduct experiments on
various kinds of deformable materials, including ropes, clothes, and stuffed
animals, demonstrating our framework's ability to model complex shapes and
dynamics. Our project page is available at https://gs-dynamics.github.io.

Comments:
- Project Page: https://gs-dynamics.github.io

---

## Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse  View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-24 | Liang Han, Junsheng Zhou, Yu-Shen Liu, Zhizhong Han | cs.CV | [PDF](http://arxiv.org/pdf/2410.18822v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from sparse inputs is a vital yet challenging task in 3D
computer vision. Previous methods explore 3D Gaussian Splatting with neural
priors (e.g. depth priors) as an additional supervision, demonstrating
promising quality and efficiency compared to the NeRF based methods. However,
the neural priors from 2D pretrained models are often noisy and blurry, which
struggle to precisely guide the learning of radiance fields. In this paper, We
propose a novel method for synthesizing novel views from sparse views with
Gaussian Splatting that does not require external prior as supervision. Our key
idea lies in exploring the self-supervisions inherent in the binocular stereo
consistency between each pair of binocular images constructed with
disparity-guided image warping. To this end, we additionally introduce a
Gaussian opacity constraint which regularizes the Gaussian locations and avoids
Gaussian redundancy for improving the robustness and efficiency of inferring 3D
Gaussians from sparse views. Extensive experiments on the LLFF, DTU, and
Blender datasets demonstrate that our method significantly outperforms the
state-of-the-art methods.

Comments:
- Accepted by NeurIPS 2024. Project page:
  https://hanl2010.github.io/Binocular3DGS/

---

## 3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-24 | Hansheng Chen, Bokui Shen, Yulin Liu, Ruoxi Shi, Linqi Zhou, Connor Z. Lin, Jiayuan Gu, Hao Su, Gordon Wetzstein, Leonidas Guibas | cs.CV | [PDF](http://arxiv.org/pdf/2410.18974v1){: .btn .btn-green } |

**Abstract**: Multi-view image diffusion models have significantly advanced open-domain 3D
object generation. However, most existing models rely on 2D network
architectures that lack inherent 3D biases, resulting in compromised geometric
consistency. To address this challenge, we introduce 3D-Adapter, a plug-in
module designed to infuse 3D geometry awareness into pretrained image diffusion
models. Central to our approach is the idea of 3D feedback augmentation: for
each denoising step in the sampling loop, 3D-Adapter decodes intermediate
multi-view features into a coherent 3D representation, then re-encodes the
rendered RGBD views to augment the pretrained base model through feature
addition. We study two variants of 3D-Adapter: a fast feed-forward version
based on Gaussian splatting and a versatile training-free version utilizing
neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter
not only greatly enhances the geometry quality of text-to-multi-view models
such as Instant3D and Zero123++, but also enables high-quality 3D generation
using the plain text-to-image Stable Diffusion. Furthermore, we showcase the
broad application potential of 3D-Adapter by presenting high quality results in
text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.

Comments:
- Project page: https://lakonik.github.io/3d-adapter/

---

## Sort-free Gaussian Splatting via Weighted Sum Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-24 | Qiqi Hou, Randall Rauwendaal, Zifeng Li, Hoang Le, Farzad Farhadzadeh, Fatih Porikli, Alexei Bourd, Amir Said | cs.CV | [PDF](http://arxiv.org/pdf/2410.18931v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant
advancement in 3D scene reconstruction, attracting considerable attention due
to its ability to recover high-fidelity details while maintaining low
complexity. Despite the promising results achieved by 3DGS, its rendering
performance is constrained by its dependence on costly non-commutative
alpha-blending operations. These operations mandate complex view dependent
sorting operations that introduce computational overhead, especially on the
resource-constrained platforms such as mobile phones. In this paper, we propose
Weighted Sum Rendering, which approximates alpha blending with weighted sums,
thereby removing the need for sorting. This simplifies implementation, delivers
superior performance, and eliminates the "popping" artifacts caused by sorting.
Experimental results show that optimizing a generalized Gaussian splatting
formulation to the new differentiable rendering yields competitive image
quality. The method was implemented and tested in a mobile device GPU,
achieving on average $1.23\times$ faster rendering.



---

## Real-time 3D-aware Portrait Video Relighting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-24 | Ziqi Cai, Kaiwen Jiang, Shu-Yu Chen, Yu-Kun Lai, Hongbo Fu, Boxin Shi, Lin Gao | cs.CV | [PDF](http://arxiv.org/pdf/2410.18355v1){: .btn .btn-green } |

**Abstract**: Synthesizing realistic videos of talking faces under custom lighting
conditions and viewing angles benefits various downstream applications like
video conferencing. However, most existing relighting methods are either
time-consuming or unable to adjust the viewpoints. In this paper, we present
the first real-time 3D-aware method for relighting in-the-wild videos of
talking faces based on Neural Radiance Fields (NeRF). Given an input portrait
video, our method can synthesize talking faces under both novel views and novel
lighting conditions with a photo-realistic and disentangled 3D representation.
Specifically, we infer an albedo tri-plane, as well as a shading tri-plane
based on a desired lighting condition for each video frame with fast
dual-encoders. We also leverage a temporal consistency network to ensure smooth
transitions and reduce flickering artifacts. Our method runs at 32.98 fps on
consumer-level hardware and achieves state-of-the-art results in terms of
reconstruction quality, lighting error, lighting instability, temporal
consistency and inference speed. We demonstrate the effectiveness and
interactivity of our method on various portrait videos with diverse lighting
and viewing conditions.

Comments:
- Accepted to CVPR 2024 (Highlight). Project page:
  http://geometrylearning.com/VideoRelighting

---

## VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian  Splatting and Neural Points

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-23 | Linus Franke, Laura Fink, Marc Stamminger | cs.CV | [PDF](http://arxiv.org/pdf/2410.17932v1){: .btn .btn-green } |

**Abstract**: Recent advances in novel view synthesis (NVS), particularly neural radiance
fields (NeRF) and Gaussian splatting (3DGS), have demonstrated impressive
results in photorealistic scene rendering. These techniques hold great
potential for applications in virtual tourism and teleportation, where
immersive realism is crucial. However, the high-performance demands of virtual
reality (VR) systems present challenges in directly utilizing even such
fast-to-render scene representations like 3DGS due to latency and computational
constraints.
  In this paper, we propose foveated rendering as a promising solution to these
obstacles. We analyze state-of-the-art NVS methods with respect to their
rendering performance and compatibility with the human visual system. Our
approach introduces a novel foveated rendering approach for Virtual Reality,
that leverages the sharp, detailed output of neural point rendering for the
foveal region, fused with a smooth rendering of 3DGS for the peripheral vision.
  Our evaluation confirms that perceived sharpness and detail-richness are
increased by our approach compared to a standard VR-ready 3DGS configuration.
Our system meets the necessary performance requirements for real-time VR
interactions, ultimately enhancing the user's immersive experience.
  Project page: https://lfranke.github.io/vr_splatting



---

## PLGS: Robust Panoptic Lifting with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-23 | Yu Wang, Xiaobao Wei, Ming Lu, Guoliang Kang | cs.CV | [PDF](http://arxiv.org/pdf/2410.17505v1){: .btn .btn-green } |

**Abstract**: Previous methods utilize the Neural Radiance Field (NeRF) for panoptic
lifting, while their training and rendering speed are unsatisfactory. In
contrast, 3D Gaussian Splatting (3DGS) has emerged as a prominent technique due
to its rapid training and rendering speed. However, unlike NeRF, the
conventional 3DGS may not satisfy the basic smoothness assumption as it does
not rely on any parameterized structures to render (e.g., MLPs). Consequently,
the conventional 3DGS is, in nature, more susceptible to noisy 2D mask
supervision. In this paper, we propose a new method called PLGS that enables
3DGS to generate consistent panoptic segmentation masks from noisy 2D
segmentation masks while maintaining superior efficiency compared to NeRF-based
methods. Specifically, we build a panoptic-aware structured 3D Gaussian model
to introduce smoothness and design effective noise reduction strategies. For
the semantic field, instead of initialization with structure from motion, we
construct reliable semantic anchor points to initialize the 3D Gaussians. We
then use these anchor points as smooth regularization during training.
Additionally, we present a self-training approach using pseudo labels generated
by merging the rendered masks with the noisy masks to enhance the robustness of
PLGS. For the instance field, we project the 2D instance masks into 3D space
and match them with oriented bounding boxes to generate cross-view consistent
instance masks for supervision. Experiments on various benchmarks demonstrate
that our method outperforms previous state-of-the-art methods in terms of both
segmentation quality and speed.



---

## Few-shot NeRF by Adaptive Rendering Loss Regularization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-23 | Qingshan Xu, Xuanyu Yi, Jianyao Xu, Wenbing Tao, Yew-Soon Ong, Hanwang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2410.17839v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis with sparse inputs poses great challenges to Neural
Radiance Field (NeRF). Recent works demonstrate that the frequency
regularization of Positional Encoding (PE) can achieve promising results for
few-shot NeRF. In this work, we reveal that there exists an inconsistency
between the frequency regularization of PE and rendering loss. This prevents
few-shot NeRF from synthesizing higher-quality novel views. To mitigate this
inconsistency, we propose Adaptive Rendering loss regularization for few-shot
NeRF, dubbed AR-NeRF. Specifically, we present a two-phase rendering
supervision and an adaptive rendering loss weight learning strategy to align
the frequency relationship between PE and 2D-pixel supervision. In this way,
AR-NeRF can learn global structures better in the early training phase and
adaptively learn local details throughout the training process. Extensive
experiments show that our AR-NeRF achieves state-of-the-art performance on
different datasets, including object-level and complex scenes.

Comments:
- Accepted by ECCV2024

---

## Efficient Neural Implicit Representation for 3D Human Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-23 | Zexu Huang, Sarah Monazam Erfani, Siying Lu, Mingming Gong | cs.CV | [PDF](http://arxiv.org/pdf/2410.17741v1){: .btn .btn-green } |

**Abstract**: High-fidelity digital human representations are increasingly in demand in the
digital world, particularly for interactive telepresence, AR/VR, 3D graphics,
and the rapidly evolving metaverse. Even though they work well in small spaces,
conventional methods for reconstructing 3D human motion frequently require the
use of expensive hardware and have high processing costs. This study presents
HumanAvatar, an innovative approach that efficiently reconstructs precise human
avatars from monocular video sources. At the core of our methodology, we
integrate the pre-trained HuMoR, a model celebrated for its proficiency in
human motion estimation. This is adeptly fused with the cutting-edge neural
radiance field technology, Instant-NGP, and the state-of-the-art articulated
model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By
combining these two technologies, a system is created that can render quickly
and effectively while also providing estimation of human pose parameters that
are unmatched in accuracy. We have enhanced our system with an advanced
posture-sensitive space reduction technique, which optimally balances rendering
quality with computational efficiency. In our detailed experimental analysis
using both artificial and real-world monocular videos, we establish the
advanced performance of our approach. HumanAvatar consistently equals or
surpasses contemporary leading-edge reconstruction techniques in quality.
Furthermore, it achieves these complex reconstructions in minutes, a fraction
of the time typically required by existing methods. Our models achieve a
training speed that is 110X faster than that of State-of-The-Art (SoTA)
NeRF-based models. Our technique performs noticeably better than SoTA dynamic
human NeRF methods if given an identical runtime limit. HumanAvatar can provide
effective visuals after only 30 seconds of training.



---

## E-3DGS: Gaussian Splatting with Exposure and Motion Events

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-22 | Xiaoting Yin, Hao Shi, Yuhan Bao, Zhenshan Bing, Yiyi Liao, Kailun Yang, Kaiwei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2410.16995v1){: .btn .btn-green } |

**Abstract**: Estimating Neural Radiance Fields (NeRFs) from images captured under optimal
conditions has been extensively explored in the vision community. However,
robotic applications often face challenges such as motion blur, insufficient
illumination, and high computational overhead, which adversely affect
downstream tasks like navigation, inspection, and scene visualization. To
address these challenges, we propose E-3DGS, a novel event-based approach that
partitions events into motion (from camera or object movement) and exposure
(from camera exposure), using the former to handle fast-motion scenes and using
the latter to reconstruct grayscale images for high-quality training and
optimization of event-based 3D Gaussian Splatting (3DGS). We introduce a novel
integration of 3DGS with exposure events for high-quality reconstruction of
explicit scene representations. Our versatile framework can operate on motion
events alone for 3D reconstruction, enhance quality using exposure events, or
adopt a hybrid mode that balances quality and effectiveness by optimizing with
initial exposure events followed by high-speed motion events. We also introduce
EME-3D, a real-world 3D dataset with exposure events, motion events, camera
calibration parameters, and sparse point clouds. Our method is faster and
delivers better reconstruction quality than event-based NeRF while being more
cost-effective than NeRF methods that combine event and RGB data by using a
single event sensor. By combining motion and exposure events, E-3DGS sets a new
benchmark for event-based 3D reconstruction with robust performance in
challenging conditions and lower hardware demands. The source code and dataset
will be available at https://github.com/MasterHow/E-3DGS.

Comments:
- The source code and dataset will be available at
  https://github.com/MasterHow/E-3DGS

---

## Advancing Super-Resolution in Neural Radiance Fields via Variational  Diffusion Strategies

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-22 | Shrey Vishen, Jatin Sarabu, Chinmay Bharathulwar, Rithwick Lakshmanan, Vishnu Srinivas | cs.CV | [PDF](http://arxiv.org/pdf/2410.18137v1){: .btn .btn-green } |

**Abstract**: We present a novel method for diffusion-guided frameworks for view-consistent
super-resolution (SR) in neural rendering. Our approach leverages existing 2D
SR models in conjunction with advanced techniques such as Variational Score
Distilling (VSD) and a LoRA fine-tuning helper, with spatial training to
significantly boost the quality and consistency of upscaled 2D images compared
to the previous methods in the literature, such as Renoised Score Distillation
(RSD) proposed in DiSR-NeRF (1), or SDS proposed in DreamFusion. The VSD score
facilitates precise fine-tuning of SR models, resulting in high-quality,
view-consistent images. To address the common challenge of inconsistencies
among independent SR 2D images, we integrate Iterative 3D Synchronization
(I3DS) from the DiSR-NeRF framework. Our quantitative benchmarks and
qualitative results on the LLFF dataset demonstrate the superior performance of
our system compared to existing methods such as DiSR-NeRF.

Comments:
- All our code is available at
  https://github.com/shreyvish5678/Advancing-Super-Resolution-in-Neural-Radiance-Fields-via-Variational-Diffusion-Strategies

---

## AG-SLAM: Active Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-22 | Wen Jiang, Boshu Lei, Katrina Ashton, Kostas Daniilidis | cs.RO | [PDF](http://arxiv.org/pdf/2410.17422v1){: .btn .btn-green } |

**Abstract**: We present AG-SLAM, the first active SLAM system utilizing 3D Gaussian
Splatting (3DGS) for online scene reconstruction. In recent years, radiance
field scene representations, including 3DGS have been widely used in SLAM and
exploration, but actively planning trajectories for robotic exploration is
still unvisited. In particular, many exploration methods assume precise
localization and thus do not mitigate the significant risk of constructing a
trajectory, which is difficult for a SLAM system to operate on. This can cause
camera tracking failure and lead to failures in real-world robotic
applications. Our method leverages Fisher Information to balance the dual
objectives of maximizing the information gain for the environment while
minimizing the cost of localization errors. Experiments conducted on the Gibson
and Habitat-Matterport 3D datasets demonstrate state-of-the-art results of the
proposed method.



---

## SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-22 | Cheng-De Fan, Chen-Wei Chang, Yi-Ruei Liu, Jie-Ying Lee, Jiun-Long Huang, Yu-Chee Tseng, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2410.17249v1){: .btn .btn-green } |

**Abstract**: We present SpectroMotion, a novel approach that combines 3D Gaussian
Splatting (3DGS) with physically-based rendering (PBR) and deformation fields
to reconstruct dynamic specular scenes. Previous methods extending 3DGS to
model dynamic scenes have struggled to accurately represent specular surfaces.
Our method addresses this limitation by introducing a residual correction
technique for accurate surface normal computation during deformation,
complemented by a deformable environment map that adapts to time-varying
lighting conditions. We implement a coarse-to-fine training strategy that
significantly enhances both scene geometry and specular color prediction. We
demonstrate that our model outperforms prior methods for view synthesis of
scenes containing dynamic specular objects and that it is the only existing
3DGS method capable of synthesizing photorealistic real-world dynamic specular
scenes, outperforming state-of-the-art methods in rendering complex, dynamic,
and specular scenes.

Comments:
- Project page: https://cdfan0627.github.io/spectromotion/

---

## Multi-Layer Gaussian Splatting for Immersive Anatomy Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-22 | Constantin Kleinbeck, Hannah Schieber, Klaus Engel, Ralf Gutjahr, Daniel Roth | cs.GR | [PDF](http://arxiv.org/pdf/2410.16978v1){: .btn .btn-green } |

**Abstract**: In medical image visualization, path tracing of volumetric medical data like
CT scans produces lifelike three-dimensional visualizations. Immersive VR
displays can further enhance the understanding of complex anatomies. Going
beyond the diagnostic quality of traditional 2D slices, they enable interactive
3D evaluation of anatomies, supporting medical education and planning.
Rendering high-quality visualizations in real-time, however, is computationally
intensive and impractical for compute-constrained devices like mobile headsets.
  We propose a novel approach utilizing GS to create an efficient but static
intermediate representation of CT scans. We introduce a layered GS
representation, incrementally including different anatomical structures while
minimizing overlap and extending the GS training to remove inactive Gaussians.
We further compress the created model with clustering across layers.
  Our approach achieves interactive frame rates while preserving anatomical
structures, with quality adjustable to the target hardware. Compared to
standard GS, our representation retains some of the explorative qualities
initially enabled by immersive path tracing. Selective activation and clipping
of layers are possible at rendering time, adding a degree of interactivity to
otherwise static GS models. This could enable scenarios where high
computational demands would otherwise prohibit using path-traced medical
volumes.



---

## LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-22 | Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, Zexiang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2410.17242v1){: .btn .btn-green } |

**Abstract**: We propose the Large View Synthesis Model (LVSM), a novel transformer-based
approach for scalable and generalizable novel view synthesis from sparse-view
inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which
encodes input image tokens into a fixed number of 1D latent tokens, functioning
as a fully learned scene representation, and decodes novel-view images from
them; and (2) a decoder-only LVSM, which directly maps input images to
novel-view outputs, completely eliminating intermediate scene representations.
Both models bypass the 3D inductive biases used in previous methods -- from 3D
representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar
projections, plane sweeps) -- addressing novel view synthesis with a fully
data-driven approach. While the encoder-decoder model offers faster inference
due to its independent latent representation, the decoder-only LVSM achieves
superior quality, scalability, and zero-shot generalization, outperforming
previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive
evaluations across multiple datasets demonstrate that both LVSM variants
achieve state-of-the-art novel view synthesis quality. Notably, our models
surpass all previous methods even with reduced computational resources (1-2
GPUs). Please see our website for more details:
https://haian-jin.github.io/projects/LVSM/ .

Comments:
- project page: https://haian-jin.github.io/projects/LVSM/

---

## 3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with  View-consistent 2D Diffusion Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-21 | Xi Liu, Chaoyi Zhou, Siyu Huang | cs.CV | [PDF](http://arxiv.org/pdf/2410.16266v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis aims to generate novel views of a scene from multiple
input images or videos, and recent advancements like 3D Gaussian splatting
(3DGS) have achieved notable success in producing photorealistic renderings
with efficient pipelines. However, generating high-quality novel views under
challenging settings, such as sparse input views, remains difficult due to
insufficient information in under-sampled areas, often resulting in noticeable
artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing
the representation quality of 3DGS representations. We leverage 2D video
diffusion priors to address the challenging 3D view consistency problem,
reformulating it as achieving temporal consistency within a video generation
process. 3DGS-Enhancer restores view-consistent latent features of rendered
novel views and integrates them with the input views through a spatial-temporal
decoder. The enhanced views are then used to fine-tune the initial 3DGS model,
significantly improving its rendering performance. Extensive experiments on
large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields
superior reconstruction performance and high-fidelity rendering results
compared to state-of-the-art methods. The project webpage is
https://xiliu8006.github.io/3DGS-Enhancer-project .

Comments:
- Accepted by NeurIPS 2024 Spotlight

---

## Joker: Conditional 3D Head Synthesis with Extreme Facial Expressions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-21 | Malte Prinzler, Egor Zakharov, Vanessa Sklyarova, Berna Kabadayi, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2410.16395v1){: .btn .btn-green } |

**Abstract**: We introduce Joker, a new method for the conditional synthesis of 3D human
heads with extreme expressions. Given a single reference image of a person, we
synthesize a volumetric human head with the reference identity and a new
expression. We offer control over the expression via a 3D morphable model
(3DMM) and textual inputs. This multi-modal conditioning signal is essential
since 3DMMs alone fail to define subtle emotional changes and extreme
expressions, including those involving the mouth cavity and tongue
articulation. Our method is built upon a 2D diffusion-based prior that
generalizes well to out-of-domain samples, such as sculptures, heavy makeup,
and paintings while achieving high levels of expressiveness. To improve view
consistency, we propose a new 3D distillation technique that converts
predictions of our 2D prior into a neural radiance field (NeRF). Both the 2D
prior and our distillation technique produce state-of-the-art results, which
are confirmed by our extensive evaluations. Also, to the best of our knowledge,
our method is the first to achieve view-consistent extreme tongue articulation.

Comments:
- Project Page: https://malteprinzler.github.io/projects/joker/

---

## FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without  Learned Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-21 | Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2410.16271v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) face significant challenges in few-shot
scenarios, primarily due to overfitting and long training times for
high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use
frequency regularization or pre-trained priors but struggle with complex
scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework
that leverages weight-sharing voxels across multiple scales to efficiently
represent scene details. Our key contribution is a cross-scale geometric
adaptation scheme that selects pseudo ground truth depth based on reprojection
errors across scales. This guides training without relying on externally
learned priors, enabling full utilization of the training data. It can also
integrate pre-trained priors, enhancing quality without slowing convergence.
Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms
other few-shot NeRF methods while significantly reducing training time, making
it a practical solution for efficient and accurate 3D scene reconstruction.

Comments:
- Project page: https://linjohnss.github.io/frugalnerf/

---

## MSGField: A Unified Scene Representation Integrating Motion, Semantics,  and Geometry for Robotic Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-21 | Yu Sheng, Runfeng Lin, Lidian Wang, Quecheng Qiu, YanYong Zhang, Yu Zhang, Bei Hua, Jianmin Ji | cs.RO | [PDF](http://arxiv.org/pdf/2410.15730v1){: .btn .btn-green } |

**Abstract**: Combining accurate geometry with rich semantics has been proven to be highly
effective for language-guided robotic manipulation. Existing methods for
dynamic scenes either fail to update in real-time or rely on additional depth
sensors for simple scene editing, limiting their applicability in real-world.
In this paper, we introduce MSGField, a representation that uses a collection
of 2D Gaussians for high-quality reconstruction, further enhanced with
attributes to encode semantic and motion information. Specially, we represent
the motion field compactly by decomposing each primitive's motion into a
combination of a limited set of motion bases. Leveraging the differentiable
real-time rendering of Gaussian splatting, we can quickly optimize object
motion, even for complex non-rigid motions, with image supervision from only
two camera views. Additionally, we designed a pipeline that utilizes object
priors to efficiently obtain well-defined semantics. In our challenging
dataset, which includes flexible and extremely small objects, our method
achieve a success rate of 79.2% in static and 63.3% in dynamic environments for
language-guided manipulation. For specified object grasping, we achieve a
success rate of 90%, on par with point cloud-based methods. Code and dataset
will be released at:https://shengyu724.github.io/MSGField.github.io.



---

## Fully Explicit Dynamic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-21 | Junoh Lee, Chang-Yeon Won, Hyunjun Jung, Inhwan Bae, Hae-Gon Jeon | cs.CV | [PDF](http://arxiv.org/pdf/2410.15629v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has shown fast and high-quality rendering results in
static scenes by leveraging dense 3D prior and explicit representations.
Unfortunately, the benefits of the prior and representation do not involve
novel view synthesis for dynamic motions. Ironically, this is because the main
barrier is the reliance on them, which requires increasing training and
rendering times to account for dynamic motions. In this paper, we design a
Explicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate
static and dynamic Gaussians during training, and to explicitly sample
positions and rotations of the dynamic Gaussians at sparse timestamps. The
sampled positions and rotations are then interpolated to represent both
spatially and temporally continuous motions of objects in dynamic scenes as
well as reducing computational cost. Additionally, we introduce a progressive
training scheme and a point-backtracking technique that improves Ex4DGS's
convergence. We initially train Ex4DGS using short timestamps and progressively
extend timestamps, which makes it work well with a few point clouds. The
point-backtracking is used to quantify the cumulative error of each Gaussian
over time, enabling the detection and removal of erroneous Gaussians in dynamic
scenes. Comprehensive experiments on various scenes demonstrate the
state-of-the-art rendering quality from our method, achieving fast rendering of
62 fps on a single 2080Ti GPU.

Comments:
- Accepted at NeurIPS 2024

---

## EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-20 | Bohao Liao, Wei Zhai, Zengyu Wan, Tianzhu Zhang, Yang Cao, Zheng-Jun Zha | cs.CV | [PDF](http://arxiv.org/pdf/2410.15392v2){: .btn .btn-green } |

**Abstract**: Scene reconstruction from casually captured videos has wide applications in
real-world scenarios. With recent advancements in differentiable rendering
techniques, several methods have attempted to simultaneously optimize scene
representations (NeRF or 3DGS) and camera poses. Despite recent progress,
existing methods relying on traditional camera input tend to fail in high-speed
(or equivalently low-frame-rate) scenarios. Event cameras, inspired by
biological vision, record pixel-wise intensity changes asynchronously with high
temporal resolution, providing valuable scene and motion information in blind
inter-frame intervals. In this paper, we introduce the event camera to aid
scene construction from a casually captured video for the first time, and
propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly
integrates the advantages of event cameras into 3DGS through three key
components. First, we leverage the Event Generation Model (EGM) to fuse events
and frames, supervising the rendered views observed by the event stream.
Second, we adopt the Contrast Maximization (CMax) framework in a piece-wise
manner to extract motion information by maximizing the contrast of the Image of
Warped Events (IWE), thereby calibrating the estimated poses. Besides, based on
the Linear Event Generation Model (LEGM), the brightness information encoded in
the IWE is also utilized to constrain the 3DGS in the gradient domain. Third,
to mitigate the absence of color information of events, we introduce
photometric bundle adjustment (PBA) to ensure view consistency across events
and frames. We evaluate our method on the public Tanks and Temples benchmark
and a newly collected real-world dataset, RealEv-DAVIS. Our project page is
https://lbh666.github.io/ef-3dgs/.

Comments:
- Project Page: https://lbh666.github.io/ef-3dgs/

---

## Neural Radiance Field Image Refinement through End-to-End Sampling Point  Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-19 | Kazuhiro Ohta, Satoshi Ono | cs.CV | [PDF](http://arxiv.org/pdf/2410.14958v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF), capable of synthesizing high-quality novel
viewpoint images, suffers from issues like artifact occurrence due to its fixed
sampling points during rendering. This study proposes a method that optimizes
sampling points to reduce artifacts and produce more detailed images.



---

## LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian  Splatting scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-18 | Juliette Marrie, Romain Ménégaux, Michael Arbel, Diane Larlus, Julien Mairal | cs.CV | [PDF](http://arxiv.org/pdf/2410.14462v1){: .btn .btn-green } |

**Abstract**: We address the task of uplifting visual features or semantic masks from 2D
vision models to 3D scenes represented by Gaussian Splatting. Whereas common
approaches rely on iterative optimization-based procedures, we show that a
simple yet effective aggregation technique yields excellent results. Applied to
semantic masks from Segment Anything (SAM), our uplifting approach leads to
segmentation quality comparable to the state of the art. We then extend this
method to generic DINOv2 features, integrating 3D scene geometry through graph
diffusion, and achieve competitive segmentation results despite DINOv2 not
being trained on millions of annotated masks like SAM.



---

## Learning autonomous driving from aerial imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-18 | Varun Murali, Guy Rosman, Sertac Karaman, Daniela Rus | cs.RO | [PDF](http://arxiv.org/pdf/2410.14177v1){: .btn .btn-green } |

**Abstract**: In this work, we consider the problem of learning end to end perception to
control for ground vehicles solely from aerial imagery. Photogrammetric
simulators allow the synthesis of novel views through the transformation of
pre-generated assets into novel views.However, they have a large setup cost,
require careful collection of data and often human effort to create usable
simulators. We use a Neural Radiance Field (NeRF) as an intermediate
representation to synthesize novel views from the point of view of a ground
vehicle. These novel viewpoints can then be used for several downstream
autonomous navigation applications. In this work, we demonstrate the utility of
novel view synthesis though the application of training a policy for end to end
learning from images and depth data. In a traditional real to sim to real
framework, the collected data would be transformed into a visual simulator
which could then be used to generate novel views. In contrast, using a NeRF
allows a compact representation and the ability to optimize over the parameters
of the visual simulator as more data is gathered in the environment. We
demonstrate the efficacy of our method in a custom built mini-city environment
through the deployment of imitation policies on robotic cars. We additionally
consider the task of place localization and demonstrate that our method is able
to relocalize the car in the real world.

Comments:
- Presented at IROS 2024

---

## DaRePlane: Direction-aware Representations for Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-18 | Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Meng Zheng, Terrence Chen, Ziyan Wu, Jack Noble | cs.CV | [PDF](http://arxiv.org/pdf/2410.14169v1){: .btn .btn-green } |

**Abstract**: Numerous recent approaches to modeling and re-rendering dynamic scenes
leverage plane-based explicit representations, addressing slow training times
associated with models like neural radiance fields (NeRF) and Gaussian
splatting (GS). However, merely decomposing 4D dynamic scenes into multiple 2D
plane-based representations is insufficient for high-fidelity re-rendering of
scenes with complex motions. In response, we present DaRePlane, a novel
direction-aware representation approach that captures scene dynamics from six
different directions. This learned representation undergoes an inverse
dual-tree complex wavelet transformation (DTCWT) to recover plane-based
information. Within NeRF pipelines, DaRePlane computes features for each
space-time point by fusing vectors from these recovered planes, then passed to
a tiny MLP for color regression. When applied to Gaussian splatting, DaRePlane
computes the features of Gaussian points, followed by a tiny multi-head MLP for
spatial-time deformation prediction. Notably, to address redundancy introduced
by the six real and six imaginary direction-aware wavelet coefficients, we
introduce a trainable masking approach, mitigating storage issues without
significant performance decline. To demonstrate the generality and efficiency
of DaRePlane, we test it on both regular and surgical dynamic scenes, for both
NeRF and GS systems. Extensive experiments show that DaRePlane yields
state-of-the-art performance in novel view synthesis for various complex
dynamic scenes.

Comments:
- arXiv admin note: substantial text overlap with arXiv:2403.02265

---

## Neural Signed Distance Function Inference through Splatting 3D Gaussians  Pulled on Zero-Level Set

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-18 | Wenyuan Zhang, Yu-Shen Liu, Zhizhong Han | cs.CV | [PDF](http://arxiv.org/pdf/2410.14189v1){: .btn .btn-green } |

**Abstract**: It is vital to infer a signed distance function (SDF) in multi-view based
surface reconstruction. 3D Gaussian splatting (3DGS) provides a novel
perspective for volume rendering, and shows advantages in rendering efficiency
and quality. Although 3DGS provides a promising neural rendering option, it is
still hard to infer SDFs for surface reconstruction with 3DGS due to the
discreteness, the sparseness, and the off-surface drift of 3D Gaussians. To
resolve these issues, we propose a method that seamlessly merge 3DGS with the
learning of neural SDFs. Our key idea is to more effectively constrain the SDF
inference with the multi-view consistency. To this end, we dynamically align 3D
Gaussians on the zero-level set of the neural SDF using neural pulling, and
then render the aligned 3D Gaussians through the differentiable rasterization.
Meanwhile, we update the neural SDF by pulling neighboring space to the pulled
3D Gaussians, which progressively refine the signed distance field near the
surface. With both differentiable pulling and splatting, we jointly optimize 3D
Gaussians and the neural SDF with both RGB and geometry constraints, which
recovers more accurate, smooth, and complete surfaces with more geometry
details. Our numerical and visual comparisons show our superiority over the
state-of-the-art results on the widely used benchmarks.

Comments:
- Accepted by NeurIPS 2024. Project page:
  https://wen-yuan-zhang.github.io/GS-Pull/

---

## GS-LIVM: Real-Time Photo-Realistic LiDAR-Inertial-Visual Mapping with  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-18 | Yusen Xie, Zhenmin Huang, Jin Wu, Jun Ma | cs.RO | [PDF](http://arxiv.org/pdf/2410.17084v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce GS-LIVM, a real-time photo-realistic
LiDAR-Inertial-Visual mapping framework with Gaussian Splatting tailored for
outdoor scenes. Compared to existing methods based on Neural Radiance Fields
(NeRF) and 3D Gaussian Splatting (3DGS), our approach enables real-time
photo-realistic mapping while ensuring high-quality image rendering in
large-scale unbounded outdoor environments. In this work, Gaussian Process
Regression (GPR) is employed to mitigate the issues resulting from sparse and
unevenly distributed LiDAR observations. The voxel-based 3D Gaussians map
representation facilitates real-time dense mapping in large outdoor
environments with acceleration governed by custom CUDA kernels. Moreover, the
overall framework is designed in a covariance-centered manner, where the
estimated covariance is used to initialize the scale and rotation of 3D
Gaussians, as well as update the parameters of the GPR. We evaluate our
algorithm on several outdoor datasets, and the results demonstrate that our
method achieves state-of-the-art performance in terms of mapping efficiency and
rendering quality. The source code is available on GitHub.

Comments:
- 15 pages, 13 figures

---

## DepthSplat: Connecting Gaussian Splatting and Depth

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-17 | Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, Marc Pollefeys | cs.CV | [PDF](http://arxiv.org/pdf/2410.13862v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting and single/multi-view depth estimation are typically
studied in isolation. In this paper, we present DepthSplat to connect Gaussian
splatting and depth estimation and study their interactions. More specifically,
we first contribute a robust multi-view depth model by leveraging pre-trained
monocular depth features, leading to high-quality feed-forward 3D Gaussian
splatting reconstructions. We also show that Gaussian splatting can serve as an
unsupervised pre-training objective for learning powerful depth models from
large-scale unlabelled datasets. We validate the synergy between Gaussian
splatting and depth estimation through extensive ablation and cross-task
transfer experiments. Our DepthSplat achieves state-of-the-art performance on
ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and
novel view synthesis, demonstrating the mutual benefits of connecting both
tasks. Our code, models, and video results are available at
https://haofeixu.github.io/depthsplat/.

Comments:
- Project page: https://haofeixu.github.io/depthsplat/

---

## MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-17 | Xinjie Zhang, Zhening Liu, Yifan Zhang, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Zehong Lin, Shuicheng Yan, Jun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2410.13613v1){: .btn .btn-green } |

**Abstract**: 4D Gaussian Splatting (4DGS) has recently emerged as a promising technique
for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D
Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering
speeds. Despite its advantages, 4DGS faces significant challenges, notably the
requirement of millions of 4D Gaussians, each with extensive associated
attributes, leading to substantial memory and storage cost. This paper
introduces a memory-efficient framework for 4DGS. We streamline the color
attribute by decomposing it into a per-Gaussian direct color component with
only 3 parameters and a shared lightweight alternating current color predictor.
This approach eliminates the need for spherical harmonics coefficients, which
typically involve up to 144 parameters in classic 4DGS, thereby creating a
memory-efficient 4D Gaussian representation. Furthermore, we introduce an
entropy-constrained Gaussian deformation technique that uses a deformation
field to expand the action range of each Gaussian and integrates an
opacity-based entropy loss to limit the number of Gaussians, thus forcing our
model to use as few Gaussians as possible to fit a dynamic scene well. With
simple half-precision storage and zip compression, our framework achieves a
storage reduction by approximately 190$\times$ and 125$\times$ on the
Technicolor and Neural 3D Video datasets, respectively, compared to the
original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene
representation quality, setting a new standard in the field.



---

## DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation  for Dynamic Scene Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-17 | Jiahao Lu, Jiacheng Deng, Ruijie Zhu, Yanzhe Liang, Wenfei Yang, Tianzhu Zhang, Xu Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2410.13607v1){: .btn .btn-green } |

**Abstract**: Dynamic scenes rendering is an intriguing yet challenging problem. Although
current methods based on NeRF have achieved satisfactory performance, they
still can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS)
has gar?nered researchers attention due to their outstanding rendering quality
and real?time speed. Therefore, a new paradigm has been proposed: defining a
canonical 3D gaussians and deforming it to individual frames in deformable
fields. How?ever, since the coordinates of canonical 3D gaussians are filled
with noise, which can transfer noise into the deformable fields, and there is
currently no method that adequately considers the aggregation of 4D
information. Therefore, we pro?pose Denoised Deformable Network with
Temporal-Spatial Aggregation for Dy?namic Scene Rendering (DN-4DGS).
Specifically, a Noise Suppression Strategy is introduced to change the
distribution of the coordinates of the canonical 3D gaussians and suppress
noise. Additionally, a Decoupled Temporal-Spatial Ag?gregation Module is
designed to aggregate information from adjacent points and frames. Extensive
experiments on various real-world datasets demonstrate that our method achieves
state-of-the-art rendering quality under a real-time level.

Comments:
- Accepted by NeurIPS 2024

---

## DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving  Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-17 | Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, Xingang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2410.13571v2){: .btn .btn-green } |

**Abstract**: Closed-loop simulation is essential for advancing end-to-end autonomous
driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS,
rely predominantly on conditions closely aligned with training data
distributions, which are largely confined to forward-driving scenarios.
Consequently, these methods face limitations when rendering complex maneuvers
(e.g., lane change, acceleration, deceleration). Recent advancements in
autonomous-driving world models have demonstrated the potential to generate
diverse driving videos. However, these approaches remain constrained to 2D
video generation, inherently lacking the spatiotemporal coherence required to
capture intricacies of dynamic driving environments. In this paper, we
introduce DriveDreamer4D, which enhances 4D driving scene representation
leveraging world model priors. Specifically, we utilize the world model as a
data machine to synthesize novel trajectory videos based on real-world driving
data. Notably, we explicitly leverage structured conditions to control the
spatial-temporal consistency of foreground and background elements, thus the
generated data adheres closely to traffic constraints. To our knowledge,
DriveDreamer4D is the first to utilize video generation models for improving 4D
reconstruction in driving scenarios. Experimental results reveal that
DriveDreamer4D significantly enhances generation quality under novel trajectory
views, achieving a relative improvement in FID by 24.5%, 39.0%, and 10.5%
compared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D
markedly enhances the spatiotemporal coherence of driving agents, which is
verified by a comprehensive user study and the relative increases of 20.3%,
42.0%, and 13.7% in the NTA-IoU metric.

Comments:
- Project Page: https://drivedreamer4d.github.io

---

## Object Pose Estimation Using Implicit Representation For Transparent  Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-17 | Varun Burde, Artem Moroz, Vit Zeman, Pavel Burget | cs.CV | [PDF](http://arxiv.org/pdf/2410.13465v1){: .btn .btn-green } |

**Abstract**: Object pose estimation is a prominent task in computer vision. The object
pose gives the orientation and translation of the object in real-world space,
which allows various applications such as manipulation, augmented reality, etc.
Various objects exhibit different properties with light, such as reflections,
absorption, etc. This makes it challenging to understand the object's structure
in RGB and depth channels. Recent research has been moving toward
learning-based methods, which provide a more flexible and generalizable
approach to object pose estimation utilizing deep learning. One such approach
is the render-and-compare method, which renders the object from multiple views
and compares it against the given 2D image, which often requires an object
representation in the form of a CAD model. We reason that the synthetic texture
of the CAD model may not be ideal for rendering and comparing operations. We
showed that if the object is represented as an implicit (neural) representation
in the form of Neural Radiance Field (NeRF), it exhibits a more realistic
rendering of the actual scene and retains the crucial spatial features, which
makes the comparison more versatile. We evaluated our NeRF implementation of
the render-and-compare method on transparent datasets and found that it
surpassed the current state-of-the-art results.



---

## GlossyGS: Inverse Rendering of Glossy Objects with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-17 | Shuichang Lai, Letian Huang, Jie Guo, Kai Cheng, Bowen Pan, Xiaoxiao Long, Jiangjing Lyu, Chengfei Lv, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2410.13349v1){: .btn .btn-green } |

**Abstract**: Reconstructing objects from posed images is a crucial and complex task in
computer graphics and computer vision. While NeRF-based neural reconstruction
methods have exhibited impressive reconstruction ability, they tend to be
time-comsuming. Recent strategies have adopted 3D Gaussian Splatting (3D-GS)
for inverse rendering, which have led to quick and effective outcomes. However,
these techniques generally have difficulty in producing believable geometries
and materials for glossy objects, a challenge that stems from the inherent
ambiguities of inverse rendering. To address this, we introduce GlossyGS, an
innovative 3D-GS-based inverse rendering framework that aims to precisely
reconstruct the geometry and materials of glossy objects by integrating
material priors. The key idea is the use of micro-facet geometry segmentation
prior, which helps to reduce the intrinsic ambiguities and improve the
decomposition of geometries and materials. Additionally, we introduce a normal
map prefiltering strategy to more accurately simulate the normal distribution
of reflective surfaces. These strategies are integrated into a hybrid geometry
and material representation that employs both explicit and implicit methods to
depict glossy objects. We demonstrate through quantitative analysis and
qualitative visualization that the proposed method is effective to reconstruct
high-fidelity geometries and materials of glossy objects, and performs
favorably against state-of-the-arts.



---

## Differentiable Robot Rendering


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-17 | Ruoshi Liu, Alper Canberk, Shuran Song, Carl Vondrick | cs.RO | [PDF](http://arxiv.org/pdf/2410.13851v1){: .btn .btn-green } |

**Abstract**: Vision foundation models trained on massive amounts of visual data have shown
unprecedented reasoning and planning skills in open-world settings. A key
challenge in applying them to robotic tasks is the modality gap between visual
data and action data. We introduce differentiable robot rendering, a method
allowing the visual appearance of a robot body to be directly differentiable
with respect to its control parameters. Our model integrates a kinematics-aware
deformable model and Gaussians Splatting and is compatible with any robot form
factors and degrees of freedom. We demonstrate its capability and usage in
applications including reconstruction of robot poses from images and
controlling robots through vision language models. Quantitative and qualitative
results show that our differentiable rendering model provides effective
gradients for robotic control directly from pixels, setting the foundation for
the future applications of vision foundation models in robotics.

Comments:
- Project Page: https://drrobot.cs.columbia.edu/

---

## 3D Gaussian Splatting in Robotics: A Survey

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-16 | Siting Zhu, Guangming Wang, Dezhi Kong, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2410.12262v1){: .btn .btn-green } |

**Abstract**: Dense 3D representations of the environment have been a long-term goal in the
robotics field. While previous Neural Radiance Fields (NeRF) representation
have been prevalent for its implicit, coordinate-based model, the recent
emergence of 3D Gaussian Splatting (3DGS) has demonstrated remarkable potential
in its explicit radiance field representation. By leveraging 3D Gaussian
primitives for explicit scene representation and enabling differentiable
rendering, 3DGS has shown significant advantages over other radiance fields in
real-time rendering and photo-realistic performance, which is beneficial for
robotic applications. In this survey, we provide a comprehensive understanding
of 3DGS in the field of robotics. We divide our discussion of the related works
into two main categories: the application of 3DGS and the advancements in 3DGS
techniques. In the application section, we explore how 3DGS has been utilized
in various robotics tasks from scene understanding and interaction
perspectives. The advance of 3DGS section focuses on the improvements of 3DGS
own properties in its adaptability and efficiency, aiming to enhance its
performance in robotics. We then summarize the most commonly used datasets and
evaluation metrics in robotics. Finally, we identify the challenges and
limitations of current 3DGS methods and discuss the future development of 3DGS
in robotics.



---

## EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior  for Sparse View

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-16 | Zhaorong Wang, Yoshihiro Kanamori, Yuki Endo | cs.CV | [PDF](http://arxiv.org/pdf/2410.12242v1){: .btn .btn-green } |

**Abstract**: Generalizable neural radiance field (NeRF) enables neural-based digital human
rendering without per-scene retraining. When combined with human prior
knowledge, high-quality human rendering can be achieved even with sparse input
views. However, the inference of these methods is still slow, as a large number
of neural network queries on each ray are required to ensure the rendering
quality. Moreover, occluded regions often suffer from artifacts, especially
when the input views are sparse. To address these issues, we propose a
generalizable human NeRF framework that achieves high-quality and real-time
rendering with sparse input views by extensively leveraging human prior
knowledge. We accelerate the rendering with a two-stage sampling reduction
strategy: first constructing boundary meshes around the human geometry to
reduce the number of ray samples for sampling guidance regression, and then
volume rendering using fewer guided samples. To improve rendering quality,
especially in occluded regions, we propose an occlusion-aware attention
mechanism to extract occlusion information from the human priors, followed by
an image space refinement network to improve rendering quality. Furthermore,
for volume rendering, we adopt a signed ray distance function (SRDF)
formulation, which allows us to propose an SRDF loss at every sample position
to improve the rendering quality further. Our experiments demonstrate that our
method outperforms the state-of-the-art methods in rendering quality and has a
competitive rendering speed compared with speed-prioritized novel view
synthesis methods.

Comments:
- project page: https://github.com/LarsPh/EG-HumanNeRF

---

## Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage  Gaussian Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-16 | Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, Zexiang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2410.12781v1){: .btn .btn-green } |

**Abstract**: We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is
capable of reconstructing a large scene from a long sequence of input images.
Specifically, our model can process 32 source images at 960x540 resolution
within only 1.3 seconds on a single A100 80G GPU. Our architecture features a
mixture of the recent Mamba2 blocks and the classical transformer blocks which
allowed many more tokens to be processed than prior work, enhanced by efficient
token merging and Gaussian pruning steps that balance between quality and
efficiency. Unlike previous feed-forward models that are limited to processing
1~4 input images and can only reconstruct a small portion of a large scene,
Long-LRM reconstructs the entire scene in a single feed-forward step. On
large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method
achieves performance comparable to optimization-based approaches while being
two orders of magnitude more efficient. Project page:
https://arthurhero.github.io/projects/llrm



---

## Scalable Indoor Novel-View Synthesis using Drone-Captured 360 Imagery  with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-15 | Yuanbo Chen, Chengyu Zhang, Jason Wang, Xuefan Gao, Avideh Zakhor | cs.CV | [PDF](http://arxiv.org/pdf/2410.11285v1){: .btn .btn-green } |

**Abstract**: Scene reconstruction and novel-view synthesis for large, complex,
multi-story, indoor scenes is a challenging and time-consuming task. Prior
methods have utilized drones for data capture and radiance fields for scene
reconstruction, both of which present certain challenges. First, in order to
capture diverse viewpoints with the drone's front-facing camera, some
approaches fly the drone in an unstable zig-zag fashion, which hinders
drone-piloting and generates motion blur in the captured data. Secondly, most
radiance field methods do not easily scale to arbitrarily large number of
images. This paper proposes an efficient and scalable pipeline for indoor
novel-view synthesis from drone-captured 360 videos using 3D Gaussian
Splatting. 360 cameras capture a wide set of viewpoints, allowing for
comprehensive scene capture under a simple straightforward drone trajectory. To
scale our method to large scenes, we devise a divide-and-conquer strategy to
automatically split the scene into smaller blocks that can be reconstructed
individually and in parallel. We also propose a coarse-to-fine alignment
strategy to seamlessly match these blocks together to compose the entire scene.
Our experiments demonstrate marked improvement in both reconstruction quality,
i.e. PSNR and SSIM, and computation time compared to prior approaches.

Comments:
- Accepted to ECCV 2024 S3DSGR Workshop

---

## LoGS: Visual Localization via Gaussian Splatting with Fewer Training  Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-15 | Yuzhou Cheng, Jianhao Jiao, Yue Wang, Dimitrios Kanoulas | cs.CV | [PDF](http://arxiv.org/pdf/2410.11505v1){: .btn .btn-green } |

**Abstract**: Visual localization involves estimating a query image's 6-DoF (degrees of
freedom) camera pose, which is a fundamental component in various computer
vision and robotic tasks. This paper presents LoGS, a vision-based localization
pipeline utilizing the 3D Gaussian Splatting (GS) technique as scene
representation. This novel representation allows high-quality novel view
synthesis. During the mapping phase, structure-from-motion (SfM) is applied
first, followed by the generation of a GS map. During localization, the initial
position is obtained through image retrieval, local feature matching coupled
with a PnP solver, and then a high-precision pose is achieved through the
analysis-by-synthesis manner on the GS map. Experimental results on four
large-scale datasets demonstrate the proposed approach's SoTA accuracy in
estimating camera poses and robustness under challenging few-shot conditions.

Comments:
- 8 pages

---

## GS^3: Efficient Relighting with Triple Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-15 | Zoubin Bi, Yixin Zeng, Chong Zeng, Fan Pei, Xiang Feng, Kun Zhou, Hongzhi Wu | cs.CV | [PDF](http://arxiv.org/pdf/2410.11419v1){: .btn .btn-green } |

**Abstract**: We present a spatial and angular Gaussian based representation and a triple
splatting process, for real-time, high-quality novel lighting-and-view
synthesis from multi-view point-lit input images. To describe complex
appearance, we employ a Lambertian plus a mixture of angular Gaussians as an
effective reflectance function for each spatial Gaussian. To generate
self-shadow, we splat all spatial Gaussians towards the light source to obtain
shadow values, which are further refined by a small multi-layer perceptron. To
compensate for other effects like global illumination, another network is
trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness
of our representation is demonstrated on 30 samples with a wide variation in
geometry (from solid to fluffy) and appearance (from translucent to
anisotropic), as well as using different forms of input data, including
rendered images of synthetic/reconstructed objects, photographs captured with a
handheld camera and a flash, or from a professional lightstage. We achieve a
training time of 40-70 minutes and a rendering speed of 90 fps on a single
commodity GPU. Our results compare favorably with state-of-the-art techniques
in terms of quality/performance. Our code and data are publicly available at
https://GSrelight.github.io/.

Comments:
- Accepted to SIGGRAPH Asia 2024. Project page:
  https://gsrelight.github.io/

---

## MCGS: Multiview Consistency Enhancement for Sparse-View 3D Gaussian  Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-15 | Yuru Xiao, Deming Zhai, Wenbo Zhao, Kui Jiang, Junjun Jiang, Xianming Liu | cs.CV | [PDF](http://arxiv.org/pdf/2410.11394v1){: .btn .btn-green } |

**Abstract**: Radiance fields represented by 3D Gaussians excel at synthesizing novel
views, offering both high training efficiency and fast rendering. However, with
sparse input views, the lack of multi-view consistency constraints results in
poorly initialized point clouds and unreliable heuristics for optimization and
densification, leading to suboptimal performance. Existing methods often
incorporate depth priors from dense estimation networks but overlook the
inherent multi-view consistency in input images. Additionally, they rely on
multi-view stereo (MVS)-based initialization, which limits the efficiency of
scene representation. To overcome these challenges, we propose a view synthesis
framework based on 3D Gaussian Splatting, named MCGS, enabling photorealistic
scene reconstruction from sparse input views. The key innovations of MCGS in
enhancing multi-view consistency are as follows: i) We introduce an
initialization method by leveraging a sparse matcher combined with a random
filling strategy, yielding a compact yet sufficient set of initial points. This
approach enhances the initial geometry prior, promoting efficient scene
representation. ii) We develop a multi-view consistency-guided progressive
pruning strategy to refine the Gaussian field by strengthening consistency and
eliminating low-contribution Gaussians. These modular, plug-and-play strategies
enhance robustness to sparse input views, accelerate rendering, and reduce
memory consumption, making MCGS a practical and efficient framework for 3D
Gaussian Splatting.



---

## GSORB-SLAM: Gaussian Splatting SLAM benefits from ORB features and  Transmittance information

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-15 | Wancai Zheng, Xinyi Yu, Jintao Rong, Linlin Ou, Yan Wei, Libo Zhou | cs.RO | [PDF](http://arxiv.org/pdf/2410.11356v1){: .btn .btn-green } |

**Abstract**: The emergence of 3D Gaussian Splatting (3DGS) has recently sparked a renewed
wave of dense visual SLAM research. However, current methods face challenges
such as sensitivity to artifacts and noise, sub-optimal selection of training
viewpoints, and a lack of light global optimization. In this paper, we propose
a dense SLAM system that tightly couples 3DGS with ORB features. We design a
joint optimization approach for robust tracking and effectively reducing the
impact of noise and artifacts. This involves combining novel geometric
observations, derived from accumulated transmittance, with ORB features
extracted from pixel data. Furthermore, to improve mapping quality, we propose
an adaptive Gaussian expansion and regularization method that enables Gaussian
primitives to represent the scene compactly. This is coupled with a viewpoint
selection strategy based on the hybrid graph to mitigate over-fitting effects
and enhance convergence quality. Finally, our approach achieves compact and
high-quality scene representations and accurate localization. GSORB-SLAM has
been evaluated on different datasets, demonstrating outstanding performance.
The code will be available.



---

## SplatPose+: Real-time Image-Based Pose-Agnostic 3D Anomaly Detection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-15 | Yizhe Liu, Yan Song Hu, Yuhao Chen, John Zelek | cs.CV | [PDF](http://arxiv.org/pdf/2410.12080v1){: .btn .btn-green } |

**Abstract**: Image-based Pose-Agnostic 3D Anomaly Detection is an important task that has
emerged in industrial quality control. This task seeks to find anomalies from
query images of a tested object given a set of reference images of an
anomaly-free object. The challenge is that the query views (a.k.a poses) are
unknown and can be different from the reference views. Currently, new methods
such as OmniposeAD and SplatPose have emerged to bridge the gap by synthesizing
pseudo reference images at the query views for pixel-to-pixel comparison.
However, none of these methods can infer in real-time, which is critical in
industrial quality control for massive production. For this reason, we propose
SplatPose+, which employs a hybrid representation consisting of a Structure
from Motion (SfM) model for localization and a 3D Gaussian Splatting (3DGS)
model for Novel View Synthesis. Although our proposed pipeline requires the
computation of an additional SfM model, it offers real-time inference speeds
and faster training compared to SplatPose. Quality-wise, we achieved a new SOTA
on the Pose-agnostic Anomaly Detection benchmark with the Multi-Pose Anomaly
Detection (MAD-SIM) dataset.



---

## 3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for  Human-Object Interaction (HOI) and Autonomous Driving Applications

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-14 | Eduardo R. Corral-Soto, Yang Liu, Tongtong Cao, Yuan Ren, Liu Bingbing | cs.CV | [PDF](http://arxiv.org/pdf/2410.10782v1){: .btn .btn-green } |

**Abstract**: Human-object interaction (HOI) and human-scene interaction (HSI) are crucial
for human-centric scene understanding applications in Embodied Artificial
Intelligence (EAI), robotics, and augmented reality (AR). A common limitation
faced in these research areas is the data scarcity problem: insufficient
labeled human-scene object pairs on the input images, and limited interaction
complexity and granularity between them. Recent HOI and HSI methods have
addressed this issue by generating dynamic interactions with rigid objects. But
more complex dynamic interactions such as a human rider pedaling an articulated
bicycle have been unexplored. To address this limitation, and to enable
research on complex dynamic human-articulated object interactions, in this
paper we propose a method to generate simulated 3D dynamic cyclist assets and
interactions. We designed a methodology for creating a new part-based
multi-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes
that can be used to train NeRF and 3DGS-based 3D reconstruction methods. We
then propose a 3DGS-based parametric bicycle composition model to assemble
8-DoF pose-controllable 3D bicycles. Finally, using dynamic information from
cyclist videos, we build a complete synthetic dynamic 3D cyclist (rider
pedaling a bicycle) by re-posing a selectable synthetic 3D person while
automatically placing the rider onto one of our new articulated 3D bicycles
using a proposed 3D Keypoint optimization-based Inverse Kinematics pose
refinement. We present both, qualitative and quantitative results where we
compare our generated cyclists against those from a recent stable
diffusion-based method.



---

## Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-14 | Raja Kumar, Vanshika Vats | cs.CV | [PDF](http://arxiv.org/pdf/2410.11080v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has surpassed neural radiance field methods in novel
view synthesis by achieving lower computational costs and real-time
high-quality rendering. Although it produces a high-quality rendering with a
lot of input views, its performance drops significantly when only a few views
are available. In this work, we address this by proposing a depth-aware
Gaussian splatting method for few-shot novel view synthesis. We use monocular
depth prediction as a prior, along with a scale-invariant depth loss, to
constrain the 3D shape under just a few input views. We also model color using
lower-order spherical harmonics to avoid overfitting. Further, we observe that
removing splats with lower opacity periodically, as performed in the original
work, leads to a very sparse point cloud and, hence, a lower-quality rendering.
To mitigate this, we retain all the splats, leading to a better reconstruction
in a few view settings. Experimental results show that our method outperforms
the traditional 3D Gaussian splatting methods by achieving improvements of
10.5% in peak signal-to-noise ratio, 6% in structural similarity index, and
14.1% in perceptual similarity, thereby validating the effectiveness of our
approach. The code will be made available at:
https://github.com/raja-kumar/depth-aware-3DGS

Comments:
- Presented in ECCV 2024 workshop S3DSGR

---

## NeRF-enabled Analysis-Through-Synthesis for ISAR Imaging of Small  Everyday Objects with Sparse and Noisy UWB Radar Data

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-14 | Md Farhan Tasnim Oshim, Albert Reed, Suren Jayasuriya, Tauhidur Rahman | cs.RO | [PDF](http://arxiv.org/pdf/2410.10085v1){: .btn .btn-green } |

**Abstract**: Inverse Synthetic Aperture Radar (ISAR) imaging presents a formidable
challenge when it comes to small everyday objects due to their limited Radar
Cross-Section (RCS) and the inherent resolution constraints of radar systems.
Existing ISAR reconstruction methods including backprojection (BP) often
require complex setups and controlled environments, rendering them impractical
for many real-world noisy scenarios. In this paper, we propose a novel
Analysis-through-Synthesis (ATS) framework enabled by Neural Radiance Fields
(NeRF) for high-resolution coherent ISAR imaging of small objects using sparse
and noisy Ultra-Wideband (UWB) radar data with an inexpensive and portable
setup. Our end-to-end framework integrates ultra-wideband radar wave
propagation, reflection characteristics, and scene priors, enabling efficient
2D scene reconstruction without the need for costly anechoic chambers or
complex measurement test beds. With qualitative and quantitative comparisons,
we demonstrate that the proposed method outperforms traditional techniques and
generates ISAR images of complex scenes with multiple targets and complex
structures in Non-Line-of-Sight (NLOS) and noisy scenarios, particularly with
limited number of views and sparse UWB radar scans. This work represents a
significant step towards practical, cost-effective ISAR imaging of small
everyday objects, with broad implications for robotics and mobile sensing
applications.



---

## 4-LEGS: 4D Language Embedded Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-14 | Gal Fiebelman, Tamir Cohen, Ayellet Morgenstern, Peter Hedman, Hadar Averbuch-Elor | cs.CV | [PDF](http://arxiv.org/pdf/2410.10719v2){: .btn .btn-green } |

**Abstract**: The emergence of neural representations has revolutionized our means for
digitally viewing a wide range of 3D scenes, enabling the synthesis of
photorealistic images rendered from novel views. Recently, several techniques
have been proposed for connecting these low-level representations with the
high-level semantics understanding embodied within the scene. These methods
elevate the rich semantic understanding from 2D imagery to 3D representations,
distilling high-dimensional spatial features onto 3D space. In our work, we are
interested in connecting language with a dynamic modeling of the world. We show
how to lift spatio-temporal features to a 4D representation based on 3D
Gaussian Splatting. This enables an interactive interface where the user can
spatiotemporally localize events in the video from text prompts. We demonstrate
our system on public 3D video datasets of people and animals performing various
actions.

Comments:
- Project webpage: https://tau-vailab.github.io/4-LEGS/

---

## 4DStyleGaussian: Zero-shot 4D Style Transfer with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-14 | Wanlin Liang, Hongbin Xu, Weitao Chen, Feng Xiao, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2410.10412v1){: .btn .btn-green } |

**Abstract**: 3D neural style transfer has gained significant attention for its potential
to provide user-friendly stylization with spatial consistency. However,
existing 3D style transfer methods often fall short in terms of inference
efficiency, generalization ability, and struggle to handle dynamic scenes with
temporal consistency. In this paper, we introduce 4DStyleGaussian, a novel 4D
style transfer framework designed to achieve real-time stylization of arbitrary
style references while maintaining reasonable content affinity, multi-view
consistency, and temporal coherence. Our approach leverages an embedded 4D
Gaussian Splatting technique, which is trained using a reversible neural
network for reducing content loss in the feature distillation process.
Utilizing the 4D embedded Gaussians, we predict a 4D style transformation
matrix that facilitates spatially and temporally consistent style transfer with
Gaussian Splatting. Experiments demonstrate that our method can achieve
high-quality and zero-shot stylization for 4D scenarios with enhanced
efficiency and spatial-temporal consistency.



---

## Gaussian Splatting Visual MPC for Granular Media Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-13 | Wei-Cheng Tseng, Ellina Zhang, Krishna Murthy Jatavallabhula, Florian Shkurti | cs.RO | [PDF](http://arxiv.org/pdf/2410.09740v1){: .btn .btn-green } |

**Abstract**: Recent advancements in learned 3D representations have enabled significant
progress in solving complex robotic manipulation tasks, particularly for
rigid-body objects. However, manipulating granular materials such as beans,
nuts, and rice, remains challenging due to the intricate physics of particle
interactions, high-dimensional and partially observable state, inability to
visually track individual particles in a pile, and the computational demands of
accurate dynamics prediction. Current deep latent dynamics models often
struggle to generalize in granular material manipulation due to a lack of
inductive biases. In this work, we propose a novel approach that learns a
visual dynamics model over Gaussian splatting representations of scenes and
leverages this model for manipulating granular media via Model-Predictive
Control. Our method enables efficient optimization for complex manipulation
tasks on piles of granular media. We evaluate our approach in both simulated
and real-world settings, demonstrating its ability to solve unseen planning
tasks and generalize to new environments in a zero-shot transfer. We also show
significant prediction and manipulation performance improvements compared to
existing granular media manipulation methods.

Comments:
- project website https://weichengtseng.github.io/gs-granular-mani/

---

## Magnituder Layers for Implicit Neural Representations in 3D

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-13 | Sang Min Kim, Byeongchan Kim, Arijit Sehanobish, Krzysztof Choromanski, Dongseok Shim, Avinava Dubey, Min-hwan Oh | cs.CV | [PDF](http://arxiv.org/pdf/2410.09771v1){: .btn .btn-green } |

**Abstract**: Improving the efficiency and performance of implicit neural representations
in 3D, particularly Neural Radiance Fields (NeRF) and Signed Distance Fields
(SDF) is crucial for enabling their use in real-time applications. These
models, while capable of generating photo-realistic novel views and detailed 3D
reconstructions, often suffer from high computational costs and slow inference
times. To address this, we introduce a novel neural network layer called the
"magnituder", designed to reduce the number of training parameters in these
models without sacrificing their expressive power. By integrating magnituders
into standard feed-forward layer stacks, we achieve improved inference speed
and adaptability. Furthermore, our approach enables a zero-shot performance
boost in trained implicit neural representation models through layer-wise
knowledge transfer without backpropagation, leading to more efficient scene
reconstruction in dynamic environments.



---

## Improving 3D Finger Traits Recognition via Generalizable Neural  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-12 | Hongbin Xu, Junduan Huang, Yuer Ma, Zifeng Li, Wenxiong Kang | cs.CV | [PDF](http://arxiv.org/pdf/2410.09582v1){: .btn .btn-green } |

**Abstract**: 3D biometric techniques on finger traits have become a new trend and have
demonstrated a powerful ability for recognition and anti-counterfeiting.
Existing methods follow an explicit 3D pipeline that reconstructs the models
first and then extracts features from 3D models. However, these explicit 3D
methods suffer from the following problems: 1) Inevitable information dropping
during 3D reconstruction; 2) Tight coupling between specific hardware and
algorithm for 3D reconstruction. It leads us to a question: Is it indispensable
to reconstruct 3D information explicitly in recognition tasks? Hence, we
consider this problem in an implicit manner, leaving the nerve-wracking 3D
reconstruction problem for learnable neural networks with the help of neural
radiance fields (NeRFs). We propose FingerNeRF, a novel generalizable NeRF for
3D finger biometrics. To handle the shape-radiance ambiguity problem that may
result in incorrect 3D geometry, we aim to involve extra geometric priors based
on the correspondence of binary finger traits like fingerprints or finger
veins. First, we propose a novel Trait Guided Transformer (TGT) module to
enhance the feature correspondence with the guidance of finger traits. Second,
we involve extra geometric constraints on the volume rendering loss with the
proposed Depth Distillation Loss and Trait Guided Rendering Loss. To evaluate
the performance of the proposed method on different modalities, we collect two
new datasets: SCUT-Finger-3D with finger images and SCUT-FingerVein-3D with
finger vein images. Moreover, we also utilize the UNSW-3D dataset with
fingerprint images for evaluation. In experiments, our FingerNeRF can achieve
4.37% EER on SCUT-Finger-3D dataset, 8.12% EER on SCUT-FingerVein-3D dataset,
and 2.90% EER on UNSW-3D dataset, showing the superiority of the proposed
implicit method in 3D finger biometrics.

Comments:
- This paper is accepted in IJCV. For further information and access to
  the code, please visit our project page:
  https://scut-bip-lab.github.io/fingernerf/

---

## Enhancing Single Image to 3D Generation using Gaussian Splatting and  Hybrid Diffusion Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-12 | Hritam Basak, Hadi Tabatabaee, Shreekant Gayaka, Ming-Feng Li, Xin Yang, Cheng-Hao Kuo, Arnie Sen, Min Sun, Zhaozheng Yin | cs.CV | [PDF](http://arxiv.org/pdf/2410.09467v1){: .btn .btn-green } |

**Abstract**: 3D object generation from a single image involves estimating the full 3D
geometry and texture of unseen views from an unposed RGB image captured in the
wild. Accurately reconstructing an object's complete 3D structure and texture
has numerous applications in real-world scenarios, including robotic
manipulation, grasping, 3D scene understanding, and AR/VR. Recent advancements
in 3D object generation have introduced techniques that reconstruct an object's
3D shape and texture by optimizing the efficient representation of Gaussian
Splatting, guided by pre-trained 2D or 3D diffusion models. However, a notable
disparity exists between the training datasets of these models, leading to
distinct differences in their outputs. While 2D models generate highly detailed
visuals, they lack cross-view consistency in geometry and texture. In contrast,
3D models ensure consistency across different views but often result in overly
smooth textures. We propose bridging the gap between 2D and 3D diffusion models
to address this limitation by integrating a two-stage frequency-based
distillation loss with Gaussian Splatting. Specifically, we leverage geometric
priors in the low-frequency spectrum from a 3D diffusion model to maintain
consistent geometry and use a 2D diffusion model to refine the fidelity and
texture in the high-frequency spectrum of the generated 3D structure, resulting
in more detailed and fine-grained outcomes. Our approach enhances geometric
consistency and visual quality, outperforming the current SOTA. Additionally,
we demonstrate the easy adaptability of our method for efficient object pose
estimation and tracking.



---

## SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted  Surgical Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-11 | Jialei Chen, Xin Zhang, Mobarakol Islam, Francisco Vasconcelos, Danail Stoyanov, Daniel S. Elson, Baoru Huang | cs.CV | [PDF](http://arxiv.org/pdf/2410.09292v1){: .btn .btn-green } |

**Abstract**: Accurate 3D reconstruction of dynamic surgical scenes from endoscopic video
is essential for robotic-assisted surgery. While recent 3D Gaussian Splatting
methods have shown promise in achieving high-quality reconstructions with fast
rendering speeds, their use of inverse depth loss functions compresses depth
variations. This can lead to a loss of fine geometric details, limiting their
ability to capture precise 3D geometry and effectiveness in intraoperative
application. To address these challenges, we present SurgicalGS, a dynamic 3D
Gaussian Splatting framework specifically designed for surgical scene
reconstruction with improved geometric accuracy. Our approach first initialises
a Gaussian point cloud using depth priors, employing binary motion masks to
identify pixels with significant depth variations and fusing point clouds from
depth maps across frames for initialisation. We use the Flexible Deformation
Model to represent dynamic scene and introduce a normalised depth
regularisation loss along with an unsupervised depth smoothness constraint to
ensure more accurate geometric reconstruction. Extensive experiments on two
real surgical datasets demonstrate that SurgicalGS achieves state-of-the-art
reconstruction quality, especially in terms of accurate geometry, advancing the
usability of 3D Gaussian Splatting in robotic-assisted surgery.

Comments:
- 7 pages

---

## MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality  Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-11 | Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Heesung Kwon, Dinesh Manocha | cs.CV | [PDF](http://arxiv.org/pdf/2410.08941v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian splatting has gained attention for its capability to
generate high-fidelity rendering results. At the same time, most applications
such as games, animation, and AR/VR use mesh-based representations to represent
and render 3D scenes. We propose a novel approach that integrates mesh
representation with 3D Gaussian splats to perform high-quality rendering of
reconstructed real-world scenes. In particular, we introduce a distance-based
Gaussian splatting technique to align the Gaussian splats with the mesh surface
and remove redundant Gaussian splats that do not contribute to the rendering.
We consider the distance between each Gaussian splat and the mesh surface to
distinguish between tightly-bound and loosely-bound Gaussian splats. The
tightly-bound splats are flattened and aligned well with the mesh geometry. The
loosely-bound Gaussian splats are used to account for the artifacts in
reconstructed 3D meshes in terms of rendering. We present a training strategy
of binding Gaussian splats to the mesh geometry, and take into account both
types of splats. In this context, we introduce several regularization
techniques aimed at precisely aligning tightly-bound Gaussian splats with the
mesh surface during the training process. We validate the effectiveness of our
method on large and unbounded scene from mip-NeRF 360 and Deep Blending
datasets. Our method surpasses recent mesh-based neural rendering techniques by
achieving a 2dB higher PSNR, and outperforms mesh-based Gaussian splatting
methods by 1.3 dB PSNR, particularly on the outdoor mip-NeRF 360 dataset,
demonstrating better rendering quality. We provide analyses for each type of
Gaussian splat and achieve a reduction in the number of Gaussian splats by 30%
compared to the original 3D Gaussian splatting.

Comments:
- ACCV (Asian Conference on Computer Vision) 2024

---

## Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting  without Accurate Pose Initialization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-11 | Christian Schmidt, Jens Piekenbrinck, Bastian Leibe | cs.CV | [PDF](http://arxiv.org/pdf/2410.08743v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently emerged as a powerful tool for fast and
accurate novel-view synthesis from a set of posed input images. However, like
most novel-view synthesis approaches, it relies on accurate camera pose
information, limiting its applicability in real-world scenarios where acquiring
accurate camera poses can be challenging or even impossible. We propose an
extension to the 3D Gaussian Splatting framework by optimizing the extrinsic
camera parameters with respect to photometric residuals. We derive the
analytical gradients and integrate their computation with the existing
high-performance CUDA implementation. This enables downstream tasks such as
6-DoF camera pose estimation as well as joint reconstruction and camera
refinement. In particular, we achieve rapid convergence and high accuracy for
pose estimation on real-world scenes. Our method enables fast reconstruction of
3D scenes without requiring accurate pose information by jointly optimizing
geometry and camera poses, while achieving state-of-the-art results in
novel-view synthesis. Our approach is considerably faster to optimize than most
competing methods, and several times faster in rendering. We show results on
real-world scenes and complex trajectories through simulated environments,
achieving state-of-the-art results on LLFF while reducing runtime by two to
four times compared to the most efficient competing method. Source code will be
available at https://github.com/Schmiddo/noposegs .

Comments:
- Accepted in IROS 2024

---

## Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand  Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-11 | Xuan Huang, Hanhui Li, Wanquan Liu, Xiaodan Liang, Yiqiang Yan, Yuhao Cheng, Chengqiang Gao | cs.CV | [PDF](http://arxiv.org/pdf/2410.08840v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose to create animatable avatars for interacting hands
with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based
methods designed for single subjects often yield unsatisfactory results due to
limited input views, various hand poses, and occlusions. To address these
challenges, we introduce a novel two-stage interaction-aware GS framework that
exploits cross-subject hand priors and refines 3D Gaussians in interacting
areas. Particularly, to handle hand variations, we disentangle the 3D
presentation of hands into optimization-based identity maps and learning-based
latent geometric features and neural texture maps. Learning-based features are
captured by trained networks to provide reliable priors for poses, shapes, and
textures, while optimization-based identity maps enable efficient one-shot
fitting of out-of-distribution hands. Furthermore, we devise an
interaction-aware attention module and a self-adaptive Gaussian refinement
module. These modules enhance image rendering quality in areas with intra- and
inter-hand interactions, overcoming the limitations of existing GS-based
methods. Our proposed method is validated via extensive experiments on the
large-scale InterHand2.6M dataset, and it significantly improves the
state-of-the-art performance in image quality. Project Page:
\url{https://github.com/XuanHuang0/GuassianHand}.

Comments:
- Accepted to NeurIPS 2024

---

## SceneCraft: Layout-Guided 3D Scene Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-11 | Xiuyu Yang, Yunze Man, Jun-Kun Chen, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2410.09049v1){: .btn .btn-green } |

**Abstract**: The creation of complex 3D scenes tailored to user specifications has been a
tedious and challenging task with traditional 3D modeling tools. Although some
pioneering methods have achieved automatic text-to-3D generation, they are
generally limited to small-scale scenes with restricted control over the shape
and texture. We introduce SceneCraft, a novel method for generating detailed
indoor scenes that adhere to textual descriptions and spatial layout
preferences provided by users. Central to our method is a rendering-based
technique, which converts 3D semantic layouts into multi-view 2D proxy maps.
Furthermore, we design a semantic and depth conditioned diffusion model to
generate multi-view images, which are used to learn a neural radiance field
(NeRF) as the final scene representation. Without the constraints of panorama
image generation, we surpass previous methods in supporting complicated indoor
space generation beyond a single room, even as complicated as a whole
multi-bedroom apartment with irregular shapes and layouts. Through experimental
analysis, we demonstrate that our method significantly outperforms existing
approaches in complex indoor scene generation with diverse textures, consistent
geometry, and realistic visual quality. Code and more results are available at:
https://orangesodahub.github.io/SceneCraft

Comments:
- NeurIPS 2024. Code: https://github.com/OrangeSodahub/SceneCraft
  Project Page: https://orangesodahub.github.io/SceneCraft

---

## Optimizing NeRF-based SLAM with Trajectory Smoothness Constraints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-11 | Yicheng He, Guangcheng Chen, Hong Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2410.08780v1){: .btn .btn-green } |

**Abstract**: The joint optimization of Neural Radiance Fields (NeRF) and camera
trajectories has been widely applied in SLAM tasks due to its superior dense
mapping quality and consistency. NeRF-based SLAM learns camera poses using
constraints by implicit map representation. A widely observed phenomenon that
results from the constraints of this form is jerky and physically unrealistic
estimated camera motion, which in turn affects the map quality. To address this
deficiency of current NeRF-based SLAM, we propose in this paper TS-SLAM (TS for
Trajectory Smoothness). It introduces smoothness constraints on camera
trajectories by representing them with uniform cubic B-splines with continuous
acceleration that guarantees smooth camera motion. Benefiting from the
differentiability and local control properties of B-splines, TS-SLAM can
incrementally learn the control points end-to-end using a sliding window
paradigm. Additionally, we regularize camera trajectories by exploiting the
dynamics prior to further smooth trajectories. Experimental results demonstrate
that TS-SLAM achieves superior trajectory accuracy and improves mapping quality
versus NeRF-based SLAM that does not employ the above smoothness constraints.



---

## DifFRelight: Diffusion-Based Facial Performance Relighting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Mingming He, Pascal Clausen, Ahmet Levent Taşel, Li Ma, Oliver Pilarski, Wenqi Xian, Laszlo Rikker, Xueming Yu, Ryan Burgert, Ning Yu, Paul Debevec | cs.CV | [PDF](http://arxiv.org/pdf/2410.08188v1){: .btn .btn-green } |

**Abstract**: We present a novel framework for free-viewpoint facial performance relighting
using diffusion-based image-to-image translation. Leveraging a subject-specific
dataset containing diverse facial expressions captured under various lighting
conditions, including flat-lit and one-light-at-a-time (OLAT) scenarios, we
train a diffusion model for precise lighting control, enabling high-fidelity
relit facial images from flat-lit inputs. Our framework includes
spatially-aligned conditioning of flat-lit captures and random noise, along
with integrated lighting information for global control, utilizing prior
knowledge from the pre-trained Stable Diffusion model. This model is then
applied to dynamic facial performances captured in a consistent flat-lit
environment and reconstructed for novel-view synthesis using a scalable dynamic
3D Gaussian Splatting method to maintain quality and consistency in the relit
results. In addition, we introduce unified lighting control by integrating a
novel area lighting representation with directional lighting, allowing for
joint adjustments in light size and direction. We also enable high dynamic
range imaging (HDRI) composition using multiple directional lights to produce
dynamic sequences under complex lighting conditions. Our evaluations
demonstrate the models efficiency in achieving precise lighting control and
generalizing across various facial expressions while preserving detailed
features such as skintexture andhair. The model accurately reproduces complex
lighting effects like eye reflections, subsurface scattering, self-shadowing,
and translucency, advancing photorealism within our framework.

Comments:
- 18 pages, SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers
  '24), December 3--6, 2024, Tokyo, Japan. Project page:
  https://www.eyelinestudios.com/research/diffrelight.html

---

## Poison-splat: Computation Cost Attack on 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Jiahao Lu, Yifan Zhang, Qiuhong Shen, Xinchao Wang, Shuicheng Yan | cs.CV | [PDF](http://arxiv.org/pdf/2410.08190v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS), known for its groundbreaking performance and
efficiency, has become a dominant 3D representation and brought progress to
many 3D vision tasks. However, in this work, we reveal a significant security
vulnerability that has been largely overlooked in 3DGS: the computation cost of
training 3DGS could be maliciously tampered by poisoning the input data. By
developing an attack named Poison-splat, we reveal a novel attack surface where
the adversary can poison the input images to drastically increase the
computation memory and time needed for 3DGS training, pushing the algorithm
towards its worst computation complexity. In extreme cases, the attack can even
consume all allocable memory, leading to a Denial-of-Service (DoS) that
disrupts servers, resulting in practical damages to real-world 3DGS service
vendors. Such a computation cost attack is achieved by addressing a bi-level
optimization problem through three tailored strategies: attack objective
approximation, proxy model rendering, and optional constrained optimization.
These strategies not only ensure the effectiveness of our attack but also make
it difficult to defend with simple defensive measures. We hope the revelation
of this novel attack surface can spark attention to this crucial yet overlooked
vulnerability of 3DGS systems.

Comments:
- Our code is available at https://github.com/jiahaolu97/poison-splat

---

## L-VITeX: Light-weight Visual Intuition for Terrain Exploration


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Antar Mazumder, Zarin Anjum Madhiha | cs.RO | [PDF](http://arxiv.org/pdf/2410.07872v1){: .btn .btn-green } |

**Abstract**: This paper presents L-VITeX, a lightweight visual intuition system for
terrain exploration designed for resource-constrained robots and swarms.
L-VITeX aims to provide a hint of Regions of Interest (RoIs) without
computationally expensive processing. By utilizing the Faster Objects, More
Objects (FOMO) tinyML architecture, the system achieves high accuracy (>99%) in
RoI detection while operating on minimal hardware resources (Peak RAM usage <
50 KB) with near real-time inference (<200 ms). The paper evaluates L-VITeX's
performance across various terrains, including mountainous areas, underwater
shipwreck debris regions, and Martian rocky surfaces. Additionally, it
demonstrates the system's application in 3D mapping using a small mobile robot
run by ESP32-Cam and Gaussian Splats (GS), showcasing its potential to enhance
exploration efficiency and decision-making.



---

## Fast Feedforward 3D Gaussian Splatting Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Yihang Chen, Qianyi Wu, Mengyao Li, Weiyao Lin, Mehrtash Harandi, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2410.08017v2){: .btn .btn-green } |

**Abstract**: With 3D Gaussian Splatting (3DGS) advancing real-time and high-fidelity
rendering for novel view synthesis, storage requirements pose challenges for
their widespread adoption. Although various compression techniques have been
proposed, previous art suffers from a common limitation: for any existing 3DGS,
per-scene optimization is needed to achieve compression, making the compression
sluggish and slow. To address this issue, we introduce Fast Compression of 3D
Gaussian Splatting (FCGS), an optimization-free model that can compress 3DGS
representations rapidly in a single feed-forward pass, which significantly
reduces compression time from minutes to seconds. To enhance compression
efficiency, we propose a multi-path entropy module that assigns Gaussian
attributes to different entropy constraint paths for balance between size and
fidelity. We also carefully design both inter- and intra-Gaussian context
models to remove redundancies among the unstructured Gaussian blobs. Overall,
FCGS achieves a compression ratio of over 20X while maintaining fidelity,
surpassing most per-scene SOTA optimization-based methods. Our code is
available at: https://github.com/YihangChen-ee/FCGS.

Comments:
- Project Page: https://yihangchen-ee.github.io/project_fcgs/ Code:
  https://github.com/yihangchen-ee/fcgs/

---

## Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Junyi Cao, Shanyan Guan, Yanhao Ge, Wei Li, Xiaokang Yang, Chao Ma | cs.CV | [PDF](http://arxiv.org/pdf/2410.08257v1){: .btn .btn-green } |

**Abstract**: While humans effortlessly discern intrinsic dynamics and adapt to new
scenarios, modern AI systems often struggle. Current methods for visual
grounding of dynamics either use pure neural-network-based simulators (black
box), which may violate physical laws, or traditional physical simulators
(white box), which rely on expert-defined equations that may not fully capture
actual dynamics. We propose the Neural Material Adaptor (NeuMA), which
integrates existing physical laws with learned corrections, facilitating
accurate learning of actual dynamics while maintaining the generalizability and
interpretability of physical priors. Additionally, we propose Particle-GS, a
particle-driven 3D Gaussian Splatting variant that bridges simulation and
observed images, allowing back-propagate image gradients to optimize the
simulator. Comprehensive experiments on various dynamics in terms of grounded
particle accuracy, dynamic rendering quality, and generalization ability
demonstrate that NeuMA can accurately capture intrinsic dynamics.

Comments:
- NeurIPS 2024, the project page:
  https://xjay18.github.io/projects/neuma.html

---

## FusionSense: Bridging Common Sense, Vision, and Touch for Robust  Sparse-View Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Irving Fang, Kairui Shi, Xujin He, Siqi Tan, Yifan Wang, Hanwen Zhao, Hung-Jui Huang, Wenzhen Yuan, Chen Feng, Jing Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2410.08282v1){: .btn .btn-green } |

**Abstract**: Humans effortlessly integrate common-sense knowledge with sensory input from
vision and touch to understand their surroundings. Emulating this capability,
we introduce FusionSense, a novel 3D reconstruction framework that enables
robots to fuse priors from foundation models with highly sparse observations
from vision and tactile sensors. FusionSense addresses three key challenges:
(i) How can robots efficiently acquire robust global shape information about
the surrounding scene and objects? (ii) How can robots strategically select
touch points on the object using geometric and common-sense priors? (iii) How
can partial observations such as tactile signals improve the overall
representation of the object? Our framework employs 3D Gaussian Splatting as a
core representation and incorporates a hierarchical optimization strategy
involving global structure construction, object visual hull pruning and local
geometric constraints. This advancement results in fast and robust perception
in environments with traditionally challenging objects that are transparent,
reflective, or dark, enabling more downstream manipulation or navigation tasks.
Experiments on real-world data suggest that our framework outperforms
previously state-of-the-art sparse-view methods. All code and data are
open-sourced on the project website.



---

## MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Ruijie Zhu, Yanzhe Liang, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2410.07707v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction is a long-term challenge in the field of 3D
vision. Recently, the emergence of 3D Gaussian Splatting has provided new
insights into this problem. Although subsequent efforts rapidly extend static
3D Gaussian to dynamic scenes, they often lack explicit constraints on object
motion, leading to optimization difficulties and performance degradation. To
address the above issues, we propose a novel deformable 3D Gaussian splatting
framework called MotionGS, which explores explicit motion priors to guide the
deformation of 3D Gaussians. Specifically, we first introduce an optical flow
decoupling module that decouples optical flow into camera flow and motion flow,
corresponding to camera movement and object motion respectively. Then the
motion flow can effectively constrain the deformation of 3D Gaussians, thus
simulating the motion of dynamic objects. Additionally, a camera pose
refinement module is proposed to alternately optimize 3D Gaussians and camera
poses, mitigating the impact of inaccurate camera poses. Extensive experiments
in the monocular dynamic scenes validate that MotionGS surpasses
state-of-the-art methods and exhibits significant superiority in both
qualitative and quantitative results. Project page:
https://ruijiezhu94.github.io/MotionGS_page

Comments:
- Accepted by NeurIPS 2024. 21 pages, 14 figures,7 tables

---

## RGM: Reconstructing High-fidelity 3D Car Assets with Relightable 3D-GS  Generative Model from a Single Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Xiaoxue Chen, Jv Zheng, Hao Huang, Haoran Xu, Weihao Gu, Kangliang Chen, He xiang, Huan-ang Gao, Hao Zhao, Guyue Zhou, Yaqin Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2410.08181v1){: .btn .btn-green } |

**Abstract**: The generation of high-quality 3D car assets is essential for various
applications, including video games, autonomous driving, and virtual reality.
Current 3D generation methods utilizing NeRF or 3D-GS as representations for 3D
objects, generate a Lambertian object under fixed lighting and lack separated
modelings for material and global illumination. As a result, the generated
assets are unsuitable for relighting under varying lighting conditions,
limiting their applicability in downstream tasks. To address this challenge, we
propose a novel relightable 3D object generative framework that automates the
creation of 3D car assets, enabling the swift and accurate reconstruction of a
vehicle's geometry, texture, and material properties from a single input image.
Our approach begins with introducing a large-scale synthetic car dataset
comprising over 1,000 high-precision 3D vehicle models. We represent 3D objects
using global illumination and relightable 3D Gaussian primitives integrating
with BRDF parameters. Building on this representation, we introduce a
feed-forward model that takes images as input and outputs both relightable 3D
Gaussians and global illumination parameters. Experimental results demonstrate
that our method produces photorealistic 3D car assets that can be seamlessly
integrated into road scenes with different illuminations, which offers
substantial practical benefits for industrial applications.



---

## Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid  Transparency

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Florian Hahlbohm, Fabian Friederichs, Tim Weyrich, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Martin Eisemann, Marcus Magnor | cs.GR | [PDF](http://arxiv.org/pdf/2410.08129v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both
for inverse rendering as well as real-time exploration of scenes. In these
applications, coherence across camera frames and multiple views is crucial, be
it for robust convergence of a scene reconstruction or for artifact-free
fly-throughs. Recent work started mitigating artifacts that break multi-view
coherence, including popping artifacts due to inconsistent transparency sorting
and perspective-correct outlines of (2D) splats. At the same time, real-time
requirements forced such implementations to accept compromises in how
transparency of large assemblies of 3D Gaussians is resolved, in turn breaking
coherence in other ways. In our work, we aim at achieving maximum coherence, by
rendering fully perspective-correct 3D Gaussians while using a high-quality
approximation of accurate blending, hybrid transparency, on a per-pixel level,
in order to retain real-time frame rates. Our fast and perspectively accurate
approach for evaluation of 3D Gaussians does not require matrix inversions,
thereby ensuring numerical stability and eliminating the need for special
handling of degenerate splats, and the hybrid transparency formulation for
blending maintains similar quality as fully resolved per-pixel transparencies
at a fraction of the rendering costs. We further show that each of these two
components can be independently integrated into Gaussian splatting systems. In
combination, they achieve up to 2$\times$ higher frame rates, 2$\times$ faster
optimization, and equal or better image quality with fewer rendering artifacts
compared to traditional 3DGS on common benchmarks.

Comments:
- Project page: https://fhahlbohm.github.io/htgs/

---

## 3D Vision-Language Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Qucheng Peng, Benjamin Planche, Zhongpai Gao, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Chen Chen, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2410.07577v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D reconstruction methods and vision-language models
have propelled the development of multi-modal 3D scene understanding, which has
vital applications in robotics, autonomous driving, and virtual/augmented
reality. However, current multi-modal scene understanding approaches have
naively embedded semantic representations into 3D reconstruction methods
without striking a balance between visual and language modalities, which leads
to unsatisfying semantic rasterization of translucent or reflective objects, as
well as over-fitting on color modality. To alleviate these limitations, we
propose a solution that adequately handles the distinct visual and semantic
modalities, i.e., a 3D vision-language Gaussian splatting model for scene
understanding, to put emphasis on the representation learning of language
modality. We propose a novel cross-modal rasterizer, using modality fusion
along with a smoothed semantic indicator for enhancing semantic rasterization.
We also employ a camera-view blending technique to improve semantic consistency
between existing and synthesized views, thereby effectively mitigating
over-fitting. Extensive experiments demonstrate that our method achieves
state-of-the-art performance in open-vocabulary semantic segmentation,
surpassing existing methods by a significant margin.

Comments:
- main paper + supplementary material

---

## IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-10 | Jian Huang, Chengrui Dong, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2410.08107v1){: .btn .btn-green } |

**Abstract**: Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for
novel view synthesis have achieved remarkable progress with frame-based camera
(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel
type of bio-inspired visual sensor, i.e. event camera, has demonstrated
advantages in high temporal resolution, high dynamic range, low power
consumption and low latency. Due to its unique asynchronous and irregular data
capturing process, limited work has been proposed to apply neural
representation or 3D Gaussian splatting for an event camera. In this work, we
present IncEventGS, an incremental 3D Gaussian Splatting reconstruction
algorithm with a single event camera. To recover the 3D scene representation
incrementally, we exploit the tracking and mapping paradigm of conventional
SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker
firstly estimates an initial camera motion based on prior reconstructed 3D-GS
scene representation. The mapper then jointly refines both the 3D scene
representation and camera motion based on the previously estimated motion
trajectory from the tracker. The experimental results demonstrate that
IncEventGS delivers superior performance compared to prior NeRF-based methods
and other related baselines, even we do not have the ground-truth camera poses.
Furthermore, our method can also deliver better performance compared to
state-of-the-art event visual odometry methods in terms of camera motion
estimation. Code is publicly available at:
https://github.com/wu-cvgl/IncEventGS.

Comments:
- Code Page: https://github.com/wu-cvgl/IncEventGS

---

## DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh  Hybrid Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-09 | Zhiqi Li, Yiming Chen, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2410.06756v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 2D/3D generative techniques have facilitated the
generation of dynamic 3D objects from monocular videos. Previous methods mainly
rely on the implicit neural radiance fields (NeRF) or explicit Gaussian
Splatting as the underlying representation, and struggle to achieve
satisfactory spatial-temporal consistency and surface appearance. Drawing
inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a
novel framework combining mesh representation with geometric skinning technique
to generate high-quality 4D object from a monocular video. Instead of utilizing
classical texture map for appearance, we bind Gaussian splats to triangle face
of mesh for differentiable optimization of both the texture and mesh vertices.
In particular, DreamMesh4D begins with a coarse mesh obtained through an
image-to-3D generation procedure. Sparse points are then uniformly sampled
across the mesh surface, and are used to build a deformation graph to drive the
motion of the 3D object for the sake of computational efficiency and providing
additional constraint. For each step, transformations of sparse control points
are predicted using a deformation network, and the mesh vertices as well as the
surface Gaussians are deformed via a novel geometric skinning algorithm, which
is a hybrid approach combining LBS (linear blending skinning) and DQS
(dual-quaternion skinning), mitigating drawbacks associated with both
approaches. The static surface Gaussians and mesh vertices as well as the
deformation network are learned via reference view photometric loss, score
distillation loss as well as other regularizers in a two-stage manner.
Extensive experiments demonstrate superior performance of our method.
Furthermore, our method is compatible with modern graphic pipelines, showcasing
its potential in the 3D gaming and film industry.

Comments:
- NeurIPS 2024

---

## ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian  Completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-09 | Lu Chen, Yingfu Zeng, Haoang Li, Zhitao Deng, Jiafu Yan, Zhenjun Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2410.06613v1){: .btn .btn-green } |

**Abstract**: Accurate and affordable indoor 3D reconstruction is critical for effective
robot navigation and interaction. Traditional LiDAR-based mapping provides high
precision but is costly, heavy, and power-intensive, with limited ability for
novel view rendering. Vision-based mapping, while cost-effective and capable of
capturing visual data, often struggles with high-quality 3D reconstruction due
to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a
low-altitude camera and single-line LiDAR for high-quality 3D indoor
reconstruction. Our system features Visual Error Construction (VEC) to enhance
sparse point clouds by identifying and correcting areas with insufficient
geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS
initialization method guided by single-line LiDAR, overcoming the limitations
of traditional multi-view setups and enabling effective reconstruction in
resource-constrained environments. Extensive experimental results on our new
Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian
outperforms existing methods, particularly in challenging scenarios. The
project page is available at https://chenlu-china.github.io/ES-Gaussian/.

Comments:
- Project page: https://chenlu-china.github.io/ES-Gaussian/

---

## MimicTalk: Mimicking a personalized and expressive 3D talking face in  minutes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-09 | Zhenhui Ye, Tianyun Zhong, Yi Ren, Ziyue Jiang, Jiawei Huang, Rongjie Huang, Jinglin Liu, Jinzheng He, Chen Zhang, Zehan Wang, Xize Chen, Xiang Yin, Zhou Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2410.06734v1){: .btn .btn-green } |

**Abstract**: Talking face generation (TFG) aims to animate a target identity's face to
create realistic talking videos. Personalized TFG is a variant that emphasizes
the perceptual identity similarity of the synthesized result (from the
perspective of appearance and talking style). While previous works typically
solve this problem by learning an individual neural radiance field (NeRF) for
each identity to implicitly store its static and dynamic information, we find
it inefficient and non-generalized due to the per-identity-per-training
framework and the limited training data. To this end, we propose MimicTalk, the
first attempt that exploits the rich knowledge from a NeRF-based
person-agnostic generic model for improving the efficiency and robustness of
personalized TFG. To be specific, (1) we first come up with a person-agnostic
3D TFG model as the base model and propose to adapt it into a specific
identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help
the model learn the personalized static appearance and facial dynamic features;
(3) To generate the facial motion of the personalized talking style, we propose
an in-context stylized audio-to-motion model that mimics the implicit talking
style provided in the reference video without information loss by an explicit
style representation. The adaptation process to an unseen identity can be
performed in 15 minutes, which is 47 times faster than previous
person-dependent methods. Experiments show that our MimicTalk surpasses
previous baselines regarding video quality, efficiency, and expressiveness.
Source code and video samples are available at https://mimictalk.github.io .

Comments:
- Accepted by NeurIPS 2024

---

## Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction  via Spiking Neuron-based Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-09 | Weixing Zhang, Zongrui Li, De Ma, Huajin Tang, Xudong Jiang, Qian Zheng, Gang Pan | cs.CV | [PDF](http://arxiv.org/pdf/2410.07266v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes.
Despite recent advances in improving surface reconstruction accuracy, the
reconstructed results still exhibit bias and suffer from inefficiency in
storage and training. This paper provides a different observation on the cause
of the inefficiency and the reconstruction bias, which is attributed to the
integration of the low-opacity parts (LOPs) of the generated Gaussians. We show
that LOPs consist of Gaussians with overall low-opacity (LOGs) and the
low-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two
types of LOPs by integrating spiking neurons into the Gaussian Splatting
pipeline. Specifically, we introduce global and local full-precision
integrate-and-fire spiking neurons to the opacity and representation function
of flattened 3D Gaussians, respectively. Furthermore, we enhance the density
control strategy with spiking neurons' thresholds and an new criterion on the
scale of Gaussians. Our method can represent more accurate reconstructed
surfaces at a lower cost. The code is available at
\url{https://github.com/shippoT/Spiking_GS}.



---

## NeRF-Accelerated Ecological Monitoring in Mixed-Evergreen Redwood Forest

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-09 | Adam Korycki, Cory Yeaton, Gregory S. Gilbert, Colleen Josephson, Steve McGuire | cs.CV | [PDF](http://arxiv.org/pdf/2410.07418v2){: .btn .btn-green } |

**Abstract**: Forest mapping provides critical observational data needed to understand the
dynamics of forest environments. Notably, tree diameter at breast height (DBH)
is a metric used to estimate forest biomass and carbon dioxide sequestration.
Manual methods of forest mapping are labor intensive and time consuming, a
bottleneck for large-scale mapping efforts. Automated mapping relies on
acquiring dense forest reconstructions, typically in the form of point clouds.
Terrestrial laser scanning (TLS) and mobile laser scanning (MLS) generate point
clouds using expensive LiDAR sensing, and have been used successfully to
estimate tree diameter. Neural radiance fields (NeRFs) are an emergent
technology enabling photorealistic, vision-based reconstruction by training a
neural network on a sparse set of input views. In this paper, we present a
comparison of MLS and NeRF forest reconstructions for the purpose of trunk
diameter estimation in a mixed-evergreen Redwood forest. In addition, we
propose an improved DBH-estimation method using convex-hull modeling. Using
this approach, we achieved 1.68 cm RMSE, which consistently outperformed
standard cylinder modeling approaches. Our code contributions and forest
datasets are freely available at https://github.com/harelab-ucsc/RedwoodNeRF.



---

## 3D Representation Methods: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-09 | Zhengren Wang | cs.CV | [PDF](http://arxiv.org/pdf/2410.06475v1){: .btn .btn-green } |

**Abstract**: The field of 3D representation has experienced significant advancements,
driven by the increasing demand for high-fidelity 3D models in various
applications such as computer graphics, virtual reality, and autonomous
systems. This review examines the development and current state of 3D
representation methods, highlighting their research trajectories, innovations,
strength and weakness. Key techniques such as Voxel Grid, Point Cloud, Mesh,
Signed Distance Function (SDF), Neural Radiance Field (NeRF), 3D Gaussian
Splatting, Tri-Plane, and Deep Marching Tetrahedra (DMTet) are reviewed. The
review also introduces essential datasets that have been pivotal in advancing
the field, highlighting their characteristics and impact on research progress.
Finally, we explore potential research directions that hold promise for further
expanding the capabilities and applications of 3D representation methods.

Comments:
- Preliminary Draft

---

## Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D  Forest Stand Reconstruction and extraction of individual tree parameters

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-08 | Guoji Tian, Chongcheng Chen, Hongyu Huang | cs.CV | [PDF](http://arxiv.org/pdf/2410.05772v1){: .btn .btn-green } |

**Abstract**: Accurate and efficient 3D reconstruction of trees is crucial for forest
resource assessments and management. Close-Range Photogrammetry (CRP) is
commonly used for reconstructing forest scenes but faces challenges like low
efficiency and poor quality. Recently, Novel View Synthesis (NVS) technologies,
including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have
shown promise for 3D plant reconstruction with limited images. However,
existing research mainly focuses on small plants in orchards or individual
trees, leaving uncertainty regarding their application in larger, complex
forest stands. In this study, we collected sequential images of forest plots
with varying complexity and performed dense reconstruction using NeRF and 3DGS.
The resulting point clouds were compared with those from photogrammetry and
laser scanning. Results indicate that NVS methods significantly enhance
reconstruction efficiency. Photogrammetry struggles with complex stands,
leading to point clouds with excessive canopy noise and incorrectly
reconstructed trees, such as duplicated trunks. NeRF, while better for canopy
regions, may produce errors in ground areas with limited views. The 3DGS method
generates sparser point clouds, particularly in trunk areas, affecting diameter
at breast height (DBH) accuracy. All three methods can extract tree height
information, with NeRF yielding the highest accuracy; however, photogrammetry
remains superior for DBH accuracy. These findings suggest that NVS methods have
significant potential for 3D reconstruction of forest stands, offering valuable
support for complex forest resource inventory and visualization tasks.

Comments:
- 31page,15figures

---

## HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable  Sparse-View Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-08 | Shengji Tang, Weicai Ye, Peng Ye, Weihao Lin, Yang Zhou, Tao Chen, Wanli Ouyang | cs.CV | [PDF](http://arxiv.org/pdf/2410.06245v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D scenes from multiple viewpoints is a fundamental task in
stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have
enabled high-quality novel view synthesis for unseen scenes from sparse input
views by feed-forward predicting per-pixel Gaussian parameters without extra
optimization. However, existing methods typically generate single-scale 3D
Gaussians, which lack representation of both large-scale structure and texture
details, resulting in mislocation and artefacts. In this paper, we propose a
novel framework, HiSplat, which introduces a hierarchical manner in
generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via
a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained
Gaussians to capture large-scale structures, followed by fine-grained Gaussians
to enhance delicate texture details. To promote inter-scale interactions, we
propose an Error Aware Module for Gaussian compensation and a Modulating Fusion
Module for Gaussian repair. Our method achieves joint optimization of
hierarchical representations, allowing for novel view synthesis using only
two-view reference images. Comprehensive experiments on various datasets
demonstrate that HiSplat significantly enhances reconstruction quality and
cross-dataset generalization compared to prior single-scale methods. The
corresponding ablation study and analysis of different-scale 3D Gaussians
reveal the mechanism behind the effectiveness. Project website:
https://open3dvlab.github.io/HiSplat/



---

## GSLoc: Visual Localization with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-08 | Kazii Botashev, Vladislav Pyatov, Gonzalo Ferrer, Stamatios Lefkimmiatis | cs.RO | [PDF](http://arxiv.org/pdf/2410.06165v1){: .btn .btn-green } |

**Abstract**: We present GSLoc: a new visual localization method that performs dense camera
alignment using 3D Gaussian Splatting as a map representation of the scene.
GSLoc backpropagates pose gradients over the rendering pipeline to align the
rendered and target images, while it adopts a coarse-to-fine strategy by
utilizing blurring kernels to mitigate the non-convexity of the problem and
improve the convergence. The results show that our approach succeeds at visual
localization in challenging conditions of relatively small overlap between
initial and target frames inside textureless environments when state-of-the-art
neural sparse methods provide inferior results. Using the byproduct of
realistic rendering from the 3DGS map representation, we show how to enhance
localization results by mixing a set of observed and virtual reference
keyframes when solving the image retrieval problem. We evaluate our method both
on synthetic and real-world data, discussing its advantages and application
potential.



---

## SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-08 | Xinyi Liu, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi | cs.RO | [PDF](http://arxiv.org/pdf/2410.06014v1){: .btn .btn-green } |

**Abstract**: Many recent developments for robots to represent environments have focused on
photorealistic reconstructions. This paper particularly focuses on generating
sequences of images from the photorealistic Gaussian Splatting models, that
match instructions that are given by user-inputted language. We contribute a
novel framework, SplaTraj, which formulates the generation of images within
photorealistic environment representations as a continuous-time trajectory
optimization problem. Costs are designed so that a camera following the
trajectory poses will smoothly traverse through the environment and render the
specified spatial information in a photogenic manner. This is achieved by
querying a photorealistic representation with language embedding to isolate
regions that correspond to the user-specified inputs. These regions are then
projected to the camera's view as it moves over time and a cost is constructed.
We can then apply gradient-based optimization and differentiate through the
rendering to optimize the trajectory for the defined cost. The resulting
trajectory moves to photogenically view each of the specified objects. We
empirically evaluate our approach on a suite of environments and instructions,
and demonstrate the quality of generated image sequences.



---

## RelitLRM: Generative Relightable Radiance for Large Reconstruction  Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-08 | Tianyuan Zhang, Zhengfei Kuang, Haian Jin, Zexiang Xu, Sai Bi, Hao Tan, He Zhang, Yiwei Hu, Milos Hasan, William T. Freeman, Kai Zhang, Fujun Luan | cs.CV | [PDF](http://arxiv.org/pdf/2410.06231v2){: .btn .btn-green } |

**Abstract**: We propose RelitLRM, a Large Reconstruction Model (LRM) for generating
high-quality Gaussian splatting representations of 3D objects under novel
illuminations from sparse (4-8) posed images captured under unknown static
lighting. Unlike prior inverse rendering methods requiring dense captures and
slow optimization, often causing artifacts like incorrect highlights or shadow
baking, RelitLRM adopts a feed-forward transformer-based model with a novel
combination of a geometry reconstructor and a relightable appearance generator
based on diffusion. The model is trained end-to-end on synthetic multi-view
renderings of objects under varying known illuminations. This architecture
design enables to effectively decompose geometry and appearance, resolve the
ambiguity between material and lighting, and capture the multi-modal
distribution of shadows and specularity in the relit appearance. We show our
sparse-view feed-forward RelitLRM offers competitive relighting results to
state-of-the-art dense-view optimization-based baselines while being
significantly faster. Our project page is available at:
https://relit-lrm.github.io/.

Comments:
- webpage: https://relit-lrm.github.io/

---

## GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Yukang Cao, Masoud Hadi, Liang Pan, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2410.05259v1){: .btn .btn-green } |

**Abstract**: Diffusion-based 2D virtual try-on (VTON) techniques have recently
demonstrated strong performance, while the development of 3D VTON has largely
lagged behind. Despite recent advances in text-guided 3D scene editing,
integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains
challenging. The reasons are twofold. First, text prompts cannot provide
sufficient details in describing clothing. Second, 2D VTON results generated
from different viewpoints of the same 3D scene lack coherence and spatial
relationships, hence frequently leading to appearance inconsistencies and
geometric distortions. To resolve these problems, we introduce an
image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian
Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained
knowledge from 2D VTON models to 3D while improving cross-view consistency. (1)
Specifically, we propose a personalized diffusion model that utilizes low-rank
adaptation (LoRA) fine-tuning to incorporate personalized information into
pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a
reference-driven image editing approach that enables the simultaneous editing
of multi-view images while ensuring consistency. (2) Furthermore, we propose a
persona-aware 3DGS editing framework to facilitate effective editing while
maintaining consistent cross-view appearance and high-quality 3D geometry. (3)
Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which
facilitates comprehensive qualitative and quantitative 3D VTON evaluations.
Through extensive experiments and comparative analyses with existing methods,
the proposed \OM has demonstrated superior fidelity and advanced editing
capabilities, affirming its effectiveness for 3D VTON.

Comments:
- 21 pages, 11 figures

---

## PH-Dropout: Prctical Epistemic Uncertainty Quantification for View  Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Chuanhao Sun, Thanos Triantafyllou, Anthos Makris, Maja Drmač, Kai Xu, Luo Mai, Mahesh K. Marina | cs.CV | [PDF](http://arxiv.org/pdf/2410.05468v1){: .btn .btn-green } |

**Abstract**: View synthesis using Neural Radiance Fields (NeRF) and Gaussian Splatting
(GS) has demonstrated impressive fidelity in rendering real-world scenarios.
However, practical methods for accurate and efficient epistemic Uncertainty
Quantification (UQ) in view synthesis are lacking. Existing approaches for NeRF
either introduce significant computational overhead (e.g., ``10x increase in
training time" or ``10x repeated training") or are limited to specific
uncertainty conditions or models. Notably, GS models lack any systematic
approach for comprehensive epistemic UQ. This capability is crucial for
improving the robustness and scalability of neural view synthesis, enabling
active model updates, error estimation, and scalable ensemble modeling based on
uncertainty. In this paper, we revisit NeRF and GS-based methods from a
function approximation perspective, identifying key differences and connections
in 3D representation learning. Building on these insights, we introduce
PH-Dropout (Post hoc Dropout), the first real-time and accurate method for
epistemic uncertainty estimation that operates directly on pre-trained NeRF and
GS models. Extensive evaluations validate our theoretical findings and
demonstrate the effectiveness of PH-Dropout.

Comments:
- 21 pages, in submision

---

## PhotoReg: Photometrically Registering 3D Gaussian Splatting Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Ziwen Yuan, Tianyi Zhang, Matthew Johnson-Roberson, Weiming Zhi | cs.RO | [PDF](http://arxiv.org/pdf/2410.05044v1){: .btn .btn-green } |

**Abstract**: Building accurate representations of the environment is critical for
intelligent robots to make decisions during deployment. Advances in
photorealistic environment models have enabled robots to develop
hyper-realistic reconstructions, which can be used to generate images that are
intuitive for human inspection. In particular, the recently introduced
\ac{3DGS}, which describes the scene with up to millions of primitive
ellipsoids, can be rendered in real time. \ac{3DGS} has rapidly gained
prominence. However, a critical unsolved problem persists: how can we fuse
multiple \ac{3DGS} into a single coherent model? Solving this problem will
enable robot teams to jointly build \ac{3DGS} models of their surroundings. A
key insight of this work is to leverage the {duality} between photorealistic
reconstructions, which render realistic 2D images from 3D structure, and
\emph{3D foundation models}, which predict 3D structure from image pairs. To
this end, we develop PhotoReg, a framework to register multiple photorealistic
\ac{3DGS} models with 3D foundation models. As \ac{3DGS} models are generally
built from monocular camera images, they have \emph{arbitrary scale}. To
resolve this, PhotoReg actively enforces scale consistency among the different
\ac{3DGS} models by considering depth estimates within these models. Then, the
alignment is iteratively refined with fine-grained photometric losses to
produce high-quality fused \ac{3DGS} models. We rigorously evaluate PhotoReg on
both standard benchmark datasets and our custom-collected datasets, including
with two quadruped robots. The code is released at
\url{ziweny11.github.io/photoreg}.



---

## LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Qifeng Chen, Sheng Yang, Sicong Du, Tao Tang, Peng Chen, Yuchi Huo | cs.CV | [PDF](http://arxiv.org/pdf/2410.05111v1){: .btn .btn-green } |

**Abstract**: LiDAR simulation plays a crucial role in closed-loop simulation for
autonomous driving. Although recent advancements, such as the use of
reconstructed mesh and Neural Radiance Fields (NeRF), have made progress in
simulating the physical properties of LiDAR, these methods have struggled to
achieve satisfactory frame rates and rendering quality. To address these
limitations, we present LiDAR-GS, the first LiDAR Gaussian Splatting method,
for real-time high-fidelity re-simulation of LiDAR sensor scans in public urban
road scenes. The vanilla Gaussian Splatting, designed for camera models, cannot
be directly applied to LiDAR re-simulation. To bridge the gap between passive
camera and active LiDAR, our LiDAR-GS designs a differentiable laser beam
splatting, grounded in the LiDAR range view model. This innovation allows for
precise surface splatting by projecting lasers onto micro cross-sections,
effectively eliminating artifacts associated with local affine approximations.
Additionally, LiDAR-GS leverages Neural Gaussian Fields, which further
integrate view-dependent clues, to represent key LiDAR properties that are
influenced by the incident angle and external factors. Combining these
practices with some essential adaptations, e.g., dynamic instances
decomposition, our approach succeeds in simultaneously re-simulating depth,
intensity, and ray-drop channels, achieving state-of-the-art results in both
rendering frame rate and quality on publically available large scene datasets.
Our source code will be made publicly available.



---

## DreamSat: Towards a General 3D Model for Novel View Synthesis of Space  Objects


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Nidhi Mathihalli, Audrey Wei, Giovanni Lavezzi, Peng Mun Siew, Victor Rodriguez-Fernandez, Hodei Urrutxua, Richard Linares | cs.CV | [PDF](http://arxiv.org/pdf/2410.05097v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) enables to generate new images of a scene or
convert a set of 2D images into a comprehensive 3D model. In the context of
Space Domain Awareness, since space is becoming increasingly congested, NVS can
accurately map space objects and debris, improving the safety and efficiency of
space operations. Similarly, in Rendezvous and Proximity Operations missions,
3D models can provide details about a target object's shape, size, and
orientation, allowing for better planning and prediction of the target's
behavior. In this work, we explore the generalization abilities of these
reconstruction techniques, aiming to avoid the necessity of retraining for each
new scene, by presenting a novel approach to 3D spacecraft reconstruction from
single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art
single-view reconstruction model, on a high-quality dataset of 190 high-quality
spacecraft models and integrating it into the DreamGaussian framework. We
demonstrate consistent improvements in reconstruction quality across multiple
metrics, including Contrastive Language-Image Pretraining (CLIP) score
(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity
Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)
(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method
addresses the lack of domain-specific 3D reconstruction tools in the space
industry by leveraging state-of-the-art diffusion models and 3D Gaussian
splatting techniques. This approach maintains the efficiency of the
DreamGaussian framework while enhancing the accuracy and detail of spacecraft
reconstructions. The code for this work can be accessed on GitHub
(https://github.com/ARCLab-MIT/space-nvs).

Comments:
- Presented at the 75th International Astronautical Congress, October
  2024, Milan, Italy

---

## Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Matthew Strong, Boshu Lei, Aiden Swann, Wen Jiang, Kostas Daniilidis, Monroe Kennedy III | cs.RO | [PDF](http://arxiv.org/pdf/2410.04680v1){: .btn .btn-green } |

**Abstract**: We propose a framework for active next best view and touch selection for
robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a
useful explicit 3D scene representation for robotics, as it has the ability to
represent scenes in a both photorealistic and geometrically accurate manner.
However, in real-world, online robotic scenes where the number of views is
limited given efficiency requirements, random view selection for 3DGS becomes
impractical as views are often overlapping and redundant. We address this issue
by proposing an end-to-end online training and active view selection pipeline,
which enhances the performance of 3DGS in few-view robotics settings. We first
elevate the performance of few-shot 3DGS with a novel semantic depth alignment
method using Segment Anything Model 2 (SAM2) that we supplement with Pearson
depth and surface normal loss to improve color and depth reconstruction of
real-world scenes. We then extend FisherRF, a next-best-view selection method
for 3DGS, to select views and touch poses based on depth uncertainty. We
perform online view selection on a real robot system during live 3DGS training.
We motivate our improvements to few-shot GS scenes, and extend depth-based
FisherRF to them, where we demonstrate both qualitative and quantitative
improvements on challenging robot scenes. For more information, please see our
project page at https://armlabstanford.github.io/next-best-sense.



---

## 6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric  Rendering

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2410.04974v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis has advanced significantly with the development of
neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,
achieving high quality without compromising real-time rendering remains
challenging, particularly for physically-based ray tracing with view-dependent
effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D
spatial-angular representation to better incorporate view-dependent effects,
but the Gaussian representation and control scheme are sub-optimal. In this
paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),
which enhances color and opacity representations and leverages the additional
directional information in the 6D space for optimized Gaussian control. Our
approach is fully compatible with the 3DGS framework and significantly improves
real-time radiance field rendering by better modeling view-dependent effects
and fine details. Experiments demonstrate that 6DGS significantly outperforms
3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction
of 66.5% Gaussian points compared to 3DGS. The project page is:
https://gaozhongpai.github.io/6dgs/

Comments:
- Project: https://gaozhongpai.github.io/6dgs/ and fixed iteration
  typos

---

## TeX-NeRF: Neural Radiance Fields from Pseudo-TeX Vision

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Chonghao Zhong, Chao Xu | cs.CV | [PDF](http://arxiv.org/pdf/2410.04873v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) has gained significant attention for its
exceptional visual effects. However, most existing NeRF methods reconstruct 3D
scenes from RGB images captured by visible light cameras. In practical
scenarios like darkness, low light, or bad weather, visible light cameras
become ineffective. Therefore, we propose TeX-NeRF, a 3D reconstruction method
using only infrared images, which introduces the object material emissivity as
a priori, preprocesses the infrared images using Pseudo-TeX vision, and maps
the temperatures (T), emissivities (e), and textures (X) of the scene into the
saturation (S), hue (H), and value (V) channels of the HSV color space,
respectively. Novel view synthesis using the processed images has yielded
excellent results. Additionally, we introduce 3D-TeX Datasets, the first
dataset comprising infrared images and their corresponding Pseudo-TeX vision
images. Experiments demonstrate that our method not only matches the quality of
scene reconstruction achieved with high-quality RGB images but also provides
accurate temperature estimations for objects in the scene.



---

## Toward General Object-level Mapping from Sparse Views with 3D Diffusion  Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-07 | Ziwei Liao, Binbin Xu, Steven L. Waslander | cs.CV | [PDF](http://arxiv.org/pdf/2410.05514v1){: .btn .btn-green } |

**Abstract**: Object-level mapping builds a 3D map of objects in a scene with detailed
shapes and poses from multi-view sensor observations. Conventional methods
struggle to build complete shapes and estimate accurate poses due to partial
occlusions and sensor noise. They require dense observations to cover all
objects, which is challenging to achieve in robotics trajectories. Recent work
introduces generative shape priors for object-level mapping from sparse views,
but is limited to single-category objects. In this work, we propose a General
Object-level Mapping system, GOM, which leverages a 3D diffusion model as shape
prior with multi-category support and outputs Neural Radiance Fields (NeRFs)
for both texture and geometry for all objects in a scene. GOM includes an
effective formulation to guide a pre-trained diffusion model with extra
nonlinear constraints from sensor measurements without finetuning. We also
develop a probabilistic optimization formulation to fuse multi-view sensor
observations and diffusion priors for joint 3D object pose and shape
estimation. Our GOM system demonstrates superior multi-category mapping
performance from sparse views, and achieves more accurate mapping results
compared to state-of-the-art methods on the real-world benchmarks. We will
release our code: https://github.com/TRAILab/GeneralObjectMapping.

Comments:
- Accepted by CoRL 2024

---

## Deformable NeRF using Recursively Subdivided Tetrahedra

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-06 | Zherui Qiu, Chenqu Ren, Kaiwen Song, Xiaoyi Zeng, Leyuan Yang, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2410.04402v1){: .btn .btn-green } |

**Abstract**: While neural radiance fields (NeRF) have shown promise in novel view
synthesis, their implicit representation limits explicit control over object
manipulation. Existing research has proposed the integration of explicit
geometric proxies to enable deformation. However, these methods face two
primary challenges: firstly, the time-consuming and computationally demanding
tetrahedralization process; and secondly, handling complex or thin structures
often leads to either excessive, storage-intensive tetrahedral meshes or
poor-quality ones that impair deformation capabilities. To address these
challenges, we propose DeformRF, a method that seamlessly integrates the
manipulability of tetrahedral meshes with the high-quality rendering
capabilities of feature grid representations. To avoid ill-shaped tetrahedra
and tetrahedralization for each object, we propose a two-stage training
strategy. Starting with an almost-regular tetrahedral grid, our model initially
retains key tetrahedra surrounding the object and subsequently refines object
details using finer-granularity mesh in the second stage. We also present the
concept of recursively subdivided tetrahedra to create higher-resolution meshes
implicitly. This enables multi-resolution encoding while only necessitating the
storage of the coarse tetrahedral mesh generated in the first training stage.
We conduct a comprehensive evaluation of our DeformRF on both synthetic and
real-captured datasets. Both quantitative and qualitative results demonstrate
the effectiveness of our method for novel view synthesis and deformation tasks.
Project page: https://ustc3dv.github.io/DeformRF/

Comments:
- Accepted by ACM Multimedia 2024. Project Page:
  https://ustc3dv.github.io/DeformRF/

---

## Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for  Robust Ground-View Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-06 | Yonghan Lee, Jaehoon Choi, Dongki Jung, Jaeseong Yun, Soohyun Ryu, Dinesh Manocha, Suyong Yeon | cs.CV | [PDF](http://arxiv.org/pdf/2410.04646v1){: .btn .btn-green } |

**Abstract**: We present a novel-view rendering algorithm, Mode-GS, for ground-robot
trajectory datasets. Our approach is based on using anchored Gaussian splats,
which are designed to overcome the limitations of existing 3D Gaussian
splatting algorithms. Prior neural rendering methods suffer from severe splat
drift due to scene complexity and insufficient multi-view observation, and can
fail to fix splats on the true geometry in ground-robot datasets. Our method
integrates pixel-aligned anchors from monocular depths and generates Gaussian
splats around these anchors using residual-form Gaussian decoders. To address
the inherent scale ambiguity of monocular depth, we parameterize anchors with
per-view depth-scales and employ scale-consistent depth loss for online scale
calibration. Our method results in improved rendering performance, based on
PSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns,
and achieves state-of-the-art rendering performance on the R3LIVE odometry
dataset and the Tanks and Temples dataset.



---

## StreetSurfGS: Scalable Urban Street Surface Reconstruction with  Planar-based Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-06 | Xiao Cui, Weicai Ye, Yifan Wang, Guofeng Zhang, Wengang Zhou, Tong He, Houqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2410.04354v1){: .btn .btn-green } |

**Abstract**: Reconstructing urban street scenes is crucial due to its vital role in
applications such as autonomous driving and urban planning. These scenes are
characterized by long and narrow camera trajectories, occlusion, complex object
relationships, and data sparsity across multiple scales. Despite recent
advancements, existing surface reconstruction methods, which are primarily
designed for object-centric scenarios, struggle to adapt effectively to the
unique characteristics of street scenes. To address this challenge, we
introduce StreetSurfGS, the first method to employ Gaussian Splatting
specifically tailored for scalable urban street scene surface reconstruction.
StreetSurfGS utilizes a planar-based octree representation and segmented
training to reduce memory costs, accommodate unique camera characteristics, and
ensure scalability. Additionally, to mitigate depth inaccuracies caused by
object overlap, we propose a guided smoothing strategy within regularization to
eliminate inaccurate boundary points and outliers. Furthermore, to address
sparse views and multi-scale challenges, we use a dual-step matching strategy
that leverages adjacent and long-term information. Extensive experiments
validate the efficacy of StreetSurfGS in both novel view synthesis and surface
reconstruction.



---

## Hybrid NeRF-Stereo Vision: Pioneering Depth Estimation and 3D  Reconstruction in Endoscopy

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-05 | Pengcheng Chen, Wenhao Li, Nicole Gunderson, Jeremy Ruthberg, Randall Bly, Waleed M. Abuzeid, Zhenglong Sun, Eric J. Seibel | eess.IV | [PDF](http://arxiv.org/pdf/2410.04041v1){: .btn .btn-green } |

**Abstract**: The 3D reconstruction of the surgical field in minimally invasive endoscopic
surgery has posed a formidable challenge when using conventional monocular
endoscopes. Existing 3D reconstruction methodologies are frequently encumbered
by suboptimal accuracy and limited generalization capabilities. In this study,
we introduce an innovative pipeline using Neural Radiance Fields (NeRF) for 3D
reconstruction. Our approach utilizes a preliminary NeRF reconstruction that
yields a coarse model, then creates a binocular scene within the reconstructed
environment, which derives an initial depth map via stereo vision. This initial
depth map serves as depth supervision for subsequent NeRF iterations,
progressively refining the 3D reconstruction with enhanced accuracy. The
binocular depth is iteratively recalculated, with the refinement process
continuing until the depth map converges, and exhibits negligible variations.
Through this recursive process, high-fidelity depth maps are generated from
monocular endoscopic video of a realistic cranial phantom. By repeated measures
of the final 3D reconstruction compared to X-ray computed tomography, all
differences of relevant clinical distances result in sub-millimeter accuracy.



---

## Variational Bayes Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-04 | Toon Van de Maele, Ozan Catal, Alexander Tschantz, Christopher L. Buckley, Tim Verbelen | cs.CV | [PDF](http://arxiv.org/pdf/2410.03592v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting has emerged as a promising approach for
modeling 3D scenes using mixtures of Gaussians. The predominant optimization
method for these models relies on backpropagating gradients through a
differentiable rendering pipeline, which struggles with catastrophic forgetting
when dealing with continuous streams of data. To address this limitation, we
propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that
frames training a Gaussian splat as variational inference over model
parameters. By leveraging the conjugacy properties of multivariate Gaussians,
we derive a closed-form variational update rule, allowing efficient updates
from partial, sequential observations without the need for replay buffers. Our
experiments show that VBGS not only matches state-of-the-art performance on
static datasets, but also enables continual learning from sequentially streamed
2D and 3D data, drastically improving performance in this setting.



---

## SuperGS: Super-Resolution 3D Gaussian Splatting via Latent Feature Field  and Gradient-guided Splitting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-03 | Shiyun Xie, Zhiru Wang, Yinghao Zhu, Chengwei Pan | cs.CV | [PDF](http://arxiv.org/pdf/2410.02571v2){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis
with its real-time rendering capabilities and superior quality. However, it
faces challenges for high-resolution novel view synthesis (HRNVS) due to the
coarse nature of primitives derived from low-resolution input views. To address
this issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion
of 3DGS designed with a two-stage coarse-to-fine training framework, utilizing
pretrained low-resolution scene representation as an initialization for
super-resolution optimization. Moreover, we introduce Multi-resolution Feature
Gaussian Splatting (MFGS) to incorporates a latent feature field for flexible
feature sampling and Gradient-guided Selective Splitting (GSS) for effective
Gaussian upsampling. By integrating these strategies within the coarse-to-fine
framework ensure both high fidelity and memory efficiency. Extensive
experiments demonstrate that SuperGS surpasses state-of-the-art HRNVS methods
on challenging real-world datasets using only low-resolution inputs.



---

## GI-GS: Global Illumination Decomposition on Gaussian Splatting for  Inverse Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-03 | Hongze Chen, Zehong Lin, Jun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2410.02619v1){: .btn .btn-green } |

**Abstract**: We present GI-GS, a novel inverse rendering framework that leverages 3D
Gaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel
view synthesis and relighting. In inverse rendering, accurately modeling the
shading processes of objects is essential for achieving high-fidelity results.
Therefore, it is critical to incorporate global illumination to account for
indirect lighting that reaches an object after multiple bounces across the
scene. Previous 3DGS-based methods have attempted to model indirect lighting by
characterizing indirect illumination as learnable lighting volumes or
additional attributes of each Gaussian, while using baked occlusion to
represent shadow effects. These methods, however, fail to accurately model the
complex physical interactions between light and objects, making it impossible
to construct realistic indirect illumination during relighting. To address this
limitation, we propose to calculate indirect lighting using efficient path
tracing with deferred shading. In our framework, we first render a G-buffer to
capture the detailed geometry and material properties of the scene. Then, we
perform physically-based rendering (PBR) only for direct lighting. With the
G-buffer and previous rendering results, the indirect lighting can be
calculated through a lightweight path tracing. Our method effectively models
indirect lighting under any given lighting conditions, thereby achieving better
novel view synthesis and relighting. Quantitative and qualitative results show
that our GI-GS outperforms existing baselines in both rendering quality and
efficiency.



---

## Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-03 | Mingyang Xie, Haoming Cai, Sachin Shah, Yiran Xu, Brandon Y. Feng, Jia-Bin Huang, Christopher A. Metzler | cs.CV | [PDF](http://arxiv.org/pdf/2410.02764v1){: .btn .btn-green } |

**Abstract**: We introduce a simple yet effective approach for separating transmitted and
reflected light. Our key insight is that the powerful novel view synthesis
capabilities provided by modern inverse rendering methods (e.g.,~3D Gaussian
splatting) allow one to perform flash/no-flash reflection separation using
unpaired measurements -- this relaxation dramatically simplifies image
acquisition over conventional paired flash/no-flash reflection separation
methods. Through extensive real-world experiments, we demonstrate our method,
Flash-Splat, accurately reconstructs both transmitted and reflected scenes in
3D. Our method outperforms existing 3D reflection separation methods, which do
not leverage illumination control, by a large margin. Our project webpage is at
https://flash-splat.github.io/.



---

## Gaussian Splatting in Mirrors: Reflection-Aware Rendering via Virtual  Camera Optimization

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Zihan Wang, Shuzhe Wang, Matias Turkulainen, Junyuan Fang, Juho Kannala | cs.CV | [PDF](http://arxiv.org/pdf/2410.01614v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3D-GS) have revolutionized
novel view synthesis, facilitating real-time, high-quality image rendering.
However, in scenarios involving reflective surfaces, particularly mirrors,
3D-GS often misinterprets reflections as virtual spaces, resulting in blurred
and inconsistent multi-view rendering within mirrors. Our paper presents a
novel method aimed at obtaining high-quality multi-view consistent reflection
rendering by modelling reflections as physically-based virtual cameras. We
estimate mirror planes with depth and normal estimates from 3D-GS and define
virtual cameras that are placed symmetrically about the mirror plane. These
virtual cameras are then used to explain mirror reflections in the scene. To
address imperfections in mirror plane estimates, we propose a straightforward
yet effective virtual camera optimization method to enhance reflection quality.
We collect a new mirror dataset including three real-world scenarios for more
diverse evaluation. Experimental validation on both Mirror-Nerf and our
real-world dataset demonstrate the efficacy of our approach. We achieve
comparable or superior results while significantly reducing training time
compared to previous state-of-the-art.

Comments:
- To be published on 2024 British Machine Vision Conference

---

## Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Hongru Yan, Yu Zheng, Yueqi Duan | cs.CV | [PDF](http://arxiv.org/pdf/2410.01404v1){: .btn .btn-green } |

**Abstract**: Skins wrapping around our bodies, leathers covering over the sofa, sheet
metal coating the car - it suggests that objects are enclosed by a series of
continuous surfaces, which provides us with informative geometry prior for
objectness deduction. In this paper, we propose Gaussian-Det which leverages
Gaussian Splatting as surface representation for multi-view based 3D object
detection. Unlike existing monocular or NeRF-based methods which depict the
objects via discrete positional data, Gaussian-Det models the objects in a
continuous manner by formulating the input Gaussians as feature descriptors on
a mass of partial surfaces. Furthermore, to address the numerous outliers
inherently introduced by Gaussian splatting, we accordingly devise a Closure
Inferring Module (CIM) for the comprehensive surface-based objectness
deduction. CIM firstly estimates the probabilistic feature residuals for
partial surfaces given the underdetermined nature of Gaussian Splatting, which
are then coalesced into a holistic representation on the overall surface
closure of the object proposal. In this way, the surface information
Gaussian-Det exploits serves as the prior on the quality and reliability of
objectness and the information basis of proposal refinement. Experiments on
both synthetic and real-world datasets demonstrate that Gaussian-Det
outperforms various existing approaches, in terms of both average precision and
recall.



---

## EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jonathan T. Barron, Yinda Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2410.01804v3){: .btn .btn-green } |

**Abstract**: We present Exact Volumetric Ellipsoid Rendering (EVER), a method for
real-time differentiable emission-only volume rendering. Unlike recent
rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive
based representation allows for exact volume rendering, rather than alpha
compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does
not suffer from popping artifacts and view dependent density, but still
achieves frame rates of $\sim\!30$ FPS at 720p on an NVIDIA RTX4090. Since our
approach is built upon ray tracing it enables effects such as defocus blur and
camera distortion (e.g. such as from fisheye cameras), which are difficult to
achieve by rasterization. We show that our method is more accurate with fewer
blending issues than 3DGS and follow-up work on view-consistent rendering,
especially on the challenging large-scale scenes from the Zip-NeRF dataset
where it achieves sharpest results among real-time techniques.

Comments:
- Project page: https://half-potato.gitlab.io/posts/ever

---

## MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Xiaobiao Du, Yida Wang, Xin Yu | cs.CV | [PDF](http://arxiv.org/pdf/2410.02103v1){: .btn .btn-green } |

**Abstract**: Recent works in volume rendering, \textit{e.g.} NeRF and 3D Gaussian
Splatting (3DGS), significantly advance the rendering quality and efficiency
with the help of the learned implicit neural radiance field or 3D Gaussians.
Rendering on top of an explicit representation, the vanilla 3DGS and its
variants deliver real-time efficiency by optimizing the parametric model with
single-view supervision per iteration during training which is adopted from
NeRF. Consequently, certain views are overfitted, leading to unsatisfying
appearance in novel-view synthesis and imprecise 3D geometries. To solve
aforementioned problems, we propose a new 3DGS optimization method embodying
four key novel contributions: 1) We transform the conventional single-view
training paradigm into a multi-view training strategy. With our proposed
multi-view regulation, 3D Gaussian attributes are further optimized without
overfitting certain training views. As a general solution, we improve the
overall accuracy in a variety of scenarios and different Gaussian variants. 2)
Inspired by the benefit introduced by additional views, we further propose a
cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure
concerning different resolutions. 3) Built on top of our multi-view regulated
training, we further propose a cross-ray densification strategy, densifying
more Gaussian kernels in the ray-intersect regions from a selection of views.
4) By further investigating the densification strategy, we found that the
effect of densification should be enhanced when certain views are distinct
dramatically. As a solution, we propose a novel multi-view augmented
densification strategy, where 3D Gaussians are encouraged to get densified to a
sufficient number accordingly, resulting in improved reconstruction accuracy.

Comments:
- Project Page:https://xiaobiaodu.github.io/mvgs-project/

---

## 3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and  Box-Focused Sampling for 3D Object Detection

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Yang Cao, Yuanliang Jv, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2410.01647v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and
have been adapted for 3D Object Detection (3DOD), offering a promising approach
to 3DOD through view-synthesis representation. However, NeRF faces inherent
limitations: (i) limited representational capacity for 3DOD due to its implicit
nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS)
has emerged as an explicit 3D representation that addresses these limitations.
Inspired by these advantages, this paper introduces 3DGS into 3DOD for the
first time, identifying two main challenges: (i) Ambiguous spatial distribution
of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision,
resulting in unclear 3D spatial distribution of Gaussian blobs and poor
differentiation between objects and background, which hinders 3DOD; (ii)
Excessive background blobs: 2D images often include numerous background pixels,
leading to densely reconstructed 3DGS with many noisy Gaussian blobs
representing the background, negatively affecting detection. To tackle the
challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D
images, and propose an elegant and efficient solution by incorporating 2D
Boundary Guidance to significantly enhance the spatial distribution of Gaussian
blobs, resulting in clearer differentiation between objects and their
background. To address the challenge (ii), we propose a Box-Focused Sampling
strategy using 2D boxes to generate object probability distribution in 3D
spaces, allowing effective probabilistic sampling in 3D to retain more object
blobs and reduce noisy background blobs. Benefiting from our designs, our
3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det,
achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet
dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.

Comments:
- Code Page: https://github.com/yangcaoai/3DGS-DET

---

## UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Haoran Wang, Nantheera Anantrasirichai, Fan Zhang, David Bull | cs.CV | [PDF](http://arxiv.org/pdf/2410.01517v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) offers the capability to achieve real-time high
quality 3D scene rendering. However, 3DGS assumes that the scene is in a clear
medium environment and struggles to generate satisfactory representations in
underwater scenes, where light absorption and scattering are prevalent and
moving objects are involved. To overcome these, we introduce a novel Gaussian
Splatting-based method, UW-GS, designed specifically for underwater
applications. It introduces a color appearance that models distance-dependent
color variation, employs a new physics-based density control strategy to
enhance clarity for distant objects, and uses a binary motion mask to handle
dynamic content. Optimized with a well-designed loss function supporting for
scattering media and strengthened by pseudo-depth maps, UW-GS outperforms
existing methods with PSNR gains up to 1.26dB. To fully verify the
effectiveness of the model, we also developed a new underwater dataset, S-UW,
with dynamic object masks.



---

## GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene  by Primitives and Gaussians


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Shuyi Jiang, Qihao Zhao, Hossein Rahmani, De Wen Soh, Jun Liu, Na Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2410.01535v2){: .btn .btn-green } |

**Abstract**: Recently, with the development of Neural Radiance Fields and Gaussian
Splatting, 3D reconstruction techniques have achieved remarkably high fidelity.
However, the latent representations learnt by these methods are highly
entangled and lack interpretability. In this paper, we propose a novel
part-aware compositional reconstruction method, called GaussianBlock, that
enables semantically coherent and disentangled representations, allowing for
precise and physical editing akin to building blocks, while simultaneously
maintaining high fidelity. Our GaussianBlock introduces a hybrid representation
that leverages the advantages of both primitives, known for their flexible
actionability and editability, and 3D Gaussians, which excel in reconstruction
quality. Specifically, we achieve semantically coherent primitives through a
novel attention-guided centering loss derived from 2D semantic priors,
complemented by a dynamic splitting and fusion strategy. Furthermore, we
utilize 3D Gaussians that hybridize with primitives to refine structural
details and enhance fidelity. Additionally, a binding inheritance strategy is
employed to strengthen and maintain the connection between the two. Our
reconstructed scenes are evidenced to be disentangled, compositional, and
compact across diverse benchmarks, enabling seamless, direct and precise
editing while maintaining high quality.



---

## EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis  under Diverse Camera Settings

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Yingdong Hu, Zhening Liu, Jiawei Shao, Zehong Lin, Jun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2410.01425v1){: .btn .btn-green } |

**Abstract**: The feed-forward based 3D Gaussian Splatting method has demonstrated
exceptional capability in real-time human novel view synthesis. However,
existing approaches are restricted to dense viewpoint settings, which limits
their flexibility in free-viewpoint rendering across a wide range of camera
view angle discrepancies. To address this limitation, we propose a real-time
pipeline named EVA-Gaussian for 3D human novel view synthesis across diverse
camera settings. Specifically, we first introduce an Efficient cross-View
Attention (EVA) module to accurately estimate the position of each 3D Gaussian
from the source images. Then, we integrate the source images with the estimated
Gaussian position map to predict the attributes and feature embeddings of the
3D Gaussians. Moreover, we employ a recurrent feature refiner to correct
artifacts caused by geometric errors in position estimation and enhance visual
fidelity.To further improve synthesis quality, we incorporate a powerful anchor
loss function for both 3D Gaussian attributes and human face landmarks.
Experimental results on the THuman2.0 and THumansit datasets showcase the
superiority of our EVA-Gaussian approach in rendering quality across diverse
camera settings. Project page:
https://zhenliuzju.github.io/huyingdong/EVA-Gaussian.



---

## MiraGe: Editable 2D Images using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-02 | Joanna Waczyńska, Tomasz Szczepanik, Piotr Borycki, Sławomir Tadeja, Thomas Bohné, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2410.01521v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representations (INRs) approximate discrete data through
continuous functions and are commonly used for encoding 2D images. Traditional
image-based INRs employ neural networks to map pixel coordinates to RGB values,
capturing shapes, colors, and textures within the network's weights. Recently,
GaussianImage has been proposed as an alternative, using Gaussian functions
instead of neural networks to achieve comparable quality and compression. Such
a solution obtains a quality and compression ratio similar to classical INR
models but does not allow image modification. In contrast, our work introduces
a novel method, MiraGe, which uses mirror reflections to perceive 2D images in
3D space and employs flat-controlled Gaussians for precise 2D image editing.
Our approach improves the rendering quality and allows realistic image
modifications, including human-inspired perception of photos in the 3D world.
Thanks to modeling images in 3D space, we obtain the illusion of 3D-based
modification in 2D images. We also show that our Gaussian representation can be
easily combined with a physics engine to produce physics-based modification of
2D images. Consequently, MiraGe allows for better quality than the standard
approach and natural modification of 2D images.



---

## Seamless Augmented Reality Integration in Arthroscopy: A Pipeline for  Articular Reconstruction and Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-01 | Hongchao Shu, Mingxu Liu, Lalithkumar Seenivasan, Suxi Gu, Ping-Cheng Ku, Jonathan Knopf, Russell Taylor, Mathias Unberath | cs.CV | [PDF](http://arxiv.org/pdf/2410.00386v1){: .btn .btn-green } |

**Abstract**: Arthroscopy is a minimally invasive surgical procedure used to diagnose and
treat joint problems. The clinical workflow of arthroscopy typically involves
inserting an arthroscope into the joint through a small incision, during which
surgeons navigate and operate largely by relying on their visual assessment
through the arthroscope. However, the arthroscope's restricted field of view
and lack of depth perception pose challenges in navigating complex articular
structures and achieving surgical precision during procedures. Aiming at
enhancing intraoperative awareness, we present a robust pipeline that
incorporates simultaneous localization and mapping, depth estimation, and 3D
Gaussian splatting to realistically reconstruct intra-articular structures
solely based on monocular arthroscope video. Extending 3D reconstruction to
Augmented Reality (AR) applications, our solution offers AR assistance for
articular notch measurement and annotation anchoring in a human-in-the-loop
manner. Compared to traditional Structure-from-Motion and Neural Radiance
Field-based methods, our pipeline achieves dense 3D reconstruction and
competitive rendering fidelity with explicit 3D representation in 7 minutes on
average. When evaluated on four phantom datasets, our method achieves RMSE =
2.21mm reconstruction error, PSNR = 32.86 and SSIM = 0.89 on average. Because
our pipeline enables AR reconstruction and guidance directly from monocular
arthroscopy without any additional data and/or hardware, our solution may hold
the potential for enhancing intraoperative awareness and facilitating surgical
precision in arthroscopy. Our AR measurement tool achieves accuracy within 1.59
+/- 1.81mm and the AR annotation tool achieves a mIoU of 0.721.

Comments:
- 8 pages, with 2 additional pages as the supplementary. Accepted by
  AE-CAI 2024

---

## CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-01 | Dapeng Feng, Zhiqiang Chen, Yizhen Yin, Shipeng Zhong, Yuhua Qi, Hongbo Chen | cs.CV | [PDF](http://arxiv.org/pdf/2410.00486v2){: .btn .btn-green } |

**Abstract**: Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with
photorealistic scene reconstruction emerging as a key challenge. To address
this, we introduce Computational Alignment for Real-Time Gaussian Splatting
SLAM (CaRtGS), a novel method enhancing the efficiency and quality of
photorealistic scene reconstruction in real-time environments. Leveraging 3D
Gaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and
processing speed, which is crucial for scene photorealistic reconstruction. Our
approach tackles computational misalignment in Gaussian Splatting SLAM
(GS-SLAM) through an adaptive strategy that optimizes training, addresses
long-tail optimization, and refines densification. Experiments on Replica and
TUM-RGBD datasets demonstrate CaRtGS's effectiveness in achieving high-fidelity
rendering with fewer Gaussian primitives. This work propels SLAM towards
real-time, photorealistic dense rendering, significantly advancing
photorealistic scene representation. For the benefit of the research community,
we release the code on our project website:
https://dapengfeng.github.io/cartgs.

Comments:
- Upon a thorough internal review, we have identified that our
  manuscript lacks proper citation for a critical expression within the
  methodology section. In this revised version, we add Taming-3DGS as a
  citation in the splat-wise backpropagation statement

---

## GSPR: Multimodal Place Recognition Using 3D Gaussian Splatting for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-01 | Zhangshuo Qi, Junyi Ma, Jingyi Xu, Zijie Zhou, Luqi Cheng, Guangming Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2410.00299v1){: .btn .btn-green } |

**Abstract**: Place recognition is a crucial module to ensure autonomous vehicles obtain
usable localization information in GPS-denied environments. In recent years,
multimodal place recognition methods have gained increasing attention due to
their ability to overcome the weaknesses of unimodal sensor systems by
leveraging complementary information from different modalities. However,
challenges arise from the necessity of harmonizing data across modalities and
exploiting the spatio-temporal correlations between them sufficiently. In this
paper, we propose a 3D Gaussian Splatting-based multimodal place recognition
neural network dubbed GSPR. It explicitly combines multi-view RGB images and
LiDAR point clouds into a spatio-temporally unified scene representation with
the proposed Multimodal Gaussian Splatting. A network composed of 3D graph
convolution and transformer is designed to extract high-level spatio-temporal
features and global descriptors from the Gaussian scenes for place recognition.
We evaluate our method on the nuScenes dataset, and the experimental results
demonstrate that our method can effectively leverage complementary strengths of
both multi-view cameras and LiDAR, achieving SOTA place recognition performance
while maintaining solid generalization ability. Our open-source code is
available at https://github.com/QiZS-BIT/GSPR.

Comments:
- 8 pages, 6 figures

---

## GMT: Enhancing Generalizable Neural Rendering via Geometry-Driven  Multi-Reference Texture Transfer

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-10-01 | Youngho Yoon, Hyun-Kurl Jang, Kuk-Jin Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2410.00672v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) aims to generate images at arbitrary viewpoints
using multi-view images, and recent insights from neural radiance fields (NeRF)
have contributed to remarkable improvements. Recently, studies on generalizable
NeRF (G-NeRF) have addressed the challenge of per-scene optimization in NeRFs.
The construction of radiance fields on-the-fly in G-NeRF simplifies the NVS
process, making it well-suited for real-world applications. Meanwhile, G-NeRF
still struggles in representing fine details for a specific scene due to the
absence of per-scene optimization, even with texture-rich multi-view source
inputs. As a remedy, we propose a Geometry-driven Multi-reference Texture
transfer network (GMT) available as a plug-and-play module designed for G-NeRF.
Specifically, we propose ray-imposed deformable convolution (RayDCN), which
aligns input and reference features reflecting scene geometry. Additionally,
the proposed texture preserving transformer (TP-Former) aggregates multi-view
source features while preserving texture information. Consequently, our module
enables direct interaction between adjacent pixels during the image enhancement
process, which is deficient in G-NeRF models with an independent rendering
process per pixel. This addresses constraints that hinder the ability to
capture high-frequency details. Experiments show that our plug-and-play module
consistently improves G-NeRF models on various benchmark datasets.

Comments:
- Accepted at ECCV 2024. Code available at
  https://github.com/yh-yoon/GMT
