---
layout: default
title: February 2025
parent: Papers
nav_order: 202502
---

<!---metadata--->


## AuraFusion360: Augmented Unseen Region Alignment for Reference-based  360Â° Unbounded Scene Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2502.05176v1){: .btn .btn-green } |

**Abstract**: Three-dimensional scene inpainting is crucial for applications from virtual
reality to architectural visualization, yet existing methods struggle with view
consistency and geometric accuracy in 360{\deg} unbounded scenes. We present
AuraFusion360, a novel reference-based method that enables high-quality object
removal and hole filling in 3D scenes represented by Gaussian Splatting. Our
approach introduces (1) depth-aware unseen mask generation for accurate
occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot
method for accurate initial point placement without requiring additional
training, and (3) SDEdit-based detail enhancement for multi-view coherence. We
also introduce 360-USID, the first comprehensive dataset for 360{\deg}
unbounded scene inpainting with ground truth. Extensive experiments demonstrate
that AuraFusion360 significantly outperforms existing methods, achieving
superior perceptual quality while maintaining geometric accuracy across
dramatic viewpoint changes. See our project page for video results and the
dataset at https://kkennethwu.github.io/aurafusion360/.

Comments:
- Project page: https://kkennethwu.github.io/aurafusion360/

---

## High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Zihao Zou, Ziyuan Qu, Xi Peng, Vivek Boominathan, Adithya Pediredla, Praneeth Chakravarthula | cs.CV | [PDF](http://arxiv.org/pdf/2502.04630v1){: .btn .btn-green } |

**Abstract**: Capturing and reconstructing high-speed dynamic 3D scenes has numerous
applications in computer graphics, vision, and interdisciplinary fields such as
robotics, aerodynamics, and evolutionary biology. However, achieving this using
a single imaging modality remains challenging. For instance, traditional RGB
cameras suffer from low frame rates, limited exposure times, and narrow
baselines. To address this, we propose a novel sensor fusion approach using
Gaussian splatting, which combines RGB, depth, and event cameras to capture and
reconstruct deforming scenes at high speeds. The key insight of our method lies
in leveraging the complementary strengths of these imaging modalities: RGB
cameras capture detailed color information, event cameras record rapid scene
changes with microsecond resolution, and depth cameras provide 3D scene
geometry. To unify the underlying scene representation across these modalities,
we represent the scene using deformable 3D Gaussians. To handle rapid scene
movements, we jointly optimize the 3D Gaussian parameters and their temporal
deformation fields by integrating data from all three sensor modalities. This
fusion enables efficient, high-quality imaging of fast and complex scenes, even
under challenging conditions such as low light, narrow baselines, or rapid
motion. Experiments on synthetic and real datasets captured with our prototype
sensor fusion setup demonstrate that our method significantly outperforms
state-of-the-art techniques, achieving noticeable improvements in both
rendering fidelity and structural accuracy.



---

## GaussRender: Learning 3D Occupancy with Gaussian Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Loick Chambon, Eloi Zablocki, Alexandre Boulch, Mickael Chen, Matthieu Cord | cs.CV | [PDF](http://arxiv.org/pdf/2502.05040v1){: .btn .btn-green } |

**Abstract**: Understanding the 3D geometry and semantics of driving scenes is critical for
developing of safe autonomous vehicles. While 3D occupancy models are typically
trained using voxel-based supervision with standard losses (e.g.,
cross-entropy, Lovasz, dice), these approaches treat voxel predictions
independently, neglecting their spatial relationships. In this paper, we
propose GaussRender, a plug-and-play 3D-to-2D reprojection loss that enhances
voxel-based supervision. Our method projects 3D voxel representations into
arbitrary 2D perspectives and leverages Gaussian splatting as an efficient,
differentiable rendering proxy of voxels, introducing spatial dependencies
across projected elements. This approach improves semantic and geometric
consistency, handles occlusions more efficiently, and requires no architectural
modifications. Extensive experiments on multiple benchmarks
(SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate
consistent performance gains across various 3D occupancy models (TPVFormer,
SurroundOcc, Symphonies), highlighting the robustness and versatility of our
framework. The code is available at https://github.com/valeoai/GaussRender.



---

## OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and  Geometric-Aware Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Xiaoyu Zhou, Jingqi Wang, Yongtao Wang, Yufei Wei, Nan Dong, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2502.04981v1){: .btn .btn-green } |

**Abstract**: Obtaining semantic 3D occupancy from raw sensor data without manual
annotations remains an essential yet challenging task. While prior works have
approached this as a perception prediction problem, we formulate it as
scene-aware 3D occupancy reconstruction with geometry and semantics. In this
work, we propose OccGS, a novel 3D Occupancy reconstruction framework utilizing
Semantic and Geometric-Aware Gaussian Splatting in a zero-shot manner.
Leveraging semantics extracted from vision-language models and geometry guided
by LiDAR points, OccGS constructs Semantic and Geometric-Aware Gaussians from
raw multisensor data. We also develop a cumulative Gaussian-to-3D voxel
splatting method for reconstructing occupancy from the Gaussians. OccGS
performs favorably against self-supervised methods in occupancy prediction,
achieving comparable performance to fully supervised approaches and achieving
state-of-the-art performance on zero-shot semantic 3D occupancy estimation.



---

## SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Huajian Huang, Yingshu Chen, Longwei Li, Hui Cheng, Tristan Braud, Yajie Zhao, Sai-Kit Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2502.04734v1){: .btn .btn-green } |

**Abstract**: 360-degree cameras streamline data collection for radiance field 3D
reconstruction by capturing comprehensive scene data. However, traditional
radiance field methods do not address the specific challenges inherent to
360-degree images. We present SC-OmniGS, a novel self-calibrating
omnidirectional Gaussian splatting system for fast and accurate omnidirectional
radiance field reconstruction using 360-degree images. Rather than converting
360-degree images to cube maps and performing perspective image calibration, we
treat 360-degree images as a whole sphere and derive a mathematical framework
that enables direct omnidirectional camera pose calibration accompanied by 3D
Gaussians optimization. Furthermore, we introduce a differentiable
omnidirectional camera model in order to rectify the distortion of real-world
data for performance enhancement. Overall, the omnidirectional camera intrinsic
model, extrinsic poses, and 3D Gaussians are jointly optimized by minimizing
weighted spherical photometric loss. Extensive experiments have demonstrated
that our proposed SC-OmniGS is able to recover a high-quality radiance field
from noisy camera poses or even no pose prior in challenging scenarios
characterized by wide baselines and non-object-centric configurations. The
noticeable performance gain in the real-world dataset captured by
consumer-grade omnidirectional cameras verifies the effectiveness of our
general omnidirectional camera model in reducing the distortion of 360-degree
images.

Comments:
- Accepted to ICLR 2025, Project Page:
  http://www.chenyingshu.com/sc-omnigs/

---

## PoI: Pixel of Interest for Novel View Synthesis Assisted Scene  Coordinate Regression

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2502.04843v1){: .btn .btn-green } |

**Abstract**: The task of estimating camera poses can be enhanced through novel view
synthesis techniques such as NeRF and Gaussian Splatting to increase the
diversity and extension of training data. However, these techniques often
produce rendered images with issues like blurring and ghosting, which
compromise their reliability. These issues become particularly pronounced for
Scene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the
pixel level. To mitigate the problems associated with unreliable rendered
images, we introduce a novel filtering approach, which selectively extracts
well-rendered pixels while discarding the inferior ones. This filter
simultaneously measures the SCR model's real-time reprojection loss and
gradient during training. Building on this filtering technique, we also develop
a new strategy to improve scene coordinate regression using sparse inputs,
drawing on successful applications of sparse input techniques in novel view
synthesis. Our experimental results validate the effectiveness of our method,
demonstrating state-of-the-art performance on indoor and outdoor datasets.



---

## GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-05 | Mingrui Li, Weijian Chen, Na Cheng, Jingyuan Xu, Dong Li, Hongyu Wang | cs.RO | [PDF](http://arxiv.org/pdf/2502.03228v1){: .btn .btn-green } |

**Abstract**: The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread
attention due to its excellent performance in real-time high-fidelity
rendering. However, in real-world environments with dynamic objects, existing
3DGS-based SLAM systems often face mapping errors and tracking drift issues. To
address these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM
system tailored for dynamic scenes. In terms of tracking, unlike traditional
methods, we directly perform dynamic segmentation on Gaussians and map them
back to the front-end to obtain dynamic point labels through a Gaussian pyramid
network, achieving precise dynamic removal and robust tracking. For mapping, we
impose rendering penalties on dynamically labeled Gaussians, which are updated
through the network, to avoid irreversible erroneous removal caused by simple
pruning. Our results on real-world datasets demonstrate that our method is
competitive in tracking compared to baseline methods, generating fewer
artifacts and higher-quality reconstructions in rendering.



---

## MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by  Continual Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2502.02372v1){: .btn .btn-green } |

**Abstract**: The generation of a virtual digital avatar is a crucial research topic in the
field of computer vision. Many existing works utilize Neural Radiance Fields
(NeRF) to address this issue and have achieved impressive results. However,
previous works assume the images of the training person are available and fixed
while the appearances and poses of a subject could constantly change and
increase in real-world scenarios. How to update the human avatar but also
maintain the ability to render the old appearance of the person is a practical
challenge. One trivial solution is to combine the existing virtual avatar
models based on NeRF with continual learning methods. However, there are some
critical issues in this approach: learning new appearances and poses can cause
the model to forget past information, which in turn leads to a degradation in
the rendering quality of past appearances, especially color bleeding issues,
and incorrect human body poses. In this work, we propose a maintainable avatar
(MaintaAvatar) based on neural radiance fields by continual learning, which
resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose
Distillation Module. Overall, our model requires only limited data collection
to quickly fine-tune the model while avoiding catastrophic forgetting, thus
achieving a maintainable virtual avatar. The experimental results validate the
effectiveness of our MaintaAvatar model.

Comments:
- AAAI 2025. 9 pages

---

## LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene  Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Yang Zhou, Zongjin He, Qixuan Li, Chao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.01949v1){: .btn .btn-green } |

**Abstract**: Recently, the field of text-guided 3D scene generation has garnered
significant attention. High-quality generation that aligns with physical
realism and high controllability is crucial for practical 3D scene
applications. However, existing methods face fundamental limitations: (i)
difficulty capturing complex relationships between multiple objects described
in the text, (ii) inability to generate physically plausible scene layouts, and
(iii) lack of controllability and extensibility in compositional scenes. In
this paper, we introduce LayoutDreamer, a framework that leverages 3D Gaussian
Splatting (3DGS) to facilitate high-quality, physically consistent
compositional scene generation guided by text. Specifically, given a text
prompt, we convert it into a directed scene graph and adaptively adjust the
density and layout of the initial compositional 3D Gaussians. Subsequently,
dynamic camera adjustments are made based on the training focal point to ensure
entity-level generation quality. Finally, by extracting directed dependencies
from the scene graph, we tailor physical and layout energy to ensure both
realism and flexibility. Comprehensive experiments demonstrate that
LayoutDreamer outperforms other compositional scene generation quality and
semantic alignment methods. Specifically, it achieves state-of-the-art (SOTA)
performance in the multiple objects generation metric of T3Bench.



---

## SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with  Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Yifu Tao, Maurice Fallon | cs.RO | [PDF](http://arxiv.org/pdf/2502.02657v1){: .btn .btn-green } |

**Abstract**: We present a neural radiance field (NeRF) based large-scale reconstruction
system that fuses lidar and vision data to generate high-quality
reconstructions that are geometrically accurate and capture photorealistic
texture. Our system adopts the state-of-the-art NeRF representation to
additionally incorporate lidar. Adding lidar data adds strong geometric
constraints on the depth and surface normals, which is particularly useful when
modelling uniform texture surfaces which contain ambiguous visual
reconstruction cues. Furthermore, we estimate the epistemic uncertainty of the
reconstruction as the spatial variance of each point location in the radiance
field given the sensor observations from camera and lidar. This enables the
identification of areas that are reliably reconstructed by each sensor
modality, allowing the map to be filtered according to the estimated
uncertainty. Our system can also exploit the trajectory produced by a real-time
pose-graph lidar SLAM system during online mapping to bootstrap a
(post-processed) Structure-from-Motion (SfM) reconstruction procedure reducing
SfM training time by up to 70%. It also helps to properly constrain the overall
metric scale which is essential for the lidar depth loss. The
globally-consistent trajectory can then be divided into submaps using Spectral
Clustering to group sets of co-visible images together. This submapping
approach is more suitable for visual reconstruction than distance-based
partitioning. Each submap is filtered according to point-wise uncertainty
estimates and merged to obtain the final large-scale 3D reconstruction. We
demonstrate the reconstruction system using a multi-camera, lidar sensor suite
in experiments involving both robot-mounted and handheld scanning. Our test
datasets cover a total area of more than 20,000 square metres, including
multiple university buildings and an aerial survey of a multi-storey.

Comments:
- webpage: https://dynamic.robots.ox.ac.uk/projects/silvr/

---

## GP-GS: Gaussian Processes for Enhanced Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.02283v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view
synthesis method. However, its reliance on sparse Structure-from-Motion (SfM)
point clouds consistently compromises the scene reconstruction quality. To
address these limitations, this paper proposes a novel 3D reconstruction
framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output
Gaussian Process model is developed to achieve adaptive and uncertainty-guided
densification of sparse SfM point clouds. Specifically, we propose a dynamic
sampling and filtering pipeline that adaptively expands the SfM point clouds by
leveraging GP-based predictions to infer new candidate points from the input 2D
pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the
pruning of high-variance predictions, ensuring geometric consistency and
enabling the generation of dense point clouds. The densified point clouds
provide high-quality initial 3D Gaussians to enhance reconstruction
performance. Extensive experiments conducted on synthetic and real-world
datasets across various scales validate the effectiveness and practicality of
the proposed framework.

Comments:
- 14 pages,11 figures

---

## Scalable 3D Gaussian Splatting-Based RF Signal Spatial Propagation  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Kang Yang, Gaofeng Dong, Sijie Ji, Wan Du, Mani Srivastava | cs.NI | [PDF](http://arxiv.org/pdf/2502.01826v1){: .btn .btn-green } |

**Abstract**: Effective network planning and sensing in wireless networks require
resource-intensive site surveys for data collection. An alternative is
Radio-Frequency (RF) signal spatial propagation modeling, which computes
received signals given transceiver positions in a scene (e.g.s a conference
room). We identify a fundamental trade-off between scalability and fidelity in
the state-of-the-art method. To address this issue, we explore leveraging 3D
Gaussian Splatting (3DGS), an advanced technique for the image synthesis of 3D
scenes in real-time from arbitrary camera poses. By integrating domain-specific
insights, we design three components for adapting 3DGS to the RF domain,
including Gaussian-based RF scene representation, gradient-guided RF attribute
learning, and RF-customized CUDA for ray tracing. Building on them, we develop
RFSPM, an end-to-end framework for scalable RF signal Spatial Propagation
Modeling. We evaluate RFSPM in four field studies and two applications across
RFID, BLE, LoRa, and 5G, covering diverse frequencies, antennas, signals, and
scenes. The results show that RFSPM matches the fidelity of the
state-of-the-art method while reducing data requirements, training GPU-hours,
and inference latency by up to 9.8\,$\times$, 18.6\,$\times$, and
84.4\,$\times$, respectively.



---

## VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and  Locomotion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, Hang Zhao | cs.RO | [PDF](http://arxiv.org/pdf/2502.01536v1){: .btn .btn-green } |

**Abstract**: Recent success in legged robot locomotion is attributed to the integration of
reinforcement learning and physical simulators. However, these policies often
encounter challenges when deployed in real-world environments due to
sim-to-real gaps, as simulators typically fail to replicate visual realism and
complex real-world geometry. Moreover, the lack of realistic visual rendering
limits the ability of these policies to support high-level tasks requiring
RGB-based perception like ego-centric navigation. This paper presents a
Real-to-Sim-to-Real framework that generates photorealistic and physically
interactive "digital twin" simulation environments for visual navigation and
locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based
scene reconstruction from multi-view images and integrates these environments
into simulations that support ego-centric visual perception and mesh-based
physical interactions. To demonstrate its effectiveness, we train a
reinforcement learning policy within the simulator to perform a visual
goal-tracking task. Extensive experiments show that our framework achieves
RGB-only sim-to-real policy transfer. Additionally, our framework facilitates
the rapid adaptation of robot policies with effective exploration capability in
complex new environments, highlighting its potential for applications in
households and factories.

Comments:
- Project Page: https://vr-robo.github.io/

---

## UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Aashish Rai, Dilin Wang, Mihir Jain, Nikolaos Sarafianos, Arthur Chen, Srinath Sridhar, Aayush Prakash | cs.CV | [PDF](http://arxiv.org/pdf/2502.01846v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D
objects and scenes. However, generating 3DGS remains challenging due to their
discrete, unstructured, and permutation-invariant nature. In this work, we
present a simple yet effective method to overcome these challenges. We utilize
spherical mapping to transform 3DGS into a structured 2D representation, termed
UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a
concatenation of Gaussian attributes such as position, scale, color, opacity,
and rotation. We further find that these heterogeneous features can be
compressed into a lower-dimensional (e.g., 3-channel) shared feature space
using a carefully designed multi-branch network. The compressed UVGS can be
treated as typical RGB images. Remarkably, we discover that typical VAEs
trained with latent diffusion models can directly generalize to this new
representation without additional training. Our novel representation makes it
effortless to leverage foundational 2D models, such as diffusion models, to
directly model 3DGS. Additionally, one can simply increase the 2D UV resolution
to accommodate more Gaussians, making UVGS a scalable solution compared to
typical 3D backbones. This approach immediately unlocks various novel
generation applications of 3DGS by inherently utilizing the already developed
superior 2D generation capabilities. In our experiments, we demonstrate various
unconditional, conditional generation, and inpainting applications of 3DGS
based on diffusion models, which were previously non-trivial.

Comments:
- https://aashishrai3799.github.io/uvgs

---

## Radiant Foam: Real-Time Differentiable Ray Tracing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2502.01157v1){: .btn .btn-green } |

**Abstract**: Research on differentiable scene representations is consistently moving
towards more efficient, real-time models. Recently, this has led to the
popularization of splatting methods, which eschew the traditional ray-based
rendering of radiance fields in favor of rasterization. This has yielded a
significant improvement in rendering speeds due to the efficiency of
rasterization algorithms and hardware, but has come at a cost: the
approximations that make rasterization efficient also make implementation of
light transport phenomena like reflection and refraction much more difficult.
We propose a novel scene representation which avoids these approximations, but
keeps the efficiency and reconstruction quality of splatting by leveraging a
decades-old efficient volumetric mesh ray tracing algorithm which has been
largely overlooked in recent computer vision research. The resulting model,
which we name Radiant Foam, achieves rendering speed and quality comparable to
Gaussian Splatting, without the constraints of rasterization. Unlike ray traced
Gaussian models that use hardware ray tracing acceleration, our method requires
no special hardware or APIs beyond the standard features of a programmable GPU.



---

## FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Diego Gomez, Bingchen Gong, Maks Ovsjanikov | cs.CV | [PDF](http://arxiv.org/pdf/2502.01405v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce FourieRF, a novel approach for achieving fast and
high-quality reconstruction in the few-shot setting. Our method effectively
parameterizes features through an explicit curriculum training procedure,
incrementally increasing scene complexity during optimization. Experimental
results show that the prior induced by our approach is both robust and
adaptable across a wide variety of scenes, establishing FourieRF as a strong
and versatile baseline for the few-shot rendering problem. While our approach
significantly reduces artifacts, it may still lead to reconstruction errors in
severely under-constrained scenarios, particularly where view occlusion leaves
parts of the shape uncovered. In the future, our method could be enhanced by
integrating foundation models to complete missing parts using large data-driven
priors.

Comments:
- 8 pages, 3DV 2025 conference

---

## EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head  Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-02 | Junuk Cha, Seongro Yoon, Valeriya Strizhkova, Francois Bremond, Seungryul Baek | cs.CV | [PDF](http://arxiv.org/pdf/2502.00654v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting-based talking head synthesis has recently gained
attention for its ability to render high-fidelity images with real-time
inference speed. However, since it is typically trained on only a short video
that lacks the diversity in facial emotions, the resultant talking heads
struggle to represent a wide range of emotions. To address this issue, we
propose a lip-aligned emotional face generator and leverage it to train our
EmoTalkingGaussian model. It is able to manipulate facial emotions conditioned
on continuous emotion values (i.e., valence and arousal); while retaining
synchronization of lip movements with input audio. Additionally, to achieve the
accurate lip synchronization for in-the-wild audio, we introduce a
self-supervised learning method that leverages a text-to-speech network and a
visual-audio synchronization network. We experiment our EmoTalkingGaussian on
publicly available videos and have obtained better results than
state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS),
emotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy),
and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively.

Comments:
- 22 pages
