---
layout: default
title: February 2025
parent: Papers
nav_order: 202502
---

<!---metadata--->


## MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-11 | Rafał Tobiasz, Grzegorz Wilczyński, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek | cs.GR | [PDF](http://arxiv.org/pdf/2502.07754v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer
graphics. GS-based algorithms almost always bypass classical methods such as
ray tracing, which offers numerous inherent advantages for rendering. For
example, ray tracing is able to handle incoherent rays for advanced lighting
effects, including shadows and reflections. To address this limitation, we
introduce MeshSplats, a method which converts GS to a mesh-like format.
Following the completion of training, MeshSplats transforms Gaussian elements
into mesh faces, enabling rendering using ray tracing methods with all their
associated benefits. Our model can be utilized immediately following
transformation, yielding a mesh of slightly reduced quality without additional
training. Furthermore, we can enhance the reconstruction quality through the
application of a dedicated optimization algorithm that operates on mesh faces
rather than Gaussian components. The efficacy of our method is substantiated by
experimental results, underscoring its extensive applications in computer
graphics and image processing.



---

## Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained  Matching Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-11 | Lin-Zhuo Chen, Kangjie Liu, Youtian Lin, Siyu Zhu, Zhihao Li, Xun Cao, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2502.07615v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has achieved excellent rendering quality with
fast training and rendering speed. However, its optimization process lacks
explicit geometric constraints, leading to suboptimal geometric reconstruction
in regions with sparse or no observational input views. In this work, we try to
mitigate the issue by incorporating a pre-trained matching prior to the 3DGS
optimization process. We introduce Flow Distillation Sampling (FDS), a
technique that leverages pre-trained geometric knowledge to bolster the
accuracy of the Gaussian radiance field. Our method employs a strategic
sampling technique to target unobserved views adjacent to the input views,
utilizing the optical flow calculated from the matching model (Prior Flow) to
guide the flow analytically calculated from the 3DGS geometry (Radiance Flow).
Comprehensive experiments in depth rendering, mesh reconstruction, and novel
view synthesis showcase the significant advantages of FDS over state-of-the-art
methods. Additionally, our interpretive experiments and analysis aim to shed
light on the effects of FDS on geometric accuracy and rendering quality,
potentially providing readers with insights into its performance. Project page:
https://nju-3dv.github.io/projects/fds

Comments:
- Accepted by ICLR 2025

---

## PrismAvatar: Real-time animated 3D neural head avatars on edge devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira | cs.CV | [PDF](http://arxiv.org/pdf/2502.07030v1){: .btn .btn-green } |

**Abstract**: We present PrismAvatar: a 3D head avatar model which is designed specifically
to enable real-time animation and rendering on resource-constrained edge
devices, while still enjoying the benefits of neural volumetric rendering at
training time. By integrating a rigged prism lattice with a 3D morphable head
model, we use a hybrid rendering model to simultaneously reconstruct a
mesh-based head and a deformable NeRF model for regions not represented by the
3DMM. We then distill the deformable NeRF into a rigged mesh and neural
textures, which can be animated and rendered efficiently within the constraints
of the traditional triangle rendering pipeline. In addition to running at 60
fps with low memory usage on mobile devices, we find that our trained models
have comparable quality to state-of-the-art 3D avatar models on desktop
devices.

Comments:
- 8 pages, 5 figures

---

## Grounding Creativity in Physics: A Brief Survey of Physical Priors in  AIGC

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Siwei Meng, Yawei Luo, Ping Liu | cs.CV | [PDF](http://arxiv.org/pdf/2502.07007v1){: .btn .btn-green } |

**Abstract**: Recent advancements in AI-generated content have significantly improved the
realism of 3D and 4D generation. However, most existing methods prioritize
appearance consistency while neglecting underlying physical principles, leading
to artifacts such as unrealistic deformations, unstable dynamics, and
implausible objects interactions. Incorporating physics priors into generative
models has become a crucial research direction to enhance structural integrity
and motion realism. This survey provides a review of physics-aware generative
methods, systematically analyzing how physical constraints are integrated into
3D and 4D generation. First, we examine recent works in incorporating physical
priors into static and dynamic 3D generation, categorizing methods based on
representation types, including vision-based, NeRF-based, and Gaussian
Splatting-based approaches. Second, we explore emerging techniques in 4D
generation, focusing on methods that model temporal dynamics with physical
simulations. Finally, we conduct a comparative analysis of major methods,
highlighting their strengths, limitations, and suitability for different
materials and motion dynamics. By presenting an in-depth analysis of
physics-grounded AIGC, this survey aims to bridge the gap between generative
models and physical realism, providing insights that inspire future research in
physically consistent content generation.



---

## SIREN: Semantic, Initialization-Free Registration of Multi-Robot  Gaussian Splatting Maps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Ola Shorinwa, Jiankai Sun, Mac Schwager, Anirudha Majumdar | cs.RO | [PDF](http://arxiv.org/pdf/2502.06519v1){: .btn .btn-green } |

**Abstract**: We present SIREN for registration of multi-robot Gaussian Splatting (GSplat)
maps, with zero access to camera poses, images, and inter-map transforms for
initialization or fusion of local submaps. To realize these capabilities, SIREN
harnesses the versatility and robustness of semantics in three critical ways to
derive a rigorous registration pipeline for multi-robot GSplat maps. First,
SIREN utilizes semantics to identify feature-rich regions of the local maps
where the registration problem is better posed, eliminating the need for any
initialization which is generally required in prior work. Second, SIREN
identifies candidate correspondences between Gaussians in the local maps using
robust semantic features, constituting the foundation for robust geometric
optimization, coarsely aligning 3D Gaussian primitives extracted from the local
maps. Third, this key step enables subsequent photometric refinement of the
transformation between the submaps, where SIREN leverages novel-view synthesis
in GSplat maps along with a semantics-based image filter to compute a
high-accuracy non-rigid transformation for the generation of a high-fidelity
fused map. We demonstrate the superior performance of SIREN compared to
competing baselines across a range of real-world datasets, and in particular,
across the most widely-used robot hardware platforms, including a manipulator,
drone, and quadruped. In our experiments, SIREN achieves about 90x smaller
rotation errors, 300x smaller translation errors, and 44x smaller scale errors
in the most challenging scenes, where competing methods struggle. We will
release the code and provide a link to the project page after the review
process.



---

## Three-Dimensional MRI Reconstruction with Gaussian Representations:  Tackling the Undersampling Problem

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Tengya Peng, Ruyi Zha, Zhen Li, Xiaofeng Liu, Qing Zou | eess.IV | [PDF](http://arxiv.org/pdf/2502.06510v1){: .btn .btn-green } |

**Abstract**: Three-Dimensional Gaussian Splatting (3DGS) has shown substantial promise in
the field of computer vision, but remains unexplored in the field of magnetic
resonance imaging (MRI). This study explores its potential for the
reconstruction of isotropic resolution 3D MRI from undersampled k-space data.
We introduce a novel framework termed 3D Gaussian MRI (3DGSMR), which employs
3D Gaussian distributions as an explicit representation for MR volumes.
Experimental evaluations indicate that this method can effectively reconstruct
voxelized MR images, achieving a quality on par with that of well-established
3D MRI reconstruction techniques found in the literature. Notably, the 3DGSMR
scheme operates under a self-supervised framework, obviating the need for
extensive training datasets or prior model training. This approach introduces
significant innovations to the domain, notably the adaptation of 3DGS to MRI
reconstruction and the novel application of the existing 3DGS methodology to
decompose MR signals, which are presented in a complex-valued format.



---

## PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based  Implicit Neural Map

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-09 | Yue Pan, Xingguang Zhong, Liren Jin, Louis Wiesmann, Marija Popović, Jens Behley, Cyrill Stachniss | cs.RO | [PDF](http://arxiv.org/pdf/2502.05752v1){: .btn .btn-green } |

**Abstract**: Robots require high-fidelity reconstructions of their environment for
effective operation. Such scene representations should be both, geometrically
accurate and photorealistic to support downstream tasks. While this can be
achieved by building distance fields from range sensors and radiance fields
from cameras, the scalable incremental mapping of both fields consistently and
at the same time with high quality remains challenging. In this paper, we
propose a novel map representation that unifies a continuous signed distance
field and a Gaussian splatting radiance field within an elastic and compact
point-based implicit neural map. By enforcing geometric consistency between
these fields, we achieve mutual improvements by exploiting both modalities. We
devise a LiDAR-visual SLAM system called PINGS using the proposed map
representation and evaluate it on several challenging large-scale datasets.
Experimental results demonstrate that PINGS can incrementally build globally
consistent distance and radiance fields encoded with a compact set of neural
points. Compared to the state-of-the-art methods, PINGS achieves superior
photometric and geometric rendering at novel views by leveraging the
constraints from the distance field. Furthermore, by utilizing dense
photometric cues and multi-view consistency from the radiance field, PINGS
produces more accurate distance fields, leading to improved odometry estimation
and mesh reconstruction.

Comments:
- 14 pages, 8 figures

---

## Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual  Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps  Platform

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-09 | Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2502.05769v2){: .btn .btn-green } |

**Abstract**: Urban digital twins are virtual replicas of cities that use multi-source data
and data analytics to optimize urban planning, infrastructure management, and
decision-making. Towards this, we propose a framework focused on the
single-building scale. By connecting to cloud mapping platforms such as Google
Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language
Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our
Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings
framework can retrieve a building's 3D model, visual descriptions, and achieve
cloud-based mapping integration with large language model-based data analytics
using a building's address, postal code, or geographic coordinates.

Comments:
- -Fixed minor typo

---

## Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV  in Ocean Environment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-08 | Maneesha Wickramasuriya, Beomyeol Yu, Taeyoung Lee, Murray Snyder | cs.CV | [PDF](http://arxiv.org/pdf/2502.05409v1){: .btn .btn-green } |

**Abstract**: This paper proposes a vision-in-the-loop simulation environment for deep
monocular pose estimation of a UAV operating in an ocean environment. Recently,
a deep neural network with a transformer architecture has been successfully
trained to estimate the pose of a UAV relative to the flight deck of a research
vessel, overcoming several limitations of GPS-based approaches. However,
validating the deep pose estimation scheme in an actual ocean environment poses
significant challenges due to the limited availability of research vessels and
the associated operational costs. To address these issues, we present a
photo-realistic 3D virtual environment leveraging recent advancements in
Gaussian splatting, a novel technique that represents 3D scenes by modeling
image pixels as Gaussian distributions in 3D space, creating a lightweight and
high-quality visual model from multiple viewpoints. This approach enables the
creation of a virtual environment integrating multiple real-world images
collected in situ. The resulting simulation enables the indoor testing of
flight maneuvers while verifying all aspects of flight software, hardware, and
the deep monocular pose estimation scheme. This approach provides a
cost-effective solution for testing and validating the autonomous flight of
shipboard UAVs, specifically focusing on vision-based control and estimation
algorithms.

Comments:
- 8 pages, 15 figures, conference

---

## GWRF: A Generalizable Wireless Radiance Field for Wireless Signal  Propagation Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-08 | Kang Yang, Yuning Chen, Wan Du | cs.NI | [PDF](http://arxiv.org/pdf/2502.05708v1){: .btn .btn-green } |

**Abstract**: We present Generalizable Wireless Radiance Fields (GWRF), a framework for
modeling wireless signal propagation at arbitrary 3D transmitter and receiver
positions. Unlike previous methods that adapt vanilla Neural Radiance Fields
(NeRF) from the optical to the wireless signal domain, requiring extensive
per-scene training, GWRF generalizes effectively across scenes. First, a
geometry-aware Transformer encoder-based wireless scene representation module
incorporates information from geographically proximate transmitters to learn a
generalizable wireless radiance field. Second, a neural-driven ray tracing
algorithm operates on this field to automatically compute signal reception at
the receiver. Experimental results demonstrate that GWRF outperforms existing
methods on single scenes and achieves state-of-the-art performance on unseen
scenes.



---

## PoI: Pixel of Interest for Novel View Synthesis Assisted Scene  Coordinate Regression

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2502.04843v2){: .btn .btn-green } |

**Abstract**: The task of estimating camera poses can be enhanced through novel view
synthesis techniques such as NeRF and Gaussian Splatting to increase the
diversity and extension of training data. However, these techniques often
produce rendered images with issues like blurring and ghosting, which
compromise their reliability. These issues become particularly pronounced for
Scene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the
pixel level. To mitigate the problems associated with unreliable rendered
images, we introduce a novel filtering approach, which selectively extracts
well-rendered pixels while discarding the inferior ones. This filter
simultaneously measures the SCR model's real-time reprojection loss and
gradient during training. Building on this filtering technique, we also develop
a new strategy to improve scene coordinate regression using sparse inputs,
drawing on successful applications of sparse input techniques in novel view
synthesis. Our experimental results validate the effectiveness of our method,
demonstrating state-of-the-art performance on indoor and outdoor datasets.



---

## GaussRender: Learning 3D Occupancy with Gaussian Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Loick Chambon, Eloi Zablocki, Alexandre Boulch, Mickael Chen, Matthieu Cord | cs.CV | [PDF](http://arxiv.org/pdf/2502.05040v1){: .btn .btn-green } |

**Abstract**: Understanding the 3D geometry and semantics of driving scenes is critical for
developing of safe autonomous vehicles. While 3D occupancy models are typically
trained using voxel-based supervision with standard losses (e.g.,
cross-entropy, Lovasz, dice), these approaches treat voxel predictions
independently, neglecting their spatial relationships. In this paper, we
propose GaussRender, a plug-and-play 3D-to-2D reprojection loss that enhances
voxel-based supervision. Our method projects 3D voxel representations into
arbitrary 2D perspectives and leverages Gaussian splatting as an efficient,
differentiable rendering proxy of voxels, introducing spatial dependencies
across projected elements. This approach improves semantic and geometric
consistency, handles occlusions more efficiently, and requires no architectural
modifications. Extensive experiments on multiple benchmarks
(SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate
consistent performance gains across various 3D occupancy models (TPVFormer,
SurroundOcc, Symphonies), highlighting the robustness and versatility of our
framework. The code is available at https://github.com/valeoai/GaussRender.



---

## OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and  Geometric-Aware Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Xiaoyu Zhou, Jingqi Wang, Yongtao Wang, Yufei Wei, Nan Dong, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2502.04981v1){: .btn .btn-green } |

**Abstract**: Obtaining semantic 3D occupancy from raw sensor data without manual
annotations remains an essential yet challenging task. While prior works have
approached this as a perception prediction problem, we formulate it as
scene-aware 3D occupancy reconstruction with geometry and semantics. In this
work, we propose OccGS, a novel 3D Occupancy reconstruction framework utilizing
Semantic and Geometric-Aware Gaussian Splatting in a zero-shot manner.
Leveraging semantics extracted from vision-language models and geometry guided
by LiDAR points, OccGS constructs Semantic and Geometric-Aware Gaussians from
raw multisensor data. We also develop a cumulative Gaussian-to-3D voxel
splatting method for reconstructing occupancy from the Gaussians. OccGS
performs favorably against self-supervised methods in occupancy prediction,
achieving comparable performance to fully supervised approaches and achieving
state-of-the-art performance on zero-shot semantic 3D occupancy estimation.



---

## SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Huajian Huang, Yingshu Chen, Longwei Li, Hui Cheng, Tristan Braud, Yajie Zhao, Sai-Kit Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2502.04734v1){: .btn .btn-green } |

**Abstract**: 360-degree cameras streamline data collection for radiance field 3D
reconstruction by capturing comprehensive scene data. However, traditional
radiance field methods do not address the specific challenges inherent to
360-degree images. We present SC-OmniGS, a novel self-calibrating
omnidirectional Gaussian splatting system for fast and accurate omnidirectional
radiance field reconstruction using 360-degree images. Rather than converting
360-degree images to cube maps and performing perspective image calibration, we
treat 360-degree images as a whole sphere and derive a mathematical framework
that enables direct omnidirectional camera pose calibration accompanied by 3D
Gaussians optimization. Furthermore, we introduce a differentiable
omnidirectional camera model in order to rectify the distortion of real-world
data for performance enhancement. Overall, the omnidirectional camera intrinsic
model, extrinsic poses, and 3D Gaussians are jointly optimized by minimizing
weighted spherical photometric loss. Extensive experiments have demonstrated
that our proposed SC-OmniGS is able to recover a high-quality radiance field
from noisy camera poses or even no pose prior in challenging scenarios
characterized by wide baselines and non-object-centric configurations. The
noticeable performance gain in the real-world dataset captured by
consumer-grade omnidirectional cameras verifies the effectiveness of our
general omnidirectional camera model in reducing the distortion of 360-degree
images.

Comments:
- Accepted to ICLR 2025, Project Page:
  http://www.chenyingshu.com/sc-omnigs/

---

## High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Zihao Zou, Ziyuan Qu, Xi Peng, Vivek Boominathan, Adithya Pediredla, Praneeth Chakravarthula | cs.CV | [PDF](http://arxiv.org/pdf/2502.04630v1){: .btn .btn-green } |

**Abstract**: Capturing and reconstructing high-speed dynamic 3D scenes has numerous
applications in computer graphics, vision, and interdisciplinary fields such as
robotics, aerodynamics, and evolutionary biology. However, achieving this using
a single imaging modality remains challenging. For instance, traditional RGB
cameras suffer from low frame rates, limited exposure times, and narrow
baselines. To address this, we propose a novel sensor fusion approach using
Gaussian splatting, which combines RGB, depth, and event cameras to capture and
reconstruct deforming scenes at high speeds. The key insight of our method lies
in leveraging the complementary strengths of these imaging modalities: RGB
cameras capture detailed color information, event cameras record rapid scene
changes with microsecond resolution, and depth cameras provide 3D scene
geometry. To unify the underlying scene representation across these modalities,
we represent the scene using deformable 3D Gaussians. To handle rapid scene
movements, we jointly optimize the 3D Gaussian parameters and their temporal
deformation fields by integrating data from all three sensor modalities. This
fusion enables efficient, high-quality imaging of fast and complex scenes, even
under challenging conditions such as low light, narrow baselines, or rapid
motion. Experiments on synthetic and real datasets captured with our prototype
sensor fusion setup demonstrate that our method significantly outperforms
state-of-the-art techniques, achieving noticeable improvements in both
rendering fidelity and structural accuracy.



---

## AuraFusion360: Augmented Unseen Region Alignment for Reference-based  360° Unbounded Scene Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2502.05176v1){: .btn .btn-green } |

**Abstract**: Three-dimensional scene inpainting is crucial for applications from virtual
reality to architectural visualization, yet existing methods struggle with view
consistency and geometric accuracy in 360{\deg} unbounded scenes. We present
AuraFusion360, a novel reference-based method that enables high-quality object
removal and hole filling in 3D scenes represented by Gaussian Splatting. Our
approach introduces (1) depth-aware unseen mask generation for accurate
occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot
method for accurate initial point placement without requiring additional
training, and (3) SDEdit-based detail enhancement for multi-view coherence. We
also introduce 360-USID, the first comprehensive dataset for 360{\deg}
unbounded scene inpainting with ground truth. Extensive experiments demonstrate
that AuraFusion360 significantly outperforms existing methods, achieving
superior perceptual quality while maintaining geometric accuracy across
dramatic viewpoint changes. See our project page for video results and the
dataset at https://kkennethwu.github.io/aurafusion360/.

Comments:
- Project page: https://kkennethwu.github.io/aurafusion360/

---

## GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-05 | Mingrui Li, Weijian Chen, Na Cheng, Jingyuan Xu, Dong Li, Hongyu Wang | cs.RO | [PDF](http://arxiv.org/pdf/2502.03228v1){: .btn .btn-green } |

**Abstract**: The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread
attention due to its excellent performance in real-time high-fidelity
rendering. However, in real-world environments with dynamic objects, existing
3DGS-based SLAM systems often face mapping errors and tracking drift issues. To
address these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM
system tailored for dynamic scenes. In terms of tracking, unlike traditional
methods, we directly perform dynamic segmentation on Gaussians and map them
back to the front-end to obtain dynamic point labels through a Gaussian pyramid
network, achieving precise dynamic removal and robust tracking. For mapping, we
impose rendering penalties on dynamically labeled Gaussians, which are updated
through the network, to avoid irreversible erroneous removal caused by simple
pruning. Our results on real-world datasets demonstrate that our method is
competitive in tracking compared to baseline methods, generating fewer
artifacts and higher-quality reconstructions in rendering.



---

## VistaFlow: Photorealistic Volumetric Reconstruction with Dynamic  Resolution Management via Q-Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-05 | Jayram Palamadai, William Yu | cs.CV | [PDF](http://arxiv.org/pdf/2502.05222v1){: .btn .btn-green } |

**Abstract**: We introduce VistaFlow, a scalable three-dimensional imaging technique
capable of reconstructing fully interactive 3D volumetric images from a set of
2D photographs. Our model synthesizes novel viewpoints through a differentiable
rendering system capable of dynamic resolution management on photorealistic 3D
scenes. We achieve this through the introduction of QuiQ, a novel intermediate
video controller trained through Q-learning to maintain a consistently high
framerate by adjusting render resolution with millisecond precision. Notably,
VistaFlow runs natively on integrated CPU graphics, making it viable for mobile
and entry-level devices while still delivering high-performance rendering.
VistaFlow bypasses Neural Radiance Fields (NeRFs), using the PlenOctree data
structure to render complex light interactions such as reflection and
subsurface scattering with minimal hardware requirements. Our model is capable
of outperforming state-of-the-art methods with novel view synthesis at a
resolution of 1080p at over 100 frames per second on consumer hardware. By
tailoring render quality to the capabilities of each device, VistaFlow has the
potential to improve the efficiency and accessibility of photorealistic 3D
scene rendering across a wide spectrum of hardware, from high-end workstations
to inexpensive microcontrollers.



---

## GP-GS: Gaussian Processes for Enhanced Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.02283v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view
synthesis method. However, its reliance on sparse Structure-from-Motion (SfM)
point clouds consistently compromises the scene reconstruction quality. To
address these limitations, this paper proposes a novel 3D reconstruction
framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output
Gaussian Process model is developed to achieve adaptive and uncertainty-guided
densification of sparse SfM point clouds. Specifically, we propose a dynamic
sampling and filtering pipeline that adaptively expands the SfM point clouds by
leveraging GP-based predictions to infer new candidate points from the input 2D
pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the
pruning of high-variance predictions, ensuring geometric consistency and
enabling the generation of dense point clouds. The densified point clouds
provide high-quality initial 3D Gaussians to enhance reconstruction
performance. Extensive experiments conducted on synthetic and real-world
datasets across various scales validate the effectiveness and practicality of
the proposed framework.

Comments:
- 14 pages,11 figures

---

## SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with  Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Yifu Tao, Maurice Fallon | cs.RO | [PDF](http://arxiv.org/pdf/2502.02657v1){: .btn .btn-green } |

**Abstract**: We present a neural radiance field (NeRF) based large-scale reconstruction
system that fuses lidar and vision data to generate high-quality
reconstructions that are geometrically accurate and capture photorealistic
texture. Our system adopts the state-of-the-art NeRF representation to
additionally incorporate lidar. Adding lidar data adds strong geometric
constraints on the depth and surface normals, which is particularly useful when
modelling uniform texture surfaces which contain ambiguous visual
reconstruction cues. Furthermore, we estimate the epistemic uncertainty of the
reconstruction as the spatial variance of each point location in the radiance
field given the sensor observations from camera and lidar. This enables the
identification of areas that are reliably reconstructed by each sensor
modality, allowing the map to be filtered according to the estimated
uncertainty. Our system can also exploit the trajectory produced by a real-time
pose-graph lidar SLAM system during online mapping to bootstrap a
(post-processed) Structure-from-Motion (SfM) reconstruction procedure reducing
SfM training time by up to 70%. It also helps to properly constrain the overall
metric scale which is essential for the lidar depth loss. The
globally-consistent trajectory can then be divided into submaps using Spectral
Clustering to group sets of co-visible images together. This submapping
approach is more suitable for visual reconstruction than distance-based
partitioning. Each submap is filtered according to point-wise uncertainty
estimates and merged to obtain the final large-scale 3D reconstruction. We
demonstrate the reconstruction system using a multi-camera, lidar sensor suite
in experiments involving both robot-mounted and handheld scanning. Our test
datasets cover a total area of more than 20,000 square metres, including
multiple university buildings and an aerial survey of a multi-storey.

Comments:
- webpage: https://dynamic.robots.ox.ac.uk/projects/silvr/

---

## LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene  Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Yang Zhou, Zongjin He, Qixuan Li, Chao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.01949v1){: .btn .btn-green } |

**Abstract**: Recently, the field of text-guided 3D scene generation has garnered
significant attention. High-quality generation that aligns with physical
realism and high controllability is crucial for practical 3D scene
applications. However, existing methods face fundamental limitations: (i)
difficulty capturing complex relationships between multiple objects described
in the text, (ii) inability to generate physically plausible scene layouts, and
(iii) lack of controllability and extensibility in compositional scenes. In
this paper, we introduce LayoutDreamer, a framework that leverages 3D Gaussian
Splatting (3DGS) to facilitate high-quality, physically consistent
compositional scene generation guided by text. Specifically, given a text
prompt, we convert it into a directed scene graph and adaptively adjust the
density and layout of the initial compositional 3D Gaussians. Subsequently,
dynamic camera adjustments are made based on the training focal point to ensure
entity-level generation quality. Finally, by extracting directed dependencies
from the scene graph, we tailor physical and layout energy to ensure both
realism and flexibility. Comprehensive experiments demonstrate that
LayoutDreamer outperforms other compositional scene generation quality and
semantic alignment methods. Specifically, it achieves state-of-the-art (SOTA)
performance in the multiple objects generation metric of T3Bench.



---

## MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by  Continual Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2502.02372v1){: .btn .btn-green } |

**Abstract**: The generation of a virtual digital avatar is a crucial research topic in the
field of computer vision. Many existing works utilize Neural Radiance Fields
(NeRF) to address this issue and have achieved impressive results. However,
previous works assume the images of the training person are available and fixed
while the appearances and poses of a subject could constantly change and
increase in real-world scenarios. How to update the human avatar but also
maintain the ability to render the old appearance of the person is a practical
challenge. One trivial solution is to combine the existing virtual avatar
models based on NeRF with continual learning methods. However, there are some
critical issues in this approach: learning new appearances and poses can cause
the model to forget past information, which in turn leads to a degradation in
the rendering quality of past appearances, especially color bleeding issues,
and incorrect human body poses. In this work, we propose a maintainable avatar
(MaintaAvatar) based on neural radiance fields by continual learning, which
resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose
Distillation Module. Overall, our model requires only limited data collection
to quickly fine-tune the model while avoiding catastrophic forgetting, thus
achieving a maintainable virtual avatar. The experimental results validate the
effectiveness of our MaintaAvatar model.

Comments:
- AAAI 2025. 9 pages

---

## Radiant Foam: Real-Time Differentiable Ray Tracing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2502.01157v1){: .btn .btn-green } |

**Abstract**: Research on differentiable scene representations is consistently moving
towards more efficient, real-time models. Recently, this has led to the
popularization of splatting methods, which eschew the traditional ray-based
rendering of radiance fields in favor of rasterization. This has yielded a
significant improvement in rendering speeds due to the efficiency of
rasterization algorithms and hardware, but has come at a cost: the
approximations that make rasterization efficient also make implementation of
light transport phenomena like reflection and refraction much more difficult.
We propose a novel scene representation which avoids these approximations, but
keeps the efficiency and reconstruction quality of splatting by leveraging a
decades-old efficient volumetric mesh ray tracing algorithm which has been
largely overlooked in recent computer vision research. The resulting model,
which we name Radiant Foam, achieves rendering speed and quality comparable to
Gaussian Splatting, without the constraints of rasterization. Unlike ray traced
Gaussian models that use hardware ray tracing acceleration, our method requires
no special hardware or APIs beyond the standard features of a programmable GPU.



---

## FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Diego Gomez, Bingchen Gong, Maks Ovsjanikov | cs.CV | [PDF](http://arxiv.org/pdf/2502.01405v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce FourieRF, a novel approach for achieving fast and
high-quality reconstruction in the few-shot setting. Our method effectively
parameterizes features through an explicit curriculum training procedure,
incrementally increasing scene complexity during optimization. Experimental
results show that the prior induced by our approach is both robust and
adaptable across a wide variety of scenes, establishing FourieRF as a strong
and versatile baseline for the few-shot rendering problem. While our approach
significantly reduces artifacts, it may still lead to reconstruction errors in
severely under-constrained scenarios, particularly where view occlusion leaves
parts of the shape uncovered. In the future, our method could be enhanced by
integrating foundation models to complete missing parts using large data-driven
priors.

Comments:
- 8 pages, 3DV 2025 conference

---

## VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and  Locomotion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, Hang Zhao | cs.RO | [PDF](http://arxiv.org/pdf/2502.01536v1){: .btn .btn-green } |

**Abstract**: Recent success in legged robot locomotion is attributed to the integration of
reinforcement learning and physical simulators. However, these policies often
encounter challenges when deployed in real-world environments due to
sim-to-real gaps, as simulators typically fail to replicate visual realism and
complex real-world geometry. Moreover, the lack of realistic visual rendering
limits the ability of these policies to support high-level tasks requiring
RGB-based perception like ego-centric navigation. This paper presents a
Real-to-Sim-to-Real framework that generates photorealistic and physically
interactive "digital twin" simulation environments for visual navigation and
locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based
scene reconstruction from multi-view images and integrates these environments
into simulations that support ego-centric visual perception and mesh-based
physical interactions. To demonstrate its effectiveness, we train a
reinforcement learning policy within the simulator to perform a visual
goal-tracking task. Extensive experiments show that our framework achieves
RGB-only sim-to-real policy transfer. Additionally, our framework facilitates
the rapid adaptation of robot policies with effective exploration capability in
complex new environments, highlighting its potential for applications in
households and factories.

Comments:
- Project Page: https://vr-robo.github.io/

---

## Scalable 3D Gaussian Splatting-Based RF Signal Spatial Propagation  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Kang Yang, Gaofeng Dong, Sijie Ji, Wan Du, Mani Srivastava | cs.NI | [PDF](http://arxiv.org/pdf/2502.01826v1){: .btn .btn-green } |

**Abstract**: Effective network planning and sensing in wireless networks require
resource-intensive site surveys for data collection. An alternative is
Radio-Frequency (RF) signal spatial propagation modeling, which computes
received signals given transceiver positions in a scene (e.g.s a conference
room). We identify a fundamental trade-off between scalability and fidelity in
the state-of-the-art method. To address this issue, we explore leveraging 3D
Gaussian Splatting (3DGS), an advanced technique for the image synthesis of 3D
scenes in real-time from arbitrary camera poses. By integrating domain-specific
insights, we design three components for adapting 3DGS to the RF domain,
including Gaussian-based RF scene representation, gradient-guided RF attribute
learning, and RF-customized CUDA for ray tracing. Building on them, we develop
RFSPM, an end-to-end framework for scalable RF signal Spatial Propagation
Modeling. We evaluate RFSPM in four field studies and two applications across
RFID, BLE, LoRa, and 5G, covering diverse frequencies, antennas, signals, and
scenes. The results show that RFSPM matches the fidelity of the
state-of-the-art method while reducing data requirements, training GPU-hours,
and inference latency by up to 9.8\,$\times$, 18.6\,$\times$, and
84.4\,$\times$, respectively.



---

## UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Aashish Rai, Dilin Wang, Mihir Jain, Nikolaos Sarafianos, Kefan Chen, Srinath Sridhar, Aayush Prakash | cs.CV | [PDF](http://arxiv.org/pdf/2502.01846v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D
objects and scenes. However, generating 3DGS remains challenging due to their
discrete, unstructured, and permutation-invariant nature. In this work, we
present a simple yet effective method to overcome these challenges. We utilize
spherical mapping to transform 3DGS into a structured 2D representation, termed
UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a
concatenation of Gaussian attributes such as position, scale, color, opacity,
and rotation. We further find that these heterogeneous features can be
compressed into a lower-dimensional (e.g., 3-channel) shared feature space
using a carefully designed multi-branch network. The compressed UVGS can be
treated as typical RGB images. Remarkably, we discover that typical VAEs
trained with latent diffusion models can directly generalize to this new
representation without additional training. Our novel representation makes it
effortless to leverage foundational 2D models, such as diffusion models, to
directly model 3DGS. Additionally, one can simply increase the 2D UV resolution
to accommodate more Gaussians, making UVGS a scalable solution compared to
typical 3D backbones. This approach immediately unlocks various novel
generation applications of 3DGS by inherently utilizing the already developed
superior 2D generation capabilities. In our experiments, we demonstrate various
unconditional, conditional generation, and inpainting applications of 3DGS
based on diffusion models, which were previously non-trivial.

Comments:
- https://aashishrai3799.github.io/uvgs

---

## EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head  Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-02 | Junuk Cha, Seongro Yoon, Valeriya Strizhkova, Francois Bremond, Seungryul Baek | cs.CV | [PDF](http://arxiv.org/pdf/2502.00654v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting-based talking head synthesis has recently gained
attention for its ability to render high-fidelity images with real-time
inference speed. However, since it is typically trained on only a short video
that lacks the diversity in facial emotions, the resultant talking heads
struggle to represent a wide range of emotions. To address this issue, we
propose a lip-aligned emotional face generator and leverage it to train our
EmoTalkingGaussian model. It is able to manipulate facial emotions conditioned
on continuous emotion values (i.e., valence and arousal); while retaining
synchronization of lip movements with input audio. Additionally, to achieve the
accurate lip synchronization for in-the-wild audio, we introduce a
self-supervised learning method that leverages a text-to-speech network and a
visual-audio synchronization network. We experiment our EmoTalkingGaussian on
publicly available videos and have obtained better results than
state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS),
emotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy),
and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively.

Comments:
- 22 pages
