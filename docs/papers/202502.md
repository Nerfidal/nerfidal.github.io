---
layout: default
title: February 2025
parent: Papers
nav_order: 202502
---

<!---metadata--->


## HumanGif: Single-View Human Diffusion with Generative Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji | cs.CV | [PDF](http://arxiv.org/pdf/2502.12080v1){: .btn .btn-green } |

**Abstract**: While previous single-view-based 3D human reconstruction methods made
significant progress in novel view synthesis, it remains a challenge to
synthesize both view-consistent and pose-consistent results for animatable
human avatars from a single image input. Motivated by the success of 2D
character animation, we propose <strong>HumanGif</strong>, a single-view human
diffusion model with generative prior. Specifically, we formulate the
single-view-based 3D human novel view and pose synthesis as a
single-view-conditioned human diffusion process, utilizing generative priors
from foundational diffusion models. To ensure fine-grained and consistent novel
view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn
spatially aligned features from the input image, implicitly capturing the
relative camera and human pose transformation. Furthermore, we introduce an
image-level loss during optimization to bridge the gap between latent and image
spaces in diffusion models. Extensive experiments on RenderPeople and
DNA-Rendering datasets demonstrate that HumanGif achieves the best perceptual
performance, with better generalizability for novel view and pose synthesis.

Comments:
- Project page: https://skhu101.github.io/HumanGif/

---

## GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with  Pose Guidance from Text

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Gyumin Shim, Sangmin Lee, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2502.11642v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce GaussianMotion, a novel human rendering model
that generates fully animatable scenes aligned with textual descriptions using
Gaussian Splatting. Although existing methods achieve reasonable text-to-3D
generation of human bodies using various 3D representations, they often face
limitations in fidelity and efficiency, or primarily focus on static models
with limited pose control. In contrast, our method generates fully animatable
3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score
distillation, achieving high fidelity and efficient rendering for arbitrary
poses. By densely generating diverse random poses during optimization, our
deformable 3D human model learns to capture a wide range of natural motions
distilled from a pose-conditioned diffusion model in an end-to-end manner.
Furthermore, we propose Adaptive Score Distillation that effectively balances
realistic detail and smoothness to achieve optimal 3D results. Experimental
results demonstrate that our approach outperforms existing baselines by
producing high-quality textures in both static and animated results, and by
generating diverse 3D human models from various textual inputs.

Comments:
- 8 pages

---

## 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.11801v1){: .btn .btn-green } |

**Abstract**: When performing 3D inpainting using novel-view rendering methods like Neural
Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture
and geometry consistency across camera views has been a challenge. In this
paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided
Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided
by the rendered depth information from each training view, our 3DGIC exploits
background pixels visible across different views for updating the inpainting
mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive
experiments on benchmark datasets, we confirm that our 3DGIC outperforms
current state-of-the-art 3D inpainting methods quantitatively and
qualitatively.



---

## Exploring the Versal AI Engine for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Kotaro Shimamura, Ayumi Ohno, Shinya Takamaeda-Yamazaki | cs.AR | [PDF](http://arxiv.org/pdf/2502.11782v1){: .btn .btn-green } |

**Abstract**: Dataflow-oriented spatial architectures are the emerging paradigm for higher
computation performance and efficiency.
  AMD Versal AI Engine is a commercial spatial architecture consisting of tiles
of VLIW processors supporting SIMD operations arranged in a two-dimensional
mesh.
  The architecture requires the explicit design of task assignments and
dataflow configurations for each tile to maximize performance, demanding
advanced techniques and meticulous design.
  However, a few works revealed the performance characteristics of the Versal
AI Engine through practical workloads.
  In this work, we provide the comprehensive performance evaluation of the
Versal AI Engine using Gaussian feature computation in 3D Gaussian splatting as
a practical workload, and we then propose a novel dedicated algorithm to fully
exploit the hardware architecture.
  The computations of 3D Gaussian splatting include matrix multiplications and
color computations utilizing high-dimensional spherical harmonic coefficients.
  These tasks are processed efficiently by leveraging the SIMD capabilities and
their instruction-level parallelism.
  Additionally, pipelined processing is achieved by assigning different tasks
to individual cores, thereby fully exploiting the spatial parallelism of AI
Engines.
  The proposed method demonstrated a 226-fold throughput increase in
simulation-based evaluation, outperforming a naive approach.
  These findings provide valuable insights for application development that
effectively harnesses the spatial and architectural advantages of AI Engines.



---

## GS-GVINS: A Tightly-integrated GNSS-Visual-Inertial Navigation System  Augmented by 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-16 | Zelin Zhou, Saurav Uprety, Shichuang Nie, Hongzhou Yang | cs.RO | [PDF](http://arxiv.org/pdf/2502.10975v1){: .btn .btn-green } |

**Abstract**: Recently, the emergence of 3D Gaussian Splatting (3DGS) has drawn significant
attention in the area of 3D map reconstruction and visual SLAM. While extensive
research has explored 3DGS for indoor trajectory tracking using visual sensor
alone or in combination with Light Detection and Ranging (LiDAR) and Inertial
Measurement Unit (IMU), its integration with GNSS for large-scale outdoor
navigation remains underexplored. To address these concerns, we proposed
GS-GVINS: a tightly-integrated GNSS-Visual-Inertial Navigation System augmented
by 3DGS. This system leverages 3D Gaussian as a continuous differentiable scene
representation in largescale outdoor environments, enhancing navigation
performance through the constructed 3D Gaussian map. Notably, GS-GVINS is the
first GNSS-Visual-Inertial navigation application that directly utilizes the
analytical jacobians of SE3 camera pose with respect to 3D Gaussians. To
maintain the quality of 3DGS rendering in extreme dynamic states, we introduce
a motionaware 3D Gaussian pruning mechanism, updating the map based on relative
pose translation and the accumulated opacity along the camera ray. For
validation, we test our system under different driving environments: open-sky,
sub-urban, and urban. Both self-collected and public datasets are used for
evaluation. The results demonstrate the effectiveness of GS-GVINS in enhancing
navigation accuracy across diverse driving environments.



---

## OMG: Opacity Matters in Material Modeling with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-16 | Silong Yong, Venkata Nagarjun Pudureddiyur Manivannan, Bernhard Kerbl, Zifu Wan, Simon Stepputtis, Katia Sycara, Yaqi Xie | cs.CV | [PDF](http://arxiv.org/pdf/2502.10988v1){: .btn .btn-green } |

**Abstract**: Decomposing geometry, materials and lighting from a set of images, namely
inverse rendering, has been a long-standing problem in computer vision and
graphics. Recent advances in neural rendering enable photo-realistic and
plausible inverse rendering results. The emergence of 3D Gaussian Splatting has
boosted it to the next level by showing real-time rendering potentials. An
intuitive finding is that the models used for inverse rendering do not take
into account the dependency of opacity w.r.t. material properties, namely cross
section, as suggested by optics. Therefore, we develop a novel approach that
adds this dependency to the modeling itself. Inspired by radiative transfer, we
augment the opacity term by introducing a neural network that takes as input
material properties to provide modeling of cross section and a physically
correct activation function. The gradients for material properties are
therefore not only from color but also from opacity, facilitating a constraint
for their optimization. Therefore, the proposed method incorporates more
accurate physical properties compared to previous works. We implement our
method into 3 different baselines that use Gaussian Splatting for inverse
rendering and achieve significant improvements universally in terms of novel
view synthesis and material modeling.

Comments:
- Published as a conference paper at ICLR 2025

---

## E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-15 | Sohaib Zahid, Viktor Rudnev, Eddy Ilg, Vladislav Golyanik | cs.CV | [PDF](http://arxiv.org/pdf/2502.10827v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis techniques predominantly utilize RGB cameras, inheriting
their limitations such as the need for sufficient lighting, susceptibility to
motion blur, and restricted dynamic range. In contrast, event cameras are
significantly more resilient to these limitations but have been less explored
in this domain, particularly in large-scale settings. Current methodologies
primarily focus on front-facing or object-oriented (360-degree view) scenarios.
For the first time, we introduce 3D Gaussians for event-based novel view
synthesis. Our method reconstructs large and unbounded scenes with high visual
quality. We contribute the first real and synthetic event datasets tailored for
this setting. Our method demonstrates superior novel view synthesis and
consistently outperforms the baseline EventNeRF by a margin of 11-25% in PSNR
(dB) while being orders of magnitude faster in reconstruction and rendering.

Comments:
- 15 pages, 10 figures and 3 tables; project page:
  https://4dqv.mpi-inf.mpg.de/E3DGS/; International Conference on 3D Vision
  (3DV) 2025

---

## DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance  Prior

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Mingrui Li, Shuhong Liu, Tianchen Deng, Hongyu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.09111v1){: .btn .btn-green } |

**Abstract**: Gaussian SLAM systems excel in real-time rendering and fine-grained
reconstruction compared to NeRF-based systems. However, their reliance on
extensive keyframes is impractical for deployment in real-world robotic
systems, which typically operate under sparse-view conditions that can result
in substantial holes in the map. To address these challenges, we introduce
DenseSplat, the first SLAM system that effectively combines the advantages of
NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for
initializing primitives that densely populate maps and seamlessly fill gaps. It
also implements geometry-aware primitive sampling and pruning strategies to
manage granularity and enhance rendering efficiency. Moreover, DenseSplat
integrates loop closure and bundle adjustment, significantly enhancing
frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale
datasets demonstrate that DenseSplat achieves superior performance in tracking
and mapping compared to current state-of-the-art methods.



---

## Large Images are Gaussians: High-Quality Large Image Representation with  Levels of 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Lingting Zhu, Guying Lin, Jinnan Chen, Xinjie Zhang, Zhenchao Jin, Zhao Wang, Lequan Yu | cs.CV | [PDF](http://arxiv.org/pdf/2502.09039v1){: .btn .btn-green } |

**Abstract**: While Implicit Neural Representations (INRs) have demonstrated significant
success in image representation, they are often hindered by large training
memory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged
as a promising solution in 3D reconstruction due to its high-quality novel view
synthesis and rapid rendering capabilities, positioning it as a valuable tool
for a broad spectrum of applications. In particular, a GS-based representation,
2DGS, has shown potential for image fitting. In our work, we present
\textbf{L}arge \textbf{I}mages are \textbf{G}aussians (\textbf{LIG}), which
delves deeper into the application of 2DGS for image representations,
addressing the challenge of fitting large images with 2DGS in the situation of
numerous Gaussian points, through two distinct modifications: 1) we adopt a
variant of representation and optimization strategy, facilitating the fitting
of a large number of Gaussian points; 2) we propose a Level-of-Gaussian
approach for reconstructing both coarse low-frequency initialization and fine
high-frequency details. Consequently, we successfully represent large images as
Gaussian points and achieve high-quality large image representation,
demonstrating its efficacy across various types of large images. Code is
available at
{\href{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}.

Comments:
- Accepted by 39th Annual AAAI Conference on Artificial Intelligence
  (AAAI 2025). 10 pages, 4 figures

---

## X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional  Watermarks

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Zihang Cheng, Huiping Zhuang, Chun Li, Xin Meng, Ming Li, Fei Richard Yu | cs.CR | [PDF](http://arxiv.org/pdf/2502.10475v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D
generation. Training to get a 3DGS scene often takes a lot of time and
resources and even valuable inspiration. The increasing amount of 3DGS digital
asset have brought great challenges to the copyright protection. However, it
still lacks profound exploration targeted at 3DGS. In this paper, we propose a
new framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages
while keeping the original 3DGS scene almost unchanged. Generally, we have a
X-SG$^2$S injector for adding multi-modal messages simultaneously and an
extractor for extract them. Specifically, we first split the watermarks into
message patches in a fixed manner and sort the 3DGS points. A self-adaption
gate is used to pick out suitable location for watermarking. Then use a
XD(multi-dimension)-injection heads to add multi-modal messages into sorted
3DGS points. A learnable gate can recognize the location with extra messages
and XD-extraction heads can restore hidden messages from the location
recommended by the learnable gate. Extensive experiments demonstrated that the
proposed X-SG$^2$S can effectively conceal multi modal messages without
changing pretrained 3DGS pipeline or the original form of 3DGS parameters.
Meanwhile, with simple and efficient model structure and high practicality,
X-SG$^2$S still shows good performance in hiding and extracting multi-modal
inner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to
3D watermarking model for 3DGS and the first framework to add multi-modal
watermarks simultaneous in one 3DGS which pave the wave for later researches.



---

## Self-Calibrating Gaussian Splatting for Large Field of View  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec | cs.CV | [PDF](http://arxiv.org/pdf/2502.09563v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a self-calibrating framework that jointly optimizes
camera parameters, lens distortion and 3D Gaussian representations, enabling
accurate and efficient scene reconstruction. In particular, our technique
enables high-quality scene reconstruction from Large field-of-view (FOV)
imagery taken with wide-angle lenses, allowing the scene to be modeled from a
smaller number of images. Our approach introduces a novel method for modeling
complex lens distortions using a hybrid network that combines invertible
residual networks with explicit grids. This design effectively regularizes the
optimization process, achieving greater accuracy than conventional camera
models. Additionally, we propose a cubemap-based resampling strategy to support
large FOV images without sacrificing resolution or introducing distortion
artifacts. Our method is compatible with the fast rasterization of Gaussian
Splatting, adaptable to a wide variety of camera lens distortion, and
demonstrates state-of-the-art performance on both synthetic and real-world
datasets.

Comments:
- Project Page: https://denghilbert.github.io/self-cali/

---

## Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF  Architectures

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano | cs.CV | [PDF](http://arxiv.org/pdf/2502.09623v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for
representing 3D objects and scenes by encoding shape and appearance information
into the weights of a neural network. Recent works have shown how such weights
can be used as input to frameworks processing them to solve deep learning
tasks. Yet, these frameworks can only process NeRFs with a specific, predefined
architecture. In this paper, we present the first framework that can ingest
NeRFs with multiple architectures and perform inference on architectures unseen
at training time. We achieve this goal by training a Graph Meta-Network in a
representation learning framework. Moreover, we show how a contrastive
objective is conducive to obtaining an architecture-agnostic latent space. In
experiments on both MLP-based and tri-planar NeRFs, our approach demonstrates
robust performance in classification and retrieval tasks that either matches or
exceeds that of existing frameworks constrained to single architectures, thus
providing the first architecture-agnostic method to perform tasks on NeRFs by
processing their weights.

Comments:
- Under review

---

## Interactive Holographic Visualization for 3D Facial Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-12 | Tri Tung Nguyen Nguyen, Fujii Yasuyuki, Dinh Tuan Tran, Joo-Ho Lee | cs.GR | [PDF](http://arxiv.org/pdf/2502.08085v1){: .btn .btn-green } |

**Abstract**: Traditional methods for visualizing dynamic human expressions, particularly
in medical training, often rely on flat-screen displays or static mannequins,
which have proven inefficient for realistic simulation. In response, we propose
a platform that leverages a 3D interactive facial avatar capable of displaying
non-verbal feedback, including pain signals. This avatar is projected onto a
stereoscopic, view-dependent 3D display, offering a more immersive and
realistic simulated patient experience for pain assessment practice. However,
there is no existing solution that dynamically predicts and projects
interactive 3D facial avatars in real-time. To overcome this, we emphasize the
need for a 3D display projection system that can project the facial avatar
holographically, allowing users to interact with the avatar from any viewpoint.
By incorporating 3D Gaussian Splatting (3DGS) and real-time view-dependent
calibration, we significantly improve the training environment for accurate
pain recognition and assessment.



---

## Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images  with Depth and Normal Supervision

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-12 | Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo | cs.CV | [PDF](http://arxiv.org/pdf/2502.08352v1){: .btn .btn-green } |

**Abstract**: With advancements in satellite imaging technology, acquiring high-resolution
multi-view satellite imagery has become increasingly accessible, enabling rapid
and location-independent ground model reconstruction. However, traditional
stereo matching methods struggle to capture fine details, and while neural
radiance fields (NeRFs) achieve high-quality reconstructions, their training
time is prohibitively long. Moreover, challenges such as low visibility of
building facades, illumination and style differences between pixels, and weakly
textured regions in satellite imagery further make it hard to reconstruct
reasonable terrain geometry and detailed building facades. To address these
issues, we propose Sat-DN, a novel framework leveraging a progressively trained
multi-resolution hash grid reconstruction architecture with explicit depth
guidance and surface normal consistency constraints to enhance reconstruction
quality. The multi-resolution hash grid accelerates training, while the
progressive strategy incrementally increases the learning frequency, using
coarse low-frequency geometry to guide the reconstruction of fine
high-frequency details. The depth and normal constraints ensure a clear
building outline and correct planar distribution. Extensive experiments on the
DFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving
state-of-the-art results in both qualitative and quantitative evaluations. The
code is available at https://github.com/costune/SatDN.



---

## TranSplat: Surface Embedding-guided 3D Gaussian Splatting for  Transparent Object Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-11 | Jeongyun Kim, Jeongho Noh, Dong-Guw Lee, Ayoung Kim | cs.CV | [PDF](http://arxiv.org/pdf/2502.07840v1){: .btn .btn-green } |

**Abstract**: Transparent object manipulation remains a sig- nificant challenge in robotics
due to the difficulty of acquiring accurate and dense depth measurements.
Conventional depth sensors often fail with transparent objects, resulting in
in- complete or erroneous depth data. Existing depth completion methods
struggle with interframe consistency and incorrectly model transparent objects
as Lambertian surfaces, leading to poor depth reconstruction. To address these
challenges, we propose TranSplat, a surface embedding-guided 3D Gaussian
Splatting method tailored for transparent objects. TranSplat uses a latent
diffusion model to generate surface embeddings that provide consistent and
continuous representations, making it robust to changes in viewpoint and
lighting. By integrating these surface embeddings with input RGB images,
TranSplat effectively captures the complexities of transparent surfaces,
enhancing the splatting of 3D Gaussians and improving depth completion.
Evaluations on synthetic and real-world transpar- ent object benchmarks, as
well as robot grasping tasks, show that TranSplat achieves accurate and dense
depth completion, demonstrating its effectiveness in practical applications. We
open-source synthetic dataset and model: https://github.
com/jeongyun0609/TranSplat

Comments:
- 7 pages, 6 figures

---

## Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained  Matching Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-11 | Lin-Zhuo Chen, Kangjie Liu, Youtian Lin, Siyu Zhu, Zhihao Li, Xun Cao, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2502.07615v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has achieved excellent rendering quality with
fast training and rendering speed. However, its optimization process lacks
explicit geometric constraints, leading to suboptimal geometric reconstruction
in regions with sparse or no observational input views. In this work, we try to
mitigate the issue by incorporating a pre-trained matching prior to the 3DGS
optimization process. We introduce Flow Distillation Sampling (FDS), a
technique that leverages pre-trained geometric knowledge to bolster the
accuracy of the Gaussian radiance field. Our method employs a strategic
sampling technique to target unobserved views adjacent to the input views,
utilizing the optical flow calculated from the matching model (Prior Flow) to
guide the flow analytically calculated from the 3DGS geometry (Radiance Flow).
Comprehensive experiments in depth rendering, mesh reconstruction, and novel
view synthesis showcase the significant advantages of FDS over state-of-the-art
methods. Additionally, our interpretive experiments and analysis aim to shed
light on the effects of FDS on geometric accuracy and rendering quality,
potentially providing readers with insights into its performance. Project page:
https://nju-3dv.github.io/projects/fds

Comments:
- Accepted by ICLR 2025

---

## MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-11 | Rafał Tobiasz, Grzegorz Wilczyński, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek | cs.GR | [PDF](http://arxiv.org/pdf/2502.07754v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer
graphics. GS-based algorithms almost always bypass classical methods such as
ray tracing, which offers numerous inherent advantages for rendering. For
example, ray tracing is able to handle incoherent rays for advanced lighting
effects, including shadows and reflections. To address this limitation, we
introduce MeshSplats, a method which converts GS to a mesh-like format.
Following the completion of training, MeshSplats transforms Gaussian elements
into mesh faces, enabling rendering using ray tracing methods with all their
associated benefits. Our model can be utilized immediately following
transformation, yielding a mesh of slightly reduced quality without additional
training. Furthermore, we can enhance the reconstruction quality through the
application of a dedicated optimization algorithm that operates on mesh faces
rather than Gaussian components. The efficacy of our method is substantiated by
experimental results, underscoring its extensive applications in computer
graphics and image processing.



---

## PrismAvatar: Real-time animated 3D neural head avatars on edge devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira | cs.CV | [PDF](http://arxiv.org/pdf/2502.07030v1){: .btn .btn-green } |

**Abstract**: We present PrismAvatar: a 3D head avatar model which is designed specifically
to enable real-time animation and rendering on resource-constrained edge
devices, while still enjoying the benefits of neural volumetric rendering at
training time. By integrating a rigged prism lattice with a 3D morphable head
model, we use a hybrid rendering model to simultaneously reconstruct a
mesh-based head and a deformable NeRF model for regions not represented by the
3DMM. We then distill the deformable NeRF into a rigged mesh and neural
textures, which can be animated and rendered efficiently within the constraints
of the traditional triangle rendering pipeline. In addition to running at 60
fps with low memory usage on mobile devices, we find that our trained models
have comparable quality to state-of-the-art 3D avatar models on desktop
devices.

Comments:
- 8 pages, 5 figures

---

## Three-Dimensional MRI Reconstruction with Gaussian Representations:  Tackling the Undersampling Problem

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Tengya Peng, Ruyi Zha, Zhen Li, Xiaofeng Liu, Qing Zou | eess.IV | [PDF](http://arxiv.org/pdf/2502.06510v1){: .btn .btn-green } |

**Abstract**: Three-Dimensional Gaussian Splatting (3DGS) has shown substantial promise in
the field of computer vision, but remains unexplored in the field of magnetic
resonance imaging (MRI). This study explores its potential for the
reconstruction of isotropic resolution 3D MRI from undersampled k-space data.
We introduce a novel framework termed 3D Gaussian MRI (3DGSMR), which employs
3D Gaussian distributions as an explicit representation for MR volumes.
Experimental evaluations indicate that this method can effectively reconstruct
voxelized MR images, achieving a quality on par with that of well-established
3D MRI reconstruction techniques found in the literature. Notably, the 3DGSMR
scheme operates under a self-supervised framework, obviating the need for
extensive training datasets or prior model training. This approach introduces
significant innovations to the domain, notably the adaptation of 3DGS to MRI
reconstruction and the novel application of the existing 3DGS methodology to
decompose MR signals, which are presented in a complex-valued format.



---

## Grounding Creativity in Physics: A Brief Survey of Physical Priors in  AIGC

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Siwei Meng, Yawei Luo, Ping Liu | cs.CV | [PDF](http://arxiv.org/pdf/2502.07007v1){: .btn .btn-green } |

**Abstract**: Recent advancements in AI-generated content have significantly improved the
realism of 3D and 4D generation. However, most existing methods prioritize
appearance consistency while neglecting underlying physical principles, leading
to artifacts such as unrealistic deformations, unstable dynamics, and
implausible objects interactions. Incorporating physics priors into generative
models has become a crucial research direction to enhance structural integrity
and motion realism. This survey provides a review of physics-aware generative
methods, systematically analyzing how physical constraints are integrated into
3D and 4D generation. First, we examine recent works in incorporating physical
priors into static and dynamic 3D generation, categorizing methods based on
representation types, including vision-based, NeRF-based, and Gaussian
Splatting-based approaches. Second, we explore emerging techniques in 4D
generation, focusing on methods that model temporal dynamics with physical
simulations. Finally, we conduct a comparative analysis of major methods,
highlighting their strengths, limitations, and suitability for different
materials and motion dynamics. By presenting an in-depth analysis of
physics-grounded AIGC, this survey aims to bridge the gap between generative
models and physical realism, providing insights that inspire future research in
physically consistent content generation.



---

## SIREN: Semantic, Initialization-Free Registration of Multi-Robot  Gaussian Splatting Maps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Ola Shorinwa, Jiankai Sun, Mac Schwager, Anirudha Majumdar | cs.RO | [PDF](http://arxiv.org/pdf/2502.06519v1){: .btn .btn-green } |

**Abstract**: We present SIREN for registration of multi-robot Gaussian Splatting (GSplat)
maps, with zero access to camera poses, images, and inter-map transforms for
initialization or fusion of local submaps. To realize these capabilities, SIREN
harnesses the versatility and robustness of semantics in three critical ways to
derive a rigorous registration pipeline for multi-robot GSplat maps. First,
SIREN utilizes semantics to identify feature-rich regions of the local maps
where the registration problem is better posed, eliminating the need for any
initialization which is generally required in prior work. Second, SIREN
identifies candidate correspondences between Gaussians in the local maps using
robust semantic features, constituting the foundation for robust geometric
optimization, coarsely aligning 3D Gaussian primitives extracted from the local
maps. Third, this key step enables subsequent photometric refinement of the
transformation between the submaps, where SIREN leverages novel-view synthesis
in GSplat maps along with a semantics-based image filter to compute a
high-accuracy non-rigid transformation for the generation of a high-fidelity
fused map. We demonstrate the superior performance of SIREN compared to
competing baselines across a range of real-world datasets, and in particular,
across the most widely-used robot hardware platforms, including a manipulator,
drone, and quadruped. In our experiments, SIREN achieves about 90x smaller
rotation errors, 300x smaller translation errors, and 44x smaller scale errors
in the most challenging scenes, where competing methods struggle. We will
release the code and provide a link to the project page after the review
process.



---

## PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based  Implicit Neural Map

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-09 | Yue Pan, Xingguang Zhong, Liren Jin, Louis Wiesmann, Marija Popović, Jens Behley, Cyrill Stachniss | cs.RO | [PDF](http://arxiv.org/pdf/2502.05752v1){: .btn .btn-green } |

**Abstract**: Robots require high-fidelity reconstructions of their environment for
effective operation. Such scene representations should be both, geometrically
accurate and photorealistic to support downstream tasks. While this can be
achieved by building distance fields from range sensors and radiance fields
from cameras, the scalable incremental mapping of both fields consistently and
at the same time with high quality remains challenging. In this paper, we
propose a novel map representation that unifies a continuous signed distance
field and a Gaussian splatting radiance field within an elastic and compact
point-based implicit neural map. By enforcing geometric consistency between
these fields, we achieve mutual improvements by exploiting both modalities. We
devise a LiDAR-visual SLAM system called PINGS using the proposed map
representation and evaluate it on several challenging large-scale datasets.
Experimental results demonstrate that PINGS can incrementally build globally
consistent distance and radiance fields encoded with a compact set of neural
points. Compared to the state-of-the-art methods, PINGS achieves superior
photometric and geometric rendering at novel views by leveraging the
constraints from the distance field. Furthermore, by utilizing dense
photometric cues and multi-view consistency from the radiance field, PINGS
produces more accurate distance fields, leading to improved odometry estimation
and mesh reconstruction.

Comments:
- 14 pages, 8 figures

---

## Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual  Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps  Platform

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-09 | Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2502.05769v2){: .btn .btn-green } |

**Abstract**: Urban digital twins are virtual replicas of cities that use multi-source data
and data analytics to optimize urban planning, infrastructure management, and
decision-making. Towards this, we propose a framework focused on the
single-building scale. By connecting to cloud mapping platforms such as Google
Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language
Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our
Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings
framework can retrieve a building's 3D model, visual descriptions, and achieve
cloud-based mapping integration with large language model-based data analytics
using a building's address, postal code, or geographic coordinates.

Comments:
- -Fixed minor typo

---

## GWRF: A Generalizable Wireless Radiance Field for Wireless Signal  Propagation Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-08 | Kang Yang, Yuning Chen, Wan Du | cs.NI | [PDF](http://arxiv.org/pdf/2502.05708v1){: .btn .btn-green } |

**Abstract**: We present Generalizable Wireless Radiance Fields (GWRF), a framework for
modeling wireless signal propagation at arbitrary 3D transmitter and receiver
positions. Unlike previous methods that adapt vanilla Neural Radiance Fields
(NeRF) from the optical to the wireless signal domain, requiring extensive
per-scene training, GWRF generalizes effectively across scenes. First, a
geometry-aware Transformer encoder-based wireless scene representation module
incorporates information from geographically proximate transmitters to learn a
generalizable wireless radiance field. Second, a neural-driven ray tracing
algorithm operates on this field to automatically compute signal reception at
the receiver. Experimental results demonstrate that GWRF outperforms existing
methods on single scenes and achieves state-of-the-art performance on unseen
scenes.



---

## Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV  in Ocean Environment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-08 | Maneesha Wickramasuriya, Beomyeol Yu, Taeyoung Lee, Murray Snyder | cs.CV | [PDF](http://arxiv.org/pdf/2502.05409v1){: .btn .btn-green } |

**Abstract**: This paper proposes a vision-in-the-loop simulation environment for deep
monocular pose estimation of a UAV operating in an ocean environment. Recently,
a deep neural network with a transformer architecture has been successfully
trained to estimate the pose of a UAV relative to the flight deck of a research
vessel, overcoming several limitations of GPS-based approaches. However,
validating the deep pose estimation scheme in an actual ocean environment poses
significant challenges due to the limited availability of research vessels and
the associated operational costs. To address these issues, we present a
photo-realistic 3D virtual environment leveraging recent advancements in
Gaussian splatting, a novel technique that represents 3D scenes by modeling
image pixels as Gaussian distributions in 3D space, creating a lightweight and
high-quality visual model from multiple viewpoints. This approach enables the
creation of a virtual environment integrating multiple real-world images
collected in situ. The resulting simulation enables the indoor testing of
flight maneuvers while verifying all aspects of flight software, hardware, and
the deep monocular pose estimation scheme. This approach provides a
cost-effective solution for testing and validating the autonomous flight of
shipboard UAVs, specifically focusing on vision-based control and estimation
algorithms.

Comments:
- 8 pages, 15 figures, conference

---

## High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Zihao Zou, Ziyuan Qu, Xi Peng, Vivek Boominathan, Adithya Pediredla, Praneeth Chakravarthula | cs.CV | [PDF](http://arxiv.org/pdf/2502.04630v1){: .btn .btn-green } |

**Abstract**: Capturing and reconstructing high-speed dynamic 3D scenes has numerous
applications in computer graphics, vision, and interdisciplinary fields such as
robotics, aerodynamics, and evolutionary biology. However, achieving this using
a single imaging modality remains challenging. For instance, traditional RGB
cameras suffer from low frame rates, limited exposure times, and narrow
baselines. To address this, we propose a novel sensor fusion approach using
Gaussian splatting, which combines RGB, depth, and event cameras to capture and
reconstruct deforming scenes at high speeds. The key insight of our method lies
in leveraging the complementary strengths of these imaging modalities: RGB
cameras capture detailed color information, event cameras record rapid scene
changes with microsecond resolution, and depth cameras provide 3D scene
geometry. To unify the underlying scene representation across these modalities,
we represent the scene using deformable 3D Gaussians. To handle rapid scene
movements, we jointly optimize the 3D Gaussian parameters and their temporal
deformation fields by integrating data from all three sensor modalities. This
fusion enables efficient, high-quality imaging of fast and complex scenes, even
under challenging conditions such as low light, narrow baselines, or rapid
motion. Experiments on synthetic and real datasets captured with our prototype
sensor fusion setup demonstrate that our method significantly outperforms
state-of-the-art techniques, achieving noticeable improvements in both
rendering fidelity and structural accuracy.



---

## AuraFusion360: Augmented Unseen Region Alignment for Reference-based  360° Unbounded Scene Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2502.05176v1){: .btn .btn-green } |

**Abstract**: Three-dimensional scene inpainting is crucial for applications from virtual
reality to architectural visualization, yet existing methods struggle with view
consistency and geometric accuracy in 360{\deg} unbounded scenes. We present
AuraFusion360, a novel reference-based method that enables high-quality object
removal and hole filling in 3D scenes represented by Gaussian Splatting. Our
approach introduces (1) depth-aware unseen mask generation for accurate
occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot
method for accurate initial point placement without requiring additional
training, and (3) SDEdit-based detail enhancement for multi-view coherence. We
also introduce 360-USID, the first comprehensive dataset for 360{\deg}
unbounded scene inpainting with ground truth. Extensive experiments demonstrate
that AuraFusion360 significantly outperforms existing methods, achieving
superior perceptual quality while maintaining geometric accuracy across
dramatic viewpoint changes. See our project page for video results and the
dataset at https://kkennethwu.github.io/aurafusion360/.

Comments:
- Project page: https://kkennethwu.github.io/aurafusion360/

---

## GaussRender: Learning 3D Occupancy with Gaussian Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Loick Chambon, Eloi Zablocki, Alexandre Boulch, Mickael Chen, Matthieu Cord | cs.CV | [PDF](http://arxiv.org/pdf/2502.05040v1){: .btn .btn-green } |

**Abstract**: Understanding the 3D geometry and semantics of driving scenes is critical for
developing of safe autonomous vehicles. While 3D occupancy models are typically
trained using voxel-based supervision with standard losses (e.g.,
cross-entropy, Lovasz, dice), these approaches treat voxel predictions
independently, neglecting their spatial relationships. In this paper, we
propose GaussRender, a plug-and-play 3D-to-2D reprojection loss that enhances
voxel-based supervision. Our method projects 3D voxel representations into
arbitrary 2D perspectives and leverages Gaussian splatting as an efficient,
differentiable rendering proxy of voxels, introducing spatial dependencies
across projected elements. This approach improves semantic and geometric
consistency, handles occlusions more efficiently, and requires no architectural
modifications. Extensive experiments on multiple benchmarks
(SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate
consistent performance gains across various 3D occupancy models (TPVFormer,
SurroundOcc, Symphonies), highlighting the robustness and versatility of our
framework. The code is available at https://github.com/valeoai/GaussRender.



---

## SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Huajian Huang, Yingshu Chen, Longwei Li, Hui Cheng, Tristan Braud, Yajie Zhao, Sai-Kit Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2502.04734v1){: .btn .btn-green } |

**Abstract**: 360-degree cameras streamline data collection for radiance field 3D
reconstruction by capturing comprehensive scene data. However, traditional
radiance field methods do not address the specific challenges inherent to
360-degree images. We present SC-OmniGS, a novel self-calibrating
omnidirectional Gaussian splatting system for fast and accurate omnidirectional
radiance field reconstruction using 360-degree images. Rather than converting
360-degree images to cube maps and performing perspective image calibration, we
treat 360-degree images as a whole sphere and derive a mathematical framework
that enables direct omnidirectional camera pose calibration accompanied by 3D
Gaussians optimization. Furthermore, we introduce a differentiable
omnidirectional camera model in order to rectify the distortion of real-world
data for performance enhancement. Overall, the omnidirectional camera intrinsic
model, extrinsic poses, and 3D Gaussians are jointly optimized by minimizing
weighted spherical photometric loss. Extensive experiments have demonstrated
that our proposed SC-OmniGS is able to recover a high-quality radiance field
from noisy camera poses or even no pose prior in challenging scenarios
characterized by wide baselines and non-object-centric configurations. The
noticeable performance gain in the real-world dataset captured by
consumer-grade omnidirectional cameras verifies the effectiveness of our
general omnidirectional camera model in reducing the distortion of 360-degree
images.

Comments:
- Accepted to ICLR 2025, Project Page:
  http://www.chenyingshu.com/sc-omnigs/

---

## OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and  Geometric-Aware Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Xiaoyu Zhou, Jingqi Wang, Yongtao Wang, Yufei Wei, Nan Dong, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2502.04981v1){: .btn .btn-green } |

**Abstract**: Obtaining semantic 3D occupancy from raw sensor data without manual
annotations remains an essential yet challenging task. While prior works have
approached this as a perception prediction problem, we formulate it as
scene-aware 3D occupancy reconstruction with geometry and semantics. In this
work, we propose OccGS, a novel 3D Occupancy reconstruction framework utilizing
Semantic and Geometric-Aware Gaussian Splatting in a zero-shot manner.
Leveraging semantics extracted from vision-language models and geometry guided
by LiDAR points, OccGS constructs Semantic and Geometric-Aware Gaussians from
raw multisensor data. We also develop a cumulative Gaussian-to-3D voxel
splatting method for reconstructing occupancy from the Gaussians. OccGS
performs favorably against self-supervised methods in occupancy prediction,
achieving comparable performance to fully supervised approaches and achieving
state-of-the-art performance on zero-shot semantic 3D occupancy estimation.



---

## PoI: Pixel of Interest for Novel View Synthesis Assisted Scene  Coordinate Regression

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2502.04843v2){: .btn .btn-green } |

**Abstract**: The task of estimating camera poses can be enhanced through novel view
synthesis techniques such as NeRF and Gaussian Splatting to increase the
diversity and extension of training data. However, these techniques often
produce rendered images with issues like blurring and ghosting, which
compromise their reliability. These issues become particularly pronounced for
Scene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the
pixel level. To mitigate the problems associated with unreliable rendered
images, we introduce a novel filtering approach, which selectively extracts
well-rendered pixels while discarding the inferior ones. This filter
simultaneously measures the SCR model's real-time reprojection loss and
gradient during training. Building on this filtering technique, we also develop
a new strategy to improve scene coordinate regression using sparse inputs,
drawing on successful applications of sparse input techniques in novel view
synthesis. Our experimental results validate the effectiveness of our method,
demonstrating state-of-the-art performance on indoor and outdoor datasets.



---

## VistaFlow: Photorealistic Volumetric Reconstruction with Dynamic  Resolution Management via Q-Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-05 | Jayram Palamadai, William Yu | cs.CV | [PDF](http://arxiv.org/pdf/2502.05222v1){: .btn .btn-green } |

**Abstract**: We introduce VistaFlow, a scalable three-dimensional imaging technique
capable of reconstructing fully interactive 3D volumetric images from a set of
2D photographs. Our model synthesizes novel viewpoints through a differentiable
rendering system capable of dynamic resolution management on photorealistic 3D
scenes. We achieve this through the introduction of QuiQ, a novel intermediate
video controller trained through Q-learning to maintain a consistently high
framerate by adjusting render resolution with millisecond precision. Notably,
VistaFlow runs natively on integrated CPU graphics, making it viable for mobile
and entry-level devices while still delivering high-performance rendering.
VistaFlow bypasses Neural Radiance Fields (NeRFs), using the PlenOctree data
structure to render complex light interactions such as reflection and
subsurface scattering with minimal hardware requirements. Our model is capable
of outperforming state-of-the-art methods with novel view synthesis at a
resolution of 1080p at over 100 frames per second on consumer hardware. By
tailoring render quality to the capabilities of each device, VistaFlow has the
potential to improve the efficiency and accessibility of photorealistic 3D
scene rendering across a wide spectrum of hardware, from high-end workstations
to inexpensive microcontrollers.



---

## GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-05 | Mingrui Li, Weijian Chen, Na Cheng, Jingyuan Xu, Dong Li, Hongyu Wang | cs.RO | [PDF](http://arxiv.org/pdf/2502.03228v1){: .btn .btn-green } |

**Abstract**: The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread
attention due to its excellent performance in real-time high-fidelity
rendering. However, in real-world environments with dynamic objects, existing
3DGS-based SLAM systems often face mapping errors and tracking drift issues. To
address these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM
system tailored for dynamic scenes. In terms of tracking, unlike traditional
methods, we directly perform dynamic segmentation on Gaussians and map them
back to the front-end to obtain dynamic point labels through a Gaussian pyramid
network, achieving precise dynamic removal and robust tracking. For mapping, we
impose rendering penalties on dynamically labeled Gaussians, which are updated
through the network, to avoid irreversible erroneous removal caused by simple
pruning. Our results on real-world datasets demonstrate that our method is
competitive in tracking compared to baseline methods, generating fewer
artifacts and higher-quality reconstructions in rendering.



---

## SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with  Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Yifu Tao, Maurice Fallon | cs.RO | [PDF](http://arxiv.org/pdf/2502.02657v1){: .btn .btn-green } |

**Abstract**: We present a neural radiance field (NeRF) based large-scale reconstruction
system that fuses lidar and vision data to generate high-quality
reconstructions that are geometrically accurate and capture photorealistic
texture. Our system adopts the state-of-the-art NeRF representation to
additionally incorporate lidar. Adding lidar data adds strong geometric
constraints on the depth and surface normals, which is particularly useful when
modelling uniform texture surfaces which contain ambiguous visual
reconstruction cues. Furthermore, we estimate the epistemic uncertainty of the
reconstruction as the spatial variance of each point location in the radiance
field given the sensor observations from camera and lidar. This enables the
identification of areas that are reliably reconstructed by each sensor
modality, allowing the map to be filtered according to the estimated
uncertainty. Our system can also exploit the trajectory produced by a real-time
pose-graph lidar SLAM system during online mapping to bootstrap a
(post-processed) Structure-from-Motion (SfM) reconstruction procedure reducing
SfM training time by up to 70%. It also helps to properly constrain the overall
metric scale which is essential for the lidar depth loss. The
globally-consistent trajectory can then be divided into submaps using Spectral
Clustering to group sets of co-visible images together. This submapping
approach is more suitable for visual reconstruction than distance-based
partitioning. Each submap is filtered according to point-wise uncertainty
estimates and merged to obtain the final large-scale 3D reconstruction. We
demonstrate the reconstruction system using a multi-camera, lidar sensor suite
in experiments involving both robot-mounted and handheld scanning. Our test
datasets cover a total area of more than 20,000 square metres, including
multiple university buildings and an aerial survey of a multi-storey.

Comments:
- webpage: https://dynamic.robots.ox.ac.uk/projects/silvr/

---

## LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene  Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Yang Zhou, Zongjin He, Qixuan Li, Chao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.01949v1){: .btn .btn-green } |

**Abstract**: Recently, the field of text-guided 3D scene generation has garnered
significant attention. High-quality generation that aligns with physical
realism and high controllability is crucial for practical 3D scene
applications. However, existing methods face fundamental limitations: (i)
difficulty capturing complex relationships between multiple objects described
in the text, (ii) inability to generate physically plausible scene layouts, and
(iii) lack of controllability and extensibility in compositional scenes. In
this paper, we introduce LayoutDreamer, a framework that leverages 3D Gaussian
Splatting (3DGS) to facilitate high-quality, physically consistent
compositional scene generation guided by text. Specifically, given a text
prompt, we convert it into a directed scene graph and adaptively adjust the
density and layout of the initial compositional 3D Gaussians. Subsequently,
dynamic camera adjustments are made based on the training focal point to ensure
entity-level generation quality. Finally, by extracting directed dependencies
from the scene graph, we tailor physical and layout energy to ensure both
realism and flexibility. Comprehensive experiments demonstrate that
LayoutDreamer outperforms other compositional scene generation quality and
semantic alignment methods. Specifically, it achieves state-of-the-art (SOTA)
performance in the multiple objects generation metric of T3Bench.



---

## MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by  Continual Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2502.02372v1){: .btn .btn-green } |

**Abstract**: The generation of a virtual digital avatar is a crucial research topic in the
field of computer vision. Many existing works utilize Neural Radiance Fields
(NeRF) to address this issue and have achieved impressive results. However,
previous works assume the images of the training person are available and fixed
while the appearances and poses of a subject could constantly change and
increase in real-world scenarios. How to update the human avatar but also
maintain the ability to render the old appearance of the person is a practical
challenge. One trivial solution is to combine the existing virtual avatar
models based on NeRF with continual learning methods. However, there are some
critical issues in this approach: learning new appearances and poses can cause
the model to forget past information, which in turn leads to a degradation in
the rendering quality of past appearances, especially color bleeding issues,
and incorrect human body poses. In this work, we propose a maintainable avatar
(MaintaAvatar) based on neural radiance fields by continual learning, which
resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose
Distillation Module. Overall, our model requires only limited data collection
to quickly fine-tune the model while avoiding catastrophic forgetting, thus
achieving a maintainable virtual avatar. The experimental results validate the
effectiveness of our MaintaAvatar model.

Comments:
- AAAI 2025. 9 pages

---

## GP-GS: Gaussian Processes for Enhanced Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.02283v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view
synthesis method. However, its reliance on sparse Structure-from-Motion (SfM)
point clouds consistently compromises the scene reconstruction quality. To
address these limitations, this paper proposes a novel 3D reconstruction
framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output
Gaussian Process model is developed to achieve adaptive and uncertainty-guided
densification of sparse SfM point clouds. Specifically, we propose a dynamic
sampling and filtering pipeline that adaptively expands the SfM point clouds by
leveraging GP-based predictions to infer new candidate points from the input 2D
pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the
pruning of high-variance predictions, ensuring geometric consistency and
enabling the generation of dense point clouds. The densified point clouds
provide high-quality initial 3D Gaussians to enhance reconstruction
performance. Extensive experiments conducted on synthetic and real-world
datasets across various scales validate the effectiveness and practicality of
the proposed framework.

Comments:
- 14 pages,11 figures

---

## Scalable 3D Gaussian Splatting-Based RF Signal Spatial Propagation  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Kang Yang, Gaofeng Dong, Sijie Ji, Wan Du, Mani Srivastava | cs.NI | [PDF](http://arxiv.org/pdf/2502.01826v1){: .btn .btn-green } |

**Abstract**: Effective network planning and sensing in wireless networks require
resource-intensive site surveys for data collection. An alternative is
Radio-Frequency (RF) signal spatial propagation modeling, which computes
received signals given transceiver positions in a scene (e.g.s a conference
room). We identify a fundamental trade-off between scalability and fidelity in
the state-of-the-art method. To address this issue, we explore leveraging 3D
Gaussian Splatting (3DGS), an advanced technique for the image synthesis of 3D
scenes in real-time from arbitrary camera poses. By integrating domain-specific
insights, we design three components for adapting 3DGS to the RF domain,
including Gaussian-based RF scene representation, gradient-guided RF attribute
learning, and RF-customized CUDA for ray tracing. Building on them, we develop
RFSPM, an end-to-end framework for scalable RF signal Spatial Propagation
Modeling. We evaluate RFSPM in four field studies and two applications across
RFID, BLE, LoRa, and 5G, covering diverse frequencies, antennas, signals, and
scenes. The results show that RFSPM matches the fidelity of the
state-of-the-art method while reducing data requirements, training GPU-hours,
and inference latency by up to 9.8\,$\times$, 18.6\,$\times$, and
84.4\,$\times$, respectively.



---

## UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Aashish Rai, Dilin Wang, Mihir Jain, Nikolaos Sarafianos, Kefan Chen, Srinath Sridhar, Aayush Prakash | cs.CV | [PDF](http://arxiv.org/pdf/2502.01846v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D
objects and scenes. However, generating 3DGS remains challenging due to their
discrete, unstructured, and permutation-invariant nature. In this work, we
present a simple yet effective method to overcome these challenges. We utilize
spherical mapping to transform 3DGS into a structured 2D representation, termed
UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a
concatenation of Gaussian attributes such as position, scale, color, opacity,
and rotation. We further find that these heterogeneous features can be
compressed into a lower-dimensional (e.g., 3-channel) shared feature space
using a carefully designed multi-branch network. The compressed UVGS can be
treated as typical RGB images. Remarkably, we discover that typical VAEs
trained with latent diffusion models can directly generalize to this new
representation without additional training. Our novel representation makes it
effortless to leverage foundational 2D models, such as diffusion models, to
directly model 3DGS. Additionally, one can simply increase the 2D UV resolution
to accommodate more Gaussians, making UVGS a scalable solution compared to
typical 3D backbones. This approach immediately unlocks various novel
generation applications of 3DGS by inherently utilizing the already developed
superior 2D generation capabilities. In our experiments, we demonstrate various
unconditional, conditional generation, and inpainting applications of 3DGS
based on diffusion models, which were previously non-trivial.

Comments:
- https://aashishrai3799.github.io/uvgs

---

## FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Diego Gomez, Bingchen Gong, Maks Ovsjanikov | cs.CV | [PDF](http://arxiv.org/pdf/2502.01405v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce FourieRF, a novel approach for achieving fast and
high-quality reconstruction in the few-shot setting. Our method effectively
parameterizes features through an explicit curriculum training procedure,
incrementally increasing scene complexity during optimization. Experimental
results show that the prior induced by our approach is both robust and
adaptable across a wide variety of scenes, establishing FourieRF as a strong
and versatile baseline for the few-shot rendering problem. While our approach
significantly reduces artifacts, it may still lead to reconstruction errors in
severely under-constrained scenarios, particularly where view occlusion leaves
parts of the shape uncovered. In the future, our method could be enhanced by
integrating foundation models to complete missing parts using large data-driven
priors.

Comments:
- 8 pages, 3DV 2025 conference

---

## Radiant Foam: Real-Time Differentiable Ray Tracing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2502.01157v1){: .btn .btn-green } |

**Abstract**: Research on differentiable scene representations is consistently moving
towards more efficient, real-time models. Recently, this has led to the
popularization of splatting methods, which eschew the traditional ray-based
rendering of radiance fields in favor of rasterization. This has yielded a
significant improvement in rendering speeds due to the efficiency of
rasterization algorithms and hardware, but has come at a cost: the
approximations that make rasterization efficient also make implementation of
light transport phenomena like reflection and refraction much more difficult.
We propose a novel scene representation which avoids these approximations, but
keeps the efficiency and reconstruction quality of splatting by leveraging a
decades-old efficient volumetric mesh ray tracing algorithm which has been
largely overlooked in recent computer vision research. The resulting model,
which we name Radiant Foam, achieves rendering speed and quality comparable to
Gaussian Splatting, without the constraints of rasterization. Unlike ray traced
Gaussian models that use hardware ray tracing acceleration, our method requires
no special hardware or APIs beyond the standard features of a programmable GPU.



---

## VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and  Locomotion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, Hang Zhao | cs.RO | [PDF](http://arxiv.org/pdf/2502.01536v1){: .btn .btn-green } |

**Abstract**: Recent success in legged robot locomotion is attributed to the integration of
reinforcement learning and physical simulators. However, these policies often
encounter challenges when deployed in real-world environments due to
sim-to-real gaps, as simulators typically fail to replicate visual realism and
complex real-world geometry. Moreover, the lack of realistic visual rendering
limits the ability of these policies to support high-level tasks requiring
RGB-based perception like ego-centric navigation. This paper presents a
Real-to-Sim-to-Real framework that generates photorealistic and physically
interactive "digital twin" simulation environments for visual navigation and
locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based
scene reconstruction from multi-view images and integrates these environments
into simulations that support ego-centric visual perception and mesh-based
physical interactions. To demonstrate its effectiveness, we train a
reinforcement learning policy within the simulator to perform a visual
goal-tracking task. Extensive experiments show that our framework achieves
RGB-only sim-to-real policy transfer. Additionally, our framework facilitates
the rapid adaptation of robot policies with effective exploration capability in
complex new environments, highlighting its potential for applications in
households and factories.

Comments:
- Project Page: https://vr-robo.github.io/

---

## EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head  Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-02 | Junuk Cha, Seongro Yoon, Valeriya Strizhkova, Francois Bremond, Seungryul Baek | cs.CV | [PDF](http://arxiv.org/pdf/2502.00654v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting-based talking head synthesis has recently gained
attention for its ability to render high-fidelity images with real-time
inference speed. However, since it is typically trained on only a short video
that lacks the diversity in facial emotions, the resultant talking heads
struggle to represent a wide range of emotions. To address this issue, we
propose a lip-aligned emotional face generator and leverage it to train our
EmoTalkingGaussian model. It is able to manipulate facial emotions conditioned
on continuous emotion values (i.e., valence and arousal); while retaining
synchronization of lip movements with input audio. Additionally, to achieve the
accurate lip synchronization for in-the-wild audio, we introduce a
self-supervised learning method that leverages a text-to-speech network and a
visual-audio synchronization network. We experiment our EmoTalkingGaussian on
publicly available videos and have obtained better results than
state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS),
emotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy),
and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively.

Comments:
- 22 pages
