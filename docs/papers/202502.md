---
layout: default
title: February 2025
parent: Papers
nav_order: 202502
---

<!---metadata--->


## Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-24 | Chong Cheng, Gaochao Song, Yiyang Yao, Qinzheng Zhou, Gangjian Zhang, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.17377v1){: .btn .btn-green } |

**Abstract**: This paper investigates an open research challenge of reconstructing
high-quality, large 3D open scenes from images. It is observed existing methods
have various limitations, such as requiring precise camera poses for input and
dense viewpoints for supervision. To perform effective and efficient 3D scene
reconstruction, we propose a novel graph-guided 3D scene reconstruction
framework, GraphGS. Specifically, given a set of images captured by RGB cameras
on a scene, we first design a spatial prior-based scene structure estimation
method. This is then used to create a camera graph that includes information
about the camera topology. Further, we propose to apply the graph-guided
multi-view consistency constraint and adaptive sampling strategy to the 3D
Gaussian Splatting optimization process. This greatly alleviates the issue of
Gaussian points overfitting to specific sparse viewpoints and expedites the 3D
reconstruction process. We demonstrate GraphGS achieves high-fidelity 3D
reconstruction from images, which presents state-of-the-art performance through
quantitative and qualitative evaluation across multiple datasets. Project Page:
https://3dagentworld.github.io/graphgs.

Comments:
- ICLR 2025

---

## GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using  Gaussian Splatting and Temporal Flow

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-24 | Simon Boeder, Fabian Gigengack, Benjamin Risse | cs.CV | [PDF](http://arxiv.org/pdf/2502.17288v1){: .btn .btn-green } |

**Abstract**: Occupancy estimation has become a prominent task in 3D computer vision,
particularly within the autonomous driving community. In this paper, we present
a novel approach to occupancy estimation, termed GaussianFlowOcc, which is
inspired by Gaussian Splatting and replaces traditional dense voxel grids with
a sparse 3D Gaussian representation. Our efficient model architecture based on
a Gaussian Transformer significantly reduces computational and memory
requirements by eliminating the need for expensive 3D convolutions used with
inefficient voxel-based representations that predominantly represent empty 3D
spaces. GaussianFlowOcc effectively captures scene dynamics by estimating
temporal flow for each Gaussian during the overall network training process,
offering a straightforward solution to a complex problem that is often
neglected by existing methods. Moreover, GaussianFlowOcc is designed for
scalability, as it employs weak supervision and does not require costly dense
3D voxel annotations based on additional data (e.g., LiDAR). Through extensive
experimentation, we demonstrate that GaussianFlowOcc significantly outperforms
all previous methods for weakly supervised occupancy estimation on the nuScenes
dataset while featuring an inference speed that is 50 times faster than current
SOTA.



---

## VR-Pipe: Streamlining Hardware Graphics Pipeline for Volume Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-24 | Junseo Lee, Jaisung Kim, Junyong Park, Jaewoong Sim | cs.GR | [PDF](http://arxiv.org/pdf/2502.17078v1){: .btn .btn-green } |

**Abstract**: Graphics rendering that builds on machine learning and radiance fields is
gaining significant attention due to its outstanding quality and speed in
generating photorealistic images from novel viewpoints. However, prior work has
primarily focused on evaluating its performance through software-based
rendering on programmable shader cores, leaving its performance when exploiting
fixed-function graphics units largely unexplored.
  In this paper, we investigate the performance implications of performing
radiance field rendering on the hardware graphics pipeline. In doing so, we
implement the state-of-the-art radiance field method, 3D Gaussian splatting,
using graphics APIs and evaluate it across synthetic and real-world scenes on
today's graphics hardware. Based on our analysis, we present VR-Pipe, which
seamlessly integrates two innovations into graphics hardware to streamline the
hardware pipeline for volume rendering, such as radiance field methods. First,
we introduce native hardware support for early termination by repurposing
existing special-purpose hardware in modern GPUs. Second, we propose
multi-granular tile binning with quad merging, which opportunistically blends
fragments in shader cores before passing them to fixed-function blending units.
Our evaluation shows that VR-Pipe greatly improves rendering performance,
achieving up to a 2.78x speedup over the conventional graphics pipeline with
negligible hardware overhead.

Comments:
- To appear at the 31st International Symposium on High-Performance
  Computer Architecture (HPCA 2025)

---

## Semantic Neural Radiance Fields for Multi-Date Satellite Data

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-24 | Valentin Wagner, Sebastian Bullinger, Christoph Bodensteiner, Michael Arens | cs.CV | [PDF](http://arxiv.org/pdf/2502.16992v1){: .btn .btn-green } |

**Abstract**: In this work we propose a satellite specific Neural Radiance Fields (NeRF)
model capable to obtain a three-dimensional semantic representation (neural
semantic field) of the scene. The model derives the output from a set of
multi-date satellite images with corresponding pixel-wise semantic labels. We
demonstrate the robustness of our approach and its capability to improve noisy
input labels. We enhance the color prediction by utilizing the semantic
information to address temporal image inconsistencies caused by non-stationary
categories such as vehicles. To facilitate further research in this domain, we
present a dataset comprising manually generated labels for popular multi-view
satellite images. Our code and dataset are available at
https://github.com/wagnva/semantic-nerf-for-satellite-data.

Comments:
- Accepted at the CV4EO Workshop at WACV 2025

---

## Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language  Embedding Registration

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-23 | Kim Jun-Seong, GeonU Kim, Kim Yu-Ji, Yu-Chiang Frank Wang, Jaesung Choe, Tae-Hyun Oh | cs.CV | [PDF](http://arxiv.org/pdf/2502.16652v1){: .btn .btn-green } |

**Abstract**: We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene
understanding leveraging 3D Gaussian Splatting. Unlike existing
language-embedded 3DGS methods, which rely on a rendering process, our method
directly associates language-aligned CLIP embeddings with 3D Gaussians for
holistic 3D scene understanding. The key of our method is a language feature
registration technique where CLIP embeddings are assigned to the dominant
Gaussians intersected by each pixel-ray. Moreover, we integrate Product
Quantization (PQ) trained on general large-scale image data to compactly
represent embeddings without per-scene optimization. Experiments demonstrate
that our approach significantly outperforms existing approaches in 3D
perception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D
object localization, and 3D object selection tasks. For video results, please
visit : https://drsplat.github.io/

Comments:
- 20 pages

---

## Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with  Drag-Based Control

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-23 | Jinbo Yan, Alan Zhao, Yixin Hu | cs.CV | [PDF](http://arxiv.org/pdf/2502.16475v1){: .btn .btn-green } |

**Abstract**: Single-image 3D generation has emerged as a prominent research topic, playing
a vital role in virtual reality, 3D modeling, and digital content creation.
However, existing methods face challenges such as a lack of multi-view
geometric consistency and limited controllability during the generation
process, which significantly restrict their usability. % To tackle these
challenges, we introduce Dragen3D, a novel approach that achieves geometrically
consistent and controllable 3D generation leveraging 3D Gaussian Splatting
(3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS
VAE), which encodes a point cloud and a single image into anchor latents and
decode these latents into 3DGS, enabling efficient latent-space generation. To
enable multi-view geometry consistent and controllable generation, we propose a
Seed-Point-Driven strategy: first generate sparse seed points as a coarse
geometry representation, then map them to anchor latents via the Seed-Anchor
Mapping Module. Geometric consistency is ensured by the easily learned sparse
seed points, and users can intuitively drag the seed points to deform the final
3DGS geometry, with changes propagated through the anchor latents. To the best
of our knowledge, we are the first to achieve geometrically controllable 3D
Gaussian generation and editing without relying on 2D diffusion priors,
delivering comparable 3D generation quality to state-of-the-art methods.



---

## GS-TransUNet: Integrated 2D Gaussian Splatting and Transformer UNet for  Accurate Skin Lesion Analysis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-23 | Anand Kumar, Kavinder Roghit Kanthen, Josna John | cs.CV | [PDF](http://arxiv.org/pdf/2502.16748v1){: .btn .btn-green } |

**Abstract**: We can achieve fast and consistent early skin cancer detection with recent
developments in computer vision and deep learning techniques. However, the
existing skin lesion segmentation and classification prediction models run
independently, thus missing potential efficiencies from their integrated
execution. To unify skin lesion analysis, our paper presents the Gaussian
Splatting - Transformer UNet (GS-TransUNet), a novel approach that
synergistically combines 2D Gaussian splatting with the Transformer UNet
architecture for automated skin cancer diagnosis. Our unified deep learning
model efficiently delivers dual-function skin lesion classification and
segmentation for clinical diagnosis. Evaluated on ISIC-2017 and PH2 datasets,
our network demonstrates superior performance compared to existing
state-of-the-art models across multiple metrics through 5-fold
cross-validation. Our findings illustrate significant advancements in the
precision of segmentation and classification. This integration sets new
benchmarks in the field and highlights the potential for further research into
multi-task medical image analysis methodologies, promising enhancements in
automated diagnostic systems.

Comments:
- 12 pages, 7 figures, SPIE Medical Imaging 2025

---

## AquaNeRF: Neural Radiance Fields in Underwater Media with Distractor  Removal

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-22 | Luca Gough, Adrian Azzarelli, Fan Zhang, Nantheera Anantrasirichai | cs.CV | [PDF](http://arxiv.org/pdf/2502.16351v1){: .btn .btn-green } |

**Abstract**: Neural radiance field (NeRF) research has made significant progress in
modeling static video content captured in the wild. However, current models and
rendering processes rarely consider scenes captured underwater, which are
useful for studying and filming ocean life. They fail to address visual
artifacts unique to underwater scenes, such as moving fish and suspended
particles. This paper introduces a novel NeRF renderer and optimization scheme
for an implicit MLP-based NeRF model. Our renderer reduces the influence of
floaters and moving objects that interfere with static objects of interest by
estimating a single surface per ray. We use a Gaussian weight function with a
small offset to ensure that the transmittance of the surrounding media remains
constant. Additionally, we enhance our model with a depth-based scaling
function to upscale gradients for near-camera volumes. Overall, our method
outperforms the baseline Nerfacto by approximately 7.5\% and SeaThru-NeRF by
6.2% in terms of PSNR. Subjective evaluation also shows a significant reduction
of artifacts while preserving details of static targets and background compared
to the state of the arts.

Comments:
- Accepted by 2025 IEEE International Symposium on Circuits and Systems

---

## DualNeRF: Text-Driven 3D Scene Editing via Dual-Field Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-22 | Yuxuan Xiong, Yue Shi, Yishun Dou, Bingbing Ni | cs.CV | [PDF](http://arxiv.org/pdf/2502.16302v1){: .btn .btn-green } |

**Abstract**: Recently, denoising diffusion models have achieved promising results in 2D
image generation and editing. Instruct-NeRF2NeRF (IN2N) introduces the success
of diffusion into 3D scene editing through an "Iterative dataset update" (IDU)
strategy. Though achieving fascinating results, IN2N suffers from problems of
blurry backgrounds and trapping in local optima. The first problem is caused by
IN2N's lack of efficient guidance for background maintenance, while the second
stems from the interaction between image editing and NeRF training during IDU.
In this work, we introduce DualNeRF to deal with these problems. We propose a
dual-field representation to preserve features of the original scene and
utilize them as additional guidance to the model for background maintenance
during IDU. Moreover, a simulated annealing strategy is embedded into IDU to
endow our model with the power of addressing local optima issues. A CLIP-based
consistency indicator is used to further improve the editing quality by
filtering out low-quality edits. Extensive experiments demonstrate that our
method outperforms previous methods both qualitatively and quantitatively.



---

## Para-Lane: Multi-Lane Dataset Registering Parallel Scans for  Benchmarking Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-21 | Ziqian Ni, Sicong Du, Zhenghua Hou, Chenming Wu, Sheng Yang | cs.CV | [PDF](http://arxiv.org/pdf/2502.15635v2){: .btn .btn-green } |

**Abstract**: To evaluate end-to-end autonomous driving systems, a simulation environment
based on Novel View Synthesis (NVS) techniques is essential, which synthesizes
photo-realistic images and point clouds from previously recorded sequences
under new vehicle poses, particularly in cross-lane scenarios. Therefore, the
development of a multi-lane dataset and benchmark is necessary. While recent
synthetic scene-based NVS datasets have been prepared for cross-lane
benchmarking, they still lack the realism of captured images and point clouds.
To further assess the performance of existing methods based on NeRF and 3DGS,
we present the first multi-lane dataset registering parallel scans specifically
for novel driving view synthesis dataset derived from real-world scans,
comprising 25 groups of associated sequences, including 16,000 front-view
images, 64,000 surround-view images, and 16,000 LiDAR frames. All frames are
labeled to differentiate moving objects from static elements. Using this
dataset, we evaluate the performance of existing approaches in various testing
scenarios at different lanes and distances. Additionally, our method provides
the solution for solving and assessing the quality of multi-sensor poses for
multi-modal data alignment for curating such a dataset in real-world. We plan
to continually add new sequences to test the generalization of existing methods
across different scenarios. The dataset is released publicly at the project
page: https://nizqleo.github.io/paralane-dataset/.

Comments:
- Accepted by International Conference on 3D Vision (3DV) 2025

---

## DynamicGSG: Dynamic 3D Gaussian Scene Graphs for Environment Adaptation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-21 | Luzhou Ge, Xiangyu Zhu, Zhuo Yang, Xuesong Li | cs.RO | [PDF](http://arxiv.org/pdf/2502.15309v2){: .btn .btn-green } |

**Abstract**: In real-world scenarios, environment changes caused by human or agent
activities make it extremely challenging for robots to perform various
long-term tasks. Recent works typically struggle to effectively understand and
adapt to dynamic environments due to the inability to update their environment
representations in memory according to environment changes and lack of
fine-grained reconstruction of the environments. To address these challenges,
we propose DynamicGSG, a dynamic, high-fidelity, open-vocabulary scene graph
construction system leveraging Gaussian splatting. DynamicGSG builds
hierarchical scene graphs using advanced vision language models to represent
the spatial and semantic relationships between objects in the environments,
utilizes a joint feature loss we designed to supervise Gaussian instance
grouping while optimizing the Gaussian maps, and locally updates the Gaussian
scene graphs according to real environment changes for long-term environment
adaptation. Experiments and ablation studies demonstrate the performance and
efficacy of our proposed method in terms of semantic segmentation,
language-guided object retrieval, and reconstruction quality. Furthermore, we
validate the dynamic updating capabilities of our system in real laboratory
environments. The source code and supplementary experimental materials will be
released
at:~\href{https://github.com/GeLuzhou/Dynamic-GSG}{https://github.com/GeLuzhou/Dynamic-GSG}.



---

## RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-21 | Sicheng Yu, Chong Cheng, Yifan Zhou, Xiaojun Yang, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.15633v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can
produce high-fidelity novel views. However, previous GS-based methods primarily
target indoor scenes and rely on RGB-D sensors or pre-trained depth estimation
models, hence underperforming in outdoor scenarios. To address this issue, we
propose a RGB-only gaussian splatting SLAM method for unbounded outdoor
scenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network
to generate consistent pointmaps between frames for pose estimation. Compared
to commonly used depth maps, pointmaps include spatial relationships and scene
geometry across multiple views, enabling robust camera pose estimation. Then,
we propose integrating the estimated camera poses with 3DGS rendering as an
end-to-end differentiable pipeline. Our method achieves simultaneous
optimization of camera poses and 3DGS scene parameters, significantly enhancing
system tracking accuracy. Specifically, we also design an adaptive scale mapper
for the pointmap regression network, which provides more accurate pointmap
mapping to the 3DGS map representation. Our experiments on the Waymo dataset
demonstrate that OpenGS-SLAM reduces tracking error to 9.8\% of previous 3DGS
methods, and achieves state-of-the-art results in novel view synthesis. Project
Page: https://3dagentworld.github.io/opengs-slam/

Comments:
- ICRA 2025

---

## CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-20 | Qilin Zhang, Olaf Wysocki, Steffen Urban, Boris Jutzi | cs.GR | [PDF](http://arxiv.org/pdf/2502.14684v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown significant advantages in novel view
synthesis (NVS), particularly in achieving high rendering speeds and
high-quality results. However, its geometric accuracy in 3D reconstruction
remains limited due to the lack of explicit geometric constraints during
optimization. This paper introduces CDGS, a confidence-aware depth
regularization approach developed to enhance 3DGS. We leverage multi-cue
confidence maps of monocular depth estimation and sparse Structure-from-Motion
depth to adaptively adjust depth supervision during the optimization process.
Our method demonstrates improved geometric detail preservation in early
training stages and achieves competitive performance in both NVS quality and
geometric accuracy. Experiments on the publicly available Tanks and Temples
benchmark dataset show that our method achieves more stable convergence
behavior and more accurate geometric reconstruction results, with improvements
of up to 2.31 dB in PSNR for NVS and consistently lower geometric errors in
M3C2 distance metrics. Notably, our method reaches comparable F-scores to the
original 3DGS with only 50% of the training iterations. We expect this work
will facilitate the development of efficient and accurate 3D reconstruction
systems for real-world applications such as digital twin creation, heritage
preservation, or forestry applications.



---

## NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio  Disentanglement for Talking Head Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-20 | Xiaoxing Liu, Zhilei Liu, Chongke Bi | cs.GR | [PDF](http://arxiv.org/pdf/2502.14178v1){: .btn .btn-green } |

**Abstract**: Talking head synthesis is to synthesize a lip-synchronized talking head video
using audio. Recently, the capability of NeRF to enhance the realism and
texture details of synthesized talking heads has attracted the attention of
researchers. However, most current NeRF methods based on audio are exclusively
concerned with the rendering of frontal faces. These methods are unable to
generate clear talking heads in novel views. Another prevalent challenge in
current 3D talking head synthesis is the difficulty in aligning acoustic and
visual spaces, which often results in suboptimal lip-syncing of the generated
talking heads. To address these issues, we propose Neural Radiance Field with
3D Prior Aided Audio Disentanglement for Talking Head Synthesis
(NeRF-3DTalker). Specifically, the proposed method employs 3D prior information
to synthesize clear talking heads with free views. Additionally, we propose a
3D Prior Aided Audio Disentanglement module, which is designed to disentangle
the audio into two distinct categories: features related to 3D awarded speech
movements and features related to speaking style. Moreover, to reposition the
generated frames that are distant from the speaker's motion space in the real
space, we have devised a local-global Standardized Space. This method
normalizes the irregular positions in the generated frames from both global and
local semantic perspectives. Through comprehensive qualitative and quantitative
experiments, it has been demonstrated that our NeRF-3DTalker outperforms
state-of-the-art in synthesizing realistic talking head videos, exhibiting
superior image quality and lip synchronization. Project page:
https://nerf-3dtalker.github.io/NeRF-3Dtalker.

Comments:
- Accepted by ICASSP 2025

---

## OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-20 | Yedong Shen, Xinran Zhang, Yifan Duan, Shiqi Zhang, Heng Li, Yilong Wu, Jianmin Ji, Yanyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2502.14235v1){: .btn .btn-green } |

**Abstract**: Accurate and realistic 3D scene reconstruction enables the lifelike creation
of autonomous driving simulation environments. With advancements in 3D Gaussian
Splatting (3DGS), previous studies have applied it to reconstruct complex
dynamic driving scenes. These methods typically require expensive LiDAR sensors
and pre-annotated datasets of dynamic objects. To address these challenges, we
propose OG-Gaussian, a novel approach that replaces LiDAR point clouds with
Occupancy Grids (OGs) generated from surround-view camera images using
Occupancy Prediction Network (ONet). Our method leverages the semantic
information in OGs to separate dynamic vehicles from static street background,
converting these grids into two distinct sets of initial point clouds for
reconstructing both static and dynamic objects. Additionally, we estimate the
trajectories and poses of dynamic objects through a learning-based approach,
eliminating the need for complex manual annotations. Experiments on Waymo Open
dataset demonstrate that OG-Gaussian is on par with the current
state-of-the-art in terms of reconstruction quality and rendering speed,
achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while
significantly reducing computational costs and economic overhead.



---

## Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically  Categorical Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-20 | Boying Li, Vuong Chi Hao, Peter J. Stuckey, Ian Reid, Hamid Rezatofighi | cs.RO | [PDF](http://arxiv.org/pdf/2502.14931v1){: .btn .btn-green } |

**Abstract**: We propose Hier-SLAM++, a comprehensive Neuro-Symbolic semantic 3D Gaussian
Splatting SLAM method with both RGB-D and monocular input featuring an advanced
hierarchical categorical representation, which enables accurate pose estimation
as well as global 3D semantic mapping. The parameter usage in semantic SLAM
systems increases significantly with the growing complexity of the environment,
making scene understanding particularly challenging and costly. To address this
problem, we introduce a novel and general hierarchical representation that
encodes both semantic and geometric information in a compact form into 3D
Gaussian Splatting, leveraging the capabilities of large language models (LLMs)
as well as the 3D generative model. By utilizing the proposed hierarchical tree
structure, semantic information is symbolically represented and learned in an
end-to-end manner. We further introduce a novel semantic loss designed to
optimize hierarchical semantic information through both inter-level and
cross-level optimization. Additionally, we propose an improved SLAM system to
support both RGB-D and monocular inputs using a feed-forward model. To the best
of our knowledge, this is the first semantic monocular Gaussian Splatting SLAM
system, significantly reducing sensor requirements for 3D semantic
understanding and broadening the applicability of semantic Gaussian SLAM
system. We conduct experiments on both synthetic and real-world datasets,
demonstrating superior or on-par performance with state-of-the-art NeRF-based
and Gaussian-based SLAM systems, while significantly reducing storage and
training time requirements.

Comments:
- 15 pages. Under review

---

## GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian  Splatting Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-20 | Miao Tao, Yuanzhen Zhou, Haoran Xu, Zeyu He, Zhenyu Yang, Yuchang Zhang, Zhongling Su, Linning Xu, Zhenxiang Ma, Rong Fu, Hengjie Li, Xingcheng Zhang, Jidong Zhai | cs.CV | [PDF](http://arxiv.org/pdf/2502.14938v1){: .btn .btn-green } |

**Abstract**: Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant
challenges in achieving real-time, high-fidelity performance on consumer-grade
devices. Fully realizing the potential of 3DGS in applications such as virtual
reality (VR) requires addressing critical system-level challenges to support
real-time, immersive experiences. We propose GS-Cache, an end-to-end framework
that seamlessly integrates 3DGS's advanced representation with a highly
optimized rendering system. GS-Cache introduces a cache-centric pipeline to
eliminate redundant computations, an efficiency-aware scheduler for elastic
multi-GPU rendering, and optimized CUDA kernels to overcome computational
bottlenecks. This synergy between 3DGS and system design enables GS-Cache to
achieve up to 5.35x performance improvement, 35% latency reduction, and 42%
lower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with
high visual quality. By bridging the gap between 3DGS's representation power
and the demands of VR systems, GS-Cache establishes a scalable and efficient
framework for real-time neural rendering in immersive environments.



---

## Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-19 | Gan Chen, Ying He, Mulin Yu, F. Richard Yu, Gang Xu, Fei Ma, Ming Li, Guang Zhou | cs.GR | [PDF](http://arxiv.org/pdf/2502.14004v1){: .btn .btn-green } |

**Abstract**: Recent advancements in implicit 3D reconstruction methods, e.g., neural
rendering fields and Gaussian splatting, have primarily focused on novel view
synthesis of static or dynamic objects with continuous motion states. However,
these approaches struggle to efficiently model a human-interactive object with
n movable parts, requiring 2^n separate models to represent all discrete
states. To overcome this limitation, we propose Inter3D, a new benchmark and
approach for novel state synthesis of human-interactive objects. We introduce a
self-collected dataset featuring commonly encountered interactive objects and a
new evaluation pipeline, where only individual part states are observed during
training, while part combination states remain unseen. We also propose a strong
baseline approach that leverages Space Discrepancy Tensors to efficiently
modelling all states of an object. To alleviate the impractical constraints on
camera trajectories across training states, we propose a Mutual State
Regularization mechanism to enhance the spatial density consistency of movable
parts. In addition, we explore two occupancy grid sampling strategies to
facilitate training efficiency. We conduct extensive experiments on the
proposed benchmark, showcasing the challenges of the task and the superiority
of our approach.



---

## 3D Gaussian Splatting aided Localization for Large and Complex  Indoor-Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-19 | Vincent Ress, Jonas Meyer, Wei Zhang, David Skuddis, Uwe Soergel, Norbert Haala | cs.CV | [PDF](http://arxiv.org/pdf/2502.13803v1){: .btn .btn-green } |

**Abstract**: The field of visual localization has been researched for several decades and
has meanwhile found many practical applications. Despite the strong progress in
this field, there are still challenging situations in which established methods
fail. We present an approach to significantly improve the accuracy and
reliability of established visual localization methods by adding rendered
images. In detail, we first use a modern visual SLAM approach that provides a
3D Gaussian Splatting (3DGS) based map to create reference data. We demonstrate
that enriching reference data with images rendered from 3DGS at randomly
sampled poses significantly improves the performance of both geometry-based
visual localization and Scene Coordinate Regression (SCR) methods. Through
comprehensive evaluation in a large industrial environment, we analyze the
performance impact of incorporating these additional rendered views.



---

## GlossGau: Efficient Inverse Rendering for Glossy Surface with  Anisotropic Spherical Gaussian

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-19 | Bang Du, Runfa Blark Li, Chen Du, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2502.14129v1){: .btn .btn-green } |

**Abstract**: The reconstruction of 3D objects from calibrated photographs represents a
fundamental yet intricate challenge in the domains of computer graphics and
vision. Although neural reconstruction approaches based on Neural Radiance
Fields (NeRF) have shown remarkable capabilities, their processing costs remain
substantial. Recently, the advent of 3D Gaussian Splatting (3D-GS) largely
improves the training efficiency and facilitates to generate realistic
rendering in real-time. However, due to the limited ability of Spherical
Harmonics (SH) to represent high-frequency information, 3D-GS falls short in
reconstructing glossy objects. Researchers have turned to enhance the specular
expressiveness of 3D-GS through inverse rendering. Yet these methods often
struggle to maintain the training and rendering efficiency, undermining the
benefits of Gaussian Splatting techniques. In this paper, we introduce
GlossGau, an efficient inverse rendering framework that reconstructs scenes
with glossy surfaces while maintaining training and rendering speeds comparable
to vanilla 3D-GS. Specifically, we explicitly model the surface normals,
Bidirectional Reflectance Distribution Function (BRDF) parameters, as well as
incident lights and use Anisotropic Spherical Gaussian (ASG) to approximate the
per-Gaussian Normal Distribution Function under the microfacet model. We
utilize 2D Gaussian Splatting (2D-GS) as foundational primitives and apply
regularization to significantly alleviate the normal estimation challenge
encountered in related works. Experiments demonstrate that GlossGau achieves
competitive or superior reconstruction on datasets with glossy surfaces.
Compared with previous GS-based works that address the specular surface, our
optimization time is considerably less.



---

## RadSplatter: Extending 3D Gaussian Splatting to Radio Frequencies for  Wireless Radiomap Extrapolation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-18 | Yiheng Wang, Ye Xue, Shutao Zhang, Tsung-Hui Chang | eess.SP | [PDF](http://arxiv.org/pdf/2502.12686v1){: .btn .btn-green } |

**Abstract**: A radiomap represents the spatial distribution of wireless signal strength,
critical for applications like network optimization and autonomous driving.
However, constructing radiomap relies on measuring radio signal power across
the entire system, which is costly in outdoor environments due to large network
scales. We present RadSplatter, a framework that extends 3D Gaussian Splatting
(3DGS) to radio frequencies for efficient and accurate radiomap extrapolation
from sparse measurements. RadSplatter models environmental scatterers and radio
paths using 3D Gaussians, capturing key factors of radio wave propagation. It
employs a relaxed-mean (RM) scheme to reparameterize the positions of 3D
Gaussians from noisy and dense 3D point clouds. A camera-free 3DGS-based
projection is proposed to map 3D Gaussians onto 2D radio beam patterns.
Furthermore, a regularized loss function and recursive fine-tuning using highly
structured sparse measurements in real-world settings are applied to ensure
robust generalization. Experiments on synthetic and real-world data show
state-of-the-art extrapolation accuracy and execution speed.



---

## GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting  View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-18 | Pedro Martin, António Rodrigues, João Ascenso, Maria Paula Queluz | cs.MM | [PDF](http://arxiv.org/pdf/2502.13196v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) offers a promising alternative to Neural Radiance
Fields (NeRF) for real-time 3D scene rendering. Using a set of 3D Gaussians to
represent complex geometry and appearance, GS achieves faster rendering times
and reduced memory consumption compared to the neural network approach used in
NeRF. However, quality assessment of GS-generated static content is not yet
explored in-depth. This paper describes a subjective quality assessment study
that aims to evaluate synthesized videos obtained with several static GS
state-of-the-art methods. The methods were applied to diverse visual scenes,
covering both 360-degree and forward-facing (FF) camera trajectories. Moreover,
the performance of 18 objective quality metrics was analyzed using the scores
resulting from the subjective study, providing insights into their strengths,
limitations, and alignment with human perception. All videos and scores are
made available providing a comprehensive database that can be used as benchmark
on GS view synthesis and objective quality metrics.



---

## ROI-NeRFs: Hi-Fi Visualization of Objects of Interest within a Scene by  NeRFs Composition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-18 | Quoc-Anh Bui, Gilles Rougeron, Géraldine Morin, Simone Gasparini | cs.CV | [PDF](http://arxiv.org/pdf/2502.12673v1){: .btn .btn-green } |

**Abstract**: Efficient and accurate 3D reconstruction is essential for applications in
cultural heritage. This study addresses the challenge of visualizing objects
within large-scale scenes at a high level of detail (LOD) using Neural Radiance
Fields (NeRFs). The aim is to improve the visual fidelity of chosen objects
while maintaining the efficiency of the computations by focusing on details
only for relevant content. The proposed ROI-NeRFs framework divides the scene
into a Scene NeRF, which represents the overall scene at moderate detail, and
multiple ROI NeRFs that focus on user-defined objects of interest. An
object-focused camera selection module automatically groups relevant cameras
for each NeRF training during the decomposition phase. In the composition
phase, a Ray-level Compositional Rendering technique combines information from
the Scene NeRF and ROI NeRFs, allowing simultaneous multi-object rendering
composition. Quantitative and qualitative experiments conducted on two
real-world datasets, including one on a complex eighteen's century cultural
heritage room, demonstrate superior performance compared to baseline methods,
improving LOD for object regions, minimizing artifacts, and without
significantly increasing inference time.

Comments:
- 17 pages including appendix, 16 figures, 8 tables

---

## Geometry-Aware Diffusion Models for Multiview Scene Inpainting

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-18 | Ahmad Salimi, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Konstantinos G. Derpanis | cs.CV | [PDF](http://arxiv.org/pdf/2502.13335v1){: .btn .btn-green } |

**Abstract**: In this paper, we focus on 3D scene inpainting, where parts of an input image
set, captured from different viewpoints, are masked out. The main challenge
lies in generating plausible image completions that are geometrically
consistent across views. Most recent work addresses this challenge by combining
generative models with a 3D radiance field to fuse information across
viewpoints. However, a major drawback of these methods is that they often
produce blurry images due to the fusion of inconsistent cross-view images. To
avoid blurry inpaintings, we eschew the use of an explicit or implicit radiance
field altogether and instead fuse cross-view information in a learned space. In
particular, we introduce a geometry-aware conditional generative model, capable
of inpainting multi-view consistent images based on both geometric and
appearance cues from reference images. A key advantage of our approach over
existing methods is its unique ability to inpaint masked scenes with a limited
number of views (i.e., few-view inpainting), whereas previous methods require
relatively large image sets for their 3D model fitting step. Empirically, we
evaluate and compare our scene-centric inpainting method on two datasets,
SPIn-NeRF and NeRFiller, which contain images captured at narrow and wide
baselines, respectively, and achieve state-of-the-art 3D inpainting performance
on both. Additionally, we demonstrate the efficacy of our approach in the
few-view setting compared to prior methods.

Comments:
- Our project page is available at https://geomvi.github.io

---

## PUGS: Zero-shot Physical Understanding with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Yinghao Shuai, Ran Yu, Yuantao Chen, Zijian Jiang, Xiaowei Song, Nan Wang, Jv Zheng, Jianzhu Ma, Meng Yang, Zhicheng Wang, Wenbo Ding, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2502.12231v1){: .btn .btn-green } |

**Abstract**: Current robotic systems can understand the categories and poses of objects
well. But understanding physical properties like mass, friction, and hardness,
in the wild, remains challenging. We propose a new method that reconstructs 3D
objects using the Gaussian splatting representation and predicts various
physical properties in a zero-shot manner. We propose two techniques during the
reconstruction phase: a geometry-aware regularization loss function to improve
the shape quality and a region-aware feature contrastive loss function to
promote region affinity. Two other new techniques are designed during
inference: a feature-based property propagation module and a volume integration
module tailored for the Gaussian representation. Our framework is named as
zero-shot physical understanding with Gaussian splatting, or PUGS. PUGS
achieves new state-of-the-art results on the standard benchmark of ABO-500 mass
prediction. We provide extensive quantitative ablations and qualitative
visualization to demonstrate the mechanism of our designs. We show the proposed
methodology can help address challenging real-world grasping tasks. Our codes,
data, and models are available at https://github.com/EverNorif/PUGS

Comments:
- ICRA 2025, Project page: https://evernorif.github.io/PUGS/

---

## 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Sheng-Yu Huang, Zi-Ting Chou, Yu-Chiang Frank Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.11801v1){: .btn .btn-green } |

**Abstract**: When performing 3D inpainting using novel-view rendering methods like Neural
Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture
and geometry consistency across camera views has been a challenge. In this
paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided
Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided
by the rendered depth information from each training view, our 3DGIC exploits
background pixels visible across different views for updating the inpainting
mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive
experiments on benchmark datasets, we confirm that our 3DGIC outperforms
current state-of-the-art 3D inpainting methods quantitatively and
qualitatively.



---

## Exploring the Versal AI Engine for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Kotaro Shimamura, Ayumi Ohno, Shinya Takamaeda-Yamazaki | cs.AR | [PDF](http://arxiv.org/pdf/2502.11782v1){: .btn .btn-green } |

**Abstract**: Dataflow-oriented spatial architectures are the emerging paradigm for higher
computation performance and efficiency.
  AMD Versal AI Engine is a commercial spatial architecture consisting of tiles
of VLIW processors supporting SIMD operations arranged in a two-dimensional
mesh.
  The architecture requires the explicit design of task assignments and
dataflow configurations for each tile to maximize performance, demanding
advanced techniques and meticulous design.
  However, a few works revealed the performance characteristics of the Versal
AI Engine through practical workloads.
  In this work, we provide the comprehensive performance evaluation of the
Versal AI Engine using Gaussian feature computation in 3D Gaussian splatting as
a practical workload, and we then propose a novel dedicated algorithm to fully
exploit the hardware architecture.
  The computations of 3D Gaussian splatting include matrix multiplications and
color computations utilizing high-dimensional spherical harmonic coefficients.
  These tasks are processed efficiently by leveraging the SIMD capabilities and
their instruction-level parallelism.
  Additionally, pipelined processing is achieved by assigning different tasks
to individual cores, thereby fully exploiting the spatial parallelism of AI
Engines.
  The proposed method demonstrated a 226-fold throughput increase in
simulation-based evaluation, outperforming a naive approach.
  These findings provide valuable insights for application development that
effectively harnesses the spatial and architectural advantages of AI Engines.



---

## HumanGif: Single-View Human Diffusion with Generative Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji | cs.CV | [PDF](http://arxiv.org/pdf/2502.12080v2){: .btn .btn-green } |

**Abstract**: Previous 3D human creation methods have made significant progress in
synthesizing view-consistent and temporally aligned results from sparse-view
images or monocular videos. However, it remains challenging to produce
perpetually realistic, view-consistent, and temporally coherent human avatars
from a single image, as limited information is available in the single-view
input setting. Motivated by the success of 2D character animation, we propose
HumanGif, a single-view human diffusion model with generative prior.
Specifically, we formulate the single-view-based 3D human novel view and pose
synthesis as a single-view-conditioned human diffusion process, utilizing
generative priors from foundational diffusion models to complement the missing
information. To ensure fine-grained and consistent novel view and pose
synthesis, we introduce a Human NeRF module in HumanGif to learn spatially
aligned features from the input image, implicitly capturing the relative camera
and human pose transformation. Furthermore, we introduce an image-level loss
during optimization to bridge the gap between latent and image spaces in
diffusion models. Extensive experiments on RenderPeople and DNA-Rendering
datasets demonstrate that HumanGif achieves the best perceptual performance,
with better generalizability for novel view and pose synthesis.

Comments:
- Project page: https://skhu101.github.io/HumanGif/

---

## GaussianMotion: End-to-End Learning of Animatable Gaussian Avatars with  Pose Guidance from Text

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Gyumin Shim, Sangmin Lee, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2502.11642v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce GaussianMotion, a novel human rendering model
that generates fully animatable scenes aligned with textual descriptions using
Gaussian Splatting. Although existing methods achieve reasonable text-to-3D
generation of human bodies using various 3D representations, they often face
limitations in fidelity and efficiency, or primarily focus on static models
with limited pose control. In contrast, our method generates fully animatable
3D avatars by combining deformable 3D Gaussian Splatting with text-to-3D score
distillation, achieving high fidelity and efficient rendering for arbitrary
poses. By densely generating diverse random poses during optimization, our
deformable 3D human model learns to capture a wide range of natural motions
distilled from a pose-conditioned diffusion model in an end-to-end manner.
Furthermore, we propose Adaptive Score Distillation that effectively balances
realistic detail and smoothness to achieve optimal 3D results. Experimental
results demonstrate that our approach outperforms existing baselines by
producing high-quality textures in both static and animated results, and by
generating diverse 3D human models from various textual inputs.

Comments:
- 8 pages

---

## High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using  Spatiotemporal Coherent Gaussian Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-17 | Ziye Wang, Yiran Qin, Lin Zeng, Ruimao Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2502.14895v1){: .btn .btn-green } |

**Abstract**: Weather nowcasting is an essential task that involves predicting future radar
echo sequences based on current observations, offering significant benefits for
disaster management, transportation, and urban planning. Current prediction
methods are limited by training and storage efficiency, mainly focusing on 2D
spatial predictions at specific altitudes. Meanwhile, 3D volumetric predictions
at each timestamp remain largely unexplored. To address such a challenge, we
introduce a comprehensive framework for 3D radar sequence prediction in weather
nowcasting, using the newly proposed SpatioTemporal Coherent Gaussian Splatting
(STC-GS) for dynamic radar representation and GauMamba for efficient and
accurate forecasting. Specifically, rather than relying on a 4D Gaussian for
dynamic scene reconstruction, STC-GS optimizes 3D scenes at each frame by
employing a group of Gaussians while effectively capturing their movements
across consecutive frames. It ensures consistent tracking of each Gaussian over
time, making it particularly effective for prediction tasks. With the
temporally correlated Gaussian groups established, we utilize them to train
GauMamba, which integrates a memory mechanism into the Mamba framework. This
allows the model to learn the temporal evolution of Gaussian groups while
efficiently handling a large volume of Gaussian tokens. As a result, it
achieves both efficiency and accuracy in forecasting a wide range of dynamic
meteorological radar signals. The experimental results demonstrate that our
STC-GS can efficiently represent 3D radar sequences with over $16\times$ higher
spatial resolution compared with the existing 3D representation methods, while
GauMamba outperforms state-of-the-art methods in forecasting a broad spectrum
of high-dynamic weather conditions.

Comments:
- Accepted as an Oral paper at ICLR 2025. Project page:
  https://ziyeeee.github.io/stcgs.github.io

---

## GS-GVINS: A Tightly-integrated GNSS-Visual-Inertial Navigation System  Augmented by 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-16 | Zelin Zhou, Saurav Uprety, Shichuang Nie, Hongzhou Yang | cs.RO | [PDF](http://arxiv.org/pdf/2502.10975v1){: .btn .btn-green } |

**Abstract**: Recently, the emergence of 3D Gaussian Splatting (3DGS) has drawn significant
attention in the area of 3D map reconstruction and visual SLAM. While extensive
research has explored 3DGS for indoor trajectory tracking using visual sensor
alone or in combination with Light Detection and Ranging (LiDAR) and Inertial
Measurement Unit (IMU), its integration with GNSS for large-scale outdoor
navigation remains underexplored. To address these concerns, we proposed
GS-GVINS: a tightly-integrated GNSS-Visual-Inertial Navigation System augmented
by 3DGS. This system leverages 3D Gaussian as a continuous differentiable scene
representation in largescale outdoor environments, enhancing navigation
performance through the constructed 3D Gaussian map. Notably, GS-GVINS is the
first GNSS-Visual-Inertial navigation application that directly utilizes the
analytical jacobians of SE3 camera pose with respect to 3D Gaussians. To
maintain the quality of 3DGS rendering in extreme dynamic states, we introduce
a motionaware 3D Gaussian pruning mechanism, updating the map based on relative
pose translation and the accumulated opacity along the camera ray. For
validation, we test our system under different driving environments: open-sky,
sub-urban, and urban. Both self-collected and public datasets are used for
evaluation. The results demonstrate the effectiveness of GS-GVINS in enhancing
navigation accuracy across diverse driving environments.



---

## OMG: Opacity Matters in Material Modeling with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-16 | Silong Yong, Venkata Nagarjun Pudureddiyur Manivannan, Bernhard Kerbl, Zifu Wan, Simon Stepputtis, Katia Sycara, Yaqi Xie | cs.CV | [PDF](http://arxiv.org/pdf/2502.10988v1){: .btn .btn-green } |

**Abstract**: Decomposing geometry, materials and lighting from a set of images, namely
inverse rendering, has been a long-standing problem in computer vision and
graphics. Recent advances in neural rendering enable photo-realistic and
plausible inverse rendering results. The emergence of 3D Gaussian Splatting has
boosted it to the next level by showing real-time rendering potentials. An
intuitive finding is that the models used for inverse rendering do not take
into account the dependency of opacity w.r.t. material properties, namely cross
section, as suggested by optics. Therefore, we develop a novel approach that
adds this dependency to the modeling itself. Inspired by radiative transfer, we
augment the opacity term by introducing a neural network that takes as input
material properties to provide modeling of cross section and a physically
correct activation function. The gradients for material properties are
therefore not only from color but also from opacity, facilitating a constraint
for their optimization. Therefore, the proposed method incorporates more
accurate physical properties compared to previous works. We implement our
method into 3 different baselines that use Gaussian Splatting for inverse
rendering and achieve significant improvements universally in terms of novel
view synthesis and material modeling.

Comments:
- Published as a conference paper at ICLR 2025

---

## E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-15 | Sohaib Zahid, Viktor Rudnev, Eddy Ilg, Vladislav Golyanik | cs.CV | [PDF](http://arxiv.org/pdf/2502.10827v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis techniques predominantly utilize RGB cameras, inheriting
their limitations such as the need for sufficient lighting, susceptibility to
motion blur, and restricted dynamic range. In contrast, event cameras are
significantly more resilient to these limitations but have been less explored
in this domain, particularly in large-scale settings. Current methodologies
primarily focus on front-facing or object-oriented (360-degree view) scenarios.
For the first time, we introduce 3D Gaussians for event-based novel view
synthesis. Our method reconstructs large and unbounded scenes with high visual
quality. We contribute the first real and synthetic event datasets tailored for
this setting. Our method demonstrates superior novel view synthesis and
consistently outperforms the baseline EventNeRF by a margin of 11-25% in PSNR
(dB) while being orders of magnitude faster in reconstruction and rendering.

Comments:
- 15 pages, 10 figures and 3 tables; project page:
  https://4dqv.mpi-inf.mpg.de/E3DGS/; International Conference on 3D Vision
  (3DV) 2025

---

## DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance  Prior

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Mingrui Li, Shuhong Liu, Tianchen Deng, Hongyu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.09111v1){: .btn .btn-green } |

**Abstract**: Gaussian SLAM systems excel in real-time rendering and fine-grained
reconstruction compared to NeRF-based systems. However, their reliance on
extensive keyframes is impractical for deployment in real-world robotic
systems, which typically operate under sparse-view conditions that can result
in substantial holes in the map. To address these challenges, we introduce
DenseSplat, the first SLAM system that effectively combines the advantages of
NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for
initializing primitives that densely populate maps and seamlessly fill gaps. It
also implements geometry-aware primitive sampling and pruning strategies to
manage granularity and enhance rendering efficiency. Moreover, DenseSplat
integrates loop closure and bundle adjustment, significantly enhancing
frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale
datasets demonstrate that DenseSplat achieves superior performance in tracking
and mapping compared to current state-of-the-art methods.



---

## Large Images are Gaussians: High-Quality Large Image Representation with  Levels of 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Lingting Zhu, Guying Lin, Jinnan Chen, Xinjie Zhang, Zhenchao Jin, Zhao Wang, Lequan Yu | cs.CV | [PDF](http://arxiv.org/pdf/2502.09039v1){: .btn .btn-green } |

**Abstract**: While Implicit Neural Representations (INRs) have demonstrated significant
success in image representation, they are often hindered by large training
memory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged
as a promising solution in 3D reconstruction due to its high-quality novel view
synthesis and rapid rendering capabilities, positioning it as a valuable tool
for a broad spectrum of applications. In particular, a GS-based representation,
2DGS, has shown potential for image fitting. In our work, we present
\textbf{L}arge \textbf{I}mages are \textbf{G}aussians (\textbf{LIG}), which
delves deeper into the application of 2DGS for image representations,
addressing the challenge of fitting large images with 2DGS in the situation of
numerous Gaussian points, through two distinct modifications: 1) we adopt a
variant of representation and optimization strategy, facilitating the fitting
of a large number of Gaussian points; 2) we propose a Level-of-Gaussian
approach for reconstructing both coarse low-frequency initialization and fine
high-frequency details. Consequently, we successfully represent large images as
Gaussian points and achieve high-quality large image representation,
demonstrating its efficacy across various types of large images. Code is
available at
{\href{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}.

Comments:
- Accepted by 39th Annual AAAI Conference on Artificial Intelligence
  (AAAI 2025). 10 pages, 4 figures

---

## X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional  Watermarks

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Zihang Cheng, Huiping Zhuang, Chun Li, Xin Meng, Ming Li, Fei Richard Yu | cs.CR | [PDF](http://arxiv.org/pdf/2502.10475v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D
generation. Training to get a 3DGS scene often takes a lot of time and
resources and even valuable inspiration. The increasing amount of 3DGS digital
asset have brought great challenges to the copyright protection. However, it
still lacks profound exploration targeted at 3DGS. In this paper, we propose a
new framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages
while keeping the original 3DGS scene almost unchanged. Generally, we have a
X-SG$^2$S injector for adding multi-modal messages simultaneously and an
extractor for extract them. Specifically, we first split the watermarks into
message patches in a fixed manner and sort the 3DGS points. A self-adaption
gate is used to pick out suitable location for watermarking. Then use a
XD(multi-dimension)-injection heads to add multi-modal messages into sorted
3DGS points. A learnable gate can recognize the location with extra messages
and XD-extraction heads can restore hidden messages from the location
recommended by the learnable gate. Extensive experiments demonstrated that the
proposed X-SG$^2$S can effectively conceal multi modal messages without
changing pretrained 3DGS pipeline or the original form of 3DGS parameters.
Meanwhile, with simple and efficient model structure and high practicality,
X-SG$^2$S still shows good performance in hiding and extracting multi-modal
inner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to
3D watermarking model for 3DGS and the first framework to add multi-modal
watermarks simultaneous in one 3DGS which pave the wave for later researches.



---

## Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF  Architectures

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano | cs.CV | [PDF](http://arxiv.org/pdf/2502.09623v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for
representing 3D objects and scenes by encoding shape and appearance information
into the weights of a neural network. Recent works have shown how such weights
can be used as input to frameworks processing them to solve deep learning
tasks. Yet, these frameworks can only process NeRFs with a specific, predefined
architecture. In this paper, we present the first framework that can ingest
NeRFs with multiple architectures and perform inference on architectures unseen
at training time. We achieve this goal by training a Graph Meta-Network in a
representation learning framework. Moreover, we show how a contrastive
objective is conducive to obtaining an architecture-agnostic latent space. In
experiments on both MLP-based and tri-planar NeRFs, our approach demonstrates
robust performance in classification and retrieval tasks that either matches or
exceeds that of existing frameworks constrained to single architectures, thus
providing the first architecture-agnostic method to perform tasks on NeRFs by
processing their weights.

Comments:
- Under review

---

## Self-Calibrating Gaussian Splatting for Large Field of View  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-13 | Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec | cs.CV | [PDF](http://arxiv.org/pdf/2502.09563v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a self-calibrating framework that jointly optimizes
camera parameters, lens distortion and 3D Gaussian representations, enabling
accurate and efficient scene reconstruction. In particular, our technique
enables high-quality scene reconstruction from Large field-of-view (FOV)
imagery taken with wide-angle lenses, allowing the scene to be modeled from a
smaller number of images. Our approach introduces a novel method for modeling
complex lens distortions using a hybrid network that combines invertible
residual networks with explicit grids. This design effectively regularizes the
optimization process, achieving greater accuracy than conventional camera
models. Additionally, we propose a cubemap-based resampling strategy to support
large FOV images without sacrificing resolution or introducing distortion
artifacts. Our method is compatible with the fast rasterization of Gaussian
Splatting, adaptable to a wide variety of camera lens distortion, and
demonstrates state-of-the-art performance on both synthetic and real-world
datasets.

Comments:
- Project Page: https://denghilbert.github.io/self-cali/

---

## Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images  with Depth and Normal Supervision

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-12 | Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo | cs.CV | [PDF](http://arxiv.org/pdf/2502.08352v1){: .btn .btn-green } |

**Abstract**: With advancements in satellite imaging technology, acquiring high-resolution
multi-view satellite imagery has become increasingly accessible, enabling rapid
and location-independent ground model reconstruction. However, traditional
stereo matching methods struggle to capture fine details, and while neural
radiance fields (NeRFs) achieve high-quality reconstructions, their training
time is prohibitively long. Moreover, challenges such as low visibility of
building facades, illumination and style differences between pixels, and weakly
textured regions in satellite imagery further make it hard to reconstruct
reasonable terrain geometry and detailed building facades. To address these
issues, we propose Sat-DN, a novel framework leveraging a progressively trained
multi-resolution hash grid reconstruction architecture with explicit depth
guidance and surface normal consistency constraints to enhance reconstruction
quality. The multi-resolution hash grid accelerates training, while the
progressive strategy incrementally increases the learning frequency, using
coarse low-frequency geometry to guide the reconstruction of fine
high-frequency details. The depth and normal constraints ensure a clear
building outline and correct planar distribution. Extensive experiments on the
DFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving
state-of-the-art results in both qualitative and quantitative evaluations. The
code is available at https://github.com/costune/SatDN.



---

## Interactive Holographic Visualization for 3D Facial Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-12 | Tri Tung Nguyen Nguyen, Fujii Yasuyuki, Dinh Tuan Tran, Joo-Ho Lee | cs.GR | [PDF](http://arxiv.org/pdf/2502.08085v1){: .btn .btn-green } |

**Abstract**: Traditional methods for visualizing dynamic human expressions, particularly
in medical training, often rely on flat-screen displays or static mannequins,
which have proven inefficient for realistic simulation. In response, we propose
a platform that leverages a 3D interactive facial avatar capable of displaying
non-verbal feedback, including pain signals. This avatar is projected onto a
stereoscopic, view-dependent 3D display, offering a more immersive and
realistic simulated patient experience for pain assessment practice. However,
there is no existing solution that dynamically predicts and projects
interactive 3D facial avatars in real-time. To overcome this, we emphasize the
need for a 3D display projection system that can project the facial avatar
holographically, allowing users to interact with the avatar from any viewpoint.
By incorporating 3D Gaussian Splatting (3DGS) and real-time view-dependent
calibration, we significantly improve the training environment for accurate
pain recognition and assessment.



---

## TranSplat: Surface Embedding-guided 3D Gaussian Splatting for  Transparent Object Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-11 | Jeongyun Kim, Jeongho Noh, Dong-Guw Lee, Ayoung Kim | cs.CV | [PDF](http://arxiv.org/pdf/2502.07840v1){: .btn .btn-green } |

**Abstract**: Transparent object manipulation remains a sig- nificant challenge in robotics
due to the difficulty of acquiring accurate and dense depth measurements.
Conventional depth sensors often fail with transparent objects, resulting in
in- complete or erroneous depth data. Existing depth completion methods
struggle with interframe consistency and incorrectly model transparent objects
as Lambertian surfaces, leading to poor depth reconstruction. To address these
challenges, we propose TranSplat, a surface embedding-guided 3D Gaussian
Splatting method tailored for transparent objects. TranSplat uses a latent
diffusion model to generate surface embeddings that provide consistent and
continuous representations, making it robust to changes in viewpoint and
lighting. By integrating these surface embeddings with input RGB images,
TranSplat effectively captures the complexities of transparent surfaces,
enhancing the splatting of 3D Gaussians and improving depth completion.
Evaluations on synthetic and real-world transpar- ent object benchmarks, as
well as robot grasping tasks, show that TranSplat achieves accurate and dense
depth completion, demonstrating its effectiveness in practical applications. We
open-source synthetic dataset and model: https://github.
com/jeongyun0609/TranSplat

Comments:
- 7 pages, 6 figures

---

## MeshSplats: Mesh-Based Rendering with Gaussian Splatting Initialization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-11 | Rafał Tobiasz, Grzegorz Wilczyński, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek | cs.GR | [PDF](http://arxiv.org/pdf/2502.07754v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) is a recent and pivotal technique in 3D computer
graphics. GS-based algorithms almost always bypass classical methods such as
ray tracing, which offers numerous inherent advantages for rendering. For
example, ray tracing is able to handle incoherent rays for advanced lighting
effects, including shadows and reflections. To address this limitation, we
introduce MeshSplats, a method which converts GS to a mesh-like format.
Following the completion of training, MeshSplats transforms Gaussian elements
into mesh faces, enabling rendering using ray tracing methods with all their
associated benefits. Our model can be utilized immediately following
transformation, yielding a mesh of slightly reduced quality without additional
training. Furthermore, we can enhance the reconstruction quality through the
application of a dedicated optimization algorithm that operates on mesh faces
rather than Gaussian components. The efficacy of our method is substantiated by
experimental results, underscoring its extensive applications in computer
graphics and image processing.



---

## Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained  Matching Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-11 | Lin-Zhuo Chen, Kangjie Liu, Youtian Lin, Siyu Zhu, Zhihao Li, Xun Cao, Yao Yao | cs.CV | [PDF](http://arxiv.org/pdf/2502.07615v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has achieved excellent rendering quality with
fast training and rendering speed. However, its optimization process lacks
explicit geometric constraints, leading to suboptimal geometric reconstruction
in regions with sparse or no observational input views. In this work, we try to
mitigate the issue by incorporating a pre-trained matching prior to the 3DGS
optimization process. We introduce Flow Distillation Sampling (FDS), a
technique that leverages pre-trained geometric knowledge to bolster the
accuracy of the Gaussian radiance field. Our method employs a strategic
sampling technique to target unobserved views adjacent to the input views,
utilizing the optical flow calculated from the matching model (Prior Flow) to
guide the flow analytically calculated from the 3DGS geometry (Radiance Flow).
Comprehensive experiments in depth rendering, mesh reconstruction, and novel
view synthesis showcase the significant advantages of FDS over state-of-the-art
methods. Additionally, our interpretive experiments and analysis aim to shed
light on the effects of FDS on geometric accuracy and rendering quality,
potentially providing readers with insights into its performance. Project page:
https://nju-3dv.github.io/projects/fds

Comments:
- Accepted by ICLR 2025

---

## SIREN: Semantic, Initialization-Free Registration of Multi-Robot  Gaussian Splatting Maps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Ola Shorinwa, Jiankai Sun, Mac Schwager, Anirudha Majumdar | cs.RO | [PDF](http://arxiv.org/pdf/2502.06519v1){: .btn .btn-green } |

**Abstract**: We present SIREN for registration of multi-robot Gaussian Splatting (GSplat)
maps, with zero access to camera poses, images, and inter-map transforms for
initialization or fusion of local submaps. To realize these capabilities, SIREN
harnesses the versatility and robustness of semantics in three critical ways to
derive a rigorous registration pipeline for multi-robot GSplat maps. First,
SIREN utilizes semantics to identify feature-rich regions of the local maps
where the registration problem is better posed, eliminating the need for any
initialization which is generally required in prior work. Second, SIREN
identifies candidate correspondences between Gaussians in the local maps using
robust semantic features, constituting the foundation for robust geometric
optimization, coarsely aligning 3D Gaussian primitives extracted from the local
maps. Third, this key step enables subsequent photometric refinement of the
transformation between the submaps, where SIREN leverages novel-view synthesis
in GSplat maps along with a semantics-based image filter to compute a
high-accuracy non-rigid transformation for the generation of a high-fidelity
fused map. We demonstrate the superior performance of SIREN compared to
competing baselines across a range of real-world datasets, and in particular,
across the most widely-used robot hardware platforms, including a manipulator,
drone, and quadruped. In our experiments, SIREN achieves about 90x smaller
rotation errors, 300x smaller translation errors, and 44x smaller scale errors
in the most challenging scenes, where competing methods struggle. We will
release the code and provide a link to the project page after the review
process.



---

## PrismAvatar: Real-time animated 3D neural head avatars on edge devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira | cs.CV | [PDF](http://arxiv.org/pdf/2502.07030v1){: .btn .btn-green } |

**Abstract**: We present PrismAvatar: a 3D head avatar model which is designed specifically
to enable real-time animation and rendering on resource-constrained edge
devices, while still enjoying the benefits of neural volumetric rendering at
training time. By integrating a rigged prism lattice with a 3D morphable head
model, we use a hybrid rendering model to simultaneously reconstruct a
mesh-based head and a deformable NeRF model for regions not represented by the
3DMM. We then distill the deformable NeRF into a rigged mesh and neural
textures, which can be animated and rendered efficiently within the constraints
of the traditional triangle rendering pipeline. In addition to running at 60
fps with low memory usage on mobile devices, we find that our trained models
have comparable quality to state-of-the-art 3D avatar models on desktop
devices.

Comments:
- 8 pages, 5 figures

---

## Three-Dimensional MRI Reconstruction with Gaussian Representations:  Tackling the Undersampling Problem

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Tengya Peng, Ruyi Zha, Zhen Li, Xiaofeng Liu, Qing Zou | eess.IV | [PDF](http://arxiv.org/pdf/2502.06510v1){: .btn .btn-green } |

**Abstract**: Three-Dimensional Gaussian Splatting (3DGS) has shown substantial promise in
the field of computer vision, but remains unexplored in the field of magnetic
resonance imaging (MRI). This study explores its potential for the
reconstruction of isotropic resolution 3D MRI from undersampled k-space data.
We introduce a novel framework termed 3D Gaussian MRI (3DGSMR), which employs
3D Gaussian distributions as an explicit representation for MR volumes.
Experimental evaluations indicate that this method can effectively reconstruct
voxelized MR images, achieving a quality on par with that of well-established
3D MRI reconstruction techniques found in the literature. Notably, the 3DGSMR
scheme operates under a self-supervised framework, obviating the need for
extensive training datasets or prior model training. This approach introduces
significant innovations to the domain, notably the adaptation of 3DGS to MRI
reconstruction and the novel application of the existing 3DGS methodology to
decompose MR signals, which are presented in a complex-valued format.



---

## Grounding Creativity in Physics: A Brief Survey of Physical Priors in  AIGC

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-10 | Siwei Meng, Yawei Luo, Ping Liu | cs.CV | [PDF](http://arxiv.org/pdf/2502.07007v1){: .btn .btn-green } |

**Abstract**: Recent advancements in AI-generated content have significantly improved the
realism of 3D and 4D generation. However, most existing methods prioritize
appearance consistency while neglecting underlying physical principles, leading
to artifacts such as unrealistic deformations, unstable dynamics, and
implausible objects interactions. Incorporating physics priors into generative
models has become a crucial research direction to enhance structural integrity
and motion realism. This survey provides a review of physics-aware generative
methods, systematically analyzing how physical constraints are integrated into
3D and 4D generation. First, we examine recent works in incorporating physical
priors into static and dynamic 3D generation, categorizing methods based on
representation types, including vision-based, NeRF-based, and Gaussian
Splatting-based approaches. Second, we explore emerging techniques in 4D
generation, focusing on methods that model temporal dynamics with physical
simulations. Finally, we conduct a comparative analysis of major methods,
highlighting their strengths, limitations, and suitability for different
materials and motion dynamics. By presenting an in-depth analysis of
physics-grounded AIGC, this survey aims to bridge the gap between generative
models and physical realism, providing insights that inspire future research in
physically consistent content generation.



---

## PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based  Implicit Neural Map

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-09 | Yue Pan, Xingguang Zhong, Liren Jin, Louis Wiesmann, Marija Popović, Jens Behley, Cyrill Stachniss | cs.RO | [PDF](http://arxiv.org/pdf/2502.05752v1){: .btn .btn-green } |

**Abstract**: Robots require high-fidelity reconstructions of their environment for
effective operation. Such scene representations should be both, geometrically
accurate and photorealistic to support downstream tasks. While this can be
achieved by building distance fields from range sensors and radiance fields
from cameras, the scalable incremental mapping of both fields consistently and
at the same time with high quality remains challenging. In this paper, we
propose a novel map representation that unifies a continuous signed distance
field and a Gaussian splatting radiance field within an elastic and compact
point-based implicit neural map. By enforcing geometric consistency between
these fields, we achieve mutual improvements by exploiting both modalities. We
devise a LiDAR-visual SLAM system called PINGS using the proposed map
representation and evaluate it on several challenging large-scale datasets.
Experimental results demonstrate that PINGS can incrementally build globally
consistent distance and radiance fields encoded with a compact set of neural
points. Compared to the state-of-the-art methods, PINGS achieves superior
photometric and geometric rendering at novel views by leveraging the
constraints from the distance field. Furthermore, by utilizing dense
photometric cues and multi-view consistency from the radiance field, PINGS
produces more accurate distance fields, leading to improved odometry estimation
and mesh reconstruction.

Comments:
- 14 pages, 8 figures

---

## Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual  Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps  Platform

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-09 | Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2502.05769v2){: .btn .btn-green } |

**Abstract**: Urban digital twins are virtual replicas of cities that use multi-source data
and data analytics to optimize urban planning, infrastructure management, and
decision-making. Towards this, we propose a framework focused on the
single-building scale. By connecting to cloud mapping platforms such as Google
Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language
Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our
Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings
framework can retrieve a building's 3D model, visual descriptions, and achieve
cloud-based mapping integration with large language model-based data analytics
using a building's address, postal code, or geographic coordinates.

Comments:
- -Fixed minor typo

---

## GWRF: A Generalizable Wireless Radiance Field for Wireless Signal  Propagation Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-08 | Kang Yang, Yuning Chen, Wan Du | cs.NI | [PDF](http://arxiv.org/pdf/2502.05708v1){: .btn .btn-green } |

**Abstract**: We present Generalizable Wireless Radiance Fields (GWRF), a framework for
modeling wireless signal propagation at arbitrary 3D transmitter and receiver
positions. Unlike previous methods that adapt vanilla Neural Radiance Fields
(NeRF) from the optical to the wireless signal domain, requiring extensive
per-scene training, GWRF generalizes effectively across scenes. First, a
geometry-aware Transformer encoder-based wireless scene representation module
incorporates information from geographically proximate transmitters to learn a
generalizable wireless radiance field. Second, a neural-driven ray tracing
algorithm operates on this field to automatically compute signal reception at
the receiver. Experimental results demonstrate that GWRF outperforms existing
methods on single scenes and achieves state-of-the-art performance on unseen
scenes.



---

## Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV  in Ocean Environment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-08 | Maneesha Wickramasuriya, Beomyeol Yu, Taeyoung Lee, Murray Snyder | cs.CV | [PDF](http://arxiv.org/pdf/2502.05409v1){: .btn .btn-green } |

**Abstract**: This paper proposes a vision-in-the-loop simulation environment for deep
monocular pose estimation of a UAV operating in an ocean environment. Recently,
a deep neural network with a transformer architecture has been successfully
trained to estimate the pose of a UAV relative to the flight deck of a research
vessel, overcoming several limitations of GPS-based approaches. However,
validating the deep pose estimation scheme in an actual ocean environment poses
significant challenges due to the limited availability of research vessels and
the associated operational costs. To address these issues, we present a
photo-realistic 3D virtual environment leveraging recent advancements in
Gaussian splatting, a novel technique that represents 3D scenes by modeling
image pixels as Gaussian distributions in 3D space, creating a lightweight and
high-quality visual model from multiple viewpoints. This approach enables the
creation of a virtual environment integrating multiple real-world images
collected in situ. The resulting simulation enables the indoor testing of
flight maneuvers while verifying all aspects of flight software, hardware, and
the deep monocular pose estimation scheme. This approach provides a
cost-effective solution for testing and validating the autonomous flight of
shipboard UAVs, specifically focusing on vision-based control and estimation
algorithms.

Comments:
- 8 pages, 15 figures, conference

---

## OccGS: Zero-shot 3D Occupancy Reconstruction with Semantic and  Geometric-Aware Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Xiaoyu Zhou, Jingqi Wang, Yongtao Wang, Yufei Wei, Nan Dong, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2502.04981v1){: .btn .btn-green } |

**Abstract**: Obtaining semantic 3D occupancy from raw sensor data without manual
annotations remains an essential yet challenging task. While prior works have
approached this as a perception prediction problem, we formulate it as
scene-aware 3D occupancy reconstruction with geometry and semantics. In this
work, we propose OccGS, a novel 3D Occupancy reconstruction framework utilizing
Semantic and Geometric-Aware Gaussian Splatting in a zero-shot manner.
Leveraging semantics extracted from vision-language models and geometry guided
by LiDAR points, OccGS constructs Semantic and Geometric-Aware Gaussians from
raw multisensor data. We also develop a cumulative Gaussian-to-3D voxel
splatting method for reconstructing occupancy from the Gaussians. OccGS
performs favorably against self-supervised methods in occupancy prediction,
achieving comparable performance to fully supervised approaches and achieving
state-of-the-art performance on zero-shot semantic 3D occupancy estimation.



---

## PoI: Pixel of Interest for Novel View Synthesis Assisted Scene  Coordinate Regression

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2502.04843v2){: .btn .btn-green } |

**Abstract**: The task of estimating camera poses can be enhanced through novel view
synthesis techniques such as NeRF and Gaussian Splatting to increase the
diversity and extension of training data. However, these techniques often
produce rendered images with issues like blurring and ghosting, which
compromise their reliability. These issues become particularly pronounced for
Scene Coordinate Regression (SCR) methods, which estimate 3D coordinates at the
pixel level. To mitigate the problems associated with unreliable rendered
images, we introduce a novel filtering approach, which selectively extracts
well-rendered pixels while discarding the inferior ones. This filter
simultaneously measures the SCR model's real-time reprojection loss and
gradient during training. Building on this filtering technique, we also develop
a new strategy to improve scene coordinate regression using sparse inputs,
drawing on successful applications of sparse input techniques in novel view
synthesis. Our experimental results validate the effectiveness of our method,
demonstrating state-of-the-art performance on indoor and outdoor datasets.



---

## SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Huajian Huang, Yingshu Chen, Longwei Li, Hui Cheng, Tristan Braud, Yajie Zhao, Sai-Kit Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2502.04734v1){: .btn .btn-green } |

**Abstract**: 360-degree cameras streamline data collection for radiance field 3D
reconstruction by capturing comprehensive scene data. However, traditional
radiance field methods do not address the specific challenges inherent to
360-degree images. We present SC-OmniGS, a novel self-calibrating
omnidirectional Gaussian splatting system for fast and accurate omnidirectional
radiance field reconstruction using 360-degree images. Rather than converting
360-degree images to cube maps and performing perspective image calibration, we
treat 360-degree images as a whole sphere and derive a mathematical framework
that enables direct omnidirectional camera pose calibration accompanied by 3D
Gaussians optimization. Furthermore, we introduce a differentiable
omnidirectional camera model in order to rectify the distortion of real-world
data for performance enhancement. Overall, the omnidirectional camera intrinsic
model, extrinsic poses, and 3D Gaussians are jointly optimized by minimizing
weighted spherical photometric loss. Extensive experiments have demonstrated
that our proposed SC-OmniGS is able to recover a high-quality radiance field
from noisy camera poses or even no pose prior in challenging scenarios
characterized by wide baselines and non-object-centric configurations. The
noticeable performance gain in the real-world dataset captured by
consumer-grade omnidirectional cameras verifies the effectiveness of our
general omnidirectional camera model in reducing the distortion of 360-degree
images.

Comments:
- Accepted to ICLR 2025, Project Page:
  http://www.chenyingshu.com/sc-omnigs/

---

## AuraFusion360: Augmented Unseen Region Alignment for Reference-based  360° Unbounded Scene Inpainting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2502.05176v1){: .btn .btn-green } |

**Abstract**: Three-dimensional scene inpainting is crucial for applications from virtual
reality to architectural visualization, yet existing methods struggle with view
consistency and geometric accuracy in 360{\deg} unbounded scenes. We present
AuraFusion360, a novel reference-based method that enables high-quality object
removal and hole filling in 3D scenes represented by Gaussian Splatting. Our
approach introduces (1) depth-aware unseen mask generation for accurate
occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot
method for accurate initial point placement without requiring additional
training, and (3) SDEdit-based detail enhancement for multi-view coherence. We
also introduce 360-USID, the first comprehensive dataset for 360{\deg}
unbounded scene inpainting with ground truth. Extensive experiments demonstrate
that AuraFusion360 significantly outperforms existing methods, achieving
superior perceptual quality while maintaining geometric accuracy across
dramatic viewpoint changes. See our project page for video results and the
dataset at https://kkennethwu.github.io/aurafusion360/.

Comments:
- Project page: https://kkennethwu.github.io/aurafusion360/

---

## GaussRender: Learning 3D Occupancy with Gaussian Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Loick Chambon, Eloi Zablocki, Alexandre Boulch, Mickael Chen, Matthieu Cord | cs.CV | [PDF](http://arxiv.org/pdf/2502.05040v1){: .btn .btn-green } |

**Abstract**: Understanding the 3D geometry and semantics of driving scenes is critical for
developing of safe autonomous vehicles. While 3D occupancy models are typically
trained using voxel-based supervision with standard losses (e.g.,
cross-entropy, Lovasz, dice), these approaches treat voxel predictions
independently, neglecting their spatial relationships. In this paper, we
propose GaussRender, a plug-and-play 3D-to-2D reprojection loss that enhances
voxel-based supervision. Our method projects 3D voxel representations into
arbitrary 2D perspectives and leverages Gaussian splatting as an efficient,
differentiable rendering proxy of voxels, introducing spatial dependencies
across projected elements. This approach improves semantic and geometric
consistency, handles occlusions more efficiently, and requires no architectural
modifications. Extensive experiments on multiple benchmarks
(SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate
consistent performance gains across various 3D occupancy models (TPVFormer,
SurroundOcc, Symphonies), highlighting the robustness and versatility of our
framework. The code is available at https://github.com/valeoai/GaussRender.



---

## High-Speed Dynamic 3D Imaging with Sensor Fusion Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-07 | Zihao Zou, Ziyuan Qu, Xi Peng, Vivek Boominathan, Adithya Pediredla, Praneeth Chakravarthula | cs.CV | [PDF](http://arxiv.org/pdf/2502.04630v1){: .btn .btn-green } |

**Abstract**: Capturing and reconstructing high-speed dynamic 3D scenes has numerous
applications in computer graphics, vision, and interdisciplinary fields such as
robotics, aerodynamics, and evolutionary biology. However, achieving this using
a single imaging modality remains challenging. For instance, traditional RGB
cameras suffer from low frame rates, limited exposure times, and narrow
baselines. To address this, we propose a novel sensor fusion approach using
Gaussian splatting, which combines RGB, depth, and event cameras to capture and
reconstruct deforming scenes at high speeds. The key insight of our method lies
in leveraging the complementary strengths of these imaging modalities: RGB
cameras capture detailed color information, event cameras record rapid scene
changes with microsecond resolution, and depth cameras provide 3D scene
geometry. To unify the underlying scene representation across these modalities,
we represent the scene using deformable 3D Gaussians. To handle rapid scene
movements, we jointly optimize the 3D Gaussian parameters and their temporal
deformation fields by integrating data from all three sensor modalities. This
fusion enables efficient, high-quality imaging of fast and complex scenes, even
under challenging conditions such as low light, narrow baselines, or rapid
motion. Experiments on synthetic and real datasets captured with our prototype
sensor fusion setup demonstrate that our method significantly outperforms
state-of-the-art techniques, achieving noticeable improvements in both
rendering fidelity and structural accuracy.



---

## GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-05 | Mingrui Li, Weijian Chen, Na Cheng, Jingyuan Xu, Dong Li, Hongyu Wang | cs.RO | [PDF](http://arxiv.org/pdf/2502.03228v1){: .btn .btn-green } |

**Abstract**: The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread
attention due to its excellent performance in real-time high-fidelity
rendering. However, in real-world environments with dynamic objects, existing
3DGS-based SLAM systems often face mapping errors and tracking drift issues. To
address these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM
system tailored for dynamic scenes. In terms of tracking, unlike traditional
methods, we directly perform dynamic segmentation on Gaussians and map them
back to the front-end to obtain dynamic point labels through a Gaussian pyramid
network, achieving precise dynamic removal and robust tracking. For mapping, we
impose rendering penalties on dynamically labeled Gaussians, which are updated
through the network, to avoid irreversible erroneous removal caused by simple
pruning. Our results on real-world datasets demonstrate that our method is
competitive in tracking compared to baseline methods, generating fewer
artifacts and higher-quality reconstructions in rendering.



---

## VistaFlow: Photorealistic Volumetric Reconstruction with Dynamic  Resolution Management via Q-Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-05 | Jayram Palamadai, William Yu | cs.CV | [PDF](http://arxiv.org/pdf/2502.05222v1){: .btn .btn-green } |

**Abstract**: We introduce VistaFlow, a scalable three-dimensional imaging technique
capable of reconstructing fully interactive 3D volumetric images from a set of
2D photographs. Our model synthesizes novel viewpoints through a differentiable
rendering system capable of dynamic resolution management on photorealistic 3D
scenes. We achieve this through the introduction of QuiQ, a novel intermediate
video controller trained through Q-learning to maintain a consistently high
framerate by adjusting render resolution with millisecond precision. Notably,
VistaFlow runs natively on integrated CPU graphics, making it viable for mobile
and entry-level devices while still delivering high-performance rendering.
VistaFlow bypasses Neural Radiance Fields (NeRFs), using the PlenOctree data
structure to render complex light interactions such as reflection and
subsurface scattering with minimal hardware requirements. Our model is capable
of outperforming state-of-the-art methods with novel view synthesis at a
resolution of 1080p at over 100 frames per second on consumer hardware. By
tailoring render quality to the capabilities of each device, VistaFlow has the
potential to improve the efficiency and accessibility of photorealistic 3D
scene rendering across a wide spectrum of hardware, from high-end workstations
to inexpensive microcontrollers.



---

## GP-GS: Gaussian Processes for Enhanced Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.02283v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view
synthesis method. However, its reliance on sparse Structure-from-Motion (SfM)
point clouds consistently compromises the scene reconstruction quality. To
address these limitations, this paper proposes a novel 3D reconstruction
framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output
Gaussian Process model is developed to achieve adaptive and uncertainty-guided
densification of sparse SfM point clouds. Specifically, we propose a dynamic
sampling and filtering pipeline that adaptively expands the SfM point clouds by
leveraging GP-based predictions to infer new candidate points from the input 2D
pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the
pruning of high-variance predictions, ensuring geometric consistency and
enabling the generation of dense point clouds. The densified point clouds
provide high-quality initial 3D Gaussians to enhance reconstruction
performance. Extensive experiments conducted on synthetic and real-world
datasets across various scales validate the effectiveness and practicality of
the proposed framework.

Comments:
- 14 pages,11 figures

---

## LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene  Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Yang Zhou, Zongjin He, Qixuan Li, Chao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2502.01949v1){: .btn .btn-green } |

**Abstract**: Recently, the field of text-guided 3D scene generation has garnered
significant attention. High-quality generation that aligns with physical
realism and high controllability is crucial for practical 3D scene
applications. However, existing methods face fundamental limitations: (i)
difficulty capturing complex relationships between multiple objects described
in the text, (ii) inability to generate physically plausible scene layouts, and
(iii) lack of controllability and extensibility in compositional scenes. In
this paper, we introduce LayoutDreamer, a framework that leverages 3D Gaussian
Splatting (3DGS) to facilitate high-quality, physically consistent
compositional scene generation guided by text. Specifically, given a text
prompt, we convert it into a directed scene graph and adaptively adjust the
density and layout of the initial compositional 3D Gaussians. Subsequently,
dynamic camera adjustments are made based on the training focal point to ensure
entity-level generation quality. Finally, by extracting directed dependencies
from the scene graph, we tailor physical and layout energy to ensure both
realism and flexibility. Comprehensive experiments demonstrate that
LayoutDreamer outperforms other compositional scene generation quality and
semantic alignment methods. Specifically, it achieves state-of-the-art (SOTA)
performance in the multiple objects generation metric of T3Bench.



---

## MaintaAvatar: A Maintainable Avatar Based on Neural Radiance Fields by  Continual Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Shengbo Gu, Yu-Kun Qiu, Yu-Ming Tang, Ancong Wu, Wei-Shi Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2502.02372v1){: .btn .btn-green } |

**Abstract**: The generation of a virtual digital avatar is a crucial research topic in the
field of computer vision. Many existing works utilize Neural Radiance Fields
(NeRF) to address this issue and have achieved impressive results. However,
previous works assume the images of the training person are available and fixed
while the appearances and poses of a subject could constantly change and
increase in real-world scenarios. How to update the human avatar but also
maintain the ability to render the old appearance of the person is a practical
challenge. One trivial solution is to combine the existing virtual avatar
models based on NeRF with continual learning methods. However, there are some
critical issues in this approach: learning new appearances and poses can cause
the model to forget past information, which in turn leads to a degradation in
the rendering quality of past appearances, especially color bleeding issues,
and incorrect human body poses. In this work, we propose a maintainable avatar
(MaintaAvatar) based on neural radiance fields by continual learning, which
resolves the issues by utilizing a Global-Local Joint Storage Module and a Pose
Distillation Module. Overall, our model requires only limited data collection
to quickly fine-tune the model while avoiding catastrophic forgetting, thus
achieving a maintainable virtual avatar. The experimental results validate the
effectiveness of our MaintaAvatar model.

Comments:
- AAAI 2025. 9 pages

---

## SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with  Uncertainty Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-04 | Yifu Tao, Maurice Fallon | cs.RO | [PDF](http://arxiv.org/pdf/2502.02657v1){: .btn .btn-green } |

**Abstract**: We present a neural radiance field (NeRF) based large-scale reconstruction
system that fuses lidar and vision data to generate high-quality
reconstructions that are geometrically accurate and capture photorealistic
texture. Our system adopts the state-of-the-art NeRF representation to
additionally incorporate lidar. Adding lidar data adds strong geometric
constraints on the depth and surface normals, which is particularly useful when
modelling uniform texture surfaces which contain ambiguous visual
reconstruction cues. Furthermore, we estimate the epistemic uncertainty of the
reconstruction as the spatial variance of each point location in the radiance
field given the sensor observations from camera and lidar. This enables the
identification of areas that are reliably reconstructed by each sensor
modality, allowing the map to be filtered according to the estimated
uncertainty. Our system can also exploit the trajectory produced by a real-time
pose-graph lidar SLAM system during online mapping to bootstrap a
(post-processed) Structure-from-Motion (SfM) reconstruction procedure reducing
SfM training time by up to 70%. It also helps to properly constrain the overall
metric scale which is essential for the lidar depth loss. The
globally-consistent trajectory can then be divided into submaps using Spectral
Clustering to group sets of co-visible images together. This submapping
approach is more suitable for visual reconstruction than distance-based
partitioning. Each submap is filtered according to point-wise uncertainty
estimates and merged to obtain the final large-scale 3D reconstruction. We
demonstrate the reconstruction system using a multi-camera, lidar sensor suite
in experiments involving both robot-mounted and handheld scanning. Our test
datasets cover a total area of more than 20,000 square metres, including
multiple university buildings and an aerial survey of a multi-storey.

Comments:
- webpage: https://dynamic.robots.ox.ac.uk/projects/silvr/

---

## FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Diego Gomez, Bingchen Gong, Maks Ovsjanikov | cs.CV | [PDF](http://arxiv.org/pdf/2502.01405v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce FourieRF, a novel approach for achieving fast and
high-quality reconstruction in the few-shot setting. Our method effectively
parameterizes features through an explicit curriculum training procedure,
incrementally increasing scene complexity during optimization. Experimental
results show that the prior induced by our approach is both robust and
adaptable across a wide variety of scenes, establishing FourieRF as a strong
and versatile baseline for the few-shot rendering problem. While our approach
significantly reduces artifacts, it may still lead to reconstruction errors in
severely under-constrained scenarios, particularly where view occlusion leaves
parts of the shape uncovered. In the future, our method could be enhanced by
integrating foundation models to complete missing parts using large data-driven
priors.

Comments:
- 8 pages, 3DV 2025 conference

---

## UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Aashish Rai, Dilin Wang, Mihir Jain, Nikolaos Sarafianos, Kefan Chen, Srinath Sridhar, Aayush Prakash | cs.CV | [PDF](http://arxiv.org/pdf/2502.01846v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D
objects and scenes. However, generating 3DGS remains challenging due to their
discrete, unstructured, and permutation-invariant nature. In this work, we
present a simple yet effective method to overcome these challenges. We utilize
spherical mapping to transform 3DGS into a structured 2D representation, termed
UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a
concatenation of Gaussian attributes such as position, scale, color, opacity,
and rotation. We further find that these heterogeneous features can be
compressed into a lower-dimensional (e.g., 3-channel) shared feature space
using a carefully designed multi-branch network. The compressed UVGS can be
treated as typical RGB images. Remarkably, we discover that typical VAEs
trained with latent diffusion models can directly generalize to this new
representation without additional training. Our novel representation makes it
effortless to leverage foundational 2D models, such as diffusion models, to
directly model 3DGS. Additionally, one can simply increase the 2D UV resolution
to accommodate more Gaussians, making UVGS a scalable solution compared to
typical 3D backbones. This approach immediately unlocks various novel
generation applications of 3DGS by inherently utilizing the already developed
superior 2D generation capabilities. In our experiments, we demonstrate various
unconditional, conditional generation, and inpainting applications of 3DGS
based on diffusion models, which were previously non-trivial.

Comments:
- https://aashishrai3799.github.io/uvgs

---

## Scalable 3D Gaussian Splatting-Based RF Signal Spatial Propagation  Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Kang Yang, Gaofeng Dong, Sijie Ji, Wan Du, Mani Srivastava | cs.NI | [PDF](http://arxiv.org/pdf/2502.01826v1){: .btn .btn-green } |

**Abstract**: Effective network planning and sensing in wireless networks require
resource-intensive site surveys for data collection. An alternative is
Radio-Frequency (RF) signal spatial propagation modeling, which computes
received signals given transceiver positions in a scene (e.g.s a conference
room). We identify a fundamental trade-off between scalability and fidelity in
the state-of-the-art method. To address this issue, we explore leveraging 3D
Gaussian Splatting (3DGS), an advanced technique for the image synthesis of 3D
scenes in real-time from arbitrary camera poses. By integrating domain-specific
insights, we design three components for adapting 3DGS to the RF domain,
including Gaussian-based RF scene representation, gradient-guided RF attribute
learning, and RF-customized CUDA for ray tracing. Building on them, we develop
RFSPM, an end-to-end framework for scalable RF signal Spatial Propagation
Modeling. We evaluate RFSPM in four field studies and two applications across
RFID, BLE, LoRa, and 5G, covering diverse frequencies, antennas, signals, and
scenes. The results show that RFSPM matches the fidelity of the
state-of-the-art method while reducing data requirements, training GPU-hours,
and inference latency by up to 9.8\,$\times$, 18.6\,$\times$, and
84.4\,$\times$, respectively.



---

## Radiant Foam: Real-Time Differentiable Ray Tracing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2502.01157v1){: .btn .btn-green } |

**Abstract**: Research on differentiable scene representations is consistently moving
towards more efficient, real-time models. Recently, this has led to the
popularization of splatting methods, which eschew the traditional ray-based
rendering of radiance fields in favor of rasterization. This has yielded a
significant improvement in rendering speeds due to the efficiency of
rasterization algorithms and hardware, but has come at a cost: the
approximations that make rasterization efficient also make implementation of
light transport phenomena like reflection and refraction much more difficult.
We propose a novel scene representation which avoids these approximations, but
keeps the efficiency and reconstruction quality of splatting by leveraging a
decades-old efficient volumetric mesh ray tracing algorithm which has been
largely overlooked in recent computer vision research. The resulting model,
which we name Radiant Foam, achieves rendering speed and quality comparable to
Gaussian Splatting, without the constraints of rasterization. Unlike ray traced
Gaussian models that use hardware ray tracing acceleration, our method requires
no special hardware or APIs beyond the standard features of a programmable GPU.



---

## VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and  Locomotion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-03 | Shaoting Zhu, Linzhan Mou, Derun Li, Baijun Ye, Runhan Huang, Hang Zhao | cs.RO | [PDF](http://arxiv.org/pdf/2502.01536v1){: .btn .btn-green } |

**Abstract**: Recent success in legged robot locomotion is attributed to the integration of
reinforcement learning and physical simulators. However, these policies often
encounter challenges when deployed in real-world environments due to
sim-to-real gaps, as simulators typically fail to replicate visual realism and
complex real-world geometry. Moreover, the lack of realistic visual rendering
limits the ability of these policies to support high-level tasks requiring
RGB-based perception like ego-centric navigation. This paper presents a
Real-to-Sim-to-Real framework that generates photorealistic and physically
interactive "digital twin" simulation environments for visual navigation and
locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based
scene reconstruction from multi-view images and integrates these environments
into simulations that support ego-centric visual perception and mesh-based
physical interactions. To demonstrate its effectiveness, we train a
reinforcement learning policy within the simulator to perform a visual
goal-tracking task. Extensive experiments show that our framework achieves
RGB-only sim-to-real policy transfer. Additionally, our framework facilitates
the rapid adaptation of robot policies with effective exploration capability in
complex new environments, highlighting its potential for applications in
households and factories.

Comments:
- Project Page: https://vr-robo.github.io/

---

## EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head  Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-02-02 | Junuk Cha, Seongro Yoon, Valeriya Strizhkova, Francois Bremond, Seungryul Baek | cs.CV | [PDF](http://arxiv.org/pdf/2502.00654v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting-based talking head synthesis has recently gained
attention for its ability to render high-fidelity images with real-time
inference speed. However, since it is typically trained on only a short video
that lacks the diversity in facial emotions, the resultant talking heads
struggle to represent a wide range of emotions. To address this issue, we
propose a lip-aligned emotional face generator and leverage it to train our
EmoTalkingGaussian model. It is able to manipulate facial emotions conditioned
on continuous emotion values (i.e., valence and arousal); while retaining
synchronization of lip movements with input audio. Additionally, to achieve the
accurate lip synchronization for in-the-wild audio, we introduce a
self-supervised learning method that leverages a text-to-speech network and a
visual-audio synchronization network. We experiment our EmoTalkingGaussian on
publicly available videos and have obtained better results than
state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS),
emotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy),
and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively.

Comments:
- 22 pages
