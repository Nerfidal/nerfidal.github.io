---
layout: default
title: August 2022
parent: Papers
nav_order: 202208
---
<!---metadata--->

## Dual-Space NeRF: Learning Animatable Avatars and Scene Lighting in  Separate Spaces

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-31 | Yihao Zhi, Shenhan Qian, Xinhao Yan, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2208.14851v1){: .btn .btn-green } |

**Abstract**: Modeling the human body in a canonical space is a common practice for
capturing and animation. But when involving the neural radiance field (NeRF),
learning a static NeRF in the canonical space is not enough because the
lighting of the body changes when the person moves even though the scene
lighting is constant. Previous methods alleviate the inconsistency of lighting
by learning a per-frame embedding, but this operation does not generalize to
unseen poses. Given that the lighting condition is static in the world space
while the human body is consistent in the canonical space, we propose a
dual-space NeRF that models the scene lighting and the human body with two MLPs
in two separate spaces. To bridge these two spaces, previous methods mostly
rely on the linear blend skinning (LBS) algorithm. However, the blending
weights for LBS of a dynamic neural field are intractable and thus are usually
memorized with another MLP, which does not generalize to novel poses. Although
it is possible to borrow the blending weights of a parametric mesh such as
SMPL, the interpolation operation introduces more artifacts. In this paper, we
propose to use the barycentric mapping, which can directly generalize to unseen
poses and surprisingly achieves superior results than LBS with neural blending
weights. Quantitative and qualitative results on the Human3.6M and the
ZJU-MoCap datasets show the effectiveness of our method.

Comments:
- Accepted by 3DV 2022

---

## A Portable Multiscopic Camera for Novel View and Time Synthesis in  Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-30 | Tianjia Zhang, Yuen-Fui Lau, Qifeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2208.14433v1){: .btn .btn-green } |

**Abstract**: We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.

Comments:
- To be presented at IROS2022

---

## Volume Rendering Digest (for NeRF)

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-29 | Andrea Tagliasacchi, Ben Mildenhall | cs.CV | [PDF](http://arxiv.org/pdf/2209.02417v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields employ simple volume rendering as a way to overcome
the challenges of differentiating through ray-triangle intersections by
leveraging a probabilistic notion of visibility. This is achieved by assuming
the scene is composed by a cloud of light-emitting particles whose density
changes in space. This technical report summarizes the derivations for
differentiable volume rendering. It is a condensed version of previous reports,
but rewritten in the context of NeRF, and adopting its commonly used notation.

Comments:
- Overleaf: https://www.overleaf.com/read/fkhpkzxhnyws

---

## Training and Tuning Generative Neural Radiance Fields for  Attribute-Conditional 3D-Aware Face Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-26 | Jichao Zhang, Aliaksandr Siarohin, Yahui Liu, Hao Tang, Nicu Sebe, Wei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2208.12550v2){: .btn .btn-green } |

**Abstract**: Generative Neural Radiance Fields (GNeRF) based 3D-aware GANs have
demonstrated remarkable capabilities in generating high-quality images while
maintaining strong 3D consistency. Notably, significant advancements have been
made in the domain of face generation. However, most existing models prioritize
view consistency over disentanglement, resulting in limited semantic/attribute
control during generation. To address this limitation, we propose a conditional
GNeRF model incorporating specific attribute labels as input to enhance the
controllability and disentanglement abilities of 3D-aware generative models.
Our approach builds upon a pre-trained 3D-aware face model, and we introduce a
Training as Init and Optimizing for Tuning (TRIOT) method to train a
conditional normalized flow module to enable the facial attribute editing, then
optimize the latent vector to improve attribute-editing precision further. Our
extensive experiments demonstrate that our model produces high-quality edits
with superior view consistency while preserving non-target regions. Code is
available at https://github.com/zhangqianhui/TT-GNeRF.

Comments:
- 13 pages

---

## E-NeRF: Neural Radiance Fields from a Moving Event Camera

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-24 | Simon Klenk, Lukas Koestler, Davide Scaramuzza, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2208.11300v2){: .btn .btn-green } |

**Abstract**: Estimating neural radiance fields (NeRFs) from "ideal" images has been
extensively studied in the computer vision community. Most approaches assume
optimal illumination and slow camera motion. These assumptions are often
violated in robotic applications, where images may contain motion blur, and the
scene may not have suitable illumination. This can cause significant problems
for downstream tasks such as navigation, inspection, or visualization of the
scene. To alleviate these problems, we present E-NeRF, the first method which
estimates a volumetric scene representation in the form of a NeRF from a
fast-moving event camera. Our method can recover NeRFs during very fast motion
and in high-dynamic-range conditions where frame-based approaches fail. We show
that rendering high-quality frames is possible by only providing an event
stream as input. Furthermore, by combining events and frames, we can estimate
NeRFs of higher quality than state-of-the-art approaches under severe motion
blur. We also show that combining events and frames can overcome failure cases
of NeRF estimation in scenarios where only a few input views are available
without requiring additional regularization.

Comments:
- revised RAL version + added suppl. material

---

## PeRFception: Perception using Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-24 | Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park | cs.CV | [PDF](http://arxiv.org/pdf/2208.11537v1){: .btn .btn-green } |

**Abstract**: The recent progress in implicit 3D representation, i.e., Neural Radiance
Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible
in a differentiable manner. This new representation can effectively convey the
information of hundreds of high-resolution images in one compact format and
allows photorealistic synthesis of novel views. In this work, using the variant
of NeRF called Plenoxels, we create the first large-scale implicit
representation datasets for perception tasks, called the PeRFception, which
consists of two parts that incorporate both object-centric and scene-centric
scans for classification and segmentation. It shows a significant memory
compression rate (96.4\%) from the original dataset, while containing both 2D
and 3D information in a unified form. We construct the classification and
segmentation models that directly take as input this implicit format and also
propose a novel augmentation technique to avoid overfitting on backgrounds of
images. The code and data are publicly available in
https://postech-cvlab.github.io/PeRFception .

Comments:
- Project Page: https://postech-cvlab.github.io/PeRFception/

---

## Neural Capture of Animatable 3D Human from Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-18 | Gusi Te, Xiu Li, Xiao Li, Jinglu Wang, Wei Hu, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2208.08728v1){: .btn .btn-green } |

**Abstract**: We present a novel paradigm of building an animatable 3D human representation
from a monocular video input, such that it can be rendered in any unseen poses
and views. Our method is based on a dynamic Neural Radiance Field (NeRF) rigged
by a mesh-based parametric 3D human model serving as a geometry proxy. Previous
methods usually rely on multi-view videos or accurate 3D geometry information
as additional inputs; besides, most methods suffer from degraded quality when
generalized to unseen poses. We identify that the key to generalization is a
good input embedding for querying dynamic NeRF: A good input embedding should
define an injective mapping in the full volumetric space, guided by surface
mesh deformation under pose variation. Based on this observation, we propose to
embed the input query with its relationship to local surface regions spanned by
a set of geodesic nearest neighbors on mesh vertices. By including both
position and relative distance information, our embedding defines a
distance-preserved deformation mapping and generalizes well to unseen poses. To
reduce the dependency on additional inputs, we first initialize per-frame 3D
meshes using off-the-shelf tools and then propose a pipeline to jointly
optimize NeRF and refine the initial mesh. Extensive experiments show our
method can synthesize plausible human rendering results under unseen poses and
views.

Comments:
- ECCV 2022

---

## Casual Indoor HDR Radiance Capture from Omnidirectional Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-16 | Pulkit Gera, Mohammad Reza Karimi Dastjerdi, Charles Renaud, P. J. Narayanan, Jean-Fran√ßois Lalonde | cs.CV | [PDF](http://arxiv.org/pdf/2208.07903v2){: .btn .btn-green } |

**Abstract**: We present PanoHDR-NeRF, a neural representation of the full HDR radiance
field of an indoor scene, and a pipeline to capture it casually, without
elaborate setups or complex capture protocols. First, a user captures a low
dynamic range (LDR) omnidirectional video of the scene by freely waving an
off-the-shelf camera around the scene. Then, an LDR2HDR network uplifts the
captured LDR frames to HDR, which are used to train a tailored NeRF++ model.
The resulting PanoHDR-NeRF can render full HDR images from any location of the
scene. Through experiments on a novel test dataset of real scenes with the
ground truth HDR radiance captured at locations not seen during training, we
show that PanoHDR-NeRF predicts plausible HDR radiance from any scene point. We
also show that the predicted radiance can synthesize correct lighting effects,
enabling the augmentation of indoor scenes with synthetic objects that are lit
correctly. Datasets and code are available at
https://lvsn.github.io/PanoHDR-NeRF/.

Comments:
- BMVC 2022

---

## DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-15 | Bing Wang, Lu Chen, Bo Yang | cs.CV | [PDF](http://arxiv.org/pdf/2208.07227v2){: .btn .btn-green } |

**Abstract**: In this paper, we study the problem of 3D scene geometry decomposition and
manipulation from 2D views. By leveraging the recent implicit neural
representation techniques, particularly the appealing neural radiance fields,
we introduce an object field component to learn unique codes for all individual
objects in 3D space only from 2D supervision. The key to this component is a
series of carefully designed loss functions to enable every 3D point,
especially in non-occupied space, to be effectively optimized even without 3D
labels. In addition, we introduce an inverse query algorithm to freely
manipulate any specified 3D object shape in the learned scene representation.
Notably, our manipulation algorithm can explicitly tackle key issues such as
object collisions and visual occlusions. Our method, called DM-NeRF, is among
the first to simultaneously reconstruct, decompose, manipulate and render
complex 3D scenes in a single pipeline. Extensive experiments on three datasets
clearly show that our method can accurately decompose all 3D objects from 2D
views, allowing any interested object to be freely manipulated in 3D space such
as translation, rotation, size adjustment, and deformation.

Comments:
- ICLR 2023. Our data and code are available at:
  https://github.com/vLAR-group/DM-NeRF

---

## UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance  Fields for 3D Scene

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-15 | Yaosen Chen, Qi Yuan, Zhiqiang Li, Yuegen Liu, Wei Wang, Chaoping Xie, Xuming Wen, Qien Yu | cs.CV | [PDF](http://arxiv.org/pdf/2208.07059v2){: .btn .btn-green } |

**Abstract**: 3D scenes photorealistic stylization aims to generate photorealistic images
from arbitrary novel views according to a given style image while ensuring
consistency when rendering from different viewpoints. Some existing stylization
methods with neural radiance fields can effectively predict stylized scenes by
combining the features of the style image with multi-view images to train 3D
scenes. However, these methods generate novel view images that contain
objectionable artifacts. Besides, they cannot achieve universal photorealistic
stylization for a 3D scene. Therefore, a styling image must retrain a 3D scene
representation network based on a neural radiation field. We propose a novel 3D
scene photorealistic style transfer framework to address these issues. It can
realize photorealistic 3D scene style transfer with a 2D style image. We first
pre-trained a 2D photorealistic style transfer network, which can meet the
photorealistic style transfer between any given content image and style image.
Then, we use voxel features to optimize a 3D scene and get the geometric
representation of the scene. Finally, we jointly optimize a hyper network to
realize the scene photorealistic style transfer of arbitrary style images. In
the transfer stage, we use a pre-trained 2D photorealistic network to constrain
the photorealistic style of different views and different style images in the
3D scene. The experimental results show that our method not only realizes the
3D photorealistic style transfer of arbitrary style images but also outperforms
the existing methods in terms of visual quality and consistency. Project
page:https://semchan.github.io/UPST_NeRF.

Comments:
- arXiv admin note: text overlap with arXiv:2205.12183 by other authors

---

## OmniVoxel: A Fast and Precise Reconstruction Method of Omnidirectional  Neural Radiance Field



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-12 | Qiaoge Li, Itsuki Ueda, Chun Xie, Hidehiko Shishido, Itaru Kitahara | cs.CV | [PDF](http://arxiv.org/pdf/2208.06335v1){: .btn .btn-green } |

**Abstract**: This paper proposes a method to reconstruct the neural radiance field with
equirectangular omnidirectional images. Implicit neural scene representation
with a radiance field can reconstruct the 3D shape of a scene continuously
within a limited spatial area. However, training a fully implicit
representation on commercial PC hardware requires a lot of time and computing
resources (15 $\sim$ 20 hours per scene). Therefore, we propose a method to
accelerate this process significantly (20 $\sim$ 40 minutes per scene). Instead
of using a fully implicit representation of rays for radiance field
reconstruction, we adopt feature voxels that contain density and color features
in tensors. Considering omnidirectional equirectangular input and the camera
layout, we use spherical voxelization for representation instead of cubic
representation. Our voxelization method could balance the reconstruction
quality of the inner scene and outer scene. In addition, we adopt the
axis-aligned positional encoding method on the color features to increase the
total image quality. Our method achieves satisfying empirical performance on
synthetic datasets with random camera poses. Moreover, we test our method with
real scenes which contain complex geometries and also achieve state-of-the-art
performance. Our code and complete dataset will be released at the same time as
the paper publication.

Comments:
- will be appeared in GCCE 2022

---

## RelPose: Predicting Probabilistic Relative Rotation for Single Objects  in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-11 | Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani | cs.CV | [PDF](http://arxiv.org/pdf/2208.05963v2){: .btn .btn-green } |

**Abstract**: We describe a data-driven method for inferring the camera viewpoints given
multiple images of an arbitrary object. This task is a core component of
classic geometric pipelines such as SfM and SLAM, and also serves as a vital
pre-processing requirement for contemporary neural approaches (e.g. NeRF) to
object reconstruction and view synthesis. In contrast to existing
correspondence-driven methods that do not perform well given sparse views, we
propose a top-down prediction based approach for estimating camera viewpoints.
Our key technical insight is the use of an energy-based formulation for
representing distributions over relative camera rotations, thus allowing us to
explicitly represent multiple camera modes arising from object symmetries or
views. Leveraging these relative predictions, we jointly estimate a consistent
set of camera rotations from multiple images. We show that our approach
outperforms state-of-the-art SfM and SLAM methods given sparse images on both
seen and unseen categories. Further, our probabilistic approach significantly
outperforms directly regressing relative poses, suggesting that modeling
multimodality is important for coherent joint reconstruction. We demonstrate
that our system can be a stepping stone toward in-the-wild reconstruction from
multi-view datasets. The project page with code and videos can be found at
https://jasonyzhang.com/relpose.

Comments:
- In ECCV 2022. V2: updated references

---

## FDNeRF: Few-shot Dynamic Neural Radiance Fields for Face Reconstruction  and Expression Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-11 | Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao | cs.CV | [PDF](http://arxiv.org/pdf/2208.05751v2){: .btn .btn-green } |

**Abstract**: We propose a Few-shot Dynamic Neural Radiance Field (FDNeRF), the first
NeRF-based method capable of reconstruction and expression editing of 3D faces
based on a small number of dynamic images. Unlike existing dynamic NeRFs that
require dense images as input and can only be modeled for a single identity,
our method enables face reconstruction across different persons with few-shot
inputs. Compared to state-of-the-art few-shot NeRFs designed for modeling
static scenes, the proposed FDNeRF accepts view-inconsistent dynamic inputs and
supports arbitrary facial expression editing, i.e., producing faces with novel
expressions beyond the input ones. To handle the inconsistencies between
dynamic inputs, we introduce a well-designed conditional feature warping (CFW)
module to perform expression conditioned warping in 2D feature space, which is
also identity adaptive and 3D constrained. As a result, features of different
expressions are transformed into the target ones. We then construct a radiance
field based on these view-consistent features and use volumetric rendering to
synthesize novel views of the modeled faces. Extensive experiments with
quantitative and qualitative evaluation demonstrate that our method outperforms
existing dynamic and few-shot NeRFs on both 3D face reconstruction and
expression editing tasks. Code is available at
https://github.com/FDNeRF/FDNeRF.

Comments:
- Accepted at SIGGRAPH Asia 2022. Project page:
  https://fdnerf.github.io

---

## Cascaded and Generalizable Neural Radiance Fields for Fast View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-09 | Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Jiri Matas, Janne Heikkila | cs.CV | [PDF](http://arxiv.org/pdf/2208.04717v2){: .btn .btn-green } |

**Abstract**: We present CG-NeRF, a cascade and generalizable neural radiance fields method
for view synthesis. Recent generalizing view synthesis methods can render
high-quality novel views using a set of nearby input views. However, the
rendering speed is still slow due to the nature of uniformly-point sampling of
neural radiance fields. Existing scene-specific methods can train and render
novel views efficiently but can not generalize to unseen data. Our approach
addresses the problems of fast and generalizing view synthesis by proposing two
novel modules: a coarse radiance fields predictor and a convolutional-based
neural renderer. This architecture infers consistent scene geometry based on
the implicit neural fields and renders new views efficiently using a single
GPU. We first train CG-NeRF on multiple 3D scenes of the DTU dataset, and the
network can produce high-quality and accurate novel views on unseen real and
synthetic data using only photometric losses. Moreover, our method can leverage
a denser set of reference images of a single scene to produce accurate novel
views without relying on additional explicit representations and still
maintains the high-speed rendering of the pre-trained model. Experimental
results show that CG-NeRF outperforms state-of-the-art generalizable neural
rendering methods on various synthetic and real datasets.

Comments:
- Accepted at IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)

---

## 360Roam: Real-Time Indoor Roaming Using Geometry-Aware 360$^\circ$  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-04 | Huajian Huang, Yingshu Chen, Tianjia Zhang, Sai-Kit Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2208.02705v2){: .btn .btn-green } |

**Abstract**: Virtual tour among sparse 360$^\circ$ images is widely used while hindering
smooth and immersive roaming experiences. The emergence of Neural Radiance
Field (NeRF) has showcased significant progress in synthesizing novel views,
unlocking the potential for immersive scene exploration. Nevertheless, previous
NeRF works primarily focused on object-centric scenarios, resulting in
noticeable performance degradation when applied to outward-facing and
large-scale scenes due to limitations in scene parameterization. To achieve
seamless and real-time indoor roaming, we propose a novel approach using
geometry-aware radiance fields with adaptively assigned local radiance fields.
Initially, we employ multiple 360$^\circ$ images of an indoor scene to
progressively reconstruct explicit geometry in the form of a probabilistic
occupancy map, derived from a global omnidirectional radiance field.
Subsequently, we assign local radiance fields through an adaptive
divide-and-conquer strategy based on the recovered geometry. By incorporating
geometry-aware sampling and decomposition of the global radiance field, our
system effectively utilizes positional encoding and compact neural networks to
enhance rendering quality and speed. Additionally, the extracted floorplan of
the scene aids in providing visual guidance, contributing to a realistic
roaming experience. To demonstrate the effectiveness of our system, we curated
a diverse dataset of 360$^\circ$ images encompassing various real-life scenes,
on which we conducted extensive experiments. Quantitative and qualitative
comparisons against baseline approaches illustrated the superior performance of
our system in large-scale indoor scene roaming.

---

## T4DT: Tensorizing Time for Learning Temporal 3D Visual Data

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-02 | Mikhail Usvyatsov, Rafael Ballester-Rippoll, Lina Bashaeva, Konrad Schindler, Gonzalo Ferrer, Ivan Oseledets | cs.CV | [PDF](http://arxiv.org/pdf/2208.01421v2){: .btn .btn-green } |

**Abstract**: Unlike 2D raster images, there is no single dominant representation for 3D
visual data processing. Different formats like point clouds, meshes, or
implicit functions each have their strengths and weaknesses. Still, grid
representations such as signed distance functions have attractive properties
also in 3D. In particular, they offer constant-time random access and are
eminently suitable for modern machine learning. Unfortunately, the storage size
of a grid grows exponentially with its dimension. Hence they often exceed
memory limits even at moderate resolution. This work proposes using low-rank
tensor formats, including the Tucker, tensor train, and quantics tensor train
decompositions, to compress time-varying 3D data. Our method iteratively
computes, voxelizes, and compresses each frame's truncated signed distance
function and applies tensor rank truncation to condense all frames into a
single, compressed tensor that represents the entire 4D scene. We show that
low-rank tensor compression is extremely compact to store and query
time-varying signed distance functions. It significantly reduces the memory
footprint of 4D scenes while remarkably preserving their geometric quality.
Unlike existing, iterative learning-based approaches like DeepSDF and NeRF, our
method uses a closed-form algorithm with theoretical guarantees.

---

## DoF-NeRF: Depth-of-Field Meets Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-01 | Zijin Wu, Xingyi Li, Juewen Peng, Hao Lu, Zhiguo Cao, Weicai Zhong | cs.CV | [PDF](http://arxiv.org/pdf/2208.00945v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) and its variants have exhibited great success on
representing 3D scenes and synthesizing photo-realistic novel views. However,
they are generally based on the pinhole camera model and assume all-in-focus
inputs. This limits their applicability as images captured from the real world
often have finite depth-of-field (DoF). To mitigate this issue, we introduce
DoF-NeRF, a novel neural rendering approach that can deal with shallow DoF
inputs and can simulate DoF effect. In particular, it extends NeRF to simulate
the aperture of lens following the principles of geometric optics. Such a
physical guarantee allows DoF-NeRF to operate views with different focus
configurations. Benefiting from explicit aperture modeling, DoF-NeRF also
enables direct manipulation of DoF effect by adjusting virtual aperture and
focus parameters. It is plug-and-play and can be inserted into NeRF-based
frameworks. Experiments on synthetic and real-world datasets show that,
DoF-NeRF not only performs comparably with NeRF in the all-in-focus setting,
but also can synthesize all-in-focus novel views conditioned on shallow DoF
inputs. An interesting application of DoF-NeRF to DoF rendering is also
demonstrated. The source code will be made available at
https://github.com/zijinwuzijin/DoF-NeRF.

Comments:
- Accepted by ACMMM 2022