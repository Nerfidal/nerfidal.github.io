---
layout: default
title: February 2024
parent: Papers
nav_order: 202402
---

<!---metadata--->


## 3D Gaussian Model for Animation and Texturing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-29 | Xiangzhi Eric Wang, Zackary P. T. Sin | cs.GR | [PDF](http://arxiv.org/pdf/2402.19441v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has made a marked impact on neural rendering by
achieving impressive fidelity and performance. Despite this achievement,
however, it is not readily applicable to developing interactive applications.
Real-time applications like XR apps and games require functions such as
animation, UV-mapping, and model editing simultaneously manipulated through the
usage of a 3D model. We propose a modeling that is analogous to typical 3D
models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable
proxy for novel animation and texture transfer. By binding the 3D Gaussians in
texture space and re-projecting them back to world space through implicit shell
mapping, we show how our 3D modeling can serve as a valid rendering methodology
for interactive applications. It is further noted that recently, 3D mesh
reconstruction works have been able to produce high-quality mesh for rendering.
Our work, on the other hand, only requires an approximated geometry for
rendering an object in high fidelity. Applicationwise, we will show that our
proxy-based 3DGM is capable of driving novel animation without animated
training data and texture transferring via UV mapping of the 3D Gaussians. We
believe the result indicates the potential of our work for enabling interactive
applications for 3D Gaussian Splatting.



---

## NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human  Pose Estimation in Top-View Fisheye Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-28 | Jingrui Yu, Dipankar Nandi, Roman Seidel, Gangolf Hirtz | cs.CV | [PDF](http://arxiv.org/pdf/2402.18196v1){: .btn .btn-green } |

**Abstract**: Human pose estimation (HPE) in the top-view using fisheye cameras presents a
promising and innovative application domain. However, the availability of
datasets capturing this viewpoint is extremely limited, especially those with
high-quality 2D and 3D keypoint annotations. Addressing this gap, we leverage
the capabilities of Neural Radiance Fields (NeRF) technique to establish a
comprehensive pipeline for generating human pose datasets from existing 2D and
3D datasets, specifically tailored for the top-view fisheye perspective.
Through this pipeline, we create a novel dataset NToP570K (NeRF-powered
Top-view human Pose dataset for fisheye cameras with over 570 thousand images),
and conduct an extensive evaluation of its efficacy in enhancing neural
networks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-B
model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE
after finetuning on our training set. A similarly finetuned HybrIK-Transformer
model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.



---

## Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-27 | Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta | cs.RO | [PDF](http://arxiv.org/pdf/2402.17768v1){: .btn .btn-green } |

**Abstract**: A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
were not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to create these samples with diffusion models. This leads to
robust performance from few demonstrations. In experiments conducted for
non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a
success rate of 80% with as few as 8 expert demonstrations, where naive
behavior cloning reaches only 20%. DMD also outperform competing NeRF-based
augmentation schemes by 50%.

Comments:
- for project website with video, see
  https://sites.google.com/view/diffusion-meets-dagger

---

## Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-27 | Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang | cs.CV | [PDF](http://arxiv.org/pdf/2402.17364v1){: .btn .btn-green } |

**Abstract**: Recent works in implicit representations, such as Neural Radiance Fields
(NeRF), have advanced the generation of realistic and animatable head avatars
from video sequences. These implicit methods are still confronted by visual
artifacts and jitters, since the lack of explicit geometric constraints poses a
fundamental challenge in accurately modeling complex facial deformations. In
this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid
representation that encodes explicit dynamic meshes by neural networks to
ensure geometric consistency across various motions and viewpoints. DynTet is
parameterized by the coordinate-based networks which learn signed distance,
deformation, and material texture, anchoring the training data into a
predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently
decodes textured meshes with a consistent topology, enabling fast rendering
through a differentiable rasterizer and supervision via a pixel loss. To
enhance training efficiency, we incorporate classical 3D Morphable Models to
facilitate geometry learning and define a canonical space for simplifying
texture learning. These advantages are readily achievable owing to the
effective geometric representation employed in DynTet. Compared with prior
works, DynTet demonstrates significant improvements in fidelity, lip
synchronization, and real-time performance according to various metrics. Beyond
producing stable and visually appealing synthesis videos, our method also
outputs the dynamic meshes which is promising to enable many emerging
applications.

Comments:
- CVPR 2024

---

## DivAvatar: Diverse 3D Avatar Generation with a Single Prompt

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-27 | Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao | cs.CV | [PDF](http://arxiv.org/pdf/2402.17292v1){: .btn .btn-green } |

**Abstract**: Text-to-Avatar generation has recently made significant strides due to
advancements in diffusion models. However, most existing work remains
constrained by limited diversity, producing avatars with subtle differences in
appearance for a given text prompt. We design DivAvatar, a novel framework that
generates diverse avatars, empowering 3D creatives with a multitude of distinct
and richly varied 3D avatars from a single text prompt. Different from most
existing work that exploits scene-specific 3D representations such as NeRF,
DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse
avatar generation from simply noise sampling in inference time. DivAvatar has
two key designs that help achieve generation diversity and visual quality. The
first is a noise sampling technique during training phase which is critical in
generating diverse appearances. The second is a semantic-aware zoom mechanism
and a novel depth loss, the former producing appearances of high textual
fidelity by separate fine-tuning of specific body parts and the latter
improving geometry quality greatly by smoothing the generated mesh in the
features space. Extensive experiments show that DivAvatar is highly versatile
in generating avatars of diverse appearances.



---

## CharNeRF: 3D Character Generation from Concept Art

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-27 | Eddy Chu, Yiyang Chen, Chedy Raissi, Anand Bhojan | cs.CV | [PDF](http://arxiv.org/pdf/2402.17115v1){: .btn .btn-green } |

**Abstract**: 3D modeling holds significant importance in the realms of AR/VR and gaming,
allowing for both artistic creativity and practical applications. However, the
process is often time-consuming and demands a high level of skill. In this
paper, we present a novel approach to create volumetric representations of 3D
characters from consistent turnaround concept art, which serves as the standard
input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been
a game-changer in image-based 3D reconstruction, to the best of our knowledge,
there is no known research that optimizes the pipeline for concept art. To
harness the potential of concept art, with its defined body poses and specific
view angles, we propose encoding it as priors for our model. We train the
network to make use of these priors for various 3D points through a learnable
view-direction-attended multi-head self-attention layer. Additionally, we
demonstrate that a combination of ray sampling and surface sampling enhances
the inference capabilities of our network. Our model is able to generate
high-quality 360-degree views of characters. Subsequently, we provide a simple
guideline to better leverage our model to extract the 3D mesh. It is important
to note that our model's inferencing capabilities are influenced by the
training data's characteristics, primarily focusing on characters with a single
head, two arms, and two legs. Nevertheless, our methodology remains versatile
and adaptable to concept art from diverse subject matters, without imposing any
specific assumptions on the data.



---

## VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-27 | Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang | cs.CV | [PDF](http://arxiv.org/pdf/2402.17427v1){: .btn .btn-green } |

**Abstract**: Existing NeRF-based methods for large scene reconstruction often have
limitations in visual quality and rendering speed. While the recent 3D Gaussian
Splatting works well on small-scale and object-centric scenes, scaling it up to
large scenes poses challenges due to limited video memory, long optimization
time, and noticeable appearance variations. To address these challenges, we
present VastGaussian, the first method for high-quality reconstruction and
real-time rendering on large scenes based on 3D Gaussian Splatting. We propose
a progressive partitioning strategy to divide a large scene into multiple
cells, where the training cameras and point cloud are properly distributed with
an airspace-aware visibility criterion. These cells are merged into a complete
scene after parallel optimization. We also introduce decoupled appearance
modeling into the optimization process to reduce appearance variations in the
rendered images. Our approach outperforms existing NeRF-based methods and
achieves state-of-the-art results on multiple large scene datasets, enabling
fast optimization and high-fidelity real-time rendering.

Comments:
- Accepted to CVPR 2024. Project website:
  https://vastgaussian.github.io

---

## Disentangled 3D Scene Generation with Layout Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-26 | Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A. Efros, Aleksander Holynski | cs.CV | [PDF](http://arxiv.org/pdf/2402.16936v1){: .btn .btn-green } |

**Abstract**: We introduce a method to generate 3D scenes that are disentangled into their
component objects. This disentanglement is unsupervised, relying only on the
knowledge of a large pretrained text-to-image model. Our key insight is that
objects can be discovered by finding parts of a 3D scene that, when rearranged
spatially, still produce valid configurations of the same scene. Concretely,
our method jointly optimizes multiple NeRFs from scratch - each representing
its own object - along with a set of layouts that composite these objects into
scenes. We then encourage these composited scenes to be in-distribution
according to the image generator. We show that despite its simplicity, our
approach successfully generates 3D scenes decomposed into individual objects,
enabling new capabilities in text-to-3D content creation. For results and an
interactive demo, see our project page at https://dave.ml/layoutlearning/



---

## CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-26 | Hanxin Zhu, Tianyu He, Zhibo Chen | cs.CV | [PDF](http://arxiv.org/pdf/2402.16407v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has shown impressive results in novel view
synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR),
thanks to its ability to represent scenes continuously. However, when just a
few input view images are available, NeRF tends to overfit the given views and
thus make the estimated depths of pixels share almost the same value. Unlike
previous methods that conduct regularization by introducing complex priors or
additional supervisions, we propose a simple yet effective method that
explicitly builds depth-aware consistency across input views to tackle this
challenge. Our key insight is that by forcing the same spatial points to be
sampled repeatedly in different input views, we are able to strengthen the
interactions between views and therefore alleviate the overfitting problem. To
achieve this, we build the neural networks on layered representations
(\textit{i.e.}, multiplane images), and the sampling point can thus be
resampled on multiple discrete planes. Furthermore, to regularize the unseen
target views, we constrain the rendered colors and depths from different input
views to be the same. Although simple, extensive experiments demonstrate that
our proposed method can achieve better synthesis quality over state-of-the-art
methods.

Comments:
- Accepted by IEEE Conference on Virtual Reality and 3D User Interfaces
  (IEEE VR 2024)

---

## Neural Radiance Fields in Medical Imaging: Challenges and Next Steps

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-26 | Xin Wang, Shu Hu, Heng Fan, Hongtu Zhu, Xin Li | eess.IV | [PDF](http://arxiv.org/pdf/2402.17797v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,
offer great potential to revolutionize medical imaging by synthesizing
three-dimensional representations from the projected two-dimensional image
data. However, they face unique challenges when applied to medical
applications. This paper presents a comprehensive examination of applications
of NeRFs in medical imaging, highlighting four imminent challenges, including
fundamental imaging principles, inner structure requirement, object boundary
definition, and color density significance. We discuss current methods on
different organs and discuss related limitations. We also review several
datasets and evaluation metrics and propose several promising directions for
future research.



---

## DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene  Understanding and Real-to-Sim Transfer

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-26 | Yizhe Wu, Haitz Sáez de Ocáriz Borde, Jack Collins, Oiwi Parker Jones, Ingmar Posner | cs.RO | [PDF](http://arxiv.org/pdf/2402.16308v1){: .btn .btn-green } |

**Abstract**: 3D scene understanding for robotic applications exhibits a unique set of
requirements including real-time inference, object-centric latent
representation learning, accurate 6D pose estimation and 3D reconstruction of
objects. Current methods for scene understanding typically rely on a
combination of trained models paired with either an explicit or learnt
volumetric representation, all of which have their own drawbacks and
limitations. We introduce DreamUp3D, a novel Object-Centric Generative Model
(OCGM) designed explicitly to perform inference on a 3D scene informed only by
a single RGB-D image. DreamUp3D is a self-supervised model, trained end-to-end,
and is capable of segmenting objects, providing 3D object reconstructions,
generating object-centric latent representations and accurate per-object 6D
pose estimates. We compare DreamUp3D to baselines including NeRFs, pre-trained
CLIP-features, ObSurf, and ObPose, in a range of tasks including 3D scene
reconstruction, object matching and object pose estimation. Our experiments
show that our model outperforms all baselines by a significant margin in
real-world scenarios displaying its applicability for 3D scene understanding
tasks while meeting the strict demands exhibited in robotics applications.



---

## SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-26 | Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao | cs.CV | [PDF](http://arxiv.org/pdf/2402.16366v1){: .btn .btn-green } |

**Abstract**: Representing the Neural Radiance Field (NeRF) with the explicit voxel grid
(EVG) is a promising direction for improving NeRFs. However, the EVG
representation is not efficient for storage and transmission because of the
terrific memory cost. Current methods for compressing EVG mainly inherit the
methods designed for neural network compression, such as pruning and
quantization, which do not take full advantage of the spatial correlation of
voxels. Inspired by prosperous digital image compression techniques, this paper
proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG
compression. The proposed framework can remove spatial redundancy efficiently
for better compression performance.Moreover, we model the bitrate and design a
novel form of the loss function, where we can jointly optimize compression
ratio and distortion to achieve higher coding efficiency. Extensive experiments
demonstrate that our method can achieve 32% bit saving compared to the
state-of-the-art method VQRF on multiple representative test datasets, with
comparable training time.



---

## Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-24 | Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin | cs.CV | [PDF](http://arxiv.org/pdf/2402.15870v1){: .btn .btn-green } |

**Abstract**: The recent advancements in 3D Gaussian splatting (3D-GS) have not only
facilitated real-time rendering through modern GPU rasterization pipelines but
have also attained state-of-the-art rendering quality. Nevertheless, despite
its exceptional rendering quality and performance on standard datasets, 3D-GS
frequently encounters difficulties in accurately modeling specular and
anisotropic components. This issue stems from the limited ability of spherical
harmonics (SH) to represent high-frequency information. To overcome this
challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic
spherical Gaussian (ASG) appearance field instead of SH for modeling the
view-dependent appearance of each 3D Gaussian. Additionally, we have developed
a coarse-to-fine training strategy to improve learning efficiency and eliminate
floaters caused by overfitting in real-world scenes. Our experimental results
demonstrate that our method surpasses existing approaches in terms of rendering
quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to
model scenes with specular and anisotropic components without increasing the
number of 3D Gaussians. This improvement extends the applicability of 3D GS to
handle intricate scenarios with specular and anisotropic surfaces.



---

## FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-22 | Yan Xing, Pan Wang, Ligang Liu, Daolun Li, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2402.14586v2){: .btn .btn-green } |

**Abstract**: We present a novel framework, called FrameNeRF, designed to apply
off-the-shelf fast high-fidelity NeRF models with fast training speed and high
rendering quality for few-shot novel view synthesis tasks. The training
stability of fast high-fidelity models is typically constrained to dense views,
making them unsuitable for few-shot novel view synthesis tasks. To address this
limitation, we utilize a regularization model as a data generator to produce
dense views from sparse inputs, facilitating subsequent training of fast
high-fidelity models. Since these dense views are pseudo ground truth generated
by the regularization model, original sparse images are then used to fine-tune
the fast high-fidelity model. This process helps the model learn realistic
details and correct artifacts introduced in earlier stages. By leveraging an
off-the-shelf regularization model and a fast high-fidelity model, our approach
achieves state-of-the-art performance across various benchmark datasets.



---

## Consolidating Attention Features for Multi-view Image Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-22 | Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre | cs.CV | [PDF](http://arxiv.org/pdf/2402.14792v1){: .btn .btn-green } |

**Abstract**: Large-scale text-to-image models enable a wide range of image editing
techniques, using text prompts or even spatial controls. However, applying
these editing methods to multi-view images depicting a single scene leads to
3D-inconsistent results. In this work, we focus on spatial control-based
geometric manipulations and introduce a method to consolidate the editing
process across various views. We build on two insights: (1) maintaining
consistent features throughout the generative process helps attain consistency
in multi-view editing, and (2) the queries in self-attention layers
significantly influence the image structure. Hence, we propose to improve the
geometric consistency of the edited images by enforcing the consistency of the
queries. To do so, we introduce QNeRF, a neural radiance field trained on the
internal query features of the edited images. Once trained, QNeRF can render
3D-consistent queries, which are then softly injected back into the
self-attention layers during generation, greatly improving multi-view
consistency. We refine the process through a progressive, iterative method that
better consolidates queries across the diffusion timesteps. We compare our
method to a range of existing techniques and demonstrate that it can achieve
better multi-view consistency and higher fidelity to the input scene. These
advantages allow us to train NeRFs with fewer visual artifacts, that are better
aligned with the target geometry.

Comments:
- Project Page at
  https://qnerf-consolidation.github.io/qnerf-consolidation/

---

## Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-22 | Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2402.14196v1){: .btn .btn-green } |

**Abstract**: Despite the remarkable achievements of neural radiance fields (NeRF) in
representing 3D scenes and generating novel view images, the aliasing issue,
rendering "jaggies" or "blurry" images at varying camera distances, remains
unresolved in most existing approaches. The recently proposed mip-NeRF has
addressed this challenge by rendering conical frustums instead of rays.
However, it relies on MLP architecture to represent the radiance fields,
missing out on the fast training speed offered by the latest grid-based
methods. In this work, we present mip-Grid, a novel approach that integrates
anti-aliasing techniques into grid-based representations for radiance fields,
mitigating the aliasing artifacts while enjoying fast training time. The
proposed method generates multi-scale grids by applying simple convolution
operations over a shared grid representation and uses the scale-aware
coordinate to retrieve features at different scales from the generated
multi-scale grids. To test the effectiveness, we integrated the proposed method
into the two recent representative grid-based methods, TensoRF and K-Planes.
Experimental results demonstrate that mip-Grid greatly improves the rendering
performance of both methods and even outperforms mip-NeRF on multi-scale
datasets while achieving significantly faster training time. For code and demo
videos, please see https://stnamjef.github.io/mipgrid.github.io/.

Comments:
- Accepted to NeurIPS 2023

---

## GaussianPro: 3D Gaussian Splatting with Progressive Propagation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-22 | Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping Wang, Xuejin Chen | cs.CV | [PDF](http://arxiv.org/pdf/2402.14650v1){: .btn .btn-green } |

**Abstract**: The advent of 3D Gaussian Splatting (3DGS) has recently brought about a
revolution in the field of neural rendering, facilitating high-quality
renderings at real-time speed. However, 3DGS heavily depends on the initialized
point cloud produced by Structure-from-Motion (SfM) techniques. When tackling
with large-scale scenes that unavoidably contain texture-less surfaces, the SfM
techniques always fail to produce enough points in these surfaces and cannot
provide good initialization for 3DGS. As a result, 3DGS suffers from difficult
optimization and low-quality renderings. In this paper, inspired by classical
multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that
applies a progressive propagation strategy to guide the densification of the 3D
Gaussians. Compared to the simple split and clone strategies used in 3DGS, our
method leverages the priors of the existing reconstructed geometries of the
scene and patch matching techniques to produce new Gaussians with accurate
positions and orientations. Experiments on both large-scale and small-scale
scenes validate the effectiveness of our method, where our method significantly
surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in
terms of PSNR.

Comments:
- See the project page for code, data:
  https://kcheng1021.github.io/gaussianpro.github.io

---

## TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via  Direct Taylor-based Grid Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-22 | Renyi Mao, Qingshan Xu, Peng Zheng, Ye Wang, Tieru Wu, Rui Ma | cs.CV | [PDF](http://arxiv.org/pdf/2402.14415v1){: .btn .btn-green } |

**Abstract**: Coordinate-based neural implicit representation or implicit fields have been
widely studied for 3D geometry representation or novel view synthesis.
Recently, a series of efforts have been devoted to accelerating the speed and
improving the quality of the coordinate-based implicit field learning. Instead
of learning heavy MLPs to predict the neural implicit values for the query
coordinates, neural voxels or grids combined with shallow MLPs have been
proposed to achieve high-quality implicit field learning with reduced
optimization time. On the other hand, lightweight field representations such as
linear grid have been proposed to further improve the learning speed. In this
paper, we aim for both fast and high-quality implicit field learning, and
propose TaylorGrid, a novel implicit field representation which can be
efficiently computed via direct Taylor expansion optimization on 2D or 3D
grids. As a general representation, TaylorGrid can be adapted to different
implicit fields learning tasks such as SDF learning or NeRF. From extensive
quantitative and qualitative comparisons, TaylorGrid achieves a balance between
the linear grid and neural voxels, showing its superiority in fast and
high-quality implicit field learning.



---

## NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth  Supervision for Indoor Multi-View 3D Detection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-22 | Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang | cs.CV | [PDF](http://arxiv.org/pdf/2402.14464v1){: .btn .btn-green } |

**Abstract**: NeRF-Det has achieved impressive performance in indoor multi-view 3D
detection by innovatively utilizing NeRF to enhance representation learning.
Despite its notable performance, we uncover three decisive shortcomings in its
current design, including semantic ambiguity, inappropriate sampling, and
insufficient utilization of depth supervision. To combat the aforementioned
problems, we present three corresponding solutions: 1) Semantic Enhancement. We
project the freely available 3D segmentation annotations onto the 2D plane and
leverage the corresponding 2D semantic maps as the supervision signal,
significantly enhancing the semantic awareness of multi-view detectors. 2)
Perspective-aware Sampling. Instead of employing the uniform sampling strategy,
we put forward the perspective-aware sampling policy that samples densely near
the camera while sparsely in the distance, more effectively collecting the
valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to
directly regressing the depth values that are difficult to optimize, we divide
the depth range of each scene into a fixed number of ordinal bins and
reformulate the depth prediction as the combination of the classification of
depth bins as well as the regression of the residual depth values, thereby
benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has
exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets.
Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9%
in mAP@0.25 and +3.5% in mAP@0.50. The code will be publicly at
https://github.com/mrsempress/NeRF-Detplusplus.

Comments:
- 7 pages, 2 figures

---

## SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-21 | Zhentao Huang, Yukun Shi, Neil Bruce, Minglun Gong | cs.CV | [PDF](http://arxiv.org/pdf/2402.13510v1){: .btn .btn-green } |

**Abstract**: The widespread adoption of implicit neural representations, especially Neural
Radiance Fields (NeRF), highlights a growing need for editing capabilities in
implicit 3D models, essential for tasks like scene post-processing and 3D
content creation. Despite previous efforts in NeRF editing, challenges remain
due to limitations in editing flexibility and quality. The key issue is
developing a neural representation that supports local edits for real-time
updates. Current NeRF editing methods, offering pixel-level adjustments or
detailed geometry and color modifications, are mostly limited to static scenes.
This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level
editing in dynamic settings, specifically targeting the D-NeRF network. It
allows for consistent edits across sequences by mapping editing actions to a
specific timeframe, freezing the deformation network responsible for dynamic
scene representation, and using a teacher-student approach to integrate
changes.



---

## Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering  of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-21 | Joongho Jo, Hyeongwon Kim, Jongsun Park | cs.CV | [PDF](http://arxiv.org/pdf/2402.13827v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms
the neural radiance field (NeRF) in terms of both speed and image quality.
3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects
these Gaussians onto the 2D image plane for rendering. However, during the
rendering process, a substantial number of unnecessary 3D Gaussians exist for
the current view direction, resulting in significant computation costs
associated with their identification. In this paper, we propose a computational
reduction technique that quickly identifies unnecessary 3D Gaussians in
real-time for rendering the current view without compromising image quality.
This is accomplished through the offline clustering of 3D Gaussians that are
close in distance, followed by the projection of these clusters onto a 2D image
plane during runtime. Additionally, we analyze the bottleneck associated with
the proposed technique when executed on GPUs and propose an efficient hardware
architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360
dataset, the proposed technique excludes 63% of 3D Gaussians on average before
the 2D image projection, which reduces the overall rendering computation by
almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The
proposed accelerator also achieves a speedup of 10.7x compared to a GPU.



---

## NeRF Solves Undersampled MRI Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-20 | Tae Jun Jang, Chang Min Hyun | eess.IV | [PDF](http://arxiv.org/pdf/2402.13226v1){: .btn .btn-green } |

**Abstract**: This article presents a novel undersampled magnetic resonance imaging (MRI)
technique that leverages the concept of Neural Radiance Field (NeRF). With
radial undersampling, the corresponding imaging problem can be reformulated
into an image modeling task from sparse-view rendered data; therefore, a high
dimensional MR image is obtainable from undersampled $k$-space data by taking
advantage of implicit neural representation. A multi-layer perceptron, which is
designed to output an image intensity from a spatial coordinate, learns the MR
physics-driven rendering relation between given measurement data and desired
image. Effective undersampling strategies for high-quality neural
representation are investigated. The proposed method serves two benefits: (i)
The learning is based fully on single undersampled $k$-space data, not a bunch
of measured data and target image sets. It can be used potentially for
diagnostic MR imaging, such as fetal MRI, where data acquisition is relatively
rare or limited against diversity of clinical images while undersampled
reconstruction is highly demanded. (ii) A reconstructed MR image is a
scan-specific representation highly adaptive to the given $k$-space
measurement. Numerous experiments validate the feasibility and capability of
the proposed approach.



---

## Improving Robustness for Joint Optimization of Camera Poses and  Decomposed Low-Rank Tensorial Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-20 | Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2402.13252v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose an algorithm that allows joint refinement of camera
pose and scene geometry represented by decomposed low-rank tensor, using only
2D images as supervision. First, we conduct a pilot study based on a 1D signal
and relate our findings to 3D scenarios, where the naive joint pose
optimization on voxel-based NeRFs can easily lead to sub-optimal solutions.
Moreover, based on the analysis of the frequency spectrum, we propose to apply
convolutional Gaussian filters on 2D and 3D radiance fields for a
coarse-to-fine training schedule that enables joint camera pose optimization.
Leveraging the decomposition property in decomposed low-rank tensor, our method
achieves an equivalent effect to brute-force 3D convolution with only incurring
little computational overhead. To further improve the robustness and stability
of joint optimization, we also propose techniques of smoothed 2D supervision,
randomly scaled kernel parameters, and edge-guided loss mask. Extensive
quantitative and qualitative evaluations demonstrate that our proposed
framework achieves superior performance in novel view synthesis as well as
rapid convergence for optimization.

Comments:
- AAAI 2024. Project page:
  https://alex04072000.github.io/Joint-TensoRF/

---

## OccFlowNet: Towards Self-supervised Occupancy Estimation via  Differentiable Rendering and Occupancy Flow

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-20 | Simon Boeder, Fabian Gigengack, Benjamin Risse | cs.CV | [PDF](http://arxiv.org/pdf/2402.12792v1){: .btn .btn-green } |

**Abstract**: Semantic occupancy has recently gained significant traction as a prominent 3D
scene representation. However, most existing methods rely on large and costly
datasets with fine-grained 3D voxel labels for training, which limits their
practicality and scalability, increasing the need for self-monitored learning
in this domain. In this work, we present a novel approach to occupancy
estimation inspired by neural radiance field (NeRF) using only 2D labels, which
are considerably easier to acquire. In particular, we employ differentiable
volumetric rendering to predict depth and semantic maps and train a 3D network
based on 2D supervision only. To enhance geometric accuracy and increase the
supervisory signal, we introduce temporal rendering of adjacent time steps.
Additionally, we introduce occupancy flow as a mechanism to handle dynamic
objects in the scene and ensure their temporal consistency. Through extensive
experimentation we demonstrate that 2D supervision only is sufficient to
achieve state-of-the-art performance compared to methods using 3D labels, while
outperforming concurrent 2D approaches. When combining 2D supervision with 3D
labels, temporal rendering and occupancy flow we outperform all previous
occupancy estimation models significantly. We conclude that the proposed
rendering supervision and occupancy flow advances occupancy estimation and
further bridges the gap towards self-supervised learning in this domain.



---

## How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-20 | Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandström, Stefano Mattoccia, Martin R. Oswald, Matteo Poggi | cs.CV | [PDF](http://arxiv.org/pdf/2402.13255v1){: .btn .btn-green } |

**Abstract**: Over the past two decades, research in the field of Simultaneous Localization
and Mapping (SLAM) has undergone a significant evolution, highlighting its
critical role in enabling autonomous exploration of unknown environments. This
evolution ranges from hand-crafted methods, through the era of deep learning,
to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D
Gaussian Splatting (3DGS) representations. Recognizing the growing body of
research and the absence of a comprehensive survey on the topic, this paper
aims to provide the first comprehensive overview of SLAM progress through the
lens of the latest advancements in radiance fields. It sheds light on the
background, evolutionary path, inherent strengths and limitations, and serves
as a fundamental reference to highlight the dynamic progress and specific
challenges.



---

## Colorizing Monochromatic Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-19 | Yean Cheng, Renjie Wan, Shuchen Weng, Chengxuan Zhu, Yakun Chang, Boxin Shi | cs.CV | [PDF](http://arxiv.org/pdf/2402.12184v1){: .btn .btn-green } |

**Abstract**: Though Neural Radiance Fields (NeRF) can produce colorful 3D representations
of the world by using a set of 2D images, such ability becomes non-existent
when only monochromatic images are provided. Since color is necessary in
representing the world, reproducing color from monochromatic radiance fields
becomes crucial. To achieve this goal, instead of manipulating the
monochromatic radiance fields directly, we consider it as a
representation-prediction task in the Lab color space. By first constructing
the luminance and density representation using monochromatic images, our
prediction stage can recreate color representation on the basis of an image
colorization module. We then reproduce a colorful implicit model through the
representation of luminance, density, and color. Extensive experiments have
been conducted to validate the effectiveness of our approaches. Our project
page: https://liquidammonia.github.io/color-nerf.



---

## Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based  View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-19 | Christian Reiser, Stephan Garbin, Pratul P. Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan T. Barron, Peter Hedman, Andreas Geiger | cs.CV | [PDF](http://arxiv.org/pdf/2402.12377v1){: .btn .btn-green } |

**Abstract**: While surface-based view synthesis algorithms are appealing due to their low
computational requirements, they often struggle to reproduce thin structures.
In contrast, more expensive methods that model the scene's geometry as a
volumetric density field (e.g. NeRF) excel at reconstructing fine geometric
detail. However, density fields often represent geometry in a "fuzzy" manner,
which hinders exact localization of the surface. In this work, we modify
density fields to encourage them to converge towards surfaces, without
compromising their ability to reconstruct thin structures. First, we employ a
discrete opacity grid representation instead of a continuous density field,
which allows opacity values to discontinuously transition from zero to one at
the surface. Second, we anti-alias by casting multiple rays per pixel, which
allows occlusion boundaries and subpixel structures to be modelled without
using semi-transparent voxels. Third, we minimize the binary entropy of the
opacity values, which facilitates the extraction of surface geometry by
encouraging opacity values to binarize towards the end of training. Lastly, we
develop a fusion-based meshing strategy followed by mesh simplification and
appearance model fitting. The compact meshes produced by our model can be
rendered in real-time on mobile devices and achieve significantly higher view
synthesis quality compared to existing mesh-based approaches.

Comments:
- Project page at https://binary-opacity-grid.github.io

---

## Semantically-aware Neural Radiance Fields for Visual Scene  Understanding: A Comprehensive Review

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-17 | Thang-Anh-Quan Nguyen, Amine Bourki, Mátyás Macudzinski, Anthony Brunel, Mohammed Bennamoun | cs.CV | [PDF](http://arxiv.org/pdf/2402.11141v1){: .btn .btn-green } |

**Abstract**: This review thoroughly examines the role of semantically-aware Neural
Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of
over 250 scholarly papers. It explores how NeRFs adeptly infer 3D
representations for both stationary and dynamic objects in a scene. This
capability is pivotal for generating high-quality new viewpoints, completing
missing scene details (inpainting), conducting comprehensive scene segmentation
(panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and
extracting object-centric 3D models. A significant aspect of this study is the
application of semantic labels as viewpoint-invariant functions, which
effectively map spatial coordinates to a spectrum of semantic labels, thus
facilitating the recognition of distinct objects within the scene. Overall,
this survey highlights the progression and diverse applications of
semantically-aware neural radiance fields in the context of visual scene
interpretation.



---

## Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field  Conditions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-15 | Muhammad Arbab Arshad, Talukder Jubery, James Afful, Anushrut Jignasu, Aditya Balu, Baskar Ganapathysubramanian, Soumik Sarkar, Adarsh Krishnamurthy | cs.CV | [PDF](http://arxiv.org/pdf/2402.10344v1){: .btn .btn-green } |

**Abstract**: We evaluate different Neural Radiance Fields (NeRFs) techniques for
reconstructing (3D) plants in varied environments, from indoor settings to
outdoor fields. Traditional techniques often struggle to capture the complex
details of plants, which is crucial for botanical and agricultural
understanding. We evaluate three scenarios with increasing complexity and
compare the results with the point cloud obtained using LiDAR as ground truth
data. In the most realistic field scenario, the NeRF models achieve a 74.65% F1
score with 30 minutes of training on the GPU, highlighting the efficiency and
accuracy of NeRFs in challenging environments. These findings not only
demonstrate the potential of NeRF in detailed and realistic 3D plant modeling
but also suggest practical approaches for enhancing the speed and efficiency of
the 3D reconstruction process.



---

## GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object  with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-15 | Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian | cs.CV | [PDF](http://arxiv.org/pdf/2402.10259v2){: .btn .btn-green } |

**Abstract**: Reconstructing and rendering 3D objects from highly sparse views is of
critical importance for promoting applications of 3D vision techniques and
improving user experience. However, images from sparse views only contain very
limited 3D information, leading to two significant challenges: 1) Difficulty in
building multi-view consistency as images for matching are too few; 2)
Partially omitted or highly compressed object information as view coverage is
insufficient. To tackle these challenges, we propose GaussianObject, a
framework to represent and render the 3D object with Gaussian splatting, that
achieves high rendering quality with only 4 input images. We first introduce
techniques of visual hull and floater elimination which explicitly inject
structure priors into the initial optimization process for helping build
multi-view consistency, yielding a coarse 3D Gaussian representation. Then we
construct a Gaussian repair model based on diffusion models to supplement the
omitted object information, where Gaussians are further refined. We design a
self-generating strategy to obtain image pairs for training the repair model.
Our GaussianObject is evaluated on several challenging datasets, including
MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction
results from only 4 views and significantly outperforming previous
state-of-the-art methods.

Comments:
- Project page: https://gaussianobject.github.io/

---

## GES: Generalized Exponential Splatting for Efficient Radiance Field  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-15 | Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi | cs.CV | [PDF](http://arxiv.org/pdf/2402.10128v1){: .btn .btn-green } |

**Abstract**: Advancements in 3D Gaussian Splatting have significantly accelerated 3D
reconstruction and generation. However, it may require a large number of
Gaussians, which creates a substantial memory footprint. This paper introduces
GES (Generalized Exponential Splatting), a novel representation that employs
Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer
particles to represent a scene and thus significantly outperforming Gaussian
Splatting methods in efficiency with a plug-and-play replacement ability for
Gaussian-based utilities. GES is validated theoretically and empirically in
both principled 1D setup and realistic 3D scenes.
  It is shown to represent signals with sharp edges more accurately, which are
typically challenging for Gaussians due to their inherent low-pass
characteristics. Our empirical analysis demonstrates that GEF outperforms
Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and
parabolic signals), thereby reducing the need for extensive splitting
operations that increase the memory footprint of Gaussian Splatting. With the
aid of a frequency-modulated loss, GES achieves competitive performance in
novel-view synthesis benchmarks while requiring less than half the memory
storage of Gaussian Splatting and increasing the rendering speed by up to 39%.
The code is available on the project website https://abdullahamdi.com/ges .

Comments:
- preprint

---

## PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames  in Autonomous Driving Environments

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-14 | Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma | cs.CV | [PDF](http://arxiv.org/pdf/2402.09325v1){: .btn .btn-green } |

**Abstract**: Large-scale 3D scene reconstruction and novel view synthesis are vital for
autonomous vehicles, especially utilizing temporally sparse LiDAR frames.
However, conventional explicit representations remain a significant bottleneck
towards representing the reconstructed and synthetic scenes at unlimited
resolution. Although the recently developed neural radiance fields (NeRF) have
shown compelling results in implicit representations, the problem of
large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR
frames remains unexplored. To bridge this gap, we propose a 3D scene
reconstruction and novel view synthesis framework called parent-child neural
radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF,
the framework implements hierarchical spatial partitioning and multi-level
scene representation, including scene, segment, and point levels. The
multi-level scene representation enhances the efficient utilization of sparse
LiDAR point cloud data and enables the rapid acquisition of an approximate
volumetric scene representation. With extensive experiments, PC-NeRF is proven
to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in
large-scale scenes. Moreover, PC-NeRF can effectively handle situations with
sparse LiDAR frames and demonstrate high deployment efficiency with limited
training epochs. Our approach implementation and the pre-trained models are
available at https://github.com/biter0088/pc-nerf.

Comments:
- arXiv admin note: substantial text overlap with arXiv:2310.00874

---

## NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-13 | Michael Fischer, Zhengqin Li, Thu Nguyen-Phuoc, Aljaz Bozic, Zhao Dong, Carl Marshall, Tobias Ritschel | cs.CV | [PDF](http://arxiv.org/pdf/2402.08622v1){: .btn .btn-green } |

**Abstract**: A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry
and appearance of a scene. We here ask the question whether we can transfer the
appearance from a source NeRF onto a target 3D geometry in a semantically
meaningful way, such that the resulting new NeRF retains the target geometry
but has an appearance that is an analogy to the source NeRF. To this end, we
generalize classic image analogies from 2D images to NeRFs. We leverage
correspondence transfer along semantic affinity that is driven by semantic
features from large, pre-trained 2D image models to achieve multi-view
consistent appearance transfer. Our method allows exploring the mix-and-match
product space of 3D geometry and appearance. We show that our method
outperforms traditional stylization-based methods and that a large majority of
users prefer our method over several typical baselines.

Comments:
- Project page: https://mfischer-ucl.github.io/nerf_analogies/

---

## IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality  3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-13 | Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, Filippos Kokkinos | cs.CV | [PDF](http://arxiv.org/pdf/2402.08682v1){: .btn .btn-green } |

**Abstract**: Most text-to-3D generators build upon off-the-shelf text-to-image models
trained on billions of images. They use variants of Score Distillation Sampling
(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation
is to fine-tune the 2D generator to be multi-view aware, which can help
distillation or can be combined with reconstruction networks to output 3D
objects directly. In this paper, we further explore the design space of
text-to-3D models. We significantly improve multi-view generation by
considering video instead of image generators. Combined with a 3D
reconstruction algorithm which, by using Gaussian splatting, can optimize a
robust image-based loss, we directly produce high-quality 3D outputs from the
generated views. Our new method, IM-3D, reduces the number of evaluations of
the 2D generator network 10-100x, resulting in a much more efficient pipeline,
better quality, fewer geometric inconsistencies, and higher yield of usable 3D
assets.



---

## H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object  Surface Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-13 | Minyoung Park, Mirae Do, YeonJae Shin, Jaeseok Yoo, Jongkwang Hong, Joongrock Kim, Chul Lee | cs.CV | [PDF](http://arxiv.org/pdf/2402.08138v1){: .btn .btn-green } |

**Abstract**: Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance
Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D
indoor scene reconstruction. We introduce a novel two-phase learning approach,
H2O-SDF, that discriminates between object and non-object regions within indoor
environments. This method achieves a nuanced balance, carefully preserving the
geometric integrity of room layouts while also capturing intricate surface
details of specific objects. A cornerstone of our two-phase learning framework
is the introduction of the Object Surface Field (OSF), a novel concept designed
to mitigate the persistent vanishing gradient problem that has previously
hindered the capture of high-frequency details in other methods. Our proposed
approach is validated through several experiments that include ablation
studies.



---

## Preconditioners for the Stochastic Training of Implicit Neural  Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-13 | Shin-Fang Chng, Hemanth Saratchandran, Simon Lucey | cs.CV | [PDF](http://arxiv.org/pdf/2402.08784v1){: .btn .btn-green } |

**Abstract**: Implicit neural representations have emerged as a powerful technique for
encoding complex continuous multidimensional signals as neural networks,
enabling a wide range of applications in computer vision, robotics, and
geometry. While Adam is commonly used for training due to its stochastic
proficiency, it entails lengthy training durations. To address this, we explore
alternative optimization techniques for accelerated training without
sacrificing accuracy. Traditional second-order optimizers like L-BFGS are
suboptimal in stochastic settings, making them unsuitable for large-scale data
sets. Instead, we propose stochastic training using curvature-aware diagonal
preconditioners, showcasing their effectiveness across various signal
modalities such as images, shape reconstruction, and Neural Radiance Fields
(NeRF).

Comments:
- The first two authors contributed equally

---

## DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable  Object Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-12 | Chenchang Li, Zihao Ai, Tong Wu, Xiaosa Li, Wenbo Ding, Huazhe Xu | cs.RO | [PDF](http://arxiv.org/pdf/2402.07648v1){: .btn .btn-green } |

**Abstract**: Manipulating deformable objects is a ubiquitous task in household
environments, demanding adequate representation and accurate dynamics
prediction due to the objects' infinite degrees of freedom. This work proposes
DeformNet, which utilizes latent space modeling with a learned 3D
representation model to tackle these challenges effectively. The proposed
representation model combines a PointNet encoder and a conditional neural
radiance field (NeRF), facilitating a thorough acquisition of object
deformations and variations in lighting conditions. To model the complex
dynamics, we employ a recurrent state-space model (RSSM) that accurately
predicts the transformation of the latent representation over time. Extensive
simulation experiments with diverse objectives demonstrate the generalization
capabilities of DeformNet for various deformable object manipulation tasks,
even in the presence of previously unseen goals. Finally, we deploy DeformNet
on an actual UR5 robotic arm to demonstrate its capability in real-world
scenarios.

Comments:
- 7 pages, Submitted to 2024 IEEE International Conference on Robotics
  and Automation (ICRA), Japan, Yokohama

---

## GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided  Generative Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-11 | Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2402.07207v1){: .btn .btn-green } |

**Abstract**: We present GALA3D, generative 3D GAussians with LAyout-guided control, for
effective compositional text-to-3D generation. We first utilize large language
models (LLMs) to generate the initial layout and introduce a layout-guided 3D
Gaussian representation for 3D content generation with adaptive geometric
constraints. We then propose an object-scene compositional optimization
mechanism with conditioned diffusion to collaboratively generate realistic 3D
scenes with consistent geometry, texture, scale, and accurate interactions
among multiple objects while simultaneously adjusting the coarse layout priors
extracted from the LLMs to align with the generated scene. Experiments show
that GALA3D is a user-friendly, end-to-end framework for state-of-the-art
scene-level 3D content generation and controllable editing while ensuring the
high fidelity of object-level entities within the scene. Source codes and
models will be available at https://gala3d.github.io/.



---

## BioNeRF: Biologically Plausible Neural Radiance Fields for View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-11 | Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, João Paulo Papa | cs.CV | [PDF](http://arxiv.org/pdf/2402.07310v1){: .btn .btn-green } |

**Abstract**: This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.



---

## 3D Gaussian as a New Vision Era: A Survey

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-11 | Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2402.07181v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the
field of Computer Graphics, offering explicit scene representation and novel
view synthesis without the reliance on neural networks, such as Neural Radiance
Fields (NeRF). This technique has found diverse applications in areas such as
robotics, urban mapping, autonomous navigation, and virtual reality/augmented
reality, just name a few. Given the growing popularity and expanding research
in 3D Gaussian Splatting, this paper presents a comprehensive survey of
relevant papers from the past year. We organize the survey into taxonomies
based on characteristics and applications, providing an introduction to the
theoretical underpinnings of 3D Gaussian Splatting. Our goal through this
survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a
valuable reference for seminal works in the field, and inspire future research
directions, as discussed in our concluding section.



---

## GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D  Pretraining from Real-World Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-09 | Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang | cs.CV | [PDF](http://arxiv.org/pdf/2402.06198v2){: .btn .btn-green } |

**Abstract**: 3D Shape represented as point cloud has achieve advancements in multimodal
pre-training to align image and language descriptions, which is curial to
object identification, classification, and retrieval. However, the discrete
representations of point cloud lost the object's surface shape information and
creates a gap between rendering results and 2D correspondences. To address this
problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D
Gaussian Splatting) into multimodal pre-training to enhance 3D representation.
GS-CLIP leverages a pre-trained vision-language model for a learned common
visual and textual space on massive real world image-text pairs and then learns
a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel
Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature.
As a general framework for language-image-3D pre-training, GS-CLIP is agnostic
to 3D backbone networks. Experiments on challenging shows that GS-CLIP
significantly improves the state-of-the-art, outperforming the previously best
results.

Comments:
- The content of the technical report needs to be updated and retracted
  to avoid other impacts

---

## HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-09 | Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2402.06149v1){: .btn .btn-green } |

**Abstract**: Creating digital avatars from textual prompts has long been a desirable yet
challenging task. Despite the promising outcomes obtained through 2D diffusion
priors in recent works, current methods face challenges in achieving
high-quality and animated avatars effectively. In this paper, we present
$\textbf{HeadStudio}$, a novel framework that utilizes 3D Gaussian splatting to
generate realistic and animated avatars from text prompts. Our method drives 3D
Gaussians semantically to create a flexible and achievable appearance through
the intermediate FLAME representation. Specifically, we incorporate the FLAME
into both 3D representation and score distillation: 1) FLAME-based 3D Gaussian
splatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2)
FLAME-based score distillation sampling, utilizing FLAME-based fine-grained
control signal to guide score distillation from the text prompt. Extensive
experiments demonstrate the efficacy of HeadStudio in generating animatable
avatars from textual prompts, exhibiting visually appealing appearances. The
avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel
views at a resolution of 1024. They can be smoothly controlled by real-world
speech and video. We hope that HeadStudio can advance digital avatar creation
and that the present method can widely be applied across various domains.

Comments:
- 9 pages, 8 figures

---

## ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake  Generation using NeRF and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-09 | Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2402.06390v1){: .btn .btn-green } |

**Abstract**: Numerous emerging deep-learning techniques have had a substantial impact on
computer graphics. Among the most promising breakthroughs are the recent rise
of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the
object's shape and color in neural network weights using a handful of images
with known camera positions to generate novel views. In contrast, GS provides
accelerated training and inference without a decrease in rendering quality by
encoding the object's characteristics in a collection of Gaussian
distributions. These two techniques have found many use cases in spatial
computing and other domains. On the other hand, the emergence of deepfake
methods has sparked considerable controversy. Such techniques can have a form
of artificial intelligence-generated videos that closely mimic authentic
footage. Using generative models, they can modify facial features, enabling the
creation of altered identities or facial expressions that exhibit a remarkably
realistic appearance to a real person. Despite these controversies, deepfake
can offer a next-generation solution for avatar creation and gaming when of
desirable quality. To that end, we show how to combine all these emerging
technologies to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the
classical deepfake algorithm to modify all training images separately and then
train NeRF and GS on modified faces. Such relatively simple strategies can
produce plausible 3D deepfake-based avatars.



---

## OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language  Foundation Models for 3D Semantic Understanding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-07 | Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li | cs.CV | [PDF](http://arxiv.org/pdf/2402.04648v1){: .btn .btn-green } |

**Abstract**: The development of Neural Radiance Fields (NeRFs) has provided a potent
representation for encapsulating the geometric and appearance characteristics
of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D
semantic perception tasks has been a recent focus. However, current methods
that extract semantics directly from Contrastive Language-Image Pretraining
(CLIP) for semantic field learning encounter difficulties due to noisy and
view-inconsistent semantics provided by CLIP. To tackle these limitations, we
propose OV-NeRF, which exploits the potential of pre-trained vision and
language foundation models to enhance semantic field learning through proposed
single-view and cross-view strategies. First, from the single-view perspective,
we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask
proposals derived from SAM to rectify the noisy semantics of each training
view, facilitating accurate semantic field learning. Second, from the
cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy
to address the challenge raised by view-inconsistent semantics. Rather than
invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the
3D consistent semantics generated from the well-trained semantic field itself
for semantic field training, aiming to reduce ambiguity and enhance overall
semantic consistency across different views. Extensive experiments validate our
OV-NeRF outperforms current state-of-the-art methods, achieving a significant
improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet,
respectively. Furthermore, our approach exhibits consistent superior results
across various CLIP configurations, further verifying its robustness.



---

## BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial  Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-07 | Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao | cs.CV | [PDF](http://arxiv.org/pdf/2402.04554v2){: .btn .btn-green } |

**Abstract**: In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields
(NeRF) designed specifically for reconstructing large-scale scenes using aerial
imagery. Unlike previous research focused on small-scale and object-centric
NeRF reconstruction, our approach addresses multiple challenges, including (1)
Addressing the issue of slow training and rendering associated with large
models. (2) Meeting the computational demands necessitated by modeling a
substantial number of images, requiring extensive resources such as
high-performance GPUs. (3) Overcoming significant artifacts and low visual
fidelity commonly observed in large-scale reconstruction tasks due to limited
model capacity. Specifically, we present a novel bird-view pose-based spatial
decomposition algorithm that decomposes a large aerial image set into multiple
small sets with appropriately sized overlaps, allowing us to train individual
NeRFs of sub-scene. This decomposition approach not only decouples rendering
time from the scene size but also enables rendering to scale seamlessly to
arbitrarily large environments. Moreover, it allows for per-block updates of
the environment, enhancing the flexibility and adaptability of the
reconstruction process. Additionally, we propose a projection-guided novel view
re-rendering strategy, which aids in effectively utilizing the independently
trained sub-scenes to generate superior rendering results. We evaluate our
approach on existing datasets as well as against our own drone footage,
improving reconstruction speed by 10x over classical photogrammetry software
and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with
similar rendering quality.



---

## Mesh-based Gaussian Splatting for Real-time Large-scale Deformation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-07 | Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai | cs.GR | [PDF](http://arxiv.org/pdf/2402.04796v1){: .btn .btn-green } |

**Abstract**: Neural implicit representations, including Neural Distance Fields and Neural
Radiance Fields, have demonstrated significant capabilities for reconstructing
surfaces with complicated geometry and topology, and generating novel views of
a scene. Nevertheless, it is challenging for users to directly deform or
manipulate these implicit representations with large deformations in the
real-time fashion. Gaussian Splatting(GS) has recently become a promising
method with explicit geometry for representing static scenes and facilitating
high-quality and real-time synthesis of novel views. However,it cannot be
easily deformed due to the use of discrete Gaussians and lack of explicit
topology. To address this, we develop a novel GS-based method that enables
interactive deformation. Our key idea is to design an innovative mesh-based GS
representation, which is integrated into Gaussian learning and manipulation. 3D
Gaussians are defined over an explicit mesh, and they are bound with each
other: the rendering of 3D Gaussians guides the mesh face split for adaptive
refinement, and the mesh face split directs the splitting of 3D Gaussians.
Moreover, the explicit mesh constraints help regularize the Gaussian
distribution, suppressing poor-quality Gaussians(e.g. misaligned
Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and
avoiding artifacts during deformation. Based on this representation, we further
introduce a large-scale Gaussian deformation technique to enable deformable GS,
which alters the parameters of 3D Gaussians according to the manipulation of
the associated mesh. Our method benefits from existing mesh deformation
datasets for more realistic data-driven Gaussian deformation. Extensive
experiments show that our approach achieves high-quality reconstruction and
effective deformation, while maintaining the promising rendering results at a
high frame rate(65 FPS on average).

Comments:
- 11 pages, 7 figures

---

## NeRF as Non-Distant Environment Emitter in Physics-based Inverse  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-07 | Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2402.04829v1){: .btn .btn-green } |

**Abstract**: Physics-based inverse rendering aims to jointly optimize shape, materials,
and lighting from captured 2D images. Here lighting is an important part of
achieving faithful light transport simulation. While the environment map is
commonly used as the lighting model in inverse rendering, we show that its
distant lighting assumption leads to spatial invariant lighting, which can be
an inaccurate approximation in real-world inverse rendering. We propose to use
NeRF as a spatially varying environment lighting model and build an inverse
rendering pipeline using NeRF as the non-distant environment emitter. By
comparing our method with the environment map on real and synthetic datasets,
we show that our NeRF-based emitter models the scene lighting more accurately
and leads to more accurate inverse rendering. Project page and video:
https://nerfemitterpbir.github.io/.

Comments:
- Project page and video: https://nerfemitterpbir.github.io/

---

## Improved Generalization of Weight Space Networks via Augmentations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-06 | Aviv Shamsian, Aviv Navon, David W. Zhang, Yan Zhang, Ethan Fetaya, Gal Chechik, Haggai Maron | cs.LG | [PDF](http://arxiv.org/pdf/2402.04081v1){: .btn .btn-green } |

**Abstract**: Learning in deep weight spaces (DWS), where neural networks process the
weights of other neural networks, is an emerging research direction, with
applications to 2D and 3D neural fields (INRs, NeRFs), as well as making
inferences about other types of neural networks. Unfortunately, weight space
models tend to suffer from substantial overfitting. We empirically analyze the
reasons for this overfitting and find that a key reason is the lack of
diversity in DWS datasets. While a given object can be represented by many
different weight configurations, typical INR training sets fail to capture
variability across INRs that represent the same object. To address this, we
explore strategies for data augmentation in weight spaces and propose a MixUp
method adapted for weight spaces. We demonstrate the effectiveness of these
methods in two setups. In classification, they improve performance similarly to
having up to 10 times more data. In self-supervised contrastive learning, they
yield substantial 5-10% gains in downstream classification.

Comments:
- Under Review

---

## Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-06 | Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras | cs.CV | [PDF](http://arxiv.org/pdf/2402.03723v1){: .btn .btn-green } |

**Abstract**: Creating controllable 3D human portraits from casual smartphone videos is
highly desirable due to their immense value in AR/VR applications. The recent
development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering
quality and training efficiency. However, it still remains a challenge to
accurately model and disentangle head movements and facial expressions from a
single-view capture to achieve high-quality renderings. In this paper, we
introduce Rig3DGS to address this challenge. We represent the entire scene,
including the dynamic subject, using a set of 3D Gaussians in a canonical
space. Using a set of control signals, such as head pose and expressions, we
transform them to the 3D space with learned deformations to generate the
desired rendering. Our key innovation is a carefully designed deformation
method which is guided by a learnable prior derived from a 3D morphable model.
This approach is highly efficient in training and effective in controlling
facial expressions, head positions, and view synthesis across various captures.
We demonstrate the effectiveness of our learned deformation through extensive
quantitative and qualitative experiments. The project page can be found at
http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html



---

## 4D Gaussian Splatting: Towards Efficient Novel View Synthesis for  Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-05 | Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen | cs.CV | [PDF](http://arxiv.org/pdf/2402.03307v2){: .btn .btn-green } |

**Abstract**: We consider the problem of novel view synthesis (NVS) for dynamic scenes.
Recent neural approaches have accomplished exceptional NVS results for static
3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior
efforts often encode dynamics by learning a canonical space plus implicit or
explicit deformation fields, which struggle in challenging scenarios like
sudden movements or capturing high-fidelity renderings. In this paper, we
introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic
scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D
Gaussian Splatting in static scenes. We model dynamics at each timestamp by
temporally slicing the 4D Gaussians, which naturally compose dynamic 3D
Gaussians and can be seamlessly projected into images. As an explicit
spatial-temporal representation, 4DGS demonstrates powerful capabilities for
modeling complicated dynamics and fine details, especially for scenes with
abrupt motions. We further implement our temporal slicing and splatting
techniques in a highly optimized CUDA acceleration framework, achieving
real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and
583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions
showcase the superior efficiency and effectiveness of 4DGS, which consistently
outperforms existing methods both quantitatively and qualitatively.



---

## SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-05 | Mingrui Li, Shuhong Liu, Heng Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2402.03246v1){: .btn .btn-green } |

**Abstract**: Semantic understanding plays a crucial role in Dense Simultaneous
Localization and Mapping (SLAM), facilitating comprehensive scene
interpretation. Recent advancements that integrate Gaussian Splatting into SLAM
systems have demonstrated its effectiveness in generating high-quality
renderings through the use of explicit 3D Gaussian representations. Building on
this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system
grounded in 3D Gaussians, which provides precise 3D semantic segmentation
alongside high-fidelity reconstructions. Specifically, we propose to employ
multi-channel optimization during the mapping process, integrating appearance,
geometric, and semantic constraints with key-frame optimization to enhance
reconstruction quality. Extensive experiments demonstrate that SGS-SLAM
delivers state-of-the-art performance in camera pose estimation, map
reconstruction, and semantic segmentation, outperforming existing methods
meanwhile preserving real-time rendering ability.



---

## ViewFusion: Learning Composable Diffusion Models for Novel View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-05 | Bernard Spiegl, Andrea Perin, Stéphane Deny, Alexander Ilin | cs.CV | [PDF](http://arxiv.org/pdf/2402.02906v1){: .btn .btn-green } |

**Abstract**: Deep learning is providing a wealth of new approaches to the old problem of
novel view synthesis, from Neural Radiance Field (NeRF) based approaches to
end-to-end style architectures. Each approach offers specific strengths but
also comes with specific limitations in their applicability. This work
introduces ViewFusion, a state-of-the-art end-to-end generative approach to
novel view synthesis with unparalleled flexibility. ViewFusion consists in
simultaneously applying a diffusion denoising step to any number of input views
of a scene, then combining the noise gradients obtained for each view with an
(inferred) pixel-weighting mask, ensuring that for each region of the target
scene only the most informative input views are taken into account. Our
approach resolves several limitations of previous approaches by (1) being
trainable and generalizing across multiple scenes and object classes, (2)
adaptively taking in a variable number of pose-free views at both train and
test time, (3) generating plausible views even in severely undetermined
conditions (thanks to its generative nature) -- all while generating views of
quality on par or even better than state-of-the-art methods. Limitations
include not generating a 3D embedding of the scene, resulting in a relatively
slow inference speed, and our method only being tested on the relatively small
dataset NMR. Code is available.



---

## Di-NeRF: Distributed NeRF for Collaborative Learning with Unknown  Relative Poses

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Mahboubeh Asadi, Kourosh Zareinia, Sajad Saeedi | cs.RO | [PDF](http://arxiv.org/pdf/2402.01485v1){: .btn .btn-green } |

**Abstract**: Collaborative mapping of unknown environments can be done faster and more
robustly than a single robot. However, a collaborative approach requires a
distributed paradigm to be scalable and deal with communication issues. This
work presents a fully distributed algorithm enabling a group of robots to
collectively optimize the parameters of a Neural Radiance Field (NeRF). The
algorithm involves the communication of each robot's trained NeRF parameters
over a mesh network, where each robot trains its NeRF and has access to its own
visual data only. Additionally, the relative poses of all robots are jointly
optimized alongside the model parameters, enabling mapping with unknown
relative camera poses. We show that multi-robot systems can benefit from
differentiable and robust 3D reconstruction optimized from multiple NeRFs.
Experiments on real-world and synthetic data demonstrate the efficiency of the
proposed algorithm. See the website of the project for videos of the
experiments and supplementary
material(https://sites.google.com/view/di-nerf/home).

Comments:
- 9 pages, 11 figures, Submitted to IEEE-RA-L

---

## HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Paweł Batorski, Dawid Malarz, Marcin Przewięźlikowski, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2402.01524v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) are a widely accepted standard for
synthesizing new 3D object views from a small number of base images. However,
NeRFs have limited generalization properties, which means that we need to use
significant computational resources to train individual architectures for each
item we want to represent. To address this issue, we propose a few-shot
learning approach based on the hypernetwork paradigm that does not require
gradient optimization during inference. The hypernetwork gathers information
from the training data and generates an update for universal weights. As a
result, we have developed an efficient method for generating a high-quality 3D
object representation from a small number of images in a single step. This has
been confirmed by direct comparison with the state-of-the-art solutions and a
comprehensive ablation study.



---

## Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate  Distortion Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Zhiyu Zhang, Guo Lu, Huanxiong Liang, Anni Tang, Qiang Hu, Li Song | cs.CV | [PDF](http://arxiv.org/pdf/2402.01380v1){: .btn .btn-green } |

**Abstract**: Volumetric videos, benefiting from immersive 3D realism and interactivity,
hold vast potential for various applications, while the tremendous data volume
poses significant challenges for compression. Recently, NeRF has demonstrated
remarkable potential in volumetric video compression thanks to its simple
representation and powerful 3D modeling capabilities, where a notable work is
ReRF. However, ReRF separates the modeling from compression process, resulting
in suboptimal compression efficiency. In contrast, in this paper, we propose a
volumetric video compression method based on dynamic NeRF in a more compact
manner. Specifically, we decompose the NeRF representation into the coefficient
fields and the basis fields, incrementally updating the basis fields in the
temporal domain to achieve dynamic modeling. Additionally, we perform
end-to-end joint optimization on the modeling and compression process to
further improve the compression efficiency. Extensive experiments demonstrate
that our method achieves higher compression efficiency compared to ReRF on
various datasets.



---

## Robust Inverse Graphics via Probabilistic Inference

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous | cs.CV | [PDF](http://arxiv.org/pdf/2402.01915v1){: .btn .btn-green } |

**Abstract**: How do we infer a 3D scene from a single image in the presence of corruptions
like rain, snow or fog? Straightforward domain randomization relies on knowing
the family of corruptions ahead of time. Here, we propose a Bayesian
approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene
prior and an uninformative uniform corruption prior, making it applicable to a
wide range of corruptions. Given a single image, RIG performs posterior
inference jointly over the scene and the corruption. We demonstrate this idea
by training a neural radiance field (NeRF) scene prior and using a secondary
NeRF to represent the corruptions over which we place an uninformative prior.
RIG, trained only on clean data, outperforms depth estimators and alternative
NeRF approaches that perform point estimation instead of full inference. The
results hold for a number of scene prior architectures based on normalizing
flows and diffusion models. For the latter, we develop reconstruction-guidance
with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is
applicable in the presence of auxiliary latent variables such as the
corruption. RIG demonstrates how scene priors can be used beyond generation
tasks.



---

## Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect  Diffusion Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Yaokun Li, Chao Gou, Guang Tan | cs.CV | [PDF](http://arxiv.org/pdf/2402.01217v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing
novel views. However, their reliance on dense inputs and scene-specific
optimization has limited their broader applicability. Generalizable NeRFs
(Gen-NeRF), while intended to address this, often produce blurring artifacts in
unobserved regions with sparse inputs, which are full of uncertainty. In this
paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings.
We assume that NeRF's inability to effectively mitigate this uncertainty stems
from its inherent lack of generative capacity. Therefore, we innovatively
propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address
this uncertainty from a generative perspective by leveraging a distilled
diffusion prior as guidance. Specifically, to avoid model confusion caused by
directly regularizing with inconsistent samplings as in previous methods, our
approach introduces a strategy to indirectly inject the inherently missing
imagination into the learned implicit function through a diffusion-guided
latent space. Empirical evaluation across various benchmarks demonstrates the
superior performance of our approach in handling uncertainty with sparse
inputs.



---

## ConRF: Zero-shot Stylization of 3D Scenes with Conditioned Radiation  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Xingyu Miao, Yang Bai, Haoran Duan, Fan Wan, Yawen Huang, Yang Long, Yefeng Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2402.01950v1){: .btn .btn-green } |

**Abstract**: Most of the existing works on arbitrary 3D NeRF style transfer required
retraining on each single style condition. This work aims to achieve zero-shot
controlled stylization in 3D scenes utilizing text or visual input as
conditioning factors. We introduce ConRF, a novel method of zero-shot
stylization. Specifically, due to the ambiguity of CLIP features, we employ a
conversion process that maps the CLIP feature space to the style space of a
pre-trained VGG network and then refine the CLIP multi-modal knowledge into a
style transfer neural radiation field. Additionally, we use a 3D volumetric
representation to perform local style transfer. By combining these operations,
ConRF offers the capability to utilize either text or images as references,
resulting in the generation of sequences with novel views enhanced by global or
local stylization. Our experiment demonstrates that ConRF outperforms other
existing methods for 3D scene and single-text stylization in terms of visual
quality.



---

## GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2402.01459v3){: .btn .btn-green } |

**Abstract**: Recently, a range of neural network-based methods for image rendering have
been introduced. One such widely-researched neural radiance field (NeRF) relies
on a neural network to represent 3D scenes, allowing for realistic view
synthesis from a small number of 2D images. However, most NeRF models are
constrained by long training and inference times. In comparison, Gaussian
Splatting (GS) is a novel, state-of-the-art technique for rendering points in a
3D scene by approximating their contribution to image pixels through Gaussian
distributions, warranting fast training and swift, real-time rendering. A
drawback of GS is the absence of a well-defined approach for its conditioning
due to the necessity to condition several hundred thousand Gaussian components.
To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which
allows modification of Gaussian components in a similar way as meshes. We
parameterize each Gaussian component by the vertices of the mesh face.
Furthermore, our model needs mesh initialization on input or estimated mesh
during training. We also define Gaussian splats solely based on their location
on the mesh, allowing for automatic adjustments in position, scale, and
rotation during animation. As a result, we obtain a real-time rendering of
editable GS.



---

## Emo-Avatar: Efficient Monocular Video Style Avatar through Texture  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Pinxin Liu, Luchuan Song, Daoan Zhang, Hang Hua, Yunlong Tang, Huaijin Tu, Jiebo Luo, Chenliang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2402.00827v1){: .btn .btn-green } |

**Abstract**: Artistic video portrait generation is a significant and sought-after task in
the fields of computer graphics and vision. While various methods have been
developed that integrate NeRFs or StyleGANs with instructional editing models
for creating and editing drivable portraits, these approaches face several
challenges. They often rely heavily on large datasets, require extensive
customization processes, and frequently result in reduced image quality. To
address the above problems, we propose the Efficient Monotonic Video Style
Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's
capacity for producing dynamic, drivable portrait videos. We proposed a
two-stage deferred neural rendering pipeline. In the first stage, we utilize
few-shot PTI initialization to initialize the StyleGAN generator through
several extreme poses sampled from the video to capture the consistent
representation of aligned faces from the target portrait. In the second stage,
we propose a Laplacian pyramid for high-frequency texture sampling from UV maps
deformed by dynamic flow of expression for motion-aware texture prior
integration to provide torso features to enhance StyleGAN's ability to generate
complete and upper body for portrait video rendering. Emo-Avatar reduces style
customization time from hours to merely 5 minutes compared with existing
methods. In addition, Emo-Avatar requires only a single reference image for
editing and employs region-aware contrastive learning with semantic invariant
CLIP guidance, ensuring consistent high-resolution output and identity
preservation. Through both quantitative and qualitative assessments, Emo-Avatar
demonstrates superior performance over existing methods in terms of training
efficiency, rendering quality and editability in self- and cross-reenactment.



---

## ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Jiahua Dong, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2402.00864v1){: .btn .btn-green } |

**Abstract**: We introduce ViCA-NeRF, the first view-consistency-aware method for 3D
editing with text instructions. In addition to the implicit neural radiance
field (NeRF) modeling, our key insight is to exploit two sources of
regularization that explicitly propagate the editing information across
different views, thus ensuring multi-view consistency. For geometric
regularization, we leverage the depth information derived from NeRF to
establish image correspondences between different views. For learned
regularization, we align the latent codes in the 2D diffusion model between
edited and unedited images, enabling us to edit key views and propagate the
update throughout the entire scene. Incorporating these two strategies, our
ViCA-NeRF operates in two stages. In the initial stage, we blend edits from
different views to create a preliminary 3D edit. This is followed by a second
stage of NeRF training, dedicated to further refining the scene's appearance.
Experimental results demonstrate that ViCA-NeRF provides more flexible,
efficient (3 times faster) editing with higher levels of consistency and
details, compared with the state of the art. Our code is publicly available.

Comments:
- Neurips2023; project page: https://github.com/Dongjiahua/VICA-NeRF

---

## StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2402.00525v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has emerged as a prominent model for constructing 3D
representations from images across diverse domains. However, the efficiency of
the 3D Gaussian Splatting rendering pipeline relies on several simplifications.
Notably, reducing Gaussian to 2D splats with a single view-space depth
introduces popping and blending artifacts during view rotation. Addressing this
issue requires accurate per-pixel depth computation, yet a full per-pixel sort
proves excessively costly compared to a global sort operation. In this paper,
we present a novel hierarchical rasterization approach that systematically
resorts and culls splats with minimal processing overhead. Our software
rasterizer effectively eliminates popping artifacts and view inconsistencies,
as demonstrated through both quantitative and qualitative measurements.
Simultaneously, our method mitigates the potential for cheating view-dependent
effects with popping, ensuring a more authentic representation. Despite the
elimination of cheating, our approach achieves comparable quantitative results
for test images, while increasing the consistency for novel view synthesis in
motion. Due to its design, our hierarchical approach is only 4% slower on
average than the original Gaussian Splatting. Notably, enforcing consistency
enables a reduction in the number of Gaussians by approximately half with
nearly identical quality and view-consistency. Consequently, rendering
performance is nearly doubled, making our approach 1.6x faster than the
original Gaussian Splatting, with a 50% reduction in memory requirements.

Comments:
- Video: https://youtu.be/RJQlSORNkr0

---

## Optimal Projection for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2402.00752v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has garnered extensive attention and application in
real-time neural rendering. Concurrently, concerns have been raised about the
limitations of this technology in aspects such as point cloud storage,
performance , and robustness in sparse viewpoints , leading to various
improvements. However, there has been a notable lack of attention to the
projection errors introduced by the local affine approximation inherent in the
splatting itself, and the consequential impact of these errors on the quality
of photo-realistic rendering. This paper addresses the projection error
function of 3D Gaussian Splatting, commencing with the residual error from the
first-order Taylor expansion of the projection function $\phi$. The analysis
establishes a correlation between the error and the Gaussian mean position.
Subsequently, leveraging function optimization theory, this paper analyzes the
function's minima to provide an optimal projection strategy for Gaussian
Splatting referred to Optimal Gaussian Splatting. Experimental validation
further confirms that this projection methodology reduces artifacts, resulting
in a more convincingly realistic rendering.



---

## 360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2402.00763v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) has recently attracted great attention with
real-time and photo-realistic renderings. This technique typically takes
perspective images as input and optimizes a set of 3D elliptical Gaussians by
splatting them onto the image planes, resulting in 2D Gaussians. However,
applying 3D-GS to panoramic inputs presents challenges in effectively modeling
the projection onto the spherical surface of ${360^\circ}$ images using 2D
Gaussians. In practical applications, input panoramas are often sparse, leading
to unreliable initialization of 3D Gaussians and subsequent degradation of
3D-GS quality. In addition, due to the under-constrained geometry of
texture-less planes (e.g., walls and floors), 3D-GS struggles to model these
flat regions with elliptical Gaussians, resulting in significant floaters in
novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$
Gaussian splatting for a limited set of panoramic inputs. Instead of splatting
3D Gaussians directly onto the spherical surface, 360-GS projects them onto the
tangent plane of the unit sphere and then maps them to the spherical
projections. This adaptation enables the representation of the projection using
Gaussians. We guide the optimization of 360-GS by exploiting layout priors
within panoramas, which are simple to obtain and contain strong structural
information about the indoor scene. Our experimental results demonstrate that
360-GS allows panoramic rendering and outperforms state-of-the-art methods with
fewer artifacts in novel view synthesis, thus providing immersive roaming in
indoor scenarios.

Comments:
- 11 pages, 10 figures
