---
layout: default
title: February 2024
parent: Papers
nav_order: 202402
---

<!---metadata--->


## ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Jiahua Dong, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2402.00864v1){: .btn .btn-green } |

**Abstract**: We introduce ViCA-NeRF, the first view-consistency-aware method for 3D
editing with text instructions. In addition to the implicit neural radiance
field (NeRF) modeling, our key insight is to exploit two sources of
regularization that explicitly propagate the editing information across
different views, thus ensuring multi-view consistency. For geometric
regularization, we leverage the depth information derived from NeRF to
establish image correspondences between different views. For learned
regularization, we align the latent codes in the 2D diffusion model between
edited and unedited images, enabling us to edit key views and propagate the
update throughout the entire scene. Incorporating these two strategies, our
ViCA-NeRF operates in two stages. In the initial stage, we blend edits from
different views to create a preliminary 3D edit. This is followed by a second
stage of NeRF training, dedicated to further refining the scene's appearance.
Experimental results demonstrate that ViCA-NeRF provides more flexible,
efficient (3 times faster) editing with higher levels of consistency and
details, compared with the state of the art. Our code is publicly available.

Comments:
- Neurips2023; project page: https://github.com/Dongjiahua/VICA-NeRF

---

## GS++: Error Analyzing and Optimal Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2402.00752v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has garnered extensive attention and application in
real-time neural rendering. Concurrently, concerns have been raised about the
limitations of this technology in aspects such as point cloud storage,
performance , and robustness in sparse viewpoints , leading to various
improvements. However, there has been a notable lack of attention to the
projection errors introduced by the local affine approximation inherent in the
splatting itself, and the consequential impact of these errors on the quality
of photo-realistic rendering. This paper addresses the projection error
function of 3D Gaussian Splatting, commencing with the residual error from the
first-order Taylor expansion of the projection function $\phi$. The analysis
establishes a correlation between the error and the Gaussian mean position.
Subsequently, leveraging function optimization theory, this paper analyzes the
function's minima to provide an optimal projection strategy for Gaussian
Splatting referred to Optimal Gaussian Splatting. Experimental validation
further confirms that this projection methodology reduces artifacts, resulting
in a more convincingly realistic rendering.



---

## StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2402.00525v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has emerged as a prominent model for constructing 3D
representations from images across diverse domains. However, the efficiency of
the 3D Gaussian Splatting rendering pipeline relies on several simplifications.
Notably, reducing Gaussian to 2D splats with a single view-space depth
introduces popping and blending artifacts during view rotation. Addressing this
issue requires accurate per-pixel depth computation, yet a full per-pixel sort
proves excessively costly compared to a global sort operation. In this paper,
we present a novel hierarchical rasterization approach that systematically
resorts and culls splats with minimal processing overhead. Our software
rasterizer effectively eliminates popping artifacts and view inconsistencies,
as demonstrated through both quantitative and qualitative measurements.
Simultaneously, our method mitigates the potential for cheating view-dependent
effects with popping, ensuring a more authentic representation. Despite the
elimination of cheating, our approach achieves comparable quantitative results
for test images, while increasing the consistency for novel view synthesis in
motion. Due to its design, our hierarchical approach is only 4% slower on
average than the original Gaussian Splatting. Notably, enforcing consistency
enables a reduction in the number of Gaussians by approximately half with
nearly identical quality and view-consistency. Consequently, rendering
performance is nearly doubled, making our approach 1.6x faster than the
original Gaussian Splatting, with a 50% reduction in memory requirements.

Comments:
- Video: https://youtu.be/RJQlSORNkr0

---

## Emo-Avatar: Efficient Monocular Video Style Avatar through Texture  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Pinxin Liu, Luchuan Song, Daoan Zhang, Hang Hua, Yunlong Tang, Huaijin Tu, Jiebo Luo, Chenliang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2402.00827v1){: .btn .btn-green } |

**Abstract**: Artistic video portrait generation is a significant and sought-after task in
the fields of computer graphics and vision. While various methods have been
developed that integrate NeRFs or StyleGANs with instructional editing models
for creating and editing drivable portraits, these approaches face several
challenges. They often rely heavily on large datasets, require extensive
customization processes, and frequently result in reduced image quality. To
address the above problems, we propose the Efficient Monotonic Video Style
Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's
capacity for producing dynamic, drivable portrait videos. We proposed a
two-stage deferred neural rendering pipeline. In the first stage, we utilize
few-shot PTI initialization to initialize the StyleGAN generator through
several extreme poses sampled from the video to capture the consistent
representation of aligned faces from the target portrait. In the second stage,
we propose a Laplacian pyramid for high-frequency texture sampling from UV maps
deformed by dynamic flow of expression for motion-aware texture prior
integration to provide torso features to enhance StyleGAN's ability to generate
complete and upper body for portrait video rendering. Emo-Avatar reduces style
customization time from hours to merely 5 minutes compared with existing
methods. In addition, Emo-Avatar requires only a single reference image for
editing and employs region-aware contrastive learning with semantic invariant
CLIP guidance, ensuring consistent high-resolution output and identity
preservation. Through both quantitative and qualitative assessments, Emo-Avatar
demonstrates superior performance over existing methods in terms of training
efficiency, rendering quality and editability in self- and cross-reenactment.



---

## 360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2402.00763v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) has recently attracted great attention with
real-time and photo-realistic renderings. This technique typically takes
perspective images as input and optimizes a set of 3D elliptical Gaussians by
splatting them onto the image planes, resulting in 2D Gaussians. However,
applying 3D-GS to panoramic inputs presents challenges in effectively modeling
the projection onto the spherical surface of ${360^\circ}$ images using 2D
Gaussians. In practical applications, input panoramas are often sparse, leading
to unreliable initialization of 3D Gaussians and subsequent degradation of
3D-GS quality. In addition, due to the under-constrained geometry of
texture-less planes (e.g., walls and floors), 3D-GS struggles to model these
flat regions with elliptical Gaussians, resulting in significant floaters in
novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$
Gaussian splatting for a limited set of panoramic inputs. Instead of splatting
3D Gaussians directly onto the spherical surface, 360-GS projects them onto the
tangent plane of the unit sphere and then maps them to the spherical
projections. This adaptation enables the representation of the projection using
Gaussians. We guide the optimization of 360-GS by exploiting layout priors
within panoramas, which are simple to obtain and contain strong structural
information about the indoor scene. Our experimental results demonstrate that
360-GS allows panoramic rendering and outperforms state-of-the-art methods with
fewer artifacts in novel view synthesis, thus providing immersive roaming in
indoor scenarios.

Comments:
- 11 pages, 10 figures
