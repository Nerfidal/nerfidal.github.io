---
layout: default
title: February 2024
parent: Papers
nav_order: 202402
---

<!---metadata--->


## DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable  Object Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-12 | Chenchang Li, Zihao Ai, Tong Wu, Xiaosa Li, Wenbo Ding, Huazhe Xu | cs.RO | [PDF](http://arxiv.org/pdf/2402.07648v1){: .btn .btn-green } |

**Abstract**: Manipulating deformable objects is a ubiquitous task in household
environments, demanding adequate representation and accurate dynamics
prediction due to the objects' infinite degrees of freedom. This work proposes
DeformNet, which utilizes latent space modeling with a learned 3D
representation model to tackle these challenges effectively. The proposed
representation model combines a PointNet encoder and a conditional neural
radiance field (NeRF), facilitating a thorough acquisition of object
deformations and variations in lighting conditions. To model the complex
dynamics, we employ a recurrent state-space model (RSSM) that accurately
predicts the transformation of the latent representation over time. Extensive
simulation experiments with diverse objectives demonstrate the generalization
capabilities of DeformNet for various deformable object manipulation tasks,
even in the presence of previously unseen goals. Finally, we deploy DeformNet
on an actual UR5 robotic arm to demonstrate its capability in real-world
scenarios.

Comments:
- 7 pages, Submitted to 2024 IEEE International Conference on Robotics
  and Automation (ICRA), Japan, Yokohama

---

## BioNeRF: Biologically Plausible Neural Radiance Fields for View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-11 | Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, João Paulo Papa | cs.CV | [PDF](http://arxiv.org/pdf/2402.07310v1){: .btn .btn-green } |

**Abstract**: This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.



---

## GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided  Generative Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-11 | Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2402.07207v1){: .btn .btn-green } |

**Abstract**: We present GALA3D, generative 3D GAussians with LAyout-guided control, for
effective compositional text-to-3D generation. We first utilize large language
models (LLMs) to generate the initial layout and introduce a layout-guided 3D
Gaussian representation for 3D content generation with adaptive geometric
constraints. We then propose an object-scene compositional optimization
mechanism with conditioned diffusion to collaboratively generate realistic 3D
scenes with consistent geometry, texture, scale, and accurate interactions
among multiple objects while simultaneously adjusting the coarse layout priors
extracted from the LLMs to align with the generated scene. Experiments show
that GALA3D is a user-friendly, end-to-end framework for state-of-the-art
scene-level 3D content generation and controllable editing while ensuring the
high fidelity of object-level entities within the scene. Source codes and
models will be available at https://gala3d.github.io/.



---

## 3D Gaussian as a New Vision Era: A Survey

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-11 | Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2402.07181v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the
field of Computer Graphics, offering explicit scene representation and novel
view synthesis without the reliance on neural networks, such as Neural Radiance
Fields (NeRF). This technique has found diverse applications in areas such as
robotics, urban mapping, autonomous navigation, and virtual reality/augmented
reality, just name a few. Given the growing popularity and expanding research
in 3D Gaussian Splatting, this paper presents a comprehensive survey of
relevant papers from the past year. We organize the survey into taxonomies
based on characteristics and applications, providing an introduction to the
theoretical underpinnings of 3D Gaussian Splatting. Our goal through this
survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a
valuable reference for seminal works in the field, and inspire future research
directions, as discussed in our concluding section.



---

## ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake  Generation using NeRF and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-09 | Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2402.06390v1){: .btn .btn-green } |

**Abstract**: Numerous emerging deep-learning techniques have had a substantial impact on
computer graphics. Among the most promising breakthroughs are the recent rise
of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the
object's shape and color in neural network weights using a handful of images
with known camera positions to generate novel views. In contrast, GS provides
accelerated training and inference without a decrease in rendering quality by
encoding the object's characteristics in a collection of Gaussian
distributions. These two techniques have found many use cases in spatial
computing and other domains. On the other hand, the emergence of deepfake
methods has sparked considerable controversy. Such techniques can have a form
of artificial intelligence-generated videos that closely mimic authentic
footage. Using generative models, they can modify facial features, enabling the
creation of altered identities or facial expressions that exhibit a remarkably
realistic appearance to a real person. Despite these controversies, deepfake
can offer a next-generation solution for avatar creation and gaming when of
desirable quality. To that end, we show how to combine all these emerging
technologies to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the
classical deepfake algorithm to modify all training images separately and then
train NeRF and GS on modified faces. Such relatively simple strategies can
produce plausible 3D deepfake-based avatars.



---

## HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-09 | Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2402.06149v1){: .btn .btn-green } |

**Abstract**: Creating digital avatars from textual prompts has long been a desirable yet
challenging task. Despite the promising outcomes obtained through 2D diffusion
priors in recent works, current methods face challenges in achieving
high-quality and animated avatars effectively. In this paper, we present
$\textbf{HeadStudio}$, a novel framework that utilizes 3D Gaussian splatting to
generate realistic and animated avatars from text prompts. Our method drives 3D
Gaussians semantically to create a flexible and achievable appearance through
the intermediate FLAME representation. Specifically, we incorporate the FLAME
into both 3D representation and score distillation: 1) FLAME-based 3D Gaussian
splatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2)
FLAME-based score distillation sampling, utilizing FLAME-based fine-grained
control signal to guide score distillation from the text prompt. Extensive
experiments demonstrate the efficacy of HeadStudio in generating animatable
avatars from textual prompts, exhibiting visually appealing appearances. The
avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel
views at a resolution of 1024. They can be smoothly controlled by real-world
speech and video. We hope that HeadStudio can advance digital avatar creation
and that the present method can widely be applied across various domains.

Comments:
- 9 pages, 8 figures

---

## GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D  Pretraining from Real-World Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-09 | Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang | cs.CV | [PDF](http://arxiv.org/pdf/2402.06198v1){: .btn .btn-green } |

**Abstract**: 3D Shape represented as point cloud has achieve advancements in multimodal
pre-training to align image and language descriptions, which is curial to
object identification, classification, and retrieval. However, the discrete
representations of point cloud lost the object's surface shape information and
creates a gap between rendering results and 2D correspondences. To address this
problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D
Gaussian Splatting) into multimodal pre-training to enhance 3D representation.
GS-CLIP leverages a pre-trained vision-language model for a learned common
visual and textual space on massive real world image-text pairs and then learns
a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel
Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature.
As a general framework for language-image-3D pre-training, GS-CLIP is agnostic
to 3D backbone networks. Experiments on challenging shows that GS-CLIP
significantly improves the state-of-the-art, outperforming the previously best
results.

Comments:
- 6-page technical report

---

## Mesh-based Gaussian Splatting for Real-time Large-scale Deformation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-07 | Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai | cs.GR | [PDF](http://arxiv.org/pdf/2402.04796v1){: .btn .btn-green } |

**Abstract**: Neural implicit representations, including Neural Distance Fields and Neural
Radiance Fields, have demonstrated significant capabilities for reconstructing
surfaces with complicated geometry and topology, and generating novel views of
a scene. Nevertheless, it is challenging for users to directly deform or
manipulate these implicit representations with large deformations in the
real-time fashion. Gaussian Splatting(GS) has recently become a promising
method with explicit geometry for representing static scenes and facilitating
high-quality and real-time synthesis of novel views. However,it cannot be
easily deformed due to the use of discrete Gaussians and lack of explicit
topology. To address this, we develop a novel GS-based method that enables
interactive deformation. Our key idea is to design an innovative mesh-based GS
representation, which is integrated into Gaussian learning and manipulation. 3D
Gaussians are defined over an explicit mesh, and they are bound with each
other: the rendering of 3D Gaussians guides the mesh face split for adaptive
refinement, and the mesh face split directs the splitting of 3D Gaussians.
Moreover, the explicit mesh constraints help regularize the Gaussian
distribution, suppressing poor-quality Gaussians(e.g. misaligned
Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and
avoiding artifacts during deformation. Based on this representation, we further
introduce a large-scale Gaussian deformation technique to enable deformable GS,
which alters the parameters of 3D Gaussians according to the manipulation of
the associated mesh. Our method benefits from existing mesh deformation
datasets for more realistic data-driven Gaussian deformation. Extensive
experiments show that our approach achieves high-quality reconstruction and
effective deformation, while maintaining the promising rendering results at a
high frame rate(65 FPS on average).

Comments:
- 11 pages, 7 figures

---

## OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language  Foundation Models for 3D Semantic Understanding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-07 | Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li | cs.CV | [PDF](http://arxiv.org/pdf/2402.04648v1){: .btn .btn-green } |

**Abstract**: The development of Neural Radiance Fields (NeRFs) has provided a potent
representation for encapsulating the geometric and appearance characteristics
of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D
semantic perception tasks has been a recent focus. However, current methods
that extract semantics directly from Contrastive Language-Image Pretraining
(CLIP) for semantic field learning encounter difficulties due to noisy and
view-inconsistent semantics provided by CLIP. To tackle these limitations, we
propose OV-NeRF, which exploits the potential of pre-trained vision and
language foundation models to enhance semantic field learning through proposed
single-view and cross-view strategies. First, from the single-view perspective,
we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask
proposals derived from SAM to rectify the noisy semantics of each training
view, facilitating accurate semantic field learning. Second, from the
cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy
to address the challenge raised by view-inconsistent semantics. Rather than
invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the
3D consistent semantics generated from the well-trained semantic field itself
for semantic field training, aiming to reduce ambiguity and enhance overall
semantic consistency across different views. Extensive experiments validate our
OV-NeRF outperforms current state-of-the-art methods, achieving a significant
improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet,
respectively. Furthermore, our approach exhibits consistent superior results
across various CLIP configurations, further verifying its robustness.



---

## BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial  Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-07 | Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao | cs.CV | [PDF](http://arxiv.org/pdf/2402.04554v2){: .btn .btn-green } |

**Abstract**: In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields
(NeRF) designed specifically for reconstructing large-scale scenes using aerial
imagery. Unlike previous research focused on small-scale and object-centric
NeRF reconstruction, our approach addresses multiple challenges, including (1)
Addressing the issue of slow training and rendering associated with large
models. (2) Meeting the computational demands necessitated by modeling a
substantial number of images, requiring extensive resources such as
high-performance GPUs. (3) Overcoming significant artifacts and low visual
fidelity commonly observed in large-scale reconstruction tasks due to limited
model capacity. Specifically, we present a novel bird-view pose-based spatial
decomposition algorithm that decomposes a large aerial image set into multiple
small sets with appropriately sized overlaps, allowing us to train individual
NeRFs of sub-scene. This decomposition approach not only decouples rendering
time from the scene size but also enables rendering to scale seamlessly to
arbitrarily large environments. Moreover, it allows for per-block updates of
the environment, enhancing the flexibility and adaptability of the
reconstruction process. Additionally, we propose a projection-guided novel view
re-rendering strategy, which aids in effectively utilizing the independently
trained sub-scenes to generate superior rendering results. We evaluate our
approach on existing datasets as well as against our own drone footage,
improving reconstruction speed by 10x over classical photogrammetry software
and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with
similar rendering quality.



---

## NeRF as Non-Distant Environment Emitter in Physics-based Inverse  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-07 | Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2402.04829v1){: .btn .btn-green } |

**Abstract**: Physics-based inverse rendering aims to jointly optimize shape, materials,
and lighting from captured 2D images. Here lighting is an important part of
achieving faithful light transport simulation. While the environment map is
commonly used as the lighting model in inverse rendering, we show that its
distant lighting assumption leads to spatial invariant lighting, which can be
an inaccurate approximation in real-world inverse rendering. We propose to use
NeRF as a spatially varying environment lighting model and build an inverse
rendering pipeline using NeRF as the non-distant environment emitter. By
comparing our method with the environment map on real and synthetic datasets,
we show that our NeRF-based emitter models the scene lighting more accurately
and leads to more accurate inverse rendering. Project page and video:
https://nerfemitterpbir.github.io/.

Comments:
- Project page and video: https://nerfemitterpbir.github.io/

---

## Improved Generalization of Weight Space Networks via Augmentations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-06 | Aviv Shamsian, Aviv Navon, David W. Zhang, Yan Zhang, Ethan Fetaya, Gal Chechik, Haggai Maron | cs.LG | [PDF](http://arxiv.org/pdf/2402.04081v1){: .btn .btn-green } |

**Abstract**: Learning in deep weight spaces (DWS), where neural networks process the
weights of other neural networks, is an emerging research direction, with
applications to 2D and 3D neural fields (INRs, NeRFs), as well as making
inferences about other types of neural networks. Unfortunately, weight space
models tend to suffer from substantial overfitting. We empirically analyze the
reasons for this overfitting and find that a key reason is the lack of
diversity in DWS datasets. While a given object can be represented by many
different weight configurations, typical INR training sets fail to capture
variability across INRs that represent the same object. To address this, we
explore strategies for data augmentation in weight spaces and propose a MixUp
method adapted for weight spaces. We demonstrate the effectiveness of these
methods in two setups. In classification, they improve performance similarly to
having up to 10 times more data. In self-supervised contrastive learning, they
yield substantial 5-10% gains in downstream classification.

Comments:
- Under Review

---

## Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-06 | Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras | cs.CV | [PDF](http://arxiv.org/pdf/2402.03723v1){: .btn .btn-green } |

**Abstract**: Creating controllable 3D human portraits from casual smartphone videos is
highly desirable due to their immense value in AR/VR applications. The recent
development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering
quality and training efficiency. However, it still remains a challenge to
accurately model and disentangle head movements and facial expressions from a
single-view capture to achieve high-quality renderings. In this paper, we
introduce Rig3DGS to address this challenge. We represent the entire scene,
including the dynamic subject, using a set of 3D Gaussians in a canonical
space. Using a set of control signals, such as head pose and expressions, we
transform them to the 3D space with learned deformations to generate the
desired rendering. Our key innovation is a carefully designed deformation
method which is guided by a learnable prior derived from a 3D morphable model.
This approach is highly efficient in training and effective in controlling
facial expressions, head positions, and view synthesis across various captures.
We demonstrate the effectiveness of our learned deformation through extensive
quantitative and qualitative experiments. The project page can be found at
http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html



---

## ViewFusion: Learning Composable Diffusion Models for Novel View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-05 | Bernard Spiegl, Andrea Perin, Stéphane Deny, Alexander Ilin | cs.CV | [PDF](http://arxiv.org/pdf/2402.02906v1){: .btn .btn-green } |

**Abstract**: Deep learning is providing a wealth of new approaches to the old problem of
novel view synthesis, from Neural Radiance Field (NeRF) based approaches to
end-to-end style architectures. Each approach offers specific strengths but
also comes with specific limitations in their applicability. This work
introduces ViewFusion, a state-of-the-art end-to-end generative approach to
novel view synthesis with unparalleled flexibility. ViewFusion consists in
simultaneously applying a diffusion denoising step to any number of input views
of a scene, then combining the noise gradients obtained for each view with an
(inferred) pixel-weighting mask, ensuring that for each region of the target
scene only the most informative input views are taken into account. Our
approach resolves several limitations of previous approaches by (1) being
trainable and generalizing across multiple scenes and object classes, (2)
adaptively taking in a variable number of pose-free views at both train and
test time, (3) generating plausible views even in severely undetermined
conditions (thanks to its generative nature) -- all while generating views of
quality on par or even better than state-of-the-art methods. Limitations
include not generating a 3D embedding of the scene, resulting in a relatively
slow inference speed, and our method only being tested on the relatively small
dataset NMR. Code is available.



---

## 4D Gaussian Splatting: Towards Efficient Novel View Synthesis for  Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-05 | Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen | cs.CV | [PDF](http://arxiv.org/pdf/2402.03307v2){: .btn .btn-green } |

**Abstract**: We consider the problem of novel view synthesis (NVS) for dynamic scenes.
Recent neural approaches have accomplished exceptional NVS results for static
3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior
efforts often encode dynamics by learning a canonical space plus implicit or
explicit deformation fields, which struggle in challenging scenarios like
sudden movements or capturing high-fidelity renderings. In this paper, we
introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic
scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D
Gaussian Splatting in static scenes. We model dynamics at each timestamp by
temporally slicing the 4D Gaussians, which naturally compose dynamic 3D
Gaussians and can be seamlessly projected into images. As an explicit
spatial-temporal representation, 4DGS demonstrates powerful capabilities for
modeling complicated dynamics and fine details, especially for scenes with
abrupt motions. We further implement our temporal slicing and splatting
techniques in a highly optimized CUDA acceleration framework, achieving
real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and
583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions
showcase the superior efficiency and effectiveness of 4DGS, which consistently
outperforms existing methods both quantitatively and qualitatively.



---

## SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-05 | Mingrui Li, Shuhong Liu, Heng Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2402.03246v1){: .btn .btn-green } |

**Abstract**: Semantic understanding plays a crucial role in Dense Simultaneous
Localization and Mapping (SLAM), facilitating comprehensive scene
interpretation. Recent advancements that integrate Gaussian Splatting into SLAM
systems have demonstrated its effectiveness in generating high-quality
renderings through the use of explicit 3D Gaussian representations. Building on
this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system
grounded in 3D Gaussians, which provides precise 3D semantic segmentation
alongside high-fidelity reconstructions. Specifically, we propose to employ
multi-channel optimization during the mapping process, integrating appearance,
geometric, and semantic constraints with key-frame optimization to enhance
reconstruction quality. Extensive experiments demonstrate that SGS-SLAM
delivers state-of-the-art performance in camera pose estimation, map
reconstruction, and semantic segmentation, outperforming existing methods
meanwhile preserving real-time rendering ability.



---

## Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect  Diffusion Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Yaokun Li, Chao Gou, Guang Tan | cs.CV | [PDF](http://arxiv.org/pdf/2402.01217v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing
novel views. However, their reliance on dense inputs and scene-specific
optimization has limited their broader applicability. Generalizable NeRFs
(Gen-NeRF), while intended to address this, often produce blurring artifacts in
unobserved regions with sparse inputs, which are full of uncertainty. In this
paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings.
We assume that NeRF's inability to effectively mitigate this uncertainty stems
from its inherent lack of generative capacity. Therefore, we innovatively
propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address
this uncertainty from a generative perspective by leveraging a distilled
diffusion prior as guidance. Specifically, to avoid model confusion caused by
directly regularizing with inconsistent samplings as in previous methods, our
approach introduces a strategy to indirectly inject the inherently missing
imagination into the learned implicit function through a diffusion-guided
latent space. Empirical evaluation across various benchmarks demonstrates the
superior performance of our approach in handling uncertainty with sparse
inputs.



---

## GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2402.01459v2){: .btn .btn-green } |

**Abstract**: In recent years, a range of neural network-based methods for image rendering
have been introduced. For instance, widely-researched neural radiance fields
(NeRF) rely on a neural network to represent 3D scenes, allowing for realistic
view synthesis from a small number of 2D images. However, most NeRF models are
constrained by long training and inference times. In comparison, Gaussian
Splatting (GS) is a novel, state-of-theart technique for rendering points in a
3D scene by approximating their contribution to image pixels through Gaussian
distributions, warranting fast training and swift, real-time rendering. A
drawback of GS is the absence of a well-defined approach for its conditioning
due to the necessity to condition several hundred thousand Gaussian components.
To solve this, we introduce Gaussian Mesh Splatting (GaMeS) model, a hybrid of
mesh and a Gaussian distribution, that pin all Gaussians splats on the object
surface (mesh). The unique contribution of our methods is defining Gaussian
splats solely based on their location on the mesh, allowing for automatic
adjustments in position, scale, and rotation during animation. As a result, we
obtain high-quality renders in the real-time generation of high-quality views.
Furthermore, we demonstrate that in the absence of a predefined mesh, it is
possible to fine-tune the initial mesh during the learning process.



---

## Di-NeRF: Distributed NeRF for Collaborative Learning with Unknown  Relative Poses

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Mahboubeh Asadi, Kourosh Zareinia, Sajad Saeedi | cs.RO | [PDF](http://arxiv.org/pdf/2402.01485v1){: .btn .btn-green } |

**Abstract**: Collaborative mapping of unknown environments can be done faster and more
robustly than a single robot. However, a collaborative approach requires a
distributed paradigm to be scalable and deal with communication issues. This
work presents a fully distributed algorithm enabling a group of robots to
collectively optimize the parameters of a Neural Radiance Field (NeRF). The
algorithm involves the communication of each robot's trained NeRF parameters
over a mesh network, where each robot trains its NeRF and has access to its own
visual data only. Additionally, the relative poses of all robots are jointly
optimized alongside the model parameters, enabling mapping with unknown
relative camera poses. We show that multi-robot systems can benefit from
differentiable and robust 3D reconstruction optimized from multiple NeRFs.
Experiments on real-world and synthetic data demonstrate the efficiency of the
proposed algorithm. See the website of the project for videos of the
experiments and supplementary
material(https://sites.google.com/view/di-nerf/home).

Comments:
- 9 pages, 11 figures, Submitted to IEEE-RA-L

---

## ConRF: Zero-shot Stylization of 3D Scenes with Conditioned Radiation  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Xingyu Miao, Yang Bai, Haoran Duan, Fan Wan, Yawen Huang, Yang Long, Yefeng Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2402.01950v1){: .btn .btn-green } |

**Abstract**: Most of the existing works on arbitrary 3D NeRF style transfer required
retraining on each single style condition. This work aims to achieve zero-shot
controlled stylization in 3D scenes utilizing text or visual input as
conditioning factors. We introduce ConRF, a novel method of zero-shot
stylization. Specifically, due to the ambiguity of CLIP features, we employ a
conversion process that maps the CLIP feature space to the style space of a
pre-trained VGG network and then refine the CLIP multi-modal knowledge into a
style transfer neural radiation field. Additionally, we use a 3D volumetric
representation to perform local style transfer. By combining these operations,
ConRF offers the capability to utilize either text or images as references,
resulting in the generation of sequences with novel views enhanced by global or
local stylization. Our experiment demonstrates that ConRF outperforms other
existing methods for 3D scene and single-text stylization in terms of visual
quality.



---

## Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate  Distortion Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Zhiyu Zhang, Guo Lu, Huanxiong Liang, Anni Tang, Qiang Hu, Li Song | cs.CV | [PDF](http://arxiv.org/pdf/2402.01380v1){: .btn .btn-green } |

**Abstract**: Volumetric videos, benefiting from immersive 3D realism and interactivity,
hold vast potential for various applications, while the tremendous data volume
poses significant challenges for compression. Recently, NeRF has demonstrated
remarkable potential in volumetric video compression thanks to its simple
representation and powerful 3D modeling capabilities, where a notable work is
ReRF. However, ReRF separates the modeling from compression process, resulting
in suboptimal compression efficiency. In contrast, in this paper, we propose a
volumetric video compression method based on dynamic NeRF in a more compact
manner. Specifically, we decompose the NeRF representation into the coefficient
fields and the basis fields, incrementally updating the basis fields in the
temporal domain to achieve dynamic modeling. Additionally, we perform
end-to-end joint optimization on the modeling and compression process to
further improve the compression efficiency. Extensive experiments demonstrate
that our method achieves higher compression efficiency compared to ReRF on
various datasets.



---

## HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Paweł Batorski, Dawid Malarz, Marcin Przewięźlikowski, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2402.01524v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) are a widely accepted standard for
synthesizing new 3D object views from a small number of base images. However,
NeRFs have limited generalization properties, which means that we need to use
significant computational resources to train individual architectures for each
item we want to represent. To address this issue, we propose a few-shot
learning approach based on the hypernetwork paradigm that does not require
gradient optimization during inference. The hypernetwork gathers information
from the training data and generates an update for universal weights. As a
result, we have developed an efficient method for generating a high-quality 3D
object representation from a small number of images in a single step. This has
been confirmed by direct comparison with the state-of-the-art solutions and a
comprehensive ablation study.



---

## Robust Inverse Graphics via Probabilistic Inference

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-02 | Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous | cs.CV | [PDF](http://arxiv.org/pdf/2402.01915v1){: .btn .btn-green } |

**Abstract**: How do we infer a 3D scene from a single image in the presence of corruptions
like rain, snow or fog? Straightforward domain randomization relies on knowing
the family of corruptions ahead of time. Here, we propose a Bayesian
approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene
prior and an uninformative uniform corruption prior, making it applicable to a
wide range of corruptions. Given a single image, RIG performs posterior
inference jointly over the scene and the corruption. We demonstrate this idea
by training a neural radiance field (NeRF) scene prior and using a secondary
NeRF to represent the corruptions over which we place an uninformative prior.
RIG, trained only on clean data, outperforms depth estimators and alternative
NeRF approaches that perform point estimation instead of full inference. The
results hold for a number of scene prior architectures based on normalizing
flows and diffusion models. For the latter, we develop reconstruction-guidance
with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is
applicable in the presence of auxiliary latent variables such as the
corruption. RIG demonstrates how scene priors can be used beyond generation
tasks.



---

## StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger | cs.GR | [PDF](http://arxiv.org/pdf/2402.00525v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has emerged as a prominent model for constructing 3D
representations from images across diverse domains. However, the efficiency of
the 3D Gaussian Splatting rendering pipeline relies on several simplifications.
Notably, reducing Gaussian to 2D splats with a single view-space depth
introduces popping and blending artifacts during view rotation. Addressing this
issue requires accurate per-pixel depth computation, yet a full per-pixel sort
proves excessively costly compared to a global sort operation. In this paper,
we present a novel hierarchical rasterization approach that systematically
resorts and culls splats with minimal processing overhead. Our software
rasterizer effectively eliminates popping artifacts and view inconsistencies,
as demonstrated through both quantitative and qualitative measurements.
Simultaneously, our method mitigates the potential for cheating view-dependent
effects with popping, ensuring a more authentic representation. Despite the
elimination of cheating, our approach achieves comparable quantitative results
for test images, while increasing the consistency for novel view synthesis in
motion. Due to its design, our hierarchical approach is only 4% slower on
average than the original Gaussian Splatting. Notably, enforcing consistency
enables a reduction in the number of Gaussians by approximately half with
nearly identical quality and view-consistency. Consequently, rendering
performance is nearly doubled, making our approach 1.6x faster than the
original Gaussian Splatting, with a 50% reduction in memory requirements.

Comments:
- Video: https://youtu.be/RJQlSORNkr0

---

## ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Jiahua Dong, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2402.00864v1){: .btn .btn-green } |

**Abstract**: We introduce ViCA-NeRF, the first view-consistency-aware method for 3D
editing with text instructions. In addition to the implicit neural radiance
field (NeRF) modeling, our key insight is to exploit two sources of
regularization that explicitly propagate the editing information across
different views, thus ensuring multi-view consistency. For geometric
regularization, we leverage the depth information derived from NeRF to
establish image correspondences between different views. For learned
regularization, we align the latent codes in the 2D diffusion model between
edited and unedited images, enabling us to edit key views and propagate the
update throughout the entire scene. Incorporating these two strategies, our
ViCA-NeRF operates in two stages. In the initial stage, we blend edits from
different views to create a preliminary 3D edit. This is followed by a second
stage of NeRF training, dedicated to further refining the scene's appearance.
Experimental results demonstrate that ViCA-NeRF provides more flexible,
efficient (3 times faster) editing with higher levels of consistency and
details, compared with the state of the art. Our code is publicly available.

Comments:
- Neurips2023; project page: https://github.com/Dongjiahua/VICA-NeRF

---

## Emo-Avatar: Efficient Monocular Video Style Avatar through Texture  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Pinxin Liu, Luchuan Song, Daoan Zhang, Hang Hua, Yunlong Tang, Huaijin Tu, Jiebo Luo, Chenliang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2402.00827v1){: .btn .btn-green } |

**Abstract**: Artistic video portrait generation is a significant and sought-after task in
the fields of computer graphics and vision. While various methods have been
developed that integrate NeRFs or StyleGANs with instructional editing models
for creating and editing drivable portraits, these approaches face several
challenges. They often rely heavily on large datasets, require extensive
customization processes, and frequently result in reduced image quality. To
address the above problems, we propose the Efficient Monotonic Video Style
Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's
capacity for producing dynamic, drivable portrait videos. We proposed a
two-stage deferred neural rendering pipeline. In the first stage, we utilize
few-shot PTI initialization to initialize the StyleGAN generator through
several extreme poses sampled from the video to capture the consistent
representation of aligned faces from the target portrait. In the second stage,
we propose a Laplacian pyramid for high-frequency texture sampling from UV maps
deformed by dynamic flow of expression for motion-aware texture prior
integration to provide torso features to enhance StyleGAN's ability to generate
complete and upper body for portrait video rendering. Emo-Avatar reduces style
customization time from hours to merely 5 minutes compared with existing
methods. In addition, Emo-Avatar requires only a single reference image for
editing and employs region-aware contrastive learning with semantic invariant
CLIP guidance, ensuring consistent high-resolution output and identity
preservation. Through both quantitative and qualitative assessments, Emo-Avatar
demonstrates superior performance over existing methods in terms of training
efficiency, rendering quality and editability in self- and cross-reenactment.



---

## Optimal Projection for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Letian Huang, Jiayang Bai, Jie Guo, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2402.00752v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has garnered extensive attention and application in
real-time neural rendering. Concurrently, concerns have been raised about the
limitations of this technology in aspects such as point cloud storage,
performance , and robustness in sparse viewpoints , leading to various
improvements. However, there has been a notable lack of attention to the
projection errors introduced by the local affine approximation inherent in the
splatting itself, and the consequential impact of these errors on the quality
of photo-realistic rendering. This paper addresses the projection error
function of 3D Gaussian Splatting, commencing with the residual error from the
first-order Taylor expansion of the projection function $\phi$. The analysis
establishes a correlation between the error and the Gaussian mean position.
Subsequently, leveraging function optimization theory, this paper analyzes the
function's minima to provide an optimal projection strategy for Gaussian
Splatting referred to Optimal Gaussian Splatting. Experimental validation
further confirms that this projection methodology reduces artifacts, resulting
in a more convincingly realistic rendering.



---

## 360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-02-01 | Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo | cs.CV | [PDF](http://arxiv.org/pdf/2402.00763v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) has recently attracted great attention with
real-time and photo-realistic renderings. This technique typically takes
perspective images as input and optimizes a set of 3D elliptical Gaussians by
splatting them onto the image planes, resulting in 2D Gaussians. However,
applying 3D-GS to panoramic inputs presents challenges in effectively modeling
the projection onto the spherical surface of ${360^\circ}$ images using 2D
Gaussians. In practical applications, input panoramas are often sparse, leading
to unreliable initialization of 3D Gaussians and subsequent degradation of
3D-GS quality. In addition, due to the under-constrained geometry of
texture-less planes (e.g., walls and floors), 3D-GS struggles to model these
flat regions with elliptical Gaussians, resulting in significant floaters in
novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$
Gaussian splatting for a limited set of panoramic inputs. Instead of splatting
3D Gaussians directly onto the spherical surface, 360-GS projects them onto the
tangent plane of the unit sphere and then maps them to the spherical
projections. This adaptation enables the representation of the projection using
Gaussians. We guide the optimization of 360-GS by exploiting layout priors
within panoramas, which are simple to obtain and contain strong structural
information about the indoor scene. Our experimental results demonstrate that
360-GS allows panoramic rendering and outperforms state-of-the-art methods with
fewer artifacts in novel view synthesis, thus providing immersive roaming in
indoor scenarios.

Comments:
- 11 pages, 10 figures
