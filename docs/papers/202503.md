---
layout: default
title: March 2025
parent: Papers
nav_order: 202503
---

<!---metadata--->


## Tracking-Aware Deformation Field Estimation for Non-rigid 3D  Reconstruction in Robotic Surgeries

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Zeqing Wang, Han Fang, Yihong Xu, Yutong Ban | cs.CV | [PDF](http://arxiv.org/pdf/2503.02558v1){: .btn .btn-green } |

**Abstract**: Minimally invasive procedures have been advanced rapidly by the robotic
laparoscopic surgery. The latter greatly assists surgeons in sophisticated and
precise operations with reduced invasiveness. Nevertheless, it is still safety
critical to be aware of even the least tissue deformation during
instrument-tissue interactions, especially in 3D space. To address this, recent
works rely on NeRF to render 2D videos from different perspectives and
eliminate occlusions. However, most of the methods fail to predict the accurate
3D shapes and associated deformation estimates robustly. Differently, we
propose Tracking-Aware Deformation Field (TADF), a novel framework which
reconstructs the 3D mesh along with the 3D tissue deformation simultaneously.
It first tracks the key points of soft tissue by a foundation vision model,
providing an accurate 2D deformation field. Then, the 2D deformation field is
smoothly incorporated with a neural implicit reconstruction network to obtain
tissue deformation in the 3D space. Finally, we experimentally demonstrate that
the proposed method provides more accurate deformation estimation compared with
other 3D neural reconstruction methods in two public datasets.



---

## DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Haoyuan Li, Ziqin Ye, Yue Hao, Weiyang Lin, Chao Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.02223v1){: .btn .btn-green } |

**Abstract**: Accurate object perception is essential for robotic applications such as
object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM
system that seamlessly integrates object pose estimation and reconstruction. We
employ 3D Gaussian Splatting for high-fidelity object reconstruction and
leverage quadrics for precise object pose estimation. Both of them management
is handled on the CPU, while optimization is performed on the GPU,
significantly improving system efficiency. By associating objects with unique
IDs, our system enables rapid object extraction from the scene. Extensive
experimental results on object reconstruction and pose estimation demonstrate
that DQO-MAP achieves outstanding performance in terms of precision,
reconstruction quality, and computational efficiency. The code and dataset are
available at: https://github.com/LiHaoy-ux/DQO-MAP.



---

## Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yan Miao, Will Shen, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2503.02198v1){: .btn .btn-green } |

**Abstract**: We present the first framework demonstrating zero-shot sim-to-real transfer
of visual control policies learned in a Neural Radiance Field (NeRF)
environment for quadrotors to fly through racing gates. Robust transfer from
simulation to real flight poses a major challenge, as standard simulators often
lack sufficient visual fidelity. To address this, we construct a photorealistic
simulation environment of quadrotor racing tracks, called FalconGym, which
provides effectively unlimited synthetic images for training. Within FalconGym,
we develop a pipelined approach for crossing gates that combines (i) a Neural
Pose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor
poses from single-frame RGB images and IMU data, and (ii) a
self-attention-based multi-modal controller that adaptively integrates visual
features and pose estimation. This multi-modal design compensates for
perception noise and intermittent gate visibility. We train this controller
purely in FalconGym with imitation learning and deploy the resulting policy to
real hardware with no additional fine-tuning. Simulation experiments on three
distinct tracks (circle, U-turn and figure-8) demonstrate that our controller
outperforms a vision-only state-of-the-art baseline in both success rate and
gate-crossing accuracy. In 30 live hardware flights spanning three tracks and
120 gates, our controller achieves a 95.8% success rate and an average error of
just 10 cm when flying through 38 cm-radius gates.



---

## 2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Qipeng Yan, Mingyang Sun, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.02452v1){: .btn .btn-green } |

**Abstract**: Real-time rendering of high-fidelity and animatable avatars from monocular
videos remains a challenging problem in computer vision and graphics. Over the
past few years, the Neural Radiance Field (NeRF) has made significant progress
in rendering quality but behaves poorly in run-time performance due to the low
efficiency of volumetric rendering. Recently, methods based on 3D Gaussian
Splatting (3DGS) have shown great potential in fast training and real-time
rendering. However, they still suffer from artifacts caused by inaccurate
geometry. To address these problems, we propose 2DGS-Avatar, a novel approach
based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars
with high-fidelity and fast training performance. Given monocular RGB videos as
input, our method generates an avatar that can be driven by poses and rendered
in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the
advantages of fast training and rendering while also capturing detailed,
dynamic, and photo-realistic appearances. We conduct abundant experiments on
popular datasets such as AvatarRex and THuman4.0, demonstrating impressive
performance in both qualitative and quantitative metrics.

Comments:
- ICVRV 2024

---

## Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic  Guidance from Dense Novel Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yingji Zhong, Kaichen Zhou, Zhihao Li, Lanqing Hong, Zhenguo Li, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.02230v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable capabilities for
photorealistic novel view synthesis. One major deficiency of NeRF is that dense
inputs are typically required, and the rendering quality will drop drastically
given sparse inputs. In this paper, we highlight the effectiveness of rendered
semantics from dense novel views, and show that rendered semantics can be
treated as a more robust form of augmented data than rendered RGB. Our method
enhances NeRF's performance by incorporating guidance derived from the rendered
semantics. The rendered semantic guidance encompasses two levels: the
supervision level and the feature level. The supervision-level guidance
incorporates a bi-directional verification module that decides the validity of
each rendered semantic label, while the feature-level guidance integrates a
learnable codebook that encodes semantic-aware information, which is queried by
each point via the attention mechanism to obtain semantic-relevant predictions.
The overall semantic guidance is embedded into a self-improved pipeline. We
also introduce a more challenging sparse-input indoor benchmark, where the
number of inputs is limited to as few as 6. Experiments demonstrate the
effectiveness of our method and it exhibits superior performance compared to
existing approaches.



---

## Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, Mohamed Sayed | cs.CV | [PDF](http://arxiv.org/pdf/2503.02009v1){: .btn .btn-green } |

**Abstract**: Exploring real-world spaces using novel-view synthesis is fun, and
reimagining those worlds in a different style adds another layer of excitement.
Stylized worlds can also be used for downstream tasks where there is limited
training data and a need to expand a model's training distribution. Most
current novel-view synthesis stylization techniques lack the ability to
convincingly change geometry. This is because any geometry change requires
increased style strength which is often capped for stylization stability and
consistency. In this work, we propose a new autoregressive 3D Gaussian
Splatting stylization method. As part of this method, we contribute a new RGBD
diffusion model that allows for strength control over appearance and shape
stylization. To ensure consistency across stylized frames, we use a combination
of novel depth-guided cross attention, feature injection, and a Warp ControlNet
conditioned on composite frames for guiding the stylization of new frames. We
validate our method via extensive qualitative results, quantitative
experiments, and a user study. Code will be released online.



---

## Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling | cs.CV | [PDF](http://arxiv.org/pdf/2503.01774v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D
reconstruction and novel-view synthesis task. However, achieving photorealistic
rendering from extreme novel viewpoints remains challenging, as artifacts
persist across representations. In this work, we introduce Difix3D+, a novel
pipeline designed to enhance 3D reconstruction and novel-view synthesis through
single-step diffusion models. At the core of our approach is Difix, a
single-step image diffusion model trained to enhance and remove artifacts in
rendered novel views caused by underconstrained regions of the 3D
representation. Difix serves two critical roles in our pipeline. First, it is
used during the reconstruction phase to clean up pseudo-training views that are
rendered from the reconstruction and then distilled back into 3D. This greatly
enhances underconstrained regions and improves the overall 3D representation
quality. More importantly, Difix also acts as a neural enhancer during
inference, effectively removing residual artifacts arising from imperfect 3D
supervision and the limited capacity of current reconstruction models. Difix3D+
is a general solution, a single model compatible with both NeRF and 3DGS
representations, and it achieves an average 2$\times$ improvement in FID score
over baselines while maintaining 3D consistency.

Comments:
- CVPR 2025

---

## OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for  Object-Level Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Dianyi Yang, Yu Gao, Xihan Wang, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.01646v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have significantly improved the
efficiency and quality of dense semantic SLAM. However, previous methods are
generally constrained by limited-category pre-trained classifiers and implicit
semantic representation, which hinder their performance in open-set scenarios
and restrict 3D object-level scene understanding. To address these issues, we
propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian
representation to perform dense semantic SLAM in open-set environments. Our
system integrates explicit semantic labels derived from 2D foundational models
into the 3D Gaussian framework, facilitating robust 3D object-level scene
understanding. We introduce Gaussian Voting Splatting to enable fast 2D label
map rendering and scene updating. Additionally, we propose a Confidence-based
2D Label Consensus method to ensure consistent labeling across multiple views.
Furthermore, we employ a Segmentation Counter Pruning strategy to improve the
accuracy of semantic scene representation. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method
in scene understanding, tracking, and mapping, achieving 10 times faster
semantic rendering and 2 times lower storage costs compared to existing
methods. Project page: https://young-bit.github.io/opengs-github.github.io/.



---

## Category-level Meta-learned NeRF Priors for Efficient Object Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez | cs.CV | [PDF](http://arxiv.org/pdf/2503.01582v1){: .btn .btn-green } |

**Abstract**: In 3D object mapping, category-level priors enable efficient object
reconstruction and canonical pose estimation, requiring only a single prior per
semantic category (e.g., chair, book, laptop). Recently, DeepSDF has
predominantly been used as a category-level shape prior, but it struggles to
reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs
capture fine details but have yet to be effectively integrated with
category-level priors in a real-time multi-object mapping framework. To bridge
this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper
that integrates category-level priors with object-level NeRFs to enhance
reconstruction efficiency while enabling canonical object pose estimation.
PRENOM gets to know objects on a first-name basis by meta-learning on synthetic
reconstruction tasks generated from open-source shape datasets. To account for
object category variations, it employs a multi-objective genetic algorithm to
optimize the NeRF architecture for each category, balancing reconstruction
quality and training time. Additionally, prior-based probabilistic ray sampling
directs sampling toward expected object regions, accelerating convergence and
improving reconstruction quality under constrained resources. Experimental
results on a low-end GPU highlight the ability of PRENOM to achieve
high-quality reconstructions while maintaining computational feasibility.
Specifically, comparisons with prior-free NeRF-based approaches on a synthetic
dataset show a 21% lower Chamfer distance, demonstrating better reconstruction
quality. Furthermore, evaluations against other approaches using shape priors
on a noisy real-world dataset indicate a 13% improvement averaged across all
reconstruction metrics, a boost in rotation estimation accuracy, and comparable
translation and size estimation performance, while being trained for 5x less
time.



---

## Data Augmentation for NeRFs in the Low Data Limit

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Ayush Gaggar, Todd D. Murphey | cs.CV | [PDF](http://arxiv.org/pdf/2503.02092v1){: .btn .btn-green } |

**Abstract**: Current methods based on Neural Radiance Fields fail in the low data limit,
particularly when training on incomplete scene data. Prior works augment
training data only in next-best-view applications, which lead to hallucinations
and model collapse with sparse data. In contrast, we propose adding a set of
views during training by rejection sampling from a posterior uncertainty
distribution, generated by combining a volumetric uncertainty estimator with
spatial coverage. We validate our results on partially observed scenes; on
average, our method performs 39.9% better with 87.5% less variability across
established scene reconstruction benchmarks, as compared to state of the art
baselines. We further demonstrate that augmenting the training set by sampling
from any distribution leads to better, more consistent scene reconstruction in
sparse environments. This work is foundational for robotic tasks where
augmenting a dataset with informative data is critical in resource-constrained,
a priori unknown environments. Videos and source code are available at
https://murpheylab.github.io/low-data-nerf/.

Comments:
- To be published in 2025 IEEE International Conference on Robotics and
  Automation (ICRA 2025)
