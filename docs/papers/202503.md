---
layout: default
title: March 2025
parent: Papers
nav_order: 202503
---

<!---metadata--->


## D2GV: Deformable 2D Gaussian Splatting for Video Representation in  400FPS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Mufan Liu, Qi Yang, Miaoran Zhao, He Huang, Le Yang, Zhu Li, Yiling Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.05600v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representations (INRs) have emerged as a powerful approach
for video representation, offering versatility across tasks such as compression
and inpainting. However, their implicit formulation limits both
interpretability and efficacy, undermining their practicality as a
comprehensive solution. We propose a novel video representation based on
deformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key
objectives: 1) improved efficiency while delivering superior quality; 2)
enhanced scalability and interpretability; and 3) increased friendliness for
downstream tasks. Specifically, we initially divide the video sequence into
fixed-length Groups of Pictures (GoP) to allow parallel training and linear
scalability with video length. For each GoP, D2GV represents video frames by
applying differentiable rasterization to 2D Gaussians, which are deformed from
a canonical space into their corresponding timestamps. Notably, leveraging
efficient CUDA-based rasterization, D2GV converges fast and decodes at speeds
exceeding 400 FPS, while delivering quality that matches or surpasses
state-of-the-art INRs. Moreover, we incorporate a learnable pruning and
quantization strategy to streamline D2GV into a more compact representation. We
demonstrate D2GV's versatility in tasks including video interpolation,
inpainting and denoising, underscoring its potential as a promising solution
for video representation. Code is available at:
\href{https://github.com/Evan-sudo/D2GV}{https://github.com/Evan-sudo/D2GV}.



---

## MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface  Reconstruction under Various Light Conditions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Qingyuan Zhou, Yuehu Gong, Weidong Yang, Jiaze Li, Yeqi Luo, Baixin Xu, Shuhao Li, Ben Fei, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2503.05182v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) and surface reconstruction (SR) are essential
tasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks
are often addressed independently, with GS-based rendering methods struggling
under diverse light conditions and failing to produce accurate surfaces, while
GS-based reconstruction methods frequently compromise rendering quality. This
raises a central question: must rendering and reconstruction always involve a
trade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian
splatting for Surface Reconstruction that enhances both rendering quality and
3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS
and the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,
providing precise geometry information to the 3D-GS branch. Leveraging this
geometry, the 3D-GS branch employs a geometry-guided illumination decomposition
module that captures reflected and transmitted components, enabling realistic
rendering under varied light conditions. Using the transmitted component as
supervision, the 2D-GS branch also achieves high-fidelity surface
reconstruction. Throughout the optimization process, the 2D-GS and 3D-GS
branches undergo alternating optimization, providing mutual supervision. Prior
to this, each branch completes an independent warm-up phase, with an early
stopping strategy implemented to reduce computational costs. We evaluate MGSR
on a diverse set of synthetic and real-world datasets, at both object and scene
levels, demonstrating strong performance in rendering and surface
reconstruction.

Comments:
- 11 pages, 7 figures

---

## LiDAR-enhanced 3D Gaussian Splatting Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jian Shen, Huai Yu, Ji Wu, Wen Yang, Gui-Song Xia | cs.RO | [PDF](http://arxiv.org/pdf/2503.05425v1){: .btn .btn-green } |

**Abstract**: This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting
(3DGS) mapping framework that improves the accuracy and robustness of 3D scene
mapping by integrating LiDAR data. LiGSM constructs joint loss from images and
LiDAR point clouds to estimate the poses and optimize their extrinsic
parameters, enabling dynamic adaptation to variations in sensor alignment.
Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a
denser and more reliable starting points compared to sparse SfM points. In
scene rendering, the framework augments standard image-based supervision with
depth maps generated from LiDAR projections, ensuring an accurate scene
representation in both geometry and photometry. Experiments on public and
self-collected datasets demonstrate that LiGSM outperforms comparative methods
in pose tracking and scene rendering.

Comments:
- Accepted by ICRA 2025

---

## Self-Modeling Robots by Photographing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Kejun Hu, Peng Yu, Ning Tan | cs.RO | [PDF](http://arxiv.org/pdf/2503.05398v1){: .btn .btn-green } |

**Abstract**: Self-modeling enables robots to build task-agnostic models of their
morphology and kinematics based on data that can be automatically collected,
with minimal human intervention and prior information, thereby enhancing
machine intelligence. Recent research has highlighted the potential of
data-driven technology in modeling the morphology and kinematics of robots.
However, existing self-modeling methods suffer from either low modeling quality
or excessive data acquisition costs. Beyond morphology and kinematics, texture
is also a crucial component of robots, which is challenging to model and
remains unexplored. In this work, a high-quality, texture-aware, and link-level
method is proposed for robot self-modeling. We utilize three-dimensional (3D)
Gaussians to represent the static morphology and texture of robots, and cluster
the 3D Gaussians to construct neural ellipsoid bones, whose deformations are
controlled by the transformation matrices generated by a kinematic neural
network. The 3D Gaussians and kinematic neural network are trained using data
pairs composed of joint angles, camera parameters and multi-view images without
depth information. By feeding the kinematic neural network with joint angles,
we can utilize the well-trained model to describe the corresponding morphology,
kinematics and texture of robots at the link level, and render robot images
from different perspectives with the aid of 3D Gaussian splatting. Furthermore,
we demonstrate that the established model can be exploited to perform
downstream tasks such as motion planning and inverse kinematics.



---

## CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from  Motion-Blurred Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jungho Lee, Donghyeong Kim, Dogyoon Lee, Suhwan Cho, Minhyeok Lee, Wonjoon Lee, Taeoh Kim, Dongyoon Wee, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.05332v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for their
high-quality novel view rendering, motivating research to address real-world
challenges. A critical issue is the camera motion blur caused by movement
during exposure, which hinders accurate 3D scene reconstruction. In this study,
we propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that
reconstructs precise 3D scenes from motion-blurred images while maintaining
real-time rendering speed. Considering the complex motion patterns inherent in
real-world camera movements, we predict continuous camera trajectories using
neural ordinary differential equations (ODEs). To ensure accurate modeling, we
employ rigid body transformations, preserving the shape and size of the object
but rely on the discrete integration of sampled frames. To better approximate
the continuous nature of motion blur, we introduce a continuous motion
refinement (CMR) transformation that refines rigid transformations by
incorporating additional learnable parameters. By revisiting fundamental camera
theory and leveraging advanced neural ODE techniques, we achieve precise
modeling of continuous camera trajectories, leading to improved reconstruction
accuracy. Extensive experiments demonstrate state-of-the-art performance both
quantitatively and qualitatively on benchmark datasets, which include a wide
range of motion blur scenarios, from moderate to extreme blur.

Comments:
- Revised Version of CRiM-GS, Github:
  https://github.com/Jho-Yonsei/CoMoGaussian

---

## STGA: Selective-Training Gaussian Head Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Hanzhi Guo, Yixiao Chen, Dongye Xiaonuo, Zeyu Tian, Dongdong Weng, Le Luo | cs.GR | [PDF](http://arxiv.org/pdf/2503.05196v1){: .btn .btn-green } |

**Abstract**: We propose selective-training Gaussian head avatars (STGA) to enhance the
details of dynamic head Gaussian. The dynamic head Gaussian model is trained
based on the FLAME parameterized model. Each Gaussian splat is embedded within
the FLAME mesh to achieve mesh-based animation of the Gaussian model. Before
training, our selection strategy calculates the 3D Gaussian splat to be
optimized in each frame. The parameters of these 3D Gaussian splats are
optimized in the training of each frame, while those of the other splats are
frozen. This means that the splats participating in the optimization process
differ in each frame, to improve the realism of fine details. Compared with
network-based methods, our method achieves better results with shorter training
time. Compared with mesh-based methods, our method produces more realistic
details within the same training time. Additionally, the ablation experiment
confirms that our method effectively enhances the quality of details.



---

## Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot  Manipulation of Irregularly Shaped Objects


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2503.05189v1){: .btn .btn-green } |

**Abstract**: Tracking and manipulating irregularly-shaped, previously unseen objects in
dynamic environments is important for robotic applications in manufacturing,
assembly, and logistics. Recently introduced Gaussian Splats efficiently model
object geometry, but lack persistent state estimation for task-oriented
manipulation. We present Persistent Object Gaussian Splat (POGS), a system that
embeds semantics, self-supervised visual features, and object grouping features
into a compact representation that can be continuously updated to estimate the
pose of scanned objects. POGS updates object states without requiring expensive
rescanning or prior CAD models of objects. After an initial multi-view scene
capture and training phase, POGS uses a single stereo camera to integrate depth
estimates along with self-supervised vision encoder features for object pose
estimation. POGS supports grasping, reorientation, and natural language-driven
manipulation by refining object pose estimates, facilitating sequential object
reset operations with human-induced object perturbations and tool servoing,
where robots recover tool pose despite tool perturbations of up to 30{\deg}.
POGS achieves up to 12 consecutive successful object resets and recovers from
80% of in-grasp tool perturbations.

Comments:
- Accepted to ICRA 2025

---

## SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Xiaotong Huang, He Zhu, Zihan Liu, Weikai Lin, Xiaohong Liu, Zhezhi He, Jingwen Leng, Minyi Guo, Yu Feng | cs.GR | [PDF](http://arxiv.org/pdf/2503.05168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a crucial rendering technique for
many real-time applications. However, the limited hardware resources on today's
mobile platforms hinder these applications, as they struggle to achieve
real-time performance. In this paper, we propose SeeLe, a general framework
designed to accelerate the 3DGS pipeline for resource-constrained mobile
devices.
  Specifically, we propose two GPU-oriented techniques: hybrid preprocessing
and contribution-aware rasterization. Hybrid preprocessing alleviates the GPU
compute and memory pressure by reducing the number of irrelevant Gaussians
during rendering. The key is to combine our view-dependent scene representation
with online filtering. Meanwhile, contribution-aware rasterization improves the
GPU utilization at the rasterization stage by prioritizing Gaussians with high
contributions while reducing computations for those with low contributions.
Both techniques can be seamlessly integrated into existing 3DGS pipelines with
minimal fine-tuning. Collectively, our framework achieves 2.6$\times$ speedup
and 32.3\% model reduction while achieving superior rendering quality compared
to existing methods.



---

## SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image  via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Linqi Yang, Xiongwei Zhao, Qihao Sun, Ke Wang, Ao Chen, Peng Kang | cs.CV | [PDF](http://arxiv.org/pdf/2503.05174v1){: .btn .btn-green } |

**Abstract**: 6-DoF pose estimation is a fundamental task in computer vision with
wide-ranging applications in augmented reality and robotics. Existing single
RGB-based methods often compromise accuracy due to their reliance on initial
pose estimates and susceptibility to rotational ambiguity, while approaches
requiring depth sensors or multi-view setups incur significant deployment
costs. To address these limitations, we introduce SplatPose, a novel framework
that synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neural
architecture to achieve high-precision pose estimation using only a single RGB
image. Central to our approach is the Dual-Attention Ray Scoring Network
(DARS-Net), which innovatively decouples positional and angular alignment
through geometry-domain attention mechanisms, explicitly modeling directional
dependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fine
optimization pipeline progressively refines pose estimates by aligning dense 2D
features between query images and 3DGS-synthesized views, effectively
correcting feature misalignment and depth errors from sparse ray sampling.
Experiments on three benchmark datasets demonstrate that SplatPose achieves
state-of-the-art 6-DoF pose estimation accuracy in single RGB settings,
rivaling approaches that depend on depth or multi-view images.

Comments:
- Submitted to IROS 2025

---

## Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D  Gaussian Splatting from Sparse Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Yingji Zhong, Zhihao Li, Dave Zhenyu Chen, Lanqing Hong, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.05082v1){: .btn .btn-green } |

**Abstract**: Despite recent successes in novel view synthesis using 3D Gaussian Splatting
(3DGS), modeling scenes with sparse inputs remains a challenge. In this work,
we address two critical yet overlooked issues in real-world sparse-input
modeling: extrapolation and occlusion. To tackle these issues, we propose to
use a reconstruction by generation pipeline that leverages learned priors from
video diffusion models to provide plausible interpretations for regions outside
the field of view or occluded. However, the generated sequences exhibit
inconsistencies that do not fully benefit subsequent 3DGS modeling. To address
the challenge of inconsistencies, we introduce a novel scene-grounding guidance
based on rendered sequences from an optimized 3DGS, which tames the diffusion
model to generate consistent sequences. This guidance is training-free and does
not require any fine-tuning of the diffusion model. To facilitate holistic
scene modeling, we also propose a trajectory initialization method. It
effectively identifies regions that are outside the field of view and occluded.
We further design a scheme tailored for 3DGS optimization with generated
sequences. Experiments demonstrate that our method significantly improves upon
the baseline and achieves state-of-the-art performance on challenging
benchmarks.

Comments:
- Accepted by CVPR2025. The project page is available at
  https://zhongyingji.github.io/guidevd-3dgs/

---

## Free Your Hands: Lightweight Relightable Turntable Capture Pipeline

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jiahui Fan, Fujun Luan, Miloš Hašan, Jian Yang, Beibei Wang | cs.GR | [PDF](http://arxiv.org/pdf/2503.05511v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) from multiple captured photos of an object is a
widely studied problem. Achieving high quality typically requires dense
sampling of input views, which can lead to frustrating and tedious manual
labor. Manually positioning cameras to maintain an optimal desired distribution
can be difficult for humans, and if a good distribution is found, it is not
easy to replicate. Additionally, the captured data can suffer from motion blur
and defocus due to human error. In this paper, we present a lightweight object
capture pipeline to reduce the manual workload and standardize the acquisition
setup. We use a consumer turntable to carry the object and a tripod to hold the
camera. As the turntable rotates, we automatically capture dense samples from
various views and lighting conditions; we can repeat this for several camera
positions. This way, we can easily capture hundreds of valid images in several
minutes without hands-on effort. However, in the object reference frame, the
light conditions vary; this is harmful to a standard NVS method like 3D
Gaussian splatting (3DGS) which assumes fixed lighting. We design a neural
radiance representation conditioned on light rotations, which addresses this
issue and allows relightability as an additional benefit. We demonstrate our
pipeline using 3DGS as the underlying framework, achieving competitive quality
compared to previous methods with exhaustive acquisition and showcasing its
potential for relighting and harmonization tasks.



---

## EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D  Gaussian Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Chao Zhang, Yifeng Zhou, Shuheng Wang, Wenfa Li, Degang Wang, Yi Xu, Shaohui Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2503.05162v1){: .btn .btn-green } |

**Abstract**: We have recently seen great progress in 3D scene reconstruction through
explicit point-based 3D Gaussian Splatting (3DGS), notable for its high quality
and fast rendering speed. However, reconstructing dynamic scenes such as
complex human performances with long durations remains challenging. Prior
efforts fall short of modeling a long-term sequence with drastic motions,
frequent topology changes or interactions with props, and resort to segmenting
the whole sequence into groups of frames that are processed independently,
which undermines temporal stability and thereby leads to an unpleasant viewing
experience and inefficient storage footprint. In view of this, we introduce
EvolvingGS, a two-stage strategy that first deforms the Gaussian model to
coarsely align with the target frame, and then refines it with minimal point
addition/subtraction, particularly in fast-changing areas. Owing to the
flexibility of the incrementally evolving representation, our method
outperforms existing approaches in terms of both per-frame and temporal quality
metrics while maintaining fast rendering through its purely explicit
representation. Moreover, by exploiting temporal coherence between successive
frames, we propose a simple yet effective compression algorithm that achieves
over 50x compression rate. Extensive experiments on both public benchmarks and
challenging custom datasets demonstrate that our method significantly advances
the state-of-the-art in dynamic scene reconstruction, particularly for extended
sequences with complex human performances.



---

## GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Kohei Honda, Takeshi Ishita, Yasuhiro Yoshimura, Ryo Yonitani | cs.RO | [PDF](http://arxiv.org/pdf/2503.05152v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel approach to image-goal navigation by integrating
3D Gaussian Splatting (3DGS) with Visual Navigation Models (VNMs), a method we
refer to as GSplatVNM. VNMs offer a promising paradigm for image-goal
navigation by guiding a robot through a sequence of point-of-view images
without requiring metrical localization or environment-specific training.
However, constructing a dense and traversable sequence of target viewpoints
from start to goal remains a central challenge, particularly when the available
image database is sparse. To address these challenges, we propose a 3DGS-based
viewpoint synthesis framework for VNMs that synthesizes intermediate viewpoints
to seamlessly bridge gaps in sparse data while significantly reducing storage
overhead. Experimental results in a photorealistic simulator demonstrate that
our approach not only enhances navigation efficiency but also exhibits
robustness under varying levels of image database sparsity.

Comments:
- 8 pages, 4 figures

---

## GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three  Orthographic Views Using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Zheng Zhou, Zhe Li, Bo Yu, Lina Hu, Liang Dong, Zijian Yang, Xiaoli Liu, Ning Xu, Ziwei Wang, Yonghao Dang, Jianqin Yin | cs.CV | [PDF](http://arxiv.org/pdf/2503.05161v1){: .btn .btn-green } |

**Abstract**: The automatic reconstruction of 3D computer-aided design (CAD) models from
CAD sketches has recently gained significant attention in the computer vision
community. Most existing methods, however, rely on vector CAD sketches and 3D
ground truth for supervision, which are often difficult to be obtained in
industrial applications and are sensitive to noise inputs. We propose viewing
CAD reconstruction as a specific instance of sparse-view 3D reconstruction to
overcome these limitations. While this reformulation offers a promising
perspective, existing 3D reconstruction methods typically require natural
images and corresponding camera poses as inputs, which introduces two major
significant challenges: (1) modality discrepancy between CAD sketches and
natural images, and (2) difficulty of accurate camera pose estimation for CAD
sketches. To solve these issues, we first transform the CAD sketches into
representations resembling natural images and extract corresponding masks.
Next, we manually calculate the camera poses for the orthographic views to
ensure accurate alignment within the 3D coordinate system. Finally, we employ a
customized sparse-view 3D reconstruction method to achieve high-quality
reconstructions from aligned orthographic views. By leveraging raster CAD
sketches for self-supervision, our approach eliminates the reliance on vector
CAD sketches and 3D ground truth. Experiments on the Sub-Fusion360 dataset
demonstrate that our proposed method significantly outperforms previous
approaches in CAD reconstruction performance and exhibits strong robustness to
noisy inputs.



---

## GaussianVideo: Efficient Video Representation and Compression by  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Inseo Lee, Youngyoon Choi, Joonseok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.04333v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representation for Videos (NeRV) has introduced a novel
paradigm for video representation and compression, outperforming traditional
codecs. As model size grows, however, slow encoding and decoding speed and high
memory consumption hinder its application in practice. To address these
limitations, we propose a new video representation and compression method based
on 2D Gaussian Splatting to efficiently handle video data. Our proposed
deformable 2D Gaussian Splatting dynamically adapts the transformation of 2D
Gaussians at each frame, significantly reducing memory cost. Equipped with a
multi-plane-based spatiotemporal encoder and a lightweight decoder, it predicts
changes in color, coordinates, and shape of initialized Gaussians, given the
time step. By leveraging temporal gradients, our model effectively captures
temporal redundancy at negligible cost, significantly enhancing video
representation efficiency. Our method reduces GPU memory usage by up to 78.4%,
and significantly expedites video processing, achieving 5.5x faster training
and 12.5x faster decoding compared to the state-of-the-art NeRV methods.



---

## Beyond Existance: Fulfill 3D Reconstructed Scenes with Pseudo Details

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Yifei Gao, Jun Huang, Lei Wang, Ruiting Dai, Jun Cheng | cs.GR | [PDF](http://arxiv.org/pdf/2503.04037v1){: .btn .btn-green } |

**Abstract**: The emergence of 3D Gaussian Splatting (3D-GS) has significantly advanced 3D
reconstruction by providing high fidelity and fast training speeds across
various scenarios. While recent efforts have mainly focused on improving model
structures to compress data volume or reduce artifacts during zoom-in and
zoom-out operations, they often overlook an underlying issue: training sampling
deficiency. In zoomed-in views, Gaussian primitives can appear unregulated and
distorted due to their dilation limitations and the insufficient availability
of scale-specific training samples. Consequently, incorporating pseudo-details
that ensure the completeness and alignment of the scene becomes essential. In
this paper, we introduce a new training method that integrates diffusion models
and multi-scale training using pseudo-ground-truth data. This approach not only
notably mitigates the dilation and zoomed-in artifacts but also enriches
reconstructed scenes with precise details out of existing scenarios. Our method
achieves state-of-the-art performance across various benchmarks and extends the
capabilities of 3D reconstruction beyond training datasets.



---

## GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian  Radiance Fields and Differentiable Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Qianzhong Chen, Jiankai Sun, Naixiang Gao, JunEn Low, Timothy Chen, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2503.03984v1){: .btn .btn-green } |

**Abstract**: Autonomous visual navigation is an essential element in robot autonomy.
Reinforcement learning (RL) offers a promising policy training paradigm.
However existing RL methods suffer from high sample complexity, poor
sim-to-real transfer, and limited runtime adaptability to navigation scenarios
not seen during training. These problems are particularly challenging for
drones, with complex nonlinear and unstable dynamics, and strong dynamic
coupling between control and perception. In this paper, we propose a novel
framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep
reinforcement learning (DDRL) to train vision-based drone navigation policies.
By leveraging high-fidelity 3D scene representations and differentiable
simulation, our method improves sample efficiency and sim-to-real transfer.
Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt
to environmental variations at runtime. Moreover, by curriculum training in a
mixture of different surrounding environments, we achieve in-task
generalization, the ability to solve new instances of a task not seen during
training. Drone hardware experiments demonstrate our method's high training
efficiency compared to state-of-the-art RL methods, zero shot sim-to-real
transfer for real robot deployment without fine tuning, and ability to adapt to
new instances within the same task class (e.g. to fly through a gate at
different locations with different distractors in the environment).



---

## GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world  Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Xihan Wang, Dianyi Yang, Yu Gao, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.04034v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting(3DGS) have significantly
improved semantic scene understanding, enabling natural language queries to
localize objects within a scene. However, existing methods primarily focus on
embedding compressed CLIP features to 3D Gaussians, suffering from low object
segmentation accuracy and lack spatial reasoning capabilities. To address these
limitations, we propose GaussianGraph, a novel framework that enhances
3DGS-based scene understanding by integrating adaptive semantic clustering and
scene graph generation. We introduce a "Control-Follow" clustering strategy,
which dynamically adapts to scene scale and feature distribution, avoiding
feature compression and significantly improving segmentation accuracy.
Additionally, we enrich scene representation by integrating object attributes
and spatial relations extracted from 2D foundation models. To address
inaccuracies in spatial relationships, we propose 3D correction modules that
filter implausible relations through spatial consistency verification, ensuring
reliable scene graph construction. Extensive experiments on three datasets
demonstrate that GaussianGraph outperforms state-of-the-art methods in both
semantic segmentation and object grounding tasks, providing a robust solution
for complex scene understanding and interaction.



---

## S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Yecong Wan, Mingwen Shao, Yuanshuo Cheng, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2503.04314v1){: .btn .btn-green } |

**Abstract**: In this paper, we aim ambitiously for a realistic yet challenging problem,
namely, how to reconstruct high-quality 3D scenes from sparse low-resolution
views that simultaneously suffer from deficient perspectives and clarity.
Whereas existing methods only deal with either sparse views or low-resolution
observations, they fail to handle such hybrid and complicated scenarios. To
this end, we propose a novel Sparse-view Super-resolution 3D Gaussian Splatting
framework, dubbed S2Gaussian, that can reconstruct structure-accurate and
detail-faithful 3D scenes with only sparse and low-resolution views. The
S2Gaussian operates in a two-stage fashion. In the first stage, we initially
optimize a low-resolution Gaussian representation with depth regularization and
densify it to initialize the high-resolution Gaussians through a tailored
Gaussian Shuffle Split operation. In the second stage, we refine the
high-resolution Gaussians with the super-resolved images generated from both
original sparse views and pseudo-views rendered by the low-resolution
Gaussians. In which a customized blur-free inconsistency modeling scheme and a
3D robust optimization strategy are elaborately designed to mitigate multi-view
inconsistency and eliminate erroneous updates caused by imperfect supervision.
Extensive experiments demonstrate superior results and in particular
establishing new state-of-the-art performances with more consistent geometry
and finer details.

Comments:
- CVPR 2025

---

## Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall, Yumeng Wang, Paul Maria Scheikl, Axel Krieger | cs.CV | [PDF](http://arxiv.org/pdf/2503.04079v1){: .btn .btn-green } |

**Abstract**: Accurate geometric reconstruction of deformable tissues in monocular
endoscopic video remains a fundamental challenge in robot-assisted minimally
invasive surgery. Although recent volumetric and point primitive methods based
on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently
rendered surgical scenes, they still struggle with handling artifact-free tool
occlusions and preserving fine anatomical details. These limitations stem from
unrestricted Gaussian scaling and insufficient surface alignment constraints
during reconstruction. To address these issues, we introduce Surgical Gaussian
Surfels (SGS), which transforms anisotropic point primitives into
surface-aligned elliptical splats by constraining the scale component of the
Gaussian covariance matrix along the view-aligned axis. We predict accurate
surfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled
with locality constraints to handle complex tissue deformations. We use
homodirectional view-space positional gradients to capture fine image details
by splitting Gaussian Surfels in over-reconstructed regions. In addition, we
define surface normals as the direction of the steepest density change within
each Gaussian surfel primitive, enabling accurate normal estimation without
requiring monocular normal priors. We evaluate our method on two in-vivo
surgical datasets, where it outperforms current state-of-the-art methods in
surface geometry, normal map quality, and rendering efficiency, while remaining
competitive in real-time rendering performance. We make our code available at
https://github.com/aloma85/SurgicalGaussianSurfels



---

## Instrument-Splatting: Controllable Photorealistic Reconstruction of  Surgical Instruments Using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Shuojue Yang, Zijian Wu, Mingxuan Hong, Qian Li, Daiyun Shen, Septimiu E. Salcudean, Yueming Jin | cs.CV | [PDF](http://arxiv.org/pdf/2503.04082v1){: .btn .btn-green } |

**Abstract**: Real2Sim is becoming increasingly important with the rapid development of
surgical artificial intelligence (AI) and autonomy. In this work, we propose a
novel Real2Sim methodology, \textit{Instrument-Splatting}, that leverages 3D
Gaussian Splatting to provide fully controllable 3D reconstruction of surgical
instruments from monocular surgical videos. To maintain both high visual
fidelity and manipulability, we introduce a geometry pre-training to bind
Gaussian point clouds on part mesh with accurate geometric priors and define a
forward kinematics to control the Gaussians as flexible as real instruments.
Afterward, to handle unposed videos, we design a novel instrument pose tracking
method leveraging semantics-embedded Gaussians to robustly refine per-frame
instrument poses and joint states in a render-and-compare manner, which allows
our instrument Gaussian to accurately learn textures and reach photorealistic
rendering. We validated our method on 2 publicly released surgical videos and 4
videos collected on ex vivo tissues and green screens. Quantitative and
qualitative evaluations demonstrate the effectiveness and superiority of the
proposed method.

Comments:
- 11 pages, 5 figures

---

## NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian  Splatting Based on Thermodynamics


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-05 | Kun Yang, Yuxiang Liu, Zeyu Cui, Yu Liu, Maojun Zhang, Shen Yan, Qing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.03115v1){: .btn .btn-green } |

**Abstract**: Thermal infrared imaging offers the advantage of all-weather capability,
enabling non-intrusive measurement of an object's surface temperature.
Consequently, thermal infrared images are employed to reconstruct 3D models
that accurately reflect the temperature distribution of a scene, aiding in
applications such as building monitoring and energy management. However,
existing approaches predominantly focus on static 3D reconstruction for a
single time period, overlooking the impact of environmental factors on thermal
radiation and failing to predict or analyze temperature variations over time.
To address these challenges, we propose the NTR-Gaussian method, which treats
temperature as a form of thermal radiation, incorporating elements like
convective heat transfer and radiative heat dissipation. Our approach utilizes
neural networks to predict thermodynamic parameters such as emissivity,
convective heat transfer coefficient, and heat capacity. By integrating these
predictions, we can accurately forecast thermal temperatures at various times
throughout a nighttime scene. Furthermore, we introduce a dynamic dataset
specifically for nighttime thermal imagery. Extensive experiments and
evaluations demonstrate that NTR-Gaussian significantly outperforms comparison
methods in thermal reconstruction, achieving a predicted temperature error
within 1 degree Celsius.

Comments:
- IEEE Conference on Computer Vision and Pattern Recognition 2025

---

## LensDFF: Language-enhanced Sparse Feature Distillation for Efficient  Few-Shot Dexterous Manipulation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-05 | Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll | cs.RO | [PDF](http://arxiv.org/pdf/2503.03890v1){: .btn .btn-green } |

**Abstract**: Learning dexterous manipulation from few-shot demonstrations is a significant
yet challenging problem for advanced, human-like robotic systems. Dense
distilled feature fields have addressed this challenge by distilling rich
semantic features from 2D visual foundation models into the 3D domain. However,
their reliance on neural rendering models such as Neural Radiance Fields (NeRF)
or Gaussian Splatting results in high computational costs. In contrast,
previous approaches based on sparse feature fields either suffer from
inefficiencies due to multi-view dependencies and extensive training or lack
sufficient grasp dexterity. To overcome these limitations, we propose
Language-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently
distills view-consistent 2D features onto 3D points using our novel
language-enhanced feature fusion strategy, thereby enabling single-view
few-shot generalization. Based on LensDFF, we further introduce a few-shot
dexterous manipulation framework that integrates grasp primitives into the
demonstrations to generate stable and highly dexterous grasps. Moreover, we
present a real2sim grasp evaluation pipeline for efficient grasp assessment and
hyperparameter tuning. Through extensive simulation experiments based on the
real2sim pipeline and real-world experiments, our approach achieves competitive
grasping performance, outperforming state-of-the-art approaches.

Comments:
- 8 pages

---

## Tracking-Aware Deformation Field Estimation for Non-rigid 3D  Reconstruction in Robotic Surgeries

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Zeqing Wang, Han Fang, Yihong Xu, Yutong Ban | cs.CV | [PDF](http://arxiv.org/pdf/2503.02558v1){: .btn .btn-green } |

**Abstract**: Minimally invasive procedures have been advanced rapidly by the robotic
laparoscopic surgery. The latter greatly assists surgeons in sophisticated and
precise operations with reduced invasiveness. Nevertheless, it is still safety
critical to be aware of even the least tissue deformation during
instrument-tissue interactions, especially in 3D space. To address this, recent
works rely on NeRF to render 2D videos from different perspectives and
eliminate occlusions. However, most of the methods fail to predict the accurate
3D shapes and associated deformation estimates robustly. Differently, we
propose Tracking-Aware Deformation Field (TADF), a novel framework which
reconstructs the 3D mesh along with the 3D tissue deformation simultaneously.
It first tracks the key points of soft tissue by a foundation vision model,
providing an accurate 2D deformation field. Then, the 2D deformation field is
smoothly incorporated with a neural implicit reconstruction network to obtain
tissue deformation in the 3D space. Finally, we experimentally demonstrate that
the proposed method provides more accurate deformation estimation compared with
other 3D neural reconstruction methods in two public datasets.



---

## Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic  Guidance from Dense Novel Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yingji Zhong, Kaichen Zhou, Zhihao Li, Lanqing Hong, Zhenguo Li, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.02230v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable capabilities for
photorealistic novel view synthesis. One major deficiency of NeRF is that dense
inputs are typically required, and the rendering quality will drop drastically
given sparse inputs. In this paper, we highlight the effectiveness of rendered
semantics from dense novel views, and show that rendered semantics can be
treated as a more robust form of augmented data than rendered RGB. Our method
enhances NeRF's performance by incorporating guidance derived from the rendered
semantics. The rendered semantic guidance encompasses two levels: the
supervision level and the feature level. The supervision-level guidance
incorporates a bi-directional verification module that decides the validity of
each rendered semantic label, while the feature-level guidance integrates a
learnable codebook that encodes semantic-aware information, which is queried by
each point via the attention mechanism to obtain semantic-relevant predictions.
The overall semantic guidance is embedded into a self-improved pipeline. We
also introduce a more challenging sparse-input indoor benchmark, where the
number of inputs is limited to as few as 6. Experiments demonstrate the
effectiveness of our method and it exhibits superior performance compared to
existing approaches.



---

## 2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Qipeng Yan, Mingyang Sun, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.02452v1){: .btn .btn-green } |

**Abstract**: Real-time rendering of high-fidelity and animatable avatars from monocular
videos remains a challenging problem in computer vision and graphics. Over the
past few years, the Neural Radiance Field (NeRF) has made significant progress
in rendering quality but behaves poorly in run-time performance due to the low
efficiency of volumetric rendering. Recently, methods based on 3D Gaussian
Splatting (3DGS) have shown great potential in fast training and real-time
rendering. However, they still suffer from artifacts caused by inaccurate
geometry. To address these problems, we propose 2DGS-Avatar, a novel approach
based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars
with high-fidelity and fast training performance. Given monocular RGB videos as
input, our method generates an avatar that can be driven by poses and rendered
in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the
advantages of fast training and rendering while also capturing detailed,
dynamic, and photo-realistic appearances. We conduct abundant experiments on
popular datasets such as AvatarRex and THuman4.0, demonstrating impressive
performance in both qualitative and quantitative metrics.

Comments:
- ICVRV 2024

---

## Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yan Miao, Will Shen, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2503.02198v1){: .btn .btn-green } |

**Abstract**: We present the first framework demonstrating zero-shot sim-to-real transfer
of visual control policies learned in a Neural Radiance Field (NeRF)
environment for quadrotors to fly through racing gates. Robust transfer from
simulation to real flight poses a major challenge, as standard simulators often
lack sufficient visual fidelity. To address this, we construct a photorealistic
simulation environment of quadrotor racing tracks, called FalconGym, which
provides effectively unlimited synthetic images for training. Within FalconGym,
we develop a pipelined approach for crossing gates that combines (i) a Neural
Pose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor
poses from single-frame RGB images and IMU data, and (ii) a
self-attention-based multi-modal controller that adaptively integrates visual
features and pose estimation. This multi-modal design compensates for
perception noise and intermittent gate visibility. We train this controller
purely in FalconGym with imitation learning and deploy the resulting policy to
real hardware with no additional fine-tuning. Simulation experiments on three
distinct tracks (circle, U-turn and figure-8) demonstrate that our controller
outperforms a vision-only state-of-the-art baseline in both success rate and
gate-crossing accuracy. In 30 live hardware flights spanning three tracks and
120 gates, our controller achieves a 95.8% success rate and an average error of
just 10 cm when flying through 38 cm-radius gates.



---

## DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Haoyuan Li, Ziqin Ye, Yue Hao, Weiyang Lin, Chao Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.02223v1){: .btn .btn-green } |

**Abstract**: Accurate object perception is essential for robotic applications such as
object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM
system that seamlessly integrates object pose estimation and reconstruction. We
employ 3D Gaussian Splatting for high-fidelity object reconstruction and
leverage quadrics for precise object pose estimation. Both of them management
is handled on the CPU, while optimization is performed on the GPU,
significantly improving system efficiency. By associating objects with unique
IDs, our system enables rapid object extraction from the scene. Extensive
experimental results on object reconstruction and pose estimation demonstrate
that DQO-MAP achieves outstanding performance in terms of precision,
reconstruction quality, and computational efficiency. The code and dataset are
available at: https://github.com/LiHaoy-ux/DQO-MAP.



---

## FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with  Sparse and Dense Map Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Yansong Xu, Junlin Li, Wei Zhang, Siyu Chen, Shengyong Zhang, Yuquan Leng, Weijia Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.01109v1){: .btn .btn-green } |

**Abstract**: 3D gaussian splatting has advanced simultaneous localization and mapping
(SLAM) technology by enabling real-time positioning and the construction of
high-fidelity maps. However, the uncertainty in gaussian position and
initialization parameters introduces challenges, often requiring extensive
iterative convergence and resulting in redundant or insufficient gaussian
representations. To address this, we introduce a novel adaptive densification
method based on Fourier frequency domain analysis to establish gaussian priors
for rapid convergence. Additionally, we propose constructing independent and
unified sparse and dense maps, where a sparse map supports efficient tracking
via Generalized Iterative Closest Point (GICP) and a dense map creates
high-fidelity visual representations. This is the first SLAM system leveraging
frequency domain analysis to achieve high-quality gaussian mapping in
real-time. Experimental results demonstrate an average frame rate of 36 FPS on
Replica and TUM RGB-D datasets, achieving competitive accuracy in both
localization and mapping.



---

## LiteGS: A High-Performance Modular Framework for Gaussian Splatting  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Kaimin Liao | cs.CV | [PDF](http://arxiv.org/pdf/2503.01199v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has emerged as a powerful technique for reconstruction of
3D scenes in computer graphics and vision. However, conventional
implementations often suffer from inefficiencies, limited flexibility, and high
computational overhead, which constrain their adaptability to diverse
applications. In this paper, we present LiteGS,a high-performance and modular
framework that enhances both the efficiency and usability of Gaussian
splatting. LiteGS achieves a 3.4x speedup over the original 3DGS implementation
while reducing GPU memory usage by approximately 30%. Its modular design
decomposes the splatting process into multiple highly optimized operators, and
it provides dual API support via a script-based interface and a CUDA-based
interface. The script-based interface, in combination with autograd, enables
rapid prototyping and straightforward customization of new ideas, while the
CUDA-based interface delivers optimal training speeds for performance-critical
applications. LiteGS retains the core algorithm of 3DGS, ensuring
compatibility. Comprehensive experiments on the Mip-NeRF 360 dataset
demonstrate that LiteGS accelerates training without compromising accuracy,
making it an ideal solution for both rapid prototyping and production
environments.



---

## Category-level Meta-learned NeRF Priors for Efficient Object Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez | cs.CV | [PDF](http://arxiv.org/pdf/2503.01582v2){: .btn .btn-green } |

**Abstract**: In 3D object mapping, category-level priors enable efficient object
reconstruction and canonical pose estimation, requiring only a single prior per
semantic category (e.g., chair, book, laptop). Recently, DeepSDF has
predominantly been used as a category-level shape prior, but it struggles to
reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs
capture fine details but have yet to be effectively integrated with
category-level priors in a real-time multi-object mapping framework. To bridge
this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper
that integrates category-level priors with object-level NeRFs to enhance
reconstruction efficiency while enabling canonical object pose estimation.
PRENOM gets to know objects on a first-name basis by meta-learning on synthetic
reconstruction tasks generated from open-source shape datasets. To account for
object category variations, it employs a multi-objective genetic algorithm to
optimize the NeRF architecture for each category, balancing reconstruction
quality and training time. Additionally, prior-based probabilistic ray sampling
directs sampling toward expected object regions, accelerating convergence and
improving reconstruction quality under constrained resources. Experimental
results on a low-end GPU highlight the ability of PRENOM to achieve
high-quality reconstructions while maintaining computational feasibility.
Specifically, comparisons with prior-free NeRF-based approaches on a synthetic
dataset show a 21% lower Chamfer distance, demonstrating better reconstruction
quality. Furthermore, evaluations against other approaches using shape priors
on a noisy real-world dataset indicate a 13% improvement averaged across all
reconstruction metrics, and comparable pose and size estimation accuracy, while
being trained for 5x less time.



---

## Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, Mohamed Sayed | cs.CV | [PDF](http://arxiv.org/pdf/2503.02009v1){: .btn .btn-green } |

**Abstract**: Exploring real-world spaces using novel-view synthesis is fun, and
reimagining those worlds in a different style adds another layer of excitement.
Stylized worlds can also be used for downstream tasks where there is limited
training data and a need to expand a model's training distribution. Most
current novel-view synthesis stylization techniques lack the ability to
convincingly change geometry. This is because any geometry change requires
increased style strength which is often capped for stylization stability and
consistency. In this work, we propose a new autoregressive 3D Gaussian
Splatting stylization method. As part of this method, we contribute a new RGBD
diffusion model that allows for strength control over appearance and shape
stylization. To ensure consistency across stylized frames, we use a combination
of novel depth-guided cross attention, feature injection, and a Warp ControlNet
conditioned on composite frames for guiding the stylization of new frames. We
validate our method via extensive qualitative results, quantitative
experiments, and a user study. Code will be released online.



---

## Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling | cs.CV | [PDF](http://arxiv.org/pdf/2503.01774v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D
reconstruction and novel-view synthesis task. However, achieving photorealistic
rendering from extreme novel viewpoints remains challenging, as artifacts
persist across representations. In this work, we introduce Difix3D+, a novel
pipeline designed to enhance 3D reconstruction and novel-view synthesis through
single-step diffusion models. At the core of our approach is Difix, a
single-step image diffusion model trained to enhance and remove artifacts in
rendered novel views caused by underconstrained regions of the 3D
representation. Difix serves two critical roles in our pipeline. First, it is
used during the reconstruction phase to clean up pseudo-training views that are
rendered from the reconstruction and then distilled back into 3D. This greatly
enhances underconstrained regions and improves the overall 3D representation
quality. More importantly, Difix also acts as a neural enhancer during
inference, effectively removing residual artifacts arising from imperfect 3D
supervision and the limited capacity of current reconstruction models. Difix3D+
is a general solution, a single model compatible with both NeRF and 3DGS
representations, and it achieves an average 2$\times$ improvement in FID score
over baselines while maintaining 3D consistency.

Comments:
- CVPR 2025

---

## Data Augmentation for NeRFs in the Low Data Limit

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Ayush Gaggar, Todd D. Murphey | cs.CV | [PDF](http://arxiv.org/pdf/2503.02092v1){: .btn .btn-green } |

**Abstract**: Current methods based on Neural Radiance Fields fail in the low data limit,
particularly when training on incomplete scene data. Prior works augment
training data only in next-best-view applications, which lead to hallucinations
and model collapse with sparse data. In contrast, we propose adding a set of
views during training by rejection sampling from a posterior uncertainty
distribution, generated by combining a volumetric uncertainty estimator with
spatial coverage. We validate our results on partially observed scenes; on
average, our method performs 39.9% better with 87.5% less variability across
established scene reconstruction benchmarks, as compared to state of the art
baselines. We further demonstrate that augmenting the training set by sampling
from any distribution leads to better, more consistent scene reconstruction in
sparse environments. This work is foundational for robotic tasks where
augmenting a dataset with informative data is critical in resource-constrained,
a priori unknown environments. Videos and source code are available at
https://murpheylab.github.io/low-data-nerf/.

Comments:
- To be published in 2025 IEEE International Conference on Robotics and
  Automation (ICRA 2025)

---

## OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for  Object-Level Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Dianyi Yang, Yu Gao, Xihan Wang, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.01646v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have significantly improved the
efficiency and quality of dense semantic SLAM. However, previous methods are
generally constrained by limited-category pre-trained classifiers and implicit
semantic representation, which hinder their performance in open-set scenarios
and restrict 3D object-level scene understanding. To address these issues, we
propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian
representation to perform dense semantic SLAM in open-set environments. Our
system integrates explicit semantic labels derived from 2D foundational models
into the 3D Gaussian framework, facilitating robust 3D object-level scene
understanding. We introduce Gaussian Voting Splatting to enable fast 2D label
map rendering and scene updating. Additionally, we propose a Confidence-based
2D Label Consensus method to ensure consistent labeling across multiple views.
Furthermore, we employ a Segmentation Counter Pruning strategy to improve the
accuracy of semantic scene representation. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method
in scene understanding, tracking, and mapping, achieving 10 times faster
semantic rendering and 2 times lower storage costs compared to existing
methods. Project page: https://young-bit.github.io/opengs-github.github.io/.



---

## DreamPrinting: Volumetric Printing Primitives for High-Fidelity 3D  Printing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Youjia Wang, Ruixiang Cao, Teng Xu, Yifei Liu, Dong Zhang, Yiwen Wu, Jingyi Yu | cs.GR | [PDF](http://arxiv.org/pdf/2503.00887v1){: .btn .btn-green } |

**Abstract**: Translating the rich visual fidelity of volumetric rendering techniques into
physically realizable 3D prints remains an open challenge. We introduce
DreamPrinting, a novel pipeline that transforms radiance-based volumetric
representations into explicit, material-centric Volumetric Printing Primitives
(VPPs). While volumetric rendering primitives (e.g., NeRF) excel at capturing
intricate geometry and appearance, they lack the physical constraints necessary
for real-world fabrication, such as pigment compatibility and material density.
DreamPrinting addresses these challenges by integrating the Kubelka-Munk model
with a spectrophotometric calibration process to characterize and mix pigments
for accurate reproduction of color and translucency. The result is a
continuous-to-discrete mapping that determines optimal pigment concentrations
for each voxel, ensuring fidelity to both geometry and optical properties. A 3D
stochastic halftoning procedure then converts these concentrations into
printable labels, enabling fine-grained control over opacity, texture, and
color gradients. Our evaluations show that DreamPrinting achieves exceptional
detail in reproducing semi-transparent structures-such as fur, leaves, and
clouds-while outperforming traditional surface-based methods in managing
translucency and internal consistency. Furthermore, by seamlessly integrating
VPPs with cutting-edge 3D generation techniques, DreamPrinting expands the
potential for complex, high-quality volumetric prints, providing a robust
framework for printing objects that closely mirror their digital origins.



---

## Evolving High-Quality Rendering and Reconstruction in a Unified  Framework with Contribution-Adaptive Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao | cs.CV | [PDF](http://arxiv.org/pdf/2503.00881v1){: .btn .btn-green } |

**Abstract**: Representing 3D scenes from multiview images is a core challenge in computer
vision and graphics, which requires both precise rendering and accurate
reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant
attention for its high-quality rendering and fast inference speed. Yet, due to
the unstructured and irregular nature of Gaussian point clouds, ensuring
accurate geometry reconstruction remains difficult. Existing methods primarily
focus on geometry regularization, with common approaches including
primitive-based and dual-model frameworks. However, the former suffers from
inherent conflicts between rendering and reconstruction, while the latter is
computationally and storage-intensive. To address these challenges, we propose
CarGS, a unified model leveraging Contribution-adaptive regularization to
achieve simultaneous, high-quality rendering and surface reconstruction. The
essence of our framework is learning adaptive contribution for Gaussian
primitives by squeezing the knowledge from geometry regularization into a
compact MLP. Additionally, we introduce a geometry-guided densification
strategy with clues from both normals and Signed Distance Fields (SDF) to
improve the capability of capturing high-frequency details. Our design improves
the mutual learning of the two tasks, meanwhile its unified structure does not
require separate models as in dual-model based approaches, guaranteeing
efficiency. Extensive experiments demonstrate the ability to achieve
state-of-the-art (SOTA) results in both rendering fidelity and reconstruction
accuracy while maintaining real-time speed and minimal storage size.



---

## DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Liao Shen, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy | cs.CV | [PDF](http://arxiv.org/pdf/2503.00746v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable
success in representing 3D scenes and generating high-quality, novel views in
real-time. However, 3D-GS and its variants assume that input images are
captured based on pinhole imaging and are fully in focus. This assumption
limits their applicability, as real-world images often feature shallow
depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable
depth-of-field method for 3D-GS. We develop a lens-based imaging model based on
geometric optics principles to control DoF effects. To ensure accurate scene
geometry, we incorporate depth priors adjusted per scene, and we apply
defocus-to-focus adaptation to minimize the gap in the circle of confusion. We
also introduce a synthetic dataset to assess refocusing capabilities and the
model's ability to learn precise lens parameters. Our framework is customizable
and supports various interactive applications. Extensive experiments confirm
the effectiveness of our method. Our project is available at
https://dof-gaussian.github.io.

Comments:
- CVPR 2025

---

## PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency  Recovery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | BoCheng Li, WenJuan Zhang, Bing Zhang, YiLing Yao, YaNing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.00848v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D GS) achieves impressive results in novel view
synthesis for small, single-object scenes through Gaussian ellipsoid
initialization and adaptive density control. However, when applied to
large-scale remote sensing scenes, 3D GS faces challenges: the point clouds
generated by Structure-from-Motion (SfM) are often sparse, and the inherent
smoothing behavior of 3D GS leads to over-reconstruction in high-frequency
regions, where have detailed textures and color variations. This results in the
generation of large, opaque Gaussian ellipsoids that cause gradient artifacts.
Moreover, the simultaneous optimization of both geometry and texture may lead
to densification of Gaussian ellipsoids at incorrect geometric locations,
resulting in artifacts in other views. To address these issues, we propose
PSRGS, a progressive optimization scheme based on spectral residual maps.
Specifically, we create a spectral residual significance map to separate
low-frequency and high-frequency regions. In the low-frequency region, we apply
depth-aware and depth-smooth losses to initialize the scene geometry with low
threshold. For the high-frequency region, we use gradient features with higher
threshold to split and clone ellipsoids, refining the scene. The sampling rate
is determined by feature responses and gradient loss. Finally, we introduce a
pre-trained network that jointly computes perceptual loss from multiple views,
ensuring accurate restoration of high-frequency details in both Gaussian
ellipsoids geometry and color. We conduct experiments on multiple datasets to
assess the effectiveness of our method, which demonstrates competitive
rendering quality, especially in recovering texture details in high-frequency
regions.



---

## Enhancing Monocular 3D Scene Completion with Diffusion Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Changlin Song, Jiaqi Wang, Liyun Zhu, He Weng | cs.GR | [PDF](http://arxiv.org/pdf/2503.00726v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction is essential for applications in virtual reality,
robotics, and autonomous driving, enabling machines to understand and interact
with complex environments. Traditional 3D Gaussian Splatting techniques rely on
images captured from multiple viewpoints to achieve optimal performance, but
this dependence limits their use in scenarios where only a single image is
available. In this work, we introduce FlashDreamer, a novel approach for
reconstructing a complete 3D scene from a single image, significantly reducing
the need for multi-view inputs. Our approach leverages a pre-trained
vision-language model to generate descriptive prompts for the scene, guiding a
diffusion model to produce images from various perspectives, which are then
fused to form a cohesive 3D reconstruction. Extensive experiments show that our
method effectively and robustly expands single-image inputs into a
comprehensive 3D scene, extending monocular 3D reconstruction capabilities
without further training. Our code is available
https://github.com/CharlieSong1999/FlashDreamer/tree/main.

Comments:
- All authors had equal contribution

---

## Vid2Fluid: 3D Dynamic Fluid Assets from Single-View Videos with  Generative Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Zhiwei Zhao, Alan Zhao, Minchen Li, Yixin Hu | cs.GR | [PDF](http://arxiv.org/pdf/2503.00868v1){: .btn .btn-green } |

**Abstract**: The generation of 3D content from single-view images has been extensively
studied, but 3D dynamic scene generation with physical consistency from videos
remains in its early stages. We propose a novel framework leveraging generative
3D Gaussian Splatting (3DGS) models to extract 3D dynamic fluid objects from
single-view videos. The fluid geometry represented by 3DGS is initially
generated from single-frame images, then denoised, densified, and aligned
across frames. We estimate the fluid surface velocity using optical flow and
compute the mainstream of the fluid to refine it. The 3D volumetric velocity
field is then derived from the enclosed surface. The velocity field is then
converted into a divergence-free, grid-based representation, enabling the
optimization of simulation parameters through its differentiability across
frames. This process results in simulation-ready fluid assets with physical
dynamics closely matching those observed in the source video. Our approach is
applicable to various fluid types, including gas, liquid, and viscous fluids,
and allows users to edit the output geometry or extend movement durations
seamlessly. Our automatic method for creating 3D dynamic fluid assets from
single-view videos, easily obtainable from the internet, shows great potential
for generating large-scale 3D fluid assets at a low cost.



---

## Seeing A 3D World in A Grain of Sand

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yufan Zhang, Yu Ji, Yu Guo, Jinwei Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.00260v1){: .btn .btn-green } |

**Abstract**: We present a snapshot imaging technique for recovering 3D surrounding views
of miniature scenes. Due to their intricacy, miniature scenes with objects
sized in millimeters are difficult to reconstruct, yet miniatures are common in
life and their 3D digitalization is desirable. We design a catadioptric imaging
system with a single camera and eight pairs of planar mirrors for snapshot 3D
reconstruction from a dollhouse perspective. We place paired mirrors on nested
pyramid surfaces for capturing surrounding multi-view images in a single shot.
Our mirror design is customizable based on the size of the scene for optimized
view coverage. We use the 3D Gaussian Splatting (3DGS) representation for scene
reconstruction and novel view synthesis. We overcome the challenge posed by our
sparse view input by integrating visual hull-derived depth constraint. Our
method demonstrates state-of-the-art performance on a variety of synthetic and
real miniature scenes.



---

## CAT-3DGS: A Context-Adaptive Triplane Approach to  Rate-Distortion-Optimized 3DGS Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, Wen-Hsiao Peng | cs.CV | [PDF](http://arxiv.org/pdf/2503.00357v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D
representation. Much research has been focused on reducing its storage
requirements and memory footprint. However, the needs to compress and transmit
the 3DGS representation to the remote side are overlooked. This new application
calls for rate-distortion-optimized 3DGS compression. How to quantize and
entropy encode sparse Gaussian primitives in the 3D space remains largely
unexplored. Few early attempts resort to the hyperprior framework from learned
image compression. But, they fail to utilize fully the inter and intra
correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work,
termed CAT-3DGS, introduces a context-adaptive triplane approach to their
rate-distortion-optimized coding. It features multi-scale triplanes, oriented
according to the principal axes of Gaussian primitives in the 3D space, to
capture their inter correlation (i.e. spatial correlation) for spatial
autoregressive coding in the projected 2D planes. With these triplanes serving
as the hyperprior, we further perform channel-wise autoregressive coding to
leverage the intra correlation within each individual Gaussian primitive. Our
CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively
skips from coding those Gaussian primitives that potentially have little impact
on the rendering quality. When trained end-to-end to strike a good
rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art
compression performance on the commonly used real-world datasets.

Comments:
- Accepted for Publication in International Conference on Learning
  Representations (ICLR)

---

## Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic  Pick-and-Place Setups

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Nicholas Pfaff, Evelyn Fu, Jeremy Binagia, Phillip Isola, Russ Tedrake | cs.RO | [PDF](http://arxiv.org/pdf/2503.00370v1){: .btn .btn-green } |

**Abstract**: Simulating object dynamics from real-world perception shows great promise for
digital twins and robotic manipulation but often demands labor-intensive
measurements and expertise. We present a fully automated Real2Sim pipeline that
generates simulation-ready assets for real-world objects through robotic
interaction. Using only a robot's joint torque sensors and an external camera,
the pipeline identifies visual geometry, collision geometry, and physical
properties such as inertial parameters. Our approach introduces a general
method for extracting high-quality, object-centric meshes from photometric
reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing
alpha-transparent training while explicitly distinguishing foreground
occlusions from background subtraction. We validate the full pipeline through
extensive experiments, demonstrating its effectiveness across diverse objects.
By eliminating the need for manual intervention or environment modifications,
our pipeline can be integrated directly into existing pick-and-place setups,
enabling scalable and efficient dataset creation.



---

## Abstract Rendering: Computing All that is Seen in Gaussian Splat Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yangge Li, Chenxi Ji, Xiangru Zhong, Huan Zhang, Sayan Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2503.00308v2){: .btn .btn-green } |

**Abstract**: We introduce abstract rendering, a method for computing a set of images by
rendering a scene from a continuously varying range of camera positions. The
resulting abstract image-which encodes an infinite collection of possible
renderings-is represented using constraints on the image matrix, enabling
rigorous uncertainty propagation through the rendering process. This capability
is particularly valuable for the formal verification of vision-based autonomous
systems and other safety-critical applications. Our approach operates on
Gaussian splat scenes, an emerging representation in computer vision and
robotics. We leverage efficient piecewise linear bound propagation to abstract
fundamental rendering operations, while addressing key challenges that arise in
matrix inversion and depth sorting-two operations not directly amenable to
standard approximations. To handle these, we develop novel linear relational
abstractions that maintain precision while ensuring computational efficiency.
These abstractions not only power our abstract rendering algorithm but also
provide broadly applicable tools for other rendering problems. Our
implementation, AbstractSplat, is optimized for scalability, handling up to
750k Gaussians while allowing users to balance memory and runtime through tile
and batch-based computation. Compared to the only existing abstract image
method for mesh-based scenes, AbstractSplat achieves 2-14x speedups while
preserving precision. Our results demonstrate that continuous camera motion,
rotations, and scene variations can be rigorously analyzed at scale, making
abstract rendering a powerful tool for uncertainty-aware vision applications.



---

## GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation  Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Runyi Li, Xuanyu Zhang, Chuhan Tong, Zhipei Xu, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.00531v1){: .btn .btn-green } |

**Abstract**: With the advancement of AIGC technologies, the modalities generated by models
have expanded from images and videos to 3D objects, leading to an increasing
number of works focused on 3D Gaussian Splatting (3DGS) generative models.
Existing research on copyright protection for generative models has primarily
concentrated on watermarking in image and text modalities, with little
exploration into the copyright protection of 3D object generative models. In
this paper, we propose the first bit watermarking framework for 3DGS generative
models, named GaussianSeal, to enable the decoding of bits as copyright
identifiers from the rendered outputs of generated 3DGS. By incorporating
adaptive bit modulation modules into the generative model and embedding them
into the network blocks in an adaptive way, we achieve high-precision bit
decoding with minimal training overhead while maintaining the fidelity of the
model's outputs. Experiments demonstrate that our method outperforms
post-processing watermarking approaches for 3DGS objects, achieving superior
performance of watermark decoding accuracy and preserving the quality of the
generated results.


