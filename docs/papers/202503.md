---
layout: default
title: March 2025
parent: Papers
nav_order: 202503
---

<!---metadata--->


## Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency  Modulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Umar Farooq, Jean-Yves Guillemaut, Adrian Hilton, Marco Volino | cs.GR | [PDF](http://arxiv.org/pdf/2503.14475v1){: .btn .btn-green } |

**Abstract**: The field of Novel View Synthesis has been revolutionized by 3D Gaussian
Splatting (3DGS), which enables high-quality scene reconstruction that can be
rendered in real-time. 3DGS-based techniques typically suffer from high GPU
memory and disk storage requirements which limits their practical application
on consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated
coarse-to-fine optimization framework that aims to minimize the number of
Gaussian primitives used to represent a scene, thus reducing memory and storage
demands. Opti3DGS leverages image frequency modulation, initially enforcing a
coarse scene representation and progressively refining it by modulating
frequency details in the training images. On the baseline 3DGS, we demonstrate
an average reduction of 62% in Gaussians, a 40% reduction in the training GPU
memory requirements and a 20% reduction in optimization time without
sacrificing the visual quality. Furthermore, we show that our method integrates
seamlessly with many 3DGS-based techniques, consistently reducing the number of
Gaussian primitives while maintaining, and often improving, visual quality.
Additionally, Opti3DGS inherently produces a level-of-detail scene
representation at no extra cost, a natural byproduct of the optimization
pipeline. Results and code will be made publicly available.



---

## Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Runsong Zhu, Shi Qiu, Zhengzhe Liu, Ka-Hei Hui, Qianyi Wu, Pheng-Ann Heng, Chi-Wing Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.14029v1){: .btn .btn-green } |

**Abstract**: Lifting multi-view 2D instance segmentation to a radiance field has proven to
be effective to enhance 3D understanding. Existing methods rely on direct
matching for end-to-end lifting, yielding inferior results; or employ a
two-stage solution constrained by complex pre- or post-processing. In this
work, we design a new end-to-end object-aware lifting approach, named
Unified-Lift that provides accurate 3D segmentation based on the 3D Gaussian
representation. To start, we augment each Gaussian point with an additional
Gaussian-level feature learned using a contrastive loss to encode instance
information. Importantly, we introduce a learnable object-level codebook to
account for individual objects in the scene for an explicit object-level
understanding and associate the encoded object-level features with the
Gaussian-level point features for segmentation predictions. While promising,
achieving effective codebook learning is non-trivial and a naive solution leads
to degraded performance. Therefore, we formulate the association learning
module and the noisy label filtering module for effective and robust codebook
learning. We conduct experiments on three benchmarks: LERF-Masked, Replica, and
Messy Rooms datasets. Both qualitative and quantitative results manifest that
our Unified-Lift clearly outperforms existing methods in terms of segmentation
quality and time efficiency. The code is publicly available at
\href{https://github.com/Runsong123/Unified-Lift}{https://github.com/Runsong123/Unified-Lift}.

Comments:
- CVPR 2025. The code is publicly available at this https URL
  (https://github.com/Runsong123/Unified-Lift)

---

## Improving Adaptive Density Control for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Glenn Grubert, Florian Barthel, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2503.14274v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become one of the most influential works in
the past year. Due to its efficient and high-quality novel view synthesis
capabilities, it has been widely adopted in many research fields and
applications. Nevertheless, 3DGS still faces challenges to properly manage the
number of Gaussian primitives that are used during scene reconstruction.
Following the adaptive density control (ADC) mechanism of 3D Gaussian
Splatting, new Gaussians in under-reconstructed regions are created, while
Gaussians that do not contribute to the rendering quality are pruned. We
observe that those criteria for densifying and pruning Gaussians can sometimes
lead to worse rendering by introducing artifacts. We especially observe
under-reconstructed background or overfitted foreground regions. To encounter
both problems, we propose three new improvements to the adaptive density
control mechanism. Those include a correction for the scene extent calculation
that does not only rely on camera positions, an exponentially ascending
gradient threshold to improve training convergence, and significance-aware
pruning strategy to avoid background artifacts. With these adaptions, we show
that the rendering quality improves while using the same number of Gaussians
primitives. Furthermore, with our improvements, the training converges
considerably faster, allowing for more than twice as fast training times while
yielding better quality than 3DGS. Finally, our contributions are easily
compatible with most existing derivative works of 3DGS making them relevant for
future works.



---

## Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via  Context Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Mufan Liu, Qi Yang, He Huang, Wenjie Huang, Zhenlong Yuan, Zhu Li, Yiling Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.13948v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as an efficient and high-fidelity
paradigm for novel view synthesis. To adapt 3DGS for dynamic content,
deformable 3DGS incorporates temporally deformable primitives with learnable
latent embeddings to capture complex motions. Despite its impressive
performance, the high-dimensional embeddings and vast number of primitives lead
to substantial storage requirements. In this paper, we introduce a
\textbf{Light}weight \textbf{4}D\textbf{GS} framework, called Light4GS, that
employs significance pruning with a deep context model to provide a lightweight
storage-efficient dynamic 3DGS representation. The proposed Light4GS is based
on 4DGS that is a typical representation of deformable 3DGS. Specifically, our
framework is built upon two core components: (1) a spatio-temporal significance
pruning strategy that eliminates over 64\% of the deformable primitives,
followed by an entropy-constrained spherical harmonics compression applied to
the remainder; and (2) a deep context model that integrates intra- and
inter-prediction with hyperprior into a coarse-to-fine context structure to
enable efficient multiscale latent embedding compression. Our approach achieves
over 120x compression and increases rendering FPS up to 20\% compared to the
baseline 4DGS, and also superior to frame-wise state-of-the-art 3DGS
compression methods, revealing the effectiveness of our Light4GS in terms of
both intra- and inter-prediction methods without sacrificing rendering quality.



---

## Segmentation-Guided Neural Radiance Fields for Novel Street View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Yizhou Li, Yusuke Monno, Masatoshi Okutomi, Yuuichi Tanaka, Seiichi Kataoka, Teruaki Kosiba | cs.CV | [PDF](http://arxiv.org/pdf/2503.14219v1){: .btn .btn-green } |

**Abstract**: Recent advances in Neural Radiance Fields (NeRF) have shown great potential
in 3D reconstruction and novel view synthesis, particularly for indoor and
small-scale scenes. However, extending NeRF to large-scale outdoor environments
presents challenges such as transient objects, sparse cameras and textures, and
varying lighting conditions. In this paper, we propose a segmentation-guided
enhancement to NeRF for outdoor street scenes, focusing on complex urban
environments. Our approach extends ZipNeRF and utilizes Grounded SAM for
segmentation mask generation, enabling effective handling of transient objects,
modeling of the sky, and regularization of the ground. We also introduce
appearance embeddings to adapt to inconsistent lighting across view sequences.
Experimental results demonstrate that our method outperforms the baseline
ZipNeRF, improving novel view synthesis quality with fewer artifacts and
sharper details.

Comments:
- Presented at VISAPP2025. Project page:
  http://www.ok.sc.e.titech.ac.jp/res/NVS/index.html

---

## Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Simon Niedermayr, Christoph Neuhauser RÃ¼diger Westermann | cs.CV | [PDF](http://arxiv.org/pdf/2503.14171v1){: .btn .btn-green } |

**Abstract**: We introduce an image upscaling technique tailored for 3D Gaussian Splatting
(3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher
rendering speeds and reduces artifacts commonly observed in 3DGS
reconstructions. Our technique upscales low-resolution 3DGS renderings with a
marginal increase in cost by directly leveraging the analytical image gradients
of Gaussians for gradient-based bicubic spline interpolation. The technique is
agnostic to the specific 3DGS implementation, achieving novel view synthesis at
rates 3x-4x higher than the baseline implementation. Through extensive
experiments on multiple datasets, we showcase the performance improvements and
high reconstruction fidelity attainable with gradient-aware upscaling of 3DGS
images. We further demonstrate the integration of gradient-aware upscaling into
the gradient-based optimization of a 3DGS model and analyze its effects on
reconstruction quality and performance.



---

## RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from  Sparse Multi-View Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Junjin Xiao, Qing Zhang, Yonewei Nie, Lei Zhu, Wei-Shi Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2503.14198v1){: .btn .btn-green } |

**Abstract**: This paper presents RoGSplat, a novel approach for synthesizing high-fidelity
novel views of unseen human from sparse multi-view images, while requiring no
cumbersome per-subject optimization. Unlike previous methods that typically
struggle with sparse views with few overlappings and are less effective in
reconstructing complex human geometry, the proposed method enables robust
reconstruction in such challenging conditions. Our key idea is to lift SMPL
vertices to dense and reliable 3D prior points representing accurate human body
geometry, and then regress human Gaussian parameters based on the points. To
account for possible misalignment between SMPL model and images, we propose to
predict image-aligned 3D prior points by leveraging both pixel-level features
and voxel-level features, from which we regress the coarse Gaussians. To
enhance the ability to capture high-frequency details, we further render depth
maps from the coarse 3D Gaussians to help regress fine-grained pixel-wise
Gaussians. Experiments on several benchmark datasets demonstrate that our
method outperforms state-of-the-art methods in novel view synthesis and
cross-dataset generalization. Our code is available at
https://github.com/iSEE-Laboratory/RoGSplat.

Comments:
- Accepted to CVPR2025

---

## Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near  Real-Time 3DGS Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Yiwei Xu, Yifei Yu, Wentian Gan, Tengfei Wang, Zongqian Zhan, Hao Cheng, Xin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.13086v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast
real-time performance, but existing methods rely on offline training after full
Structure-from-Motion (SfM) processing. In contrast, this work introduces
On-the-Fly GS, a progressive framework enabling near real-time 3DGS
optimization during image capture. As each image arrives, its pose and sparse
points are updated via on-the-fly SfM, and newly optimized Gaussians are
immediately integrated into the 3DGS field. We propose a progressive local
optimization strategy to prioritize new images and their neighbors by their
corresponding overlapping relationship, allowing the new image and its
overlapping images to get more training. To further stabilize training across
old and new images, an adaptive learning rate schedule balances the iterations
and the learning rate. Moreover, to maintain overall quality of the 3DGS field,
an efficient global optimization scheme prevents overfitting to the newly added
images. Experiments on multiple benchmark datasets show that our On-the-Fly GS
reduces training time significantly, optimizing each new image in seconds with
minimal rendering loss, offering the first practical step toward rapid,
progressive 3DGS reconstruction.



---

## CAT-3DGS Pro: A New Benchmark for Efficient 3DGS Compression

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Yu-Ting Zhan, He-bi Yang, Cheng-Yuan Ho, Jui-Chiu Chiang, Wen-Hsiao Peng | cs.CV | [PDF](http://arxiv.org/pdf/2503.12862v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown immense potential for novel view
synthesis. However, achieving rate-distortion-optimized compression of 3DGS
representations for transmission and/or storage applications remains a
challenge. CAT-3DGS introduces a context-adaptive triplane hyperprior for
end-to-end optimized compression, delivering state-of-the-art coding
performance. Despite this, it requires prolonged training and decoding time. To
address these limitations, we propose CAT-3DGS Pro, an enhanced version of
CAT-3DGS that improves both compression performance and computational
efficiency. First, we introduce a PCA-guided vector-matrix hyperprior, which
replaces the triplane-based hyperprior to reduce redundant parameters. To
achieve a more balanced rate-distortion trade-off and faster encoding, we
propose an alternate optimization strategy (A-RDO). Additionally, we refine the
sampling rate optimization method in CAT-3DGS, leading to significant
improvements in rate-distortion performance. These enhancements result in a
46.6% BD-rate reduction and 3x speedup in training time on BungeeNeRF, while
achieving 5x acceleration in decoding speed for the Amsterdam scene compared to
CAT-3DGS.



---

## DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency  for Few-shot View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Ingyun Lee, Jae Won Jang, Seunghyeon Seo, Nojun Kwak | cs.CV | [PDF](http://arxiv.org/pdf/2503.12947v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has shown remarkable performance in novel view
synthesis but requires many multiview images, making it impractical for
few-shot scenarios. Ray augmentation was proposed to prevent overfitting for
sparse training data by generating additional rays. However, existing methods,
which generate augmented rays only near the original rays, produce severe
floaters and appearance distortion due to limited viewpoints and inconsistent
rays obstructed by nearby obstacles and complex surfaces. To address these
problems, we propose DivCon-NeRF, which significantly enhances both diversity
and consistency. It employs surface-sphere augmentation, which preserves the
distance between the original camera and the predicted surface point. This
allows the model to compare the order of high-probability surface points and
filter out inconsistent rays easily without requiring the exact depth. By
introducing inner-sphere augmentation, DivCon-NeRF randomizes angles and
distances for diverse viewpoints, further increasing diversity. Consequently,
our method significantly reduces floaters and visual distortions, achieving
state-of-the-art performance on the Blender, LLFF, and DTU datasets. Our code
will be publicly available.

Comments:
- 11 pages, 6 figures

---

## AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Hadam Baek, Hannie Shin, Jiyoung Seo, Chanwoo Kim, Saerom Kim, Hyeongbok Kim, Sangpil Kim | cs.MM | [PDF](http://arxiv.org/pdf/2503.12806v1){: .btn .btn-green } |

**Abstract**: Accurately modeling sound propagation with complex real-world environments is
essential for Novel View Acoustic Synthesis (NVAS). While previous studies have
leveraged visual perception to estimate spatial acoustics, the combined use of
surface normal and structural details from 3D representations in acoustic
modeling has been underexplored. Given their direct impact on sound wave
reflections and propagation, surface normals should be jointly modeled with
structural details to achieve accurate spatial acoustics. In this paper, we
propose a surface-enhanced geometry-aware approach for NVAS to improve spatial
acoustic modeling. To achieve this, we exploit geometric priors, such as image,
depth map, surface normals, and point clouds obtained using a 3D Gaussian
Splatting (3DGS) based framework. We introduce a dual cross-attention-based
transformer integrating geometrical constraints into frequency query to
understand the surroundings of the emitter. Additionally, we design a
ConvNeXt-based spectral features processing network called Spectral Refinement
Network (SRN) to synthesize realistic binaural audio. Experimental results on
the RWAVS and SoundSpace datasets highlight the necessity of our approach, as
it surpasses existing methods in novel view acoustic synthesis.



---

## DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for  Distractor-free 3D Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Rui Wang, Quentin Lohmeyer, Mirko Meboldt, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2503.13176v1){: .btn .btn-green } |

**Abstract**: Reconstructing clean, distractor-free 3D scenes from real-world captures
remains a significant challenge, particularly in highly dynamic and cluttered
settings such as egocentric videos. To tackle this problem, we introduce
DeGauss, a simple and robust self-supervised framework for dynamic scene
reconstruction based on a decoupled dynamic-static Gaussian Splatting design.
DeGauss models dynamic elements with foreground Gaussians and static content
with background Gaussians, using a probabilistic mask to coordinate their
composition and enable independent yet complementary optimization. DeGauss
generalizes robustly across a wide range of real-world scenarios, from casual
image collections to long, dynamic egocentric videos, without relying on
complex heuristics or extensive supervision. Experiments on benchmarks
including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that
DeGauss consistently outperforms existing methods, establishing a strong
baseline for generalizable, distractor-free 3D reconstructionin highly dynamic,
interaction-rich environments.



---

## TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing  Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Jiaming Kang, Keyan Chen, Zhengxia Zou, Zhenwei Shi | cs.CV | [PDF](http://arxiv.org/pdf/2503.13347v1){: .btn .btn-green } |

**Abstract**: Remote sensing novel view synthesis (NVS) offers significant potential for 3D
interpretation of remote sensing scenes, with important applications in urban
planning and environmental monitoring. However, remote sensing scenes
frequently lack sufficient multi-view images due to acquisition constraints.
While existing NVS methods tend to overfit when processing limited input views,
advanced few-shot NVS methods are computationally intensive and perform
sub-optimally in remote sensing scenes. This paper presents TriDF, an efficient
hybrid 3D representation for fast remote sensing NVS from as few as 3 input
views. Our approach decouples color and volume density information, modeling
them independently to reduce the computational burden on implicit radiance
fields and accelerate reconstruction. We explore the potential of the triplane
representation in few-shot NVS tasks by mapping high-frequency color
information onto this compact structure, and the direct optimization of feature
planes significantly speeds up convergence. Volume density is modeled as
continuous density fields, incorporating reference features from neighboring
views through image-based rendering to compensate for limited input data.
Additionally, we introduce depth-guided optimization based on point clouds,
which effectively mitigates the overfitting problem in few-shot NVS.
Comprehensive experiments across multiple remote sensing scenes demonstrate
that our hybrid representation achieves a 30x speed increase compared to
NeRF-based methods, while simultaneously improving rendering quality metrics
over advanced few-shot methods (7.4% increase in PSNR, 12.2% in SSIM, and 18.7%
in LPIPS). The code is publicly available at https://github.com/kanehub/TriDF



---

## Improving Geometric Consistency for 360-Degree Neural Radiance Fields in  Indoor Scenarios

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Iryna Repinetska, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2503.13710v1){: .btn .btn-green } |

**Abstract**: Photo-realistic rendering and novel view synthesis play a crucial role in
human-computer interaction tasks, from gaming to path planning. Neural Radiance
Fields (NeRFs) model scenes as continuous volumetric functions and achieve
remarkable rendering quality. However, NeRFs often struggle in large,
low-textured areas, producing cloudy artifacts known as ''floaters'' that
reduce scene realism, especially in indoor environments with featureless
architectural surfaces like walls, ceilings, and floors. To overcome this
limitation, prior work has integrated geometric constraints into the NeRF
pipeline, typically leveraging depth information derived from Structure from
Motion or Multi-View Stereo. Yet, conventional RGB-feature correspondence
methods face challenges in accurately estimating depth in textureless regions,
leading to unreliable constraints. This challenge is further complicated in
360-degree ''inside-out'' views, where sparse visual overlap between adjacent
images further hinders depth estimation. In order to address these issues, we
propose an efficient and robust method for computing dense depth priors,
specifically tailored for large low-textured architectural surfaces in indoor
environments. We introduce a novel depth loss function to enhance rendering
quality in these challenging, low-feature regions, while complementary
depth-patch regularization further refines depth consistency across other
areas. Experiments with Instant-NGP on two synthetic 360-degree indoor scenes
demonstrate improved visual fidelity with our method compared to standard
photometric loss and Mean Squared Error depth supervision.



---

## Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion  Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Katja Schwarz, Norman Mueller, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2503.13272v1){: .btn .btn-green } |

**Abstract**: Synthesizing consistent and photorealistic 3D scenes is an open problem in
computer vision. Video diffusion models generate impressive videos but cannot
directly synthesize 3D representations, i.e., lack 3D consistency in the
generated sequences. In addition, directly training generative 3D models is
challenging due to a lack of 3D training data at scale. In this work, we
present Generative Gaussian Splatting (GGS) -- a novel approach that integrates
a 3D representation with a pre-trained latent video diffusion model.
Specifically, our model synthesizes a feature field parameterized via 3D
Gaussian primitives. The feature field is then either rendered to feature maps
and decoded into multi-view images, or directly upsampled into a 3D radiance
field. We evaluate our approach on two common benchmark datasets for scene
synthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model
significantly improves both the 3D consistency of the generated multi-view
images, and the quality of the generated 3D scenes over all relevant baselines.
Compared to a similar model without 3D representation, GGS improves FID on the
generated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page:
https://katjaschwarz.github.io/ggs/



---

## CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2503.12836v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D
reconstruction and novel view synthesis, leading to its widespread commercial
use. Consequently, copyright protection via watermarking has become critical.
However, because 3DGS relies on millions of Gaussians, which require gigabytes
of storage, efficient transfer and storage require compression. Existing 3DGS
watermarking methods are vulnerable to quantization-based compression, often
resulting in the loss of the embedded watermark. To address this challenge, we
propose a novel watermarking method that ensures watermark robustness after
model compression while maintaining high rendering quality. In detail, we
incorporate a quantization distortion layer that simulates compression during
training, preserving the watermark under quantization-based compression. Also,
we propose a learnable watermark embedding feature that embeds the watermark
into the anchor feature, ensuring structural consistency and seamless
integration into the 3D scene. Furthermore, we present a frequency-aware anchor
growing mechanism to enhance image quality in high-frequency regions by
effectively identifying Guassians within these regions. Experimental results
confirm that our method preserves the watermark and maintains superior image
quality under high compression, validating it as a promising approach for a
secure 3DGS model.

Comments:
- 23 pages, 17 figures

---

## SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor  Open-World Free-view Synthesis from Sparse Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Guibiao Liao, Qing Li, Zhenyu Bao, Guoping Qiu, Kanglin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.12535v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting-based indoor open-world free-view synthesis approaches
have shown significant performance with dense input images. However, they
exhibit poor performance when confronted with sparse inputs, primarily due to
the sparse distribution of Gaussian points and insufficient view supervision.
To relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based
Gaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)
Regularization for open-world free view synthesis with sparse inputs.
Specifically, SGI provides a dense, scene-layout-based Gaussian distribution by
utilizing view-changed images generated from the video generation model and
view-constraint Gaussian points densification. Additionally, SPC mitigates
limited view supervision by employing semantic-prompt-based consistency
constraints developed by SAM2. This approach leverages available semantics from
training views, serving as instructive prompts, to optimize visually
overlapping regions in novel views with 2D and 3D consistency constraints.
Extensive experiments demonstrate the superior performance of SPC-GS across
Replica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in
PSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world
semantic segmentation.

Comments:
- Accepted by CVPR2025. The project page is available at
  https://gbliao.github.io/SPC-GS.github.io/

---

## Deblur Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Francesco Girlanda, Denys Rozumnyi, Marc Pollefeys, Martin R. Oswald | cs.CV | [PDF](http://arxiv.org/pdf/2503.12572v1){: .btn .btn-green } |

**Abstract**: We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp
reconstructions from motion-blurred inputs. The proposed method bridges the
strengths of both frame-to-frame and frame-to-model approaches to model
sub-frame camera trajectories that lead to high-fidelity reconstructions in
motion-blurred settings. Moreover, our pipeline incorporates techniques such as
online loop closure and global bundle adjustment to achieve a dense and precise
global trajectory. We model the physical image formation process of
motion-blurred images and minimize the error between the observed blurry images
and rendered blurry images obtained by averaging sharp virtual sub-frame
images. Additionally, by utilizing a monocular depth estimator alongside the
online deformation of Gaussians, we ensure precise mapping and enhanced image
deblurring. The proposed SLAM pipeline integrates all these components to
improve the results. We achieve state-of-the-art results for sharp map
estimation and sub-frame trajectory recovery both on synthetic and real-world
blurry input data.



---

## MTGS: Multi-Traversal Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Tianyu Li, Yihang Qiu, Zhenhua Wu, Carl LindstrÃ¶m, Peng Su, Matthias NieÃner, Hongyang Li | cs.CV | [PDF](http://arxiv.org/pdf/2503.12552v1){: .btn .btn-green } |

**Abstract**: Multi-traversal data, commonly collected through daily commutes or by
self-driving fleets, provides multiple viewpoints for scene reconstruction
within a road block. This data offers significant potential for high-quality
novel view synthesis, which is crucial for applications such as autonomous
vehicle simulators. However, inherent challenges in multi-traversal data often
result in suboptimal reconstruction quality, including variations in appearance
and the presence of dynamic objects. To address these issues, we propose
Multi-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs
high-quality driving scenes from arbitrarily collected multi-traversal data by
modeling a shared static geometry while separately handling dynamic elements
and appearance variations. Our method employs a multi-traversal dynamic scene
graph with a shared static node and traversal-specific dynamic nodes,
complemented by color correction nodes with learnable spherical harmonics
coefficient residuals. This approach enables high-fidelity novel view synthesis
and provides flexibility to navigate any viewpoint. We conduct extensive
experiments on a large-scale driving dataset, nuPlan, with multi-traversal
data. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry
accuracy by 46.3% compared to single-traversal baselines. The code and data
would be available to the public.



---

## TopoGaussian: Inferring Internal Topology Structures from Visual Clues

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Xiaoyu Xiong, Changyu Hu, Chunru Lin, Pingchuan Ma, Chuang Gan, Tao Du | cs.CV | [PDF](http://arxiv.org/pdf/2503.12343v1){: .btn .btn-green } |

**Abstract**: We present TopoGaussian, a holistic, particle-based pipeline for inferring
the interior structure of an opaque object from easily accessible photos and
videos as input. Traditional mesh-based approaches require tedious and
error-prone mesh filling and fixing process, while typically output rough
boundary surface. Our pipeline combines Gaussian Splatting with a novel,
versatile particle-based differentiable simulator that simultaneously
accommodates constitutive model, actuator, and collision, without interference
with mesh. Based on the gradients from this simulator, we provide flexible
choice of topology representation for optimization, including particle, neural
implicit surface, and quadratic surface. The resultant pipeline takes easily
accessible photos and videos as input and outputs the topology that matches the
physical characteristics of the input. We demonstrate the efficacy of our
pipeline on a synthetic dataset and four real-world tasks with 3D-printed
prototypes. Compared with existing mesh-based method, our pipeline is 5.26x
faster on average with improved shape quality. These results highlight the
potential of our pipeline in 3D vision, soft robotics, and manufacturing
applications.



---

## GS-I$^{3}$: Gaussian Splatting for Surface Reconstruction from  Illumination-Inconsistent Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Tengfei Wang, Yongmao Hou, Zhaoning Zhang, Yiwei Xu, Zongqian Zhan, Xin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.12335v2){: .btn .btn-green } |

**Abstract**: Accurate geometric surface reconstruction, providing essential environmental
information for navigation and manipulation tasks, is critical for enabling
robotic self-exploration and interaction. Recently, 3D Gaussian Splatting
(3DGS) has gained significant attention in the field of surface reconstruction
due to its impressive geometric quality and computational efficiency. While
recent relevant advancements in novel view synthesis under inconsistent
illumination using 3DGS have shown promise, the challenge of robust surface
reconstruction under such conditions is still being explored. To address this
challenge, we propose a method called GS-3I. Specifically, to mitigate 3D
Gaussian optimization bias caused by underexposed regions in single-view
images, based on Convolutional Neural Network (CNN), a tone mapping correction
framework is introduced. Furthermore, inconsistent lighting across multi-view
images, resulting from variations in camera settings and complex scene
illumination, often leads to geometric constraint mismatches and deviations in
the reconstructed surface. To overcome this, we propose a normal compensation
mechanism that integrates reference normals extracted from single-view image
with normals computed from multi-view observations to effectively constrain
geometric inconsistencies. Extensive experimental evaluations demonstrate that
GS-3I can achieve robust and accurate surface reconstruction across complex
illumination scenarios, highlighting its effectiveness and versatility in this
critical challenge. https://github.com/TFwang-9527/GS-3I

Comments:
- Comments: This work has been submitted to the 2025 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2025) for
  possible publication

---

## VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Songen Gu, Haoxuan Song, Binjie Liu, Qian Yu, Sanyi Zhang, Haiyong Jiang, Jin Huang, Feng Tian | cs.CV | [PDF](http://arxiv.org/pdf/2503.12383v1){: .btn .btn-green } |

**Abstract**: We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native
3D object generation framework that incorporates a 3D Gaussian Splatting
representation. As part of our work, we introduce VRSS, the first large-scale
paired dataset containing VR sketches, text, images, and 3DGS, bridging the gap
in multi-modal VR sketch-based generation. Our approach features the following
key innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage
alignment strategy that bridges the domain gap between sparse VR sketch
embeddings and rich CLIP embeddings, facilitating both VR sketch-based
retrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We
disentangle the 3D generation process by using explicit VR sketches for
geometric conditioning and text descriptions for appearance control. To
facilitate this, we propose a generalizable VR sketch encoder that effectively
aligns different modalities. 3) Efficient and high-fidelity 3D native
generation. Our method leverages a 3D-native generation approach that enables
fast and texture-rich 3D object synthesis. Experiments conducted on our VRSS
dataset demonstrate that our method achieves high-quality, multi-modal VR
sketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian
method will be beneficial for the 3D generation community.



---

## Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and  efficient reconstruction of dynamic scene

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Jiahao Wu, Rui Peng, Zhiyan Wang, Lu Xiao, Luyang Tang, Jinbo Yan, Kaiqiang Xiong, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.12307v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis has long been a practical but challenging task, although
the introduction of numerous methods to solve this problem, even combining
advanced representations like 3D Gaussian Splatting, they still struggle to
recover high-quality results and often consume too much storage memory and
training time. In this paper we propose Swift4D, a divide-and-conquer 3D
Gaussian Splatting method that can handle static and dynamic primitives
separately, achieving a good trade-off between rendering quality and
efficiency, motivated by the fact that most of the scene is the static
primitive and does not require additional dynamic properties. Concretely, we
focus on modeling dynamic transformations only for the dynamic primitives which
benefits both efficiency and quality. We first employ a learnable decomposition
strategy to separate the primitives, which relies on an additional parameter to
classify primitives as static or dynamic. For the dynamic primitives, we employ
a compact multi-resolution 4D Hash mapper to transform these primitives from
canonical space into deformation space at each timestamp, and then mix the
static and dynamic primitives to produce the final output. This
divide-and-conquer method facilitates efficient training and reduces storage
redundancy. Our method not only achieves state-of-the-art rendering quality
while being 20X faster in training than previous SOTA methods with a minimum
storage requirement of only 30MB on real-world datasets. Code is available at
https://github.com/WuJH2001/swift4d.

Comments:
- ICLR 2025

---

## REdiSplats: Ray Tracing for Editable Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Krzysztof Byrski, Grzegorz WilczyÅski, Weronika Smolak-DyÅ¼ewska, Piotr Borycki, Dawid Baran, SÅawomir Tadeja, PrzemysÅaw Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2503.12284v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has become one of the most important neural rendering
algorithms. GS represents 3D scenes using Gaussian components with trainable
color and opacity. This representation achieves high-quality renderings with
fast inference. Regrettably, it is challenging to integrate such a solution
with varying light conditions, including shadows and light reflections, manual
adjustments, and a physical engine. Recently, a few approaches have appeared
that incorporate ray-tracing or mesh primitives into GS to address some of
these caveats. However, no such solution can simultaneously solve all the
existing limitations of the classical GS. Consequently, we introduce
REdiSplats, which employs ray tracing and a mesh-based representation of flat
3D Gaussians. In practice, we model the scene using flat Gaussian distributions
parameterized by the mesh. We can leverage fast ray tracing and control
Gaussian modification by adjusting the mesh vertices. Moreover, REdiSplats
allows modeling of light conditions, manual adjustments, and physical
simulation. Furthermore, we can render our models using 3D tools such as
Blender or Nvdiffrast, which opens the possibility of integrating them with all
existing 3D graphics techniques dedicated to mesh representations.



---

## FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Rui Qian, Chenyangguang Zhang, Yan Di, Guangyao Zhai, Ruida Zhang, Jiayu Guo, Benjamin Busam, Jian Pu | cs.CV | [PDF](http://arxiv.org/pdf/2503.12086v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have exhibited highly effective performance for
photorealistic novel view synthesis recently. However, the key limitation it
meets is the reliance on a hand-crafted frequency annealing strategy to recover
3D scenes with imperfect camera poses. The strategy exploits a temporal
low-pass filter to guarantee convergence while decelerating the joint
optimization of implicit scene reconstruction and camera registration. In this
work, we introduce the Frequency Adapted Bundle Adjusting Radiance Field
(FA-BARF), substituting the temporal low-pass filter for a frequency-adapted
spatial low-pass filter to address the decelerating problem. We establish a
theoretical framework to interpret the relationship between position encoding
of NeRF and camera registration and show that our frequency-adapted filter can
mitigate frequency fluctuation caused by the temporal filter. Furthermore, we
show that applying a spatial low-pass filter in NeRF can optimize camera poses
productively through radial uncertainty overlaps among various views. Extensive
experiments show that FA-BARF can accelerate the joint optimization process
under little perturbations in object-centric scenes and recover real-world
scenes with unknown camera poses. This implies wider possibilities for NeRF
applied in dense 3D mapping and reconstruction under real-time requirements.
The code will be released upon paper acceptance.



---

## 3D Gaussian Splatting against Moving Objects for High-Fidelity Street  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Peizhen Zheng, Longfei Wei, Dongjing Jiang, Jianfei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.12001v2){: .btn .btn-green } |

**Abstract**: The accurate reconstruction of dynamic street scenes is critical for
applications in autonomous driving, augmented reality, and virtual reality.
Traditional methods relying on dense point clouds and triangular meshes
struggle with moving objects, occlusions, and real-time processing constraints,
limiting their effectiveness in complex urban environments. While multi-view
stereo and neural radiance fields have advanced 3D reconstruction, they face
challenges in computational efficiency and handling scene dynamics. This paper
proposes a novel 3D Gaussian point distribution method for dynamic street scene
reconstruction. Our approach introduces an adaptive transparency mechanism that
eliminates moving objects while preserving high-fidelity static scene details.
Additionally, iterative refinement of Gaussian point distribution enhances
geometric accuracy and texture representation. We integrate directional
encoding with spatial position optimization to optimize storage and rendering
efficiency, reducing redundancy while maintaining scene integrity. Experimental
results demonstrate that our method achieves high reconstruction quality,
improved rendering performance, and adaptability in large-scale dynamic
environments. These contributions establish a robust framework for real-time,
high-precision 3D reconstruction, advancing the practicality of dynamic scene
modeling across multiple applications.



---

## DecompDreamer: Advancing Structured 3D Asset Generation with  Multi-Object Decomposition and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Utkarsh Nath, Rajeev Goel, Rahul Khurana, Kyle Min, Mark Ollila, Pavan Turaga, Varun Jampani, Tejaswi Gowda | cs.CV | [PDF](http://arxiv.org/pdf/2503.11981v1){: .btn .btn-green } |

**Abstract**: Text-to-3D generation saw dramatic advances in recent years by leveraging
Text-to-Image models. However, most existing techniques struggle with
compositional prompts, which describe multiple objects and their spatial
relationships. They often fail to capture fine-grained inter-object
interactions. We introduce DecompDreamer, a Gaussian splatting-based training
routine designed to generate high-quality 3D compositions from such complex
prompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose
scenes into structured components and their relationships. We propose a
progressive optimization strategy that first prioritizes joint relationship
modeling before gradually shifting toward targeted object refinement. Our
qualitative and quantitative evaluations against state-of-the-art text-to-3D
models demonstrate that DecompDreamer effectively generates intricate 3D
compositions with superior object disentanglement, offering enhanced control
and flexibility in 3D generation. Project page :
https://decompdreamer3d.github.io



---

## DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering,  Tracking, Motion Predictions of Moving Objects in Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Runfa Blark Li, Mahdi Shaghaghi, Keito Suzuki, Xinshuang Liu, Varun Moparthi, Bang Du, Walker Curtis, Martin Renschler, Ki Myung Brian Lee, Nikolay Atanasov, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2503.11979v1){: .btn .btn-green } |

**Abstract**: Simultaneous Localization and Mapping (SLAM) is one of the most important
environment-perception and navigation algorithms for computer vision, robotics,
and autonomous cars/drones. Hence, high quality and fast mapping becomes a
fundamental problem. With the advent of 3D Gaussian Splatting (3DGS) as an
explicit representation with excellent rendering quality and speed,
state-of-the-art (SOTA) works introduce GS to SLAM. Compared to classical
pointcloud-SLAM, GS-SLAM generates photometric information by learning from
input camera views and synthesize unseen views with high-quality textures.
However, these GS-SLAM fail when moving objects occupy the scene that violate
the static assumption of bundle adjustment. The failed updates of moving GS
affects the static GS and contaminates the full map over long frames. Although
some efforts have been made by concurrent works to consider moving objects for
GS-SLAM, they simply detect and remove the moving regions from GS rendering
("anti'' dynamic GS-SLAM), where only the static background could benefit from
GS. To this end, we propose the first real-time GS-SLAM, "DynaGSLAM'', that
achieves high-quality online GS rendering, tracking, motion predictions of
moving objects in dynamic scenes while jointly estimating accurate ego motion.
Our DynaGSLAM outperforms SOTA static & "Anti'' dynamic GS-SLAM on three
dynamic real datasets, while keeping speed and memory efficiency in practice.



---

## EgoSplat: Open-Vocabulary Egocentric Scene Understanding with Language  Embedded 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-14 | Di Li, Jie Feng, Jiahao Chen, Weisheng Dong, Guanbin Li, Guangming Shi, Licheng Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2503.11345v1){: .btn .btn-green } |

**Abstract**: Egocentric scenes exhibit frequent occlusions, varied viewpoints, and dynamic
interactions compared to typical scene understanding tasks. Occlusions and
varied viewpoints can lead to multi-view semantic inconsistencies, while
dynamic objects may act as transient distractors, introducing artifacts into
semantic feature modeling. To address these challenges, we propose EgoSplat, a
language-embedded 3D Gaussian Splatting framework for open-vocabulary
egocentric scene understanding. A multi-view consistent instance feature
aggregation method is designed to leverage the segmentation and tracking
capabilities of SAM2 to selectively aggregate complementary features across
views for each instance, ensuring precise semantic representation of scenes.
Additionally, an instance-aware spatial-temporal transient prediction module is
constructed to improve spatial integrity and temporal continuity in predictions
by incorporating spatial-temporal associations across multi-view instances,
effectively reducing artifacts in the semantic reconstruction of egocentric
scenes. EgoSplat achieves state-of-the-art performance in both localization and
segmentation tasks on two datasets, outperforming existing methods with a 8.2%
improvement in localization accuracy and a 3.7% improvement in segmentation
mIoU on the ADT dataset, and setting a new benchmark in open-vocabulary
egocentric scene understanding. The code will be made publicly available.



---

## Uncertainty-Aware Normal-Guided Gaussian Splatting for Surface  Reconstruction from Sparse Image Sequences

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-14 | Zhen Tan, Xieyuanli Chen, Jinpu Zhang, Lei Feng, Dewen Hu | cs.CV | [PDF](http://arxiv.org/pdf/2503.11172v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has achieved impressive rendering performance in
novel view synthesis. However, its efficacy diminishes considerably in sparse
image sequences, where inherent data sparsity amplifies geometric uncertainty
during optimization. This often leads to convergence at suboptimal local
minima, resulting in noticeable structural artifacts in the reconstructed
scenes.To mitigate these issues, we propose Uncertainty-aware Normal-Guided
Gaussian Splatting (UNG-GS), a novel framework featuring an explicit Spatial
Uncertainty Field (SUF) to quantify geometric uncertainty within the 3DGS
pipeline. UNG-GS enables high-fidelity rendering and achieves high-precision
reconstruction without relying on priors. Specifically, we first integrate
Gaussian-based probabilistic modeling into the training of 3DGS to optimize the
SUF, providing the model with adaptive error tolerance. An uncertainty-aware
depth rendering strategy is then employed to weight depth contributions based
on the SUF, effectively reducing noise while preserving fine details.
Furthermore, an uncertainty-guided normal refinement method adjusts the
influence of neighboring depth values in normal estimation, promoting robust
results. Extensive experiments demonstrate that UNG-GS significantly
outperforms state-of-the-art methods in both sparse and dense sequences. The
code will be open-source.

Comments:
- 12 pages, 8 figures

---

## Advancing 3D Gaussian Splatting Editing with Complementary and Consensus  Information

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-14 | Xuanqi Zhang, Jieun Lee, Chris Joslin, Wonsook Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.11601v1){: .btn .btn-green } |

**Abstract**: We present a novel framework for enhancing the visual fidelity and
consistency of text-guided 3D Gaussian Splatting (3DGS) editing. Existing
editing approaches face two critical challenges: inconsistent geometric
reconstructions across multiple viewpoints, particularly in challenging camera
positions, and ineffective utilization of depth information during image
manipulation, resulting in over-texture artifacts and degraded object
boundaries. To address these limitations, we introduce: 1) A complementary
information mutual learning network that enhances depth map estimation from
3DGS, enabling precise depth-conditioned 3D editing while preserving geometric
structures. 2) A wavelet consensus attention mechanism that effectively aligns
latent codes during the diffusion denoising process, ensuring multi-view
consistency in the edited results. Through extensive experimentation, our
method demonstrates superior performance in rendering quality and view
consistency compared to state-of-the-art approaches. The results validate our
framework as an effective solution for text-guided editing of 3D scenes.

Comments:
- 7 pages, 9 figures

---

## RI3D: Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion  Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Avinash Paliwal, Xilong Zhou, Wei Ye, Jinhui Xiong, Rakesh Ranjan, Nima Khademi Kalantari | cs.CV | [PDF](http://arxiv.org/pdf/2503.10860v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose RI3D, a novel 3DGS-based approach that harnesses
the power of diffusion models to reconstruct high-quality novel views given a
sparse set of input images. Our key contribution is separating the view
synthesis process into two tasks of reconstructing visible regions and
hallucinating missing regions, and introducing two personalized diffusion
models, each tailored to one of these tasks. Specifically, one model ('repair')
takes a rendered image as input and predicts the corresponding high-quality
image, which in turn is used as a pseudo ground truth image to constrain the
optimization. The other model ('inpainting') primarily focuses on hallucinating
details in unobserved areas. To integrate these models effectively, we
introduce a two-stage optimization strategy: the first stage reconstructs
visible areas using the repair model, and the second stage reconstructs missing
regions with the inpainting model while ensuring coherence through further
optimization. Moreover, we augment the optimization with a novel Gaussian
initialization method that obtains per-image depth by combining 3D-consistent
and smooth depth with highly detailed relative depth. We demonstrate that by
separating the process into two tasks and addressing them with the repair and
inpainting models, we produce results with detailed textures in both visible
and missing regions that outperform state-of-the-art approaches on a diverse
set of scenes with extremely sparse inputs.

Comments:
- Project page: https://people.engr.tamu.edu/nimak/Papers/RI3D, Code:
  https://github.com/avinashpaliwal/RI3D

---

## VicaSplat: A Single Run is All You Need for 3D Gaussian Splatting and  Camera Estimation from Unposed Video Frames

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Zhiqi Li, Chengrui Dong, Yiming Chen, Zhangchi Huang, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.10286v1){: .btn .btn-green } |

**Abstract**: We present VicaSplat, a novel framework for joint 3D Gaussians reconstruction
and camera pose estimation from a sequence of unposed video frames, which is a
critical yet underexplored task in real-world 3D applications. The core of our
method lies in a novel transformer-based network architecture. In particular,
our model starts with an image encoder that maps each image to a list of visual
tokens. All visual tokens are concatenated with additional inserted learnable
camera tokens. The obtained tokens then fully communicate with each other
within a tailored transformer decoder. The camera tokens causally aggregate
features from visual tokens of different views, and further modulate them
frame-wisely to inject view-dependent features. 3D Gaussian splats and camera
pose parameters can then be estimated via different prediction heads.
Experiments show that VicaSplat surpasses baseline methods for multi-view
inputs, and achieves comparable performance to prior two-view approaches.
Remarkably, VicaSplat also demonstrates exceptional cross-dataset
generalization capability on the ScanNet benchmark, achieving superior
performance without any fine-tuning. Project page:
https://lizhiqi49.github.io/VicaSplat.



---

## ROODI: Reconstructing Occluded Objects with Denoising Inpainters

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Yeonjin Chang, Erqun Dong, Seunghyeon Seo, Nojun Kwak, Kwang Moo Yi | cs.CV | [PDF](http://arxiv.org/pdf/2503.10256v1){: .btn .btn-green } |

**Abstract**: While the quality of novel-view images has improved dramatically with 3D
Gaussian Splatting, extracting specific objects from scenes remains
challenging. Isolating individual 3D Gaussian primitives for each object and
handling occlusions in scenes remain far from being solved. We propose a novel
object extraction method based on two key principles: (1) being object-centric
by pruning irrelevant primitives; and (2) leveraging generative inpainting to
compensate for missing observations caused by occlusions. For pruning, we
analyze the local structure of primitives using K-nearest neighbors, and retain
only relevant ones. For inpainting, we employ an off-the-shelf diffusion-based
inpainter combined with occlusion reasoning, utilizing the 3D representation of
the entire scene. Our findings highlight the crucial synergy between pruning
and inpainting, both of which significantly enhance extraction performance. We
evaluate our method on a standard real-world dataset and introduce a synthetic
dataset for quantitative analysis. Our approach outperforms the
state-of-the-art, demonstrating its effectiveness in object extraction from
complex scenes.

Comments:
- Project page: https://yeonjin-chang.github.io/ROODI/

---

## 4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large  Language Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, Hanspeter Pfister | cs.CV | [PDF](http://arxiv.org/pdf/2503.10437v1){: .btn .btn-green } |

**Abstract**: Learning 4D language fields to enable time-sensitive, open-ended language
queries in dynamic scenes is essential for many real-world applications. While
LangSplat successfully grounds CLIP features into 3D Gaussian representations,
achieving precision and efficiency in 3D static scenes, it lacks the ability to
handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot
capture temporal dynamics in videos. Real-world environments are inherently
dynamic, with object semantics evolving over time. Building a precise 4D
language field necessitates obtaining pixel-aligned, object-wise video
features, which current vision models struggle to achieve. To address these
challenges, we propose 4D LangSplat, which learns 4D language fields to handle
time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes
efficiently. 4D LangSplat bypasses learning the language field from vision
features and instead learns directly from text generated from object-wise video
captions via Multimodal Large Language Models (MLLMs). Specifically, we propose
a multimodal object-wise video prompting method, consisting of visual and text
prompts that guide MLLMs to generate detailed, temporally consistent,
high-quality captions for objects throughout a video. These captions are
encoded using a Large Language Model into high-quality sentence embeddings,
which then serve as pixel-aligned, object-specific feature supervision,
facilitating open-vocabulary text queries through shared embedding spaces.
Recognizing that objects in 4D scenes exhibit smooth transitions across states,
we further propose a status deformable network to model these continuous
changes over time effectively. Our results across multiple benchmarks
demonstrate that 4D LangSplat attains precise and efficient results for both
time-sensitive and time-agnostic open-vocabulary queries.

Comments:
- CVPR 2025. Project Page: https://4d-langsplat.github.io

---

## GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for  Geometrically Consistent Rendering and Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Jianheng Liu, Yunfei Wan, Bowen Wang, Chunran Zheng, Jiarong Lin, Fu Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2503.10170v1){: .btn .btn-green } |

**Abstract**: Digital twins are fundamental to the development of autonomous driving and
embodied artificial intelligence. However, achieving high-granularity surface
reconstruction and high-fidelity rendering remains a challenge. Gaussian
splatting offers efficient photorealistic rendering but struggles with
geometric inconsistencies due to fragmented primitives and sparse observational
data in robotics applications. Existing regularization methods, which rely on
render-derived constraints, often fail in complex environments. Moreover,
effectively integrating sparse LiDAR data with Gaussian splatting remains
challenging. We propose a unified LiDAR-visual system that synergizes Gaussian
splatting with a neural signed distance field. The accurate LiDAR point clouds
enable a trained neural signed distance field to offer a manifold geometry
field, This motivates us to offer an SDF-based Gaussian initialization for
physically grounded primitive placement and a comprehensive geometric
regularization for geometrically consistent rendering and reconstruction.
Experiments demonstrate superior reconstruction accuracy and rendering quality
across diverse trajectories. To benefit the community, the codes will be
released at https://github.com/hku-mars/GS-SDF.



---

## GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D  and 2D Local Tone Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Jinfeng Liu, Lingtong Kong, Bo Li, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.10143v1){: .btn .btn-green } |

**Abstract**: High dynamic range (HDR) novel view synthesis (NVS) aims to reconstruct HDR
scenes by leveraging multi-view low dynamic range (LDR) images captured at
different exposure levels. Current training paradigms with 3D tone mapping
often result in unstable HDR reconstruction, while training with 2D tone
mapping reduces the model's capacity to fit LDR images. Additionally, the
global tone mapper used in existing methods can impede the learning of both HDR
and LDR representations. To address these challenges, we present GaussHDR,
which unifies 3D and 2D local tone mapping through 3D Gaussian splatting.
Specifically, we design a residual local tone mapper for both 3D and 2D tone
mapping that accepts an additional context feature as input. We then propose
combining the dual LDR rendering results from both 3D and 2D local tone mapping
at the loss level. Finally, recognizing that different scenes may exhibit
varying balances between the dual results, we introduce uncertainty learning
and use the uncertainties for adaptive modulation. Extensive experiments
demonstrate that GaussHDR significantly outperforms state-of-the-art methods in
both synthetic and real-world scenarios.

Comments:
- This paper is accepted by CVPR 2025. Project page is available at
  https://liujf1226.github.io/GaussHDR

---

## AI-assisted 3D Preservation and Reconstruction of Temple Arts

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Naai-Jung Shih | cs.GR | [PDF](http://arxiv.org/pdf/2503.10031v1){: .btn .btn-green } |

**Abstract**: How does AI connect to the past in conservation? What can 17 years old photos
be helpful in a renewed effort of preservation? This research aims to use AI to
connect both in a seamless 3D reconstruction of heritage from imagery data
taken from Gongfan Palace, Yunlin Taiwan. AI-assisted 3D modeling was used to
reconstruct correspondent details across different 3D platforms of 3DGS or NeRF
models generated by Postshot or KIRI Engine. Polygon or point models by Zephyr
were referred to and assessed in two sets. The results also include AI-assist
modeling outcomes in Stable Diffusion and Postshot-based animation. The evolved
documenta-tion and interpretation in AI presents a novel arrangement of working
processes contributed by new structure and management of resources, formats,
and interfaces, as a continuous preservation effort.

Comments:
- 13 pages, 9 figures, 1 table

---

## MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Yingshuang Zou, Yikang Ding, Chuanrui Zhang, Jiazhe Guo, Bohan Li, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.10604v1){: .btn .btn-green } |

**Abstract**: Recent breakthroughs in radiance fields have significantly advanced 3D scene
reconstruction and novel view synthesis (NVS) in autonomous driving.
Nevertheless, critical limitations persist: reconstruction-based methods
exhibit substantial performance deterioration under significant viewpoint
deviations from training trajectories, while generation-based techniques
struggle with temporal coherence and precise scene controllability. To overcome
these challenges, we present MuDG, an innovative framework that integrates
Multi-modal Diffusion model with Gaussian Splatting (GS) for Urban Scene
Reconstruction. MuDG leverages aggregated LiDAR point clouds with RGB and
geometric priors to condition a multi-modal video diffusion model, synthesizing
photorealistic RGB, depth, and semantic outputs for novel viewpoints. This
synthesis pipeline enables feed-forward NVS without computationally intensive
per-scene optimization, providing comprehensive supervision signals to refine
3DGS representations for rendering robustness enhancement under extreme
viewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDG
outperforms existing methods in both reconstruction and synthesis quality.



---

## 3D Student Splatting and Scooping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Jialin Zhu, Jiangbei Yue, Feixiang He, He Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.10148v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel
view synthesis, and has spiked a new wave of research in neural rendering and
related applications. As 3DGS is becoming a foundational component of many
models, any improvement on 3DGS itself can bring huge benefits. To this end, we
aim to improve the fundamental paradigm and formulation of 3DGS. We argue that
as an unnormalized mixture model, it needs to be neither Gaussians nor
splatting. We subsequently propose a new mixture model consisting of flexible
Student's t distributions, with both positive (splatting) and negative
(scooping) densities. We name our model Student Splatting and Scooping, or SSS.
When providing better expressivity, SSS also poses new challenges in learning.
Therefore, we also propose a new principled sampling approach for optimization.
Through exhaustive evaluation and comparison, across multiple datasets,
settings, and metrics, we demonstrate that SSS outperforms existing methods in
terms of quality and parameter efficiency, e.g. achieving matching or better
quality with similar numbers of components, and obtaining comparable results
while reducing the component number by as much as 82%.



---

## LHM: Large Animatable Human Reconstruction Model from a Single Image in  Seconds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo | cs.CV | [PDF](http://arxiv.org/pdf/2503.10625v1){: .btn .btn-green } |

**Abstract**: Animatable 3D human reconstruction from a single image is a challenging
problem due to the ambiguity in decoupling geometry, appearance, and
deformation. Recent advances in 3D human reconstruction mainly focus on static
human modeling, and the reliance of using synthetic 3D scans for training
limits their generalization ability. Conversely, optimization-based video
methods achieve higher fidelity but demand controlled capture conditions and
computationally intensive refinement processes. Motivated by the emergence of
large reconstruction models for efficient static reconstruction, we propose LHM
(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars
represented as 3D Gaussian splatting in a feed-forward pass. Our model
leverages a multimodal transformer architecture to effectively encode the human
body positional features and image features with attention mechanism, enabling
detailed preservation of clothing geometry and texture. To further boost the
face identity preservation and fine detail recovery, we propose a head feature
pyramid encoding scheme to aggregate multi-scale features of the head regions.
Extensive experiments demonstrate that our LHM generates plausible animatable
human in seconds without post-processing for face and hands, outperforming
existing methods in both reconstruction accuracy and generalization ability.

Comments:
- Project Page: https://lingtengqiu.github.io/LHM/

---

## Flow-NeRF: Joint Learning of Geometry, Poses, and Dense Flow within  Unified Neural Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Xunzhi Zheng, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.10464v1){: .btn .btn-green } |

**Abstract**: Learning accurate scene reconstruction without pose priors in neural radiance
fields is challenging due to inherent geometric ambiguity. Recent development
either relies on correspondence priors for regularization or uses off-the-shelf
flow estimators to derive analytical poses. However, the potential for jointly
learning scene geometry, camera poses, and dense flow within a unified neural
representation remains largely unexplored. In this paper, we present Flow-NeRF,
a unified framework that simultaneously optimizes scene geometry, camera poses,
and dense optical flow all on-the-fly. To enable the learning of dense flow
within the neural radiance field, we design and build a bijective mapping for
flow estimation, conditioned on pose. To make the scene reconstruction benefit
from the flow estimation, we develop an effective feature enhancement mechanism
to pass canonical space features to world space representations, significantly
enhancing scene geometry. We validate our model across four important tasks,
i.e., novel view synthesis, depth estimation, camera pose prediction, and dense
optical flow estimation, using several datasets. Our approach surpasses
previous methods in almost all metrics for novel-view view synthesis and depth
estimation and yields both qualitatively sound and quantitatively accurate
novel-view flow. Our project page is https://zhengxunzhi.github.io/flownerf/.



---

## SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Dai Sun, Huhao Guan, Kun Zhang, Xike Xie, S. Kevin Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.09332v1){: .btn .btn-green } |

**Abstract**: Dynamic and static components in scenes often exhibit distinct properties,
yet most 4D reconstruction methods treat them indiscriminately, leading to
suboptimal performance in both cases. This work introduces SDD-4DGS, the first
framework for static-dynamic decoupled 4D scene reconstruction based on
Gaussian Splatting. Our approach is built upon a novel probabilistic dynamic
perception coefficient that is naturally integrated into the Gaussian
reconstruction pipeline, enabling adaptive separation of static and dynamic
components. With carefully designed implementation strategies to realize this
theoretical framework, our method effectively facilitates explicit learning of
motion patterns for dynamic elements while maintaining geometric stability for
static structures. Extensive experiments on five benchmark datasets demonstrate
that SDD-4DGS consistently outperforms state-of-the-art methods in
reconstruction fidelity, with enhanced detail restoration for static structures
and precise modeling of dynamic motions. The code will be released.



---

## GASPACHO: Gaussian Splatting for Controllable Humans and Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Aymen Mir, Arthur Moreau, Helisa Dhamo, Zhensong Zhang, Eduardo PÃ©rez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2503.09342v1){: .btn .btn-green } |

**Abstract**: We present GASPACHO: a method for generating photorealistic controllable
renderings of human-object interactions. Given a set of multi-view RGB images
of human-object interactions, our method reconstructs animatable templates of
the human and object as separate sets of Gaussians simultaneously. Different
from existing work, which focuses on human reconstruction and ignores objects
as background, our method explicitly reconstructs both humans and objects,
thereby allowing for controllable renderings of novel human object interactions
in different poses from novel-camera viewpoints. During reconstruction, we
constrain the Gaussians that generate rendered images to be a linear function
of a set of canonical Gaussians. By simply changing the parameters of the
linear deformation functions after training, our method can generate renderings
of novel human-object interaction in novel poses from novel camera viewpoints.
We learn the 3D Gaussian properties of the canonical Gaussians on the
underlying 2D manifold of the canonical human and object templates. This in
turns requires a canonical object template with a fixed UV unwrapping. To
define such an object template, we use a feature based representation to track
the object across the multi-view sequence. We further propose an occlusion
aware photometric loss that allows for reconstructions under significant
occlusions. Several experiments on two human-object datasets - BEHAVE and
DNA-Rendering - demonstrate that our method allows for high-quality
reconstruction of human and object templates under significant occlusion and
the synthesis of controllable renderings of novel human-object interactions in
novel human poses from novel camera views.



---

## Online Language Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Saimouli Katragadda, Cho-Ying Wu, Yuliang Guo, Xinyu Huang, Guoquan Huang, Liu Ren | cs.AI | [PDF](http://arxiv.org/pdf/2503.09447v1){: .btn .btn-green } |

**Abstract**: To enable AI agents to interact seamlessly with both humans and 3D
environments, they must not only perceive the 3D world accurately but also
align human language with 3D spatial representations. While prior work has made
significant progress by integrating language features into geometrically
detailed 3D scene representations using 3D Gaussian Splatting (GS), these
approaches rely on computationally intensive offline preprocessing of language
features for each input image, limiting adaptability to new environments. In
this work, we introduce Online Language Splatting, the first framework to
achieve online, near real-time, open-vocabulary language mapping within a
3DGS-SLAM system without requiring pre-generated language features. The key
challenge lies in efficiently fusing high-dimensional language features into 3D
representations while balancing the computation speed, memory usage, rendering
quality and open-vocabulary capability. To this end, we innovatively design:
(1) a high-resolution CLIP embedding module capable of generating detailed
language feature maps in 18ms per frame, (2) a two-stage online auto-encoder
that compresses 768-dimensional CLIP features to 15 dimensions while preserving
open-vocabulary capabilities, and (3) a color-language disentangled
optimization approach to improve rendering quality. Experimental results show
that our online method not only surpasses the state-of-the-art offline methods
in accuracy but also achieves more than 40x efficiency boost, demonstrating the
potential for dynamic and interactive AI applications.



---

## Motion Blender Gaussian Splatting for Dynamic Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias | cs.CV | [PDF](http://arxiv.org/pdf/2503.09040v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has emerged as a powerful tool for high-fidelity
reconstruction of dynamic scenes. However, existing methods primarily rely on
implicit motion representations, such as encoding motions into neural networks
or per-Gaussian parameters, which makes it difficult to further manipulate the
reconstructed motions. This lack of explicit controllability limits existing
methods to replaying recorded motions only, which hinders a wider application.
To address this, we propose Motion Blender Gaussian Splatting (MB-GS), a novel
framework that uses motion graph as an explicit and sparse motion
representation. The motion of graph links is propagated to individual Gaussians
via dual quaternion skinning, with learnable weight painting functions
determining the influence of each link. The motion graphs and 3D Gaussians are
jointly optimized from input videos via differentiable rendering. Experiments
show that MB-GS achieves state-of-the-art performance on the iPhone dataset
while being competitive on HyperNeRF. Additionally, we demonstrate the
application potential of our method in generating novel object motions and
robot demonstrations through motion editing. Video demonstrations can be found
at https://mlzxy.github.io/mbgs.



---

## Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and  Physics-Based Simulation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | MÃ¡tÃ© TÃ³th, PÃ©ter KovÃ¡cs, ZoltÃ¡n Bendefy, ZoltÃ¡n Hortsin, BalÃ¡zs TerÃ©ki, TamÃ¡s Matuszka | cs.GR | [PDF](http://arxiv.org/pdf/2503.09464v1){: .btn .btn-green } |

**Abstract**: Neural reconstruction models for autonomous driving simulation have made
significant strides in recent years, with dynamic models becoming increasingly
prevalent. However, these models are typically limited to handling in-domain
objects closely following their original trajectories. We introduce a hybrid
approach that combines the strengths of neural reconstruction with
physics-based rendering. This method enables the virtual placement of
traditional mesh-based dynamic agents at arbitrary locations, adjustments to
environmental conditions, and rendering from novel camera viewpoints. Our
approach significantly enhances novel view synthesis quality -- especially for
road surfaces and lane markings -- while maintaining interactive frame rates
through our novel training method, NeRF2GS. This technique leverages the
superior generalization capabilities of NeRF-based methods and the real-time
rendering speed of 3D Gaussian Splatting (3DGS). We achieve this by training a
customized NeRF model on the original images with depth regularization derived
from a noisy LiDAR point cloud, then using it as a teacher model for 3DGS
training. This process ensures accurate depth, surface normals, and camera
appearance modeling as supervision. With our block-based training
parallelization, the method can handle large-scale reconstructions (greater
than or equal to 100,000 square meters) and predict segmentation masks, surface
normals, and depth maps. During simulation, it supports a rasterization-based
rendering backend with depth-based composition and multiple camera models for
real-time camera simulation, as well as a ray-traced backend for precise LiDAR
simulation.



---

## Physics-Aware Human-Object Rendering from Sparse Views via 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Weiquan Wang, Jun Xiao, Yueting Zhuang, Long Chen | cs.GR | [PDF](http://arxiv.org/pdf/2503.09640v1){: .btn .btn-green } |

**Abstract**: Rendering realistic human-object interactions (HOIs) from sparse-view inputs
is challenging due to occlusions and incomplete observations, yet crucial for
various real-world applications. Existing methods always struggle with either
low rendering qualities (\eg, visual fidelity and physically plausible HOIs) or
high computational costs. To address these limitations, we propose HOGS
(Human-Object Rendering via 3D Gaussian Splatting), a novel framework for
efficient and physically plausible HOI rendering from sparse views.
Specifically, HOGS combines 3D Gaussian Splatting with a physics-aware
optimization process. It incorporates a Human Pose Refinement module for
accurate pose estimation and a Sparse-View Human-Object Contact Prediction
module for efficient contact region identification. This combination enables
coherent joint rendering of human and object Gaussians while enforcing
physically plausible interactions. Extensive experiments on the HODome dataset
demonstrate that HOGS achieves superior rendering quality, efficiency, and
physical plausibility compared to existing methods. We further show its
extensibility to hand-object grasp rendering tasks, presenting its broader
applicability to articulated object interactions.



---

## Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting  with Progressive Self-Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Jiatong Xia, Lingqiao Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.09396v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in
synthesizing novel views after training on a given set of viewpoints. However,
its rendering quality deteriorates when the synthesized view deviates
significantly from the training views. This decline occurs due to (1) the
model's difficulty in generalizing to out-of-distribution scenarios and (2)
challenges in interpolating fine details caused by substantial resolution
changes and occlusions. A notable case of this limitation is close-up view
generation--producing views that are significantly closer to the object than
those in the training set. To tackle this issue, we propose a novel approach
for close-up view generation based by progressively training the 3DGS model
with self-generated data. Our solution is based on three key ideas. First, we
leverage the See3D model, a recently introduced 3D-aware generative model, to
enhance the details of rendered views. Second, we propose a strategy to
progressively expand the ``trust regions'' of the 3DGS model and update a set
of reference views for See3D. Finally, we introduce a fine-tuning strategy to
carefully update the 3DGS model with training data generated from the above
schemes. We further define metrics for close-up views evaluation to facilitate
better research on this problem. By conducting evaluations on specifically
selected scenarios for close-up views, our proposed approach demonstrates a
clear advantage over competitive solutions.



---

## GigaSLAM: Large-Scale Monocular SLAM with Hierachical Gaussian Splats

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Kai Deng, Jian Yang, Shenlong Wang, Jin Xie | cs.RO | [PDF](http://arxiv.org/pdf/2503.08071v1){: .btn .btn-green } |

**Abstract**: Tracking and mapping in large-scale, unbounded outdoor environments using
only monocular RGB input presents substantial challenges for existing SLAM
systems. Traditional Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS) SLAM methods are typically limited to small, bounded indoor settings. To
overcome these challenges, we introduce GigaSLAM, the first NeRF/3DGS-based
SLAM framework for kilometer-scale outdoor environments, as demonstrated on the
KITTI and KITTI 360 datasets. Our approach employs a hierarchical sparse voxel
map representation, where Gaussians are decoded by neural networks at multiple
levels of detail. This design enables efficient, scalable mapping and
high-fidelity viewpoint rendering across expansive, unbounded scenes. For
front-end tracking, GigaSLAM utilizes a metric depth model combined with
epipolar geometry and PnP algorithms to accurately estimate poses, while
incorporating a Bag-of-Words-based loop closure mechanism to maintain robust
alignment over long trajectories. Consequently, GigaSLAM delivers
high-precision tracking and visually faithful rendering on urban outdoor
benchmarks, establishing a robust SLAM solution for large-scale, long-term
scenarios, and significantly extending the applicability of Gaussian Splatting
SLAM systems to unbounded outdoor environments.



---

## TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction  via Spatio-Temporal Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Fengyi Zhang, Huitong Yang, Zheng Zhang, Zi Huang, Yadan Luo | cs.CV | [PDF](http://arxiv.org/pdf/2503.08485v1){: .btn .btn-green } |

**Abstract**: Self-supervised 3D occupancy prediction offers a promising solution for
understanding complex driving scenes without requiring costly 3D annotations.
However, training dense voxel decoders to capture fine-grained geometry and
semantics can demand hundreds of GPU hours, and such models often fail to adapt
to varying voxel resolutions or new classes without extensive retraining. To
overcome these limitations, we propose a practical and flexible test-time
occupancy prediction framework termed TT-GaussOcc. Our approach incrementally
optimizes time-aware 3D Gaussians instantiated from raw sensor streams at
runtime, enabling voxelization at arbitrary user-specified resolution.
Specifically, TT-GaussOcc operates in a "lift-move-voxel" symphony: we first
"lift" surrounding-view semantics obtained from 2D vision foundation models
(VLMs) to instantiate Gaussians at non-empty 3D space; Next, we "move" dynamic
Gaussians from previous frames along estimated Gaussian scene flow to complete
appearance and eliminate trailing artifacts of fast-moving objects, while
accumulating static Gaussians to enforce temporal consistency; Finally, we
mitigate inherent noises in semantic predictions and scene flow vectors by
periodically smoothing neighboring Gaussians during optimization, using
proposed trilateral RBF kernels that jointly consider color, semantic, and
spatial affinities. The historical static and current dynamic Gaussians are
then combined and voxelized to generate occupancy prediction. Extensive
experiments on Occ3D and nuCraft with varying voxel resolutions demonstrate
that TT-GaussOcc surpasses self-supervised baselines by 46% on mIoU without any
offline training, and supports finer voxel resolutions at 2.6 FPS inference
speed.



---

## NeRF-VIO: Map-Based Visual-Inertial Odometry with Initialization  Leveraging Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Yanyu Zhang, Dongming Wang, Jie Xu, Mengyuan Liu, Pengxiang Zhu, Wei Ren | cs.CV | [PDF](http://arxiv.org/pdf/2503.07952v1){: .btn .btn-green } |

**Abstract**: A prior map serves as a foundational reference for localization in
context-aware applications such as augmented reality (AR). Providing valuable
contextual information about the environment, the prior map is a vital tool for
mitigating drift. In this paper, we propose a map-based visual-inertial
localization algorithm (NeRF-VIO) with initialization using neural radiance
fields (NeRF). Our algorithm utilizes a multilayer perceptron model and
redefines the loss function as the geodesic distance on \(SE(3)\), ensuring the
invariance of the initialization model under a frame change within
\(\mathfrak{se}(3)\). The evaluation demonstrates that our model outperforms
existing NeRF-based initialization solution in both accuracy and efficiency. By
integrating a two-stage update mechanism within a multi-state constraint Kalman
filter (MSCKF) framework, the state of NeRF-VIO is constrained by both captured
images from an onboard camera and rendered images from a pre-trained NeRF
model. The proposed algorithm is validated using a real-world AR dataset, the
results indicate that our two-stage update pipeline outperforms MSCKF across
all data sequences.



---

## HRAvatar: High-Quality and Relightable Gaussian Head Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Kangjie Chen, Minghan Qin, Yu Li, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.08224v1){: .btn .btn-green } |

**Abstract**: Reconstructing animatable and high-quality 3D head avatars from monocular
videos, especially with realistic relighting, is a valuable task. However, the
limited information from single-view input, combined with the complex head
poses and facial movements, makes this challenging. Previous methods achieve
real-time performance by combining 3D Gaussian Splatting with a parametric head
model, but the resulting head quality suffers from inaccurate face tracking and
limited expressiveness of the deformation model. These methods also fail to
produce realistic effects under novel lighting conditions. To address these
issues, we propose HRAvatar, a 3DGS-based method that reconstructs
high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors
through end-to-end optimization and better captures individual facial
deformations using learnable blendshapes and learnable linear blend skinning.
Additionally, it decomposes head appearance into several physical properties
and incorporates physically-based shading to account for environmental
lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs
superior-quality heads but also achieves realistic visual effects under varying
lighting conditions.

Comments:
- Project page: https://eastbeanzhang.github.io/HRAvatar

---

## PCGS: Progressive Compression of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Yihang Chen, Mengyao Li, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2503.08511v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) achieves impressive rendering fidelity and speed
for novel view synthesis. However, its substantial data size poses a
significant challenge for practical applications. While many compression
techniques have been proposed, they fail to efficiently utilize existing
bitstreams in on-demand applications due to their lack of progressivity,
leading to a waste of resource. To address this issue, we propose PCGS
(Progressive Compression of 3D Gaussian Splatting), which adaptively controls
both the quantity and quality of Gaussians (or anchors) to enable effective
progressivity for on-demand applications. Specifically, for quantity, we
introduce a progressive masking strategy that incrementally incorporates new
anchors while refining existing ones to enhance fidelity. For quality, we
propose a progressive quantization approach that gradually reduces quantization
step sizes to achieve finer modeling of Gaussian attributes. Furthermore, to
compact the incremental bitstreams, we leverage existing quantization results
to refine probability prediction, improving entropy coding efficiency across
progressive levels. Overall, PCGS achieves progressivity while maintaining
compression performance comparable to SoTA non-progressive methods. Code
available at: github.com/YihangChen-ee/PCGS.

Comments:
- Project Page: https://yihangchen-ee.github.io/project_pcgs/ Code:
  https://github.com/YihangChen-ee/PCGS

---

## ELECTRA: A Symmetry-breaking Cartesian Network for Charge Density  Prediction with Floating Orbitals

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Jonas Elsborg, Luca Thiede, AlÃ¡n Aspuru-Guzik, Tejs Vegge, Arghya Bhowmik | cs.LG | [PDF](http://arxiv.org/pdf/2503.08305v1){: .btn .btn-green } |

**Abstract**: We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an
equivariant model for predicting electronic charge densities using "floating"
orbitals. Floating orbitals are a long-standing idea in the quantum chemistry
community that promises more compact and accurate representations by placing
orbitals freely in space, as opposed to centering all orbitals at the position
of atoms. Finding ideal placements of these orbitals requires extensive domain
knowledge though, which thus far has prevented widespread adoption. We solve
this in a data-driven manner by training a Cartesian tensor network to predict
orbital positions along with orbital coefficients. This is made possible
through a symmetry-breaking mechanism that is used to learn position
displacements with lower symmetry than the input molecule while preserving the
rotation equivariance of the charge density itself. Inspired by recent
successes of Gaussian Splatting in representing densities in space, we are
using Gaussians as our orbitals and predict their weights and covariance
matrices. Our method achieves a state-of-the-art balance between computational
efficiency and predictive accuracy on established benchmarks.

Comments:
- 8 pages, 3 figures, 1 table

---

## 7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2503.07946v1){: .btn .btn-green } |

**Abstract**: Real-time rendering of dynamic scenes with view-dependent effects remains a
fundamental challenge in computer graphics. While recent advances in Gaussian
Splatting have shown promising results separately handling dynamic scenes
(4DGS) and view-dependent effects (6DGS), no existing method unifies these
capabilities while maintaining real-time performance. We present 7D Gaussian
Splatting (7DGS), a unified framework representing scene elements as
seven-dimensional Gaussians spanning position (3D), time (1D), and viewing
direction (3D). Our key contribution is an efficient conditional slicing
mechanism that transforms 7D Gaussians into view- and time-conditioned 3D
Gaussians, maintaining compatibility with existing 3D Gaussian Splatting
pipelines while enabling joint optimization. Experiments demonstrate that 7DGS
outperforms prior methods by up to 7.36 dB in PSNR while achieving real-time
rendering (401 FPS) on challenging dynamic scenes with complex view-dependent
effects. The project page is: https://gaozhongpai.github.io/7dgs/.



---

## MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Chenfeng Hou, Qi Xun Yeo, Mengqi Guo, Yongxin Su, Yanyan Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.08093v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for its
high-quality rendering capabilities, ultra-fast training, and inference speeds.
However, when we apply 3DGS to surface reconstruction tasks, especially in
environments with dynamic objects and distractors, the method suffers from
floating artifacts and color errors due to inconsistency from different
viewpoints. To address this challenge, we propose Multi-View Consistency
Gaussian Splatting for the domain of Robust Surface Reconstruction
(\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a
{heuristics-guided distractor masking} strategy for robust surface
reconstruction in non-static environments. Compared to existing methods that
rely on MLPs for distractor segmentation strategies, our approach separates
distractors from static scene elements by comparing multi-view feature
consistency, allowing us to obtain precise distractor masks early in training.
Furthermore, we introduce a pruning measure based on multi-view contributions
to reset transmittance, effectively reducing floating artifacts. Finally, a
multi-view consistency loss is applied to achieve high-quality performance in
surface reconstruction tasks. Experimental results demonstrate that MVGSR
achieves competitive geometric accuracy and rendering fidelity compared to the
state-of-the-art surface reconstruction algorithms. More information is
available on our project page (https://mvgsr.github.io).

Comments:
- project page https://mvgsr.github.io

---

## Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for  Dynamic Driving Scenarios

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Zikang Yuan, Yuechuan Pu, Hongcheng Luo, Fengtian Lang, Cheng Chi, Teng Li, Yingying Shen, Haiyang Sun, Bing Wang, Xin Yang | cs.RO | [PDF](http://arxiv.org/pdf/2503.08317v1){: .btn .btn-green } |

**Abstract**: Ensuring the safety of autonomous vehicles necessitates comprehensive
simulation of multi-sensor data, encompassing inputs from both cameras and
LiDAR sensors, across various dynamic driving scenarios. Neural rendering
techniques, which utilize collected raw sensor data to simulate these dynamic
environments, have emerged as a leading methodology. While NeRF-based
approaches can uniformly represent scenes for rendering data from both camera
and LiDAR, they are hindered by slow rendering speeds due to dense sampling.
Conversely, Gaussian Splatting-based methods employ Gaussian primitives for
scene representation and achieve rapid rendering through rasterization.
However, these rasterization-based techniques struggle to accurately model
non-linear optical sensors. This limitation restricts their applicability to
sensors beyond pinhole cameras. To address these challenges and enable unified
representation of dynamic driving scenarios using Gaussian primitives, this
study proposes a novel hybrid approach. Our method utilizes rasterization for
rendering image data while employing Gaussian ray-tracing for LiDAR data
rendering. Experimental results on public datasets demonstrate that our
approach outperforms current state-of-the-art methods. This work presents a
unified and efficient solution for realistic simulation of camera and LiDAR
data in autonomous driving scenarios using Gaussian primitives, offering
significant advancements in both rendering quality and computational
efficiency.

Comments:
- 10 pages

---

## ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated  Objects using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Junfu Guo, Yu Xin, Gaoyi Liu, Kai Xu, Ligang Liu, Ruizhen Hu | cs.CV | [PDF](http://arxiv.org/pdf/2503.08135v1){: .btn .btn-green } |

**Abstract**: We tackle the challenge of concurrent reconstruction at the part level with
the RGB appearance and estimation of motion parameters for building digital
twins of articulated objects using the 3D Gaussian Splatting (3D-GS) method.
With two distinct sets of multi-view imagery, each depicting an object in
separate static articulation configurations, we reconstruct the articulated
object in 3D Gaussian representations with both appearance and geometry
information at the same time. Our approach decoupled multiple highly
interdependent parameters through a multi-step optimization process, thereby
achieving a stable optimization procedure and high-quality outcomes. We
introduce ArticulatedGS, a self-supervised, comprehensive framework that
autonomously learns to model shapes and appearances at the part level and
synchronizes the optimization of motion parameters, all without reliance on 3D
supervision, motion cues, or semantic labels. Our experimental results
demonstrate that, among comparable methodologies, our approach has achieved
optimal outcomes in terms of part segmentation accuracy, motion estimation
accuracy, and visual quality.



---

## Dynamic Scene Reconstruction: Recent Advance in Real-time Rendering and  Streaming

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Jiaxuan Zhu, Hao Tang | cs.GR | [PDF](http://arxiv.org/pdf/2503.08166v1){: .btn .btn-green } |

**Abstract**: Representing and rendering dynamic scenes from 2D images is a fundamental yet
challenging problem in computer vision and graphics. This survey provides a
comprehensive review of the evolution and advancements in dynamic scene
representation and rendering, with a particular emphasis on recent progress in
Neural Radiance Fields based and 3D Gaussian Splatting based reconstruction
methods. We systematically summarize existing approaches, categorize them
according to their core principles, compile relevant datasets, compare the
performance of various methods on these benchmarks, and explore the challenges
and future research directions in this rapidly evolving field. In total, we
review over 170 relevant papers, offering a broad perspective on the state of
the art in this domain.

Comments:
- 20 pages, 6 figures

---

## FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of  Large-Scale Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | GeonU Kim, Kim Youwang, Lee Hyoseok, Tae-Hyun Oh | cs.GR | [PDF](http://arxiv.org/pdf/2503.09635v1){: .btn .btn-green } |

**Abstract**: We present FPGS, a feed-forward photorealistic style transfer method of
large-scale radiance fields represented by Gaussian Splatting. FPGS, stylizes
large-scale 3D scenes with arbitrary, multiple style reference images without
additional optimization while preserving multi-view consistency and real-time
rendering speed of 3D Gaussians. Prior arts required tedious per-style
optimization or time-consuming per-scene training stage and were limited to
small-scale 3D scenes. FPGS efficiently stylizes large-scale 3D scenes by
introducing a style-decomposed 3D feature field, which inherits AdaIN's
feed-forward stylization machinery, supporting arbitrary style reference
images. Furthermore, FPGS supports multi-reference stylization with the
semantic correspondence matching and local AdaIN, which adds diverse user
control for 3D scene styles. FPGS also preserves multi-view consistency by
applying semantic matching and style transfer processes directly onto queried
features in 3D space. In experiments, we demonstrate that FPGS achieves
favorable photorealistic quality scene stylization for large-scale static and
dynamic 3D scenes with diverse reference images. Project page:
https://kim-geonu.github.io/FPGS/

Comments:
- Project page: https://kim-geonu.github.io/FPGS/. arXiv admin note:
  substantial text overlap with arXiv:2401.05516

---

## Mitigating Ambiguities in 3D Classification with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Ruiqi Zhang, Hao Zhu, Jingyi Zhao, Qi Zhang, Xun Cao, Zhan Ma | cs.CV | [PDF](http://arxiv.org/pdf/2503.08352v1){: .btn .btn-green } |

**Abstract**: 3D classification with point cloud input is a fundamental problem in 3D
vision. However, due to the discrete nature and the insufficient material
description of point cloud representations, there are ambiguities in
distinguishing wire-like and flat surfaces, as well as transparent or
reflective objects. To address these issues, we propose Gaussian Splatting (GS)
point cloud-based 3D classification. We find that the scale and rotation
coefficients in the GS point cloud help characterize surface types.
Specifically, wire-like surfaces consist of multiple slender Gaussian
ellipsoids, while flat surfaces are composed of a few flat Gaussian ellipsoids.
Additionally, the opacity in the GS point cloud represents the transparency
characteristics of objects. As a result, ambiguities in point cloud-based 3D
classification can be mitigated utilizing GS point cloud as input. To verify
the effectiveness of GS point cloud input, we construct the first real-world GS
point cloud dataset in the community, which includes 20 categories with 200
objects in each category. Experiments not only validate the superiority of GS
point cloud input, especially in distinguishing ambiguous objects, but also
demonstrate the generalization ability across different classification methods.

Comments:
- Accepted by CVPR 2025

---

## S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Guangting Zheng, Jiajun Deng, Xiaomeng Chu, Yu Yuan, Houqiang Li, Yanyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.08217v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has reshaped the field of
photorealistic 3D reconstruction, achieving impressive rendering quality and
speed. However, when applied to large-scale street scenes, existing methods
suffer from rapidly escalating per-viewpoint reconstruction costs as scene size
increases, leading to significant computational overhead. After revisiting the
conventional pipeline, we identify three key factors accounting for this issue:
unnecessary local-to-global transformations, excessive 3D-to-2D projections,
and inefficient rendering of distant content. To address these challenges, we
propose S3R-GS, a 3DGS framework that Streamlines the pipeline for large-scale
Street Scene Reconstruction, effectively mitigating these limitations.
Moreover, most existing street 3DGS methods rely on ground-truth 3D bounding
boxes to separate dynamic and static components, but 3D bounding boxes are
difficult to obtain, limiting real-world applicability. To address this, we
propose an alternative solution with 2D boxes, which are easier to annotate or
can be predicted by off-the-shelf vision foundation models. Such designs
together make S3R-GS readily adapt to large, in-the-wild scenarios. Extensive
experiments demonstrate that S3R-GS enhances rendering quality and
significantly accelerates reconstruction. Remarkably, when applied to videos
from the challenging Argoverse2 dataset, it achieves state-of-the-art PSNR and
SSIM, reducing reconstruction time to below 50%--and even 20%--of competing
methods.



---

## GAS-NeRF: Geometry-Aware Stylization of Dynamic Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Nhat Phuong Anh Vu, Abhishek Saroha, Or Litany, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2503.08483v1){: .btn .btn-green } |

**Abstract**: Current 3D stylization techniques primarily focus on static scenes, while our
world is inherently dynamic, filled with moving objects and changing
environments. Existing style transfer methods primarily target appearance --
such as color and texture transformation -- but often neglect the geometric
characteristics of the style image, which are crucial for achieving a complete
and coherent stylization effect. To overcome these shortcomings, we propose
GAS-NeRF, a novel approach for joint appearance and geometry stylization in
dynamic Radiance Fields. Our method leverages depth maps to extract and
transfer geometric details into the radiance field, followed by appearance
transfer. Experimental results on synthetic and real-world datasets demonstrate
that our approach significantly enhances the stylization quality while
maintaining temporal coherence in dynamic scenes.



---

## Frequency-Aware Density Control via Reparameterization for High-Quality  Rendering of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Zhaojie Zeng, Yuesong Wang, Lili Ju, Tao Guan | cs.CV | [PDF](http://arxiv.org/pdf/2503.07000v1){: .btn .btn-green } |

**Abstract**: By adaptively controlling the density and generating more Gaussians in
regions with high-frequency information, 3D Gaussian Splatting (3DGS) can
better represent scene details. From the signal processing perspective,
representing details usually needs more Gaussians with relatively smaller
scales. However, 3DGS currently lacks an explicit constraint linking the
density and scale of 3D Gaussians across the domain, leading to 3DGS using
improper-scale Gaussians to express frequency information, resulting in the
loss of accuracy. In this paper, we propose to establish a direct relation
between density and scale through the reparameterization of the scaling
parameters and ensure the consistency between them via explicit constraints
(i.e., density responds well to changes in frequency). Furthermore, we develop
a frequency-aware density control strategy, consisting of densification and
deletion, to improve representation quality with fewer Gaussians. A dynamic
threshold encourages densification in high-frequency regions, while a
scale-based filter deletes Gaussians with improper scale. Experimental results
on various datasets demonstrate that our method outperforms existing
state-of-the-art methods quantitatively and qualitatively.

Comments:
- Accepted to AAAI2025

---

## Multi-Modal 3D Mesh Reconstruction from Images and Text

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Melvin Reka, Tessa Pulli, Markus Vincze | cs.CV | [PDF](http://arxiv.org/pdf/2503.07190v1){: .btn .btn-green } |

**Abstract**: 6D object pose estimation for unseen objects is essential in robotics but
traditionally relies on trained models that require large datasets, high
computational costs, and struggle to generalize. Zero-shot approaches eliminate
the need for training but depend on pre-existing 3D object models, which are
often impractical to obtain. To address this, we propose a language-guided
few-shot 3D reconstruction method, reconstructing a 3D mesh from few input
images. In the proposed pipeline, receives a set of input images and a language
query. A combination of GroundingDINO and Segment Anything Model outputs
segmented masks from which a sparse point cloud is reconstructed with VGGSfM.
Subsequently, the mesh is reconstructed with the Gaussian Splatting method
SuGAR. In a final cleaning step, artifacts are removed, resulting in the final
3D mesh of the queried object. We evaluate the method in terms of accuracy and
quality of the geometry and texture. Furthermore, we study the impact of
imaging conditions such as viewing angle, number of input images, and image
overlap on 3D object reconstruction quality, efficiency, and computational
scalability.

Comments:
- under review

---

## EigenGS Representation: From Eigenspace to Gaussian Image Space


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Lo-Wei Tai, Ching-En Li, Cheng-Lin Chen, Chih-Jung Tsai, Hwann-Tzong Chen, Tyng-Luh Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.07446v2){: .btn .btn-green } |

**Abstract**: Principal Component Analysis (PCA), a classical dimensionality reduction
technique, and 2D Gaussian representation, an adaptation of 3D Gaussian
Splatting for image representation, offer distinct approaches to modeling
visual data. We present EigenGS, a novel method that bridges these paradigms
through an efficient transformation pipeline connecting eigenspace and
image-space Gaussian representations. Our approach enables instant
initialization of Gaussian parameters for new images without requiring
per-image optimization from scratch, dramatically accelerating convergence.
EigenGS introduces a frequency-aware learning mechanism that encourages
Gaussians to adapt to different scales, effectively modeling varied spatial
frequencies and preventing artifacts in high-resolution reconstruction.
Extensive experiments demonstrate that EigenGS not only achieves superior
reconstruction quality compared to direct 2D Gaussian fitting but also reduces
necessary parameter count and training time. The results highlight EigenGS's
effectiveness and generalization ability across images with varying resolutions
and diverse categories, making Gaussian-based image representation both
high-quality and viable for real-time applications.



---

## ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Konstantinos D. Polyzos, Athanasios Bacharis, Saketh Madhuvarasu, Nikos Papanikolopoulos, Tara Javidi | cs.CV | [PDF](http://arxiv.org/pdf/2503.06859v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting (GS) along with its extensions and variants provides
outstanding performance in real-time scene rendering while meeting reduced
storage demands and computational efficiency. While the selection of 2D images
capturing the scene of interest is crucial for the proper initialization and
training of GS, hence markedly affecting the rendering performance, prior works
rely on passively and typically densely selected 2D images. In contrast, this
paper proposes `ActiveInitSplat', a novel framework for active selection of
training images for proper initialization and training of GS. ActiveInitSplat
relies on density and occupancy criteria of the resultant 3D scene
representation from the selected 2D images, to ensure that the latter are
captured from diverse viewpoints leading to better scene coverage and that the
initialized Gaussian functions are well aligned with the actual 3D structure.
Numerical tests on well-known simulated and real environments demonstrate the
merits of ActiveInitSplat resulting in significant GS rendering performance
improvement over passive GS baselines, in the widely adopted LPIPS, SSIM, and
PSNR metrics.



---

## DirectTriGS: Triplane-based Gaussian Splatting Field Representation for  3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Xiaoliang Ju, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2503.06900v1){: .btn .btn-green } |

**Abstract**: We present DirectTriGS, a novel framework designed for 3D object generation
with Gaussian Splatting (GS). GS-based rendering for 3D content has gained
considerable attention recently. However, there has been limited exploration in
directly generating 3D Gaussians compared to traditional generative modeling
approaches. The main challenge lies in the complex data structure of GS
represented by discrete point clouds with multiple channels. To overcome this
challenge, we propose employing the triplane representation, which allows us to
represent Gaussian Splatting as an image-like continuous field. This
representation effectively encodes both the geometry and texture information,
enabling smooth transformation back to Gaussian point clouds and rendering into
images by a TriRenderer, with only 2D supervisions. The proposed TriRenderer is
fully differentiable, so that the rendering loss can supervise both texture and
geometry encoding. Furthermore, the triplane representation can be compressed
using a Variational Autoencoder (VAE), which can subsequently be utilized in
latent diffusion to generate 3D objects. The experiments demonstrate that the
proposed generation framework can produce high-quality 3D object geometry and
rendering results in the text-to-3D task.

Comments:
- Accepted by CVPR 2025

---

## All That Glitters Is Not Gold: Key-Secured 3D Secrets within 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Yan Ren, Shilin Lu, Adams Wai-Kin Kong | cs.GR | [PDF](http://arxiv.org/pdf/2503.07191v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have revolutionized scene
reconstruction, opening new possibilities for 3D steganography by hiding 3D
secrets within 3D covers. The key challenge in steganography is ensuring
imperceptibility while maintaining high-fidelity reconstruction. However,
existing methods often suffer from detectability risks and utilize only
suboptimal 3DGS features, limiting their full potential. We propose a novel
end-to-end key-secured 3D steganography framework (KeySS) that jointly
optimizes a 3DGS model and a key-secured decoder for secret reconstruction. Our
approach reveals that Gaussian features contribute unequally to secret hiding.
The framework incorporates a key-controllable mechanism enabling multi-secret
hiding and unauthorized access prevention, while systematically exploring
optimal feature update to balance fidelity and security. To rigorously evaluate
steganographic imperceptibility beyond conventional 2D metrics, we introduce
3D-Sinkhorn distance analysis, which quantifies distributional differences
between original and steganographic Gaussian parameters in the representation
space. Extensive experiments demonstrate that our method achieves
state-of-the-art performance in both cover and secret reconstruction while
maintaining high security levels, advancing the field of 3D steganography. Code
is available at https://github.com/RY-Paper/KeySS



---

## Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D  Environments

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Andrei Chubarau, Yinan Wang, James J. Clark | cs.CV | [PDF](http://arxiv.org/pdf/2503.07828v1){: .btn .btn-green } |

**Abstract**: We introduce Neural Radiance and Gaze Fields (NeRGs) as a novel approach for
representing visual attention patterns in 3D scenes. Our system renders a 2D
view of a 3D scene with a pre-trained Neural Radiance Field (NeRF) and
visualizes the gaze field for arbitrary observer positions, which may be
decoupled from the render camera perspective. We achieve this by augmenting a
standard NeRF with an additional neural network that models the gaze
probability distribution. The output of a NeRG is a rendered image of the scene
viewed from the camera perspective and a pixel-wise salience map representing
conditional probability that an observer fixates on a given surface within the
3D scene as visible in the rendered image. Much like how NeRFs perform novel
view synthesis, NeRGs enable the reconstruction of gaze patterns from arbitrary
perspectives within complex 3D scenes. To ensure consistent gaze
reconstructions, we constrain gaze prediction on the 3D structure of the scene
and model gaze occlusion due to intervening surfaces when the observer's
viewpoint is decoupled from the rendering camera. For training, we leverage
ground truth head pose data from skeleton tracking data or predictions from 2D
salience models. We demonstrate the effectiveness of NeRGs in a real-world
convenience store setting, where head pose tracking data is available.

Comments:
- 11 pages, 8 figures

---

## SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Jiahui Zhang, Fangneng Zhan, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.07476v1){: .btn .btn-green } |

**Abstract**: Anchor-based 3D Gaussian splatting (3D-GS) exploits anchor features in 3D
Gaussian prediction, which has achieved impressive 3D rendering quality with
reduced Gaussian redundancy. On the other hand, it often encounters the dilemma
among anchor features, model size, and rendering quality - large anchor
features lead to large 3D models and high-quality rendering whereas reducing
anchor features degrades Gaussian attribute prediction which leads to clear
artifacts in the rendered textures and geometries. We design SOGS, an
anchor-based 3D-GS technique that introduces second-order anchors to achieve
superior rendering quality and reduced anchor features and model size
simultaneously. Specifically, SOGS incorporates covariance-based second-order
statistics and correlation across feature dimensions to augment features within
each anchor, compensating for the reduced feature size and improving rendering
quality effectively. In addition, it introduces a selective gradient loss to
enhance the optimization of scene textures and scene geometries, leading to
high-quality rendering with small anchor features. Extensive experiments over
multiple widely adopted benchmarks show that SOGS achieves superior rendering
quality in novel view synthesis with clearly reduced model size.

Comments:
- Accepted by CVPR 2025

---

## POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Joey Wilson, Marcelino Almeida, Sachit Mahajan, Martin Labrie, Maani Ghaffari, Omid Ghasemalizadeh, Min Sun, Cheng-Hao Kuo, Arnab Sen | cs.CV | [PDF](http://arxiv.org/pdf/2503.07819v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel algorithm for quantifying uncertainty and
information gained within 3D Gaussian Splatting (3D-GS) through P-Optimality.
While 3D-GS has proven to be a useful world model with high-quality
rasterizations, it does not natively quantify uncertainty. Quantifying
uncertainty in parameters of 3D-GS is necessary to understand the information
gained from acquiring new images as in active perception, or identify redundant
images which can be removed from memory due to resource constraints in online
3D-GS SLAM. We propose to quantify uncertainty and information gain in 3D-GS by
reformulating the problem through the lens of optimal experimental design,
which is a classical solution to measuring information gain. By restructuring
information quantification of 3D-GS through optimal experimental design, we
arrive at multiple solutions, of which T-Optimality and D-Optimality perform
the best quantitatively and qualitatively as measured on two popular datasets.
Additionally, we propose a block diagonal approximation of the 3D-GS
uncertainty, which provides a measure of correlation for computing more
accurate information gain, at the expense of a greater computation cost.



---

## CATPlan: Loss-based Collision Prediction in End-to-End Autonomous  Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Ziliang Xiong, Shipeng Liu, Nathaniel Helgesen, Joakim Johnander, Per-Erik Forssen | cs.RO | [PDF](http://arxiv.org/pdf/2503.07425v1){: .btn .btn-green } |

**Abstract**: In recent years, there has been increased interest in the design, training,
and evaluation of end-to-end autonomous driving (AD) systems. One often
overlooked aspect is the uncertainty of planned trajectories predicted by these
systems, despite awareness of their own uncertainty being key to achieve safety
and robustness. We propose to estimate this uncertainty by adapting loss
prediction from the uncertainty quantification literature. To this end, we
introduce a novel light-weight module, dubbed CATPlan, that is trained to
decode motion and planning embeddings into estimates of the collision loss used
to partially supervise end-to-end AD systems. During inference, these estimates
are interpreted as collision risk. We evaluate CATPlan on the safety-critical,
nerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detect
collisions with a $54.8\%$ relative improvement to average precision over a
GMM-based baseline in which the predicted trajectory is compared to the
forecasted trajectories of other road users. Our findings indicate that the
addition of CATPlan can lead to safer end-to-end AD systems and hope that our
work will spark increased interest in uncertainty quantification for such
systems.



---

## CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation  Awareness for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Rui Song, Chenwei Liang, Yan Xia, Walter Zimmer, Hu Cao, Holger Caesar, Andreas Festag, Alois Knoll | cs.CV | [PDF](http://arxiv.org/pdf/2503.06744v1){: .btn .btn-green } |

**Abstract**: Dynamic scene rendering opens new avenues in autonomous driving by enabling
closed-loop simulations with photorealistic data, which is crucial for
validating end-to-end algorithms. However, the complex and highly dynamic
nature of traffic environments presents significant challenges in accurately
rendering these scenes. In this paper, we introduce a novel 4D Gaussian
Splatting (4DGS) approach, which incorporates context and temporal deformation
awareness to improve dynamic scene rendering. Specifically, we employ a 2D
semantic segmentation foundation model to self-supervise the 4D semantic
features of Gaussians, ensuring meaningful contextual embedding.
Simultaneously, we track the temporal deformation of each Gaussian across
adjacent frames. By aggregating and encoding both semantic and temporal
deformation features, each Gaussian is equipped with cues for potential
deformation compensation within 3D space, facilitating a more precise
representation of dynamic scenes. Experimental results show that our method
improves 4DGS's ability to capture fine details in dynamic scene rendering for
autonomous driving and outperforms other self-supervised methods in 4D
reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms
semantic features with each Gaussian, enabling broader applications.



---

## Pixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D  Gaussian Modeling


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Long Peng, Anran Wu, Wenbo Li, Peizhe Xia, Xueyuan Dai, Xinjie Zhang, Xin Di, Haoze Sun, Renjing Pei, Yang Wang, Yang Cao, Zheng-Jun Zha | cs.CV | [PDF](http://arxiv.org/pdf/2503.06617v1){: .btn .btn-green } |

**Abstract**: Arbitrary-scale super-resolution (ASSR) aims to reconstruct high-resolution
(HR) images from low-resolution (LR) inputs with arbitrary upsampling factors
using a single model, addressing the limitations of traditional SR methods
constrained to fixed-scale factors (\textit{e.g.}, $\times$ 2). Recent advances
leveraging implicit neural representation (INR) have achieved great progress by
modeling coordinate-to-pixel mappings. However, the efficiency of these methods
may suffer from repeated upsampling and decoding, while their reconstruction
fidelity and quality are constrained by the intrinsic representational
limitations of coordinate-based functions. To address these challenges, we
propose a novel ContinuousSR framework with a Pixel-to-Gaussian paradigm, which
explicitly reconstructs 2D continuous HR signals from LR images using Gaussian
Splatting. This approach eliminates the need for time-consuming upsampling and
decoding, enabling extremely fast arbitrary-scale super-resolution. Once the
Gaussian field is built in a single pass, ContinuousSR can perform
arbitrary-scale rendering in just 1ms per scale. Our method introduces several
key innovations. Through statistical ana

Comments:
- Tech Report

---

## Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy  Surface Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Xiaoming Peng, Yixin Yang, Yang Zhou, Hui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06587v1){: .btn .btn-green } |

**Abstract**: Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry
reconstruction quality than the popular 3DGS by using 2D surfels to approximate
thin surfaces. However, it falls short when dealing with glossy surfaces,
resulting in visible holes in these areas. We found the reflection
discontinuity causes the issue. To fit the jump from diffuse to specular
reflection at different viewing angles, depth bias is introduced in the
optimized Gaussian primitives. To address that, we first replace the depth
distortion loss in 2DGS with a novel depth convergence loss, which imposes a
strong constraint on depth continuity. Then, we rectified the depth criterion
in determining the actual surface, which fully accounts for all the
intersecting Gaussians along the ray. Qualitative and quantitative evaluations
across various datasets reveal that our method significantly improves
reconstruction quality, with more complete and accurate surfaces than 2DGS.



---

## Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate  Representation and Reconstruction of Neural Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Abdelaziz Bouzidi, Hamid Laga, Hazem Wannous | cs.CV | [PDF](http://arxiv.org/pdf/2503.06762v1){: .btn .btn-green } |

**Abstract**: Neural fields such as DeepSDF and Neural Radiance Fields have recently
revolutionized novel-view synthesis and 3D reconstruction from RGB images and
videos. However, achieving high-quality representation, reconstruction, and
rendering requires deep neural networks, which are slow to train and evaluate.
Although several acceleration techniques have been proposed, they often trade
off speed for memory. Gaussian splatting-based methods, on the other hand,
accelerate the rendering time but remain costly in terms of training speed and
memory needed to store the parameters of a large number of Gaussians. In this
paper, we introduce a novel neural representation that is fast, both at
training and inference times, and lightweight. Our key observation is that the
neurons used in traditional MLPs perform simple computations (a dot product
followed by ReLU activation) and thus one needs to use either wide and deep
MLPs or high-resolution and high-dimensional feature grids to parameterize
complex nonlinear functions. We show in this paper that by replacing
traditional neurons with Radial Basis Function (RBF) kernels, one can achieve
highly accurate representation of 2D (RGB images), 3D (geometry), and 5D
(radiance fields) signals with just a single layer of such neurons. The
representation is highly parallelizable, operates on low-resolution feature
grids, and is compact and memory-efficient. We demonstrate that the proposed
novel representation can be trained for 3D geometry representation in less than
15 seconds and for novel view synthesis in less than 15 mins. At runtime, it
can synthesize novel views at more than 60 fps without sacrificing quality.

Comments:
- Our code is available at https://grbfnet.github.io/

---

## D3DR: Lighting-Aware Object Insertion in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Vsevolod Skorokhodov, Nikita Durasov, Pascal Fua | cs.CV | [PDF](http://arxiv.org/pdf/2503.06740v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has become a popular technique for various 3D Computer
Vision tasks, including novel view synthesis, scene reconstruction, and dynamic
scene rendering. However, the challenge of natural-looking object insertion,
where the object's appearance seamlessly matches the scene, remains unsolved.
In this work, we propose a method, dubbed D3DR, for inserting a
3DGS-parametrized object into 3DGS scenes while correcting its lighting,
shadows, and other visual artifacts to ensure consistency, a problem that has
not been successfully addressed before. We leverage advances in diffusion
models, which, trained on real-world data, implicitly understand correct scene
lighting. After inserting the object, we optimize a diffusion-based Delta
Denoising Score (DDS)-inspired objective to adjust its 3D Gaussian parameters
for proper lighting correction. Utilizing diffusion model personalization
techniques to improve optimization quality, our approach ensures seamless
object insertion and natural appearance. Finally, we demonstrate the method's
effectiveness by comparing it to existing approaches, achieving 0.5 PSNR and
0.15 SSIM improvements in relighting quality.



---

## REArtGS: Reconstructing and Generating Articulated Objects via 3D  Gaussian Splatting with Geometric and Motion Constraints

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Di Wu, Liu Liu, Zhou Linli, Anran Huang, Liangtu Song, Qiaojun Yu, Qi Wu, Cewu Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.06677v1){: .btn .btn-green } |

**Abstract**: Articulated objects, as prevalent entities in human life, their 3D
representations play crucial roles across various applications. However,
achieving both high-fidelity textured surface reconstruction and dynamic
generation for articulated objects remains challenging for existing methods. In
this paper, we present REArtGS, a novel framework that introduces additional
geometric and motion constraints to 3D Gaussian primitives, enabling
high-quality textured surface reconstruction and generation for articulated
objects. Specifically, given multi-view RGB images of arbitrary two states of
articulated objects, we first introduce an unbiased Signed Distance Field (SDF)
guidance to regularize Gaussian opacity fields, enhancing geometry constraints
and improving surface reconstruction quality. Then we establish deformable
fields for 3D Gaussians constrained by the kinematic structures of articulated
objects, achieving unsupervised generation of surface meshes in unseen states.
Extensive experiments on both synthetic and real datasets demonstrate our
approach achieves high-quality textured surface reconstruction for given
states, and enables high-fidelity surface generation for unseen states. Codes
will be released within the next four months.

Comments:
- 11pages, 6 figures

---

## StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for  Superior 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Zexu Huang, Min Xu, Stuart Perry | cs.CV | [PDF](http://arxiv.org/pdf/2503.06462v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D reconstruction coupled with neural rendering
techniques have greatly improved the creation of photo-realistic 3D scenes,
influencing both academic research and industry applications. The technique of
3D Gaussian Splatting and its variants incorporate the strengths of both
primitive-based and volumetric representations, achieving superior rendering
quality. While 3D Geometric Scattering (3DGS) and its variants have advanced
the field of 3D representation, they fall short in capturing the stochastic
properties of non-local structural information during the training process.
Additionally, the initialisation of spherical functions in 3DGS-based methods
often fails to engage higher-order terms in early training rounds, leading to
unnecessary computational overhead as training progresses. Furthermore, current
3DGS-based approaches require training on higher resolution images to render
higher resolution outputs, significantly increasing memory demands and
prolonging training durations. We introduce StructGS, a framework that enhances
3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D
reconstruction. StructGS innovatively incorporates a patch-based SSIM loss,
dynamic spherical harmonics initialisation and a Multi-scale Residual Network
(MSRN) to address the above-mentioned limitations, respectively. Our framework
significantly reduces computational redundancy, enhances detail capture and
supports high-resolution rendering from low-resolution inputs. Experimentally,
StructGS demonstrates superior performance over state-of-the-art (SOTA) models,
achieving higher quality and more detailed renderings with fewer artifacts.



---

## Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical  Deformable Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Kai Li, Junhao Wang, William Han, Ding Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2503.06161v1){: .btn .btn-green } |

**Abstract**: Minimally invasive surgery (MIS) has transformed clinical practice by
reducing recovery times, minimizing complications, and enhancing precision.
Nonetheless, MIS inherently relies on indirect visualization and precise
instrument control, posing unique challenges. Recent advances in artificial
intelligence have enabled real-time surgical scene understanding through
techniques such as image classification, object detection, and segmentation,
with scene reconstruction emerging as a key element for enhanced intraoperative
guidance. Although neural radiance fields (NeRFs) have been explored for this
purpose, their substantial data requirements and slow rendering inhibit
real-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more
efficient alternative, achieving state-of-the-art performance in dynamic
surgical scene reconstruction. In this work, we introduce Feature-EndoGaussian
(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D
rendering to enable real-time semantic and scene reconstruction. By leveraging
pretrained segmentation foundation models, FEG incorporates semantic feature
distillation within the Gaussian deformation framework, thereby enhancing both
reconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG
achieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)
compared to leading methods. Additionally, on the EndoVis18 dataset, FEG
demonstrates competitive class-wise segmentation metrics while balancing model
size and real-time performance.

Comments:
- 14 pages, 5 figures

---

## StreamGS: Online Generalizable Gaussian Splatting Reconstruction for  Unposed Image Streams

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Yang LI, Jinglu Wang, Lei Chu, Xiao Li, Shiu-hong Kao, Ying-Cong Chen, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.06235v1){: .btn .btn-green } |

**Abstract**: The advent of 3D Gaussian Splatting (3DGS) has advanced 3D scene
reconstruction and novel view synthesis. With the growing interest of
interactive applications that need immediate feedback, online 3DGS
reconstruction in real-time is in high demand. However, none of existing
methods yet meet the demand due to three main challenges: the absence of
predetermined camera parameters, the need for generalizable 3DGS optimization,
and the necessity of reducing redundancy. We propose StreamGS, an online
generalizable 3DGS reconstruction method for unposed image streams, which
progressively transform image streams to 3D Gaussian streams by predicting and
aggregating per-frame Gaussians. Our method overcomes the limitation of the
initial point reconstruction \cite{dust3r} in tackling out-of-domain (OOD)
issues by introducing a content adaptive refinement. The refinement enhances
cross-frame consistency by establishing reliable pixel correspondences between
adjacent frames. Such correspondences further aid in merging redundant
Gaussians through cross-frame feature aggregation. The density of Gaussians is
thereby reduced, empowering online reconstruction by significantly lowering
computational and memory costs. Extensive experiments on diverse datasets have
demonstrated that StreamGS achieves quality on par with optimization-based
approaches but does so 150 times faster, and exhibits superior generalizability
in handling OOD scenes.

Comments:
- 8 pages

---

## ForestSplats: Deformable transient field for Gaussian Splatting in the  Wild

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Wongi Park, Myeongseok Nam, Siwon Kim, Sangwoo Jo, Soomok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.06179v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time
rendering speeds and high-quality results in static scenes. Although 3D-GS
shows effectiveness in static scenes, their performance significantly degrades
in real-world environments due to transient objects, lighting variations, and
diverse levels of occlusion. To tackle this, existing methods estimate
occluders or transient elements by leveraging pre-trained models or integrating
additional transient field pipelines. However, these methods still suffer from
two defects: 1) Using semantic features from the Vision Foundation model (VFM)
causes additional computational costs. 2) The transient field requires
significant memory to handle transient elements with per-view Gaussians and
struggles to define clear boundaries for occluders, solely relying on
photometric errors. To address these problems, we propose ForestSplats, a novel
approach that leverages the deformable transient field and a superpixel-aware
mask to efficiently represent transient elements in the 2D scene across
unconstrained image collections and effectively decompose static scenes from
transient distractors without VFM. We designed the transient field to be
deformable, capturing per-view transient elements. Furthermore, we introduce a
superpixel-aware mask that clearly defines the boundaries of occluders by
considering photometric errors and superpixels. Additionally, we propose
uncertainty-aware densification to avoid generating Gaussians within the
boundaries of occluders during densification. Through extensive experiments
across several benchmark datasets, we demonstrate that ForestSplats outperforms
existing methods without VFM and shows significant memory efficiency in
representing transient elements.



---

## SplatTalk: 3D VQA with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Anh Thai, Songyou Peng, Kyle Genova, Leonidas Guibas, Thomas Funkhouser | cs.CV | [PDF](http://arxiv.org/pdf/2503.06271v1){: .btn .btn-green } |

**Abstract**: Language-guided 3D scene understanding is important for advancing
applications in robotics, AR/VR, and human-computer interaction, enabling
models to comprehend and interact with 3D environments through natural
language. While 2D vision-language models (VLMs) have achieved remarkable
success in 2D VQA tasks, progress in the 3D domain has been significantly
slower due to the complexity of 3D data and the high cost of manual
annotations. In this work, we introduce SplatTalk, a novel method that uses a
generalizable 3D Gaussian Splatting (3DGS) framework to produce 3D tokens
suitable for direct input into a pretrained LLM, enabling effective zero-shot
3D visual question answering (3D VQA) for scenes with only posed images. During
experiments on multiple benchmarks, our approach outperforms both 3D models
trained specifically for the task and previous 2D-LMM-based models utilizing
only images (our setting), while achieving competitive performance with
state-of-the-art 3D LMMs that additionally utilize 3D inputs.



---

## SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting  Steganography

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Xuanyu Zhang, Jiarui Meng, Zhipei Xu, Shuzhou Yang, Yanmin Wu, Ronggang Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06118v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a premier method for 3D
representation due to its real-time rendering and high-quality outputs,
underscoring the critical need to protect the privacy of 3D assets. Traditional
NeRF steganography methods fail to address the explicit nature of 3DGS since
its point cloud files are publicly accessible. Existing GS steganography
solutions mitigate some issues but still struggle with reduced rendering
fidelity, increased computational demands, and security flaws, especially in
the security of the geometric structure of the visualized point cloud. To
address these demands, we propose a SecureGS, a secure and efficient 3DGS
steganography framework inspired by Scaffold-GS's anchor point design and
neural decoding. SecureGS uses a hybrid decoupled Gaussian encryption mechanism
to embed offsets, scales, rotations, and RGB attributes of the hidden 3D
Gaussian points in anchor point features, retrievable only by authorized users
through privacy-preserving neural networks. To further enhance security, we
propose a density region-aware anchor growing and pruning strategy that
adaptively locates optimal hiding regions without exposing hidden information.
Extensive experiments show that SecureGS significantly surpasses existing GS
steganography methods in rendering fidelity, speed, and security.

Comments:
- Accepted by ICLR 2025

---

## GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video  Diffusion for Single-Image 3D Object Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Ye Tao, Jiawei Zhang, Yahao Shi, Dongqing Zou, Bin Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.06136v1){: .btn .btn-green } |

**Abstract**: Image-based 3D generation has vast applications in robotics and gaming, where
high-quality, diverse outputs and consistent 3D representations are crucial.
However, existing methods have limitations: 3D diffusion models are limited by
dataset scarcity and the absence of strong pre-trained priors, while 2D
diffusion-based approaches struggle with geometric consistency. We propose a
method that leverages 2D diffusion models' implicit 3D reasoning ability while
ensuring 3D consistency via Gaussian-splatting-based geometric distillation.
Specifically, the proposed Gaussian Splatting Decoder enforces 3D consistency
by transforming SV3D latent outputs into an explicit 3D representation. Unlike
SV3D, which only relies on implicit 2D representations for video generation,
Gaussian Splatting explicitly encodes spatial and appearance attributes,
enabling multi-view consistency through geometric constraints. These
constraints correct view inconsistencies, ensuring robust geometric
consistency. As a result, our approach simultaneously generates high-quality,
multi-view-consistent images and accurate 3D models, providing a scalable
solution for single-image-based 3D generation and bridging the gap between 2D
Diffusion diversity and 3D structural coherence. Experimental results
demonstrate state-of-the-art multi-view consistency and strong generalization
across diverse datasets. The code will be made publicly available upon
acceptance.



---

## NeuraLoc: Visual Localization in Neural Implicit Map with Dual  Complementary Features

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Hongjia Zhai, Boming Zhao, Hai Li, Xiaokun Pan, Yijia He, Zhaopeng Cui, Hujun Bao, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06117v1){: .btn .btn-green } |

**Abstract**: Recently, neural radiance fields (NeRF) have gained significant attention in
the field of visual localization. However, existing NeRF-based approaches
either lack geometric constraints or require extensive storage for feature
matching, limiting their practical applications. To address these challenges,
we propose an efficient and novel visual localization approach based on the
neural implicit map with complementary features. Specifically, to enforce
geometric constraints and reduce storage requirements, we implicitly learn a 3D
keypoint descriptor field, avoiding the need to explicitly store point-wise
features. To further address the semantic ambiguity of descriptors, we
introduce additional semantic contextual feature fields, which enhance the
quality and reliability of 2D-3D correspondences. Besides, we propose
descriptor similarity distribution alignment to minimize the domain gap between
2D and 3D feature spaces during matching. Finally, we construct the matching
graph using both complementary descriptors and contextual features to establish
accurate 2D-3D correspondences for 6-DoF pose estimation. Compared with the
recent NeRF-based approaches, our method achieves a 3$\times$ faster training
speed and a 45$\times$ reduction in model storage. Extensive experiments on two
widely used datasets demonstrate that our approach outperforms or is highly
competitive with other state-of-the-art NeRF-based visual localization methods.
Project page:
\href{https://zju3dv.github.io/neuraloc}{https://zju3dv.github.io/neuraloc}

Comments:
- ICRA 2025

---

## STGA: Selective-Training Gaussian Head Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Hanzhi Guo, Yixiao Chen, Dongye Xiaonuo, Zeyu Tian, Dongdong Weng, Le Luo | cs.GR | [PDF](http://arxiv.org/pdf/2503.05196v1){: .btn .btn-green } |

**Abstract**: We propose selective-training Gaussian head avatars (STGA) to enhance the
details of dynamic head Gaussian. The dynamic head Gaussian model is trained
based on the FLAME parameterized model. Each Gaussian splat is embedded within
the FLAME mesh to achieve mesh-based animation of the Gaussian model. Before
training, our selection strategy calculates the 3D Gaussian splat to be
optimized in each frame. The parameters of these 3D Gaussian splats are
optimized in the training of each frame, while those of the other splats are
frozen. This means that the splats participating in the optimization process
differ in each frame, to improve the realism of fine details. Compared with
network-based methods, our method achieves better results with shorter training
time. Compared with mesh-based methods, our method produces more realistic
details within the same training time. Additionally, the ablation experiment
confirms that our method effectively enhances the quality of details.



---

## GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Kohei Honda, Takeshi Ishita, Yasuhiro Yoshimura, Ryo Yonitani | cs.RO | [PDF](http://arxiv.org/pdf/2503.05152v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel approach to image-goal navigation by integrating
3D Gaussian Splatting (3DGS) with Visual Navigation Models (VNMs), a method we
refer to as GSplatVNM. VNMs offer a promising paradigm for image-goal
navigation by guiding a robot through a sequence of point-of-view images
without requiring metrical localization or environment-specific training.
However, constructing a dense and traversable sequence of target viewpoints
from start to goal remains a central challenge, particularly when the available
image database is sparse. To address these challenges, we propose a 3DGS-based
viewpoint synthesis framework for VNMs that synthesizes intermediate viewpoints
to seamlessly bridge gaps in sparse data while significantly reducing storage
overhead. Experimental results in a photorealistic simulator demonstrate that
our approach not only enhances navigation efficiency but also exhibits
robustness under varying levels of image database sparsity.

Comments:
- 8 pages, 4 figures

---

## SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image  via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Linqi Yang, Xiongwei Zhao, Qihao Sun, Ke Wang, Ao Chen, Peng Kang | cs.CV | [PDF](http://arxiv.org/pdf/2503.05174v1){: .btn .btn-green } |

**Abstract**: 6-DoF pose estimation is a fundamental task in computer vision with
wide-ranging applications in augmented reality and robotics. Existing single
RGB-based methods often compromise accuracy due to their reliance on initial
pose estimates and susceptibility to rotational ambiguity, while approaches
requiring depth sensors or multi-view setups incur significant deployment
costs. To address these limitations, we introduce SplatPose, a novel framework
that synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neural
architecture to achieve high-precision pose estimation using only a single RGB
image. Central to our approach is the Dual-Attention Ray Scoring Network
(DARS-Net), which innovatively decouples positional and angular alignment
through geometry-domain attention mechanisms, explicitly modeling directional
dependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fine
optimization pipeline progressively refines pose estimates by aligning dense 2D
features between query images and 3DGS-synthesized views, effectively
correcting feature misalignment and depth errors from sparse ray sampling.
Experiments on three benchmark datasets demonstrate that SplatPose achieves
state-of-the-art 6-DoF pose estimation accuracy in single RGB settings,
rivaling approaches that depend on depth or multi-view images.

Comments:
- Submitted to IROS 2025

---

## D2GV: Deformable 2D Gaussian Splatting for Video Representation in  400FPS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Mufan Liu, Qi Yang, Miaoran Zhao, He Huang, Le Yang, Zhu Li, Yiling Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.05600v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representations (INRs) have emerged as a powerful approach
for video representation, offering versatility across tasks such as compression
and inpainting. However, their implicit formulation limits both
interpretability and efficacy, undermining their practicality as a
comprehensive solution. We propose a novel video representation based on
deformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key
objectives: 1) improved efficiency while delivering superior quality; 2)
enhanced scalability and interpretability; and 3) increased friendliness for
downstream tasks. Specifically, we initially divide the video sequence into
fixed-length Groups of Pictures (GoP) to allow parallel training and linear
scalability with video length. For each GoP, D2GV represents video frames by
applying differentiable rasterization to 2D Gaussians, which are deformed from
a canonical space into their corresponding timestamps. Notably, leveraging
efficient CUDA-based rasterization, D2GV converges fast and decodes at speeds
exceeding 400 FPS, while delivering quality that matches or surpasses
state-of-the-art INRs. Moreover, we incorporate a learnable pruning and
quantization strategy to streamline D2GV into a more compact representation. We
demonstrate D2GV's versatility in tasks including video interpolation,
inpainting and denoising, underscoring its potential as a promising solution
for video representation. Code is available at:
\href{https://github.com/Evan-sudo/D2GV}{https://github.com/Evan-sudo/D2GV}.



---

## Bayesian Fields: Task-driven Open-Set Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Dominic Maggio, Luca Carlone | cs.CV | [PDF](http://arxiv.org/pdf/2503.05949v1){: .btn .btn-green } |

**Abstract**: Open-set semantic mapping requires (i) determining the correct granularity to
represent the scene (e.g., how should objects be defined), and (ii) fusing
semantic knowledge across multiple 2D observations into an overall 3D
reconstruction -ideally with a high-fidelity yet low-memory footprint. While
most related works bypass the first issue by grouping together primitives with
similar semantics (according to some manually tuned threshold), we recognize
that the object granularity is task-dependent, and develop a task-driven
semantic mapping approach. To address the second issue, current practice is to
average visual embedding vectors over multiple views. Instead, we show the
benefits of using a probabilistic approach based on the properties of the
underlying visual-language foundation model, and leveraging Bayesian updating
to aggregate multiple observations of the scene. The result is Bayesian Fields,
a task-driven and probabilistic approach for open-set semantic mapping. To
enable high-fidelity objects and a dense scene representation, Bayesian Fields
uses 3D Gaussians which we cluster into task-relevant objects, allowing for
both easy 3D object extraction and reduced memory usage. We release Bayesian
Fields open-source at https: //github.com/MIT-SPARK/Bayesian-Fields.



---

## Free Your Hands: Lightweight Relightable Turntable Capture Pipeline

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jiahui Fan, Fujun Luan, MiloÅ¡ HaÅ¡an, Jian Yang, Beibei Wang | cs.GR | [PDF](http://arxiv.org/pdf/2503.05511v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) from multiple captured photos of an object is a
widely studied problem. Achieving high quality typically requires dense
sampling of input views, which can lead to frustrating and tedious manual
labor. Manually positioning cameras to maintain an optimal desired distribution
can be difficult for humans, and if a good distribution is found, it is not
easy to replicate. Additionally, the captured data can suffer from motion blur
and defocus due to human error. In this paper, we present a lightweight object
capture pipeline to reduce the manual workload and standardize the acquisition
setup. We use a consumer turntable to carry the object and a tripod to hold the
camera. As the turntable rotates, we automatically capture dense samples from
various views and lighting conditions; we can repeat this for several camera
positions. This way, we can easily capture hundreds of valid images in several
minutes without hands-on effort. However, in the object reference frame, the
light conditions vary; this is harmful to a standard NVS method like 3D
Gaussian splatting (3DGS) which assumes fixed lighting. We design a neural
radiance representation conditioned on light rotations, which addresses this
issue and allows relightability as an additional benefit. We demonstrate our
pipeline using 3DGS as the underlying framework, achieving competitive quality
compared to previous methods with exhaustive acquisition and showcasing its
potential for relighting and harmonization tasks.



---

## Self-Modeling Robots by Photographing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Kejun Hu, Peng Yu, Ning Tan | cs.RO | [PDF](http://arxiv.org/pdf/2503.05398v1){: .btn .btn-green } |

**Abstract**: Self-modeling enables robots to build task-agnostic models of their
morphology and kinematics based on data that can be automatically collected,
with minimal human intervention and prior information, thereby enhancing
machine intelligence. Recent research has highlighted the potential of
data-driven technology in modeling the morphology and kinematics of robots.
However, existing self-modeling methods suffer from either low modeling quality
or excessive data acquisition costs. Beyond morphology and kinematics, texture
is also a crucial component of robots, which is challenging to model and
remains unexplored. In this work, a high-quality, texture-aware, and link-level
method is proposed for robot self-modeling. We utilize three-dimensional (3D)
Gaussians to represent the static morphology and texture of robots, and cluster
the 3D Gaussians to construct neural ellipsoid bones, whose deformations are
controlled by the transformation matrices generated by a kinematic neural
network. The 3D Gaussians and kinematic neural network are trained using data
pairs composed of joint angles, camera parameters and multi-view images without
depth information. By feeding the kinematic neural network with joint angles,
we can utilize the well-trained model to describe the corresponding morphology,
kinematics and texture of robots at the link level, and render robot images
from different perspectives with the aid of 3D Gaussian splatting. Furthermore,
we demonstrate that the established model can be exploited to perform
downstream tasks such as motion planning and inverse kinematics.



---

## SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Xiaotong Huang, He Zhu, Zihan Liu, Weikai Lin, Xiaohong Liu, Zhezhi He, Jingwen Leng, Minyi Guo, Yu Feng | cs.GR | [PDF](http://arxiv.org/pdf/2503.05168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a crucial rendering technique for
many real-time applications. However, the limited hardware resources on today's
mobile platforms hinder these applications, as they struggle to achieve
real-time performance. In this paper, we propose SeeLe, a general framework
designed to accelerate the 3DGS pipeline for resource-constrained mobile
devices.
  Specifically, we propose two GPU-oriented techniques: hybrid preprocessing
and contribution-aware rasterization. Hybrid preprocessing alleviates the GPU
compute and memory pressure by reducing the number of irrelevant Gaussians
during rendering. The key is to combine our view-dependent scene representation
with online filtering. Meanwhile, contribution-aware rasterization improves the
GPU utilization at the rasterization stage by prioritizing Gaussians with high
contributions while reducing computations for those with low contributions.
Both techniques can be seamlessly integrated into existing 3DGS pipelines with
minimal fine-tuning. Collectively, our framework achieves 2.6$\times$ speedup
and 32.3\% model reduction while achieving superior rendering quality compared
to existing methods.



---

## LiDAR-enhanced 3D Gaussian Splatting Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jian Shen, Huai Yu, Ji Wu, Wen Yang, Gui-Song Xia | cs.RO | [PDF](http://arxiv.org/pdf/2503.05425v1){: .btn .btn-green } |

**Abstract**: This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting
(3DGS) mapping framework that improves the accuracy and robustness of 3D scene
mapping by integrating LiDAR data. LiGSM constructs joint loss from images and
LiDAR point clouds to estimate the poses and optimize their extrinsic
parameters, enabling dynamic adaptation to variations in sensor alignment.
Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a
denser and more reliable starting points compared to sparse SfM points. In
scene rendering, the framework augments standard image-based supervision with
depth maps generated from LiDAR projections, ensuring an accurate scene
representation in both geometry and photometry. Experiments on public and
self-collected datasets demonstrate that LiGSM outperforms comparative methods
in pose tracking and scene rendering.

Comments:
- Accepted by ICRA 2025

---

## EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D  Gaussian Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Chao Zhang, Yifeng Zhou, Shuheng Wang, Wenfa Li, Degang Wang, Yi Xu, Shaohui Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2503.05162v1){: .btn .btn-green } |

**Abstract**: We have recently seen great progress in 3D scene reconstruction through
explicit point-based 3D Gaussian Splatting (3DGS), notable for its high quality
and fast rendering speed. However, reconstructing dynamic scenes such as
complex human performances with long durations remains challenging. Prior
efforts fall short of modeling a long-term sequence with drastic motions,
frequent topology changes or interactions with props, and resort to segmenting
the whole sequence into groups of frames that are processed independently,
which undermines temporal stability and thereby leads to an unpleasant viewing
experience and inefficient storage footprint. In view of this, we introduce
EvolvingGS, a two-stage strategy that first deforms the Gaussian model to
coarsely align with the target frame, and then refines it with minimal point
addition/subtraction, particularly in fast-changing areas. Owing to the
flexibility of the incrementally evolving representation, our method
outperforms existing approaches in terms of both per-frame and temporal quality
metrics while maintaining fast rendering through its purely explicit
representation. Moreover, by exploiting temporal coherence between successive
frames, we propose a simple yet effective compression algorithm that achieves
over 50x compression rate. Extensive experiments on both public benchmarks and
challenging custom datasets demonstrate that our method significantly advances
the state-of-the-art in dynamic scene reconstruction, particularly for extended
sequences with complex human performances.



---

## Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot  Manipulation of Irregularly Shaped Objects


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2503.05189v1){: .btn .btn-green } |

**Abstract**: Tracking and manipulating irregularly-shaped, previously unseen objects in
dynamic environments is important for robotic applications in manufacturing,
assembly, and logistics. Recently introduced Gaussian Splats efficiently model
object geometry, but lack persistent state estimation for task-oriented
manipulation. We present Persistent Object Gaussian Splat (POGS), a system that
embeds semantics, self-supervised visual features, and object grouping features
into a compact representation that can be continuously updated to estimate the
pose of scanned objects. POGS updates object states without requiring expensive
rescanning or prior CAD models of objects. After an initial multi-view scene
capture and training phase, POGS uses a single stereo camera to integrate depth
estimates along with self-supervised vision encoder features for object pose
estimation. POGS supports grasping, reorientation, and natural language-driven
manipulation by refining object pose estimates, facilitating sequential object
reset operations with human-induced object perturbations and tool servoing,
where robots recover tool pose despite tool perturbations of up to 30{\deg}.
POGS achieves up to 12 consecutive successful object resets and recovers from
80% of in-grasp tool perturbations.

Comments:
- Accepted to ICRA 2025

---

## MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface  Reconstruction under Various Light Conditions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Qingyuan Zhou, Yuehu Gong, Weidong Yang, Jiaze Li, Yeqi Luo, Baixin Xu, Shuhao Li, Ben Fei, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2503.05182v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) and surface reconstruction (SR) are essential
tasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks
are often addressed independently, with GS-based rendering methods struggling
under diverse light conditions and failing to produce accurate surfaces, while
GS-based reconstruction methods frequently compromise rendering quality. This
raises a central question: must rendering and reconstruction always involve a
trade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian
splatting for Surface Reconstruction that enhances both rendering quality and
3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS
and the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,
providing precise geometry information to the 3D-GS branch. Leveraging this
geometry, the 3D-GS branch employs a geometry-guided illumination decomposition
module that captures reflected and transmitted components, enabling realistic
rendering under varied light conditions. Using the transmitted component as
supervision, the 2D-GS branch also achieves high-fidelity surface
reconstruction. Throughout the optimization process, the 2D-GS and 3D-GS
branches undergo alternating optimization, providing mutual supervision. Prior
to this, each branch completes an independent warm-up phase, with an early
stopping strategy implemented to reduce computational costs. We evaluate MGSR
on a diverse set of synthetic and real-world datasets, at both object and scene
levels, demonstrating strong performance in rendering and surface
reconstruction.

Comments:
- 11 pages, 7 figures

---

## Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D  Gaussian Splatting from Sparse Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Yingji Zhong, Zhihao Li, Dave Zhenyu Chen, Lanqing Hong, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.05082v1){: .btn .btn-green } |

**Abstract**: Despite recent successes in novel view synthesis using 3D Gaussian Splatting
(3DGS), modeling scenes with sparse inputs remains a challenge. In this work,
we address two critical yet overlooked issues in real-world sparse-input
modeling: extrapolation and occlusion. To tackle these issues, we propose to
use a reconstruction by generation pipeline that leverages learned priors from
video diffusion models to provide plausible interpretations for regions outside
the field of view or occluded. However, the generated sequences exhibit
inconsistencies that do not fully benefit subsequent 3DGS modeling. To address
the challenge of inconsistencies, we introduce a novel scene-grounding guidance
based on rendered sequences from an optimized 3DGS, which tames the diffusion
model to generate consistent sequences. This guidance is training-free and does
not require any fine-tuning of the diffusion model. To facilitate holistic
scene modeling, we also propose a trajectory initialization method. It
effectively identifies regions that are outside the field of view and occluded.
We further design a scheme tailored for 3DGS optimization with generated
sequences. Experiments demonstrate that our method significantly improves upon
the baseline and achieves state-of-the-art performance on challenging
benchmarks.

Comments:
- Accepted by CVPR2025. The project page is available at
  https://zhongyingji.github.io/guidevd-3dgs/

---

## CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from  Motion-Blurred Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jungho Lee, Donghyeong Kim, Dogyoon Lee, Suhwan Cho, Minhyeok Lee, Wonjoon Lee, Taeoh Kim, Dongyoon Wee, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.05332v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for their
high-quality novel view rendering, motivating research to address real-world
challenges. A critical issue is the camera motion blur caused by movement
during exposure, which hinders accurate 3D scene reconstruction. In this study,
we propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that
reconstructs precise 3D scenes from motion-blurred images while maintaining
real-time rendering speed. Considering the complex motion patterns inherent in
real-world camera movements, we predict continuous camera trajectories using
neural ordinary differential equations (ODEs). To ensure accurate modeling, we
employ rigid body transformations, preserving the shape and size of the object
but rely on the discrete integration of sampled frames. To better approximate
the continuous nature of motion blur, we introduce a continuous motion
refinement (CMR) transformation that refines rigid transformations by
incorporating additional learnable parameters. By revisiting fundamental camera
theory and leveraging advanced neural ODE techniques, we achieve precise
modeling of continuous camera trajectories, leading to improved reconstruction
accuracy. Extensive experiments demonstrate state-of-the-art performance both
quantitatively and qualitatively on benchmark datasets, which include a wide
range of motion blur scenarios, from moderate to extreme blur.

Comments:
- Revised Version of CRiM-GS, Github:
  https://github.com/Jho-Yonsei/CoMoGaussian

---

## GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three  Orthographic Views Using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Zheng Zhou, Zhe Li, Bo Yu, Lina Hu, Liang Dong, Zijian Yang, Xiaoli Liu, Ning Xu, Ziwei Wang, Yonghao Dang, Jianqin Yin | cs.CV | [PDF](http://arxiv.org/pdf/2503.05161v1){: .btn .btn-green } |

**Abstract**: The automatic reconstruction of 3D computer-aided design (CAD) models from
CAD sketches has recently gained significant attention in the computer vision
community. Most existing methods, however, rely on vector CAD sketches and 3D
ground truth for supervision, which are often difficult to be obtained in
industrial applications and are sensitive to noise inputs. We propose viewing
CAD reconstruction as a specific instance of sparse-view 3D reconstruction to
overcome these limitations. While this reformulation offers a promising
perspective, existing 3D reconstruction methods typically require natural
images and corresponding camera poses as inputs, which introduces two major
significant challenges: (1) modality discrepancy between CAD sketches and
natural images, and (2) difficulty of accurate camera pose estimation for CAD
sketches. To solve these issues, we first transform the CAD sketches into
representations resembling natural images and extract corresponding masks.
Next, we manually calculate the camera poses for the orthographic views to
ensure accurate alignment within the 3D coordinate system. Finally, we employ a
customized sparse-view 3D reconstruction method to achieve high-quality
reconstructions from aligned orthographic views. By leveraging raster CAD
sketches for self-supervision, our approach eliminates the reliance on vector
CAD sketches and 3D ground truth. Experiments on the Sub-Fusion360 dataset
demonstrate that our proposed method significantly outperforms previous
approaches in CAD reconstruction performance and exhibits strong robustness to
noisy inputs.



---

## GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian  Radiance Fields and Differentiable Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Qianzhong Chen, Jiankai Sun, Naixiang Gao, JunEn Low, Timothy Chen, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2503.03984v1){: .btn .btn-green } |

**Abstract**: Autonomous visual navigation is an essential element in robot autonomy.
Reinforcement learning (RL) offers a promising policy training paradigm.
However existing RL methods suffer from high sample complexity, poor
sim-to-real transfer, and limited runtime adaptability to navigation scenarios
not seen during training. These problems are particularly challenging for
drones, with complex nonlinear and unstable dynamics, and strong dynamic
coupling between control and perception. In this paper, we propose a novel
framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep
reinforcement learning (DDRL) to train vision-based drone navigation policies.
By leveraging high-fidelity 3D scene representations and differentiable
simulation, our method improves sample efficiency and sim-to-real transfer.
Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt
to environmental variations at runtime. Moreover, by curriculum training in a
mixture of different surrounding environments, we achieve in-task
generalization, the ability to solve new instances of a task not seen during
training. Drone hardware experiments demonstrate our method's high training
efficiency compared to state-of-the-art RL methods, zero shot sim-to-real
transfer for real robot deployment without fine tuning, and ability to adapt to
new instances within the same task class (e.g. to fly through a gate at
different locations with different distractors in the environment).



---

## Instrument-Splatting: Controllable Photorealistic Reconstruction of  Surgical Instruments Using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Shuojue Yang, Zijian Wu, Mingxuan Hong, Qian Li, Daiyun Shen, Septimiu E. Salcudean, Yueming Jin | cs.CV | [PDF](http://arxiv.org/pdf/2503.04082v1){: .btn .btn-green } |

**Abstract**: Real2Sim is becoming increasingly important with the rapid development of
surgical artificial intelligence (AI) and autonomy. In this work, we propose a
novel Real2Sim methodology, \textit{Instrument-Splatting}, that leverages 3D
Gaussian Splatting to provide fully controllable 3D reconstruction of surgical
instruments from monocular surgical videos. To maintain both high visual
fidelity and manipulability, we introduce a geometry pre-training to bind
Gaussian point clouds on part mesh with accurate geometric priors and define a
forward kinematics to control the Gaussians as flexible as real instruments.
Afterward, to handle unposed videos, we design a novel instrument pose tracking
method leveraging semantics-embedded Gaussians to robustly refine per-frame
instrument poses and joint states in a render-and-compare manner, which allows
our instrument Gaussian to accurately learn textures and reach photorealistic
rendering. We validated our method on 2 publicly released surgical videos and 4
videos collected on ex vivo tissues and green screens. Quantitative and
qualitative evaluations demonstrate the effectiveness and superiority of the
proposed method.

Comments:
- 11 pages, 5 figures

---

## S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Yecong Wan, Mingwen Shao, Yuanshuo Cheng, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2503.04314v1){: .btn .btn-green } |

**Abstract**: In this paper, we aim ambitiously for a realistic yet challenging problem,
namely, how to reconstruct high-quality 3D scenes from sparse low-resolution
views that simultaneously suffer from deficient perspectives and clarity.
Whereas existing methods only deal with either sparse views or low-resolution
observations, they fail to handle such hybrid and complicated scenarios. To
this end, we propose a novel Sparse-view Super-resolution 3D Gaussian Splatting
framework, dubbed S2Gaussian, that can reconstruct structure-accurate and
detail-faithful 3D scenes with only sparse and low-resolution views. The
S2Gaussian operates in a two-stage fashion. In the first stage, we initially
optimize a low-resolution Gaussian representation with depth regularization and
densify it to initialize the high-resolution Gaussians through a tailored
Gaussian Shuffle Split operation. In the second stage, we refine the
high-resolution Gaussians with the super-resolved images generated from both
original sparse views and pseudo-views rendered by the low-resolution
Gaussians. In which a customized blur-free inconsistency modeling scheme and a
3D robust optimization strategy are elaborately designed to mitigate multi-view
inconsistency and eliminate erroneous updates caused by imperfect supervision.
Extensive experiments demonstrate superior results and in particular
establishing new state-of-the-art performances with more consistent geometry
and finer details.

Comments:
- CVPR 2025

---

## Beyond Existance: Fulfill 3D Reconstructed Scenes with Pseudo Details

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Yifei Gao, Jun Huang, Lei Wang, Ruiting Dai, Jun Cheng | cs.GR | [PDF](http://arxiv.org/pdf/2503.04037v1){: .btn .btn-green } |

**Abstract**: The emergence of 3D Gaussian Splatting (3D-GS) has significantly advanced 3D
reconstruction by providing high fidelity and fast training speeds across
various scenarios. While recent efforts have mainly focused on improving model
structures to compress data volume or reduce artifacts during zoom-in and
zoom-out operations, they often overlook an underlying issue: training sampling
deficiency. In zoomed-in views, Gaussian primitives can appear unregulated and
distorted due to their dilation limitations and the insufficient availability
of scale-specific training samples. Consequently, incorporating pseudo-details
that ensure the completeness and alignment of the scene becomes essential. In
this paper, we introduce a new training method that integrates diffusion models
and multi-scale training using pseudo-ground-truth data. This approach not only
notably mitigates the dilation and zoomed-in artifacts but also enriches
reconstructed scenes with precise details out of existing scenarios. Our method
achieves state-of-the-art performance across various benchmarks and extends the
capabilities of 3D reconstruction beyond training datasets.



---

## Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall, Yumeng Wang, Paul Maria Scheikl, Axel Krieger | cs.CV | [PDF](http://arxiv.org/pdf/2503.04079v1){: .btn .btn-green } |

**Abstract**: Accurate geometric reconstruction of deformable tissues in monocular
endoscopic video remains a fundamental challenge in robot-assisted minimally
invasive surgery. Although recent volumetric and point primitive methods based
on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently
rendered surgical scenes, they still struggle with handling artifact-free tool
occlusions and preserving fine anatomical details. These limitations stem from
unrestricted Gaussian scaling and insufficient surface alignment constraints
during reconstruction. To address these issues, we introduce Surgical Gaussian
Surfels (SGS), which transforms anisotropic point primitives into
surface-aligned elliptical splats by constraining the scale component of the
Gaussian covariance matrix along the view-aligned axis. We predict accurate
surfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled
with locality constraints to handle complex tissue deformations. We use
homodirectional view-space positional gradients to capture fine image details
by splitting Gaussian Surfels in over-reconstructed regions. In addition, we
define surface normals as the direction of the steepest density change within
each Gaussian surfel primitive, enabling accurate normal estimation without
requiring monocular normal priors. We evaluate our method on two in-vivo
surgical datasets, where it outperforms current state-of-the-art methods in
surface geometry, normal map quality, and rendering efficiency, while remaining
competitive in real-time rendering performance. We make our code available at
https://github.com/aloma85/SurgicalGaussianSurfels



---

## GaussianVideo: Efficient Video Representation and Compression by  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Inseo Lee, Youngyoon Choi, Joonseok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.04333v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representation for Videos (NeRV) has introduced a novel
paradigm for video representation and compression, outperforming traditional
codecs. As model size grows, however, slow encoding and decoding speed and high
memory consumption hinder its application in practice. To address these
limitations, we propose a new video representation and compression method based
on 2D Gaussian Splatting to efficiently handle video data. Our proposed
deformable 2D Gaussian Splatting dynamically adapts the transformation of 2D
Gaussians at each frame, significantly reducing memory cost. Equipped with a
multi-plane-based spatiotemporal encoder and a lightweight decoder, it predicts
changes in color, coordinates, and shape of initialized Gaussians, given the
time step. By leveraging temporal gradients, our model effectively captures
temporal redundancy at negligible cost, significantly enhancing video
representation efficiency. Our method reduces GPU memory usage by up to 78.4%,
and significantly expedites video processing, achieving 5.5x faster training
and 12.5x faster decoding compared to the state-of-the-art NeRV methods.



---

## GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world  Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Xihan Wang, Dianyi Yang, Yu Gao, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.04034v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting(3DGS) have significantly
improved semantic scene understanding, enabling natural language queries to
localize objects within a scene. However, existing methods primarily focus on
embedding compressed CLIP features to 3D Gaussians, suffering from low object
segmentation accuracy and lack spatial reasoning capabilities. To address these
limitations, we propose GaussianGraph, a novel framework that enhances
3DGS-based scene understanding by integrating adaptive semantic clustering and
scene graph generation. We introduce a "Control-Follow" clustering strategy,
which dynamically adapts to scene scale and feature distribution, avoiding
feature compression and significantly improving segmentation accuracy.
Additionally, we enrich scene representation by integrating object attributes
and spatial relations extracted from 2D foundation models. To address
inaccuracies in spatial relationships, we propose 3D correction modules that
filter implausible relations through spatial consistency verification, ensuring
reliable scene graph construction. Extensive experiments on three datasets
demonstrate that GaussianGraph outperforms state-of-the-art methods in both
semantic segmentation and object grounding tasks, providing a robust solution
for complex scene understanding and interaction.



---

## LensDFF: Language-enhanced Sparse Feature Distillation for Efficient  Few-Shot Dexterous Manipulation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-05 | Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll | cs.RO | [PDF](http://arxiv.org/pdf/2503.03890v1){: .btn .btn-green } |

**Abstract**: Learning dexterous manipulation from few-shot demonstrations is a significant
yet challenging problem for advanced, human-like robotic systems. Dense
distilled feature fields have addressed this challenge by distilling rich
semantic features from 2D visual foundation models into the 3D domain. However,
their reliance on neural rendering models such as Neural Radiance Fields (NeRF)
or Gaussian Splatting results in high computational costs. In contrast,
previous approaches based on sparse feature fields either suffer from
inefficiencies due to multi-view dependencies and extensive training or lack
sufficient grasp dexterity. To overcome these limitations, we propose
Language-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently
distills view-consistent 2D features onto 3D points using our novel
language-enhanced feature fusion strategy, thereby enabling single-view
few-shot generalization. Based on LensDFF, we further introduce a few-shot
dexterous manipulation framework that integrates grasp primitives into the
demonstrations to generate stable and highly dexterous grasps. Moreover, we
present a real2sim grasp evaluation pipeline for efficient grasp assessment and
hyperparameter tuning. Through extensive simulation experiments based on the
real2sim pipeline and real-world experiments, our approach achieves competitive
grasping performance, outperforming state-of-the-art approaches.

Comments:
- 8 pages

---

## NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian  Splatting Based on Thermodynamics


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-05 | Kun Yang, Yuxiang Liu, Zeyu Cui, Yu Liu, Maojun Zhang, Shen Yan, Qing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.03115v1){: .btn .btn-green } |

**Abstract**: Thermal infrared imaging offers the advantage of all-weather capability,
enabling non-intrusive measurement of an object's surface temperature.
Consequently, thermal infrared images are employed to reconstruct 3D models
that accurately reflect the temperature distribution of a scene, aiding in
applications such as building monitoring and energy management. However,
existing approaches predominantly focus on static 3D reconstruction for a
single time period, overlooking the impact of environmental factors on thermal
radiation and failing to predict or analyze temperature variations over time.
To address these challenges, we propose the NTR-Gaussian method, which treats
temperature as a form of thermal radiation, incorporating elements like
convective heat transfer and radiative heat dissipation. Our approach utilizes
neural networks to predict thermodynamic parameters such as emissivity,
convective heat transfer coefficient, and heat capacity. By integrating these
predictions, we can accurately forecast thermal temperatures at various times
throughout a nighttime scene. Furthermore, we introduce a dynamic dataset
specifically for nighttime thermal imagery. Extensive experiments and
evaluations demonstrate that NTR-Gaussian significantly outperforms comparison
methods in thermal reconstruction, achieving a predicted temperature error
within 1 degree Celsius.

Comments:
- IEEE Conference on Computer Vision and Pattern Recognition 2025

---

## Tracking-Aware Deformation Field Estimation for Non-rigid 3D  Reconstruction in Robotic Surgeries

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Zeqing Wang, Han Fang, Yihong Xu, Yutong Ban | cs.CV | [PDF](http://arxiv.org/pdf/2503.02558v1){: .btn .btn-green } |

**Abstract**: Minimally invasive procedures have been advanced rapidly by the robotic
laparoscopic surgery. The latter greatly assists surgeons in sophisticated and
precise operations with reduced invasiveness. Nevertheless, it is still safety
critical to be aware of even the least tissue deformation during
instrument-tissue interactions, especially in 3D space. To address this, recent
works rely on NeRF to render 2D videos from different perspectives and
eliminate occlusions. However, most of the methods fail to predict the accurate
3D shapes and associated deformation estimates robustly. Differently, we
propose Tracking-Aware Deformation Field (TADF), a novel framework which
reconstructs the 3D mesh along with the 3D tissue deformation simultaneously.
It first tracks the key points of soft tissue by a foundation vision model,
providing an accurate 2D deformation field. Then, the 2D deformation field is
smoothly incorporated with a neural implicit reconstruction network to obtain
tissue deformation in the 3D space. Finally, we experimentally demonstrate that
the proposed method provides more accurate deformation estimation compared with
other 3D neural reconstruction methods in two public datasets.



---

## 2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Qipeng Yan, Mingyang Sun, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.02452v1){: .btn .btn-green } |

**Abstract**: Real-time rendering of high-fidelity and animatable avatars from monocular
videos remains a challenging problem in computer vision and graphics. Over the
past few years, the Neural Radiance Field (NeRF) has made significant progress
in rendering quality but behaves poorly in run-time performance due to the low
efficiency of volumetric rendering. Recently, methods based on 3D Gaussian
Splatting (3DGS) have shown great potential in fast training and real-time
rendering. However, they still suffer from artifacts caused by inaccurate
geometry. To address these problems, we propose 2DGS-Avatar, a novel approach
based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars
with high-fidelity and fast training performance. Given monocular RGB videos as
input, our method generates an avatar that can be driven by poses and rendered
in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the
advantages of fast training and rendering while also capturing detailed,
dynamic, and photo-realistic appearances. We conduct abundant experiments on
popular datasets such as AvatarRex and THuman4.0, demonstrating impressive
performance in both qualitative and quantitative metrics.

Comments:
- ICVRV 2024

---

## Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic  Guidance from Dense Novel Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yingji Zhong, Kaichen Zhou, Zhihao Li, Lanqing Hong, Zhenguo Li, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.02230v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable capabilities for
photorealistic novel view synthesis. One major deficiency of NeRF is that dense
inputs are typically required, and the rendering quality will drop drastically
given sparse inputs. In this paper, we highlight the effectiveness of rendered
semantics from dense novel views, and show that rendered semantics can be
treated as a more robust form of augmented data than rendered RGB. Our method
enhances NeRF's performance by incorporating guidance derived from the rendered
semantics. The rendered semantic guidance encompasses two levels: the
supervision level and the feature level. The supervision-level guidance
incorporates a bi-directional verification module that decides the validity of
each rendered semantic label, while the feature-level guidance integrates a
learnable codebook that encodes semantic-aware information, which is queried by
each point via the attention mechanism to obtain semantic-relevant predictions.
The overall semantic guidance is embedded into a self-improved pipeline. We
also introduce a more challenging sparse-input indoor benchmark, where the
number of inputs is limited to as few as 6. Experiments demonstrate the
effectiveness of our method and it exhibits superior performance compared to
existing approaches.



---

## Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yan Miao, Will Shen, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2503.02198v1){: .btn .btn-green } |

**Abstract**: We present the first framework demonstrating zero-shot sim-to-real transfer
of visual control policies learned in a Neural Radiance Field (NeRF)
environment for quadrotors to fly through racing gates. Robust transfer from
simulation to real flight poses a major challenge, as standard simulators often
lack sufficient visual fidelity. To address this, we construct a photorealistic
simulation environment of quadrotor racing tracks, called FalconGym, which
provides effectively unlimited synthetic images for training. Within FalconGym,
we develop a pipelined approach for crossing gates that combines (i) a Neural
Pose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor
poses from single-frame RGB images and IMU data, and (ii) a
self-attention-based multi-modal controller that adaptively integrates visual
features and pose estimation. This multi-modal design compensates for
perception noise and intermittent gate visibility. We train this controller
purely in FalconGym with imitation learning and deploy the resulting policy to
real hardware with no additional fine-tuning. Simulation experiments on three
distinct tracks (circle, U-turn and figure-8) demonstrate that our controller
outperforms a vision-only state-of-the-art baseline in both success rate and
gate-crossing accuracy. In 30 live hardware flights spanning three tracks and
120 gates, our controller achieves a 95.8% success rate and an average error of
just 10 cm when flying through 38 cm-radius gates.



---

## DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Haoyuan Li, Ziqin Ye, Yue Hao, Weiyang Lin, Chao Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.02223v1){: .btn .btn-green } |

**Abstract**: Accurate object perception is essential for robotic applications such as
object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM
system that seamlessly integrates object pose estimation and reconstruction. We
employ 3D Gaussian Splatting for high-fidelity object reconstruction and
leverage quadrics for precise object pose estimation. Both of them management
is handled on the CPU, while optimization is performed on the GPU,
significantly improving system efficiency. By associating objects with unique
IDs, our system enables rapid object extraction from the scene. Extensive
experimental results on object reconstruction and pose estimation demonstrate
that DQO-MAP achieves outstanding performance in terms of precision,
reconstruction quality, and computational efficiency. The code and dataset are
available at: https://github.com/LiHaoy-ux/DQO-MAP.



---

## Data Augmentation for NeRFs in the Low Data Limit

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Ayush Gaggar, Todd D. Murphey | cs.CV | [PDF](http://arxiv.org/pdf/2503.02092v1){: .btn .btn-green } |

**Abstract**: Current methods based on Neural Radiance Fields fail in the low data limit,
particularly when training on incomplete scene data. Prior works augment
training data only in next-best-view applications, which lead to hallucinations
and model collapse with sparse data. In contrast, we propose adding a set of
views during training by rejection sampling from a posterior uncertainty
distribution, generated by combining a volumetric uncertainty estimator with
spatial coverage. We validate our results on partially observed scenes; on
average, our method performs 39.9% better with 87.5% less variability across
established scene reconstruction benchmarks, as compared to state of the art
baselines. We further demonstrate that augmenting the training set by sampling
from any distribution leads to better, more consistent scene reconstruction in
sparse environments. This work is foundational for robotic tasks where
augmenting a dataset with informative data is critical in resource-constrained,
a priori unknown environments. Videos and source code are available at
https://murpheylab.github.io/low-data-nerf/.

Comments:
- To be published in 2025 IEEE International Conference on Robotics and
  Automation (ICRA 2025)

---

## FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with  Sparse and Dense Map Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Yansong Xu, Junlin Li, Wei Zhang, Siyu Chen, Shengyong Zhang, Yuquan Leng, Weijia Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.01109v1){: .btn .btn-green } |

**Abstract**: 3D gaussian splatting has advanced simultaneous localization and mapping
(SLAM) technology by enabling real-time positioning and the construction of
high-fidelity maps. However, the uncertainty in gaussian position and
initialization parameters introduces challenges, often requiring extensive
iterative convergence and resulting in redundant or insufficient gaussian
representations. To address this, we introduce a novel adaptive densification
method based on Fourier frequency domain analysis to establish gaussian priors
for rapid convergence. Additionally, we propose constructing independent and
unified sparse and dense maps, where a sparse map supports efficient tracking
via Generalized Iterative Closest Point (GICP) and a dense map creates
high-fidelity visual representations. This is the first SLAM system leveraging
frequency domain analysis to achieve high-quality gaussian mapping in
real-time. Experimental results demonstrate an average frame rate of 36 FPS on
Replica and TUM RGB-D datasets, achieving competitive accuracy in both
localization and mapping.



---

## OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for  Object-Level Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Dianyi Yang, Yu Gao, Xihan Wang, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.01646v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have significantly improved the
efficiency and quality of dense semantic SLAM. However, previous methods are
generally constrained by limited-category pre-trained classifiers and implicit
semantic representation, which hinder their performance in open-set scenarios
and restrict 3D object-level scene understanding. To address these issues, we
propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian
representation to perform dense semantic SLAM in open-set environments. Our
system integrates explicit semantic labels derived from 2D foundational models
into the 3D Gaussian framework, facilitating robust 3D object-level scene
understanding. We introduce Gaussian Voting Splatting to enable fast 2D label
map rendering and scene updating. Additionally, we propose a Confidence-based
2D Label Consensus method to ensure consistent labeling across multiple views.
Furthermore, we employ a Segmentation Counter Pruning strategy to improve the
accuracy of semantic scene representation. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method
in scene understanding, tracking, and mapping, achieving 10 times faster
semantic rendering and 2 times lower storage costs compared to existing
methods. Project page: https://young-bit.github.io/opengs-github.github.io/.



---

## Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, Mohamed Sayed | cs.CV | [PDF](http://arxiv.org/pdf/2503.02009v1){: .btn .btn-green } |

**Abstract**: Exploring real-world spaces using novel-view synthesis is fun, and
reimagining those worlds in a different style adds another layer of excitement.
Stylized worlds can also be used for downstream tasks where there is limited
training data and a need to expand a model's training distribution. Most
current novel-view synthesis stylization techniques lack the ability to
convincingly change geometry. This is because any geometry change requires
increased style strength which is often capped for stylization stability and
consistency. In this work, we propose a new autoregressive 3D Gaussian
Splatting stylization method. As part of this method, we contribute a new RGBD
diffusion model that allows for strength control over appearance and shape
stylization. To ensure consistency across stylized frames, we use a combination
of novel depth-guided cross attention, feature injection, and a Warp ControlNet
conditioned on composite frames for guiding the stylization of new frames. We
validate our method via extensive qualitative results, quantitative
experiments, and a user study. Code will be released online.



---

## LiteGS: A High-Performance Modular Framework for Gaussian Splatting  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Kaimin Liao | cs.CV | [PDF](http://arxiv.org/pdf/2503.01199v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has emerged as a powerful technique for reconstruction of
3D scenes in computer graphics and vision. However, conventional
implementations often suffer from inefficiencies, limited flexibility, and high
computational overhead, which constrain their adaptability to diverse
applications. In this paper, we present LiteGS,a high-performance and modular
framework that enhances both the efficiency and usability of Gaussian
splatting. LiteGS achieves a 3.4x speedup over the original 3DGS implementation
while reducing GPU memory usage by approximately 30%. Its modular design
decomposes the splatting process into multiple highly optimized operators, and
it provides dual API support via a script-based interface and a CUDA-based
interface. The script-based interface, in combination with autograd, enables
rapid prototyping and straightforward customization of new ideas, while the
CUDA-based interface delivers optimal training speeds for performance-critical
applications. LiteGS retains the core algorithm of 3DGS, ensuring
compatibility. Comprehensive experiments on the Mip-NeRF 360 dataset
demonstrate that LiteGS accelerates training without compromising accuracy,
making it an ideal solution for both rapid prototyping and production
environments.



---

## Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling | cs.CV | [PDF](http://arxiv.org/pdf/2503.01774v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D
reconstruction and novel-view synthesis task. However, achieving photorealistic
rendering from extreme novel viewpoints remains challenging, as artifacts
persist across representations. In this work, we introduce Difix3D+, a novel
pipeline designed to enhance 3D reconstruction and novel-view synthesis through
single-step diffusion models. At the core of our approach is Difix, a
single-step image diffusion model trained to enhance and remove artifacts in
rendered novel views caused by underconstrained regions of the 3D
representation. Difix serves two critical roles in our pipeline. First, it is
used during the reconstruction phase to clean up pseudo-training views that are
rendered from the reconstruction and then distilled back into 3D. This greatly
enhances underconstrained regions and improves the overall 3D representation
quality. More importantly, Difix also acts as a neural enhancer during
inference, effectively removing residual artifacts arising from imperfect 3D
supervision and the limited capacity of current reconstruction models. Difix3D+
is a general solution, a single model compatible with both NeRF and 3DGS
representations, and it achieves an average 2$\times$ improvement in FID score
over baselines while maintaining 3D consistency.

Comments:
- CVPR 2025

---

## Category-level Meta-learned NeRF Priors for Efficient Object Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez | cs.CV | [PDF](http://arxiv.org/pdf/2503.01582v2){: .btn .btn-green } |

**Abstract**: In 3D object mapping, category-level priors enable efficient object
reconstruction and canonical pose estimation, requiring only a single prior per
semantic category (e.g., chair, book, laptop). Recently, DeepSDF has
predominantly been used as a category-level shape prior, but it struggles to
reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs
capture fine details but have yet to be effectively integrated with
category-level priors in a real-time multi-object mapping framework. To bridge
this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper
that integrates category-level priors with object-level NeRFs to enhance
reconstruction efficiency while enabling canonical object pose estimation.
PRENOM gets to know objects on a first-name basis by meta-learning on synthetic
reconstruction tasks generated from open-source shape datasets. To account for
object category variations, it employs a multi-objective genetic algorithm to
optimize the NeRF architecture for each category, balancing reconstruction
quality and training time. Additionally, prior-based probabilistic ray sampling
directs sampling toward expected object regions, accelerating convergence and
improving reconstruction quality under constrained resources. Experimental
results on a low-end GPU highlight the ability of PRENOM to achieve
high-quality reconstructions while maintaining computational feasibility.
Specifically, comparisons with prior-free NeRF-based approaches on a synthetic
dataset show a 21% lower Chamfer distance, demonstrating better reconstruction
quality. Furthermore, evaluations against other approaches using shape priors
on a noisy real-world dataset indicate a 13% improvement averaged across all
reconstruction metrics, and comparable pose and size estimation accuracy, while
being trained for 5x less time.



---

## DreamPrinting: Volumetric Printing Primitives for High-Fidelity 3D  Printing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Youjia Wang, Ruixiang Cao, Teng Xu, Yifei Liu, Dong Zhang, Yiwen Wu, Jingyi Yu | cs.GR | [PDF](http://arxiv.org/pdf/2503.00887v1){: .btn .btn-green } |

**Abstract**: Translating the rich visual fidelity of volumetric rendering techniques into
physically realizable 3D prints remains an open challenge. We introduce
DreamPrinting, a novel pipeline that transforms radiance-based volumetric
representations into explicit, material-centric Volumetric Printing Primitives
(VPPs). While volumetric rendering primitives (e.g., NeRF) excel at capturing
intricate geometry and appearance, they lack the physical constraints necessary
for real-world fabrication, such as pigment compatibility and material density.
DreamPrinting addresses these challenges by integrating the Kubelka-Munk model
with a spectrophotometric calibration process to characterize and mix pigments
for accurate reproduction of color and translucency. The result is a
continuous-to-discrete mapping that determines optimal pigment concentrations
for each voxel, ensuring fidelity to both geometry and optical properties. A 3D
stochastic halftoning procedure then converts these concentrations into
printable labels, enabling fine-grained control over opacity, texture, and
color gradients. Our evaluations show that DreamPrinting achieves exceptional
detail in reproducing semi-transparent structures-such as fur, leaves, and
clouds-while outperforming traditional surface-based methods in managing
translucency and internal consistency. Furthermore, by seamlessly integrating
VPPs with cutting-edge 3D generation techniques, DreamPrinting expands the
potential for complex, high-quality volumetric prints, providing a robust
framework for printing objects that closely mirror their digital origins.



---

## Evolving High-Quality Rendering and Reconstruction in a Unified  Framework with Contribution-Adaptive Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao | cs.CV | [PDF](http://arxiv.org/pdf/2503.00881v1){: .btn .btn-green } |

**Abstract**: Representing 3D scenes from multiview images is a core challenge in computer
vision and graphics, which requires both precise rendering and accurate
reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant
attention for its high-quality rendering and fast inference speed. Yet, due to
the unstructured and irregular nature of Gaussian point clouds, ensuring
accurate geometry reconstruction remains difficult. Existing methods primarily
focus on geometry regularization, with common approaches including
primitive-based and dual-model frameworks. However, the former suffers from
inherent conflicts between rendering and reconstruction, while the latter is
computationally and storage-intensive. To address these challenges, we propose
CarGS, a unified model leveraging Contribution-adaptive regularization to
achieve simultaneous, high-quality rendering and surface reconstruction. The
essence of our framework is learning adaptive contribution for Gaussian
primitives by squeezing the knowledge from geometry regularization into a
compact MLP. Additionally, we introduce a geometry-guided densification
strategy with clues from both normals and Signed Distance Fields (SDF) to
improve the capability of capturing high-frequency details. Our design improves
the mutual learning of the two tasks, meanwhile its unified structure does not
require separate models as in dual-model based approaches, guaranteeing
efficiency. Extensive experiments demonstrate the ability to achieve
state-of-the-art (SOTA) results in both rendering fidelity and reconstruction
accuracy while maintaining real-time speed and minimal storage size.



---

## DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Liao Shen, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy | cs.CV | [PDF](http://arxiv.org/pdf/2503.00746v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable
success in representing 3D scenes and generating high-quality, novel views in
real-time. However, 3D-GS and its variants assume that input images are
captured based on pinhole imaging and are fully in focus. This assumption
limits their applicability, as real-world images often feature shallow
depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable
depth-of-field method for 3D-GS. We develop a lens-based imaging model based on
geometric optics principles to control DoF effects. To ensure accurate scene
geometry, we incorporate depth priors adjusted per scene, and we apply
defocus-to-focus adaptation to minimize the gap in the circle of confusion. We
also introduce a synthetic dataset to assess refocusing capabilities and the
model's ability to learn precise lens parameters. Our framework is customizable
and supports various interactive applications. Extensive experiments confirm
the effectiveness of our method. Our project is available at
https://dof-gaussian.github.io.

Comments:
- CVPR 2025

---

## Enhancing Monocular 3D Scene Completion with Diffusion Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Changlin Song, Jiaqi Wang, Liyun Zhu, He Weng | cs.GR | [PDF](http://arxiv.org/pdf/2503.00726v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction is essential for applications in virtual reality,
robotics, and autonomous driving, enabling machines to understand and interact
with complex environments. Traditional 3D Gaussian Splatting techniques rely on
images captured from multiple viewpoints to achieve optimal performance, but
this dependence limits their use in scenarios where only a single image is
available. In this work, we introduce FlashDreamer, a novel approach for
reconstructing a complete 3D scene from a single image, significantly reducing
the need for multi-view inputs. Our approach leverages a pre-trained
vision-language model to generate descriptive prompts for the scene, guiding a
diffusion model to produce images from various perspectives, which are then
fused to form a cohesive 3D reconstruction. Extensive experiments show that our
method effectively and robustly expands single-image inputs into a
comprehensive 3D scene, extending monocular 3D reconstruction capabilities
without further training. Our code is available
https://github.com/CharlieSong1999/FlashDreamer/tree/main.

Comments:
- All authors had equal contribution

---

## PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency  Recovery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | BoCheng Li, WenJuan Zhang, Bing Zhang, YiLing Yao, YaNing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.00848v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D GS) achieves impressive results in novel view
synthesis for small, single-object scenes through Gaussian ellipsoid
initialization and adaptive density control. However, when applied to
large-scale remote sensing scenes, 3D GS faces challenges: the point clouds
generated by Structure-from-Motion (SfM) are often sparse, and the inherent
smoothing behavior of 3D GS leads to over-reconstruction in high-frequency
regions, where have detailed textures and color variations. This results in the
generation of large, opaque Gaussian ellipsoids that cause gradient artifacts.
Moreover, the simultaneous optimization of both geometry and texture may lead
to densification of Gaussian ellipsoids at incorrect geometric locations,
resulting in artifacts in other views. To address these issues, we propose
PSRGS, a progressive optimization scheme based on spectral residual maps.
Specifically, we create a spectral residual significance map to separate
low-frequency and high-frequency regions. In the low-frequency region, we apply
depth-aware and depth-smooth losses to initialize the scene geometry with low
threshold. For the high-frequency region, we use gradient features with higher
threshold to split and clone ellipsoids, refining the scene. The sampling rate
is determined by feature responses and gradient loss. Finally, we introduce a
pre-trained network that jointly computes perceptual loss from multiple views,
ensuring accurate restoration of high-frequency details in both Gaussian
ellipsoids geometry and color. We conduct experiments on multiple datasets to
assess the effectiveness of our method, which demonstrates competitive
rendering quality, especially in recovering texture details in high-frequency
regions.



---

## Vid2Fluid: 3D Dynamic Fluid Assets from Single-View Videos with  Generative Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Zhiwei Zhao, Alan Zhao, Minchen Li, Yixin Hu | cs.GR | [PDF](http://arxiv.org/pdf/2503.00868v1){: .btn .btn-green } |

**Abstract**: The generation of 3D content from single-view images has been extensively
studied, but 3D dynamic scene generation with physical consistency from videos
remains in its early stages. We propose a novel framework leveraging generative
3D Gaussian Splatting (3DGS) models to extract 3D dynamic fluid objects from
single-view videos. The fluid geometry represented by 3DGS is initially
generated from single-frame images, then denoised, densified, and aligned
across frames. We estimate the fluid surface velocity using optical flow and
compute the mainstream of the fluid to refine it. The 3D volumetric velocity
field is then derived from the enclosed surface. The velocity field is then
converted into a divergence-free, grid-based representation, enabling the
optimization of simulation parameters through its differentiability across
frames. This process results in simulation-ready fluid assets with physical
dynamics closely matching those observed in the source video. Our approach is
applicable to various fluid types, including gas, liquid, and viscous fluids,
and allows users to edit the output geometry or extend movement durations
seamlessly. Our automatic method for creating 3D dynamic fluid assets from
single-view videos, easily obtainable from the internet, shows great potential
for generating large-scale 3D fluid assets at a low cost.



---

## Abstract Rendering: Computing All that is Seen in Gaussian Splat Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yangge Li, Chenxi Ji, Xiangru Zhong, Huan Zhang, Sayan Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2503.00308v2){: .btn .btn-green } |

**Abstract**: We introduce abstract rendering, a method for computing a set of images by
rendering a scene from a continuously varying range of camera positions. The
resulting abstract image-which encodes an infinite collection of possible
renderings-is represented using constraints on the image matrix, enabling
rigorous uncertainty propagation through the rendering process. This capability
is particularly valuable for the formal verification of vision-based autonomous
systems and other safety-critical applications. Our approach operates on
Gaussian splat scenes, an emerging representation in computer vision and
robotics. We leverage efficient piecewise linear bound propagation to abstract
fundamental rendering operations, while addressing key challenges that arise in
matrix inversion and depth sorting-two operations not directly amenable to
standard approximations. To handle these, we develop novel linear relational
abstractions that maintain precision while ensuring computational efficiency.
These abstractions not only power our abstract rendering algorithm but also
provide broadly applicable tools for other rendering problems. Our
implementation, AbstractSplat, is optimized for scalability, handling up to
750k Gaussians while allowing users to balance memory and runtime through tile
and batch-based computation. Compared to the only existing abstract image
method for mesh-based scenes, AbstractSplat achieves 2-14x speedups while
preserving precision. Our results demonstrate that continuous camera motion,
rotations, and scene variations can be rigorously analyzed at scale, making
abstract rendering a powerful tool for uncertainty-aware vision applications.



---

## GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation  Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Runyi Li, Xuanyu Zhang, Chuhan Tong, Zhipei Xu, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.00531v1){: .btn .btn-green } |

**Abstract**: With the advancement of AIGC technologies, the modalities generated by models
have expanded from images and videos to 3D objects, leading to an increasing
number of works focused on 3D Gaussian Splatting (3DGS) generative models.
Existing research on copyright protection for generative models has primarily
concentrated on watermarking in image and text modalities, with little
exploration into the copyright protection of 3D object generative models. In
this paper, we propose the first bit watermarking framework for 3DGS generative
models, named GaussianSeal, to enable the decoding of bits as copyright
identifiers from the rendered outputs of generated 3DGS. By incorporating
adaptive bit modulation modules into the generative model and embedding them
into the network blocks in an adaptive way, we achieve high-precision bit
decoding with minimal training overhead while maintaining the fidelity of the
model's outputs. Experiments demonstrate that our method outperforms
post-processing watermarking approaches for 3DGS objects, achieving superior
performance of watermark decoding accuracy and preserving the quality of the
generated results.



---

## CAT-3DGS: A Context-Adaptive Triplane Approach to  Rate-Distortion-Optimized 3DGS Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, Wen-Hsiao Peng | cs.CV | [PDF](http://arxiv.org/pdf/2503.00357v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D
representation. Much research has been focused on reducing its storage
requirements and memory footprint. However, the needs to compress and transmit
the 3DGS representation to the remote side are overlooked. This new application
calls for rate-distortion-optimized 3DGS compression. How to quantize and
entropy encode sparse Gaussian primitives in the 3D space remains largely
unexplored. Few early attempts resort to the hyperprior framework from learned
image compression. But, they fail to utilize fully the inter and intra
correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work,
termed CAT-3DGS, introduces a context-adaptive triplane approach to their
rate-distortion-optimized coding. It features multi-scale triplanes, oriented
according to the principal axes of Gaussian primitives in the 3D space, to
capture their inter correlation (i.e. spatial correlation) for spatial
autoregressive coding in the projected 2D planes. With these triplanes serving
as the hyperprior, we further perform channel-wise autoregressive coding to
leverage the intra correlation within each individual Gaussian primitive. Our
CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively
skips from coding those Gaussian primitives that potentially have little impact
on the rendering quality. When trained end-to-end to strike a good
rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art
compression performance on the commonly used real-world datasets.

Comments:
- Accepted for Publication in International Conference on Learning
  Representations (ICLR)

---

## Seeing A 3D World in A Grain of Sand

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yufan Zhang, Yu Ji, Yu Guo, Jinwei Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.00260v1){: .btn .btn-green } |

**Abstract**: We present a snapshot imaging technique for recovering 3D surrounding views
of miniature scenes. Due to their intricacy, miniature scenes with objects
sized in millimeters are difficult to reconstruct, yet miniatures are common in
life and their 3D digitalization is desirable. We design a catadioptric imaging
system with a single camera and eight pairs of planar mirrors for snapshot 3D
reconstruction from a dollhouse perspective. We place paired mirrors on nested
pyramid surfaces for capturing surrounding multi-view images in a single shot.
Our mirror design is customizable based on the size of the scene for optimized
view coverage. We use the 3D Gaussian Splatting (3DGS) representation for scene
reconstruction and novel view synthesis. We overcome the challenge posed by our
sparse view input by integrating visual hull-derived depth constraint. Our
method demonstrates state-of-the-art performance on a variety of synthetic and
real miniature scenes.



---

## Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic  Pick-and-Place Setups

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Nicholas Pfaff, Evelyn Fu, Jeremy Binagia, Phillip Isola, Russ Tedrake | cs.RO | [PDF](http://arxiv.org/pdf/2503.00370v1){: .btn .btn-green } |

**Abstract**: Simulating object dynamics from real-world perception shows great promise for
digital twins and robotic manipulation but often demands labor-intensive
measurements and expertise. We present a fully automated Real2Sim pipeline that
generates simulation-ready assets for real-world objects through robotic
interaction. Using only a robot's joint torque sensors and an external camera,
the pipeline identifies visual geometry, collision geometry, and physical
properties such as inertial parameters. Our approach introduces a general
method for extracting high-quality, object-centric meshes from photometric
reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing
alpha-transparent training while explicitly distinguishing foreground
occlusions from background subtraction. We validate the full pipeline through
extensive experiments, demonstrating its effectiveness across diverse objects.
By eliminating the need for manual intervention or environment modifications,
our pipeline can be integrated directly into existing pick-and-place setups,
enabling scalable and efficient dataset creation.


