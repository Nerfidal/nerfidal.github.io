---
layout: default
title: March 2025
parent: Papers
nav_order: 202503
---

<!---metadata--->


## PGC: Physics-Based Gaussian Cloth from a Single Pose


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-26 | Michelle Guo, Matt Jen-Yuan Chiang, Igor Santesteban, Nikolaos Sarafianos, Hsiao-yu Chen, Oshri Halimi, Aljaž Božič, Shunsuke Saito, Jiajun Wu, C. Karen Liu, Tuur Stuyck, Egor Larionov | cs.GR | [PDF](http://arxiv.org/pdf/2503.20779v1){: .btn .btn-green } |

**Abstract**: We introduce a novel approach to reconstruct simulation-ready garments with
intricate appearance. Despite recent advancements, existing methods often
struggle to balance the need for accurate garment reconstruction with the
ability to generalize to new poses and body shapes or require large amounts of
data to achieve this. In contrast, our method only requires a multi-view
capture of a single static frame. We represent garments as hybrid mesh-embedded
3D Gaussian splats, where the Gaussians capture near-field shading and
high-frequency details, while the mesh encodes far-field albedo and optimized
reflectance parameters. We achieve novel pose generalization by exploiting the
mesh from our hybrid approach, enabling physics-based simulation and surface
rendering techniques, while also capturing fine details with Gaussians that
accurately reconstruct garment details. Our optimized garments can be used for
simulating garments on novel poses, and garment relighting. Project page:
https://phys-gaussian-cloth.github.io .



---

## EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View  Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-26 | Sheng Miao, Jiaxin Huang, Dongfeng Bai, Xu Yan, Hongyu Zhou, Yue Wang, Bingbing Liu, Andreas Geiger, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2503.20168v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis of urban scenes is essential for autonomous
driving-related applications.Existing NeRF and 3DGS-based methods show
promising results in achieving photorealistic renderings but require slow,
per-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian
Splatting model for urban scenes that works in a feed-forward manner. Unlike
existing feed-forward, pixel-aligned 3DGS methods, which often suffer from
issues like multi-view inconsistencies and duplicated content, our approach
predicts 3D Gaussians across multiple frames within a unified volume using a 3D
convolutional network. This is achieved by initializing 3D Gaussians with noisy
depth predictions, and then refining their geometric properties in 3D space and
predicting color based on 2D textures. Our model also handles distant views and
the sky with a flexible hemisphere background model. This enables us to perform
fast, feed-forward reconstruction while achieving real-time rendering.
Experimental evaluations on the KITTI-360 and Waymo datasets show that our
method achieves state-of-the-art quality compared to existing feed-forward
3DGS- and NeRF-based methods.

Comments:
- CVPR2025

---

## Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile  Gaussian Feature Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-26 | Shijie Zhou, Hui Ren, Yijia Weng, Shuwang Zhang, Zhen Wang, Dejia Xu, Zhiwen Fan, Suya You, Zhangyang Wang, Leonidas Guibas, Achuta Kadambi | cs.CV | [PDF](http://arxiv.org/pdf/2503.20776v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 2D and multimodal models have achieved remarkable
success by leveraging large-scale training on extensive datasets. However,
extending these achievements to enable free-form interactions and high-level
semantic operations with complex 3D/4D scenes remains challenging. This
difficulty stems from the limited availability of large-scale, annotated 3D/4D
or multi-view datasets, which are crucial for generalizable vision and language
tasks such as open-vocabulary and prompt-based segmentation, language-guided
editing, and visual question answering (VQA). In this paper, we introduce
Feature4X, a universal framework designed to extend any functionality from 2D
vision foundation model into the 4D realm, using only monocular video input,
which is widely available from user-generated content. The "X" in Feature4X
represents its versatility, enabling any task through adaptable,
model-conditioned 4D feature field distillation. At the core of our framework
is a dynamic optimization strategy that unifies multiple model capabilities
into a single representation. Additionally, to the best of our knowledge,
Feature4X is the first method to distill and lift the features of video
foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field
using Gaussian Splatting. Our experiments showcase novel view segment anything,
geometric and appearance scene editing, and free-form VQA across all time
steps, empowered by LLMs in feedback loops. These advancements broaden the
scope of agentic AI applications by providing a foundation for scalable,
contextually and spatiotemporally aware systems capable of immersive dynamic 4D
scene interaction.



---

## AccidentSim: Generating Physically Realistic Vehicle Collision Videos  from Real-World Accident Reports

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-26 | Xiangwen Zhang, Qian Zhang, Longfei Han, Qiang Qu, Xiaoming Chen | cs.CV | [PDF](http://arxiv.org/pdf/2503.20654v1){: .btn .btn-green } |

**Abstract**: Collecting real-world vehicle accident videos for autonomous driving research
is challenging due to their rarity and complexity. While existing driving video
generation methods may produce visually realistic videos, they often fail to
deliver physically realistic simulations because they lack the capability to
generate accurate post-collision trajectories. In this paper, we introduce
AccidentSim, a novel framework that generates physically realistic vehicle
collision videos by extracting and utilizing the physical clues and contextual
information available in real-world vehicle accident reports. Specifically,
AccidentSim leverages a reliable physical simulator to replicate post-collision
vehicle trajectories from the physical and contextual information in the
accident reports and to build a vehicle collision trajectory dataset. This
dataset is then used to fine-tune a language model, enabling it to respond to
user prompts and predict physically consistent post-collision trajectories
across various driving scenarios based on user descriptions. Finally, we employ
Neural Radiance Fields (NeRF) to render high-quality backgrounds, merging them
with the foreground vehicles that exhibit physically realistic trajectories to
generate vehicle collision videos. Experimental results demonstrate that the
videos produced by AccidentSim excel in both visual and physical authenticity.



---

## TC-GS: Tri-plane based compression for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-26 | Taorui Wang, Zitong Yu, Yong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.20221v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has emerged as a prominent framework
for novel view synthesis, providing high fidelity and rapid rendering speed.
However, the substantial data volume of 3DGS and its attributes impede its
practical utility, requiring compression techniques for reducing memory cost.
Nevertheless, the unorganized shape of 3DGS leads to difficulties in
compression. To formulate unstructured attributes into normative distribution,
we propose a well-structured tri-plane to encode Gaussian attributes,
leveraging the distribution of attributes for compression. To exploit the
correlations among adjacent Gaussians, K-Nearest Neighbors (KNN) is used when
decoding Gaussian distribution from the Tri-plane. We also introduce Gaussian
position information as a prior of the position-sensitive decoder.
Additionally, we incorporate an adaptive wavelet loss, aiming to focus on the
high-frequency details as iterations increase. Our approach has achieved
results that are comparable to or surpass that of SOTA 3D Gaussians Splatting
compression work in extensive experiments across multiple datasets. The codes
are released at https://github.com/timwang2001/TC-GS.

Comments:
- Accepted by ICME 2025

---

## MATT-GS: Masked Attention-based 3DGS for Robot Perception and Object  Detection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Jee Won Lee, Hansol Lim, SooYeun Yang, Jongseong Brad Choi | cs.GR | [PDF](http://arxiv.org/pdf/2503.19330v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel masked attention-based 3D Gaussian Splatting
(3DGS) approach to enhance robotic perception and object detection in
industrial and smart factory environments. U2-Net is employed for background
removal to isolate target objects from raw images, thereby minimizing clutter
and ensuring that the model processes only relevant data. Additionally, a Sobel
filter-based attention mechanism is integrated into the 3DGS framework to
enhance fine details - capturing critical features such as screws, wires, and
intricate textures essential for high-precision tasks. We validate our approach
using quantitative metrics, including L1 loss, SSIM, PSNR, comparing the
performance of the background-removed and attention-incorporated 3DGS model
against the ground truth images and the original 3DGS training baseline. The
results demonstrate significant improves in visual fidelity and detail
preservation, highlighting the effectiveness of our method in enhancing robotic
vision for object recognition and manipulation in complex industrial settings.

Comments:
- This work has been submitted to the 2025 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) for possible publication

---

## GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Shujuan Li, Yu-Shen Liu, Zhizhong Han | cs.CV | [PDF](http://arxiv.org/pdf/2503.19458v1){: .btn .btn-green } |

**Abstract**: Reconstructing open surfaces from multi-view images is vital in digitalizing
complex objects in daily life. A widely used strategy is to learn unsigned
distance functions (UDFs) by checking if their appearance conforms to the image
observations through neural rendering. However, it is still hard to learn
continuous and implicit UDF representations through 3D Gaussians splatting
(3DGS) due to the discrete and explicit scene representation, i.e., 3D
Gaussians. To resolve this issue, we propose a novel approach to bridge the gap
between 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2D
Gaussian planes on surfaces, and then, leverage the self-supervision and
gradient-based inference to supervise unsigned distances in both near and far
area to surfaces. To this end, we introduce novel constraints and strategies to
constrain the learning of 2D Gaussians to pursue more stable optimization and
more reliable self-supervision, addressing the challenges brought by
complicated gradient field on or near the zero level set of UDFs. We report
numerical and visual comparisons with the state-of-the-art on widely used
benchmarks and real data to show our advantages in terms of accuracy,
efficiency, completeness, and sharpness of reconstructed open surfaces with
boundaries. Project page: https://lisj575.github.io/GaussianUDF/



---

## SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with  Generative Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Yiqing Li, Xuan Wang, Jiawei Wu, Yikun Ma, Zhi Jin | cs.CV | [PDF](http://arxiv.org/pdf/2503.19452v1){: .btn .btn-green } |

**Abstract**: Synthesizing novel views of large-scale scenes from unconstrained in-the-wild
images is an important but challenging task in computer vision. Existing
methods, which optimize per-image appearance and transient occlusion through
implicit neural networks from dense training views (approximately 1000 images),
struggle to perform effectively under sparse input conditions, resulting in
noticeable artifacts. To this end, we propose SparseGS-W, a novel framework
based on 3D Gaussian Splatting that enables the reconstruction of complex
outdoor scenes and handles occlusions and appearance changes with as few as
five training images. We leverage geometric priors and constrained diffusion
priors to compensate for the lack of multi-view information from extremely
sparse input. Specifically, we propose a plug-and-play Constrained Novel-View
Enhancement module to iteratively improve the quality of rendered novel views
during the Gaussian optimization process. Furthermore, we propose an Occlusion
Handling module, which flexibly removes occlusions utilizing the inherent
high-quality inpainting capability of constrained diffusion priors. Both
modules are capable of extracting appearance features from any user-provided
reference image, enabling flexible modeling of illumination-consistent scenes.
Extensive experiments on the PhotoTourism and Tanks and Temples datasets
demonstrate that SparseGS-W achieves state-of-the-art performance not only in
full-reference metrics, but also in commonly used non-reference metrics such as
FID, ClipIQA, and MUSIQ.



---

## MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for  Neural Rendering across Multiple Imaging Modalities

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Federico Lincetto, Gianluca Agresti, Mattia Rossi, Pietro Zanuttigh | cs.GR | [PDF](http://arxiv.org/pdf/2503.19673v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive performances in the
rendering of 3D scenes from arbitrary viewpoints. While RGB images are widely
preferred for training volume rendering models, the interest in other radiance
modalities is also growing. However, the capability of the underlying implicit
neural models to learn and transfer information across heterogeneous imaging
modalities has seldom been explored, mostly due to the limited training data
availability. For this purpose, we present MultimodalStudio (MMS): it
encompasses MMS-DATA and MMS-FW. MMS-DATA is a multimodal multi-view dataset
containing 32 scenes acquired with 5 different imaging modalities: RGB,
monochrome, near-infrared, polarization and multispectral. MMS-FW is a novel
modular multimodal NeRF framework designed to handle multimodal raw data and
able to support an arbitrary number of multi-channel devices. Through extensive
experiments, we demonstrate that MMS-FW trained on MMS-DATA can transfer
information between different imaging modalities and produce higher quality
renderings than using single modalities alone. We publicly release the dataset
and the framework, to promote the research on multimodal volume rendering and
beyond.

Comments:
- Accepted at CVPR 2025

---

## High-Quality Spatial Reconstruction and Orthoimage Generation Using  Efficient 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Qian Wang, Zhihao Zhan, Jialei He, Zhituo Tu, Xiang Zhu, Jie Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2503.19703v1){: .btn .btn-green } |

**Abstract**: Highly accurate geometric precision and dense image features characterize
True Digital Orthophoto Maps (TDOMs), which are in great demand for
applications such as urban planning, infrastructure management, and
environmental monitoring. Traditional TDOM generation methods need
sophisticated processes, such as Digital Surface Models (DSM) and occlusion
detection, which are computationally expensive and prone to errors. This work
presents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free
of explicit DSM and occlusion detection. With depth map generation, spatial
information for every pixel within the TDOM is retrieved and can reconstruct
the scene with high precision. Divide-and-conquer strategy achieves excellent
GS training and rendering with high-resolution TDOMs at a lower resource cost,
which preserves higher quality of rendering on complex terrain and thin
structure without a decrease in efficiency. Experimental results demonstrate
the efficiency of large-scale scene reconstruction and high-precision terrain
modeling. This approach provides accurate spatial data, which assists users in
better planning and decision-making based on maps.



---

## Divide-and-Conquer: Dual-Hierarchical Optimization for Semantic 4D  Gaussian Spatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Zhiying Yan, Yiyuan Liang, Shilv Cai, Tao Zhang, Sheng Zhong, Luxin Yan, Xu Zou | cs.CV | [PDF](http://arxiv.org/pdf/2503.19332v1){: .btn .btn-green } |

**Abstract**: Semantic 4D Gaussians can be used for reconstructing and understanding
dynamic scenes, with temporal variations than static scenes. Directly applying
static methods to understand dynamic scenes will fail to capture the temporal
features. Few works focus on dynamic scene understanding based on Gaussian
Splatting, since once the same update strategy is employed for both dynamic and
static parts, regardless of the distinction and interaction between Gaussians,
significant artifacts and noise appear. We propose Dual-Hierarchical
Optimization (DHO), which consists of Hierarchical Gaussian Flow and
Hierarchical Gaussian Guidance in a divide-and-conquer manner. The former
implements effective division of static and dynamic rendering and features. The
latter helps to mitigate the issue of dynamic foreground rendering distortion
in textured complex scenes. Extensive experiments show that our method
consistently outperforms the baselines on both synthetic and real-world
datasets, and supports various downstream tasks. Project Page:
https://sweety-yan.github.io/DHO.

Comments:
- ICME 2025

---

## COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on  Boundary-Adaptive Gaussian Splitting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Jiaxin Zhang, Junjun Jiang, Youyu Chen, Kui Jiang, Xianming Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.19443v2){: .btn .btn-green } |

**Abstract**: Accurate object segmentation is crucial for high-quality scene understanding
in the 3D vision domain. However, 3D segmentation based on 3D Gaussian
Splatting (3DGS) struggles with accurately delineating object boundaries, as
Gaussian primitives often span across object edges due to their inherent volume
and the lack of semantic guidance during training. In order to tackle these
challenges, we introduce Clear Object Boundaries for 3DGS Segmentation
(COB-GS), which aims to improve segmentation accuracy by clearly delineating
blurry boundaries of interwoven Gaussian primitives within the scene. Unlike
existing approaches that remove ambiguous Gaussians and sacrifice visual
quality, COB-GS, as a 3DGS refinement method, jointly optimizes semantic and
visual information, allowing the two different levels to cooperate with each
other effectively. Specifically, for the semantic guidance, we introduce a
boundary-adaptive Gaussian splitting technique that leverages semantic gradient
statistics to identify and split ambiguous Gaussians, aligning them closely
with object boundaries. For the visual optimization, we rectify the degraded
suboptimal texture of the 3DGS scene, particularly along the refined boundary
structures. Experimental results show that COB-GS substantially improves
segmentation accuracy and robustness against inaccurate masks from pre-trained
model, yielding clear boundaries while preserving high visual quality. Code is
available at https://github.com/ZestfulJX/COB-GS.

Comments:
- Accepted by CVPR 2025

---

## HoGS: Unified Near and Far Object Reconstruction via Homogeneous  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Xinpeng Liu, Zeyi Huang, Fumio Okura, Yasuyuki Matsushita | cs.GR | [PDF](http://arxiv.org/pdf/2503.19232v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis has demonstrated impressive progress recently, with 3D
Gaussian splatting (3DGS) offering efficient training time and photorealistic
real-time rendering. However, reliance on Cartesian coordinates limits 3DGS's
performance on distant objects, which is important for reconstructing unbounded
outdoor environments. We found that, despite its ultimate simplicity, using
homogeneous coordinates, a concept on the projective geometry, for the 3DGS
pipeline remarkably improves the rendering accuracies of distant objects. We
therefore propose Homogeneous Gaussian Splatting (HoGS) incorporating
homogeneous coordinates into the 3DGS framework, providing a unified
representation for enhancing near and distant objects. HoGS effectively manages
both expansive spatial positions and scales particularly in outdoor unbounded
environments by adopting projective geometry principles. Experiments show that
HoGS significantly enhances accuracy in reconstructing distant objects while
maintaining high-quality rendering of nearby objects, along with fast training
speed and real-time rendering capability. Our implementations are available on
our project page https://kh129.github.io/hogs/.

Comments:
- Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR'25)

---

## From Sparse to Dense: Camera Relocalization with Scene-Specific Detector  from Feature Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Zhiwei Huang, Hailin Yu, Yichun Shentu, Jin Yuan, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.19358v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel camera relocalization method, STDLoc, which
leverages Feature Gaussian as scene representation. STDLoc is a full
relocalization pipeline that can achieve accurate relocalization without
relying on any pose prior. Unlike previous coarse-to-fine localization methods
that require image retrieval first and then feature matching, we propose a
novel sparse-to-dense localization paradigm. Based on this scene
representation, we introduce a novel matching-oriented Gaussian sampling
strategy and a scene-specific detector to achieve efficient and robust initial
pose estimation. Furthermore, based on the initial localization results, we
align the query feature map to the Gaussian feature field by dense feature
matching to enable accurate localization. The experiments on indoor and outdoor
datasets show that STDLoc outperforms current state-of-the-art localization
methods in terms of localization accuracy and recall.

Comments:
- 15 pages, 12 figures, CVPR 2025

---

## Learning Scene-Level Signed Directional Distance Function with  Ellipsoidal Priors and Neural Residuals

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Zhirui Dai, Hojoon Shin, Yulun Tian, Ki Myung Brian Lee, Nikolay Atanasov | cs.RO | [PDF](http://arxiv.org/pdf/2503.20066v1){: .btn .btn-green } |

**Abstract**: Dense geometric environment representations are critical for autonomous
mobile robot navigation and exploration. Recent work shows that implicit
continuous representations of occupancy, signed distance, or radiance learned
using neural networks offer advantages in reconstruction fidelity, efficiency,
and differentiability over explicit discrete representations based on meshes,
point clouds, and voxels. In this work, we explore a directional formulation of
signed distance, called signed directional distance function (SDDF). Unlike
signed distance function (SDF) and similar to neural radiance fields (NeRF),
SDDF has a position and viewing direction as input. Like SDF and unlike NeRF,
SDDF directly provides distance to the observed surface along the direction,
rather than integrating along the view ray, allowing efficient view synthesis.
To learn and predict scene-level SDDF efficiently, we develop a differentiable
hybrid representation that combines explicit ellipsoid priors and implicit
neural residuals. This approach allows the model to effectively handle large
distance discontinuities around obstacle boundaries while preserving the
ability for dense high-fidelity prediction. We show that SDDF is competitive
with the state-of-the-art neural implicit scene models in terms of
reconstruction accuracy and rendering efficiency, while allowing differentiable
view prediction for robot trajectory optimization.



---

## Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking  with Neural Deformation Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Navami Kairanda, Marc Habermann, Shanthika Naik, Christian Theobalt, Vladislav Golyanik | cs.GR | [PDF](http://arxiv.org/pdf/2503.19976v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction of highly deformable surfaces (e.g. cloths) from monocular
RGB videos is a challenging problem, and no solution provides a consistent and
accurate recovery of fine-grained surface details. To account for the ill-posed
nature of the setting, existing methods use deformation models with
statistical, neural, or physical priors. They also predominantly rely on
nonadaptive discrete surface representations (e.g. polygonal meshes), perform
frame-by-frame optimisation leading to error propagation, and suffer from poor
gradients of the mesh-based differentiable renderers. Consequently, fine
surface details such as cloth wrinkles are often not recovered with the desired
accuracy. In response to these limitations, we propose ThinShell-SfT, a new
method for non-rigid 3D tracking that represents a surface as an implicit and
continuous spatiotemporal neural field. We incorporate continuous thin shell
physics prior based on the Kirchhoff-Love model for spatial regularisation,
which starkly contrasts the discretised alternatives of earlier works. Lastly,
we leverage 3D Gaussian splatting to differentiably render the surface into
image space and optimise the deformations based on analysis-bysynthesis
principles. Our Thin-Shell-SfT outperforms prior works qualitatively and
quantitatively thanks to our continuous surface formulation in conjunction with
a specially tailored simulation prior and surface-induced 3D Gaussians. See our
project page at https://4dqv.mpiinf.mpg.de/ThinShellSfT.

Comments:
- 15 pages, 12 figures and 3 tables; project page:
  https://4dqv.mpiinf.mpg.de/ThinShellSfT; CVPR 2025

---

## A Survey on Event-driven 3D Reconstruction: Development under Different  Categories


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-25 | Chuanzhi Xu, Haoxian Zhou, Haodong Chen, Vera Chung, Qiang Qu | cs.GR | [PDF](http://arxiv.org/pdf/2503.19753v2){: .btn .btn-green } |

**Abstract**: Event cameras have gained increasing attention for 3D reconstruction due to
their high temporal resolution, low latency, and high dynamic range. They
capture per-pixel brightness changes asynchronously, allowing accurate
reconstruction under fast motion and challenging lighting conditions. In this
survey, we provide a comprehensive review of event-driven 3D reconstruction
methods, including stereo, monocular, and multimodal systems. We further
categorize recent developments based on geometric, learning-based, and hybrid
approaches. Emerging trends, such as neural radiance fields and 3D Gaussian
splatting with event data, are also covered. The related works are structured
chronologically to illustrate the innovations and progression within the field.
To support future research, we also highlight key research gaps and future
research directions in dataset, experiment, evaluation, event representation,
etc.

Comments:
- 6 pages, 1 figure, 6 tables, submitted to an anonymous conference
  under double-blind review

---

## NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Wenyuan Zhang, Emily Yue-ting Jia, Junsheng Zhou, Baorui Ma, Kanle Shi, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.18361v1){: .btn .btn-green } |

**Abstract**: Recently, it has shown that priors are vital for neural implicit functions to
reconstruct high-quality surfaces from multi-view RGB images. However, current
priors require large-scale pre-training, and merely provide geometric clues
without considering the importance of color. In this paper, we present
NeRFPrior, which adopts a neural radiance field as a prior to learn signed
distance fields using volume rendering for surface reconstruction. Our NeRF
prior can provide both geometric and color clues, and also get trained fast
under the same scene without additional data. Based on the NeRF prior, we are
enabled to learn a signed distance function (SDF) by explicitly imposing a
multi-view consistency constraint on each ray intersection for surface
inference. Specifically, at each ray intersection, we use the density in the
prior as a coarse geometry estimation, while using the color near the surface
as a clue to check its visibility from another view angle. For the textureless
areas where the multi-view consistency constraint does not work well, we
further introduce a depth consistency loss with confidence weights to infer the
SDF. Our experimental results outperform the state-of-the-art methods under the
widely used benchmarks.

Comments:
- Accepted by CVPR 2025. Project page:
  https://wen-yuan-zhang.github.io/NeRFPrior/

---

## LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Xiaoyu Zhang, Weihong Pan, Chong Bao, Xiyu Zhang, Xiaojun Xiang, Hanqing Jiang, Hujun Bao | cs.CV | [PDF](http://arxiv.org/pdf/2503.18513v2){: .btn .btn-green } |

**Abstract**: Humans perceive and comprehend their surroundings through information
spanning multiple frequencies. In immersive scenes, people naturally scan their
environment to grasp its overall structure while examining fine details of
objects that capture their attention. However, current NeRF frameworks
primarily focus on modeling either high-frequency local views or the broad
structure of scenes with low-frequency information, which is limited to
balancing both. We introduce FA-NeRF, a novel frequency-aware framework for
view synthesis that simultaneously captures the overall scene structure and
high-definition details within a single NeRF model. To achieve this, we propose
a 3D frequency quantification method that analyzes the scene's frequency
distribution, enabling frequency-aware rendering. Our framework incorporates a
frequency grid for fast convergence and querying, a frequency-aware feature
re-weighting strategy to balance features across different frequency contents.
Extensive experiments show that our method significantly outperforms existing
approaches in modeling entire scenes while preserving fine details. Project
page: https://coscatter.github.io/LookCloser/

Comments:
- CVPR 2025. Project page: https://coscatter.github.io/LookCloser

---

## GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Lijiang Li, Jinglu Wang, Xiang Ming, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.18718v1){: .btn .btn-green } |

**Abstract**: In the Generative AI era, safeguarding 3D models has become increasingly
urgent. While invisible watermarking is well-established for 2D images with
encoder-decoder frameworks, generalizable and robust solutions for 3D remain
elusive. The main difficulty arises from the renderer between the 3D encoder
and 2D decoder, which disrupts direct gradient flow and complicates training.
Existing 3D methods typically rely on per-scene iterative optimization,
resulting in time inefficiency and limited generalization. In this work, we
propose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a
well-known yet underexplored representation for watermarking. We identify two
major challenges: (1) ensuring effective training generalized across diverse 3D
models, and (2) reliably extracting watermarks from free-view renderings, even
under distortions. Our framework, named GS-Marker, incorporates a 3D encoder to
embed messages, distortion layers to enhance resilience against various
distortions, and a 2D decoder to extract watermarks from renderings. A key
innovation is the Adaptive Marker Control mechanism that adaptively perturbs
the initially optimized 3DGS, escaping local minima and improving both training
stability and convergence. Extensive experiments show that GS-Marker
outperforms per-scene training approaches in terms of decoding accuracy and
model fidelity, while also significantly reducing computation time.



---

## StableGS: A Floater-Free Framework for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Luchao Wang, Qian Ren, Kaimin Liao, Hua Wang, Zhi Chen, Yaohua Tang | cs.CV | [PDF](http://arxiv.org/pdf/2503.18458v2){: .btn .btn-green } |

**Abstract**: Recent years have witnessed remarkable success of 3D Gaussian Splatting
(3DGS) in novel view synthesis, surpassing prior differentiable rendering
methods in both quality and efficiency. However, its training process suffers
from coupled opacity-color optimization that frequently converges to local
minima, producing floater artifacts that degrade visual fidelity. We present
StableGS, a framework that eliminates floaters through cross-view depth
consistency constraints while introducing a dual-opacity GS model to decouple
geometry and material properties of translucent objects. To further enhance
reconstruction quality in weakly-textured regions, we integrate DUSt3R depth
estimation, significantly improving geometric stability. Our method
fundamentally addresses 3DGS training instabilities, outperforming existing
state-of-the-art methods across open-source datasets.



---

## NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Yulong Zheng, Zicheng Jiang, Shengfeng He, Yandu Sun, Junyu Dong, Huaidong Zhang, Yong Du | cs.CV | [PDF](http://arxiv.org/pdf/2503.18794v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably
advanced photo-realistic novel view synthesis using images from densely spaced
camera viewpoints. However, these methods struggle in few-shot scenarios due to
limited supervision. In this paper, we present NexusGS, a 3DGS-based approach
that enhances novel view synthesis from sparse-view images by directly
embedding depth information into point clouds, without relying on complex
manual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our
method introduces a novel point cloud densification strategy that initializes
3DGS with a dense point cloud, reducing randomness in point placement while
preventing over-smoothing and overfitting. Specifically, NexusGS comprises
three key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and
Flow-Filtered Depth Pruning. These steps leverage optical flow and camera poses
to compute accurate depth maps, while mitigating the inaccuracies often
associated with optical flow. By incorporating epipolar depth priors, NexusGS
ensures reliable dense point cloud coverage and supports stable 3DGS training
under sparse-view conditions. Experiments demonstrate that NexusGS
significantly enhances depth accuracy and rendering quality, surpassing
state-of-the-art methods by a considerable margin. Furthermore, we validate the
superiority of our generated point clouds by substantially boosting the
performance of competing methods. Project page:
https://usmizuki.github.io/NexusGS/.

Comments:
- This paper is accepted by CVPR 2025

---

## Hardware-Rasterized Ray-Based Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Samuel Rota Bulò, Nemanja Bartolovic, Lorenzo Porzi, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2503.18682v1){: .btn .btn-green } |

**Abstract**: We present a novel, hardware rasterized rendering approach for ray-based 3D
Gaussian Splatting (RayGS), obtaining both fast and high-quality results for
novel view synthesis. Our work contains a mathematically rigorous and
geometrically intuitive derivation about how to efficiently estimate all
relevant quantities for rendering RayGS models, structured with respect to
standard hardware rasterization shaders. Our solution is the first enabling
rendering RayGS models at sufficiently high frame rates to support
quality-sensitive applications like Virtual and Mixed Reality. Our second
contribution enables alias-free rendering for RayGS, by addressing MIP-related
issues arising when rendering diverging scales during training and testing. We
demonstrate significant performance gains, across different benchmark scenes,
while retaining state-of-the-art appearance quality of RayGS.



---

## DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Youyu Chen, Junjun Jiang, Kui Jiang, Xiao Tang, Zhihao Li, Xianming Liu, Yinyu Nie | cs.CV | [PDF](http://arxiv.org/pdf/2503.18402v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian
primitives, where the rendering resolution and the primitive number, concluded
as the optimization complexity, dominate the time cost in primitive
optimization. In this paper, we propose DashGaussian, a scheduling scheme over
the optimization complexity of 3DGS that strips redundant complexity to
accelerate 3DGS optimization. Specifically, we formulate 3DGS optimization as
progressively fitting 3DGS to higher levels of frequency components in the
training views, and propose a dynamic rendering resolution scheme that largely
reduces the optimization complexity based on this formulation. Besides, we
argue that a specific rendering resolution should cooperate with a proper
primitive number for a better balance between computing redundancy and fitting
quality, where we schedule the growth of the primitives to synchronize with the
rendering resolution. Extensive experiments show that our method accelerates
the optimization of various 3DGS backbones by 45.7% on average while preserving
the rendering quality.

Comments:
- Accepted by CVPR2025. Project page: https://dashgaussian.github.io

---

## LLGS: Unsupervised Gaussian Splatting for Image Enhancement and  Reconstruction in Pure Dark Environment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Haoran Wang, Jingwei Huang, Lu Yang, Tianchen Deng, Gaojing Zhang, Mingrui Li | cs.CV | [PDF](http://arxiv.org/pdf/2503.18640v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has shown remarkable capabilities in novel view
rendering tasks and exhibits significant potential for multi-view
optimization.However, the original 3D Gaussian Splatting lacks color
representation for inputs in low-light environments. Simply using enhanced
images as inputs would lead to issues with multi-view consistency, and current
single-view enhancement systems rely on pre-trained data, lacking scene
generalization. These problems limit the application of 3D Gaussian Splatting
in low-light conditions in the field of robotics, including high-fidelity
modeling and feature matching. To address these challenges, we propose an
unsupervised multi-view stereoscopic system based on Gaussian Splatting, called
Low-Light Gaussian Splatting (LLGS). This system aims to enhance images in
low-light environments while reconstructing the scene. Our method introduces a
decomposable Gaussian representation called M-Color, which separately
characterizes color information for targeted enhancement. Furthermore, we
propose an unsupervised optimization method with zero-knowledge priors, using
direction-based enhancement to ensure multi-view consistency. Experiments
conducted on real-world datasets demonstrate that our system outperforms
state-of-the-art methods in both low-light enhancement and 3D Gaussian
Splatting.



---

## GI-SLAM: Gaussian-Inertial SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Xulang Liu, Ning Tan | cs.RO | [PDF](http://arxiv.org/pdf/2503.18275v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful
representation of geometry and appearance for dense Simultaneous Localization
and Mapping (SLAM). Through rapid, differentiable rasterization of 3D
Gaussians, many 3DGS SLAM methods achieve near real-time rendering and
accelerated training. However, these methods largely overlook inertial data,
witch is a critical piece of information collected from the inertial
measurement unit (IMU). In this paper, we present GI-SLAM, a novel
gaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking
module and a realistic 3D Gaussian-based scene representation for mapping. Our
method introduces an IMU loss that seamlessly integrates into the deep learning
framework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the
accuracy, robustness and efficiency of camera tracking. Moreover, our SLAM
system supports a wide range of sensor configurations, including monocular,
stereo, and RGBD cameras, both with and without IMU integration. Our method
achieves competitive performance compared with existing state-of-the-art
real-time methods on the EuRoC and TUM-RGBD datasets.

Comments:
- 10 pages, 2 figures, 5 tables

---

## 4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable  Free-Viewpoint Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-24 | Qiang Hu, Zihan Zheng, Houqiang Zhong, Sihua Fu, Li Song,  XiaoyunZhang, Guangtao Zhai, Yanfeng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.18421v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has substantial potential for enabling
photorealistic Free-Viewpoint Video (FVV) experiences. However, the vast number
of Gaussians and their associated attributes poses significant challenges for
storage and transmission. Existing methods typically handle dynamic 3DGS
representation and compression separately, neglecting motion information and
the rate-distortion (RD) trade-off during training, leading to performance
degradation and increased model redundancy. To address this gap, we propose
4DGC, a novel rate-aware 4D Gaussian compression framework that significantly
reduces storage size while maintaining superior RD performance for FVV.
Specifically, 4DGC introduces a motion-aware dynamic Gaussian representation
that utilizes a compact motion grid combined with sparse compensated Gaussians
to exploit inter-frame similarities. This representation effectively handles
large motions, preserving quality and reducing temporal redundancy.
Furthermore, we present an end-to-end compression scheme that employs
differentiable quantization and a tiny implicit entropy model to compress the
motion grid and compensated Gaussians efficiently. The entire framework is
jointly optimized using a rate-distortion trade-off. Extensive experiments
demonstrate that 4DGC supports variable bitrates and consistently outperforms
existing methods in RD performance across multiple datasets.

Comments:
- CVPR2025

---

## PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable  Objects from Videos


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-23 | Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, Yunzhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2503.17973v1){: .btn .btn-green } |

**Abstract**: Creating a physical digital twin of a real-world object has immense potential
in robotics, content creation, and XR. In this paper, we present PhysTwin, a
novel framework that uses sparse videos of dynamic objects under interaction to
produce a photo- and physically realistic, real-time interactive virtual
replica. Our approach centers on two key components: (1) a physics-informed
representation that combines spring-mass models for realistic physical
simulation, generative shape models for geometry, and Gaussian splats for
rendering; and (2) a novel multi-stage, optimization-based inverse modeling
framework that reconstructs complete geometry, infers dense physical
properties, and replicates realistic appearance from videos. Our method
integrates an inverse physics framework with visual perception cues, enabling
high-fidelity reconstruction even from partial, occluded, and limited
viewpoints. PhysTwin supports modeling various deformable objects, including
ropes, stuffed animals, cloth, and delivery packages. Experiments show that
PhysTwin outperforms competing methods in reconstruction, rendering, future
prediction, and simulation under novel interactions. We further demonstrate its
applications in interactive real-time simulation and model-based robotic motion
planning.

Comments:
- Project Page: https://jianghanxiao.github.io/phystwin-web/

---

## PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary  Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-23 | Hongjia Zhai, Hai Li, Zhenzhe Li, Xiaokun Pan, Yijia He, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.18107v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has shown encouraging performance for
open vocabulary scene understanding tasks. However, previous methods cannot
distinguish 3D instance-level information, which usually predicts a heatmap
between the scene feature and text query. In this paper, we propose PanoGS, a
novel and effective 3D panoptic open vocabulary scene understanding approach.
Technically, to learn accurate 3D language features that can scale to large
indoor scenarios, we adopt the pyramid tri-plane to model the latent continuous
parametric feature space and use a 3D feature decoder to regress the multi-view
fused 2D feature cloud. Besides, we propose language-guided graph cuts that
synergistically leverage reconstructed geometry and learned language cues to
group 3D Gaussian primitives into a set of super-primitives. To obtain 3D
consistent instance, we perform graph clustering based segmentation with
SAM-guided edge affinity computation between different super-primitives.
Extensive experiments on widely used datasets show better or more competitive
performance on 3D panoptic open vocabulary scene understanding. Project page:
\href{https://zju3dv.github.io/panogs}{https://zju3dv.github.io/panogs}.

Comments:
- CVPR 2025

---

## SceneSplat: Gaussian Splatting-based Scene Understanding with  Vision-Language Pretraining

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-23 | Yue Li, Qi Ma, Runyi Yang, Huapeng Li, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, Martin R. Oswald, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2503.18052v1){: .btn .btn-green } |

**Abstract**: Recognizing arbitrary or previously unseen categories is essential for
comprehensive real-world 3D scene understanding. Currently, all existing
methods rely on 2D or textual modalities during training, or together at
inference. This highlights a clear absence of a model capable of processing 3D
data alone for learning semantics end-to-end, along with the necessary data to
train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the
de facto standard for 3D scene representation across various vision tasks.
However, effectively integrating semantic reasoning into 3DGS in a
generalizable fashion remains an open challenge. To address these limitations
we introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene
understanding approach that operates natively on 3DGS. Furthermore, we propose
a self-supervised learning scheme that unlocks rich 3D feature learning from
unlabeled scenes. In order to power the proposed methods, we introduce
SceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising
of 6868 scenes derived from 7 established datasets like ScanNet, Matterport3D,
etc. Generating SceneSplat-7K required computational resources equivalent to
119 GPU-days on an L4 GPU, enabling standardized benchmarking for 3DGS-based
reasoning for indoor scenes. Our exhaustive experiments on SceneSplat-7K
demonstrate the significant benefit of the proposed methods over the
established baselines.

Comments:
- Our code, model, and dataset will be released at
  https://github.com/unique1i/SceneSplat

---

## Unraveling the Effects of Synthetic Data on End-to-End Autonomous  Driving

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-23 | Junhao Ge, Zuhong Liu, Longteng Fan, Yifan Jiang, Jiaqi Su, Yiming Li, Zhejun Zhang, Siheng Chen | cs.RO | [PDF](http://arxiv.org/pdf/2503.18108v1){: .btn .btn-green } |

**Abstract**: End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality
data to perform well across various driving scenarios. However, collecting
large-scale real-world data is expensive and time-consuming, making
high-fidelity synthetic data essential for enhancing data diversity and model
robustness. Existing driving simulators for synthetic data generation have
significant limitations: game-engine-based simulators struggle to produce
realistic sensor data, while NeRF-based and diffusion-based methods face
efficiency challenges. Additionally, recent simulators designed for closed-loop
evaluation provide limited interaction with other vehicles, failing to simulate
complex real-world traffic dynamics. To address these issues, we introduce
SceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D
Gaussian Splatting (3DGS). SceneCrafter not only efficiently generates
realistic driving logs across diverse traffic scenarios but also enables robust
closed-loop evaluation of end-to-end models. Experimental results demonstrate
that SceneCrafter serves as both a reliable evaluation platform and a efficient
data generator that significantly improves end-to-end model generalization.



---

## PanopticSplatting: End-to-End Panoptic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-23 | Yuxuan Xie, Xuan Yu, Changjian Jiang, Sitong Mao, Shunbo Zhou, Rui Fan, Rong Xiong, Yue Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.18073v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary panoptic reconstruction is a challenging task for
simultaneous scene reconstruction and understanding. Recently, methods have
been proposed for 3D scene understanding based on Gaussian splatting. However,
these methods are multi-staged, suffering from the accumulated errors and the
dependence of hand-designed components. To streamline the pipeline and achieve
global optimization, we propose PanopticSplatting, an end-to-end system for
open-vocabulary panoptic reconstruction. Our method introduces query-guided
Gaussian segmentation with local cross attention, lifting 2D instance masks
without cross-frame association in an end-to-end way. The local cross attention
within view frustum effectively reduces the training memory, making our model
more accessible to large scenes with more Gaussians and objects. In addition,
to address the challenge of noisy labels in 2D pseudo masks, we propose label
blending to promote consistent 3D segmentation with less noisy floaters, as
well as label warping on 2D predictions which enhances multi-view coherence and
segmentation accuracy. Our method demonstrates strong performances in 3D scene
panoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with
both NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,
PanopticSplatting can be easily generalized to numerous variants of Gaussian
splatting, and we demonstrate its robustness on different Gaussian base models.

Comments:
- 8 pages, 6 figures

---

## End-to-End Implicit Neural Representations for Classification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-23 | Alexander Gielisse, Jan van Gemert | cs.CV | [PDF](http://arxiv.org/pdf/2503.18123v1){: .btn .btn-green } |

**Abstract**: Implicit neural representations (INRs) such as NeRF and SIREN encode a signal
in neural network parameters and show excellent results for signal
reconstruction. Using INRs for downstream tasks, such as classification, is
however not straightforward. Inherent symmetries in the parameters pose
challenges and current works primarily focus on designing architectures that
are equivariant to these symmetries. However, INR-based classification still
significantly under-performs compared to pixel-based methods like CNNs. This
work presents an end-to-end strategy for initializing SIRENs together with a
learned learning-rate scheme, to yield representations that improve
classification accuracy. We show that a simple, straightforward, Transformer
model applied to a meta-learned SIREN, without incorporating explicit symmetry
equivariances, outperforms the current state-of-the-art. On the CIFAR-10 SIREN
classification task, we improve the state-of-the-art without augmentations from
38.8% to 59.6%, and from 63.4% to 64.7% with augmentations. We demonstrate
scalability on the high-resolution Imagenette dataset achieving reasonable
reconstruction quality with a classification accuracy of 60.8% and are the
first to do INR classification on the full ImageNet-1K dataset where we achieve
a SIREN classification performance of 23.6%. To the best of our knowledge, no
other SIREN classification approach has managed to set a classification
baseline for any high-resolution image dataset. Our code is available at
https://github.com/SanderGielisse/MWT

Comments:
- Accepted to CVPR 2025. 8 pages, supplementary material included

---

## GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-22 | Zexu Huang, Min Xu, Stuart Perry | cs.CV | [PDF](http://arxiv.org/pdf/2503.17798v1){: .btn .btn-green } |

**Abstract**: Recent developments in 3D reconstruction and neural rendering have
significantly propelled the capabilities of photo-realistic 3D scene rendering
across various academic and industrial fields. The 3D Gaussian Splatting
technique, alongside its derivatives, integrates the advantages of
primitive-based and volumetric representations to deliver top-tier rendering
quality and efficiency. Despite these advancements, the method tends to
generate excessive redundant noisy Gaussians overfitted to every training view,
which degrades the rendering quality. Additionally, while 3D Gaussian Splatting
excels in small-scale and object-centric scenes, its application to larger
scenes is hindered by constraints such as limited video memory, excessive
optimization duration, and variable appearance across views. To address these
challenges, we introduce GaussianFocus, an innovative approach that
incorporates a patch attention algorithm to refine rendering quality and
implements a Gaussian constraints strategy to minimize redundancy. Moreover, we
propose a subdivision reconstruction strategy for large-scale scenes, dividing
them into smaller, manageable blocks for individual training. Our results
indicate that GaussianFocus significantly reduces unnecessary Gaussians and
enhances rendering quality, surpassing existing State-of-The-Art (SoTA)
methods. Furthermore, we demonstrate the capability of our approach to
effectively manage and render large scenes, such as urban environments, whilst
maintaining high fidelity in the visual output.



---

## GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling for Long-Term  Service Robots

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-22 | Bin Fu, Jialin Li, Bin Zhang, Ruiping Wang, Xilin Chen | cs.RO | [PDF](http://arxiv.org/pdf/2503.17733v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has garnered significant attention in robotics
for its explicit, high fidelity dense scene representation, demonstrating
strong potential for robotic applications. However, 3DGS-based methods in
robotics primarily focus on static scenes, with limited attention to the
dynamic scene changes essential for long-term service robots. These robots
demand sustained task execution and efficient scene updates-challenges current
approaches fail to meet. To address these limitations, we propose GS-LTS
(Gaussian Splatting for Long-Term Service), a 3DGS-based system enabling indoor
robots to manage diverse tasks in dynamic environments over time. GS-LTS
detects scene changes (e.g., object addition or removal) via single-image
change detection, employs a rule-based policy to autonomously collect
multi-view observations, and efficiently updates the scene representation
through Gaussian editing. Additionally, we propose a simulation-based benchmark
that automatically generates scene change data as compact configuration
scripts, providing a standardized, user-friendly evaluation benchmark.
Experimental results demonstrate GS-LTS's advantages in reconstruction,
navigation, and superior scene updates-faster and higher quality than the image
training baseline-advancing 3DGS for long-term robotic operations. Code and
benchmark are available at: https://vipl-vsu.github.io/3DGS-LTS.



---

## Is there anything left? Measuring semantic residuals of objects removed  from 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-21 | Simona Kocour, Assia Benbihi, Aikaterini Adam, Torsten Sattler | cs.CV | [PDF](http://arxiv.org/pdf/2503.17574v1){: .btn .btn-green } |

**Abstract**: Searching in and editing 3D scenes has become extremely intuitive with
trainable scene representations that allow linking human concepts to elements
in the scene. These operations are often evaluated on the basis of how
accurately the searched element is segmented or extracted from the scene. In
this paper, we address the inverse problem, that is, how much of the searched
element remains in the scene after it is removed. This question is particularly
important in the context of privacy-preserving mapping when a user reconstructs
a 3D scene and wants to remove private elements before sharing the map. To the
best of our knowledge, this is the first work to address this question. To
answer this, we propose a quantitative evaluation that measures whether a
removal operation leaves object residuals that can be reasoned over. The scene
is not private when such residuals are present. Experiments on state-of-the-art
scene representations show that the proposed metrics are meaningful and
consistent with the user study that we also present. We also propose a method
to refine the removal based on spatial and semantic consistency.



---

## ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian  Prototypes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-21 | Zhengqing Gao, Dongting Hu, Jia-Wang Bian, Huan Fu, Yan Li, Tongliang Liu, Mingming Gong, Kun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.17486v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has made significant strides in novel view
synthesis but is limited by the substantial number of Gaussian primitives
required, posing challenges for deployment on lightweight devices. Recent
methods address this issue by compressing the storage size of densified
Gaussians, yet fail to preserve rendering quality and efficiency. To overcome
these limitations, we propose ProtoGS to learn Gaussian prototypes to represent
Gaussian primitives, significantly reducing the total Gaussian amount without
sacrificing visual quality. Our method directly uses Gaussian prototypes to
enable efficient rendering and leverage the resulting reconstruction loss to
guide prototype learning. To further optimize memory efficiency during
training, we incorporate structure-from-motion (SfM) points as anchor points to
group Gaussian primitives. Gaussian prototypes are derived within each group by
clustering of K-means, and both the anchor points and the prototypes are
optimized jointly. Our experiments on real-world and synthetic datasets prove
that we outperform existing methods, achieving a substantial reduction in the
number of Gaussians, and enabling high rendering speed while maintaining or
even enhancing rendering fidelity.



---

## Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-21 | Emanuele Giacomini, Luca Di Giammarino, Lorenzo De Rebotti, Giorgio Grisetti, Martin R. Oswald | cs.RO | [PDF](http://arxiv.org/pdf/2503.17491v1){: .btn .btn-green } |

**Abstract**: LiDARs provide accurate geometric measurements, making them valuable for
ego-motion estimation and reconstruction tasks. Although its success, managing
an accurate and lightweight representation of the environment still poses
challenges. Both classic and NeRF-based solutions have to trade off accuracy
over memory and processing times. In this work, we build on recent advancements
in Gaussian Splatting methods to develop a novel LiDAR odometry and mapping
pipeline that exclusively relies on Gaussian primitives for its scene
representation. Leveraging spherical projection, we drive the refinement of the
primitives uniquely from LiDAR measurements. Experiments show that our approach
matches the current registration performance, while achieving SOTA results for
mapping tasks with minimal GPU requirements. This efficiency makes it a strong
candidate for further exploration and potential adoption in real-time robotics
estimation tasks.

Comments:
- submitted to ICCV 2025

---

## FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-21 | Kwan Yun, Chaelin Kim, Hangyeul Shin, Junyong Noh | cs.GR | [PDF](http://arxiv.org/pdf/2503.17095v1){: .btn .btn-green } |

**Abstract**: Recent 3D face editing methods using masks have produced high-quality edited
images by leveraging Neural Radiance Fields (NeRF). Despite their impressive
performance, existing methods often provide limited user control due to the use
of pre-trained segmentation masks. To utilize masks with a desired layout, an
extensive training dataset is required, which is challenging to gather. We
present FFaceNeRF, a NeRF-based face editing technique that can overcome the
challenge of limited user control due to the use of fixed mask layouts. Our
method employs a geometry adapter with feature injection, allowing for
effective manipulation of geometry attributes. Additionally, we adopt latent
mixing for tri-plane augmentation, which enables training with a few samples.
This facilitates rapid model adaptation to desired mask layouts, crucial for
applications in fields like personalized medical imaging or creative face
editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses
existing mask based face editing methods in terms of flexibility, control, and
generated image quality, paving the way for future advancements in customized
and high-fidelity 3D face editing. The code is available on the
{\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.

Comments:
- CVPR2025, 11 pages, 14 figures

---

## TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented  Reality via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-21 | Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv | cs.CV | [PDF](http://arxiv.org/pdf/2503.17032v1){: .btn .btn-green } |

**Abstract**: Realistic 3D full-body talking avatars hold great potential in AR, with
applications ranging from e-commerce live streaming to holographic
communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike
avatar creation, existing methods struggle with fine-grained control of facial
expressions and body movements in full-body talking tasks. Additionally, they
often lack sufficient details and cannot run in real-time on mobile devices. We
present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking
avatar driven by various signals. Our approach starts by creating a
personalized clothed human parametric template that binds Gaussians to
represent appearances. We then pre-train a StyleUnet-based network to handle
complex pose-dependent non-rigid deformation, which can capture high-frequency
appearance details but is too resource-intensive for mobile devices. To
overcome this, we "bake" the non-rigid deformations into a lightweight
MLP-based network using a distillation technique and develop blend shapes to
compensate for details. Extensive experiments show that TaoAvatar achieves
state-of-the-art rendering quality while running in real-time across various
devices, maintaining 90 FPS on high-definition stereo devices such as the Apple
Vision Pro.

Comments:
- Accepted by CVPR 2025, project page:
  https://PixelAI-Team.github.io/TaoAvatar

---

## Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic  Scene Reconstruction via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-21 | Jinbo Yan, Rui Peng, Zhiyan Wang, Luyang Tang, Jiayu Yang, Jie Liang, Jiahao Wu, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.16979v1){: .btn .btn-green } |

**Abstract**: Building Free-Viewpoint Videos in a streaming manner offers the advantage of
rapid responsiveness compared to offline training methods, greatly enhancing
user experience. However, current streaming approaches face challenges of high
per-frame reconstruction time (10s+) and error accumulation, limiting their
broader application. In this paper, we propose Instant Gaussian Stream (IGS), a
fast and generalizable streaming framework, to address these issues. First, we
introduce a generalized Anchor-driven Gaussian Motion Network, which projects
multi-view 2D motion features into 3D space, using anchor points to drive the
motion of all Gaussians. This generalized Network generates the motion of
Gaussians for each target frame in the time required for a single inference.
Second, we propose a Key-frame-guided Streaming Strategy that refines each key
frame, enabling accurate reconstruction of temporally complex scenes while
mitigating error accumulation. We conducted extensive in-domain and
cross-domain evaluations, demonstrating that our approach can achieve streaming
with a average per-frame reconstruction time of 2s+, alongside a enhancement in
view synthesis quality.



---

## DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from  In-the-Wild Drone Imagery

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-21 | Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2503.16964v1){: .btn .btn-green } |

**Abstract**: Drones have become essential tools for reconstructing wild scenes due to
their outstanding maneuverability. Recent advances in radiance field methods
have achieved remarkable rendering quality, providing a new avenue for 3D
reconstruction from drone imagery. However, dynamic distractors in wild
environments challenge the static scene assumption in radiance fields, while
limited view constraints hinder the accurate capture of underlying scene
geometry. To address these challenges, we introduce DroneSplat, a novel
framework designed for robust 3D reconstruction from in-the-wild drone imagery.
Our method adaptively adjusts masking thresholds by integrating local-global
segmentation heuristics with statistical approaches, enabling precise
identification and elimination of dynamic distractors in static scenes. We
enhance 3D Gaussian Splatting with multi-view stereo predictions and a
voxel-guided optimization strategy, supporting high-quality rendering under
limited view constraints. For comprehensive evaluation, we provide a
drone-captured 3D reconstruction dataset encompassing both dynamic and static
scenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS
and NeRF baselines in handling in-the-wild drone imagery.



---

## Optimized Minimal 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-21 | Joo Chan Lee, Jong Hwan Ko, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2503.16924v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for
real-time, high-performance rendering, enabling a wide range of applications.
However, representing 3D scenes with numerous explicit Gaussian primitives
imposes significant storage and memory overhead. Recent studies have shown that
high-quality rendering can be achieved with a substantially reduced number of
Gaussians when represented with high-precision attributes. Nevertheless,
existing 3DGS compression methods still rely on a relatively large number of
Gaussians, focusing primarily on attribute compression. This is because a
smaller set of Gaussians becomes increasingly sensitive to lossy attribute
compression, leading to severe quality degradation. Since the number of
Gaussians is directly tied to computational costs, it is essential to reduce
the number of Gaussians effectively rather than only optimizing storage. In
this paper, we propose Optimized Minimal Gaussians representation (OMG), which
significantly reduces storage while using a minimal number of primitives.
First, we determine the distinct Gaussian from the near ones, minimizing
redundancy without sacrificing quality. Second, we propose a compact and
precise attribute representation that efficiently captures both continuity and
irregularity among primitives. Additionally, we propose a sub-vector
quantization technique for improved irregularity representation, maintaining
fast training with a negligible codebook size. Extensive experiments
demonstrate that OMG reduces storage requirements by nearly 50% compared to the
previous state-of-the-art and enables 600+ FPS rendering while maintaining high
rendering quality. Our source code is available at
https://maincold2.github.io/omg/.

Comments:
- Project page: https://maincold2.github.io/omg/

---

## VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting  Generation with Flexible Pose and Multi-View Joint Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Hyojun Go, Byeongjun Park, Hyelin Nam, Byung-Hoon Kim, Hyungjin Chung, Changick Kim | cs.CV | [PDF](http://arxiv.org/pdf/2503.15855v1){: .btn .btn-green } |

**Abstract**: We propose VideoRFSplat, a direct text-to-3D model leveraging a video
generation model to generate realistic 3D Gaussian Splatting (3DGS) for
unbounded real-world scenes. To generate diverse camera poses and unbounded
spatial extent of real-world scenes, while ensuring generalization to arbitrary
text prompts, previous methods fine-tune 2D generative models to jointly model
camera poses and multi-view images. However, these methods suffer from
instability when extending 2D generative models to joint modeling due to the
modality gap, which necessitates additional models to stabilize training and
inference. In this work, we propose an architecture and a sampling strategy to
jointly model multi-view images and camera poses when fine-tuning a video
generation model. Our core idea is a dual-stream architecture that attaches a
dedicated pose generation model alongside a pre-trained video generation model
via communication blocks, generating multi-view images and camera poses through
separate streams. This design reduces interference between the pose and image
modalities. Additionally, we propose an asynchronous sampling strategy that
denoises camera poses faster than multi-view images, allowing rapidly denoised
poses to condition multi-view generation, reducing mutual ambiguity and
enhancing cross-modal consistency. Trained on multiple large-scale real-world
datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms
existing text-to-3D direct generation methods that heavily depend on post-hoc
refinement via score distillation sampling, achieving superior results without
such refinement.

Comments:
- Project page: https://gohyojun15.github.io/VideoRFSplat/

---

## 1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Yuheng Yuan, Qiuhong Shen, Xingyi Yang, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.16422v1){: .btn .btn-green } |

**Abstract**: 4D Gaussian Splatting (4DGS) has recently gained considerable attention as a
method for reconstructing dynamic scenes. Despite achieving superior quality,
4DGS typically requires substantial storage and suffers from slow rendering
speed. In this work, we delve into these issues and identify two key sources of
temporal redundancy. (Q1) \textbf{Short-Lifespan Gaussians}: 4DGS uses a large
portion of Gaussians with short temporal span to represent scene dynamics,
leading to an excessive number of Gaussians. (Q2) \textbf{Inactive Gaussians}:
When rendering, only a small subset of Gaussians contributes to each frame.
Despite this, all Gaussians are processed during rasterization, resulting in
redundant computation overhead. To address these redundancies, we present
\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we
introduce the Spatial-Temporal Variation Score, a new pruning criterion that
effectively removes short-lifespan Gaussians while encouraging 4DGS to capture
scene dynamics using Gaussians with longer temporal spans. For Q2, we store a
mask for active Gaussians across consecutive frames, significantly reducing
redundant computations in rendering. Compared to vanilla 4DGS, our method
achieves a $41\times$ reduction in storage and $9\times$ faster rasterization
speed on complex dynamic scenes, while maintaining comparable visual quality.
Please see our project page at https://4DGS-1K.github.io.



---

## 4D Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Yanyan Li, Youxu Fang, Zunjie Zhu, Kunyi Li, Yong Ding, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2503.16710v1){: .btn .btn-green } |

**Abstract**: Simultaneously localizing camera poses and constructing Gaussian radiance
fields in dynamic scenes establish a crucial bridge between 2D images and the
4D real world. Instead of removing dynamic objects as distractors and
reconstructing only static environments, this paper proposes an efficient
architecture that incrementally tracks camera poses and establishes the 4D
Gaussian radiance fields in unknown scenarios by using a sequence of RGB-D
images. First, by generating motion masks, we obtain static and dynamic priors
for each pixel. To eliminate the influence of static scenes and improve the
efficiency on learning the motion of dynamic objects, we classify the Gaussian
primitives into static and dynamic Gaussian sets, while the sparse control
points along with an MLP is utilized to model the transformation fields of the
dynamic Gaussians. To more accurately learn the motion of dynamic Gaussians, a
novel 2D optical flow map reconstruction algorithm is designed to render
optical flows of dynamic objects between neighbor images, which are further
used to supervise the 4D Gaussian radiance fields along with traditional
photometric and geometric constraints. In experiments, qualitative and
quantitative evaluation results show that the proposed method achieves robust
tracking and high-quality view synthesis performance in real-world
environments.



---

## Digitally Prototype Your Eye Tracker: Simulating Hardware Performance  using 3D Synthetic Data

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Esther Y. H. Lin, Yimin Ding, Jogendra Kundu, Yatong An, Mohamed T. El-Haddad, Alexander Fix | cs.CV | [PDF](http://arxiv.org/pdf/2503.16742v1){: .btn .btn-green } |

**Abstract**: Eye tracking (ET) is a key enabler for Augmented and Virtual Reality (AR/VR).
Prototyping new ET hardware requires assessing the impact of hardware choices
on eye tracking performance. This task is compounded by the high cost of
obtaining data from sufficiently many variations of real hardware, especially
for machine learning, which requires large training datasets. We propose a
method for end-to-end evaluation of how hardware changes impact machine
learning-based ET performance using only synthetic data. We utilize a dataset
of real 3D eyes, reconstructed from light dome data using neural radiance
fields (NeRF), to synthesize captured eyes from novel viewpoints and camera
parameters. Using this framework, we demonstrate that we can predict the
relative performance across various hardware configurations, accounting for
variations in sensor noise, illumination brightness, and optical blur. We also
compare our simulator with the publicly available eye tracking dataset from the
Project Aria glasses, demonstrating a strong correlation with real-world
performance. Finally, we present a first-of-its-kind analysis in which we vary
ET camera positions, evaluating ET performance ranging from on-axis direct
views of the eye to peripheral views on the frame. Such an analysis would have
previously required manufacturing physical devices to capture evaluation data.
In short, our method enables faster prototyping of ET hardware.

Comments:
- 14 pages, 12 figures

---

## SAGE: Semantic-Driven Adaptive Gaussian Splatting in Extended Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Chiara Schiavo, Elena Camuffo, Leonardo Badia, Simone Milani | cs.GR | [PDF](http://arxiv.org/pdf/2503.16747v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has significantly improved the efficiency and
realism of three-dimensional scene visualization in several applications,
ranging from robotics to eXtended Reality (XR). This work presents SAGE
(Semantic-Driven Adaptive Gaussian Splatting in Extended Reality), a novel
framework designed to enhance the user experience by dynamically adapting the
Level of Detail (LOD) of different 3DGS objects identified via a semantic
segmentation. Experimental results demonstrate how SAGE effectively reduces
memory and computational overhead while keeping a desired target visual
quality, thus providing a powerful optimization for interactive XR
applications.



---

## Gaussian Graph Network: Learning Efficient and Generalizable Gaussian  Representations from Multi-view Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Shengjun Zhang, Xin Fei, Fangfu Liu, Haixu Song, Yueqi Duan | cs.CV | [PDF](http://arxiv.org/pdf/2503.16338v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis
performance. While conventional methods require per-scene optimization, more
recently several feed-forward methods have been proposed to generate
pixel-aligned Gaussian representations with a learnable network, which are
generalizable to different scenes. However, these methods simply combine
pixel-aligned Gaussians from multiple views as scene representations, thereby
leading to artifacts and extra memory cost without fully capturing the
relations of Gaussians from different images. In this paper, we propose
Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian
representations. Specifically, we construct Gaussian Graphs to model the
relations of Gaussian groups from different views. To support message passing
at Gaussian level, we reformulate the basic graph operations over Gaussian
representations, enabling each Gaussian to benefit from its connected Gaussian
groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling
layer to aggregate various Gaussian groups for efficient representations. We
conduct experiments on the large-scale RealEstate10K and ACID datasets to
demonstrate the efficiency and generalization of our method. Compared to the
state-of-the-art methods, our model uses fewer Gaussians and achieves better
image quality with higher rendering speed.

Comments:
- NeurIPS 2024

---

## GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Sixu Li, Ben Keller, Yingyan Celine Lin, Brucek Khailany | cs.GR | [PDF](http://arxiv.org/pdf/2503.16681v1){: .btn .btn-green } |

**Abstract**: 3D intelligence leverages rich 3D features and stands as a promising frontier
in AI, with 3D rendering fundamental to many downstream applications. 3D
Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method,
requires significant computation, making real-time execution on existing
GPU-equipped edge devices infeasible. Previous efforts to accelerate 3DGS rely
on dedicated accelerators that require substantial integration overhead and
hardware costs. This work proposes an acceleration strategy that leverages the
similarities between the 3DGS pipeline and the highly optimized conventional
graphics pipeline in modern GPUs. Instead of developing a dedicated
accelerator, we enhance existing GPU rasterizer hardware to efficiently support
3DGS operations. Our results demonstrate a 23$\times$ increase in processing
speed and a 24$\times$ reduction in energy consumption, with improvements
yielding 6$\times$ faster end-to-end runtime for the original 3DGS algorithm
and 4$\times$ for the latest efficiency-improved pipeline, achieving 24 FPS and
46 FPS respectively. These enhancements incur only a minimal area overhead of
0.2\% relative to the entire SoC chip area, underscoring the practicality and
efficiency of our approach for enabling 3DGS rendering on resource-constrained
platforms.

Comments:
- DAC 2025

---

## M3: 3D-Spatial MultiModal Memory

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Xueyan Zou, Yuchen Song, Ri-Zhao Qiu, Xuanbin Peng, Jianglong Ye, Sifei Liu, Xiaolong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.16413v1){: .btn .btn-green } |

**Abstract**: We present 3D Spatial MultiModal Memory (M3), a multimodal memory system
designed to retain information about medium-sized static scenes through video
sources for visual perception. By integrating 3D Gaussian Splatting techniques
with foundation models, M3 builds a multimodal memory capable of rendering
feature representations across granularities, encompassing a wide range of
knowledge. In our exploration, we identify two key challenges in previous works
on feature splatting: (1) computational constraints in storing high-dimensional
features for each Gaussian primitive, and (2) misalignment or information loss
between distilled features and foundation model features. To address these
challenges, we propose M3 with key components of principal scene components and
Gaussian memory attention, enabling efficient training and inference. To
validate M3, we conduct comprehensive quantitative evaluations of feature
similarity and downstream tasks, as well as qualitative visualizations to
highlight the pixel trace of Gaussian memory attention. Our approach
encompasses a diverse range of foundation models, including vision-language
models (VLMs), perception models, and large multimodal and language models
(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy
M3's feature field in indoor scenes on a quadruped robot. Notably, we claim
that M3 is the first work to address the core compression challenges in 3D
feature distillation.

Comments:
- ICLR2025 homepage: https://m3-spatial-memory.github.io code:
  https://github.com/MaureenZOU/m3-spatial

---

## Enhancing Close-up Novel View Synthesis via Pseudo-labeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Jiatong Xia, Libo Sun, Lingqiao Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.15908v1){: .btn .btn-green } |

**Abstract**: Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS), have demonstrated remarkable capabilities in novel view
synthesis. However, despite their success in producing high-quality images for
viewpoints similar to those seen during training, they struggle when generating
detailed images from viewpoints that significantly deviate from the training
set, particularly in close-up views. The primary challenge stems from the lack
of specific training data for close-up views, leading to the inability of
current methods to render these views accurately. To address this issue, we
introduce a novel pseudo-label-based learning strategy. This approach leverages
pseudo-labels derived from existing training data to provide targeted
supervision across a wide range of close-up viewpoints. Recognizing the absence
of benchmarks for this specific challenge, we also present a new dataset
designed to assess the effectiveness of both current and future methods in this
area. Our extensive experiments demonstrate the efficacy of our approach.

Comments:
- Accepted by AAAI 2025

---

## OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene  Reconstruction and Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Shiyong Liu, Xiao Tang, Zhihao Li, Yingfan He, Chongjie Ye, Jianzhuang Liu, Binxiao Huang, Shunbo Zhou, Xiaofei Wu | cs.GR | [PDF](http://arxiv.org/pdf/2503.16177v1){: .btn .btn-green } |

**Abstract**: In large-scale scene reconstruction using 3D Gaussian splatting, it is common
to partition the scene into multiple smaller regions and reconstruct them
individually. However, existing division methods are occlusion-agnostic,
meaning that each region may contain areas with severe occlusions. As a result,
the cameras within those regions are less correlated, leading to a low average
contribution to the overall reconstruction. In this paper, we propose an
occlusion-aware scene division strategy that clusters training cameras based on
their positions and co-visibilities to acquire multiple regions. Cameras in
such regions exhibit stronger correlations and a higher average contribution,
facilitating high-quality scene reconstruction. We further propose a
region-based rendering technique to accelerate large scene rendering, which
culls Gaussians invisible to the region where the viewpoint is located. Such a
technique significantly speeds up the rendering without compromising quality.
Extensive experiments on multiple large scenes show that our method achieves
superior reconstruction results with faster rendering speed compared to
existing state-of-the-art approaches. Project page:
https://occlugaussian.github.io.

Comments:
- Project website: https://occlugaussian.github.io

---

## BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-20 | Yiren Lu, Yunlai Zhou, Disheng Liu, Tuo Liang, Yu Yin | cs.CV | [PDF](http://arxiv.org/pdf/2503.15835v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene
reconstruction, and recent advancements have extended its application to
dynamic scenes. However, the quality of reconstructions depends heavily on
high-quality input images and precise camera poses, which are not that trivial
to fulfill in real-world scenarios. Capturing dynamic scenes with handheld
monocular cameras, for instance, typically involves simultaneous movement of
both the camera and objects within a single exposure. This combined motion
frequently results in image blur that existing methods cannot adequately
handle. To address these challenges, we introduce BARD-GS, a novel approach for
robust dynamic scene reconstruction that effectively handles blurry inputs and
imprecise camera poses. Our method comprises two main components: 1) camera
motion deblurring and 2) object motion deblurring. By explicitly decomposing
motion blur into camera motion blur and object motion blur and modeling them
separately, we achieve significantly improved rendering results in dynamic
regions. In addition, we collect a real-world motion blur dataset of dynamic
scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS
effectively reconstructs high-quality dynamic scenes under realistic
conditions, significantly outperforming existing methods.

Comments:
- CVPR2025. Project page at https://vulab-ai.github.io/BARD-GS/

---

## GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-19 | Zechuan Li, Hongshan Yu, Yihao Ding, Jinhao Qiao, Basim Azam, Naveed Akhtar | cs.CV | [PDF](http://arxiv.org/pdf/2503.15211v1){: .btn .btn-green } |

**Abstract**: We propose GO-N3RDet, a scene-geometry optimized multi-view 3D object
detector enhanced by neural radiance fields. The key to accurate 3D object
detection is in effective voxel representation. However, due to occlusion and
lack of 3D information, constructing 3D features from multi-view 2D images is
challenging. Addressing that, we introduce a unique 3D positional information
embedded voxel optimization mechanism to fuse multi-view features. To
prioritize neural field reconstruction in object regions, we also devise a
double importance sampling scheme for the NeRF branch of our detector. We
additionally propose an opacity optimization module for precise voxel opacity
prediction by enforcing multi-view consistency constraints. Moreover, to
further improve voxel density consistency across multiple perspectives, we
incorporate ray distance as a weighting factor to minimize cumulative ray
errors. Our unique modules synergetically form an end-to-end neural model that
establishes new state-of-the-art in NeRF-based multi-view 3D detection,
verified with extensive experiments on ScanNet and ARKITScenes. Code will be
available at https://github.com/ZechuanLi/GO-N3RDet.

Comments:
- Accepted by CVPR2025

---

## DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-19 | Yuming Gu, Phong Tran, Yujian Zheng, Hongyi Xu, Heyuan Li, Adilbek Karmanov, Hao Li | cs.CV | [PDF](http://arxiv.org/pdf/2503.15667v1){: .btn .btn-green } |

**Abstract**: Generating high-quality 360-degree views of human heads from single-view
images is essential for enabling accessible immersive telepresence applications
and scalable personalized content creation. While cutting-edge methods for full
head generation are limited to modeling realistic human heads, the latest
diffusion-based approaches for style-omniscient head synthesis can produce only
frontal views and struggle with view consistency, preventing their conversion
into true 3D models for rendering from arbitrary angles. We introduce a novel
approach that generates fully consistent 360-degree head views, accommodating
human, stylized, and anthropomorphic forms, including accessories like glasses
and hats. Our method builds on the DiffPortrait3D framework, incorporating a
custom ControlNet for back-of-head detail generation and a dual appearance
module to ensure global front-back consistency. By training on continuous view
sequences and integrating a back reference image, our approach achieves robust,
locally continuous view synthesis. Our model can be used to produce
high-quality neural radiance fields (NeRFs) for real-time, free-viewpoint
rendering, outperforming state-of-the-art methods in object synthesis and
360-degree head generation for very challenging input portraits.

Comments:
- Page:https://freedomgu.github.io/DiffPortrait360
  Code:https://github.com/FreedomGu/DiffPortrait360/

---

## SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-19 | Weiwen Hu, Niccolò Parodi, Marcus Zepp, Ingo Feldmann, Oliver Schreer, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2503.15712v1){: .btn .btn-green } |

**Abstract**: Open-vocabulary segmentation, powered by large visual-language models like
CLIP, has expanded 2D segmentation capabilities beyond fixed classes predefined
by the dataset, enabling zero-shot understanding across diverse scenes.
Extending these capabilities to 3D segmentation introduces challenges, as
CLIP's image-based embeddings often lack the geometric detail necessary for 3D
scene segmentation. Recent methods tend to address this by introducing
additional segmentation models or replacing CLIP with variations trained on
segmentation data, which lead to redundancy or loss on CLIP's general language
capabilities. To overcome this limitation, we introduce SPNeRF, a NeRF based
zero-shot 3D segmentation approach that leverages geometric priors. We
integrate geometric primitives derived from the 3D scene into NeRF training to
produce primitive-wise CLIP features, avoiding the ambiguity of point-wise
features. Additionally, we propose a primitive-based merging mechanism enhanced
with affinity scores. Without relying on additional segmentation models, our
method further explores CLIP's capability for 3D segmentation and achieves
notable improvements over original LERF.

Comments:
- In Proceedings of the 20th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications (2025)

---

## MultiBARF: Integrating Imagery of Different Wavelength Regions by Using  Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-19 | Kana Kurata, Hitoshi Niigaki, Xiaojun Wu, Ryuichi Tanida | cs.CV | [PDF](http://arxiv.org/pdf/2503.15070v1){: .btn .btn-green } |

**Abstract**: Optical sensor applications have become popular through digital
transformation. Linking observed data to real-world locations and combining
different image sensors is essential to make the applications practical and
efficient. However, data preparation to try different sensor combinations
requires high sensing and image processing expertise. To make data preparation
easier for users unfamiliar with sensing and image processing, we have
developed MultiBARF. This method replaces the co-registration and geometric
calibration by synthesizing pairs of two different sensor images and depth
images at assigned viewpoints. Our method extends Bundle Adjusting Neural
Radiance Fields(BARF), a deep neural network-based novel view synthesis method,
for the two imagers. Through experiments on visible light and thermographic
images, we demonstrate that our method superimposes two color channels of those
sensor images on NeRF.



---

## ClimateGS: Real-Time Climate Simulation with 3D Gaussian Style Transfer

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-19 | Yuezhen Xie, Meiying Zhang, Qi Hao | cs.GR | [PDF](http://arxiv.org/pdf/2503.14845v1){: .btn .btn-green } |

**Abstract**: Adverse climate conditions pose significant challenges for autonomous
systems, demanding reliable perception and decision-making across diverse
environments. To better simulate these conditions, physically-based NeRF
rendering methods have been explored for their ability to generate realistic
scene representations. However, these methods suffer from slow rendering speeds
and long preprocessing times, making them impractical for real-time testing and
user interaction. This paper presents ClimateGS, a novel framework integrating
3D Gaussian representations with physical simulation to enable real-time
climate effects rendering. The novelty of this work is threefold: 1) developing
a linear transformation for 3D Gaussian photorealistic style transfer, enabling
direct modification of spherical harmonics across bands for efficient and
consistent style adaptation; 2) developing a joint training strategy for 3D
style transfer, combining supervised and self-supervised learning to accelerate
convergence while preserving original scene details; 3) developing a real-time
rendering method for climate simulation, integrating physics-based effects with
3D Gaussian to achieve efficient and realistic rendering. We evaluate ClimateGS
on MipNeRF360 and Tanks and Temples, demonstrating real-time rendering with
comparable or superior visual quality to SOTA 2D/3D methods, making it suitable
for interactive applications.



---

## 3D Engine-ready Photorealistic Avatars via Dynamic Textures

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-19 | Yifan Wang, Ivan Molodetskikh, Ondrej Texler, Dimitar Dinev | cs.CV | [PDF](http://arxiv.org/pdf/2503.14943v1){: .btn .btn-green } |

**Abstract**: As the digital and physical worlds become more intertwined, there has been a
lot of interest in digital avatars that closely resemble their real-world
counterparts. Current digitization methods used in 3D production pipelines
require costly capture setups, making them impractical for mass usage among
common consumers. Recent academic literature has found success in
reconstructing humans from limited data using implicit representations (e.g.,
voxels used in NeRFs), which are able to produce impressive videos. However,
these methods are incompatible with traditional rendering pipelines, making it
difficult to use them in applications such as games. In this work, we propose
an end-to-end pipeline that builds explicitly-represented photorealistic 3D
avatars using standard 3D assets. Our key idea is the use of
dynamically-generated textures to enhance the realism and visually mask
deficiencies in the underlying mesh geometry. This allows for seamless
integration with current graphics pipelines while achieving comparable visual
quality to state-of-the-art 3D avatar generation methods.



---

## Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Simon Niedermayr, Christoph Neuhauser Rüdiger Westermann | cs.CV | [PDF](http://arxiv.org/pdf/2503.14171v1){: .btn .btn-green } |

**Abstract**: We introduce an image upscaling technique tailored for 3D Gaussian Splatting
(3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher
rendering speeds and reduces artifacts commonly observed in 3DGS
reconstructions. Our technique upscales low-resolution 3DGS renderings with a
marginal increase in cost by directly leveraging the analytical image gradients
of Gaussians for gradient-based bicubic spline interpolation. The technique is
agnostic to the specific 3DGS implementation, achieving novel view synthesis at
rates 3x-4x higher than the baseline implementation. Through extensive
experiments on multiple datasets, we showcase the performance improvements and
high reconstruction fidelity attainable with gradient-aware upscaling of 3DGS
images. We further demonstrate the integration of gradient-aware upscaling into
the gradient-based optimization of a 3DGS model and analyze its effects on
reconstruction quality and performance.



---

## Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency  Modulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Umar Farooq, Jean-Yves Guillemaut, Adrian Hilton, Marco Volino | cs.GR | [PDF](http://arxiv.org/pdf/2503.14475v1){: .btn .btn-green } |

**Abstract**: The field of Novel View Synthesis has been revolutionized by 3D Gaussian
Splatting (3DGS), which enables high-quality scene reconstruction that can be
rendered in real-time. 3DGS-based techniques typically suffer from high GPU
memory and disk storage requirements which limits their practical application
on consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated
coarse-to-fine optimization framework that aims to minimize the number of
Gaussian primitives used to represent a scene, thus reducing memory and storage
demands. Opti3DGS leverages image frequency modulation, initially enforcing a
coarse scene representation and progressively refining it by modulating
frequency details in the training images. On the baseline 3DGS, we demonstrate
an average reduction of 62% in Gaussians, a 40% reduction in the training GPU
memory requirements and a 20% reduction in optimization time without
sacrificing the visual quality. Furthermore, we show that our method integrates
seamlessly with many 3DGS-based techniques, consistently reducing the number of
Gaussian primitives while maintaining, and often improving, visual quality.
Additionally, Opti3DGS inherently produces a level-of-detail scene
representation at no extra cost, a natural byproduct of the optimization
pipeline. Results and code will be made publicly available.



---

## RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from  Sparse Multi-View Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Junjin Xiao, Qing Zhang, Yonewei Nie, Lei Zhu, Wei-Shi Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2503.14198v1){: .btn .btn-green } |

**Abstract**: This paper presents RoGSplat, a novel approach for synthesizing high-fidelity
novel views of unseen human from sparse multi-view images, while requiring no
cumbersome per-subject optimization. Unlike previous methods that typically
struggle with sparse views with few overlappings and are less effective in
reconstructing complex human geometry, the proposed method enables robust
reconstruction in such challenging conditions. Our key idea is to lift SMPL
vertices to dense and reliable 3D prior points representing accurate human body
geometry, and then regress human Gaussian parameters based on the points. To
account for possible misalignment between SMPL model and images, we propose to
predict image-aligned 3D prior points by leveraging both pixel-level features
and voxel-level features, from which we regress the coarse Gaussians. To
enhance the ability to capture high-frequency details, we further render depth
maps from the coarse 3D Gaussians to help regress fine-grained pixel-wise
Gaussians. Experiments on several benchmark datasets demonstrate that our
method outperforms state-of-the-art methods in novel view synthesis and
cross-dataset generalization. Our code is available at
https://github.com/iSEE-Laboratory/RoGSplat.

Comments:
- Accepted to CVPR2025

---

## HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Yilan Dong, Haohe Liu, Qing Wang, Jiahao Yang, Wenqing Wang, Gregory Slabaugh, Shanxin Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2503.14736v1){: .btn .btn-green } |

**Abstract**: Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on
rigid skeletal motion with an oversimplified non-rigid motion model, which
fails to capture fine geometric and appearance details. Additionally, they
perform densification based solely on per-point gradients and process poses
independently, ignoring spatial and temporal correlations. These limitations
lead to geometric detail loss, temporal instability, and inefficient point
distribution. To address these issues, we propose HandSplat, a novel Gaussian
Splatting-based framework that enhances both fidelity and stability for hand
rendering. To improve fidelity, we extend standard 3DGS attributes with
implicit geometry and appearance embeddings for finer non-rigid motion modeling
while preserving the static hand characteristic modeled by original 3DGS
attributes. Additionally, we introduce a local gradient-aware densification
strategy that dynamically refines Gaussian density in high-variation regions.
To improve stability, we incorporate pose-conditioned attribute regularization
to encourage attribute consistency across similar poses, mitigating temporal
artifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat
surpasses existing methods in fidelity and stability while achieving real-time
performance. We will release the code and pre-trained models upon acceptance.



---

## Improving Adaptive Density Control for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Glenn Grubert, Florian Barthel, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2503.14274v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become one of the most influential works in
the past year. Due to its efficient and high-quality novel view synthesis
capabilities, it has been widely adopted in many research fields and
applications. Nevertheless, 3DGS still faces challenges to properly manage the
number of Gaussian primitives that are used during scene reconstruction.
Following the adaptive density control (ADC) mechanism of 3D Gaussian
Splatting, new Gaussians in under-reconstructed regions are created, while
Gaussians that do not contribute to the rendering quality are pruned. We
observe that those criteria for densifying and pruning Gaussians can sometimes
lead to worse rendering by introducing artifacts. We especially observe
under-reconstructed background or overfitted foreground regions. To encounter
both problems, we propose three new improvements to the adaptive density
control mechanism. Those include a correction for the scene extent calculation
that does not only rely on camera positions, an exponentially ascending
gradient threshold to improve training convergence, and significance-aware
pruning strategy to avoid background artifacts. With these adaptions, we show
that the rendering quality improves while using the same number of Gaussians
primitives. Furthermore, with our improvements, the training converges
considerably faster, allowing for more than twice as fast training times while
yielding better quality than 3DGS. Finally, our contributions are easily
compatible with most existing derivative works of 3DGS making them relevant for
future works.



---

## Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Runsong Zhu, Shi Qiu, Zhengzhe Liu, Ka-Hei Hui, Qianyi Wu, Pheng-Ann Heng, Chi-Wing Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.14029v1){: .btn .btn-green } |

**Abstract**: Lifting multi-view 2D instance segmentation to a radiance field has proven to
be effective to enhance 3D understanding. Existing methods rely on direct
matching for end-to-end lifting, yielding inferior results; or employ a
two-stage solution constrained by complex pre- or post-processing. In this
work, we design a new end-to-end object-aware lifting approach, named
Unified-Lift that provides accurate 3D segmentation based on the 3D Gaussian
representation. To start, we augment each Gaussian point with an additional
Gaussian-level feature learned using a contrastive loss to encode instance
information. Importantly, we introduce a learnable object-level codebook to
account for individual objects in the scene for an explicit object-level
understanding and associate the encoded object-level features with the
Gaussian-level point features for segmentation predictions. While promising,
achieving effective codebook learning is non-trivial and a naive solution leads
to degraded performance. Therefore, we formulate the association learning
module and the noisy label filtering module for effective and robust codebook
learning. We conduct experiments on three benchmarks: LERF-Masked, Replica, and
Messy Rooms datasets. Both qualitative and quantitative results manifest that
our Unified-Lift clearly outperforms existing methods in terms of segmentation
quality and time efficiency. The code is publicly available at
\href{https://github.com/Runsong123/Unified-Lift}{https://github.com/Runsong123/Unified-Lift}.

Comments:
- CVPR 2025. The code is publicly available at this https URL
  (https://github.com/Runsong123/Unified-Lift)

---

## SplatVoxel: History-Aware Novel View Streaming without Temporal Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Yiming Wang, Lucy Chai, Xuan Luo, Michael Niemeyer, Manuel Lagunas, Stephen Lombardi, Siyu Tang, Tiancheng Sun | cs.CV | [PDF](http://arxiv.org/pdf/2503.14698v1){: .btn .btn-green } |

**Abstract**: We study the problem of novel view streaming from sparse-view videos, which
aims to generate a continuous sequence of high-quality, temporally consistent
novel views as new input frames arrive. However, existing novel view synthesis
methods struggle with temporal coherence and visual fidelity, leading to
flickering and inconsistency. To address these challenges, we introduce
history-awareness, leveraging previous frames to reconstruct the scene and
improve quality and stability. We propose a hybrid splat-voxel feed-forward
scene reconstruction approach that combines Gaussian Splatting to propagate
information over time, with a hierarchical voxel grid for temporal fusion.
Gaussian primitives are efficiently warped over time using a motion graph that
extends 2D tracking models to 3D motion, while a sparse voxel transformer
integrates new temporal observations in an error-aware manner. Crucially, our
method does not require training on multi-view video datasets, which are
currently limited in size and diversity, and can be directly applied to
sparse-view video streams in a history-aware manner at inference time. Our
approach achieves state-of-the-art performance in both static and streaming
scene reconstruction, effectively reducing temporal artifacts and visual
artifacts while running at interactive rates (15 fps with 350ms delay) on a
single H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/



---

## Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via  Context Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Mufan Liu, Qi Yang, He Huang, Wenjie Huang, Zhenlong Yuan, Zhu Li, Yiling Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.13948v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as an efficient and high-fidelity
paradigm for novel view synthesis. To adapt 3DGS for dynamic content,
deformable 3DGS incorporates temporally deformable primitives with learnable
latent embeddings to capture complex motions. Despite its impressive
performance, the high-dimensional embeddings and vast number of primitives lead
to substantial storage requirements. In this paper, we introduce a
\textbf{Light}weight \textbf{4}D\textbf{GS} framework, called Light4GS, that
employs significance pruning with a deep context model to provide a lightweight
storage-efficient dynamic 3DGS representation. The proposed Light4GS is based
on 4DGS that is a typical representation of deformable 3DGS. Specifically, our
framework is built upon two core components: (1) a spatio-temporal significance
pruning strategy that eliminates over 64\% of the deformable primitives,
followed by an entropy-constrained spherical harmonics compression applied to
the remainder; and (2) a deep context model that integrates intra- and
inter-prediction with hyperprior into a coarse-to-fine context structure to
enable efficient multiscale latent embedding compression. Our approach achieves
over 120x compression and increases rendering FPS up to 20\% compared to the
baseline 4DGS, and also superior to frame-wise state-of-the-art 3DGS
compression methods, revealing the effectiveness of our Light4GS in terms of
both intra- and inter-prediction methods without sacrificing rendering quality.



---

## Segmentation-Guided Neural Radiance Fields for Novel Street View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-18 | Yizhou Li, Yusuke Monno, Masatoshi Okutomi, Yuuichi Tanaka, Seiichi Kataoka, Teruaki Kosiba | cs.CV | [PDF](http://arxiv.org/pdf/2503.14219v1){: .btn .btn-green } |

**Abstract**: Recent advances in Neural Radiance Fields (NeRF) have shown great potential
in 3D reconstruction and novel view synthesis, particularly for indoor and
small-scale scenes. However, extending NeRF to large-scale outdoor environments
presents challenges such as transient objects, sparse cameras and textures, and
varying lighting conditions. In this paper, we propose a segmentation-guided
enhancement to NeRF for outdoor street scenes, focusing on complex urban
environments. Our approach extends ZipNeRF and utilizes Grounded SAM for
segmentation mask generation, enabling effective handling of transient objects,
modeling of the sky, and regularization of the ground. We also introduce
appearance embeddings to adapt to inconsistent lighting across view sequences.
Experimental results demonstrate that our method outperforms the baseline
ZipNeRF, improving novel view synthesis quality with fewer artifacts and
sharper details.

Comments:
- Presented at VISAPP2025. Project page:
  http://www.ok.sc.e.titech.ac.jp/res/NVS/index.html

---

## Improving Geometric Consistency for 360-Degree Neural Radiance Fields in  Indoor Scenarios

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Iryna Repinetska, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2503.13710v1){: .btn .btn-green } |

**Abstract**: Photo-realistic rendering and novel view synthesis play a crucial role in
human-computer interaction tasks, from gaming to path planning. Neural Radiance
Fields (NeRFs) model scenes as continuous volumetric functions and achieve
remarkable rendering quality. However, NeRFs often struggle in large,
low-textured areas, producing cloudy artifacts known as ''floaters'' that
reduce scene realism, especially in indoor environments with featureless
architectural surfaces like walls, ceilings, and floors. To overcome this
limitation, prior work has integrated geometric constraints into the NeRF
pipeline, typically leveraging depth information derived from Structure from
Motion or Multi-View Stereo. Yet, conventional RGB-feature correspondence
methods face challenges in accurately estimating depth in textureless regions,
leading to unreliable constraints. This challenge is further complicated in
360-degree ''inside-out'' views, where sparse visual overlap between adjacent
images further hinders depth estimation. In order to address these issues, we
propose an efficient and robust method for computing dense depth priors,
specifically tailored for large low-textured architectural surfaces in indoor
environments. We introduce a novel depth loss function to enhance rendering
quality in these challenging, low-feature regions, while complementary
depth-patch regularization further refines depth consistency across other
areas. Experiments with Instant-NGP on two synthetic 360-degree indoor scenes
demonstrate improved visual fidelity with our method compared to standard
photometric loss and Mean Squared Error depth supervision.



---

## DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for  Distractor-free 3D Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Rui Wang, Quentin Lohmeyer, Mirko Meboldt, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2503.13176v1){: .btn .btn-green } |

**Abstract**: Reconstructing clean, distractor-free 3D scenes from real-world captures
remains a significant challenge, particularly in highly dynamic and cluttered
settings such as egocentric videos. To tackle this problem, we introduce
DeGauss, a simple and robust self-supervised framework for dynamic scene
reconstruction based on a decoupled dynamic-static Gaussian Splatting design.
DeGauss models dynamic elements with foreground Gaussians and static content
with background Gaussians, using a probabilistic mask to coordinate their
composition and enable independent yet complementary optimization. DeGauss
generalizes robustly across a wide range of real-world scenarios, from casual
image collections to long, dynamic egocentric videos, without relying on
complex heuristics or extensive supervision. Experiments on benchmarks
including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that
DeGauss consistently outperforms existing methods, establishing a strong
baseline for generalizable, distractor-free 3D reconstructionin highly dynamic,
interaction-rich environments.



---

## CAT-3DGS Pro: A New Benchmark for Efficient 3DGS Compression

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Yu-Ting Zhan, He-bi Yang, Cheng-Yuan Ho, Jui-Chiu Chiang, Wen-Hsiao Peng | cs.CV | [PDF](http://arxiv.org/pdf/2503.12862v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown immense potential for novel view
synthesis. However, achieving rate-distortion-optimized compression of 3DGS
representations for transmission and/or storage applications remains a
challenge. CAT-3DGS introduces a context-adaptive triplane hyperprior for
end-to-end optimized compression, delivering state-of-the-art coding
performance. Despite this, it requires prolonged training and decoding time. To
address these limitations, we propose CAT-3DGS Pro, an enhanced version of
CAT-3DGS that improves both compression performance and computational
efficiency. First, we introduce a PCA-guided vector-matrix hyperprior, which
replaces the triplane-based hyperprior to reduce redundant parameters. To
achieve a more balanced rate-distortion trade-off and faster encoding, we
propose an alternate optimization strategy (A-RDO). Additionally, we refine the
sampling rate optimization method in CAT-3DGS, leading to significant
improvements in rate-distortion performance. These enhancements result in a
46.6% BD-rate reduction and 3x speedup in training time on BungeeNeRF, while
achieving 5x acceleration in decoding speed for the Amsterdam scene compared to
CAT-3DGS.



---

## DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency  for Few-shot View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Ingyun Lee, Jae Won Jang, Seunghyeon Seo, Nojun Kwak | cs.CV | [PDF](http://arxiv.org/pdf/2503.12947v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has shown remarkable performance in novel view
synthesis but requires many multiview images, making it impractical for
few-shot scenarios. Ray augmentation was proposed to prevent overfitting for
sparse training data by generating additional rays. However, existing methods,
which generate augmented rays only near the original rays, produce severe
floaters and appearance distortion due to limited viewpoints and inconsistent
rays obstructed by nearby obstacles and complex surfaces. To address these
problems, we propose DivCon-NeRF, which significantly enhances both diversity
and consistency. It employs surface-sphere augmentation, which preserves the
distance between the original camera and the predicted surface point. This
allows the model to compare the order of high-probability surface points and
filter out inconsistent rays easily without requiring the exact depth. By
introducing inner-sphere augmentation, DivCon-NeRF randomizes angles and
distances for diverse viewpoints, further increasing diversity. Consequently,
our method significantly reduces floaters and visual distortions, achieving
state-of-the-art performance on the Blender, LLFF, and DTU datasets. Our code
will be publicly available.

Comments:
- 11 pages, 6 figures

---

## Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion  Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Katja Schwarz, Norman Mueller, Peter Kontschieder | cs.CV | [PDF](http://arxiv.org/pdf/2503.13272v1){: .btn .btn-green } |

**Abstract**: Synthesizing consistent and photorealistic 3D scenes is an open problem in
computer vision. Video diffusion models generate impressive videos but cannot
directly synthesize 3D representations, i.e., lack 3D consistency in the
generated sequences. In addition, directly training generative 3D models is
challenging due to a lack of 3D training data at scale. In this work, we
present Generative Gaussian Splatting (GGS) -- a novel approach that integrates
a 3D representation with a pre-trained latent video diffusion model.
Specifically, our model synthesizes a feature field parameterized via 3D
Gaussian primitives. The feature field is then either rendered to feature maps
and decoded into multi-view images, or directly upsampled into a 3D radiance
field. We evaluate our approach on two common benchmark datasets for scene
synthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model
significantly improves both the 3D consistency of the generated multi-view
images, and the quality of the generated 3D scenes over all relevant baselines.
Compared to a similar model without 3D representation, GGS improves FID on the
generated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page:
https://katjaschwarz.github.io/ggs/



---

## AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Hadam Baek, Hannie Shin, Jiyoung Seo, Chanwoo Kim, Saerom Kim, Hyeongbok Kim, Sangpil Kim | cs.MM | [PDF](http://arxiv.org/pdf/2503.12806v1){: .btn .btn-green } |

**Abstract**: Accurately modeling sound propagation with complex real-world environments is
essential for Novel View Acoustic Synthesis (NVAS). While previous studies have
leveraged visual perception to estimate spatial acoustics, the combined use of
surface normal and structural details from 3D representations in acoustic
modeling has been underexplored. Given their direct impact on sound wave
reflections and propagation, surface normals should be jointly modeled with
structural details to achieve accurate spatial acoustics. In this paper, we
propose a surface-enhanced geometry-aware approach for NVAS to improve spatial
acoustic modeling. To achieve this, we exploit geometric priors, such as image,
depth map, surface normals, and point clouds obtained using a 3D Gaussian
Splatting (3DGS) based framework. We introduce a dual cross-attention-based
transformer integrating geometrical constraints into frequency query to
understand the surroundings of the emitter. Additionally, we design a
ConvNeXt-based spectral features processing network called Spectral Refinement
Network (SRN) to synthesize realistic binaural audio. Experimental results on
the RWAVS and SoundSpace datasets highlight the necessity of our approach, as
it surpasses existing methods in novel view acoustic synthesis.



---

## CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2503.12836v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D
reconstruction and novel view synthesis, leading to its widespread commercial
use. Consequently, copyright protection via watermarking has become critical.
However, because 3DGS relies on millions of Gaussians, which require gigabytes
of storage, efficient transfer and storage require compression. Existing 3DGS
watermarking methods are vulnerable to quantization-based compression, often
resulting in the loss of the embedded watermark. To address this challenge, we
propose a novel watermarking method that ensures watermark robustness after
model compression while maintaining high rendering quality. In detail, we
incorporate a quantization distortion layer that simulates compression during
training, preserving the watermark under quantization-based compression. Also,
we propose a learnable watermark embedding feature that embeds the watermark
into the anchor feature, ensuring structural consistency and seamless
integration into the 3D scene. Furthermore, we present a frequency-aware anchor
growing mechanism to enhance image quality in high-frequency regions by
effectively identifying Guassians within these regions. Experimental results
confirm that our method preserves the watermark and maintains superior image
quality under high compression, validating it as a promising approach for a
secure 3DGS model.

Comments:
- 23 pages, 17 figures

---

## TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing  Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Jiaming Kang, Keyan Chen, Zhengxia Zou, Zhenwei Shi | cs.CV | [PDF](http://arxiv.org/pdf/2503.13347v1){: .btn .btn-green } |

**Abstract**: Remote sensing novel view synthesis (NVS) offers significant potential for 3D
interpretation of remote sensing scenes, with important applications in urban
planning and environmental monitoring. However, remote sensing scenes
frequently lack sufficient multi-view images due to acquisition constraints.
While existing NVS methods tend to overfit when processing limited input views,
advanced few-shot NVS methods are computationally intensive and perform
sub-optimally in remote sensing scenes. This paper presents TriDF, an efficient
hybrid 3D representation for fast remote sensing NVS from as few as 3 input
views. Our approach decouples color and volume density information, modeling
them independently to reduce the computational burden on implicit radiance
fields and accelerate reconstruction. We explore the potential of the triplane
representation in few-shot NVS tasks by mapping high-frequency color
information onto this compact structure, and the direct optimization of feature
planes significantly speeds up convergence. Volume density is modeled as
continuous density fields, incorporating reference features from neighboring
views through image-based rendering to compensate for limited input data.
Additionally, we introduce depth-guided optimization based on point clouds,
which effectively mitigates the overfitting problem in few-shot NVS.
Comprehensive experiments across multiple remote sensing scenes demonstrate
that our hybrid representation achieves a 30x speed increase compared to
NeRF-based methods, while simultaneously improving rendering quality metrics
over advanced few-shot methods (7.4% increase in PSNR, 12.2% in SSIM, and 18.7%
in LPIPS). The code is publicly available at https://github.com/kanehub/TriDF



---

## Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near  Real-Time 3DGS Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-17 | Yiwei Xu, Yifei Yu, Wentian Gan, Tengfei Wang, Zongqian Zhan, Hao Cheng, Xin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.13086v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast
real-time performance, but existing methods rely on offline training after full
Structure-from-Motion (SfM) processing. In contrast, this work introduces
On-the-Fly GS, a progressive framework enabling near real-time 3DGS
optimization during image capture. As each image arrives, its pose and sparse
points are updated via on-the-fly SfM, and newly optimized Gaussians are
immediately integrated into the 3DGS field. We propose a progressive local
optimization strategy to prioritize new images and their neighbors by their
corresponding overlapping relationship, allowing the new image and its
overlapping images to get more training. To further stabilize training across
old and new images, an adaptive learning rate schedule balances the iterations
and the learning rate. Moreover, to maintain overall quality of the 3DGS field,
an efficient global optimization scheme prevents overfitting to the newly added
images. Experiments on multiple benchmark datasets show that our On-the-Fly GS
reduces training time significantly, optimizing each new image in seconds with
minimal rendering loss, offering the first practical step toward rapid,
progressive 3DGS reconstruction.



---

## GS-I$^{3}$: Gaussian Splatting for Surface Reconstruction from  Illumination-Inconsistent Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Tengfei Wang, Yongmao Hou, Zhaoning Zhang, Yiwei Xu, Zongqian Zhan, Xin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.12335v2){: .btn .btn-green } |

**Abstract**: Accurate geometric surface reconstruction, providing essential environmental
information for navigation and manipulation tasks, is critical for enabling
robotic self-exploration and interaction. Recently, 3D Gaussian Splatting
(3DGS) has gained significant attention in the field of surface reconstruction
due to its impressive geometric quality and computational efficiency. While
recent relevant advancements in novel view synthesis under inconsistent
illumination using 3DGS have shown promise, the challenge of robust surface
reconstruction under such conditions is still being explored. To address this
challenge, we propose a method called GS-3I. Specifically, to mitigate 3D
Gaussian optimization bias caused by underexposed regions in single-view
images, based on Convolutional Neural Network (CNN), a tone mapping correction
framework is introduced. Furthermore, inconsistent lighting across multi-view
images, resulting from variations in camera settings and complex scene
illumination, often leads to geometric constraint mismatches and deviations in
the reconstructed surface. To overcome this, we propose a normal compensation
mechanism that integrates reference normals extracted from single-view image
with normals computed from multi-view observations to effectively constrain
geometric inconsistencies. Extensive experimental evaluations demonstrate that
GS-3I can achieve robust and accurate surface reconstruction across complex
illumination scenarios, highlighting its effectiveness and versatility in this
critical challenge. https://github.com/TFwang-9527/GS-3I

Comments:
- Comments: This work has been submitted to the 2025 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2025) for
  possible publication

---

## Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and  efficient reconstruction of dynamic scene

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Jiahao Wu, Rui Peng, Zhiyan Wang, Lu Xiao, Luyang Tang, Jinbo Yan, Kaiqiang Xiong, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.12307v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis has long been a practical but challenging task, although
the introduction of numerous methods to solve this problem, even combining
advanced representations like 3D Gaussian Splatting, they still struggle to
recover high-quality results and often consume too much storage memory and
training time. In this paper we propose Swift4D, a divide-and-conquer 3D
Gaussian Splatting method that can handle static and dynamic primitives
separately, achieving a good trade-off between rendering quality and
efficiency, motivated by the fact that most of the scene is the static
primitive and does not require additional dynamic properties. Concretely, we
focus on modeling dynamic transformations only for the dynamic primitives which
benefits both efficiency and quality. We first employ a learnable decomposition
strategy to separate the primitives, which relies on an additional parameter to
classify primitives as static or dynamic. For the dynamic primitives, we employ
a compact multi-resolution 4D Hash mapper to transform these primitives from
canonical space into deformation space at each timestamp, and then mix the
static and dynamic primitives to produce the final output. This
divide-and-conquer method facilitates efficient training and reduces storage
redundancy. Our method not only achieves state-of-the-art rendering quality
while being 20X faster in training than previous SOTA methods with a minimum
storage requirement of only 30MB on real-world datasets. Code is available at
https://github.com/WuJH2001/swift4d.

Comments:
- ICLR 2025

---

## VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Songen Gu, Haoxuan Song, Binjie Liu, Qian Yu, Sanyi Zhang, Haiyong Jiang, Jin Huang, Feng Tian | cs.CV | [PDF](http://arxiv.org/pdf/2503.12383v1){: .btn .btn-green } |

**Abstract**: We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native
3D object generation framework that incorporates a 3D Gaussian Splatting
representation. As part of our work, we introduce VRSS, the first large-scale
paired dataset containing VR sketches, text, images, and 3DGS, bridging the gap
in multi-modal VR sketch-based generation. Our approach features the following
key innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage
alignment strategy that bridges the domain gap between sparse VR sketch
embeddings and rich CLIP embeddings, facilitating both VR sketch-based
retrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We
disentangle the 3D generation process by using explicit VR sketches for
geometric conditioning and text descriptions for appearance control. To
facilitate this, we propose a generalizable VR sketch encoder that effectively
aligns different modalities. 3) Efficient and high-fidelity 3D native
generation. Our method leverages a 3D-native generation approach that enables
fast and texture-rich 3D object synthesis. Experiments conducted on our VRSS
dataset demonstrate that our method achieves high-quality, multi-modal VR
sketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian
method will be beneficial for the 3D generation community.



---

## SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor  Open-World Free-view Synthesis from Sparse Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Guibiao Liao, Qing Li, Zhenyu Bao, Guoping Qiu, Kanglin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.12535v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting-based indoor open-world free-view synthesis approaches
have shown significant performance with dense input images. However, they
exhibit poor performance when confronted with sparse inputs, primarily due to
the sparse distribution of Gaussian points and insufficient view supervision.
To relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based
Gaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)
Regularization for open-world free view synthesis with sparse inputs.
Specifically, SGI provides a dense, scene-layout-based Gaussian distribution by
utilizing view-changed images generated from the video generation model and
view-constraint Gaussian points densification. Additionally, SPC mitigates
limited view supervision by employing semantic-prompt-based consistency
constraints developed by SAM2. This approach leverages available semantics from
training views, serving as instructive prompts, to optimize visually
overlapping regions in novel views with 2D and 3D consistency constraints.
Extensive experiments demonstrate the superior performance of SPC-GS across
Replica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in
PSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world
semantic segmentation.

Comments:
- Accepted by CVPR2025. The project page is available at
  https://gbliao.github.io/SPC-GS.github.io/

---

## MTGS: Multi-Traversal Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Tianyu Li, Yihang Qiu, Zhenhua Wu, Carl Lindström, Peng Su, Matthias Nießner, Hongyang Li | cs.CV | [PDF](http://arxiv.org/pdf/2503.12552v1){: .btn .btn-green } |

**Abstract**: Multi-traversal data, commonly collected through daily commutes or by
self-driving fleets, provides multiple viewpoints for scene reconstruction
within a road block. This data offers significant potential for high-quality
novel view synthesis, which is crucial for applications such as autonomous
vehicle simulators. However, inherent challenges in multi-traversal data often
result in suboptimal reconstruction quality, including variations in appearance
and the presence of dynamic objects. To address these issues, we propose
Multi-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs
high-quality driving scenes from arbitrarily collected multi-traversal data by
modeling a shared static geometry while separately handling dynamic elements
and appearance variations. Our method employs a multi-traversal dynamic scene
graph with a shared static node and traversal-specific dynamic nodes,
complemented by color correction nodes with learnable spherical harmonics
coefficient residuals. This approach enables high-fidelity novel view synthesis
and provides flexibility to navigate any viewpoint. We conduct extensive
experiments on a large-scale driving dataset, nuPlan, with multi-traversal
data. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry
accuracy by 46.3% compared to single-traversal baselines. The code and data
would be available to the public.



---

## Deblur Gaussian Splatting SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Francesco Girlanda, Denys Rozumnyi, Marc Pollefeys, Martin R. Oswald | cs.CV | [PDF](http://arxiv.org/pdf/2503.12572v1){: .btn .btn-green } |

**Abstract**: We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp
reconstructions from motion-blurred inputs. The proposed method bridges the
strengths of both frame-to-frame and frame-to-model approaches to model
sub-frame camera trajectories that lead to high-fidelity reconstructions in
motion-blurred settings. Moreover, our pipeline incorporates techniques such as
online loop closure and global bundle adjustment to achieve a dense and precise
global trajectory. We model the physical image formation process of
motion-blurred images and minimize the error between the observed blurry images
and rendered blurry images obtained by averaging sharp virtual sub-frame
images. Additionally, by utilizing a monocular depth estimator alongside the
online deformation of Gaussians, we ensure precise mapping and enhanced image
deblurring. The proposed SLAM pipeline integrates all these components to
improve the results. We achieve state-of-the-art results for sharp map
estimation and sub-frame trajectory recovery both on synthetic and real-world
blurry input data.



---

## TopoGaussian: Inferring Internal Topology Structures from Visual Clues

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-16 | Xiaoyu Xiong, Changyu Hu, Chunru Lin, Pingchuan Ma, Chuang Gan, Tao Du | cs.CV | [PDF](http://arxiv.org/pdf/2503.12343v1){: .btn .btn-green } |

**Abstract**: We present TopoGaussian, a holistic, particle-based pipeline for inferring
the interior structure of an opaque object from easily accessible photos and
videos as input. Traditional mesh-based approaches require tedious and
error-prone mesh filling and fixing process, while typically output rough
boundary surface. Our pipeline combines Gaussian Splatting with a novel,
versatile particle-based differentiable simulator that simultaneously
accommodates constitutive model, actuator, and collision, without interference
with mesh. Based on the gradients from this simulator, we provide flexible
choice of topology representation for optimization, including particle, neural
implicit surface, and quadratic surface. The resultant pipeline takes easily
accessible photos and videos as input and outputs the topology that matches the
physical characteristics of the input. We demonstrate the efficacy of our
pipeline on a synthetic dataset and four real-world tasks with 3D-printed
prototypes. Compared with existing mesh-based method, our pipeline is 5.26x
faster on average with improved shape quality. These results highlight the
potential of our pipeline in 3D vision, soft robotics, and manufacturing
applications.



---

## FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Rui Qian, Chenyangguang Zhang, Yan Di, Guangyao Zhai, Ruida Zhang, Jiayu Guo, Benjamin Busam, Jian Pu | cs.CV | [PDF](http://arxiv.org/pdf/2503.12086v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have exhibited highly effective performance for
photorealistic novel view synthesis recently. However, the key limitation it
meets is the reliance on a hand-crafted frequency annealing strategy to recover
3D scenes with imperfect camera poses. The strategy exploits a temporal
low-pass filter to guarantee convergence while decelerating the joint
optimization of implicit scene reconstruction and camera registration. In this
work, we introduce the Frequency Adapted Bundle Adjusting Radiance Field
(FA-BARF), substituting the temporal low-pass filter for a frequency-adapted
spatial low-pass filter to address the decelerating problem. We establish a
theoretical framework to interpret the relationship between position encoding
of NeRF and camera registration and show that our frequency-adapted filter can
mitigate frequency fluctuation caused by the temporal filter. Furthermore, we
show that applying a spatial low-pass filter in NeRF can optimize camera poses
productively through radial uncertainty overlaps among various views. Extensive
experiments show that FA-BARF can accelerate the joint optimization process
under little perturbations in object-centric scenes and recover real-world
scenes with unknown camera poses. This implies wider possibilities for NeRF
applied in dense 3D mapping and reconstruction under real-time requirements.
The code will be released upon paper acceptance.



---

## REdiSplats: Ray Tracing for Editable Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Krzysztof Byrski, Grzegorz Wilczyński, Weronika Smolak-Dyżewska, Piotr Borycki, Dawid Baran, Sławomir Tadeja, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2503.12284v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has become one of the most important neural rendering
algorithms. GS represents 3D scenes using Gaussian components with trainable
color and opacity. This representation achieves high-quality renderings with
fast inference. Regrettably, it is challenging to integrate such a solution
with varying light conditions, including shadows and light reflections, manual
adjustments, and a physical engine. Recently, a few approaches have appeared
that incorporate ray-tracing or mesh primitives into GS to address some of
these caveats. However, no such solution can simultaneously solve all the
existing limitations of the classical GS. Consequently, we introduce
REdiSplats, which employs ray tracing and a mesh-based representation of flat
3D Gaussians. In practice, we model the scene using flat Gaussian distributions
parameterized by the mesh. We can leverage fast ray tracing and control
Gaussian modification by adjusting the mesh vertices. Moreover, REdiSplats
allows modeling of light conditions, manual adjustments, and physical
simulation. Furthermore, we can render our models using 3D tools such as
Blender or Nvdiffrast, which opens the possibility of integrating them with all
existing 3D graphics techniques dedicated to mesh representations.



---

## 3D Gaussian Splatting against Moving Objects for High-Fidelity Street  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Peizhen Zheng, Longfei Wei, Dongjing Jiang, Jianfei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.12001v2){: .btn .btn-green } |

**Abstract**: The accurate reconstruction of dynamic street scenes is critical for
applications in autonomous driving, augmented reality, and virtual reality.
Traditional methods relying on dense point clouds and triangular meshes
struggle with moving objects, occlusions, and real-time processing constraints,
limiting their effectiveness in complex urban environments. While multi-view
stereo and neural radiance fields have advanced 3D reconstruction, they face
challenges in computational efficiency and handling scene dynamics. This paper
proposes a novel 3D Gaussian point distribution method for dynamic street scene
reconstruction. Our approach introduces an adaptive transparency mechanism that
eliminates moving objects while preserving high-fidelity static scene details.
Additionally, iterative refinement of Gaussian point distribution enhances
geometric accuracy and texture representation. We integrate directional
encoding with spatial position optimization to optimize storage and rendering
efficiency, reducing redundancy while maintaining scene integrity. Experimental
results demonstrate that our method achieves high reconstruction quality,
improved rendering performance, and adaptability in large-scale dynamic
environments. These contributions establish a robust framework for real-time,
high-precision 3D reconstruction, advancing the practicality of dynamic scene
modeling across multiple applications.



---

## DecompDreamer: Advancing Structured 3D Asset Generation with  Multi-Object Decomposition and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Utkarsh Nath, Rajeev Goel, Rahul Khurana, Kyle Min, Mark Ollila, Pavan Turaga, Varun Jampani, Tejaswi Gowda | cs.CV | [PDF](http://arxiv.org/pdf/2503.11981v1){: .btn .btn-green } |

**Abstract**: Text-to-3D generation saw dramatic advances in recent years by leveraging
Text-to-Image models. However, most existing techniques struggle with
compositional prompts, which describe multiple objects and their spatial
relationships. They often fail to capture fine-grained inter-object
interactions. We introduce DecompDreamer, a Gaussian splatting-based training
routine designed to generate high-quality 3D compositions from such complex
prompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose
scenes into structured components and their relationships. We propose a
progressive optimization strategy that first prioritizes joint relationship
modeling before gradually shifting toward targeted object refinement. Our
qualitative and quantitative evaluations against state-of-the-art text-to-3D
models demonstrate that DecompDreamer effectively generates intricate 3D
compositions with superior object disentanglement, offering enhanced control
and flexibility in 3D generation. Project page :
https://decompdreamer3d.github.io



---

## DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering,  Tracking, Motion Predictions of Moving Objects in Dynamic Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-15 | Runfa Blark Li, Mahdi Shaghaghi, Keito Suzuki, Xinshuang Liu, Varun Moparthi, Bang Du, Walker Curtis, Martin Renschler, Ki Myung Brian Lee, Nikolay Atanasov, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2503.11979v1){: .btn .btn-green } |

**Abstract**: Simultaneous Localization and Mapping (SLAM) is one of the most important
environment-perception and navigation algorithms for computer vision, robotics,
and autonomous cars/drones. Hence, high quality and fast mapping becomes a
fundamental problem. With the advent of 3D Gaussian Splatting (3DGS) as an
explicit representation with excellent rendering quality and speed,
state-of-the-art (SOTA) works introduce GS to SLAM. Compared to classical
pointcloud-SLAM, GS-SLAM generates photometric information by learning from
input camera views and synthesize unseen views with high-quality textures.
However, these GS-SLAM fail when moving objects occupy the scene that violate
the static assumption of bundle adjustment. The failed updates of moving GS
affects the static GS and contaminates the full map over long frames. Although
some efforts have been made by concurrent works to consider moving objects for
GS-SLAM, they simply detect and remove the moving regions from GS rendering
("anti'' dynamic GS-SLAM), where only the static background could benefit from
GS. To this end, we propose the first real-time GS-SLAM, "DynaGSLAM'', that
achieves high-quality online GS rendering, tracking, motion predictions of
moving objects in dynamic scenes while jointly estimating accurate ego motion.
Our DynaGSLAM outperforms SOTA static & "Anti'' dynamic GS-SLAM on three
dynamic real datasets, while keeping speed and memory efficiency in practice.



---

## Advancing 3D Gaussian Splatting Editing with Complementary and Consensus  Information

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-14 | Xuanqi Zhang, Jieun Lee, Chris Joslin, Wonsook Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.11601v1){: .btn .btn-green } |

**Abstract**: We present a novel framework for enhancing the visual fidelity and
consistency of text-guided 3D Gaussian Splatting (3DGS) editing. Existing
editing approaches face two critical challenges: inconsistent geometric
reconstructions across multiple viewpoints, particularly in challenging camera
positions, and ineffective utilization of depth information during image
manipulation, resulting in over-texture artifacts and degraded object
boundaries. To address these limitations, we introduce: 1) A complementary
information mutual learning network that enhances depth map estimation from
3DGS, enabling precise depth-conditioned 3D editing while preserving geometric
structures. 2) A wavelet consensus attention mechanism that effectively aligns
latent codes during the diffusion denoising process, ensuring multi-view
consistency in the edited results. Through extensive experimentation, our
method demonstrates superior performance in rendering quality and view
consistency compared to state-of-the-art approaches. The results validate our
framework as an effective solution for text-guided editing of 3D scenes.

Comments:
- 7 pages, 9 figures

---

## Uncertainty-Aware Normal-Guided Gaussian Splatting for Surface  Reconstruction from Sparse Image Sequences

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-14 | Zhen Tan, Xieyuanli Chen, Jinpu Zhang, Lei Feng, Dewen Hu | cs.CV | [PDF](http://arxiv.org/pdf/2503.11172v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has achieved impressive rendering performance in
novel view synthesis. However, its efficacy diminishes considerably in sparse
image sequences, where inherent data sparsity amplifies geometric uncertainty
during optimization. This often leads to convergence at suboptimal local
minima, resulting in noticeable structural artifacts in the reconstructed
scenes.To mitigate these issues, we propose Uncertainty-aware Normal-Guided
Gaussian Splatting (UNG-GS), a novel framework featuring an explicit Spatial
Uncertainty Field (SUF) to quantify geometric uncertainty within the 3DGS
pipeline. UNG-GS enables high-fidelity rendering and achieves high-precision
reconstruction without relying on priors. Specifically, we first integrate
Gaussian-based probabilistic modeling into the training of 3DGS to optimize the
SUF, providing the model with adaptive error tolerance. An uncertainty-aware
depth rendering strategy is then employed to weight depth contributions based
on the SUF, effectively reducing noise while preserving fine details.
Furthermore, an uncertainty-guided normal refinement method adjusts the
influence of neighboring depth values in normal estimation, promoting robust
results. Extensive experiments demonstrate that UNG-GS significantly
outperforms state-of-the-art methods in both sparse and dense sequences. The
code will be open-source.

Comments:
- 12 pages, 8 figures

---

## EgoSplat: Open-Vocabulary Egocentric Scene Understanding with Language  Embedded 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-14 | Di Li, Jie Feng, Jiahao Chen, Weisheng Dong, Guanbin Li, Guangming Shi, Licheng Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2503.11345v1){: .btn .btn-green } |

**Abstract**: Egocentric scenes exhibit frequent occlusions, varied viewpoints, and dynamic
interactions compared to typical scene understanding tasks. Occlusions and
varied viewpoints can lead to multi-view semantic inconsistencies, while
dynamic objects may act as transient distractors, introducing artifacts into
semantic feature modeling. To address these challenges, we propose EgoSplat, a
language-embedded 3D Gaussian Splatting framework for open-vocabulary
egocentric scene understanding. A multi-view consistent instance feature
aggregation method is designed to leverage the segmentation and tracking
capabilities of SAM2 to selectively aggregate complementary features across
views for each instance, ensuring precise semantic representation of scenes.
Additionally, an instance-aware spatial-temporal transient prediction module is
constructed to improve spatial integrity and temporal continuity in predictions
by incorporating spatial-temporal associations across multi-view instances,
effectively reducing artifacts in the semantic reconstruction of egocentric
scenes. EgoSplat achieves state-of-the-art performance in both localization and
segmentation tasks on two datasets, outperforming existing methods with a 8.2%
improvement in localization accuracy and a 3.7% improvement in segmentation
mIoU on the ADT dataset, and setting a new benchmark in open-vocabulary
egocentric scene understanding. The code will be made publicly available.



---

## MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Yingshuang Zou, Yikang Ding, Chuanrui Zhang, Jiazhe Guo, Bohan Li, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.10604v1){: .btn .btn-green } |

**Abstract**: Recent breakthroughs in radiance fields have significantly advanced 3D scene
reconstruction and novel view synthesis (NVS) in autonomous driving.
Nevertheless, critical limitations persist: reconstruction-based methods
exhibit substantial performance deterioration under significant viewpoint
deviations from training trajectories, while generation-based techniques
struggle with temporal coherence and precise scene controllability. To overcome
these challenges, we present MuDG, an innovative framework that integrates
Multi-modal Diffusion model with Gaussian Splatting (GS) for Urban Scene
Reconstruction. MuDG leverages aggregated LiDAR point clouds with RGB and
geometric priors to condition a multi-modal video diffusion model, synthesizing
photorealistic RGB, depth, and semantic outputs for novel viewpoints. This
synthesis pipeline enables feed-forward NVS without computationally intensive
per-scene optimization, providing comprehensive supervision signals to refine
3DGS representations for rendering robustness enhancement under extreme
viewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDG
outperforms existing methods in both reconstruction and synthesis quality.



---

## AI-assisted 3D Preservation and Reconstruction of Temple Arts

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Naai-Jung Shih | cs.GR | [PDF](http://arxiv.org/pdf/2503.10031v1){: .btn .btn-green } |

**Abstract**: How does AI connect to the past in conservation? What can 17 years old photos
be helpful in a renewed effort of preservation? This research aims to use AI to
connect both in a seamless 3D reconstruction of heritage from imagery data
taken from Gongfan Palace, Yunlin Taiwan. AI-assisted 3D modeling was used to
reconstruct correspondent details across different 3D platforms of 3DGS or NeRF
models generated by Postshot or KIRI Engine. Polygon or point models by Zephyr
were referred to and assessed in two sets. The results also include AI-assist
modeling outcomes in Stable Diffusion and Postshot-based animation. The evolved
documenta-tion and interpretation in AI presents a novel arrangement of working
processes contributed by new structure and management of resources, formats,
and interfaces, as a continuous preservation effort.

Comments:
- 13 pages, 9 figures, 1 table

---

## LHM: Large Animatable Human Reconstruction Model from a Single Image in  Seconds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo | cs.CV | [PDF](http://arxiv.org/pdf/2503.10625v1){: .btn .btn-green } |

**Abstract**: Animatable 3D human reconstruction from a single image is a challenging
problem due to the ambiguity in decoupling geometry, appearance, and
deformation. Recent advances in 3D human reconstruction mainly focus on static
human modeling, and the reliance of using synthetic 3D scans for training
limits their generalization ability. Conversely, optimization-based video
methods achieve higher fidelity but demand controlled capture conditions and
computationally intensive refinement processes. Motivated by the emergence of
large reconstruction models for efficient static reconstruction, we propose LHM
(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars
represented as 3D Gaussian splatting in a feed-forward pass. Our model
leverages a multimodal transformer architecture to effectively encode the human
body positional features and image features with attention mechanism, enabling
detailed preservation of clothing geometry and texture. To further boost the
face identity preservation and fine detail recovery, we propose a head feature
pyramid encoding scheme to aggregate multi-scale features of the head regions.
Extensive experiments demonstrate that our LHM generates plausible animatable
human in seconds without post-processing for face and hands, outperforming
existing methods in both reconstruction accuracy and generalization ability.

Comments:
- Project Page: https://lingtengqiu.github.io/LHM/

---

## ROODI: Reconstructing Occluded Objects with Denoising Inpainters

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Yeonjin Chang, Erqun Dong, Seunghyeon Seo, Nojun Kwak, Kwang Moo Yi | cs.CV | [PDF](http://arxiv.org/pdf/2503.10256v1){: .btn .btn-green } |

**Abstract**: While the quality of novel-view images has improved dramatically with 3D
Gaussian Splatting, extracting specific objects from scenes remains
challenging. Isolating individual 3D Gaussian primitives for each object and
handling occlusions in scenes remain far from being solved. We propose a novel
object extraction method based on two key principles: (1) being object-centric
by pruning irrelevant primitives; and (2) leveraging generative inpainting to
compensate for missing observations caused by occlusions. For pruning, we
analyze the local structure of primitives using K-nearest neighbors, and retain
only relevant ones. For inpainting, we employ an off-the-shelf diffusion-based
inpainter combined with occlusion reasoning, utilizing the 3D representation of
the entire scene. Our findings highlight the crucial synergy between pruning
and inpainting, both of which significantly enhance extraction performance. We
evaluate our method on a standard real-world dataset and introduce a synthetic
dataset for quantitative analysis. Our approach outperforms the
state-of-the-art, demonstrating its effectiveness in object extraction from
complex scenes.

Comments:
- Project page: https://yeonjin-chang.github.io/ROODI/

---

## GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D  and 2D Local Tone Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Jinfeng Liu, Lingtong Kong, Bo Li, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.10143v1){: .btn .btn-green } |

**Abstract**: High dynamic range (HDR) novel view synthesis (NVS) aims to reconstruct HDR
scenes by leveraging multi-view low dynamic range (LDR) images captured at
different exposure levels. Current training paradigms with 3D tone mapping
often result in unstable HDR reconstruction, while training with 2D tone
mapping reduces the model's capacity to fit LDR images. Additionally, the
global tone mapper used in existing methods can impede the learning of both HDR
and LDR representations. To address these challenges, we present GaussHDR,
which unifies 3D and 2D local tone mapping through 3D Gaussian splatting.
Specifically, we design a residual local tone mapper for both 3D and 2D tone
mapping that accepts an additional context feature as input. We then propose
combining the dual LDR rendering results from both 3D and 2D local tone mapping
at the loss level. Finally, recognizing that different scenes may exhibit
varying balances between the dual results, we introduce uncertainty learning
and use the uncertainties for adaptive modulation. Extensive experiments
demonstrate that GaussHDR significantly outperforms state-of-the-art methods in
both synthetic and real-world scenarios.

Comments:
- This paper is accepted by CVPR 2025. Project page is available at
  https://liujf1226.github.io/GaussHDR

---

## RI3D: Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion  Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Avinash Paliwal, Xilong Zhou, Wei Ye, Jinhui Xiong, Rakesh Ranjan, Nima Khademi Kalantari | cs.CV | [PDF](http://arxiv.org/pdf/2503.10860v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose RI3D, a novel 3DGS-based approach that harnesses
the power of diffusion models to reconstruct high-quality novel views given a
sparse set of input images. Our key contribution is separating the view
synthesis process into two tasks of reconstructing visible regions and
hallucinating missing regions, and introducing two personalized diffusion
models, each tailored to one of these tasks. Specifically, one model ('repair')
takes a rendered image as input and predicts the corresponding high-quality
image, which in turn is used as a pseudo ground truth image to constrain the
optimization. The other model ('inpainting') primarily focuses on hallucinating
details in unobserved areas. To integrate these models effectively, we
introduce a two-stage optimization strategy: the first stage reconstructs
visible areas using the repair model, and the second stage reconstructs missing
regions with the inpainting model while ensuring coherence through further
optimization. Moreover, we augment the optimization with a novel Gaussian
initialization method that obtains per-image depth by combining 3D-consistent
and smooth depth with highly detailed relative depth. We demonstrate that by
separating the process into two tasks and addressing them with the repair and
inpainting models, we produce results with detailed textures in both visible
and missing regions that outperform state-of-the-art approaches on a diverse
set of scenes with extremely sparse inputs.

Comments:
- Project page: https://people.engr.tamu.edu/nimak/Papers/RI3D, Code:
  https://github.com/avinashpaliwal/RI3D

---

## 4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large  Language Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, Hanspeter Pfister | cs.CV | [PDF](http://arxiv.org/pdf/2503.10437v1){: .btn .btn-green } |

**Abstract**: Learning 4D language fields to enable time-sensitive, open-ended language
queries in dynamic scenes is essential for many real-world applications. While
LangSplat successfully grounds CLIP features into 3D Gaussian representations,
achieving precision and efficiency in 3D static scenes, it lacks the ability to
handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot
capture temporal dynamics in videos. Real-world environments are inherently
dynamic, with object semantics evolving over time. Building a precise 4D
language field necessitates obtaining pixel-aligned, object-wise video
features, which current vision models struggle to achieve. To address these
challenges, we propose 4D LangSplat, which learns 4D language fields to handle
time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes
efficiently. 4D LangSplat bypasses learning the language field from vision
features and instead learns directly from text generated from object-wise video
captions via Multimodal Large Language Models (MLLMs). Specifically, we propose
a multimodal object-wise video prompting method, consisting of visual and text
prompts that guide MLLMs to generate detailed, temporally consistent,
high-quality captions for objects throughout a video. These captions are
encoded using a Large Language Model into high-quality sentence embeddings,
which then serve as pixel-aligned, object-specific feature supervision,
facilitating open-vocabulary text queries through shared embedding spaces.
Recognizing that objects in 4D scenes exhibit smooth transitions across states,
we further propose a status deformable network to model these continuous
changes over time effectively. Our results across multiple benchmarks
demonstrate that 4D LangSplat attains precise and efficient results for both
time-sensitive and time-agnostic open-vocabulary queries.

Comments:
- CVPR 2025. Project Page: https://4d-langsplat.github.io

---

## GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for  Geometrically Consistent Rendering and Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Jianheng Liu, Yunfei Wan, Bowen Wang, Chunran Zheng, Jiarong Lin, Fu Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2503.10170v1){: .btn .btn-green } |

**Abstract**: Digital twins are fundamental to the development of autonomous driving and
embodied artificial intelligence. However, achieving high-granularity surface
reconstruction and high-fidelity rendering remains a challenge. Gaussian
splatting offers efficient photorealistic rendering but struggles with
geometric inconsistencies due to fragmented primitives and sparse observational
data in robotics applications. Existing regularization methods, which rely on
render-derived constraints, often fail in complex environments. Moreover,
effectively integrating sparse LiDAR data with Gaussian splatting remains
challenging. We propose a unified LiDAR-visual system that synergizes Gaussian
splatting with a neural signed distance field. The accurate LiDAR point clouds
enable a trained neural signed distance field to offer a manifold geometry
field, This motivates us to offer an SDF-based Gaussian initialization for
physically grounded primitive placement and a comprehensive geometric
regularization for geometrically consistent rendering and reconstruction.
Experiments demonstrate superior reconstruction accuracy and rendering quality
across diverse trajectories. To benefit the community, the codes will be
released at https://github.com/hku-mars/GS-SDF.



---

## 3D Student Splatting and Scooping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Jialin Zhu, Jiangbei Yue, Feixiang He, He Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.10148v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel
view synthesis, and has spiked a new wave of research in neural rendering and
related applications. As 3DGS is becoming a foundational component of many
models, any improvement on 3DGS itself can bring huge benefits. To this end, we
aim to improve the fundamental paradigm and formulation of 3DGS. We argue that
as an unnormalized mixture model, it needs to be neither Gaussians nor
splatting. We subsequently propose a new mixture model consisting of flexible
Student's t distributions, with both positive (splatting) and negative
(scooping) densities. We name our model Student Splatting and Scooping, or SSS.
When providing better expressivity, SSS also poses new challenges in learning.
Therefore, we also propose a new principled sampling approach for optimization.
Through exhaustive evaluation and comparison, across multiple datasets,
settings, and metrics, we demonstrate that SSS outperforms existing methods in
terms of quality and parameter efficiency, e.g. achieving matching or better
quality with similar numbers of components, and obtaining comparable results
while reducing the component number by as much as 82%.



---

## VicaSplat: A Single Run is All You Need for 3D Gaussian Splatting and  Camera Estimation from Unposed Video Frames

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Zhiqi Li, Chengrui Dong, Yiming Chen, Zhangchi Huang, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.10286v1){: .btn .btn-green } |

**Abstract**: We present VicaSplat, a novel framework for joint 3D Gaussians reconstruction
and camera pose estimation from a sequence of unposed video frames, which is a
critical yet underexplored task in real-world 3D applications. The core of our
method lies in a novel transformer-based network architecture. In particular,
our model starts with an image encoder that maps each image to a list of visual
tokens. All visual tokens are concatenated with additional inserted learnable
camera tokens. The obtained tokens then fully communicate with each other
within a tailored transformer decoder. The camera tokens causally aggregate
features from visual tokens of different views, and further modulate them
frame-wisely to inject view-dependent features. 3D Gaussian splats and camera
pose parameters can then be estimated via different prediction heads.
Experiments show that VicaSplat surpasses baseline methods for multi-view
inputs, and achieves comparable performance to prior two-view approaches.
Remarkably, VicaSplat also demonstrates exceptional cross-dataset
generalization capability on the ScanNet benchmark, achieving superior
performance without any fine-tuning. Project page:
https://lizhiqi49.github.io/VicaSplat.



---

## Flow-NeRF: Joint Learning of Geometry, Poses, and Dense Flow within  Unified Neural Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-13 | Xunzhi Zheng, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.10464v1){: .btn .btn-green } |

**Abstract**: Learning accurate scene reconstruction without pose priors in neural radiance
fields is challenging due to inherent geometric ambiguity. Recent development
either relies on correspondence priors for regularization or uses off-the-shelf
flow estimators to derive analytical poses. However, the potential for jointly
learning scene geometry, camera poses, and dense flow within a unified neural
representation remains largely unexplored. In this paper, we present Flow-NeRF,
a unified framework that simultaneously optimizes scene geometry, camera poses,
and dense optical flow all on-the-fly. To enable the learning of dense flow
within the neural radiance field, we design and build a bijective mapping for
flow estimation, conditioned on pose. To make the scene reconstruction benefit
from the flow estimation, we develop an effective feature enhancement mechanism
to pass canonical space features to world space representations, significantly
enhancing scene geometry. We validate our model across four important tasks,
i.e., novel view synthesis, depth estimation, camera pose prediction, and dense
optical flow estimation, using several datasets. Our approach surpasses
previous methods in almost all metrics for novel-view view synthesis and depth
estimation and yields both qualitatively sound and quantitatively accurate
novel-view flow. Our project page is https://zhengxunzhi.github.io/flownerf/.



---

## Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting  with Progressive Self-Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Jiatong Xia, Lingqiao Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.09396v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in
synthesizing novel views after training on a given set of viewpoints. However,
its rendering quality deteriorates when the synthesized view deviates
significantly from the training views. This decline occurs due to (1) the
model's difficulty in generalizing to out-of-distribution scenarios and (2)
challenges in interpolating fine details caused by substantial resolution
changes and occlusions. A notable case of this limitation is close-up view
generation--producing views that are significantly closer to the object than
those in the training set. To tackle this issue, we propose a novel approach
for close-up view generation based by progressively training the 3DGS model
with self-generated data. Our solution is based on three key ideas. First, we
leverage the See3D model, a recently introduced 3D-aware generative model, to
enhance the details of rendered views. Second, we propose a strategy to
progressively expand the ``trust regions'' of the 3DGS model and update a set
of reference views for See3D. Finally, we introduce a fine-tuning strategy to
carefully update the 3DGS model with training data generated from the above
schemes. We further define metrics for close-up views evaluation to facilitate
better research on this problem. By conducting evaluations on specifically
selected scenarios for close-up views, our proposed approach demonstrates a
clear advantage over competitive solutions.



---

## SDD-4DGS: Static-Dynamic Aware Decoupling in Gaussian Splatting for 4D  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Dai Sun, Huhao Guan, Kun Zhang, Xike Xie, S. Kevin Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.09332v1){: .btn .btn-green } |

**Abstract**: Dynamic and static components in scenes often exhibit distinct properties,
yet most 4D reconstruction methods treat them indiscriminately, leading to
suboptimal performance in both cases. This work introduces SDD-4DGS, the first
framework for static-dynamic decoupled 4D scene reconstruction based on
Gaussian Splatting. Our approach is built upon a novel probabilistic dynamic
perception coefficient that is naturally integrated into the Gaussian
reconstruction pipeline, enabling adaptive separation of static and dynamic
components. With carefully designed implementation strategies to realize this
theoretical framework, our method effectively facilitates explicit learning of
motion patterns for dynamic elements while maintaining geometric stability for
static structures. Extensive experiments on five benchmark datasets demonstrate
that SDD-4DGS consistently outperforms state-of-the-art methods in
reconstruction fidelity, with enhanced detail restoration for static structures
and precise modeling of dynamic motions. The code will be released.



---

## Motion Blender Gaussian Splatting for Dynamic Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias | cs.CV | [PDF](http://arxiv.org/pdf/2503.09040v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has emerged as a powerful tool for high-fidelity
reconstruction of dynamic scenes. However, existing methods primarily rely on
implicit motion representations, such as encoding motions into neural networks
or per-Gaussian parameters, which makes it difficult to further manipulate the
reconstructed motions. This lack of explicit controllability limits existing
methods to replaying recorded motions only, which hinders a wider application.
To address this, we propose Motion Blender Gaussian Splatting (MB-GS), a novel
framework that uses motion graph as an explicit and sparse motion
representation. The motion of graph links is propagated to individual Gaussians
via dual quaternion skinning, with learnable weight painting functions
determining the influence of each link. The motion graphs and 3D Gaussians are
jointly optimized from input videos via differentiable rendering. Experiments
show that MB-GS achieves state-of-the-art performance on the iPhone dataset
while being competitive on HyperNeRF. Additionally, we demonstrate the
application potential of our method in generating novel object motions and
robot demonstrations through motion editing. Video demonstrations can be found
at https://mlzxy.github.io/mbgs.



---

## Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and  Physics-Based Simulation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Máté Tóth, Péter Kovács, Zoltán Bendefy, Zoltán Hortsin, Balázs Teréki, Tamás Matuszka | cs.GR | [PDF](http://arxiv.org/pdf/2503.09464v1){: .btn .btn-green } |

**Abstract**: Neural reconstruction models for autonomous driving simulation have made
significant strides in recent years, with dynamic models becoming increasingly
prevalent. However, these models are typically limited to handling in-domain
objects closely following their original trajectories. We introduce a hybrid
approach that combines the strengths of neural reconstruction with
physics-based rendering. This method enables the virtual placement of
traditional mesh-based dynamic agents at arbitrary locations, adjustments to
environmental conditions, and rendering from novel camera viewpoints. Our
approach significantly enhances novel view synthesis quality -- especially for
road surfaces and lane markings -- while maintaining interactive frame rates
through our novel training method, NeRF2GS. This technique leverages the
superior generalization capabilities of NeRF-based methods and the real-time
rendering speed of 3D Gaussian Splatting (3DGS). We achieve this by training a
customized NeRF model on the original images with depth regularization derived
from a noisy LiDAR point cloud, then using it as a teacher model for 3DGS
training. This process ensures accurate depth, surface normals, and camera
appearance modeling as supervision. With our block-based training
parallelization, the method can handle large-scale reconstructions (greater
than or equal to 100,000 square meters) and predict segmentation masks, surface
normals, and depth maps. During simulation, it supports a rasterization-based
rendering backend with depth-based composition and multiple camera models for
real-time camera simulation, as well as a ray-traced backend for precise LiDAR
simulation.



---

## GASPACHO: Gaussian Splatting for Controllable Humans and Objects

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Aymen Mir, Arthur Moreau, Helisa Dhamo, Zhensong Zhang, Eduardo Pérez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2503.09342v1){: .btn .btn-green } |

**Abstract**: We present GASPACHO: a method for generating photorealistic controllable
renderings of human-object interactions. Given a set of multi-view RGB images
of human-object interactions, our method reconstructs animatable templates of
the human and object as separate sets of Gaussians simultaneously. Different
from existing work, which focuses on human reconstruction and ignores objects
as background, our method explicitly reconstructs both humans and objects,
thereby allowing for controllable renderings of novel human object interactions
in different poses from novel-camera viewpoints. During reconstruction, we
constrain the Gaussians that generate rendered images to be a linear function
of a set of canonical Gaussians. By simply changing the parameters of the
linear deformation functions after training, our method can generate renderings
of novel human-object interaction in novel poses from novel camera viewpoints.
We learn the 3D Gaussian properties of the canonical Gaussians on the
underlying 2D manifold of the canonical human and object templates. This in
turns requires a canonical object template with a fixed UV unwrapping. To
define such an object template, we use a feature based representation to track
the object across the multi-view sequence. We further propose an occlusion
aware photometric loss that allows for reconstructions under significant
occlusions. Several experiments on two human-object datasets - BEHAVE and
DNA-Rendering - demonstrate that our method allows for high-quality
reconstruction of human and object templates under significant occlusion and
the synthesis of controllable renderings of novel human-object interactions in
novel human poses from novel camera views.



---

## Online Language Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Saimouli Katragadda, Cho-Ying Wu, Yuliang Guo, Xinyu Huang, Guoquan Huang, Liu Ren | cs.AI | [PDF](http://arxiv.org/pdf/2503.09447v1){: .btn .btn-green } |

**Abstract**: To enable AI agents to interact seamlessly with both humans and 3D
environments, they must not only perceive the 3D world accurately but also
align human language with 3D spatial representations. While prior work has made
significant progress by integrating language features into geometrically
detailed 3D scene representations using 3D Gaussian Splatting (GS), these
approaches rely on computationally intensive offline preprocessing of language
features for each input image, limiting adaptability to new environments. In
this work, we introduce Online Language Splatting, the first framework to
achieve online, near real-time, open-vocabulary language mapping within a
3DGS-SLAM system without requiring pre-generated language features. The key
challenge lies in efficiently fusing high-dimensional language features into 3D
representations while balancing the computation speed, memory usage, rendering
quality and open-vocabulary capability. To this end, we innovatively design:
(1) a high-resolution CLIP embedding module capable of generating detailed
language feature maps in 18ms per frame, (2) a two-stage online auto-encoder
that compresses 768-dimensional CLIP features to 15 dimensions while preserving
open-vocabulary capabilities, and (3) a color-language disentangled
optimization approach to improve rendering quality. Experimental results show
that our online method not only surpasses the state-of-the-art offline methods
in accuracy but also achieves more than 40x efficiency boost, demonstrating the
potential for dynamic and interactive AI applications.



---

## Physics-Aware Human-Object Rendering from Sparse Views via 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-12 | Weiquan Wang, Jun Xiao, Yueting Zhuang, Long Chen | cs.GR | [PDF](http://arxiv.org/pdf/2503.09640v1){: .btn .btn-green } |

**Abstract**: Rendering realistic human-object interactions (HOIs) from sparse-view inputs
is challenging due to occlusions and incomplete observations, yet crucial for
various real-world applications. Existing methods always struggle with either
low rendering qualities (\eg, visual fidelity and physically plausible HOIs) or
high computational costs. To address these limitations, we propose HOGS
(Human-Object Rendering via 3D Gaussian Splatting), a novel framework for
efficient and physically plausible HOI rendering from sparse views.
Specifically, HOGS combines 3D Gaussian Splatting with a physics-aware
optimization process. It incorporates a Human Pose Refinement module for
accurate pose estimation and a Sparse-View Human-Object Contact Prediction
module for efficient contact region identification. This combination enables
coherent joint rendering of human and object Gaussians while enforcing
physically plausible interactions. Extensive experiments on the HODome dataset
demonstrate that HOGS achieves superior rendering quality, efficiency, and
physical plausibility compared to existing methods. We further show its
extensibility to hand-object grasp rendering tasks, presenting its broader
applicability to articulated object interactions.



---

## Mitigating Ambiguities in 3D Classification with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Ruiqi Zhang, Hao Zhu, Jingyi Zhao, Qi Zhang, Xun Cao, Zhan Ma | cs.CV | [PDF](http://arxiv.org/pdf/2503.08352v1){: .btn .btn-green } |

**Abstract**: 3D classification with point cloud input is a fundamental problem in 3D
vision. However, due to the discrete nature and the insufficient material
description of point cloud representations, there are ambiguities in
distinguishing wire-like and flat surfaces, as well as transparent or
reflective objects. To address these issues, we propose Gaussian Splatting (GS)
point cloud-based 3D classification. We find that the scale and rotation
coefficients in the GS point cloud help characterize surface types.
Specifically, wire-like surfaces consist of multiple slender Gaussian
ellipsoids, while flat surfaces are composed of a few flat Gaussian ellipsoids.
Additionally, the opacity in the GS point cloud represents the transparency
characteristics of objects. As a result, ambiguities in point cloud-based 3D
classification can be mitigated utilizing GS point cloud as input. To verify
the effectiveness of GS point cloud input, we construct the first real-world GS
point cloud dataset in the community, which includes 20 categories with 200
objects in each category. Experiments not only validate the superiority of GS
point cloud input, especially in distinguishing ambiguous objects, but also
demonstrate the generalization ability across different classification methods.

Comments:
- Accepted by CVPR 2025

---

## ELECTRA: A Symmetry-breaking Cartesian Network for Charge Density  Prediction with Floating Orbitals

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Jonas Elsborg, Luca Thiede, Alán Aspuru-Guzik, Tejs Vegge, Arghya Bhowmik | cs.LG | [PDF](http://arxiv.org/pdf/2503.08305v1){: .btn .btn-green } |

**Abstract**: We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an
equivariant model for predicting electronic charge densities using "floating"
orbitals. Floating orbitals are a long-standing idea in the quantum chemistry
community that promises more compact and accurate representations by placing
orbitals freely in space, as opposed to centering all orbitals at the position
of atoms. Finding ideal placements of these orbitals requires extensive domain
knowledge though, which thus far has prevented widespread adoption. We solve
this in a data-driven manner by training a Cartesian tensor network to predict
orbital positions along with orbital coefficients. This is made possible
through a symmetry-breaking mechanism that is used to learn position
displacements with lower symmetry than the input molecule while preserving the
rotation equivariance of the charge density itself. Inspired by recent
successes of Gaussian Splatting in representing densities in space, we are
using Gaussians as our orbitals and predict their weights and covariance
matrices. Our method achieves a state-of-the-art balance between computational
efficiency and predictive accuracy on established benchmarks.

Comments:
- 8 pages, 3 figures, 1 table

---

## GigaSLAM: Large-Scale Monocular SLAM with Hierachical Gaussian Splats

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Kai Deng, Jian Yang, Shenlong Wang, Jin Xie | cs.RO | [PDF](http://arxiv.org/pdf/2503.08071v1){: .btn .btn-green } |

**Abstract**: Tracking and mapping in large-scale, unbounded outdoor environments using
only monocular RGB input presents substantial challenges for existing SLAM
systems. Traditional Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS) SLAM methods are typically limited to small, bounded indoor settings. To
overcome these challenges, we introduce GigaSLAM, the first NeRF/3DGS-based
SLAM framework for kilometer-scale outdoor environments, as demonstrated on the
KITTI and KITTI 360 datasets. Our approach employs a hierarchical sparse voxel
map representation, where Gaussians are decoded by neural networks at multiple
levels of detail. This design enables efficient, scalable mapping and
high-fidelity viewpoint rendering across expansive, unbounded scenes. For
front-end tracking, GigaSLAM utilizes a metric depth model combined with
epipolar geometry and PnP algorithms to accurately estimate poses, while
incorporating a Bag-of-Words-based loop closure mechanism to maintain robust
alignment over long trajectories. Consequently, GigaSLAM delivers
high-precision tracking and visually faithful rendering on urban outdoor
benchmarks, establishing a robust SLAM solution for large-scale, long-term
scenarios, and significantly extending the applicability of Gaussian Splatting
SLAM systems to unbounded outdoor environments.



---

## TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction  via Spatio-Temporal Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Fengyi Zhang, Huitong Yang, Zheng Zhang, Zi Huang, Yadan Luo | cs.CV | [PDF](http://arxiv.org/pdf/2503.08485v1){: .btn .btn-green } |

**Abstract**: Self-supervised 3D occupancy prediction offers a promising solution for
understanding complex driving scenes without requiring costly 3D annotations.
However, training dense voxel decoders to capture fine-grained geometry and
semantics can demand hundreds of GPU hours, and such models often fail to adapt
to varying voxel resolutions or new classes without extensive retraining. To
overcome these limitations, we propose a practical and flexible test-time
occupancy prediction framework termed TT-GaussOcc. Our approach incrementally
optimizes time-aware 3D Gaussians instantiated from raw sensor streams at
runtime, enabling voxelization at arbitrary user-specified resolution.
Specifically, TT-GaussOcc operates in a "lift-move-voxel" symphony: we first
"lift" surrounding-view semantics obtained from 2D vision foundation models
(VLMs) to instantiate Gaussians at non-empty 3D space; Next, we "move" dynamic
Gaussians from previous frames along estimated Gaussian scene flow to complete
appearance and eliminate trailing artifacts of fast-moving objects, while
accumulating static Gaussians to enforce temporal consistency; Finally, we
mitigate inherent noises in semantic predictions and scene flow vectors by
periodically smoothing neighboring Gaussians during optimization, using
proposed trilateral RBF kernels that jointly consider color, semantic, and
spatial affinities. The historical static and current dynamic Gaussians are
then combined and voxelized to generate occupancy prediction. Extensive
experiments on Occ3D and nuCraft with varying voxel resolutions demonstrate
that TT-GaussOcc surpasses self-supervised baselines by 46% on mIoU without any
offline training, and supports finer voxel resolutions at 2.6 FPS inference
speed.



---

## NeRF-VIO: Map-Based Visual-Inertial Odometry with Initialization  Leveraging Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Yanyu Zhang, Dongming Wang, Jie Xu, Mengyuan Liu, Pengxiang Zhu, Wei Ren | cs.CV | [PDF](http://arxiv.org/pdf/2503.07952v1){: .btn .btn-green } |

**Abstract**: A prior map serves as a foundational reference for localization in
context-aware applications such as augmented reality (AR). Providing valuable
contextual information about the environment, the prior map is a vital tool for
mitigating drift. In this paper, we propose a map-based visual-inertial
localization algorithm (NeRF-VIO) with initialization using neural radiance
fields (NeRF). Our algorithm utilizes a multilayer perceptron model and
redefines the loss function as the geodesic distance on \(SE(3)\), ensuring the
invariance of the initialization model under a frame change within
\(\mathfrak{se}(3)\). The evaluation demonstrates that our model outperforms
existing NeRF-based initialization solution in both accuracy and efficiency. By
integrating a two-stage update mechanism within a multi-state constraint Kalman
filter (MSCKF) framework, the state of NeRF-VIO is constrained by both captured
images from an onboard camera and rendered images from a pre-trained NeRF
model. The proposed algorithm is validated using a real-world AR dataset, the
results indicate that our two-stage update pipeline outperforms MSCKF across
all data sequences.



---

## HRAvatar: High-Quality and Relightable Gaussian Head Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Kangjie Chen, Minghan Qin, Yu Li, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.08224v1){: .btn .btn-green } |

**Abstract**: Reconstructing animatable and high-quality 3D head avatars from monocular
videos, especially with realistic relighting, is a valuable task. However, the
limited information from single-view input, combined with the complex head
poses and facial movements, makes this challenging. Previous methods achieve
real-time performance by combining 3D Gaussian Splatting with a parametric head
model, but the resulting head quality suffers from inaccurate face tracking and
limited expressiveness of the deformation model. These methods also fail to
produce realistic effects under novel lighting conditions. To address these
issues, we propose HRAvatar, a 3DGS-based method that reconstructs
high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors
through end-to-end optimization and better captures individual facial
deformations using learnable blendshapes and learnable linear blend skinning.
Additionally, it decomposes head appearance into several physical properties
and incorporates physically-based shading to account for environmental
lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs
superior-quality heads but also achieves realistic visual effects under varying
lighting conditions.

Comments:
- Project page: https://eastbeanzhang.github.io/HRAvatar

---

## MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Chenfeng Hou, Qi Xun Yeo, Mengqi Guo, Yongxin Su, Yanyan Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.08093v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for its
high-quality rendering capabilities, ultra-fast training, and inference speeds.
However, when we apply 3DGS to surface reconstruction tasks, especially in
environments with dynamic objects and distractors, the method suffers from
floating artifacts and color errors due to inconsistency from different
viewpoints. To address this challenge, we propose Multi-View Consistency
Gaussian Splatting for the domain of Robust Surface Reconstruction
(\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a
{heuristics-guided distractor masking} strategy for robust surface
reconstruction in non-static environments. Compared to existing methods that
rely on MLPs for distractor segmentation strategies, our approach separates
distractors from static scene elements by comparing multi-view feature
consistency, allowing us to obtain precise distractor masks early in training.
Furthermore, we introduce a pruning measure based on multi-view contributions
to reset transmittance, effectively reducing floating artifacts. Finally, a
multi-view consistency loss is applied to achieve high-quality performance in
surface reconstruction tasks. Experimental results demonstrate that MVGSR
achieves competitive geometric accuracy and rendering fidelity compared to the
state-of-the-art surface reconstruction algorithms. More information is
available on our project page (https://mvgsr.github.io).

Comments:
- project page https://mvgsr.github.io

---

## ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated  Objects using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Junfu Guo, Yu Xin, Gaoyi Liu, Kai Xu, Ligang Liu, Ruizhen Hu | cs.CV | [PDF](http://arxiv.org/pdf/2503.08135v1){: .btn .btn-green } |

**Abstract**: We tackle the challenge of concurrent reconstruction at the part level with
the RGB appearance and estimation of motion parameters for building digital
twins of articulated objects using the 3D Gaussian Splatting (3D-GS) method.
With two distinct sets of multi-view imagery, each depicting an object in
separate static articulation configurations, we reconstruct the articulated
object in 3D Gaussian representations with both appearance and geometry
information at the same time. Our approach decoupled multiple highly
interdependent parameters through a multi-step optimization process, thereby
achieving a stable optimization procedure and high-quality outcomes. We
introduce ArticulatedGS, a self-supervised, comprehensive framework that
autonomously learns to model shapes and appearances at the part level and
synchronizes the optimization of motion parameters, all without reliance on 3D
supervision, motion cues, or semantic labels. Our experimental results
demonstrate that, among comparable methodologies, our approach has achieved
optimal outcomes in terms of part segmentation accuracy, motion estimation
accuracy, and visual quality.



---

## S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Guangting Zheng, Jiajun Deng, Xiaomeng Chu, Yu Yuan, Houqiang Li, Yanyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.08217v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has reshaped the field of
photorealistic 3D reconstruction, achieving impressive rendering quality and
speed. However, when applied to large-scale street scenes, existing methods
suffer from rapidly escalating per-viewpoint reconstruction costs as scene size
increases, leading to significant computational overhead. After revisiting the
conventional pipeline, we identify three key factors accounting for this issue:
unnecessary local-to-global transformations, excessive 3D-to-2D projections,
and inefficient rendering of distant content. To address these challenges, we
propose S3R-GS, a 3DGS framework that Streamlines the pipeline for large-scale
Street Scene Reconstruction, effectively mitigating these limitations.
Moreover, most existing street 3DGS methods rely on ground-truth 3D bounding
boxes to separate dynamic and static components, but 3D bounding boxes are
difficult to obtain, limiting real-world applicability. To address this, we
propose an alternative solution with 2D boxes, which are easier to annotate or
can be predicted by off-the-shelf vision foundation models. Such designs
together make S3R-GS readily adapt to large, in-the-wild scenarios. Extensive
experiments demonstrate that S3R-GS enhances rendering quality and
significantly accelerates reconstruction. Remarkably, when applied to videos
from the challenging Argoverse2 dataset, it achieves state-of-the-art PSNR and
SSIM, reducing reconstruction time to below 50%--and even 20%--of competing
methods.



---

## 7DGS: Unified Spatial-Temporal-Angular Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2503.07946v1){: .btn .btn-green } |

**Abstract**: Real-time rendering of dynamic scenes with view-dependent effects remains a
fundamental challenge in computer graphics. While recent advances in Gaussian
Splatting have shown promising results separately handling dynamic scenes
(4DGS) and view-dependent effects (6DGS), no existing method unifies these
capabilities while maintaining real-time performance. We present 7D Gaussian
Splatting (7DGS), a unified framework representing scene elements as
seven-dimensional Gaussians spanning position (3D), time (1D), and viewing
direction (3D). Our key contribution is an efficient conditional slicing
mechanism that transforms 7D Gaussians into view- and time-conditioned 3D
Gaussians, maintaining compatibility with existing 3D Gaussian Splatting
pipelines while enabling joint optimization. Experiments demonstrate that 7DGS
outperforms prior methods by up to 7.36 dB in PSNR while achieving real-time
rendering (401 FPS) on challenging dynamic scenes with complex view-dependent
effects. The project page is: https://gaozhongpai.github.io/7dgs/.



---

## PCGS: Progressive Compression of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Yihang Chen, Mengyao Li, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2503.08511v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) achieves impressive rendering fidelity and speed
for novel view synthesis. However, its substantial data size poses a
significant challenge for practical applications. While many compression
techniques have been proposed, they fail to efficiently utilize existing
bitstreams in on-demand applications due to their lack of progressivity,
leading to a waste of resource. To address this issue, we propose PCGS
(Progressive Compression of 3D Gaussian Splatting), which adaptively controls
both the quantity and quality of Gaussians (or anchors) to enable effective
progressivity for on-demand applications. Specifically, for quantity, we
introduce a progressive masking strategy that incrementally incorporates new
anchors while refining existing ones to enhance fidelity. For quality, we
propose a progressive quantization approach that gradually reduces quantization
step sizes to achieve finer modeling of Gaussian attributes. Furthermore, to
compact the incremental bitstreams, we leverage existing quantization results
to refine probability prediction, improving entropy coding efficiency across
progressive levels. Overall, PCGS achieves progressivity while maintaining
compression performance comparable to SoTA non-progressive methods. Code
available at: github.com/YihangChen-ee/PCGS.

Comments:
- Project Page: https://yihangchen-ee.github.io/project_pcgs/ Code:
  https://github.com/YihangChen-ee/PCGS

---

## Dynamic Scene Reconstruction: Recent Advance in Real-time Rendering and  Streaming

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Jiaxuan Zhu, Hao Tang | cs.GR | [PDF](http://arxiv.org/pdf/2503.08166v1){: .btn .btn-green } |

**Abstract**: Representing and rendering dynamic scenes from 2D images is a fundamental yet
challenging problem in computer vision and graphics. This survey provides a
comprehensive review of the evolution and advancements in dynamic scene
representation and rendering, with a particular emphasis on recent progress in
Neural Radiance Fields based and 3D Gaussian Splatting based reconstruction
methods. We systematically summarize existing approaches, categorize them
according to their core principles, compile relevant datasets, compare the
performance of various methods on these benchmarks, and explore the challenges
and future research directions in this rapidly evolving field. In total, we
review over 170 relevant papers, offering a broad perspective on the state of
the art in this domain.

Comments:
- 20 pages, 6 figures

---

## FPGS: Feed-Forward Semantic-aware Photorealistic Style Transfer of  Large-Scale Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | GeonU Kim, Kim Youwang, Lee Hyoseok, Tae-Hyun Oh | cs.GR | [PDF](http://arxiv.org/pdf/2503.09635v1){: .btn .btn-green } |

**Abstract**: We present FPGS, a feed-forward photorealistic style transfer method of
large-scale radiance fields represented by Gaussian Splatting. FPGS, stylizes
large-scale 3D scenes with arbitrary, multiple style reference images without
additional optimization while preserving multi-view consistency and real-time
rendering speed of 3D Gaussians. Prior arts required tedious per-style
optimization or time-consuming per-scene training stage and were limited to
small-scale 3D scenes. FPGS efficiently stylizes large-scale 3D scenes by
introducing a style-decomposed 3D feature field, which inherits AdaIN's
feed-forward stylization machinery, supporting arbitrary style reference
images. Furthermore, FPGS supports multi-reference stylization with the
semantic correspondence matching and local AdaIN, which adds diverse user
control for 3D scene styles. FPGS also preserves multi-view consistency by
applying semantic matching and style transfer processes directly onto queried
features in 3D space. In experiments, we demonstrate that FPGS achieves
favorable photorealistic quality scene stylization for large-scale static and
dynamic 3D scenes with diverse reference images. Project page:
https://kim-geonu.github.io/FPGS/

Comments:
- Project page: https://kim-geonu.github.io/FPGS/. arXiv admin note:
  substantial text overlap with arXiv:2401.05516

---

## Uni-Gaussians: Unifying Camera and Lidar Simulation with Gaussians for  Dynamic Driving Scenarios

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Zikang Yuan, Yuechuan Pu, Hongcheng Luo, Fengtian Lang, Cheng Chi, Teng Li, Yingying Shen, Haiyang Sun, Bing Wang, Xin Yang | cs.RO | [PDF](http://arxiv.org/pdf/2503.08317v1){: .btn .btn-green } |

**Abstract**: Ensuring the safety of autonomous vehicles necessitates comprehensive
simulation of multi-sensor data, encompassing inputs from both cameras and
LiDAR sensors, across various dynamic driving scenarios. Neural rendering
techniques, which utilize collected raw sensor data to simulate these dynamic
environments, have emerged as a leading methodology. While NeRF-based
approaches can uniformly represent scenes for rendering data from both camera
and LiDAR, they are hindered by slow rendering speeds due to dense sampling.
Conversely, Gaussian Splatting-based methods employ Gaussian primitives for
scene representation and achieve rapid rendering through rasterization.
However, these rasterization-based techniques struggle to accurately model
non-linear optical sensors. This limitation restricts their applicability to
sensors beyond pinhole cameras. To address these challenges and enable unified
representation of dynamic driving scenarios using Gaussian primitives, this
study proposes a novel hybrid approach. Our method utilizes rasterization for
rendering image data while employing Gaussian ray-tracing for LiDAR data
rendering. Experimental results on public datasets demonstrate that our
approach outperforms current state-of-the-art methods. This work presents a
unified and efficient solution for realistic simulation of camera and LiDAR
data in autonomous driving scenarios using Gaussian primitives, offering
significant advancements in both rendering quality and computational
efficiency.

Comments:
- 10 pages

---

## GAS-NeRF: Geometry-Aware Stylization of Dynamic Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-11 | Nhat Phuong Anh Vu, Abhishek Saroha, Or Litany, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2503.08483v1){: .btn .btn-green } |

**Abstract**: Current 3D stylization techniques primarily focus on static scenes, while our
world is inherently dynamic, filled with moving objects and changing
environments. Existing style transfer methods primarily target appearance --
such as color and texture transformation -- but often neglect the geometric
characteristics of the style image, which are crucial for achieving a complete
and coherent stylization effect. To overcome these shortcomings, we propose
GAS-NeRF, a novel approach for joint appearance and geometry stylization in
dynamic Radiance Fields. Our method leverages depth maps to extract and
transfer geometric details into the radiance field, followed by appearance
transfer. Experimental results on synthetic and real-world datasets demonstrate
that our approach significantly enhances the stylization quality while
maintaining temporal coherence in dynamic scenes.



---

## Frequency-Aware Density Control via Reparameterization for High-Quality  Rendering of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Zhaojie Zeng, Yuesong Wang, Lili Ju, Tao Guan | cs.CV | [PDF](http://arxiv.org/pdf/2503.07000v1){: .btn .btn-green } |

**Abstract**: By adaptively controlling the density and generating more Gaussians in
regions with high-frequency information, 3D Gaussian Splatting (3DGS) can
better represent scene details. From the signal processing perspective,
representing details usually needs more Gaussians with relatively smaller
scales. However, 3DGS currently lacks an explicit constraint linking the
density and scale of 3D Gaussians across the domain, leading to 3DGS using
improper-scale Gaussians to express frequency information, resulting in the
loss of accuracy. In this paper, we propose to establish a direct relation
between density and scale through the reparameterization of the scaling
parameters and ensure the consistency between them via explicit constraints
(i.e., density responds well to changes in frequency). Furthermore, we develop
a frequency-aware density control strategy, consisting of densification and
deletion, to improve representation quality with fewer Gaussians. A dynamic
threshold encourages densification in high-frequency regions, while a
scale-based filter deletes Gaussians with improper scale. Experimental results
on various datasets demonstrate that our method outperforms existing
state-of-the-art methods quantitatively and qualitatively.

Comments:
- Accepted to AAAI2025

---

## DirectTriGS: Triplane-based Gaussian Splatting Field Representation for  3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Xiaoliang Ju, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2503.06900v1){: .btn .btn-green } |

**Abstract**: We present DirectTriGS, a novel framework designed for 3D object generation
with Gaussian Splatting (GS). GS-based rendering for 3D content has gained
considerable attention recently. However, there has been limited exploration in
directly generating 3D Gaussians compared to traditional generative modeling
approaches. The main challenge lies in the complex data structure of GS
represented by discrete point clouds with multiple channels. To overcome this
challenge, we propose employing the triplane representation, which allows us to
represent Gaussian Splatting as an image-like continuous field. This
representation effectively encodes both the geometry and texture information,
enabling smooth transformation back to Gaussian point clouds and rendering into
images by a TriRenderer, with only 2D supervisions. The proposed TriRenderer is
fully differentiable, so that the rendering loss can supervise both texture and
geometry encoding. Furthermore, the triplane representation can be compressed
using a Variational Autoencoder (VAE), which can subsequently be utilized in
latent diffusion to generate 3D objects. The experiments demonstrate that the
proposed generation framework can produce high-quality 3D object geometry and
rendering results in the text-to-3D task.

Comments:
- Accepted by CVPR 2025

---

## Multi-Modal 3D Mesh Reconstruction from Images and Text

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Melvin Reka, Tessa Pulli, Markus Vincze | cs.CV | [PDF](http://arxiv.org/pdf/2503.07190v1){: .btn .btn-green } |

**Abstract**: 6D object pose estimation for unseen objects is essential in robotics but
traditionally relies on trained models that require large datasets, high
computational costs, and struggle to generalize. Zero-shot approaches eliminate
the need for training but depend on pre-existing 3D object models, which are
often impractical to obtain. To address this, we propose a language-guided
few-shot 3D reconstruction method, reconstructing a 3D mesh from few input
images. In the proposed pipeline, receives a set of input images and a language
query. A combination of GroundingDINO and Segment Anything Model outputs
segmented masks from which a sparse point cloud is reconstructed with VGGSfM.
Subsequently, the mesh is reconstructed with the Gaussian Splatting method
SuGAR. In a final cleaning step, artifacts are removed, resulting in the final
3D mesh of the queried object. We evaluate the method in terms of accuracy and
quality of the geometry and texture. Furthermore, we study the impact of
imaging conditions such as viewing angle, number of input images, and image
overlap on 3D object reconstruction quality, efficiency, and computational
scalability.

Comments:
- under review

---

## SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Jiahui Zhang, Fangneng Zhan, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.07476v1){: .btn .btn-green } |

**Abstract**: Anchor-based 3D Gaussian splatting (3D-GS) exploits anchor features in 3D
Gaussian prediction, which has achieved impressive 3D rendering quality with
reduced Gaussian redundancy. On the other hand, it often encounters the dilemma
among anchor features, model size, and rendering quality - large anchor
features lead to large 3D models and high-quality rendering whereas reducing
anchor features degrades Gaussian attribute prediction which leads to clear
artifacts in the rendered textures and geometries. We design SOGS, an
anchor-based 3D-GS technique that introduces second-order anchors to achieve
superior rendering quality and reduced anchor features and model size
simultaneously. Specifically, SOGS incorporates covariance-based second-order
statistics and correlation across feature dimensions to augment features within
each anchor, compensating for the reduced feature size and improving rendering
quality effectively. In addition, it introduces a selective gradient loss to
enhance the optimization of scene textures and scene geometries, leading to
high-quality rendering with small anchor features. Extensive experiments over
multiple widely adopted benchmarks show that SOGS achieves superior rendering
quality in novel view synthesis with clearly reduced model size.

Comments:
- Accepted by CVPR 2025

---

## EigenGS Representation: From Eigenspace to Gaussian Image Space


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Lo-Wei Tai, Ching-En Li, Cheng-Lin Chen, Chih-Jung Tsai, Hwann-Tzong Chen, Tyng-Luh Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.07446v2){: .btn .btn-green } |

**Abstract**: Principal Component Analysis (PCA), a classical dimensionality reduction
technique, and 2D Gaussian representation, an adaptation of 3D Gaussian
Splatting for image representation, offer distinct approaches to modeling
visual data. We present EigenGS, a novel method that bridges these paradigms
through an efficient transformation pipeline connecting eigenspace and
image-space Gaussian representations. Our approach enables instant
initialization of Gaussian parameters for new images without requiring
per-image optimization from scratch, dramatically accelerating convergence.
EigenGS introduces a frequency-aware learning mechanism that encourages
Gaussians to adapt to different scales, effectively modeling varied spatial
frequencies and preventing artifacts in high-resolution reconstruction.
Extensive experiments demonstrate that EigenGS not only achieves superior
reconstruction quality compared to direct 2D Gaussian fitting but also reduces
necessary parameter count and training time. The results highlight EigenGS's
effectiveness and generalization ability across images with varying resolutions
and diverse categories, making Gaussian-based image representation both
high-quality and viable for real-time applications.



---

## CATPlan: Loss-based Collision Prediction in End-to-End Autonomous  Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Ziliang Xiong, Shipeng Liu, Nathaniel Helgesen, Joakim Johnander, Per-Erik Forssen | cs.RO | [PDF](http://arxiv.org/pdf/2503.07425v1){: .btn .btn-green } |

**Abstract**: In recent years, there has been increased interest in the design, training,
and evaluation of end-to-end autonomous driving (AD) systems. One often
overlooked aspect is the uncertainty of planned trajectories predicted by these
systems, despite awareness of their own uncertainty being key to achieve safety
and robustness. We propose to estimate this uncertainty by adapting loss
prediction from the uncertainty quantification literature. To this end, we
introduce a novel light-weight module, dubbed CATPlan, that is trained to
decode motion and planning embeddings into estimates of the collision loss used
to partially supervise end-to-end AD systems. During inference, these estimates
are interpreted as collision risk. We evaluate CATPlan on the safety-critical,
nerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detect
collisions with a $54.8\%$ relative improvement to average precision over a
GMM-based baseline in which the predicted trajectory is compared to the
forecasted trajectories of other road users. Our findings indicate that the
addition of CATPlan can lead to safer end-to-end AD systems and hope that our
work will spark increased interest in uncertainty quantification for such
systems.



---

## All That Glitters Is Not Gold: Key-Secured 3D Secrets within 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Yan Ren, Shilin Lu, Adams Wai-Kin Kong | cs.GR | [PDF](http://arxiv.org/pdf/2503.07191v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have revolutionized scene
reconstruction, opening new possibilities for 3D steganography by hiding 3D
secrets within 3D covers. The key challenge in steganography is ensuring
imperceptibility while maintaining high-fidelity reconstruction. However,
existing methods often suffer from detectability risks and utilize only
suboptimal 3DGS features, limiting their full potential. We propose a novel
end-to-end key-secured 3D steganography framework (KeySS) that jointly
optimizes a 3DGS model and a key-secured decoder for secret reconstruction. Our
approach reveals that Gaussian features contribute unequally to secret hiding.
The framework incorporates a key-controllable mechanism enabling multi-secret
hiding and unauthorized access prevention, while systematically exploring
optimal feature update to balance fidelity and security. To rigorously evaluate
steganographic imperceptibility beyond conventional 2D metrics, we introduce
3D-Sinkhorn distance analysis, which quantifies distributional differences
between original and steganographic Gaussian parameters in the representation
space. Extensive experiments demonstrate that our method achieves
state-of-the-art performance in both cover and secret reconstruction while
maintaining high security levels, advancing the field of 3D steganography. Code
is available at https://github.com/RY-Paper/KeySS



---

## POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Joey Wilson, Marcelino Almeida, Sachit Mahajan, Martin Labrie, Maani Ghaffari, Omid Ghasemalizadeh, Min Sun, Cheng-Hao Kuo, Arnab Sen | cs.CV | [PDF](http://arxiv.org/pdf/2503.07819v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel algorithm for quantifying uncertainty and
information gained within 3D Gaussian Splatting (3D-GS) through P-Optimality.
While 3D-GS has proven to be a useful world model with high-quality
rasterizations, it does not natively quantify uncertainty. Quantifying
uncertainty in parameters of 3D-GS is necessary to understand the information
gained from acquiring new images as in active perception, or identify redundant
images which can be removed from memory due to resource constraints in online
3D-GS SLAM. We propose to quantify uncertainty and information gain in 3D-GS by
reformulating the problem through the lens of optimal experimental design,
which is a classical solution to measuring information gain. By restructuring
information quantification of 3D-GS through optimal experimental design, we
arrive at multiple solutions, of which T-Optimality and D-Optimality perform
the best quantitatively and qualitatively as measured on two popular datasets.
Additionally, we propose a block diagonal approximation of the 3D-GS
uncertainty, which provides a measure of correlation for computing more
accurate information gain, at the expense of a greater computation cost.



---

## ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Konstantinos D. Polyzos, Athanasios Bacharis, Saketh Madhuvarasu, Nikos Papanikolopoulos, Tara Javidi | cs.CV | [PDF](http://arxiv.org/pdf/2503.06859v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting (GS) along with its extensions and variants provides
outstanding performance in real-time scene rendering while meeting reduced
storage demands and computational efficiency. While the selection of 2D images
capturing the scene of interest is crucial for the proper initialization and
training of GS, hence markedly affecting the rendering performance, prior works
rely on passively and typically densely selected 2D images. In contrast, this
paper proposes `ActiveInitSplat', a novel framework for active selection of
training images for proper initialization and training of GS. ActiveInitSplat
relies on density and occupancy criteria of the resultant 3D scene
representation from the selected 2D images, to ensure that the latter are
captured from diverse viewpoints leading to better scene coverage and that the
initialized Gaussian functions are well aligned with the actual 3D structure.
Numerical tests on well-known simulated and real environments demonstrate the
merits of ActiveInitSplat resulting in significant GS rendering performance
improvement over passive GS baselines, in the widely adopted LPIPS, SSIM, and
PSNR metrics.



---

## Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D  Environments

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Andrei Chubarau, Yinan Wang, James J. Clark | cs.CV | [PDF](http://arxiv.org/pdf/2503.07828v1){: .btn .btn-green } |

**Abstract**: We introduce Neural Radiance and Gaze Fields (NeRGs) as a novel approach for
representing visual attention patterns in 3D scenes. Our system renders a 2D
view of a 3D scene with a pre-trained Neural Radiance Field (NeRF) and
visualizes the gaze field for arbitrary observer positions, which may be
decoupled from the render camera perspective. We achieve this by augmenting a
standard NeRF with an additional neural network that models the gaze
probability distribution. The output of a NeRG is a rendered image of the scene
viewed from the camera perspective and a pixel-wise salience map representing
conditional probability that an observer fixates on a given surface within the
3D scene as visible in the rendered image. Much like how NeRFs perform novel
view synthesis, NeRGs enable the reconstruction of gaze patterns from arbitrary
perspectives within complex 3D scenes. To ensure consistent gaze
reconstructions, we constrain gaze prediction on the 3D structure of the scene
and model gaze occlusion due to intervening surfaces when the observer's
viewpoint is decoupled from the rendering camera. For training, we leverage
ground truth head pose data from skeleton tracking data or predictions from 2D
salience models. We demonstrate the effectiveness of NeRGs in a real-world
convenience store setting, where head pose tracking data is available.

Comments:
- 11 pages, 8 figures

---

## StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for  Superior 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Zexu Huang, Min Xu, Stuart Perry | cs.CV | [PDF](http://arxiv.org/pdf/2503.06462v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D reconstruction coupled with neural rendering
techniques have greatly improved the creation of photo-realistic 3D scenes,
influencing both academic research and industry applications. The technique of
3D Gaussian Splatting and its variants incorporate the strengths of both
primitive-based and volumetric representations, achieving superior rendering
quality. While 3D Geometric Scattering (3DGS) and its variants have advanced
the field of 3D representation, they fall short in capturing the stochastic
properties of non-local structural information during the training process.
Additionally, the initialisation of spherical functions in 3DGS-based methods
often fails to engage higher-order terms in early training rounds, leading to
unnecessary computational overhead as training progresses. Furthermore, current
3DGS-based approaches require training on higher resolution images to render
higher resolution outputs, significantly increasing memory demands and
prolonging training durations. We introduce StructGS, a framework that enhances
3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D
reconstruction. StructGS innovatively incorporates a patch-based SSIM loss,
dynamic spherical harmonics initialisation and a Multi-scale Residual Network
(MSRN) to address the above-mentioned limitations, respectively. Our framework
significantly reduces computational redundancy, enhances detail capture and
supports high-resolution rendering from low-resolution inputs. Experimentally,
StructGS demonstrates superior performance over state-of-the-art (SOTA) models,
achieving higher quality and more detailed renderings with fewer artifacts.



---

## Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy  Surface Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Xiaoming Peng, Yixin Yang, Yang Zhou, Hui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06587v1){: .btn .btn-green } |

**Abstract**: Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry
reconstruction quality than the popular 3DGS by using 2D surfels to approximate
thin surfaces. However, it falls short when dealing with glossy surfaces,
resulting in visible holes in these areas. We found the reflection
discontinuity causes the issue. To fit the jump from diffuse to specular
reflection at different viewing angles, depth bias is introduced in the
optimized Gaussian primitives. To address that, we first replace the depth
distortion loss in 2DGS with a novel depth convergence loss, which imposes a
strong constraint on depth continuity. Then, we rectified the depth criterion
in determining the actual surface, which fully accounts for all the
intersecting Gaussians along the ray. Qualitative and quantitative evaluations
across various datasets reveal that our method significantly improves
reconstruction quality, with more complete and accurate surfaces than 2DGS.



---

## REArtGS: Reconstructing and Generating Articulated Objects via 3D  Gaussian Splatting with Geometric and Motion Constraints

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Di Wu, Liu Liu, Zhou Linli, Anran Huang, Liangtu Song, Qiaojun Yu, Qi Wu, Cewu Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.06677v1){: .btn .btn-green } |

**Abstract**: Articulated objects, as prevalent entities in human life, their 3D
representations play crucial roles across various applications. However,
achieving both high-fidelity textured surface reconstruction and dynamic
generation for articulated objects remains challenging for existing methods. In
this paper, we present REArtGS, a novel framework that introduces additional
geometric and motion constraints to 3D Gaussian primitives, enabling
high-quality textured surface reconstruction and generation for articulated
objects. Specifically, given multi-view RGB images of arbitrary two states of
articulated objects, we first introduce an unbiased Signed Distance Field (SDF)
guidance to regularize Gaussian opacity fields, enhancing geometry constraints
and improving surface reconstruction quality. Then we establish deformable
fields for 3D Gaussians constrained by the kinematic structures of articulated
objects, achieving unsupervised generation of surface meshes in unseen states.
Extensive experiments on both synthetic and real datasets demonstrate our
approach achieves high-quality textured surface reconstruction for given
states, and enables high-fidelity surface generation for unseen states. Codes
will be released within the next four months.

Comments:
- 11pages, 6 figures

---

## D3DR: Lighting-Aware Object Insertion in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Vsevolod Skorokhodov, Nikita Durasov, Pascal Fua | cs.CV | [PDF](http://arxiv.org/pdf/2503.06740v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has become a popular technique for various 3D Computer
Vision tasks, including novel view synthesis, scene reconstruction, and dynamic
scene rendering. However, the challenge of natural-looking object insertion,
where the object's appearance seamlessly matches the scene, remains unsolved.
In this work, we propose a method, dubbed D3DR, for inserting a
3DGS-parametrized object into 3DGS scenes while correcting its lighting,
shadows, and other visual artifacts to ensure consistency, a problem that has
not been successfully addressed before. We leverage advances in diffusion
models, which, trained on real-world data, implicitly understand correct scene
lighting. After inserting the object, we optimize a diffusion-based Delta
Denoising Score (DDS)-inspired objective to adjust its 3D Gaussian parameters
for proper lighting correction. Utilizing diffusion model personalization
techniques to improve optimization quality, our approach ensures seamless
object insertion and natural appearance. Finally, we demonstrate the method's
effectiveness by comparing it to existing approaches, achieving 0.5 PSNR and
0.15 SSIM improvements in relighting quality.



---

## Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate  Representation and Reconstruction of Neural Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Abdelaziz Bouzidi, Hamid Laga, Hazem Wannous | cs.CV | [PDF](http://arxiv.org/pdf/2503.06762v1){: .btn .btn-green } |

**Abstract**: Neural fields such as DeepSDF and Neural Radiance Fields have recently
revolutionized novel-view synthesis and 3D reconstruction from RGB images and
videos. However, achieving high-quality representation, reconstruction, and
rendering requires deep neural networks, which are slow to train and evaluate.
Although several acceleration techniques have been proposed, they often trade
off speed for memory. Gaussian splatting-based methods, on the other hand,
accelerate the rendering time but remain costly in terms of training speed and
memory needed to store the parameters of a large number of Gaussians. In this
paper, we introduce a novel neural representation that is fast, both at
training and inference times, and lightweight. Our key observation is that the
neurons used in traditional MLPs perform simple computations (a dot product
followed by ReLU activation) and thus one needs to use either wide and deep
MLPs or high-resolution and high-dimensional feature grids to parameterize
complex nonlinear functions. We show in this paper that by replacing
traditional neurons with Radial Basis Function (RBF) kernels, one can achieve
highly accurate representation of 2D (RGB images), 3D (geometry), and 5D
(radiance fields) signals with just a single layer of such neurons. The
representation is highly parallelizable, operates on low-resolution feature
grids, and is compact and memory-efficient. We demonstrate that the proposed
novel representation can be trained for 3D geometry representation in less than
15 seconds and for novel view synthesis in less than 15 mins. At runtime, it
can synthesize novel views at more than 60 fps without sacrificing quality.

Comments:
- Our code is available at https://grbfnet.github.io/

---

## CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation  Awareness for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Rui Song, Chenwei Liang, Yan Xia, Walter Zimmer, Hu Cao, Holger Caesar, Andreas Festag, Alois Knoll | cs.CV | [PDF](http://arxiv.org/pdf/2503.06744v1){: .btn .btn-green } |

**Abstract**: Dynamic scene rendering opens new avenues in autonomous driving by enabling
closed-loop simulations with photorealistic data, which is crucial for
validating end-to-end algorithms. However, the complex and highly dynamic
nature of traffic environments presents significant challenges in accurately
rendering these scenes. In this paper, we introduce a novel 4D Gaussian
Splatting (4DGS) approach, which incorporates context and temporal deformation
awareness to improve dynamic scene rendering. Specifically, we employ a 2D
semantic segmentation foundation model to self-supervise the 4D semantic
features of Gaussians, ensuring meaningful contextual embedding.
Simultaneously, we track the temporal deformation of each Gaussian across
adjacent frames. By aggregating and encoding both semantic and temporal
deformation features, each Gaussian is equipped with cues for potential
deformation compensation within 3D space, facilitating a more precise
representation of dynamic scenes. Experimental results show that our method
improves 4DGS's ability to capture fine details in dynamic scene rendering for
autonomous driving and outperforms other self-supervised methods in 4D
reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms
semantic features with each Gaussian, enabling broader applications.



---

## Pixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D  Gaussian Modeling


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Long Peng, Anran Wu, Wenbo Li, Peizhe Xia, Xueyuan Dai, Xinjie Zhang, Xin Di, Haoze Sun, Renjing Pei, Yang Wang, Yang Cao, Zheng-Jun Zha | cs.CV | [PDF](http://arxiv.org/pdf/2503.06617v1){: .btn .btn-green } |

**Abstract**: Arbitrary-scale super-resolution (ASSR) aims to reconstruct high-resolution
(HR) images from low-resolution (LR) inputs with arbitrary upsampling factors
using a single model, addressing the limitations of traditional SR methods
constrained to fixed-scale factors (\textit{e.g.}, $\times$ 2). Recent advances
leveraging implicit neural representation (INR) have achieved great progress by
modeling coordinate-to-pixel mappings. However, the efficiency of these methods
may suffer from repeated upsampling and decoding, while their reconstruction
fidelity and quality are constrained by the intrinsic representational
limitations of coordinate-based functions. To address these challenges, we
propose a novel ContinuousSR framework with a Pixel-to-Gaussian paradigm, which
explicitly reconstructs 2D continuous HR signals from LR images using Gaussian
Splatting. This approach eliminates the need for time-consuming upsampling and
decoding, enabling extremely fast arbitrary-scale super-resolution. Once the
Gaussian field is built in a single pass, ContinuousSR can perform
arbitrary-scale rendering in just 1ms per scale. Our method introduces several
key innovations. Through statistical ana

Comments:
- Tech Report

---

## GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video  Diffusion for Single-Image 3D Object Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Ye Tao, Jiawei Zhang, Yahao Shi, Dongqing Zou, Bin Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.06136v1){: .btn .btn-green } |

**Abstract**: Image-based 3D generation has vast applications in robotics and gaming, where
high-quality, diverse outputs and consistent 3D representations are crucial.
However, existing methods have limitations: 3D diffusion models are limited by
dataset scarcity and the absence of strong pre-trained priors, while 2D
diffusion-based approaches struggle with geometric consistency. We propose a
method that leverages 2D diffusion models' implicit 3D reasoning ability while
ensuring 3D consistency via Gaussian-splatting-based geometric distillation.
Specifically, the proposed Gaussian Splatting Decoder enforces 3D consistency
by transforming SV3D latent outputs into an explicit 3D representation. Unlike
SV3D, which only relies on implicit 2D representations for video generation,
Gaussian Splatting explicitly encodes spatial and appearance attributes,
enabling multi-view consistency through geometric constraints. These
constraints correct view inconsistencies, ensuring robust geometric
consistency. As a result, our approach simultaneously generates high-quality,
multi-view-consistent images and accurate 3D models, providing a scalable
solution for single-image-based 3D generation and bridging the gap between 2D
Diffusion diversity and 3D structural coherence. Experimental results
demonstrate state-of-the-art multi-view consistency and strong generalization
across diverse datasets. The code will be made publicly available upon
acceptance.



---

## NeuraLoc: Visual Localization in Neural Implicit Map with Dual  Complementary Features

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Hongjia Zhai, Boming Zhao, Hai Li, Xiaokun Pan, Yijia He, Zhaopeng Cui, Hujun Bao, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06117v1){: .btn .btn-green } |

**Abstract**: Recently, neural radiance fields (NeRF) have gained significant attention in
the field of visual localization. However, existing NeRF-based approaches
either lack geometric constraints or require extensive storage for feature
matching, limiting their practical applications. To address these challenges,
we propose an efficient and novel visual localization approach based on the
neural implicit map with complementary features. Specifically, to enforce
geometric constraints and reduce storage requirements, we implicitly learn a 3D
keypoint descriptor field, avoiding the need to explicitly store point-wise
features. To further address the semantic ambiguity of descriptors, we
introduce additional semantic contextual feature fields, which enhance the
quality and reliability of 2D-3D correspondences. Besides, we propose
descriptor similarity distribution alignment to minimize the domain gap between
2D and 3D feature spaces during matching. Finally, we construct the matching
graph using both complementary descriptors and contextual features to establish
accurate 2D-3D correspondences for 6-DoF pose estimation. Compared with the
recent NeRF-based approaches, our method achieves a 3$\times$ faster training
speed and a 45$\times$ reduction in model storage. Extensive experiments on two
widely used datasets demonstrate that our approach outperforms or is highly
competitive with other state-of-the-art NeRF-based visual localization methods.
Project page:
\href{https://zju3dv.github.io/neuraloc}{https://zju3dv.github.io/neuraloc}

Comments:
- ICRA 2025

---

## SplatTalk: 3D VQA with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Anh Thai, Songyou Peng, Kyle Genova, Leonidas Guibas, Thomas Funkhouser | cs.CV | [PDF](http://arxiv.org/pdf/2503.06271v1){: .btn .btn-green } |

**Abstract**: Language-guided 3D scene understanding is important for advancing
applications in robotics, AR/VR, and human-computer interaction, enabling
models to comprehend and interact with 3D environments through natural
language. While 2D vision-language models (VLMs) have achieved remarkable
success in 2D VQA tasks, progress in the 3D domain has been significantly
slower due to the complexity of 3D data and the high cost of manual
annotations. In this work, we introduce SplatTalk, a novel method that uses a
generalizable 3D Gaussian Splatting (3DGS) framework to produce 3D tokens
suitable for direct input into a pretrained LLM, enabling effective zero-shot
3D visual question answering (3D VQA) for scenes with only posed images. During
experiments on multiple benchmarks, our approach outperforms both 3D models
trained specifically for the task and previous 2D-LMM-based models utilizing
only images (our setting), while achieving competitive performance with
state-of-the-art 3D LMMs that additionally utilize 3D inputs.



---

## StreamGS: Online Generalizable Gaussian Splatting Reconstruction for  Unposed Image Streams

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Yang LI, Jinglu Wang, Lei Chu, Xiao Li, Shiu-hong Kao, Ying-Cong Chen, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.06235v1){: .btn .btn-green } |

**Abstract**: The advent of 3D Gaussian Splatting (3DGS) has advanced 3D scene
reconstruction and novel view synthesis. With the growing interest of
interactive applications that need immediate feedback, online 3DGS
reconstruction in real-time is in high demand. However, none of existing
methods yet meet the demand due to three main challenges: the absence of
predetermined camera parameters, the need for generalizable 3DGS optimization,
and the necessity of reducing redundancy. We propose StreamGS, an online
generalizable 3DGS reconstruction method for unposed image streams, which
progressively transform image streams to 3D Gaussian streams by predicting and
aggregating per-frame Gaussians. Our method overcomes the limitation of the
initial point reconstruction \cite{dust3r} in tackling out-of-domain (OOD)
issues by introducing a content adaptive refinement. The refinement enhances
cross-frame consistency by establishing reliable pixel correspondences between
adjacent frames. Such correspondences further aid in merging redundant
Gaussians through cross-frame feature aggregation. The density of Gaussians is
thereby reduced, empowering online reconstruction by significantly lowering
computational and memory costs. Extensive experiments on diverse datasets have
demonstrated that StreamGS achieves quality on par with optimization-based
approaches but does so 150 times faster, and exhibits superior generalizability
in handling OOD scenes.

Comments:
- 8 pages

---

## ForestSplats: Deformable transient field for Gaussian Splatting in the  Wild

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Wongi Park, Myeongseok Nam, Siwon Kim, Sangwoo Jo, Soomok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.06179v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time
rendering speeds and high-quality results in static scenes. Although 3D-GS
shows effectiveness in static scenes, their performance significantly degrades
in real-world environments due to transient objects, lighting variations, and
diverse levels of occlusion. To tackle this, existing methods estimate
occluders or transient elements by leveraging pre-trained models or integrating
additional transient field pipelines. However, these methods still suffer from
two defects: 1) Using semantic features from the Vision Foundation model (VFM)
causes additional computational costs. 2) The transient field requires
significant memory to handle transient elements with per-view Gaussians and
struggles to define clear boundaries for occluders, solely relying on
photometric errors. To address these problems, we propose ForestSplats, a novel
approach that leverages the deformable transient field and a superpixel-aware
mask to efficiently represent transient elements in the 2D scene across
unconstrained image collections and effectively decompose static scenes from
transient distractors without VFM. We designed the transient field to be
deformable, capturing per-view transient elements. Furthermore, we introduce a
superpixel-aware mask that clearly defines the boundaries of occluders by
considering photometric errors and superpixels. Additionally, we propose
uncertainty-aware densification to avoid generating Gaussians within the
boundaries of occluders during densification. Through extensive experiments
across several benchmark datasets, we demonstrate that ForestSplats outperforms
existing methods without VFM and shows significant memory efficiency in
representing transient elements.



---

## Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical  Deformable Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Kai Li, Junhao Wang, William Han, Ding Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2503.06161v1){: .btn .btn-green } |

**Abstract**: Minimally invasive surgery (MIS) has transformed clinical practice by
reducing recovery times, minimizing complications, and enhancing precision.
Nonetheless, MIS inherently relies on indirect visualization and precise
instrument control, posing unique challenges. Recent advances in artificial
intelligence have enabled real-time surgical scene understanding through
techniques such as image classification, object detection, and segmentation,
with scene reconstruction emerging as a key element for enhanced intraoperative
guidance. Although neural radiance fields (NeRFs) have been explored for this
purpose, their substantial data requirements and slow rendering inhibit
real-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more
efficient alternative, achieving state-of-the-art performance in dynamic
surgical scene reconstruction. In this work, we introduce Feature-EndoGaussian
(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D
rendering to enable real-time semantic and scene reconstruction. By leveraging
pretrained segmentation foundation models, FEG incorporates semantic feature
distillation within the Gaussian deformation framework, thereby enhancing both
reconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG
achieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)
compared to leading methods. Additionally, on the EndoVis18 dataset, FEG
demonstrates competitive class-wise segmentation metrics while balancing model
size and real-time performance.

Comments:
- 14 pages, 5 figures

---

## SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting  Steganography

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Xuanyu Zhang, Jiarui Meng, Zhipei Xu, Shuzhou Yang, Yanmin Wu, Ronggang Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06118v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a premier method for 3D
representation due to its real-time rendering and high-quality outputs,
underscoring the critical need to protect the privacy of 3D assets. Traditional
NeRF steganography methods fail to address the explicit nature of 3DGS since
its point cloud files are publicly accessible. Existing GS steganography
solutions mitigate some issues but still struggle with reduced rendering
fidelity, increased computational demands, and security flaws, especially in
the security of the geometric structure of the visualized point cloud. To
address these demands, we propose a SecureGS, a secure and efficient 3DGS
steganography framework inspired by Scaffold-GS's anchor point design and
neural decoding. SecureGS uses a hybrid decoupled Gaussian encryption mechanism
to embed offsets, scales, rotations, and RGB attributes of the hidden 3D
Gaussian points in anchor point features, retrievable only by authorized users
through privacy-preserving neural networks. To further enhance security, we
propose a density region-aware anchor growing and pruning strategy that
adaptively locates optimal hiding regions without exposing hidden information.
Extensive experiments show that SecureGS significantly surpasses existing GS
steganography methods in rendering fidelity, speed, and security.

Comments:
- Accepted by ICLR 2025

---

## D2GV: Deformable 2D Gaussian Splatting for Video Representation in  400FPS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Mufan Liu, Qi Yang, Miaoran Zhao, He Huang, Le Yang, Zhu Li, Yiling Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.05600v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representations (INRs) have emerged as a powerful approach
for video representation, offering versatility across tasks such as compression
and inpainting. However, their implicit formulation limits both
interpretability and efficacy, undermining their practicality as a
comprehensive solution. We propose a novel video representation based on
deformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key
objectives: 1) improved efficiency while delivering superior quality; 2)
enhanced scalability and interpretability; and 3) increased friendliness for
downstream tasks. Specifically, we initially divide the video sequence into
fixed-length Groups of Pictures (GoP) to allow parallel training and linear
scalability with video length. For each GoP, D2GV represents video frames by
applying differentiable rasterization to 2D Gaussians, which are deformed from
a canonical space into their corresponding timestamps. Notably, leveraging
efficient CUDA-based rasterization, D2GV converges fast and decodes at speeds
exceeding 400 FPS, while delivering quality that matches or surpasses
state-of-the-art INRs. Moreover, we incorporate a learnable pruning and
quantization strategy to streamline D2GV into a more compact representation. We
demonstrate D2GV's versatility in tasks including video interpolation,
inpainting and denoising, underscoring its potential as a promising solution
for video representation. Code is available at:
\href{https://github.com/Evan-sudo/D2GV}{https://github.com/Evan-sudo/D2GV}.



---

## SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image  via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Linqi Yang, Xiongwei Zhao, Qihao Sun, Ke Wang, Ao Chen, Peng Kang | cs.CV | [PDF](http://arxiv.org/pdf/2503.05174v1){: .btn .btn-green } |

**Abstract**: 6-DoF pose estimation is a fundamental task in computer vision with
wide-ranging applications in augmented reality and robotics. Existing single
RGB-based methods often compromise accuracy due to their reliance on initial
pose estimates and susceptibility to rotational ambiguity, while approaches
requiring depth sensors or multi-view setups incur significant deployment
costs. To address these limitations, we introduce SplatPose, a novel framework
that synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neural
architecture to achieve high-precision pose estimation using only a single RGB
image. Central to our approach is the Dual-Attention Ray Scoring Network
(DARS-Net), which innovatively decouples positional and angular alignment
through geometry-domain attention mechanisms, explicitly modeling directional
dependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fine
optimization pipeline progressively refines pose estimates by aligning dense 2D
features between query images and 3DGS-synthesized views, effectively
correcting feature misalignment and depth errors from sparse ray sampling.
Experiments on three benchmark datasets demonstrate that SplatPose achieves
state-of-the-art 6-DoF pose estimation accuracy in single RGB settings,
rivaling approaches that depend on depth or multi-view images.

Comments:
- Submitted to IROS 2025

---

## Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot  Manipulation of Irregularly Shaped Objects


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2503.05189v1){: .btn .btn-green } |

**Abstract**: Tracking and manipulating irregularly-shaped, previously unseen objects in
dynamic environments is important for robotic applications in manufacturing,
assembly, and logistics. Recently introduced Gaussian Splats efficiently model
object geometry, but lack persistent state estimation for task-oriented
manipulation. We present Persistent Object Gaussian Splat (POGS), a system that
embeds semantics, self-supervised visual features, and object grouping features
into a compact representation that can be continuously updated to estimate the
pose of scanned objects. POGS updates object states without requiring expensive
rescanning or prior CAD models of objects. After an initial multi-view scene
capture and training phase, POGS uses a single stereo camera to integrate depth
estimates along with self-supervised vision encoder features for object pose
estimation. POGS supports grasping, reorientation, and natural language-driven
manipulation by refining object pose estimates, facilitating sequential object
reset operations with human-induced object perturbations and tool servoing,
where robots recover tool pose despite tool perturbations of up to 30{\deg}.
POGS achieves up to 12 consecutive successful object resets and recovers from
80% of in-grasp tool perturbations.

Comments:
- Accepted to ICRA 2025

---

## SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Xiaotong Huang, He Zhu, Zihan Liu, Weikai Lin, Xiaohong Liu, Zhezhi He, Jingwen Leng, Minyi Guo, Yu Feng | cs.GR | [PDF](http://arxiv.org/pdf/2503.05168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a crucial rendering technique for
many real-time applications. However, the limited hardware resources on today's
mobile platforms hinder these applications, as they struggle to achieve
real-time performance. In this paper, we propose SeeLe, a general framework
designed to accelerate the 3DGS pipeline for resource-constrained mobile
devices.
  Specifically, we propose two GPU-oriented techniques: hybrid preprocessing
and contribution-aware rasterization. Hybrid preprocessing alleviates the GPU
compute and memory pressure by reducing the number of irrelevant Gaussians
during rendering. The key is to combine our view-dependent scene representation
with online filtering. Meanwhile, contribution-aware rasterization improves the
GPU utilization at the rasterization stage by prioritizing Gaussians with high
contributions while reducing computations for those with low contributions.
Both techniques can be seamlessly integrated into existing 3DGS pipelines with
minimal fine-tuning. Collectively, our framework achieves 2.6$\times$ speedup
and 32.3\% model reduction while achieving superior rendering quality compared
to existing methods.



---

## Bayesian Fields: Task-driven Open-Set Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Dominic Maggio, Luca Carlone | cs.CV | [PDF](http://arxiv.org/pdf/2503.05949v1){: .btn .btn-green } |

**Abstract**: Open-set semantic mapping requires (i) determining the correct granularity to
represent the scene (e.g., how should objects be defined), and (ii) fusing
semantic knowledge across multiple 2D observations into an overall 3D
reconstruction -ideally with a high-fidelity yet low-memory footprint. While
most related works bypass the first issue by grouping together primitives with
similar semantics (according to some manually tuned threshold), we recognize
that the object granularity is task-dependent, and develop a task-driven
semantic mapping approach. To address the second issue, current practice is to
average visual embedding vectors over multiple views. Instead, we show the
benefits of using a probabilistic approach based on the properties of the
underlying visual-language foundation model, and leveraging Bayesian updating
to aggregate multiple observations of the scene. The result is Bayesian Fields,
a task-driven and probabilistic approach for open-set semantic mapping. To
enable high-fidelity objects and a dense scene representation, Bayesian Fields
uses 3D Gaussians which we cluster into task-relevant objects, allowing for
both easy 3D object extraction and reduced memory usage. We release Bayesian
Fields open-source at https: //github.com/MIT-SPARK/Bayesian-Fields.



---

## EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D  Gaussian Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Chao Zhang, Yifeng Zhou, Shuheng Wang, Wenfa Li, Degang Wang, Yi Xu, Shaohui Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2503.05162v1){: .btn .btn-green } |

**Abstract**: We have recently seen great progress in 3D scene reconstruction through
explicit point-based 3D Gaussian Splatting (3DGS), notable for its high quality
and fast rendering speed. However, reconstructing dynamic scenes such as
complex human performances with long durations remains challenging. Prior
efforts fall short of modeling a long-term sequence with drastic motions,
frequent topology changes or interactions with props, and resort to segmenting
the whole sequence into groups of frames that are processed independently,
which undermines temporal stability and thereby leads to an unpleasant viewing
experience and inefficient storage footprint. In view of this, we introduce
EvolvingGS, a two-stage strategy that first deforms the Gaussian model to
coarsely align with the target frame, and then refines it with minimal point
addition/subtraction, particularly in fast-changing areas. Owing to the
flexibility of the incrementally evolving representation, our method
outperforms existing approaches in terms of both per-frame and temporal quality
metrics while maintaining fast rendering through its purely explicit
representation. Moreover, by exploiting temporal coherence between successive
frames, we propose a simple yet effective compression algorithm that achieves
over 50x compression rate. Extensive experiments on both public benchmarks and
challenging custom datasets demonstrate that our method significantly advances
the state-of-the-art in dynamic scene reconstruction, particularly for extended
sequences with complex human performances.



---

## Self-Modeling Robots by Photographing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Kejun Hu, Peng Yu, Ning Tan | cs.RO | [PDF](http://arxiv.org/pdf/2503.05398v1){: .btn .btn-green } |

**Abstract**: Self-modeling enables robots to build task-agnostic models of their
morphology and kinematics based on data that can be automatically collected,
with minimal human intervention and prior information, thereby enhancing
machine intelligence. Recent research has highlighted the potential of
data-driven technology in modeling the morphology and kinematics of robots.
However, existing self-modeling methods suffer from either low modeling quality
or excessive data acquisition costs. Beyond morphology and kinematics, texture
is also a crucial component of robots, which is challenging to model and
remains unexplored. In this work, a high-quality, texture-aware, and link-level
method is proposed for robot self-modeling. We utilize three-dimensional (3D)
Gaussians to represent the static morphology and texture of robots, and cluster
the 3D Gaussians to construct neural ellipsoid bones, whose deformations are
controlled by the transformation matrices generated by a kinematic neural
network. The 3D Gaussians and kinematic neural network are trained using data
pairs composed of joint angles, camera parameters and multi-view images without
depth information. By feeding the kinematic neural network with joint angles,
we can utilize the well-trained model to describe the corresponding morphology,
kinematics and texture of robots at the link level, and render robot images
from different perspectives with the aid of 3D Gaussian splatting. Furthermore,
we demonstrate that the established model can be exploited to perform
downstream tasks such as motion planning and inverse kinematics.



---

## GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Kohei Honda, Takeshi Ishita, Yasuhiro Yoshimura, Ryo Yonitani | cs.RO | [PDF](http://arxiv.org/pdf/2503.05152v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel approach to image-goal navigation by integrating
3D Gaussian Splatting (3DGS) with Visual Navigation Models (VNMs), a method we
refer to as GSplatVNM. VNMs offer a promising paradigm for image-goal
navigation by guiding a robot through a sequence of point-of-view images
without requiring metrical localization or environment-specific training.
However, constructing a dense and traversable sequence of target viewpoints
from start to goal remains a central challenge, particularly when the available
image database is sparse. To address these challenges, we propose a 3DGS-based
viewpoint synthesis framework for VNMs that synthesizes intermediate viewpoints
to seamlessly bridge gaps in sparse data while significantly reducing storage
overhead. Experimental results in a photorealistic simulator demonstrate that
our approach not only enhances navigation efficiency but also exhibits
robustness under varying levels of image database sparsity.

Comments:
- 8 pages, 4 figures

---

## CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from  Motion-Blurred Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jungho Lee, Donghyeong Kim, Dogyoon Lee, Suhwan Cho, Minhyeok Lee, Wonjoon Lee, Taeoh Kim, Dongyoon Wee, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.05332v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for their
high-quality novel view rendering, motivating research to address real-world
challenges. A critical issue is the camera motion blur caused by movement
during exposure, which hinders accurate 3D scene reconstruction. In this study,
we propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that
reconstructs precise 3D scenes from motion-blurred images while maintaining
real-time rendering speed. Considering the complex motion patterns inherent in
real-world camera movements, we predict continuous camera trajectories using
neural ordinary differential equations (ODEs). To ensure accurate modeling, we
employ rigid body transformations, preserving the shape and size of the object
but rely on the discrete integration of sampled frames. To better approximate
the continuous nature of motion blur, we introduce a continuous motion
refinement (CMR) transformation that refines rigid transformations by
incorporating additional learnable parameters. By revisiting fundamental camera
theory and leveraging advanced neural ODE techniques, we achieve precise
modeling of continuous camera trajectories, leading to improved reconstruction
accuracy. Extensive experiments demonstrate state-of-the-art performance both
quantitatively and qualitatively on benchmark datasets, which include a wide
range of motion blur scenarios, from moderate to extreme blur.

Comments:
- Revised Version of CRiM-GS, Github:
  https://github.com/Jho-Yonsei/CoMoGaussian

---

## LiDAR-enhanced 3D Gaussian Splatting Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jian Shen, Huai Yu, Ji Wu, Wen Yang, Gui-Song Xia | cs.RO | [PDF](http://arxiv.org/pdf/2503.05425v1){: .btn .btn-green } |

**Abstract**: This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting
(3DGS) mapping framework that improves the accuracy and robustness of 3D scene
mapping by integrating LiDAR data. LiGSM constructs joint loss from images and
LiDAR point clouds to estimate the poses and optimize their extrinsic
parameters, enabling dynamic adaptation to variations in sensor alignment.
Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a
denser and more reliable starting points compared to sparse SfM points. In
scene rendering, the framework augments standard image-based supervision with
depth maps generated from LiDAR projections, ensuring an accurate scene
representation in both geometry and photometry. Experiments on public and
self-collected datasets demonstrate that LiGSM outperforms comparative methods
in pose tracking and scene rendering.

Comments:
- Accepted by ICRA 2025

---

## GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three  Orthographic Views Using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Zheng Zhou, Zhe Li, Bo Yu, Lina Hu, Liang Dong, Zijian Yang, Xiaoli Liu, Ning Xu, Ziwei Wang, Yonghao Dang, Jianqin Yin | cs.CV | [PDF](http://arxiv.org/pdf/2503.05161v1){: .btn .btn-green } |

**Abstract**: The automatic reconstruction of 3D computer-aided design (CAD) models from
CAD sketches has recently gained significant attention in the computer vision
community. Most existing methods, however, rely on vector CAD sketches and 3D
ground truth for supervision, which are often difficult to be obtained in
industrial applications and are sensitive to noise inputs. We propose viewing
CAD reconstruction as a specific instance of sparse-view 3D reconstruction to
overcome these limitations. While this reformulation offers a promising
perspective, existing 3D reconstruction methods typically require natural
images and corresponding camera poses as inputs, which introduces two major
significant challenges: (1) modality discrepancy between CAD sketches and
natural images, and (2) difficulty of accurate camera pose estimation for CAD
sketches. To solve these issues, we first transform the CAD sketches into
representations resembling natural images and extract corresponding masks.
Next, we manually calculate the camera poses for the orthographic views to
ensure accurate alignment within the 3D coordinate system. Finally, we employ a
customized sparse-view 3D reconstruction method to achieve high-quality
reconstructions from aligned orthographic views. By leveraging raster CAD
sketches for self-supervision, our approach eliminates the reliance on vector
CAD sketches and 3D ground truth. Experiments on the Sub-Fusion360 dataset
demonstrate that our proposed method significantly outperforms previous
approaches in CAD reconstruction performance and exhibits strong robustness to
noisy inputs.



---

## MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface  Reconstruction under Various Light Conditions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Qingyuan Zhou, Yuehu Gong, Weidong Yang, Jiaze Li, Yeqi Luo, Baixin Xu, Shuhao Li, Ben Fei, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2503.05182v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) and surface reconstruction (SR) are essential
tasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks
are often addressed independently, with GS-based rendering methods struggling
under diverse light conditions and failing to produce accurate surfaces, while
GS-based reconstruction methods frequently compromise rendering quality. This
raises a central question: must rendering and reconstruction always involve a
trade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian
splatting for Surface Reconstruction that enhances both rendering quality and
3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS
and the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,
providing precise geometry information to the 3D-GS branch. Leveraging this
geometry, the 3D-GS branch employs a geometry-guided illumination decomposition
module that captures reflected and transmitted components, enabling realistic
rendering under varied light conditions. Using the transmitted component as
supervision, the 2D-GS branch also achieves high-fidelity surface
reconstruction. Throughout the optimization process, the 2D-GS and 3D-GS
branches undergo alternating optimization, providing mutual supervision. Prior
to this, each branch completes an independent warm-up phase, with an early
stopping strategy implemented to reduce computational costs. We evaluate MGSR
on a diverse set of synthetic and real-world datasets, at both object and scene
levels, demonstrating strong performance in rendering and surface
reconstruction.

Comments:
- 11 pages, 7 figures

---

## Free Your Hands: Lightweight Relightable Turntable Capture Pipeline

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jiahui Fan, Fujun Luan, Miloš Hašan, Jian Yang, Beibei Wang | cs.GR | [PDF](http://arxiv.org/pdf/2503.05511v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) from multiple captured photos of an object is a
widely studied problem. Achieving high quality typically requires dense
sampling of input views, which can lead to frustrating and tedious manual
labor. Manually positioning cameras to maintain an optimal desired distribution
can be difficult for humans, and if a good distribution is found, it is not
easy to replicate. Additionally, the captured data can suffer from motion blur
and defocus due to human error. In this paper, we present a lightweight object
capture pipeline to reduce the manual workload and standardize the acquisition
setup. We use a consumer turntable to carry the object and a tripod to hold the
camera. As the turntable rotates, we automatically capture dense samples from
various views and lighting conditions; we can repeat this for several camera
positions. This way, we can easily capture hundreds of valid images in several
minutes without hands-on effort. However, in the object reference frame, the
light conditions vary; this is harmful to a standard NVS method like 3D
Gaussian splatting (3DGS) which assumes fixed lighting. We design a neural
radiance representation conditioned on light rotations, which addresses this
issue and allows relightability as an additional benefit. We demonstrate our
pipeline using 3DGS as the underlying framework, achieving competitive quality
compared to previous methods with exhaustive acquisition and showcasing its
potential for relighting and harmonization tasks.



---

## Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D  Gaussian Splatting from Sparse Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Yingji Zhong, Zhihao Li, Dave Zhenyu Chen, Lanqing Hong, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.05082v1){: .btn .btn-green } |

**Abstract**: Despite recent successes in novel view synthesis using 3D Gaussian Splatting
(3DGS), modeling scenes with sparse inputs remains a challenge. In this work,
we address two critical yet overlooked issues in real-world sparse-input
modeling: extrapolation and occlusion. To tackle these issues, we propose to
use a reconstruction by generation pipeline that leverages learned priors from
video diffusion models to provide plausible interpretations for regions outside
the field of view or occluded. However, the generated sequences exhibit
inconsistencies that do not fully benefit subsequent 3DGS modeling. To address
the challenge of inconsistencies, we introduce a novel scene-grounding guidance
based on rendered sequences from an optimized 3DGS, which tames the diffusion
model to generate consistent sequences. This guidance is training-free and does
not require any fine-tuning of the diffusion model. To facilitate holistic
scene modeling, we also propose a trajectory initialization method. It
effectively identifies regions that are outside the field of view and occluded.
We further design a scheme tailored for 3DGS optimization with generated
sequences. Experiments demonstrate that our method significantly improves upon
the baseline and achieves state-of-the-art performance on challenging
benchmarks.

Comments:
- Accepted by CVPR2025. The project page is available at
  https://zhongyingji.github.io/guidevd-3dgs/

---

## STGA: Selective-Training Gaussian Head Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Hanzhi Guo, Yixiao Chen, Dongye Xiaonuo, Zeyu Tian, Dongdong Weng, Le Luo | cs.GR | [PDF](http://arxiv.org/pdf/2503.05196v1){: .btn .btn-green } |

**Abstract**: We propose selective-training Gaussian head avatars (STGA) to enhance the
details of dynamic head Gaussian. The dynamic head Gaussian model is trained
based on the FLAME parameterized model. Each Gaussian splat is embedded within
the FLAME mesh to achieve mesh-based animation of the Gaussian model. Before
training, our selection strategy calculates the 3D Gaussian splat to be
optimized in each frame. The parameters of these 3D Gaussian splats are
optimized in the training of each frame, while those of the other splats are
frozen. This means that the splats participating in the optimization process
differ in each frame, to improve the realism of fine details. Compared with
network-based methods, our method achieves better results with shorter training
time. Compared with mesh-based methods, our method produces more realistic
details within the same training time. Additionally, the ablation experiment
confirms that our method effectively enhances the quality of details.



---

## GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian  Radiance Fields and Differentiable Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Qianzhong Chen, Jiankai Sun, Naixiang Gao, JunEn Low, Timothy Chen, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2503.03984v1){: .btn .btn-green } |

**Abstract**: Autonomous visual navigation is an essential element in robot autonomy.
Reinforcement learning (RL) offers a promising policy training paradigm.
However existing RL methods suffer from high sample complexity, poor
sim-to-real transfer, and limited runtime adaptability to navigation scenarios
not seen during training. These problems are particularly challenging for
drones, with complex nonlinear and unstable dynamics, and strong dynamic
coupling between control and perception. In this paper, we propose a novel
framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep
reinforcement learning (DDRL) to train vision-based drone navigation policies.
By leveraging high-fidelity 3D scene representations and differentiable
simulation, our method improves sample efficiency and sim-to-real transfer.
Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt
to environmental variations at runtime. Moreover, by curriculum training in a
mixture of different surrounding environments, we achieve in-task
generalization, the ability to solve new instances of a task not seen during
training. Drone hardware experiments demonstrate our method's high training
efficiency compared to state-of-the-art RL methods, zero shot sim-to-real
transfer for real robot deployment without fine tuning, and ability to adapt to
new instances within the same task class (e.g. to fly through a gate at
different locations with different distractors in the environment).



---

## S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Yecong Wan, Mingwen Shao, Yuanshuo Cheng, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2503.04314v1){: .btn .btn-green } |

**Abstract**: In this paper, we aim ambitiously for a realistic yet challenging problem,
namely, how to reconstruct high-quality 3D scenes from sparse low-resolution
views that simultaneously suffer from deficient perspectives and clarity.
Whereas existing methods only deal with either sparse views or low-resolution
observations, they fail to handle such hybrid and complicated scenarios. To
this end, we propose a novel Sparse-view Super-resolution 3D Gaussian Splatting
framework, dubbed S2Gaussian, that can reconstruct structure-accurate and
detail-faithful 3D scenes with only sparse and low-resolution views. The
S2Gaussian operates in a two-stage fashion. In the first stage, we initially
optimize a low-resolution Gaussian representation with depth regularization and
densify it to initialize the high-resolution Gaussians through a tailored
Gaussian Shuffle Split operation. In the second stage, we refine the
high-resolution Gaussians with the super-resolved images generated from both
original sparse views and pseudo-views rendered by the low-resolution
Gaussians. In which a customized blur-free inconsistency modeling scheme and a
3D robust optimization strategy are elaborately designed to mitigate multi-view
inconsistency and eliminate erroneous updates caused by imperfect supervision.
Extensive experiments demonstrate superior results and in particular
establishing new state-of-the-art performances with more consistent geometry
and finer details.

Comments:
- CVPR 2025

---

## Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall, Yumeng Wang, Paul Maria Scheikl, Axel Krieger | cs.CV | [PDF](http://arxiv.org/pdf/2503.04079v1){: .btn .btn-green } |

**Abstract**: Accurate geometric reconstruction of deformable tissues in monocular
endoscopic video remains a fundamental challenge in robot-assisted minimally
invasive surgery. Although recent volumetric and point primitive methods based
on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently
rendered surgical scenes, they still struggle with handling artifact-free tool
occlusions and preserving fine anatomical details. These limitations stem from
unrestricted Gaussian scaling and insufficient surface alignment constraints
during reconstruction. To address these issues, we introduce Surgical Gaussian
Surfels (SGS), which transforms anisotropic point primitives into
surface-aligned elliptical splats by constraining the scale component of the
Gaussian covariance matrix along the view-aligned axis. We predict accurate
surfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled
with locality constraints to handle complex tissue deformations. We use
homodirectional view-space positional gradients to capture fine image details
by splitting Gaussian Surfels in over-reconstructed regions. In addition, we
define surface normals as the direction of the steepest density change within
each Gaussian surfel primitive, enabling accurate normal estimation without
requiring monocular normal priors. We evaluate our method on two in-vivo
surgical datasets, where it outperforms current state-of-the-art methods in
surface geometry, normal map quality, and rendering efficiency, while remaining
competitive in real-time rendering performance. We make our code available at
https://github.com/aloma85/SurgicalGaussianSurfels



---

## GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world  Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Xihan Wang, Dianyi Yang, Yu Gao, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.04034v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting(3DGS) have significantly
improved semantic scene understanding, enabling natural language queries to
localize objects within a scene. However, existing methods primarily focus on
embedding compressed CLIP features to 3D Gaussians, suffering from low object
segmentation accuracy and lack spatial reasoning capabilities. To address these
limitations, we propose GaussianGraph, a novel framework that enhances
3DGS-based scene understanding by integrating adaptive semantic clustering and
scene graph generation. We introduce a "Control-Follow" clustering strategy,
which dynamically adapts to scene scale and feature distribution, avoiding
feature compression and significantly improving segmentation accuracy.
Additionally, we enrich scene representation by integrating object attributes
and spatial relations extracted from 2D foundation models. To address
inaccuracies in spatial relationships, we propose 3D correction modules that
filter implausible relations through spatial consistency verification, ensuring
reliable scene graph construction. Extensive experiments on three datasets
demonstrate that GaussianGraph outperforms state-of-the-art methods in both
semantic segmentation and object grounding tasks, providing a robust solution
for complex scene understanding and interaction.



---

## Instrument-Splatting: Controllable Photorealistic Reconstruction of  Surgical Instruments Using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Shuojue Yang, Zijian Wu, Mingxuan Hong, Qian Li, Daiyun Shen, Septimiu E. Salcudean, Yueming Jin | cs.CV | [PDF](http://arxiv.org/pdf/2503.04082v1){: .btn .btn-green } |

**Abstract**: Real2Sim is becoming increasingly important with the rapid development of
surgical artificial intelligence (AI) and autonomy. In this work, we propose a
novel Real2Sim methodology, \textit{Instrument-Splatting}, that leverages 3D
Gaussian Splatting to provide fully controllable 3D reconstruction of surgical
instruments from monocular surgical videos. To maintain both high visual
fidelity and manipulability, we introduce a geometry pre-training to bind
Gaussian point clouds on part mesh with accurate geometric priors and define a
forward kinematics to control the Gaussians as flexible as real instruments.
Afterward, to handle unposed videos, we design a novel instrument pose tracking
method leveraging semantics-embedded Gaussians to robustly refine per-frame
instrument poses and joint states in a render-and-compare manner, which allows
our instrument Gaussian to accurately learn textures and reach photorealistic
rendering. We validated our method on 2 publicly released surgical videos and 4
videos collected on ex vivo tissues and green screens. Quantitative and
qualitative evaluations demonstrate the effectiveness and superiority of the
proposed method.

Comments:
- 11 pages, 5 figures

---

## GaussianVideo: Efficient Video Representation and Compression by  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Inseo Lee, Youngyoon Choi, Joonseok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.04333v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representation for Videos (NeRV) has introduced a novel
paradigm for video representation and compression, outperforming traditional
codecs. As model size grows, however, slow encoding and decoding speed and high
memory consumption hinder its application in practice. To address these
limitations, we propose a new video representation and compression method based
on 2D Gaussian Splatting to efficiently handle video data. Our proposed
deformable 2D Gaussian Splatting dynamically adapts the transformation of 2D
Gaussians at each frame, significantly reducing memory cost. Equipped with a
multi-plane-based spatiotemporal encoder and a lightweight decoder, it predicts
changes in color, coordinates, and shape of initialized Gaussians, given the
time step. By leveraging temporal gradients, our model effectively captures
temporal redundancy at negligible cost, significantly enhancing video
representation efficiency. Our method reduces GPU memory usage by up to 78.4%,
and significantly expedites video processing, achieving 5.5x faster training
and 12.5x faster decoding compared to the state-of-the-art NeRV methods.



---

## Beyond Existance: Fulfill 3D Reconstructed Scenes with Pseudo Details

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Yifei Gao, Jun Huang, Lei Wang, Ruiting Dai, Jun Cheng | cs.GR | [PDF](http://arxiv.org/pdf/2503.04037v1){: .btn .btn-green } |

**Abstract**: The emergence of 3D Gaussian Splatting (3D-GS) has significantly advanced 3D
reconstruction by providing high fidelity and fast training speeds across
various scenarios. While recent efforts have mainly focused on improving model
structures to compress data volume or reduce artifacts during zoom-in and
zoom-out operations, they often overlook an underlying issue: training sampling
deficiency. In zoomed-in views, Gaussian primitives can appear unregulated and
distorted due to their dilation limitations and the insufficient availability
of scale-specific training samples. Consequently, incorporating pseudo-details
that ensure the completeness and alignment of the scene becomes essential. In
this paper, we introduce a new training method that integrates diffusion models
and multi-scale training using pseudo-ground-truth data. This approach not only
notably mitigates the dilation and zoomed-in artifacts but also enriches
reconstructed scenes with precise details out of existing scenarios. Our method
achieves state-of-the-art performance across various benchmarks and extends the
capabilities of 3D reconstruction beyond training datasets.



---

## LensDFF: Language-enhanced Sparse Feature Distillation for Efficient  Few-Shot Dexterous Manipulation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-05 | Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll | cs.RO | [PDF](http://arxiv.org/pdf/2503.03890v1){: .btn .btn-green } |

**Abstract**: Learning dexterous manipulation from few-shot demonstrations is a significant
yet challenging problem for advanced, human-like robotic systems. Dense
distilled feature fields have addressed this challenge by distilling rich
semantic features from 2D visual foundation models into the 3D domain. However,
their reliance on neural rendering models such as Neural Radiance Fields (NeRF)
or Gaussian Splatting results in high computational costs. In contrast,
previous approaches based on sparse feature fields either suffer from
inefficiencies due to multi-view dependencies and extensive training or lack
sufficient grasp dexterity. To overcome these limitations, we propose
Language-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently
distills view-consistent 2D features onto 3D points using our novel
language-enhanced feature fusion strategy, thereby enabling single-view
few-shot generalization. Based on LensDFF, we further introduce a few-shot
dexterous manipulation framework that integrates grasp primitives into the
demonstrations to generate stable and highly dexterous grasps. Moreover, we
present a real2sim grasp evaluation pipeline for efficient grasp assessment and
hyperparameter tuning. Through extensive simulation experiments based on the
real2sim pipeline and real-world experiments, our approach achieves competitive
grasping performance, outperforming state-of-the-art approaches.

Comments:
- 8 pages

---

## NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian  Splatting Based on Thermodynamics


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-05 | Kun Yang, Yuxiang Liu, Zeyu Cui, Yu Liu, Maojun Zhang, Shen Yan, Qing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.03115v1){: .btn .btn-green } |

**Abstract**: Thermal infrared imaging offers the advantage of all-weather capability,
enabling non-intrusive measurement of an object's surface temperature.
Consequently, thermal infrared images are employed to reconstruct 3D models
that accurately reflect the temperature distribution of a scene, aiding in
applications such as building monitoring and energy management. However,
existing approaches predominantly focus on static 3D reconstruction for a
single time period, overlooking the impact of environmental factors on thermal
radiation and failing to predict or analyze temperature variations over time.
To address these challenges, we propose the NTR-Gaussian method, which treats
temperature as a form of thermal radiation, incorporating elements like
convective heat transfer and radiative heat dissipation. Our approach utilizes
neural networks to predict thermodynamic parameters such as emissivity,
convective heat transfer coefficient, and heat capacity. By integrating these
predictions, we can accurately forecast thermal temperatures at various times
throughout a nighttime scene. Furthermore, we introduce a dynamic dataset
specifically for nighttime thermal imagery. Extensive experiments and
evaluations demonstrate that NTR-Gaussian significantly outperforms comparison
methods in thermal reconstruction, achieving a predicted temperature error
within 1 degree Celsius.

Comments:
- IEEE Conference on Computer Vision and Pattern Recognition 2025

---

## Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic  Guidance from Dense Novel Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yingji Zhong, Kaichen Zhou, Zhihao Li, Lanqing Hong, Zhenguo Li, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.02230v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable capabilities for
photorealistic novel view synthesis. One major deficiency of NeRF is that dense
inputs are typically required, and the rendering quality will drop drastically
given sparse inputs. In this paper, we highlight the effectiveness of rendered
semantics from dense novel views, and show that rendered semantics can be
treated as a more robust form of augmented data than rendered RGB. Our method
enhances NeRF's performance by incorporating guidance derived from the rendered
semantics. The rendered semantic guidance encompasses two levels: the
supervision level and the feature level. The supervision-level guidance
incorporates a bi-directional verification module that decides the validity of
each rendered semantic label, while the feature-level guidance integrates a
learnable codebook that encodes semantic-aware information, which is queried by
each point via the attention mechanism to obtain semantic-relevant predictions.
The overall semantic guidance is embedded into a self-improved pipeline. We
also introduce a more challenging sparse-input indoor benchmark, where the
number of inputs is limited to as few as 6. Experiments demonstrate the
effectiveness of our method and it exhibits superior performance compared to
existing approaches.



---

## Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yan Miao, Will Shen, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2503.02198v1){: .btn .btn-green } |

**Abstract**: We present the first framework demonstrating zero-shot sim-to-real transfer
of visual control policies learned in a Neural Radiance Field (NeRF)
environment for quadrotors to fly through racing gates. Robust transfer from
simulation to real flight poses a major challenge, as standard simulators often
lack sufficient visual fidelity. To address this, we construct a photorealistic
simulation environment of quadrotor racing tracks, called FalconGym, which
provides effectively unlimited synthetic images for training. Within FalconGym,
we develop a pipelined approach for crossing gates that combines (i) a Neural
Pose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor
poses from single-frame RGB images and IMU data, and (ii) a
self-attention-based multi-modal controller that adaptively integrates visual
features and pose estimation. This multi-modal design compensates for
perception noise and intermittent gate visibility. We train this controller
purely in FalconGym with imitation learning and deploy the resulting policy to
real hardware with no additional fine-tuning. Simulation experiments on three
distinct tracks (circle, U-turn and figure-8) demonstrate that our controller
outperforms a vision-only state-of-the-art baseline in both success rate and
gate-crossing accuracy. In 30 live hardware flights spanning three tracks and
120 gates, our controller achieves a 95.8% success rate and an average error of
just 10 cm when flying through 38 cm-radius gates.



---

## DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Haoyuan Li, Ziqin Ye, Yue Hao, Weiyang Lin, Chao Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.02223v1){: .btn .btn-green } |

**Abstract**: Accurate object perception is essential for robotic applications such as
object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM
system that seamlessly integrates object pose estimation and reconstruction. We
employ 3D Gaussian Splatting for high-fidelity object reconstruction and
leverage quadrics for precise object pose estimation. Both of them management
is handled on the CPU, while optimization is performed on the GPU,
significantly improving system efficiency. By associating objects with unique
IDs, our system enables rapid object extraction from the scene. Extensive
experimental results on object reconstruction and pose estimation demonstrate
that DQO-MAP achieves outstanding performance in terms of precision,
reconstruction quality, and computational efficiency. The code and dataset are
available at: https://github.com/LiHaoy-ux/DQO-MAP.



---

## 2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Qipeng Yan, Mingyang Sun, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.02452v1){: .btn .btn-green } |

**Abstract**: Real-time rendering of high-fidelity and animatable avatars from monocular
videos remains a challenging problem in computer vision and graphics. Over the
past few years, the Neural Radiance Field (NeRF) has made significant progress
in rendering quality but behaves poorly in run-time performance due to the low
efficiency of volumetric rendering. Recently, methods based on 3D Gaussian
Splatting (3DGS) have shown great potential in fast training and real-time
rendering. However, they still suffer from artifacts caused by inaccurate
geometry. To address these problems, we propose 2DGS-Avatar, a novel approach
based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars
with high-fidelity and fast training performance. Given monocular RGB videos as
input, our method generates an avatar that can be driven by poses and rendered
in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the
advantages of fast training and rendering while also capturing detailed,
dynamic, and photo-realistic appearances. We conduct abundant experiments on
popular datasets such as AvatarRex and THuman4.0, demonstrating impressive
performance in both qualitative and quantitative metrics.

Comments:
- ICVRV 2024

---

## Tracking-Aware Deformation Field Estimation for Non-rigid 3D  Reconstruction in Robotic Surgeries

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Zeqing Wang, Han Fang, Yihong Xu, Yutong Ban | cs.CV | [PDF](http://arxiv.org/pdf/2503.02558v1){: .btn .btn-green } |

**Abstract**: Minimally invasive procedures have been advanced rapidly by the robotic
laparoscopic surgery. The latter greatly assists surgeons in sophisticated and
precise operations with reduced invasiveness. Nevertheless, it is still safety
critical to be aware of even the least tissue deformation during
instrument-tissue interactions, especially in 3D space. To address this, recent
works rely on NeRF to render 2D videos from different perspectives and
eliminate occlusions. However, most of the methods fail to predict the accurate
3D shapes and associated deformation estimates robustly. Differently, we
propose Tracking-Aware Deformation Field (TADF), a novel framework which
reconstructs the 3D mesh along with the 3D tissue deformation simultaneously.
It first tracks the key points of soft tissue by a foundation vision model,
providing an accurate 2D deformation field. Then, the 2D deformation field is
smoothly incorporated with a neural implicit reconstruction network to obtain
tissue deformation in the 3D space. Finally, we experimentally demonstrate that
the proposed method provides more accurate deformation estimation compared with
other 3D neural reconstruction methods in two public datasets.



---

## LiteGS: A High-Performance Modular Framework for Gaussian Splatting  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Kaimin Liao | cs.CV | [PDF](http://arxiv.org/pdf/2503.01199v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has emerged as a powerful technique for reconstruction of
3D scenes in computer graphics and vision. However, conventional
implementations often suffer from inefficiencies, limited flexibility, and high
computational overhead, which constrain their adaptability to diverse
applications. In this paper, we present LiteGS,a high-performance and modular
framework that enhances both the efficiency and usability of Gaussian
splatting. LiteGS achieves a 3.4x speedup over the original 3DGS implementation
while reducing GPU memory usage by approximately 30%. Its modular design
decomposes the splatting process into multiple highly optimized operators, and
it provides dual API support via a script-based interface and a CUDA-based
interface. The script-based interface, in combination with autograd, enables
rapid prototyping and straightforward customization of new ideas, while the
CUDA-based interface delivers optimal training speeds for performance-critical
applications. LiteGS retains the core algorithm of 3DGS, ensuring
compatibility. Comprehensive experiments on the Mip-NeRF 360 dataset
demonstrate that LiteGS accelerates training without compromising accuracy,
making it an ideal solution for both rapid prototyping and production
environments.



---

## Category-level Meta-learned NeRF Priors for Efficient Object Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez | cs.CV | [PDF](http://arxiv.org/pdf/2503.01582v2){: .btn .btn-green } |

**Abstract**: In 3D object mapping, category-level priors enable efficient object
reconstruction and canonical pose estimation, requiring only a single prior per
semantic category (e.g., chair, book, laptop). Recently, DeepSDF has
predominantly been used as a category-level shape prior, but it struggles to
reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs
capture fine details but have yet to be effectively integrated with
category-level priors in a real-time multi-object mapping framework. To bridge
this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper
that integrates category-level priors with object-level NeRFs to enhance
reconstruction efficiency while enabling canonical object pose estimation.
PRENOM gets to know objects on a first-name basis by meta-learning on synthetic
reconstruction tasks generated from open-source shape datasets. To account for
object category variations, it employs a multi-objective genetic algorithm to
optimize the NeRF architecture for each category, balancing reconstruction
quality and training time. Additionally, prior-based probabilistic ray sampling
directs sampling toward expected object regions, accelerating convergence and
improving reconstruction quality under constrained resources. Experimental
results on a low-end GPU highlight the ability of PRENOM to achieve
high-quality reconstructions while maintaining computational feasibility.
Specifically, comparisons with prior-free NeRF-based approaches on a synthetic
dataset show a 21% lower Chamfer distance, demonstrating better reconstruction
quality. Furthermore, evaluations against other approaches using shape priors
on a noisy real-world dataset indicate a 13% improvement averaged across all
reconstruction metrics, and comparable pose and size estimation accuracy, while
being trained for 5x less time.



---

## Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling | cs.CV | [PDF](http://arxiv.org/pdf/2503.01774v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D
reconstruction and novel-view synthesis task. However, achieving photorealistic
rendering from extreme novel viewpoints remains challenging, as artifacts
persist across representations. In this work, we introduce Difix3D+, a novel
pipeline designed to enhance 3D reconstruction and novel-view synthesis through
single-step diffusion models. At the core of our approach is Difix, a
single-step image diffusion model trained to enhance and remove artifacts in
rendered novel views caused by underconstrained regions of the 3D
representation. Difix serves two critical roles in our pipeline. First, it is
used during the reconstruction phase to clean up pseudo-training views that are
rendered from the reconstruction and then distilled back into 3D. This greatly
enhances underconstrained regions and improves the overall 3D representation
quality. More importantly, Difix also acts as a neural enhancer during
inference, effectively removing residual artifacts arising from imperfect 3D
supervision and the limited capacity of current reconstruction models. Difix3D+
is a general solution, a single model compatible with both NeRF and 3DGS
representations, and it achieves an average 2$\times$ improvement in FID score
over baselines while maintaining 3D consistency.

Comments:
- CVPR 2025

---

## Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, Mohamed Sayed | cs.CV | [PDF](http://arxiv.org/pdf/2503.02009v1){: .btn .btn-green } |

**Abstract**: Exploring real-world spaces using novel-view synthesis is fun, and
reimagining those worlds in a different style adds another layer of excitement.
Stylized worlds can also be used for downstream tasks where there is limited
training data and a need to expand a model's training distribution. Most
current novel-view synthesis stylization techniques lack the ability to
convincingly change geometry. This is because any geometry change requires
increased style strength which is often capped for stylization stability and
consistency. In this work, we propose a new autoregressive 3D Gaussian
Splatting stylization method. As part of this method, we contribute a new RGBD
diffusion model that allows for strength control over appearance and shape
stylization. To ensure consistency across stylized frames, we use a combination
of novel depth-guided cross attention, feature injection, and a Warp ControlNet
conditioned on composite frames for guiding the stylization of new frames. We
validate our method via extensive qualitative results, quantitative
experiments, and a user study. Code will be released online.



---

## OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for  Object-Level Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Dianyi Yang, Yu Gao, Xihan Wang, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.01646v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have significantly improved the
efficiency and quality of dense semantic SLAM. However, previous methods are
generally constrained by limited-category pre-trained classifiers and implicit
semantic representation, which hinder their performance in open-set scenarios
and restrict 3D object-level scene understanding. To address these issues, we
propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian
representation to perform dense semantic SLAM in open-set environments. Our
system integrates explicit semantic labels derived from 2D foundational models
into the 3D Gaussian framework, facilitating robust 3D object-level scene
understanding. We introduce Gaussian Voting Splatting to enable fast 2D label
map rendering and scene updating. Additionally, we propose a Confidence-based
2D Label Consensus method to ensure consistent labeling across multiple views.
Furthermore, we employ a Segmentation Counter Pruning strategy to improve the
accuracy of semantic scene representation. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method
in scene understanding, tracking, and mapping, achieving 10 times faster
semantic rendering and 2 times lower storage costs compared to existing
methods. Project page: https://young-bit.github.io/opengs-github.github.io/.



---

## FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with  Sparse and Dense Map Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Yansong Xu, Junlin Li, Wei Zhang, Siyu Chen, Shengyong Zhang, Yuquan Leng, Weijia Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.01109v1){: .btn .btn-green } |

**Abstract**: 3D gaussian splatting has advanced simultaneous localization and mapping
(SLAM) technology by enabling real-time positioning and the construction of
high-fidelity maps. However, the uncertainty in gaussian position and
initialization parameters introduces challenges, often requiring extensive
iterative convergence and resulting in redundant or insufficient gaussian
representations. To address this, we introduce a novel adaptive densification
method based on Fourier frequency domain analysis to establish gaussian priors
for rapid convergence. Additionally, we propose constructing independent and
unified sparse and dense maps, where a sparse map supports efficient tracking
via Generalized Iterative Closest Point (GICP) and a dense map creates
high-fidelity visual representations. This is the first SLAM system leveraging
frequency domain analysis to achieve high-quality gaussian mapping in
real-time. Experimental results demonstrate an average frame rate of 36 FPS on
Replica and TUM RGB-D datasets, achieving competitive accuracy in both
localization and mapping.



---

## Data Augmentation for NeRFs in the Low Data Limit

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Ayush Gaggar, Todd D. Murphey | cs.CV | [PDF](http://arxiv.org/pdf/2503.02092v1){: .btn .btn-green } |

**Abstract**: Current methods based on Neural Radiance Fields fail in the low data limit,
particularly when training on incomplete scene data. Prior works augment
training data only in next-best-view applications, which lead to hallucinations
and model collapse with sparse data. In contrast, we propose adding a set of
views during training by rejection sampling from a posterior uncertainty
distribution, generated by combining a volumetric uncertainty estimator with
spatial coverage. We validate our results on partially observed scenes; on
average, our method performs 39.9% better with 87.5% less variability across
established scene reconstruction benchmarks, as compared to state of the art
baselines. We further demonstrate that augmenting the training set by sampling
from any distribution leads to better, more consistent scene reconstruction in
sparse environments. This work is foundational for robotic tasks where
augmenting a dataset with informative data is critical in resource-constrained,
a priori unknown environments. Videos and source code are available at
https://murpheylab.github.io/low-data-nerf/.

Comments:
- To be published in 2025 IEEE International Conference on Robotics and
  Automation (ICRA 2025)

---

## Evolving High-Quality Rendering and Reconstruction in a Unified  Framework with Contribution-Adaptive Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao | cs.CV | [PDF](http://arxiv.org/pdf/2503.00881v1){: .btn .btn-green } |

**Abstract**: Representing 3D scenes from multiview images is a core challenge in computer
vision and graphics, which requires both precise rendering and accurate
reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant
attention for its high-quality rendering and fast inference speed. Yet, due to
the unstructured and irregular nature of Gaussian point clouds, ensuring
accurate geometry reconstruction remains difficult. Existing methods primarily
focus on geometry regularization, with common approaches including
primitive-based and dual-model frameworks. However, the former suffers from
inherent conflicts between rendering and reconstruction, while the latter is
computationally and storage-intensive. To address these challenges, we propose
CarGS, a unified model leveraging Contribution-adaptive regularization to
achieve simultaneous, high-quality rendering and surface reconstruction. The
essence of our framework is learning adaptive contribution for Gaussian
primitives by squeezing the knowledge from geometry regularization into a
compact MLP. Additionally, we introduce a geometry-guided densification
strategy with clues from both normals and Signed Distance Fields (SDF) to
improve the capability of capturing high-frequency details. Our design improves
the mutual learning of the two tasks, meanwhile its unified structure does not
require separate models as in dual-model based approaches, guaranteeing
efficiency. Extensive experiments demonstrate the ability to achieve
state-of-the-art (SOTA) results in both rendering fidelity and reconstruction
accuracy while maintaining real-time speed and minimal storage size.



---

## PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency  Recovery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | BoCheng Li, WenJuan Zhang, Bing Zhang, YiLing Yao, YaNing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.00848v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D GS) achieves impressive results in novel view
synthesis for small, single-object scenes through Gaussian ellipsoid
initialization and adaptive density control. However, when applied to
large-scale remote sensing scenes, 3D GS faces challenges: the point clouds
generated by Structure-from-Motion (SfM) are often sparse, and the inherent
smoothing behavior of 3D GS leads to over-reconstruction in high-frequency
regions, where have detailed textures and color variations. This results in the
generation of large, opaque Gaussian ellipsoids that cause gradient artifacts.
Moreover, the simultaneous optimization of both geometry and texture may lead
to densification of Gaussian ellipsoids at incorrect geometric locations,
resulting in artifacts in other views. To address these issues, we propose
PSRGS, a progressive optimization scheme based on spectral residual maps.
Specifically, we create a spectral residual significance map to separate
low-frequency and high-frequency regions. In the low-frequency region, we apply
depth-aware and depth-smooth losses to initialize the scene geometry with low
threshold. For the high-frequency region, we use gradient features with higher
threshold to split and clone ellipsoids, refining the scene. The sampling rate
is determined by feature responses and gradient loss. Finally, we introduce a
pre-trained network that jointly computes perceptual loss from multiple views,
ensuring accurate restoration of high-frequency details in both Gaussian
ellipsoids geometry and color. We conduct experiments on multiple datasets to
assess the effectiveness of our method, which demonstrates competitive
rendering quality, especially in recovering texture details in high-frequency
regions.



---

## DreamPrinting: Volumetric Printing Primitives for High-Fidelity 3D  Printing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Youjia Wang, Ruixiang Cao, Teng Xu, Yifei Liu, Dong Zhang, Yiwen Wu, Jingyi Yu | cs.GR | [PDF](http://arxiv.org/pdf/2503.00887v1){: .btn .btn-green } |

**Abstract**: Translating the rich visual fidelity of volumetric rendering techniques into
physically realizable 3D prints remains an open challenge. We introduce
DreamPrinting, a novel pipeline that transforms radiance-based volumetric
representations into explicit, material-centric Volumetric Printing Primitives
(VPPs). While volumetric rendering primitives (e.g., NeRF) excel at capturing
intricate geometry and appearance, they lack the physical constraints necessary
for real-world fabrication, such as pigment compatibility and material density.
DreamPrinting addresses these challenges by integrating the Kubelka-Munk model
with a spectrophotometric calibration process to characterize and mix pigments
for accurate reproduction of color and translucency. The result is a
continuous-to-discrete mapping that determines optimal pigment concentrations
for each voxel, ensuring fidelity to both geometry and optical properties. A 3D
stochastic halftoning procedure then converts these concentrations into
printable labels, enabling fine-grained control over opacity, texture, and
color gradients. Our evaluations show that DreamPrinting achieves exceptional
detail in reproducing semi-transparent structures-such as fur, leaves, and
clouds-while outperforming traditional surface-based methods in managing
translucency and internal consistency. Furthermore, by seamlessly integrating
VPPs with cutting-edge 3D generation techniques, DreamPrinting expands the
potential for complex, high-quality volumetric prints, providing a robust
framework for printing objects that closely mirror their digital origins.



---

## Enhancing Monocular 3D Scene Completion with Diffusion Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Changlin Song, Jiaqi Wang, Liyun Zhu, He Weng | cs.GR | [PDF](http://arxiv.org/pdf/2503.00726v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction is essential for applications in virtual reality,
robotics, and autonomous driving, enabling machines to understand and interact
with complex environments. Traditional 3D Gaussian Splatting techniques rely on
images captured from multiple viewpoints to achieve optimal performance, but
this dependence limits their use in scenarios where only a single image is
available. In this work, we introduce FlashDreamer, a novel approach for
reconstructing a complete 3D scene from a single image, significantly reducing
the need for multi-view inputs. Our approach leverages a pre-trained
vision-language model to generate descriptive prompts for the scene, guiding a
diffusion model to produce images from various perspectives, which are then
fused to form a cohesive 3D reconstruction. Extensive experiments show that our
method effectively and robustly expands single-image inputs into a
comprehensive 3D scene, extending monocular 3D reconstruction capabilities
without further training. Our code is available
https://github.com/CharlieSong1999/FlashDreamer/tree/main.

Comments:
- All authors had equal contribution

---

## Vid2Fluid: 3D Dynamic Fluid Assets from Single-View Videos with  Generative Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Zhiwei Zhao, Alan Zhao, Minchen Li, Yixin Hu | cs.GR | [PDF](http://arxiv.org/pdf/2503.00868v1){: .btn .btn-green } |

**Abstract**: The generation of 3D content from single-view images has been extensively
studied, but 3D dynamic scene generation with physical consistency from videos
remains in its early stages. We propose a novel framework leveraging generative
3D Gaussian Splatting (3DGS) models to extract 3D dynamic fluid objects from
single-view videos. The fluid geometry represented by 3DGS is initially
generated from single-frame images, then denoised, densified, and aligned
across frames. We estimate the fluid surface velocity using optical flow and
compute the mainstream of the fluid to refine it. The 3D volumetric velocity
field is then derived from the enclosed surface. The velocity field is then
converted into a divergence-free, grid-based representation, enabling the
optimization of simulation parameters through its differentiability across
frames. This process results in simulation-ready fluid assets with physical
dynamics closely matching those observed in the source video. Our approach is
applicable to various fluid types, including gas, liquid, and viscous fluids,
and allows users to edit the output geometry or extend movement durations
seamlessly. Our automatic method for creating 3D dynamic fluid assets from
single-view videos, easily obtainable from the internet, shows great potential
for generating large-scale 3D fluid assets at a low cost.



---

## DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Liao Shen, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy | cs.CV | [PDF](http://arxiv.org/pdf/2503.00746v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable
success in representing 3D scenes and generating high-quality, novel views in
real-time. However, 3D-GS and its variants assume that input images are
captured based on pinhole imaging and are fully in focus. This assumption
limits their applicability, as real-world images often feature shallow
depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable
depth-of-field method for 3D-GS. We develop a lens-based imaging model based on
geometric optics principles to control DoF effects. To ensure accurate scene
geometry, we incorporate depth priors adjusted per scene, and we apply
defocus-to-focus adaptation to minimize the gap in the circle of confusion. We
also introduce a synthetic dataset to assess refocusing capabilities and the
model's ability to learn precise lens parameters. Our framework is customizable
and supports various interactive applications. Extensive experiments confirm
the effectiveness of our method. Our project is available at
https://dof-gaussian.github.io.

Comments:
- CVPR 2025

---

## CAT-3DGS: A Context-Adaptive Triplane Approach to  Rate-Distortion-Optimized 3DGS Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, Wen-Hsiao Peng | cs.CV | [PDF](http://arxiv.org/pdf/2503.00357v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D
representation. Much research has been focused on reducing its storage
requirements and memory footprint. However, the needs to compress and transmit
the 3DGS representation to the remote side are overlooked. This new application
calls for rate-distortion-optimized 3DGS compression. How to quantize and
entropy encode sparse Gaussian primitives in the 3D space remains largely
unexplored. Few early attempts resort to the hyperprior framework from learned
image compression. But, they fail to utilize fully the inter and intra
correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work,
termed CAT-3DGS, introduces a context-adaptive triplane approach to their
rate-distortion-optimized coding. It features multi-scale triplanes, oriented
according to the principal axes of Gaussian primitives in the 3D space, to
capture their inter correlation (i.e. spatial correlation) for spatial
autoregressive coding in the projected 2D planes. With these triplanes serving
as the hyperprior, we further perform channel-wise autoregressive coding to
leverage the intra correlation within each individual Gaussian primitive. Our
CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively
skips from coding those Gaussian primitives that potentially have little impact
on the rendering quality. When trained end-to-end to strike a good
rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art
compression performance on the commonly used real-world datasets.

Comments:
- Accepted for Publication in International Conference on Learning
  Representations (ICLR)

---

## Abstract Rendering: Computing All that is Seen in Gaussian Splat Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yangge Li, Chenxi Ji, Xiangru Zhong, Huan Zhang, Sayan Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2503.00308v2){: .btn .btn-green } |

**Abstract**: We introduce abstract rendering, a method for computing a set of images by
rendering a scene from a continuously varying range of camera positions. The
resulting abstract image-which encodes an infinite collection of possible
renderings-is represented using constraints on the image matrix, enabling
rigorous uncertainty propagation through the rendering process. This capability
is particularly valuable for the formal verification of vision-based autonomous
systems and other safety-critical applications. Our approach operates on
Gaussian splat scenes, an emerging representation in computer vision and
robotics. We leverage efficient piecewise linear bound propagation to abstract
fundamental rendering operations, while addressing key challenges that arise in
matrix inversion and depth sorting-two operations not directly amenable to
standard approximations. To handle these, we develop novel linear relational
abstractions that maintain precision while ensuring computational efficiency.
These abstractions not only power our abstract rendering algorithm but also
provide broadly applicable tools for other rendering problems. Our
implementation, AbstractSplat, is optimized for scalability, handling up to
750k Gaussians while allowing users to balance memory and runtime through tile
and batch-based computation. Compared to the only existing abstract image
method for mesh-based scenes, AbstractSplat achieves 2-14x speedups while
preserving precision. Our results demonstrate that continuous camera motion,
rotations, and scene variations can be rigorously analyzed at scale, making
abstract rendering a powerful tool for uncertainty-aware vision applications.



---

## GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation  Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Runyi Li, Xuanyu Zhang, Chuhan Tong, Zhipei Xu, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.00531v1){: .btn .btn-green } |

**Abstract**: With the advancement of AIGC technologies, the modalities generated by models
have expanded from images and videos to 3D objects, leading to an increasing
number of works focused on 3D Gaussian Splatting (3DGS) generative models.
Existing research on copyright protection for generative models has primarily
concentrated on watermarking in image and text modalities, with little
exploration into the copyright protection of 3D object generative models. In
this paper, we propose the first bit watermarking framework for 3DGS generative
models, named GaussianSeal, to enable the decoding of bits as copyright
identifiers from the rendered outputs of generated 3DGS. By incorporating
adaptive bit modulation modules into the generative model and embedding them
into the network blocks in an adaptive way, we achieve high-precision bit
decoding with minimal training overhead while maintaining the fidelity of the
model's outputs. Experiments demonstrate that our method outperforms
post-processing watermarking approaches for 3DGS objects, achieving superior
performance of watermark decoding accuracy and preserving the quality of the
generated results.



---

## Seeing A 3D World in A Grain of Sand

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yufan Zhang, Yu Ji, Yu Guo, Jinwei Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.00260v1){: .btn .btn-green } |

**Abstract**: We present a snapshot imaging technique for recovering 3D surrounding views
of miniature scenes. Due to their intricacy, miniature scenes with objects
sized in millimeters are difficult to reconstruct, yet miniatures are common in
life and their 3D digitalization is desirable. We design a catadioptric imaging
system with a single camera and eight pairs of planar mirrors for snapshot 3D
reconstruction from a dollhouse perspective. We place paired mirrors on nested
pyramid surfaces for capturing surrounding multi-view images in a single shot.
Our mirror design is customizable based on the size of the scene for optimized
view coverage. We use the 3D Gaussian Splatting (3DGS) representation for scene
reconstruction and novel view synthesis. We overcome the challenge posed by our
sparse view input by integrating visual hull-derived depth constraint. Our
method demonstrates state-of-the-art performance on a variety of synthetic and
real miniature scenes.



---

## Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic  Pick-and-Place Setups

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Nicholas Pfaff, Evelyn Fu, Jeremy Binagia, Phillip Isola, Russ Tedrake | cs.RO | [PDF](http://arxiv.org/pdf/2503.00370v1){: .btn .btn-green } |

**Abstract**: Simulating object dynamics from real-world perception shows great promise for
digital twins and robotic manipulation but often demands labor-intensive
measurements and expertise. We present a fully automated Real2Sim pipeline that
generates simulation-ready assets for real-world objects through robotic
interaction. Using only a robot's joint torque sensors and an external camera,
the pipeline identifies visual geometry, collision geometry, and physical
properties such as inertial parameters. Our approach introduces a general
method for extracting high-quality, object-centric meshes from photometric
reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing
alpha-transparent training while explicitly distinguishing foreground
occlusions from background subtraction. We validate the full pipeline through
extensive experiments, demonstrating its effectiveness across diverse objects.
By eliminating the need for manual intervention or environment modifications,
our pipeline can be integrated directly into existing pick-and-place setups,
enabling scalable and efficient dataset creation.


