---
layout: default
title: March 2025
parent: Papers
nav_order: 202503
---

<!---metadata--->


## SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Jiahui Zhang, Fangneng Zhan, Ling Shao, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.07476v1){: .btn .btn-green } |

**Abstract**: Anchor-based 3D Gaussian splatting (3D-GS) exploits anchor features in 3D
Gaussian prediction, which has achieved impressive 3D rendering quality with
reduced Gaussian redundancy. On the other hand, it often encounters the dilemma
among anchor features, model size, and rendering quality - large anchor
features lead to large 3D models and high-quality rendering whereas reducing
anchor features degrades Gaussian attribute prediction which leads to clear
artifacts in the rendered textures and geometries. We design SOGS, an
anchor-based 3D-GS technique that introduces second-order anchors to achieve
superior rendering quality and reduced anchor features and model size
simultaneously. Specifically, SOGS incorporates covariance-based second-order
statistics and correlation across feature dimensions to augment features within
each anchor, compensating for the reduced feature size and improving rendering
quality effectively. In addition, it introduces a selective gradient loss to
enhance the optimization of scene textures and scene geometries, leading to
high-quality rendering with small anchor features. Extensive experiments over
multiple widely adopted benchmarks show that SOGS achieves superior rendering
quality in novel view synthesis with clearly reduced model size.

Comments:
- Accepted by CVPR 2025

---

## DirectTriGS: Triplane-based Gaussian Splatting Field Representation for  3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Xiaoliang Ju, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2503.06900v1){: .btn .btn-green } |

**Abstract**: We present DirectTriGS, a novel framework designed for 3D object generation
with Gaussian Splatting (GS). GS-based rendering for 3D content has gained
considerable attention recently. However, there has been limited exploration in
directly generating 3D Gaussians compared to traditional generative modeling
approaches. The main challenge lies in the complex data structure of GS
represented by discrete point clouds with multiple channels. To overcome this
challenge, we propose employing the triplane representation, which allows us to
represent Gaussian Splatting as an image-like continuous field. This
representation effectively encodes both the geometry and texture information,
enabling smooth transformation back to Gaussian point clouds and rendering into
images by a TriRenderer, with only 2D supervisions. The proposed TriRenderer is
fully differentiable, so that the rendering loss can supervise both texture and
geometry encoding. Furthermore, the triplane representation can be compressed
using a Variational Autoencoder (VAE), which can subsequently be utilized in
latent diffusion to generate 3D objects. The experiments demonstrate that the
proposed generation framework can produce high-quality 3D object geometry and
rendering results in the text-to-3D task.

Comments:
- Accepted by CVPR 2025

---

## EigenGS Representation: From Eigenspace to Gaussian Image Space


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Lo-Wei Tai, Ching-En Li, Cheng-Lin Chen, Chih-Jung Tsai, Hwann-Tzong Chen, Tyng-Luh Liu | cs.CV | [PDF](http://arxiv.org/pdf/2503.07446v1){: .btn .btn-green } |

**Abstract**: Principal Component Analysis (PCA), a classical dimensionality reduction
technique, and 2D Gaussian representation, an adaptation of 3D Gaussian
Splatting for image representation, offer distinct approaches to modeling
visual data. We present EigenGS, a novel method that bridges these paradigms
through an efficient transformation pipeline connecting eigenspace and
image-space Gaussian representations. Our approach enables instant
initialization of Gaussian parameters for new images without requiring
per-image optimization from scratch, dramatically accelerating convergence.
EigenGS introduces a frequency-aware learning mechanism that encourages
Gaussians to adapt to different scales, effectively modeling varied spatial
frequencies and preventing artifacts in high-resolution reconstruction.
Extensive experiments demonstrate that EigenGS not only achieves superior
reconstruction quality compared to direct 2D Gaussian fitting but also reduces
necessary parameter count and training time. The results highlight EigenGS's
effectiveness and generalization ability across images with varying resolutions
and diverse categories, making Gaussian-based image representation both
high-quality and viable for real-time applications.



---

## ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Konstantinos D. Polyzos, Athanasios Bacharis, Saketh Madhuvarasu, Nikos Papanikolopoulos, Tara Javidi | cs.CV | [PDF](http://arxiv.org/pdf/2503.06859v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting (GS) along with its extensions and variants provides
outstanding performance in real-time scene rendering while meeting reduced
storage demands and computational efficiency. While the selection of 2D images
capturing the scene of interest is crucial for the proper initialization and
training of GS, hence markedly affecting the rendering performance, prior works
rely on passively and typically densely selected 2D images. In contrast, this
paper proposes `ActiveInitSplat', a novel framework for active selection of
training images for proper initialization and training of GS. ActiveInitSplat
relies on density and occupancy criteria of the resultant 3D scene
representation from the selected 2D images, to ensure that the latter are
captured from diverse viewpoints leading to better scene coverage and that the
initialized Gaussian functions are well aligned with the actual 3D structure.
Numerical tests on well-known simulated and real environments demonstrate the
merits of ActiveInitSplat resulting in significant GS rendering performance
improvement over passive GS baselines, in the widely adopted LPIPS, SSIM, and
PSNR metrics.



---

## CATPlan: Loss-based Collision Prediction in End-to-End Autonomous  Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Ziliang Xiong, Shipeng Liu, Nathaniel Helgesen, Joakim Johnander, Per-Erik Forssen | cs.RO | [PDF](http://arxiv.org/pdf/2503.07425v1){: .btn .btn-green } |

**Abstract**: In recent years, there has been increased interest in the design, training,
and evaluation of end-to-end autonomous driving (AD) systems. One often
overlooked aspect is the uncertainty of planned trajectories predicted by these
systems, despite awareness of their own uncertainty being key to achieve safety
and robustness. We propose to estimate this uncertainty by adapting loss
prediction from the uncertainty quantification literature. To this end, we
introduce a novel light-weight module, dubbed CATPlan, that is trained to
decode motion and planning embeddings into estimates of the collision loss used
to partially supervise end-to-end AD systems. During inference, these estimates
are interpreted as collision risk. We evaluate CATPlan on the safety-critical,
nerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detect
collisions with a $54.8\%$ relative improvement to average precision over a
GMM-based baseline in which the predicted trajectory is compared to the
forecasted trajectories of other road users. Our findings indicate that the
addition of CATPlan can lead to safer end-to-end AD systems and hope that our
work will spark increased interest in uncertainty quantification for such
systems.



---

## Frequency-Aware Density Control via Reparameterization for High-Quality  Rendering of 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Zhaojie Zeng, Yuesong Wang, Lili Ju, Tao Guan | cs.CV | [PDF](http://arxiv.org/pdf/2503.07000v1){: .btn .btn-green } |

**Abstract**: By adaptively controlling the density and generating more Gaussians in
regions with high-frequency information, 3D Gaussian Splatting (3DGS) can
better represent scene details. From the signal processing perspective,
representing details usually needs more Gaussians with relatively smaller
scales. However, 3DGS currently lacks an explicit constraint linking the
density and scale of 3D Gaussians across the domain, leading to 3DGS using
improper-scale Gaussians to express frequency information, resulting in the
loss of accuracy. In this paper, we propose to establish a direct relation
between density and scale through the reparameterization of the scaling
parameters and ensure the consistency between them via explicit constraints
(i.e., density responds well to changes in frequency). Furthermore, we develop
a frequency-aware density control strategy, consisting of densification and
deletion, to improve representation quality with fewer Gaussians. A dynamic
threshold encourages densification in high-frequency regions, while a
scale-based filter deletes Gaussians with improper scale. Experimental results
on various datasets demonstrate that our method outperforms existing
state-of-the-art methods quantitatively and qualitatively.

Comments:
- Accepted to AAAI2025

---

## All That Glitters Is Not Gold: Key-Secured 3D Secrets within 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Yan Ren, Shilin Lu, Adams Wai-Kin Kong | cs.GR | [PDF](http://arxiv.org/pdf/2503.07191v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have revolutionized scene
reconstruction, opening new possibilities for 3D steganography by hiding 3D
secrets within 3D covers. The key challenge in steganography is ensuring
imperceptibility while maintaining high-fidelity reconstruction. However,
existing methods often suffer from detectability risks and utilize only
suboptimal 3DGS features, limiting their full potential. We propose a novel
end-to-end key-secured 3D steganography framework (KeySS) that jointly
optimizes a 3DGS model and a key-secured decoder for secret reconstruction. Our
approach reveals that Gaussian features contribute unequally to secret hiding.
The framework incorporates a key-controllable mechanism enabling multi-secret
hiding and unauthorized access prevention, while systematically exploring
optimal feature update to balance fidelity and security. To rigorously evaluate
steganographic imperceptibility beyond conventional 2D metrics, we introduce
3D-Sinkhorn distance analysis, which quantifies distributional differences
between original and steganographic Gaussian parameters in the representation
space. Extensive experiments demonstrate that our method achieves
state-of-the-art performance in both cover and secret reconstruction while
maintaining high security levels, advancing the field of 3D steganography. Code
is available at https://github.com/RY-Paper/KeySS



---

## Multi-Modal 3D Mesh Reconstruction from Images and Text

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-10 | Melvin Reka, Tessa Pulli, Markus Vincze | cs.CV | [PDF](http://arxiv.org/pdf/2503.07190v1){: .btn .btn-green } |

**Abstract**: 6D object pose estimation for unseen objects is essential in robotics but
traditionally relies on trained models that require large datasets, high
computational costs, and struggle to generalize. Zero-shot approaches eliminate
the need for training but depend on pre-existing 3D object models, which are
often impractical to obtain. To address this, we propose a language-guided
few-shot 3D reconstruction method, reconstructing a 3D mesh from few input
images. In the proposed pipeline, receives a set of input images and a language
query. A combination of GroundingDINO and Segment Anything Model outputs
segmented masks from which a sparse point cloud is reconstructed with VGGSfM.
Subsequently, the mesh is reconstructed with the Gaussian Splatting method
SuGAR. In a final cleaning step, artifacts are removed, resulting in the final
3D mesh of the queried object. We evaluate the method in terms of accuracy and
quality of the geometry and texture. Furthermore, we study the impact of
imaging conditions such as viewing angle, number of input images, and image
overlap on 3D object reconstruction quality, efficiency, and computational
scalability.

Comments:
- under review

---

## Pixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D  Gaussian Modeling


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Long Peng, Anran Wu, Wenbo Li, Peizhe Xia, Xueyuan Dai, Xinjie Zhang, Xin Di, Haoze Sun, Renjing Pei, Yang Wang, Yang Cao, Zheng-Jun Zha | cs.CV | [PDF](http://arxiv.org/pdf/2503.06617v1){: .btn .btn-green } |

**Abstract**: Arbitrary-scale super-resolution (ASSR) aims to reconstruct high-resolution
(HR) images from low-resolution (LR) inputs with arbitrary upsampling factors
using a single model, addressing the limitations of traditional SR methods
constrained to fixed-scale factors (\textit{e.g.}, $\times$ 2). Recent advances
leveraging implicit neural representation (INR) have achieved great progress by
modeling coordinate-to-pixel mappings. However, the efficiency of these methods
may suffer from repeated upsampling and decoding, while their reconstruction
fidelity and quality are constrained by the intrinsic representational
limitations of coordinate-based functions. To address these challenges, we
propose a novel ContinuousSR framework with a Pixel-to-Gaussian paradigm, which
explicitly reconstructs 2D continuous HR signals from LR images using Gaussian
Splatting. This approach eliminates the need for time-consuming upsampling and
decoding, enabling extremely fast arbitrary-scale super-resolution. Once the
Gaussian field is built in a single pass, ContinuousSR can perform
arbitrary-scale rendering in just 1ms per scale. Our method introduces several
key innovations. Through statistical ana

Comments:
- Tech Report

---

## Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy  Surface Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Xiaoming Peng, Yixin Yang, Yang Zhou, Hui Huang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06587v1){: .btn .btn-green } |

**Abstract**: Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry
reconstruction quality than the popular 3DGS by using 2D surfels to approximate
thin surfaces. However, it falls short when dealing with glossy surfaces,
resulting in visible holes in these areas. We found the reflection
discontinuity causes the issue. To fit the jump from diffuse to specular
reflection at different viewing angles, depth bias is introduced in the
optimized Gaussian primitives. To address that, we first replace the depth
distortion loss in 2DGS with a novel depth convergence loss, which imposes a
strong constraint on depth continuity. Then, we rectified the depth criterion
in determining the actual surface, which fully accounts for all the
intersecting Gaussians along the ray. Qualitative and quantitative evaluations
across various datasets reveal that our method significantly improves
reconstruction quality, with more complete and accurate surfaces than 2DGS.



---

## CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation  Awareness for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Rui Song, Chenwei Liang, Yan Xia, Walter Zimmer, Hu Cao, Holger Caesar, Andreas Festag, Alois Knoll | cs.CV | [PDF](http://arxiv.org/pdf/2503.06744v1){: .btn .btn-green } |

**Abstract**: Dynamic scene rendering opens new avenues in autonomous driving by enabling
closed-loop simulations with photorealistic data, which is crucial for
validating end-to-end algorithms. However, the complex and highly dynamic
nature of traffic environments presents significant challenges in accurately
rendering these scenes. In this paper, we introduce a novel 4D Gaussian
Splatting (4DGS) approach, which incorporates context and temporal deformation
awareness to improve dynamic scene rendering. Specifically, we employ a 2D
semantic segmentation foundation model to self-supervise the 4D semantic
features of Gaussians, ensuring meaningful contextual embedding.
Simultaneously, we track the temporal deformation of each Gaussian across
adjacent frames. By aggregating and encoding both semantic and temporal
deformation features, each Gaussian is equipped with cues for potential
deformation compensation within 3D space, facilitating a more precise
representation of dynamic scenes. Experimental results show that our method
improves 4DGS's ability to capture fine details in dynamic scene rendering for
autonomous driving and outperforms other self-supervised methods in 4D
reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms
semantic features with each Gaussian, enabling broader applications.



---

## REArtGS: Reconstructing and Generating Articulated Objects via 3D  Gaussian Splatting with Geometric and Motion Constraints

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Di Wu, Liu Liu, Zhou Linli, Anran Huang, Liangtu Song, Qiaojun Yu, Qi Wu, Cewu Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.06677v1){: .btn .btn-green } |

**Abstract**: Articulated objects, as prevalent entities in human life, their 3D
representations play crucial roles across various applications. However,
achieving both high-fidelity textured surface reconstruction and dynamic
generation for articulated objects remains challenging for existing methods. In
this paper, we present REArtGS, a novel framework that introduces additional
geometric and motion constraints to 3D Gaussian primitives, enabling
high-quality textured surface reconstruction and generation for articulated
objects. Specifically, given multi-view RGB images of arbitrary two states of
articulated objects, we first introduce an unbiased Signed Distance Field (SDF)
guidance to regularize Gaussian opacity fields, enhancing geometry constraints
and improving surface reconstruction quality. Then we establish deformable
fields for 3D Gaussians constrained by the kinematic structures of articulated
objects, achieving unsupervised generation of surface meshes in unseen states.
Extensive experiments on both synthetic and real datasets demonstrate our
approach achieves high-quality textured surface reconstruction for given
states, and enables high-fidelity surface generation for unseen states. Codes
will be released within the next four months.

Comments:
- 11pages, 6 figures

---

## D3DR: Lighting-Aware Object Insertion in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Vsevolod Skorokhodov, Nikita Durasov, Pascal Fua | cs.CV | [PDF](http://arxiv.org/pdf/2503.06740v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has become a popular technique for various 3D Computer
Vision tasks, including novel view synthesis, scene reconstruction, and dynamic
scene rendering. However, the challenge of natural-looking object insertion,
where the object's appearance seamlessly matches the scene, remains unsolved.
In this work, we propose a method, dubbed D3DR, for inserting a
3DGS-parametrized object into 3DGS scenes while correcting its lighting,
shadows, and other visual artifacts to ensure consistency, a problem that has
not been successfully addressed before. We leverage advances in diffusion
models, which, trained on real-world data, implicitly understand correct scene
lighting. After inserting the object, we optimize a diffusion-based Delta
Denoising Score (DDS)-inspired objective to adjust its 3D Gaussian parameters
for proper lighting correction. Utilizing diffusion model personalization
techniques to improve optimization quality, our approach ensures seamless
object insertion and natural appearance. Finally, we demonstrate the method's
effectiveness by comparing it to existing approaches, achieving 0.5 PSNR and
0.15 SSIM improvements in relighting quality.



---

## Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate  Representation and Reconstruction of Neural Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Abdelaziz Bouzidi, Hamid Laga, Hazem Wannous | cs.CV | [PDF](http://arxiv.org/pdf/2503.06762v1){: .btn .btn-green } |

**Abstract**: Neural fields such as DeepSDF and Neural Radiance Fields have recently
revolutionized novel-view synthesis and 3D reconstruction from RGB images and
videos. However, achieving high-quality representation, reconstruction, and
rendering requires deep neural networks, which are slow to train and evaluate.
Although several acceleration techniques have been proposed, they often trade
off speed for memory. Gaussian splatting-based methods, on the other hand,
accelerate the rendering time but remain costly in terms of training speed and
memory needed to store the parameters of a large number of Gaussians. In this
paper, we introduce a novel neural representation that is fast, both at
training and inference times, and lightweight. Our key observation is that the
neurons used in traditional MLPs perform simple computations (a dot product
followed by ReLU activation) and thus one needs to use either wide and deep
MLPs or high-resolution and high-dimensional feature grids to parameterize
complex nonlinear functions. We show in this paper that by replacing
traditional neurons with Radial Basis Function (RBF) kernels, one can achieve
highly accurate representation of 2D (RGB images), 3D (geometry), and 5D
(radiance fields) signals with just a single layer of such neurons. The
representation is highly parallelizable, operates on low-resolution feature
grids, and is compact and memory-efficient. We demonstrate that the proposed
novel representation can be trained for 3D geometry representation in less than
15 seconds and for novel view synthesis in less than 15 mins. At runtime, it
can synthesize novel views at more than 60 fps without sacrificing quality.

Comments:
- Our code is available at https://grbfnet.github.io/

---

## StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for  Superior 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-09 | Zexu Huang, Min Xu, Stuart Perry | cs.CV | [PDF](http://arxiv.org/pdf/2503.06462v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D reconstruction coupled with neural rendering
techniques have greatly improved the creation of photo-realistic 3D scenes,
influencing both academic research and industry applications. The technique of
3D Gaussian Splatting and its variants incorporate the strengths of both
primitive-based and volumetric representations, achieving superior rendering
quality. While 3D Geometric Scattering (3DGS) and its variants have advanced
the field of 3D representation, they fall short in capturing the stochastic
properties of non-local structural information during the training process.
Additionally, the initialisation of spherical functions in 3DGS-based methods
often fails to engage higher-order terms in early training rounds, leading to
unnecessary computational overhead as training progresses. Furthermore, current
3DGS-based approaches require training on higher resolution images to render
higher resolution outputs, significantly increasing memory demands and
prolonging training durations. We introduce StructGS, a framework that enhances
3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D
reconstruction. StructGS innovatively incorporates a patch-based SSIM loss,
dynamic spherical harmonics initialisation and a Multi-scale Residual Network
(MSRN) to address the above-mentioned limitations, respectively. Our framework
significantly reduces computational redundancy, enhances detail capture and
supports high-resolution rendering from low-resolution inputs. Experimentally,
StructGS demonstrates superior performance over state-of-the-art (SOTA) models,
achieving higher quality and more detailed renderings with fewer artifacts.



---

## Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical  Deformable Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Kai Li, Junhao Wang, William Han, Ding Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2503.06161v1){: .btn .btn-green } |

**Abstract**: Minimally invasive surgery (MIS) has transformed clinical practice by
reducing recovery times, minimizing complications, and enhancing precision.
Nonetheless, MIS inherently relies on indirect visualization and precise
instrument control, posing unique challenges. Recent advances in artificial
intelligence have enabled real-time surgical scene understanding through
techniques such as image classification, object detection, and segmentation,
with scene reconstruction emerging as a key element for enhanced intraoperative
guidance. Although neural radiance fields (NeRFs) have been explored for this
purpose, their substantial data requirements and slow rendering inhibit
real-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more
efficient alternative, achieving state-of-the-art performance in dynamic
surgical scene reconstruction. In this work, we introduce Feature-EndoGaussian
(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D
rendering to enable real-time semantic and scene reconstruction. By leveraging
pretrained segmentation foundation models, FEG incorporates semantic feature
distillation within the Gaussian deformation framework, thereby enhancing both
reconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG
achieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)
compared to leading methods. Additionally, on the EndoVis18 dataset, FEG
demonstrates competitive class-wise segmentation metrics while balancing model
size and real-time performance.

Comments:
- 14 pages, 5 figures

---

## StreamGS: Online Generalizable Gaussian Splatting Reconstruction for  Unposed Image Streams

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Yang LI, Jinglu Wang, Lei Chu, Xiao Li, Shiu-hong Kao, Ying-Cong Chen, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2503.06235v1){: .btn .btn-green } |

**Abstract**: The advent of 3D Gaussian Splatting (3DGS) has advanced 3D scene
reconstruction and novel view synthesis. With the growing interest of
interactive applications that need immediate feedback, online 3DGS
reconstruction in real-time is in high demand. However, none of existing
methods yet meet the demand due to three main challenges: the absence of
predetermined camera parameters, the need for generalizable 3DGS optimization,
and the necessity of reducing redundancy. We propose StreamGS, an online
generalizable 3DGS reconstruction method for unposed image streams, which
progressively transform image streams to 3D Gaussian streams by predicting and
aggregating per-frame Gaussians. Our method overcomes the limitation of the
initial point reconstruction \cite{dust3r} in tackling out-of-domain (OOD)
issues by introducing a content adaptive refinement. The refinement enhances
cross-frame consistency by establishing reliable pixel correspondences between
adjacent frames. Such correspondences further aid in merging redundant
Gaussians through cross-frame feature aggregation. The density of Gaussians is
thereby reduced, empowering online reconstruction by significantly lowering
computational and memory costs. Extensive experiments on diverse datasets have
demonstrated that StreamGS achieves quality on par with optimization-based
approaches but does so 150 times faster, and exhibits superior generalizability
in handling OOD scenes.

Comments:
- 8 pages

---

## ForestSplats: Deformable transient field for Gaussian Splatting in the  Wild

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Wongi Park, Myeongseok Nam, Siwon Kim, Sangwoo Jo, Soomok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.06179v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time
rendering speeds and high-quality results in static scenes. Although 3D-GS
shows effectiveness in static scenes, their performance significantly degrades
in real-world environments due to transient objects, lighting variations, and
diverse levels of occlusion. To tackle this, existing methods estimate
occluders or transient elements by leveraging pre-trained models or integrating
additional transient field pipelines. However, these methods still suffer from
two defects: 1) Using semantic features from the Vision Foundation model (VFM)
causes additional computational costs. 2) The transient field requires
significant memory to handle transient elements with per-view Gaussians and
struggles to define clear boundaries for occluders, solely relying on
photometric errors. To address these problems, we propose ForestSplats, a novel
approach that leverages the deformable transient field and a superpixel-aware
mask to efficiently represent transient elements in the 2D scene across
unconstrained image collections and effectively decompose static scenes from
transient distractors without VFM. We designed the transient field to be
deformable, capturing per-view transient elements. Furthermore, we introduce a
superpixel-aware mask that clearly defines the boundaries of occluders by
considering photometric errors and superpixels. Additionally, we propose
uncertainty-aware densification to avoid generating Gaussians within the
boundaries of occluders during densification. Through extensive experiments
across several benchmark datasets, we demonstrate that ForestSplats outperforms
existing methods without VFM and shows significant memory efficiency in
representing transient elements.



---

## SplatTalk: 3D VQA with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Anh Thai, Songyou Peng, Kyle Genova, Leonidas Guibas, Thomas Funkhouser | cs.CV | [PDF](http://arxiv.org/pdf/2503.06271v1){: .btn .btn-green } |

**Abstract**: Language-guided 3D scene understanding is important for advancing
applications in robotics, AR/VR, and human-computer interaction, enabling
models to comprehend and interact with 3D environments through natural
language. While 2D vision-language models (VLMs) have achieved remarkable
success in 2D VQA tasks, progress in the 3D domain has been significantly
slower due to the complexity of 3D data and the high cost of manual
annotations. In this work, we introduce SplatTalk, a novel method that uses a
generalizable 3D Gaussian Splatting (3DGS) framework to produce 3D tokens
suitable for direct input into a pretrained LLM, enabling effective zero-shot
3D visual question answering (3D VQA) for scenes with only posed images. During
experiments on multiple benchmarks, our approach outperforms both 3D models
trained specifically for the task and previous 2D-LMM-based models utilizing
only images (our setting), while achieving competitive performance with
state-of-the-art 3D LMMs that additionally utilize 3D inputs.



---

## SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting  Steganography

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Xuanyu Zhang, Jiarui Meng, Zhipei Xu, Shuzhou Yang, Yanmin Wu, Ronggang Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06118v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a premier method for 3D
representation due to its real-time rendering and high-quality outputs,
underscoring the critical need to protect the privacy of 3D assets. Traditional
NeRF steganography methods fail to address the explicit nature of 3DGS since
its point cloud files are publicly accessible. Existing GS steganography
solutions mitigate some issues but still struggle with reduced rendering
fidelity, increased computational demands, and security flaws, especially in
the security of the geometric structure of the visualized point cloud. To
address these demands, we propose a SecureGS, a secure and efficient 3DGS
steganography framework inspired by Scaffold-GS's anchor point design and
neural decoding. SecureGS uses a hybrid decoupled Gaussian encryption mechanism
to embed offsets, scales, rotations, and RGB attributes of the hidden 3D
Gaussian points in anchor point features, retrievable only by authorized users
through privacy-preserving neural networks. To further enhance security, we
propose a density region-aware anchor growing and pruning strategy that
adaptively locates optimal hiding regions without exposing hidden information.
Extensive experiments show that SecureGS significantly surpasses existing GS
steganography methods in rendering fidelity, speed, and security.

Comments:
- Accepted by ICLR 2025

---

## NeuraLoc: Visual Localization in Neural Implicit Map with Dual  Complementary Features

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Hongjia Zhai, Boming Zhao, Hai Li, Xiaokun Pan, Yijia He, Zhaopeng Cui, Hujun Bao, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.06117v1){: .btn .btn-green } |

**Abstract**: Recently, neural radiance fields (NeRF) have gained significant attention in
the field of visual localization. However, existing NeRF-based approaches
either lack geometric constraints or require extensive storage for feature
matching, limiting their practical applications. To address these challenges,
we propose an efficient and novel visual localization approach based on the
neural implicit map with complementary features. Specifically, to enforce
geometric constraints and reduce storage requirements, we implicitly learn a 3D
keypoint descriptor field, avoiding the need to explicitly store point-wise
features. To further address the semantic ambiguity of descriptors, we
introduce additional semantic contextual feature fields, which enhance the
quality and reliability of 2D-3D correspondences. Besides, we propose
descriptor similarity distribution alignment to minimize the domain gap between
2D and 3D feature spaces during matching. Finally, we construct the matching
graph using both complementary descriptors and contextual features to establish
accurate 2D-3D correspondences for 6-DoF pose estimation. Compared with the
recent NeRF-based approaches, our method achieves a 3$\times$ faster training
speed and a 45$\times$ reduction in model storage. Extensive experiments on two
widely used datasets demonstrate that our approach outperforms or is highly
competitive with other state-of-the-art NeRF-based visual localization methods.
Project page:
\href{https://zju3dv.github.io/neuraloc}{https://zju3dv.github.io/neuraloc}

Comments:
- ICRA 2025

---

## GSV3D: Gaussian Splatting-based Geometric Distillation with Stable Video  Diffusion for Single-Image 3D Object Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-08 | Ye Tao, Jiawei Zhang, Yahao Shi, Dongqing Zou, Bin Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.06136v1){: .btn .btn-green } |

**Abstract**: Image-based 3D generation has vast applications in robotics and gaming, where
high-quality, diverse outputs and consistent 3D representations are crucial.
However, existing methods have limitations: 3D diffusion models are limited by
dataset scarcity and the absence of strong pre-trained priors, while 2D
diffusion-based approaches struggle with geometric consistency. We propose a
method that leverages 2D diffusion models' implicit 3D reasoning ability while
ensuring 3D consistency via Gaussian-splatting-based geometric distillation.
Specifically, the proposed Gaussian Splatting Decoder enforces 3D consistency
by transforming SV3D latent outputs into an explicit 3D representation. Unlike
SV3D, which only relies on implicit 2D representations for video generation,
Gaussian Splatting explicitly encodes spatial and appearance attributes,
enabling multi-view consistency through geometric constraints. These
constraints correct view inconsistencies, ensuring robust geometric
consistency. As a result, our approach simultaneously generates high-quality,
multi-view-consistent images and accurate 3D models, providing a scalable
solution for single-image-based 3D generation and bridging the gap between 2D
Diffusion diversity and 3D structural coherence. Experimental results
demonstrate state-of-the-art multi-view consistency and strong generalization
across diverse datasets. The code will be made publicly available upon
acceptance.



---

## SeeLe: A Unified Acceleration Framework for Real-Time Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Xiaotong Huang, He Zhu, Zihan Liu, Weikai Lin, Xiaohong Liu, Zhezhi He, Jingwen Leng, Minyi Guo, Yu Feng | cs.GR | [PDF](http://arxiv.org/pdf/2503.05168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a crucial rendering technique for
many real-time applications. However, the limited hardware resources on today's
mobile platforms hinder these applications, as they struggle to achieve
real-time performance. In this paper, we propose SeeLe, a general framework
designed to accelerate the 3DGS pipeline for resource-constrained mobile
devices.
  Specifically, we propose two GPU-oriented techniques: hybrid preprocessing
and contribution-aware rasterization. Hybrid preprocessing alleviates the GPU
compute and memory pressure by reducing the number of irrelevant Gaussians
during rendering. The key is to combine our view-dependent scene representation
with online filtering. Meanwhile, contribution-aware rasterization improves the
GPU utilization at the rasterization stage by prioritizing Gaussians with high
contributions while reducing computations for those with low contributions.
Both techniques can be seamlessly integrated into existing 3DGS pipelines with
minimal fine-tuning. Collectively, our framework achieves 2.6$\times$ speedup
and 32.3\% model reduction while achieving superior rendering quality compared
to existing methods.



---

## GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three  Orthographic Views Using 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Zheng Zhou, Zhe Li, Bo Yu, Lina Hu, Liang Dong, Zijian Yang, Xiaoli Liu, Ning Xu, Ziwei Wang, Yonghao Dang, Jianqin Yin | cs.CV | [PDF](http://arxiv.org/pdf/2503.05161v1){: .btn .btn-green } |

**Abstract**: The automatic reconstruction of 3D computer-aided design (CAD) models from
CAD sketches has recently gained significant attention in the computer vision
community. Most existing methods, however, rely on vector CAD sketches and 3D
ground truth for supervision, which are often difficult to be obtained in
industrial applications and are sensitive to noise inputs. We propose viewing
CAD reconstruction as a specific instance of sparse-view 3D reconstruction to
overcome these limitations. While this reformulation offers a promising
perspective, existing 3D reconstruction methods typically require natural
images and corresponding camera poses as inputs, which introduces two major
significant challenges: (1) modality discrepancy between CAD sketches and
natural images, and (2) difficulty of accurate camera pose estimation for CAD
sketches. To solve these issues, we first transform the CAD sketches into
representations resembling natural images and extract corresponding masks.
Next, we manually calculate the camera poses for the orthographic views to
ensure accurate alignment within the 3D coordinate system. Finally, we employ a
customized sparse-view 3D reconstruction method to achieve high-quality
reconstructions from aligned orthographic views. By leveraging raster CAD
sketches for self-supervision, our approach eliminates the reliance on vector
CAD sketches and 3D ground truth. Experiments on the Sub-Fusion360 dataset
demonstrate that our proposed method significantly outperforms previous
approaches in CAD reconstruction performance and exhibits strong robustness to
noisy inputs.



---

## GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Kohei Honda, Takeshi Ishita, Yasuhiro Yoshimura, Ryo Yonitani | cs.RO | [PDF](http://arxiv.org/pdf/2503.05152v1){: .btn .btn-green } |

**Abstract**: This paper presents a novel approach to image-goal navigation by integrating
3D Gaussian Splatting (3DGS) with Visual Navigation Models (VNMs), a method we
refer to as GSplatVNM. VNMs offer a promising paradigm for image-goal
navigation by guiding a robot through a sequence of point-of-view images
without requiring metrical localization or environment-specific training.
However, constructing a dense and traversable sequence of target viewpoints
from start to goal remains a central challenge, particularly when the available
image database is sparse. To address these challenges, we propose a 3DGS-based
viewpoint synthesis framework for VNMs that synthesizes intermediate viewpoints
to seamlessly bridge gaps in sparse data while significantly reducing storage
overhead. Experimental results in a photorealistic simulator demonstrate that
our approach not only enhances navigation efficiency but also exhibits
robustness under varying levels of image database sparsity.

Comments:
- 8 pages, 4 figures

---

## EvolvingGS: High-Fidelity Streamable Volumetric Video via Evolving 3D  Gaussian Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Chao Zhang, Yifeng Zhou, Shuheng Wang, Wenfa Li, Degang Wang, Yi Xu, Shaohui Jiao | cs.CV | [PDF](http://arxiv.org/pdf/2503.05162v1){: .btn .btn-green } |

**Abstract**: We have recently seen great progress in 3D scene reconstruction through
explicit point-based 3D Gaussian Splatting (3DGS), notable for its high quality
and fast rendering speed. However, reconstructing dynamic scenes such as
complex human performances with long durations remains challenging. Prior
efforts fall short of modeling a long-term sequence with drastic motions,
frequent topology changes or interactions with props, and resort to segmenting
the whole sequence into groups of frames that are processed independently,
which undermines temporal stability and thereby leads to an unpleasant viewing
experience and inefficient storage footprint. In view of this, we introduce
EvolvingGS, a two-stage strategy that first deforms the Gaussian model to
coarsely align with the target frame, and then refines it with minimal point
addition/subtraction, particularly in fast-changing areas. Owing to the
flexibility of the incrementally evolving representation, our method
outperforms existing approaches in terms of both per-frame and temporal quality
metrics while maintaining fast rendering through its purely explicit
representation. Moreover, by exploiting temporal coherence between successive
frames, we propose a simple yet effective compression algorithm that achieves
over 50x compression rate. Extensive experiments on both public benchmarks and
challenging custom datasets demonstrate that our method significantly advances
the state-of-the-art in dynamic scene reconstruction, particularly for extended
sequences with complex human performances.



---

## Free Your Hands: Lightweight Relightable Turntable Capture Pipeline

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jiahui Fan, Fujun Luan, Miloš Hašan, Jian Yang, Beibei Wang | cs.GR | [PDF](http://arxiv.org/pdf/2503.05511v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) from multiple captured photos of an object is a
widely studied problem. Achieving high quality typically requires dense
sampling of input views, which can lead to frustrating and tedious manual
labor. Manually positioning cameras to maintain an optimal desired distribution
can be difficult for humans, and if a good distribution is found, it is not
easy to replicate. Additionally, the captured data can suffer from motion blur
and defocus due to human error. In this paper, we present a lightweight object
capture pipeline to reduce the manual workload and standardize the acquisition
setup. We use a consumer turntable to carry the object and a tripod to hold the
camera. As the turntable rotates, we automatically capture dense samples from
various views and lighting conditions; we can repeat this for several camera
positions. This way, we can easily capture hundreds of valid images in several
minutes without hands-on effort. However, in the object reference frame, the
light conditions vary; this is harmful to a standard NVS method like 3D
Gaussian splatting (3DGS) which assumes fixed lighting. We design a neural
radiance representation conditioned on light rotations, which addresses this
issue and allows relightability as an additional benefit. We demonstrate our
pipeline using 3DGS as the underlying framework, achieving competitive quality
compared to previous methods with exhaustive acquisition and showcasing its
potential for relighting and harmonization tasks.



---

## Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D  Gaussian Splatting from Sparse Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Yingji Zhong, Zhihao Li, Dave Zhenyu Chen, Lanqing Hong, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.05082v1){: .btn .btn-green } |

**Abstract**: Despite recent successes in novel view synthesis using 3D Gaussian Splatting
(3DGS), modeling scenes with sparse inputs remains a challenge. In this work,
we address two critical yet overlooked issues in real-world sparse-input
modeling: extrapolation and occlusion. To tackle these issues, we propose to
use a reconstruction by generation pipeline that leverages learned priors from
video diffusion models to provide plausible interpretations for regions outside
the field of view or occluded. However, the generated sequences exhibit
inconsistencies that do not fully benefit subsequent 3DGS modeling. To address
the challenge of inconsistencies, we introduce a novel scene-grounding guidance
based on rendered sequences from an optimized 3DGS, which tames the diffusion
model to generate consistent sequences. This guidance is training-free and does
not require any fine-tuning of the diffusion model. To facilitate holistic
scene modeling, we also propose a trajectory initialization method. It
effectively identifies regions that are outside the field of view and occluded.
We further design a scheme tailored for 3DGS optimization with generated
sequences. Experiments demonstrate that our method significantly improves upon
the baseline and achieves state-of-the-art performance on challenging
benchmarks.

Comments:
- Accepted by CVPR2025. The project page is available at
  https://zhongyingji.github.io/guidevd-3dgs/

---

## SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image  via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Linqi Yang, Xiongwei Zhao, Qihao Sun, Ke Wang, Ao Chen, Peng Kang | cs.CV | [PDF](http://arxiv.org/pdf/2503.05174v1){: .btn .btn-green } |

**Abstract**: 6-DoF pose estimation is a fundamental task in computer vision with
wide-ranging applications in augmented reality and robotics. Existing single
RGB-based methods often compromise accuracy due to their reliance on initial
pose estimates and susceptibility to rotational ambiguity, while approaches
requiring depth sensors or multi-view setups incur significant deployment
costs. To address these limitations, we introduce SplatPose, a novel framework
that synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neural
architecture to achieve high-precision pose estimation using only a single RGB
image. Central to our approach is the Dual-Attention Ray Scoring Network
(DARS-Net), which innovatively decouples positional and angular alignment
through geometry-domain attention mechanisms, explicitly modeling directional
dependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fine
optimization pipeline progressively refines pose estimates by aligning dense 2D
features between query images and 3DGS-synthesized views, effectively
correcting feature misalignment and depth errors from sparse ray sampling.
Experiments on three benchmark datasets demonstrate that SplatPose achieves
state-of-the-art 6-DoF pose estimation accuracy in single RGB settings,
rivaling approaches that depend on depth or multi-view images.

Comments:
- Submitted to IROS 2025

---

## Self-Modeling Robots by Photographing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Kejun Hu, Peng Yu, Ning Tan | cs.RO | [PDF](http://arxiv.org/pdf/2503.05398v1){: .btn .btn-green } |

**Abstract**: Self-modeling enables robots to build task-agnostic models of their
morphology and kinematics based on data that can be automatically collected,
with minimal human intervention and prior information, thereby enhancing
machine intelligence. Recent research has highlighted the potential of
data-driven technology in modeling the morphology and kinematics of robots.
However, existing self-modeling methods suffer from either low modeling quality
or excessive data acquisition costs. Beyond morphology and kinematics, texture
is also a crucial component of robots, which is challenging to model and
remains unexplored. In this work, a high-quality, texture-aware, and link-level
method is proposed for robot self-modeling. We utilize three-dimensional (3D)
Gaussians to represent the static morphology and texture of robots, and cluster
the 3D Gaussians to construct neural ellipsoid bones, whose deformations are
controlled by the transformation matrices generated by a kinematic neural
network. The 3D Gaussians and kinematic neural network are trained using data
pairs composed of joint angles, camera parameters and multi-view images without
depth information. By feeding the kinematic neural network with joint angles,
we can utilize the well-trained model to describe the corresponding morphology,
kinematics and texture of robots at the link level, and render robot images
from different perspectives with the aid of 3D Gaussian splatting. Furthermore,
we demonstrate that the established model can be exploited to perform
downstream tasks such as motion planning and inverse kinematics.



---

## Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot  Manipulation of Irregularly Shaped Objects


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2503.05189v1){: .btn .btn-green } |

**Abstract**: Tracking and manipulating irregularly-shaped, previously unseen objects in
dynamic environments is important for robotic applications in manufacturing,
assembly, and logistics. Recently introduced Gaussian Splats efficiently model
object geometry, but lack persistent state estimation for task-oriented
manipulation. We present Persistent Object Gaussian Splat (POGS), a system that
embeds semantics, self-supervised visual features, and object grouping features
into a compact representation that can be continuously updated to estimate the
pose of scanned objects. POGS updates object states without requiring expensive
rescanning or prior CAD models of objects. After an initial multi-view scene
capture and training phase, POGS uses a single stereo camera to integrate depth
estimates along with self-supervised vision encoder features for object pose
estimation. POGS supports grasping, reorientation, and natural language-driven
manipulation by refining object pose estimates, facilitating sequential object
reset operations with human-induced object perturbations and tool servoing,
where robots recover tool pose despite tool perturbations of up to 30{\deg}.
POGS achieves up to 12 consecutive successful object resets and recovers from
80% of in-grasp tool perturbations.

Comments:
- Accepted to ICRA 2025

---

## STGA: Selective-Training Gaussian Head Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Hanzhi Guo, Yixiao Chen, Dongye Xiaonuo, Zeyu Tian, Dongdong Weng, Le Luo | cs.GR | [PDF](http://arxiv.org/pdf/2503.05196v1){: .btn .btn-green } |

**Abstract**: We propose selective-training Gaussian head avatars (STGA) to enhance the
details of dynamic head Gaussian. The dynamic head Gaussian model is trained
based on the FLAME parameterized model. Each Gaussian splat is embedded within
the FLAME mesh to achieve mesh-based animation of the Gaussian model. Before
training, our selection strategy calculates the 3D Gaussian splat to be
optimized in each frame. The parameters of these 3D Gaussian splats are
optimized in the training of each frame, while those of the other splats are
frozen. This means that the splats participating in the optimization process
differ in each frame, to improve the realism of fine details. Compared with
network-based methods, our method achieves better results with shorter training
time. Compared with mesh-based methods, our method produces more realistic
details within the same training time. Additionally, the ablation experiment
confirms that our method effectively enhances the quality of details.



---

## CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from  Motion-Blurred Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jungho Lee, Donghyeong Kim, Dogyoon Lee, Suhwan Cho, Minhyeok Lee, Wonjoon Lee, Taeoh Kim, Dongyoon Wee, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.05332v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for their
high-quality novel view rendering, motivating research to address real-world
challenges. A critical issue is the camera motion blur caused by movement
during exposure, which hinders accurate 3D scene reconstruction. In this study,
we propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that
reconstructs precise 3D scenes from motion-blurred images while maintaining
real-time rendering speed. Considering the complex motion patterns inherent in
real-world camera movements, we predict continuous camera trajectories using
neural ordinary differential equations (ODEs). To ensure accurate modeling, we
employ rigid body transformations, preserving the shape and size of the object
but rely on the discrete integration of sampled frames. To better approximate
the continuous nature of motion blur, we introduce a continuous motion
refinement (CMR) transformation that refines rigid transformations by
incorporating additional learnable parameters. By revisiting fundamental camera
theory and leveraging advanced neural ODE techniques, we achieve precise
modeling of continuous camera trajectories, leading to improved reconstruction
accuracy. Extensive experiments demonstrate state-of-the-art performance both
quantitatively and qualitatively on benchmark datasets, which include a wide
range of motion blur scenarios, from moderate to extreme blur.

Comments:
- Revised Version of CRiM-GS, Github:
  https://github.com/Jho-Yonsei/CoMoGaussian

---

## MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface  Reconstruction under Various Light Conditions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Qingyuan Zhou, Yuehu Gong, Weidong Yang, Jiaze Li, Yeqi Luo, Baixin Xu, Shuhao Li, Ben Fei, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2503.05182v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) and surface reconstruction (SR) are essential
tasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks
are often addressed independently, with GS-based rendering methods struggling
under diverse light conditions and failing to produce accurate surfaces, while
GS-based reconstruction methods frequently compromise rendering quality. This
raises a central question: must rendering and reconstruction always involve a
trade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian
splatting for Surface Reconstruction that enhances both rendering quality and
3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS
and the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,
providing precise geometry information to the 3D-GS branch. Leveraging this
geometry, the 3D-GS branch employs a geometry-guided illumination decomposition
module that captures reflected and transmitted components, enabling realistic
rendering under varied light conditions. Using the transmitted component as
supervision, the 2D-GS branch also achieves high-fidelity surface
reconstruction. Throughout the optimization process, the 2D-GS and 3D-GS
branches undergo alternating optimization, providing mutual supervision. Prior
to this, each branch completes an independent warm-up phase, with an early
stopping strategy implemented to reduce computational costs. We evaluate MGSR
on a diverse set of synthetic and real-world datasets, at both object and scene
levels, demonstrating strong performance in rendering and surface
reconstruction.

Comments:
- 11 pages, 7 figures

---

## D2GV: Deformable 2D Gaussian Splatting for Video Representation in  400FPS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Mufan Liu, Qi Yang, Miaoran Zhao, He Huang, Le Yang, Zhu Li, Yiling Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.05600v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representations (INRs) have emerged as a powerful approach
for video representation, offering versatility across tasks such as compression
and inpainting. However, their implicit formulation limits both
interpretability and efficacy, undermining their practicality as a
comprehensive solution. We propose a novel video representation based on
deformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key
objectives: 1) improved efficiency while delivering superior quality; 2)
enhanced scalability and interpretability; and 3) increased friendliness for
downstream tasks. Specifically, we initially divide the video sequence into
fixed-length Groups of Pictures (GoP) to allow parallel training and linear
scalability with video length. For each GoP, D2GV represents video frames by
applying differentiable rasterization to 2D Gaussians, which are deformed from
a canonical space into their corresponding timestamps. Notably, leveraging
efficient CUDA-based rasterization, D2GV converges fast and decodes at speeds
exceeding 400 FPS, while delivering quality that matches or surpasses
state-of-the-art INRs. Moreover, we incorporate a learnable pruning and
quantization strategy to streamline D2GV into a more compact representation. We
demonstrate D2GV's versatility in tasks including video interpolation,
inpainting and denoising, underscoring its potential as a promising solution
for video representation. Code is available at:
\href{https://github.com/Evan-sudo/D2GV}{https://github.com/Evan-sudo/D2GV}.



---

## Bayesian Fields: Task-driven Open-Set Semantic Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Dominic Maggio, Luca Carlone | cs.CV | [PDF](http://arxiv.org/pdf/2503.05949v1){: .btn .btn-green } |

**Abstract**: Open-set semantic mapping requires (i) determining the correct granularity to
represent the scene (e.g., how should objects be defined), and (ii) fusing
semantic knowledge across multiple 2D observations into an overall 3D
reconstruction -ideally with a high-fidelity yet low-memory footprint. While
most related works bypass the first issue by grouping together primitives with
similar semantics (according to some manually tuned threshold), we recognize
that the object granularity is task-dependent, and develop a task-driven
semantic mapping approach. To address the second issue, current practice is to
average visual embedding vectors over multiple views. Instead, we show the
benefits of using a probabilistic approach based on the properties of the
underlying visual-language foundation model, and leveraging Bayesian updating
to aggregate multiple observations of the scene. The result is Bayesian Fields,
a task-driven and probabilistic approach for open-set semantic mapping. To
enable high-fidelity objects and a dense scene representation, Bayesian Fields
uses 3D Gaussians which we cluster into task-relevant objects, allowing for
both easy 3D object extraction and reduced memory usage. We release Bayesian
Fields open-source at https: //github.com/MIT-SPARK/Bayesian-Fields.



---

## LiDAR-enhanced 3D Gaussian Splatting Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-07 | Jian Shen, Huai Yu, Ji Wu, Wen Yang, Gui-Song Xia | cs.RO | [PDF](http://arxiv.org/pdf/2503.05425v1){: .btn .btn-green } |

**Abstract**: This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting
(3DGS) mapping framework that improves the accuracy and robustness of 3D scene
mapping by integrating LiDAR data. LiGSM constructs joint loss from images and
LiDAR point clouds to estimate the poses and optimize their extrinsic
parameters, enabling dynamic adaptation to variations in sensor alignment.
Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a
denser and more reliable starting points compared to sparse SfM points. In
scene rendering, the framework augments standard image-based supervision with
depth maps generated from LiDAR projections, ensuring an accurate scene
representation in both geometry and photometry. Experiments on public and
self-collected datasets demonstrate that LiGSM outperforms comparative methods
in pose tracking and scene rendering.

Comments:
- Accepted by ICRA 2025

---

## GaussianVideo: Efficient Video Representation and Compression by  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Inseo Lee, Youngyoon Choi, Joonseok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2503.04333v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representation for Videos (NeRV) has introduced a novel
paradigm for video representation and compression, outperforming traditional
codecs. As model size grows, however, slow encoding and decoding speed and high
memory consumption hinder its application in practice. To address these
limitations, we propose a new video representation and compression method based
on 2D Gaussian Splatting to efficiently handle video data. Our proposed
deformable 2D Gaussian Splatting dynamically adapts the transformation of 2D
Gaussians at each frame, significantly reducing memory cost. Equipped with a
multi-plane-based spatiotemporal encoder and a lightweight decoder, it predicts
changes in color, coordinates, and shape of initialized Gaussians, given the
time step. By leveraging temporal gradients, our model effectively captures
temporal redundancy at negligible cost, significantly enhancing video
representation efficiency. Our method reduces GPU memory usage by up to 78.4%,
and significantly expedites video processing, achieving 5.5x faster training
and 12.5x faster decoding compared to the state-of-the-art NeRV methods.



---

## Beyond Existance: Fulfill 3D Reconstructed Scenes with Pseudo Details

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Yifei Gao, Jun Huang, Lei Wang, Ruiting Dai, Jun Cheng | cs.GR | [PDF](http://arxiv.org/pdf/2503.04037v1){: .btn .btn-green } |

**Abstract**: The emergence of 3D Gaussian Splatting (3D-GS) has significantly advanced 3D
reconstruction by providing high fidelity and fast training speeds across
various scenarios. While recent efforts have mainly focused on improving model
structures to compress data volume or reduce artifacts during zoom-in and
zoom-out operations, they often overlook an underlying issue: training sampling
deficiency. In zoomed-in views, Gaussian primitives can appear unregulated and
distorted due to their dilation limitations and the insufficient availability
of scale-specific training samples. Consequently, incorporating pseudo-details
that ensure the completeness and alignment of the scene becomes essential. In
this paper, we introduce a new training method that integrates diffusion models
and multi-scale training using pseudo-ground-truth data. This approach not only
notably mitigates the dilation and zoomed-in artifacts but also enriches
reconstructed scenes with precise details out of existing scenarios. Our method
achieves state-of-the-art performance across various benchmarks and extends the
capabilities of 3D reconstruction beyond training datasets.



---

## GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian  Radiance Fields and Differentiable Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Qianzhong Chen, Jiankai Sun, Naixiang Gao, JunEn Low, Timothy Chen, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2503.03984v1){: .btn .btn-green } |

**Abstract**: Autonomous visual navigation is an essential element in robot autonomy.
Reinforcement learning (RL) offers a promising policy training paradigm.
However existing RL methods suffer from high sample complexity, poor
sim-to-real transfer, and limited runtime adaptability to navigation scenarios
not seen during training. These problems are particularly challenging for
drones, with complex nonlinear and unstable dynamics, and strong dynamic
coupling between control and perception. In this paper, we propose a novel
framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep
reinforcement learning (DDRL) to train vision-based drone navigation policies.
By leveraging high-fidelity 3D scene representations and differentiable
simulation, our method improves sample efficiency and sim-to-real transfer.
Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt
to environmental variations at runtime. Moreover, by curriculum training in a
mixture of different surrounding environments, we achieve in-task
generalization, the ability to solve new instances of a task not seen during
training. Drone hardware experiments demonstrate our method's high training
efficiency compared to state-of-the-art RL methods, zero shot sim-to-real
transfer for real robot deployment without fine tuning, and ability to adapt to
new instances within the same task class (e.g. to fly through a gate at
different locations with different distractors in the environment).



---

## GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world  Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Xihan Wang, Dianyi Yang, Yu Gao, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.04034v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting(3DGS) have significantly
improved semantic scene understanding, enabling natural language queries to
localize objects within a scene. However, existing methods primarily focus on
embedding compressed CLIP features to 3D Gaussians, suffering from low object
segmentation accuracy and lack spatial reasoning capabilities. To address these
limitations, we propose GaussianGraph, a novel framework that enhances
3DGS-based scene understanding by integrating adaptive semantic clustering and
scene graph generation. We introduce a "Control-Follow" clustering strategy,
which dynamically adapts to scene scale and feature distribution, avoiding
feature compression and significantly improving segmentation accuracy.
Additionally, we enrich scene representation by integrating object attributes
and spatial relations extracted from 2D foundation models. To address
inaccuracies in spatial relationships, we propose 3D correction modules that
filter implausible relations through spatial consistency verification, ensuring
reliable scene graph construction. Extensive experiments on three datasets
demonstrate that GaussianGraph outperforms state-of-the-art methods in both
semantic segmentation and object grounding tasks, providing a robust solution
for complex scene understanding and interaction.



---

## S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Yecong Wan, Mingwen Shao, Yuanshuo Cheng, Wangmeng Zuo | cs.CV | [PDF](http://arxiv.org/pdf/2503.04314v1){: .btn .btn-green } |

**Abstract**: In this paper, we aim ambitiously for a realistic yet challenging problem,
namely, how to reconstruct high-quality 3D scenes from sparse low-resolution
views that simultaneously suffer from deficient perspectives and clarity.
Whereas existing methods only deal with either sparse views or low-resolution
observations, they fail to handle such hybrid and complicated scenarios. To
this end, we propose a novel Sparse-view Super-resolution 3D Gaussian Splatting
framework, dubbed S2Gaussian, that can reconstruct structure-accurate and
detail-faithful 3D scenes with only sparse and low-resolution views. The
S2Gaussian operates in a two-stage fashion. In the first stage, we initially
optimize a low-resolution Gaussian representation with depth regularization and
densify it to initialize the high-resolution Gaussians through a tailored
Gaussian Shuffle Split operation. In the second stage, we refine the
high-resolution Gaussians with the super-resolved images generated from both
original sparse views and pseudo-views rendered by the low-resolution
Gaussians. In which a customized blur-free inconsistency modeling scheme and a
3D robust optimization strategy are elaborately designed to mitigate multi-view
inconsistency and eliminate erroneous updates caused by imperfect supervision.
Extensive experiments demonstrate superior results and in particular
establishing new state-of-the-art performances with more consistent geometry
and finer details.

Comments:
- CVPR 2025

---

## Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall, Yumeng Wang, Paul Maria Scheikl, Axel Krieger | cs.CV | [PDF](http://arxiv.org/pdf/2503.04079v1){: .btn .btn-green } |

**Abstract**: Accurate geometric reconstruction of deformable tissues in monocular
endoscopic video remains a fundamental challenge in robot-assisted minimally
invasive surgery. Although recent volumetric and point primitive methods based
on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently
rendered surgical scenes, they still struggle with handling artifact-free tool
occlusions and preserving fine anatomical details. These limitations stem from
unrestricted Gaussian scaling and insufficient surface alignment constraints
during reconstruction. To address these issues, we introduce Surgical Gaussian
Surfels (SGS), which transforms anisotropic point primitives into
surface-aligned elliptical splats by constraining the scale component of the
Gaussian covariance matrix along the view-aligned axis. We predict accurate
surfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled
with locality constraints to handle complex tissue deformations. We use
homodirectional view-space positional gradients to capture fine image details
by splitting Gaussian Surfels in over-reconstructed regions. In addition, we
define surface normals as the direction of the steepest density change within
each Gaussian surfel primitive, enabling accurate normal estimation without
requiring monocular normal priors. We evaluate our method on two in-vivo
surgical datasets, where it outperforms current state-of-the-art methods in
surface geometry, normal map quality, and rendering efficiency, while remaining
competitive in real-time rendering performance. We make our code available at
https://github.com/aloma85/SurgicalGaussianSurfels



---

## Instrument-Splatting: Controllable Photorealistic Reconstruction of  Surgical Instruments Using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-06 | Shuojue Yang, Zijian Wu, Mingxuan Hong, Qian Li, Daiyun Shen, Septimiu E. Salcudean, Yueming Jin | cs.CV | [PDF](http://arxiv.org/pdf/2503.04082v1){: .btn .btn-green } |

**Abstract**: Real2Sim is becoming increasingly important with the rapid development of
surgical artificial intelligence (AI) and autonomy. In this work, we propose a
novel Real2Sim methodology, \textit{Instrument-Splatting}, that leverages 3D
Gaussian Splatting to provide fully controllable 3D reconstruction of surgical
instruments from monocular surgical videos. To maintain both high visual
fidelity and manipulability, we introduce a geometry pre-training to bind
Gaussian point clouds on part mesh with accurate geometric priors and define a
forward kinematics to control the Gaussians as flexible as real instruments.
Afterward, to handle unposed videos, we design a novel instrument pose tracking
method leveraging semantics-embedded Gaussians to robustly refine per-frame
instrument poses and joint states in a render-and-compare manner, which allows
our instrument Gaussian to accurately learn textures and reach photorealistic
rendering. We validated our method on 2 publicly released surgical videos and 4
videos collected on ex vivo tissues and green screens. Quantitative and
qualitative evaluations demonstrate the effectiveness and superiority of the
proposed method.

Comments:
- 11 pages, 5 figures

---

## LensDFF: Language-enhanced Sparse Feature Distillation for Efficient  Few-Shot Dexterous Manipulation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-05 | Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll | cs.RO | [PDF](http://arxiv.org/pdf/2503.03890v1){: .btn .btn-green } |

**Abstract**: Learning dexterous manipulation from few-shot demonstrations is a significant
yet challenging problem for advanced, human-like robotic systems. Dense
distilled feature fields have addressed this challenge by distilling rich
semantic features from 2D visual foundation models into the 3D domain. However,
their reliance on neural rendering models such as Neural Radiance Fields (NeRF)
or Gaussian Splatting results in high computational costs. In contrast,
previous approaches based on sparse feature fields either suffer from
inefficiencies due to multi-view dependencies and extensive training or lack
sufficient grasp dexterity. To overcome these limitations, we propose
Language-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently
distills view-consistent 2D features onto 3D points using our novel
language-enhanced feature fusion strategy, thereby enabling single-view
few-shot generalization. Based on LensDFF, we further introduce a few-shot
dexterous manipulation framework that integrates grasp primitives into the
demonstrations to generate stable and highly dexterous grasps. Moreover, we
present a real2sim grasp evaluation pipeline for efficient grasp assessment and
hyperparameter tuning. Through extensive simulation experiments based on the
real2sim pipeline and real-world experiments, our approach achieves competitive
grasping performance, outperforming state-of-the-art approaches.

Comments:
- 8 pages

---

## NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian  Splatting Based on Thermodynamics


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-05 | Kun Yang, Yuxiang Liu, Zeyu Cui, Yu Liu, Maojun Zhang, Shen Yan, Qing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.03115v1){: .btn .btn-green } |

**Abstract**: Thermal infrared imaging offers the advantage of all-weather capability,
enabling non-intrusive measurement of an object's surface temperature.
Consequently, thermal infrared images are employed to reconstruct 3D models
that accurately reflect the temperature distribution of a scene, aiding in
applications such as building monitoring and energy management. However,
existing approaches predominantly focus on static 3D reconstruction for a
single time period, overlooking the impact of environmental factors on thermal
radiation and failing to predict or analyze temperature variations over time.
To address these challenges, we propose the NTR-Gaussian method, which treats
temperature as a form of thermal radiation, incorporating elements like
convective heat transfer and radiative heat dissipation. Our approach utilizes
neural networks to predict thermodynamic parameters such as emissivity,
convective heat transfer coefficient, and heat capacity. By integrating these
predictions, we can accurately forecast thermal temperatures at various times
throughout a nighttime scene. Furthermore, we introduce a dynamic dataset
specifically for nighttime thermal imagery. Extensive experiments and
evaluations demonstrate that NTR-Gaussian significantly outperforms comparison
methods in thermal reconstruction, achieving a predicted temperature error
within 1 degree Celsius.

Comments:
- IEEE Conference on Computer Vision and Pattern Recognition 2025

---

## Tracking-Aware Deformation Field Estimation for Non-rigid 3D  Reconstruction in Robotic Surgeries

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Zeqing Wang, Han Fang, Yihong Xu, Yutong Ban | cs.CV | [PDF](http://arxiv.org/pdf/2503.02558v1){: .btn .btn-green } |

**Abstract**: Minimally invasive procedures have been advanced rapidly by the robotic
laparoscopic surgery. The latter greatly assists surgeons in sophisticated and
precise operations with reduced invasiveness. Nevertheless, it is still safety
critical to be aware of even the least tissue deformation during
instrument-tissue interactions, especially in 3D space. To address this, recent
works rely on NeRF to render 2D videos from different perspectives and
eliminate occlusions. However, most of the methods fail to predict the accurate
3D shapes and associated deformation estimates robustly. Differently, we
propose Tracking-Aware Deformation Field (TADF), a novel framework which
reconstructs the 3D mesh along with the 3D tissue deformation simultaneously.
It first tracks the key points of soft tissue by a foundation vision model,
providing an accurate 2D deformation field. Then, the 2D deformation field is
smoothly incorporated with a neural implicit reconstruction network to obtain
tissue deformation in the 3D space. Finally, we experimentally demonstrate that
the proposed method provides more accurate deformation estimation compared with
other 3D neural reconstruction methods in two public datasets.



---

## Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yan Miao, Will Shen, Sayan Mitra | cs.RO | [PDF](http://arxiv.org/pdf/2503.02198v1){: .btn .btn-green } |

**Abstract**: We present the first framework demonstrating zero-shot sim-to-real transfer
of visual control policies learned in a Neural Radiance Field (NeRF)
environment for quadrotors to fly through racing gates. Robust transfer from
simulation to real flight poses a major challenge, as standard simulators often
lack sufficient visual fidelity. To address this, we construct a photorealistic
simulation environment of quadrotor racing tracks, called FalconGym, which
provides effectively unlimited synthetic images for training. Within FalconGym,
we develop a pipelined approach for crossing gates that combines (i) a Neural
Pose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor
poses from single-frame RGB images and IMU data, and (ii) a
self-attention-based multi-modal controller that adaptively integrates visual
features and pose estimation. This multi-modal design compensates for
perception noise and intermittent gate visibility. We train this controller
purely in FalconGym with imitation learning and deploy the resulting policy to
real hardware with no additional fine-tuning. Simulation experiments on three
distinct tracks (circle, U-turn and figure-8) demonstrate that our controller
outperforms a vision-only state-of-the-art baseline in both success rate and
gate-crossing accuracy. In 30 live hardware flights spanning three tracks and
120 gates, our controller achieves a 95.8% success rate and an average error of
just 10 cm when flying through 38 cm-radius gates.



---

## DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Haoyuan Li, Ziqin Ye, Yue Hao, Weiyang Lin, Chao Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.02223v1){: .btn .btn-green } |

**Abstract**: Accurate object perception is essential for robotic applications such as
object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM
system that seamlessly integrates object pose estimation and reconstruction. We
employ 3D Gaussian Splatting for high-fidelity object reconstruction and
leverage quadrics for precise object pose estimation. Both of them management
is handled on the CPU, while optimization is performed on the GPU,
significantly improving system efficiency. By associating objects with unique
IDs, our system enables rapid object extraction from the scene. Extensive
experimental results on object reconstruction and pose estimation demonstrate
that DQO-MAP achieves outstanding performance in terms of precision,
reconstruction quality, and computational efficiency. The code and dataset are
available at: https://github.com/LiHaoy-ux/DQO-MAP.



---

## Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic  Guidance from Dense Novel Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Yingji Zhong, Kaichen Zhou, Zhihao Li, Lanqing Hong, Zhenguo Li, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2503.02230v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable capabilities for
photorealistic novel view synthesis. One major deficiency of NeRF is that dense
inputs are typically required, and the rendering quality will drop drastically
given sparse inputs. In this paper, we highlight the effectiveness of rendered
semantics from dense novel views, and show that rendered semantics can be
treated as a more robust form of augmented data than rendered RGB. Our method
enhances NeRF's performance by incorporating guidance derived from the rendered
semantics. The rendered semantic guidance encompasses two levels: the
supervision level and the feature level. The supervision-level guidance
incorporates a bi-directional verification module that decides the validity of
each rendered semantic label, while the feature-level guidance integrates a
learnable codebook that encodes semantic-aware information, which is queried by
each point via the attention mechanism to obtain semantic-relevant predictions.
The overall semantic guidance is embedded into a self-improved pipeline. We
also introduce a more challenging sparse-input indoor benchmark, where the
number of inputs is limited to as few as 6. Experiments demonstrate the
effectiveness of our method and it exhibits superior performance compared to
existing approaches.



---

## 2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-04 | Qipeng Yan, Mingyang Sun, Lihua Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.02452v1){: .btn .btn-green } |

**Abstract**: Real-time rendering of high-fidelity and animatable avatars from monocular
videos remains a challenging problem in computer vision and graphics. Over the
past few years, the Neural Radiance Field (NeRF) has made significant progress
in rendering quality but behaves poorly in run-time performance due to the low
efficiency of volumetric rendering. Recently, methods based on 3D Gaussian
Splatting (3DGS) have shown great potential in fast training and real-time
rendering. However, they still suffer from artifacts caused by inaccurate
geometry. To address these problems, we propose 2DGS-Avatar, a novel approach
based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars
with high-fidelity and fast training performance. Given monocular RGB videos as
input, our method generates an avatar that can be driven by poses and rendered
in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the
advantages of fast training and rendering while also capturing detailed,
dynamic, and photo-realistic appearances. We conduct abundant experiments on
popular datasets such as AvatarRex and THuman4.0, demonstrating impressive
performance in both qualitative and quantitative metrics.

Comments:
- ICVRV 2024

---

## LiteGS: A High-Performance Modular Framework for Gaussian Splatting  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Kaimin Liao | cs.CV | [PDF](http://arxiv.org/pdf/2503.01199v1){: .btn .btn-green } |

**Abstract**: Gaussian splatting has emerged as a powerful technique for reconstruction of
3D scenes in computer graphics and vision. However, conventional
implementations often suffer from inefficiencies, limited flexibility, and high
computational overhead, which constrain their adaptability to diverse
applications. In this paper, we present LiteGS,a high-performance and modular
framework that enhances both the efficiency and usability of Gaussian
splatting. LiteGS achieves a 3.4x speedup over the original 3DGS implementation
while reducing GPU memory usage by approximately 30%. Its modular design
decomposes the splatting process into multiple highly optimized operators, and
it provides dual API support via a script-based interface and a CUDA-based
interface. The script-based interface, in combination with autograd, enables
rapid prototyping and straightforward customization of new ideas, while the
CUDA-based interface delivers optimal training speeds for performance-critical
applications. LiteGS retains the core algorithm of 3DGS, ensuring
compatibility. Comprehensive experiments on the Mip-NeRF 360 dataset
demonstrate that LiteGS accelerates training without compromising accuracy,
making it an ideal solution for both rapid prototyping and production
environments.



---

## Category-level Meta-learned NeRF Priors for Efficient Object Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez | cs.CV | [PDF](http://arxiv.org/pdf/2503.01582v2){: .btn .btn-green } |

**Abstract**: In 3D object mapping, category-level priors enable efficient object
reconstruction and canonical pose estimation, requiring only a single prior per
semantic category (e.g., chair, book, laptop). Recently, DeepSDF has
predominantly been used as a category-level shape prior, but it struggles to
reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs
capture fine details but have yet to be effectively integrated with
category-level priors in a real-time multi-object mapping framework. To bridge
this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper
that integrates category-level priors with object-level NeRFs to enhance
reconstruction efficiency while enabling canonical object pose estimation.
PRENOM gets to know objects on a first-name basis by meta-learning on synthetic
reconstruction tasks generated from open-source shape datasets. To account for
object category variations, it employs a multi-objective genetic algorithm to
optimize the NeRF architecture for each category, balancing reconstruction
quality and training time. Additionally, prior-based probabilistic ray sampling
directs sampling toward expected object regions, accelerating convergence and
improving reconstruction quality under constrained resources. Experimental
results on a low-end GPU highlight the ability of PRENOM to achieve
high-quality reconstructions while maintaining computational feasibility.
Specifically, comparisons with prior-free NeRF-based approaches on a synthetic
dataset show a 21% lower Chamfer distance, demonstrating better reconstruction
quality. Furthermore, evaluations against other approaches using shape priors
on a noisy real-world dataset indicate a 13% improvement averaged across all
reconstruction metrics, and comparable pose and size estimation accuracy, while
being trained for 5x less time.



---

## Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, Mohamed Sayed | cs.CV | [PDF](http://arxiv.org/pdf/2503.02009v1){: .btn .btn-green } |

**Abstract**: Exploring real-world spaces using novel-view synthesis is fun, and
reimagining those worlds in a different style adds another layer of excitement.
Stylized worlds can also be used for downstream tasks where there is limited
training data and a need to expand a model's training distribution. Most
current novel-view synthesis stylization techniques lack the ability to
convincingly change geometry. This is because any geometry change requires
increased style strength which is often capped for stylization stability and
consistency. In this work, we propose a new autoregressive 3D Gaussian
Splatting stylization method. As part of this method, we contribute a new RGBD
diffusion model that allows for strength control over appearance and shape
stylization. To ensure consistency across stylized frames, we use a combination
of novel depth-guided cross attention, feature injection, and a Warp ControlNet
conditioned on composite frames for guiding the stylization of new frames. We
validate our method via extensive qualitative results, quantitative
experiments, and a user study. Code will be released online.



---

## Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling | cs.CV | [PDF](http://arxiv.org/pdf/2503.01774v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D
reconstruction and novel-view synthesis task. However, achieving photorealistic
rendering from extreme novel viewpoints remains challenging, as artifacts
persist across representations. In this work, we introduce Difix3D+, a novel
pipeline designed to enhance 3D reconstruction and novel-view synthesis through
single-step diffusion models. At the core of our approach is Difix, a
single-step image diffusion model trained to enhance and remove artifacts in
rendered novel views caused by underconstrained regions of the 3D
representation. Difix serves two critical roles in our pipeline. First, it is
used during the reconstruction phase to clean up pseudo-training views that are
rendered from the reconstruction and then distilled back into 3D. This greatly
enhances underconstrained regions and improves the overall 3D representation
quality. More importantly, Difix also acts as a neural enhancer during
inference, effectively removing residual artifacts arising from imperfect 3D
supervision and the limited capacity of current reconstruction models. Difix3D+
is a general solution, a single model compatible with both NeRF and 3DGS
representations, and it achieves an average 2$\times$ improvement in FID score
over baselines while maintaining 3D consistency.

Comments:
- CVPR 2025

---

## Data Augmentation for NeRFs in the Low Data Limit

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Ayush Gaggar, Todd D. Murphey | cs.CV | [PDF](http://arxiv.org/pdf/2503.02092v1){: .btn .btn-green } |

**Abstract**: Current methods based on Neural Radiance Fields fail in the low data limit,
particularly when training on incomplete scene data. Prior works augment
training data only in next-best-view applications, which lead to hallucinations
and model collapse with sparse data. In contrast, we propose adding a set of
views during training by rejection sampling from a posterior uncertainty
distribution, generated by combining a volumetric uncertainty estimator with
spatial coverage. We validate our results on partially observed scenes; on
average, our method performs 39.9% better with 87.5% less variability across
established scene reconstruction benchmarks, as compared to state of the art
baselines. We further demonstrate that augmenting the training set by sampling
from any distribution leads to better, more consistent scene reconstruction in
sparse environments. This work is foundational for robotic tasks where
augmenting a dataset with informative data is critical in resource-constrained,
a priori unknown environments. Videos and source code are available at
https://murpheylab.github.io/low-data-nerf/.

Comments:
- To be published in 2025 IEEE International Conference on Robotics and
  Automation (ICRA 2025)

---

## OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for  Object-Level Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Dianyi Yang, Yu Gao, Xihan Wang, Yufeng Yue, Yi Yang, Mengyin Fu | cs.CV | [PDF](http://arxiv.org/pdf/2503.01646v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have significantly improved the
efficiency and quality of dense semantic SLAM. However, previous methods are
generally constrained by limited-category pre-trained classifiers and implicit
semantic representation, which hinder their performance in open-set scenarios
and restrict 3D object-level scene understanding. To address these issues, we
propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian
representation to perform dense semantic SLAM in open-set environments. Our
system integrates explicit semantic labels derived from 2D foundational models
into the 3D Gaussian framework, facilitating robust 3D object-level scene
understanding. We introduce Gaussian Voting Splatting to enable fast 2D label
map rendering and scene updating. Additionally, we propose a Confidence-based
2D Label Consensus method to ensure consistent labeling across multiple views.
Furthermore, we employ a Segmentation Counter Pruning strategy to improve the
accuracy of semantic scene representation. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method
in scene understanding, tracking, and mapping, achieving 10 times faster
semantic rendering and 2 times lower storage costs compared to existing
methods. Project page: https://young-bit.github.io/opengs-github.github.io/.



---

## FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with  Sparse and Dense Map Fusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-03 | Yansong Xu, Junlin Li, Wei Zhang, Siyu Chen, Shengyong Zhang, Yuquan Leng, Weijia Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2503.01109v1){: .btn .btn-green } |

**Abstract**: 3D gaussian splatting has advanced simultaneous localization and mapping
(SLAM) technology by enabling real-time positioning and the construction of
high-fidelity maps. However, the uncertainty in gaussian position and
initialization parameters introduces challenges, often requiring extensive
iterative convergence and resulting in redundant or insufficient gaussian
representations. To address this, we introduce a novel adaptive densification
method based on Fourier frequency domain analysis to establish gaussian priors
for rapid convergence. Additionally, we propose constructing independent and
unified sparse and dense maps, where a sparse map supports efficient tracking
via Generalized Iterative Closest Point (GICP) and a dense map creates
high-fidelity visual representations. This is the first SLAM system leveraging
frequency domain analysis to achieve high-quality gaussian mapping in
real-time. Experimental results demonstrate an average frame rate of 36 FPS on
Replica and TUM RGB-D datasets, achieving competitive accuracy in both
localization and mapping.



---

## DreamPrinting: Volumetric Printing Primitives for High-Fidelity 3D  Printing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Youjia Wang, Ruixiang Cao, Teng Xu, Yifei Liu, Dong Zhang, Yiwen Wu, Jingyi Yu | cs.GR | [PDF](http://arxiv.org/pdf/2503.00887v1){: .btn .btn-green } |

**Abstract**: Translating the rich visual fidelity of volumetric rendering techniques into
physically realizable 3D prints remains an open challenge. We introduce
DreamPrinting, a novel pipeline that transforms radiance-based volumetric
representations into explicit, material-centric Volumetric Printing Primitives
(VPPs). While volumetric rendering primitives (e.g., NeRF) excel at capturing
intricate geometry and appearance, they lack the physical constraints necessary
for real-world fabrication, such as pigment compatibility and material density.
DreamPrinting addresses these challenges by integrating the Kubelka-Munk model
with a spectrophotometric calibration process to characterize and mix pigments
for accurate reproduction of color and translucency. The result is a
continuous-to-discrete mapping that determines optimal pigment concentrations
for each voxel, ensuring fidelity to both geometry and optical properties. A 3D
stochastic halftoning procedure then converts these concentrations into
printable labels, enabling fine-grained control over opacity, texture, and
color gradients. Our evaluations show that DreamPrinting achieves exceptional
detail in reproducing semi-transparent structures-such as fur, leaves, and
clouds-while outperforming traditional surface-based methods in managing
translucency and internal consistency. Furthermore, by seamlessly integrating
VPPs with cutting-edge 3D generation techniques, DreamPrinting expands the
potential for complex, high-quality volumetric prints, providing a robust
framework for printing objects that closely mirror their digital origins.



---

## Evolving High-Quality Rendering and Reconstruction in a Unified  Framework with Contribution-Adaptive Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao | cs.CV | [PDF](http://arxiv.org/pdf/2503.00881v1){: .btn .btn-green } |

**Abstract**: Representing 3D scenes from multiview images is a core challenge in computer
vision and graphics, which requires both precise rendering and accurate
reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant
attention for its high-quality rendering and fast inference speed. Yet, due to
the unstructured and irregular nature of Gaussian point clouds, ensuring
accurate geometry reconstruction remains difficult. Existing methods primarily
focus on geometry regularization, with common approaches including
primitive-based and dual-model frameworks. However, the former suffers from
inherent conflicts between rendering and reconstruction, while the latter is
computationally and storage-intensive. To address these challenges, we propose
CarGS, a unified model leveraging Contribution-adaptive regularization to
achieve simultaneous, high-quality rendering and surface reconstruction. The
essence of our framework is learning adaptive contribution for Gaussian
primitives by squeezing the knowledge from geometry regularization into a
compact MLP. Additionally, we introduce a geometry-guided densification
strategy with clues from both normals and Signed Distance Fields (SDF) to
improve the capability of capturing high-frequency details. Our design improves
the mutual learning of the two tasks, meanwhile its unified structure does not
require separate models as in dual-model based approaches, guaranteeing
efficiency. Extensive experiments demonstrate the ability to achieve
state-of-the-art (SOTA) results in both rendering fidelity and reconstruction
accuracy while maintaining real-time speed and minimal storage size.



---

## DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Liao Shen, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy | cs.CV | [PDF](http://arxiv.org/pdf/2503.00746v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable
success in representing 3D scenes and generating high-quality, novel views in
real-time. However, 3D-GS and its variants assume that input images are
captured based on pinhole imaging and are fully in focus. This assumption
limits their applicability, as real-world images often feature shallow
depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable
depth-of-field method for 3D-GS. We develop a lens-based imaging model based on
geometric optics principles to control DoF effects. To ensure accurate scene
geometry, we incorporate depth priors adjusted per scene, and we apply
defocus-to-focus adaptation to minimize the gap in the circle of confusion. We
also introduce a synthetic dataset to assess refocusing capabilities and the
model's ability to learn precise lens parameters. Our framework is customizable
and supports various interactive applications. Extensive experiments confirm
the effectiveness of our method. Our project is available at
https://dof-gaussian.github.io.

Comments:
- CVPR 2025

---

## PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency  Recovery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | BoCheng Li, WenJuan Zhang, Bing Zhang, YiLing Yao, YaNing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2503.00848v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D GS) achieves impressive results in novel view
synthesis for small, single-object scenes through Gaussian ellipsoid
initialization and adaptive density control. However, when applied to
large-scale remote sensing scenes, 3D GS faces challenges: the point clouds
generated by Structure-from-Motion (SfM) are often sparse, and the inherent
smoothing behavior of 3D GS leads to over-reconstruction in high-frequency
regions, where have detailed textures and color variations. This results in the
generation of large, opaque Gaussian ellipsoids that cause gradient artifacts.
Moreover, the simultaneous optimization of both geometry and texture may lead
to densification of Gaussian ellipsoids at incorrect geometric locations,
resulting in artifacts in other views. To address these issues, we propose
PSRGS, a progressive optimization scheme based on spectral residual maps.
Specifically, we create a spectral residual significance map to separate
low-frequency and high-frequency regions. In the low-frequency region, we apply
depth-aware and depth-smooth losses to initialize the scene geometry with low
threshold. For the high-frequency region, we use gradient features with higher
threshold to split and clone ellipsoids, refining the scene. The sampling rate
is determined by feature responses and gradient loss. Finally, we introduce a
pre-trained network that jointly computes perceptual loss from multiple views,
ensuring accurate restoration of high-frequency details in both Gaussian
ellipsoids geometry and color. We conduct experiments on multiple datasets to
assess the effectiveness of our method, which demonstrates competitive
rendering quality, especially in recovering texture details in high-frequency
regions.



---

## Enhancing Monocular 3D Scene Completion with Diffusion Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Changlin Song, Jiaqi Wang, Liyun Zhu, He Weng | cs.GR | [PDF](http://arxiv.org/pdf/2503.00726v1){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction is essential for applications in virtual reality,
robotics, and autonomous driving, enabling machines to understand and interact
with complex environments. Traditional 3D Gaussian Splatting techniques rely on
images captured from multiple viewpoints to achieve optimal performance, but
this dependence limits their use in scenarios where only a single image is
available. In this work, we introduce FlashDreamer, a novel approach for
reconstructing a complete 3D scene from a single image, significantly reducing
the need for multi-view inputs. Our approach leverages a pre-trained
vision-language model to generate descriptive prompts for the scene, guiding a
diffusion model to produce images from various perspectives, which are then
fused to form a cohesive 3D reconstruction. Extensive experiments show that our
method effectively and robustly expands single-image inputs into a
comprehensive 3D scene, extending monocular 3D reconstruction capabilities
without further training. Our code is available
https://github.com/CharlieSong1999/FlashDreamer/tree/main.

Comments:
- All authors had equal contribution

---

## Vid2Fluid: 3D Dynamic Fluid Assets from Single-View Videos with  Generative Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-02 | Zhiwei Zhao, Alan Zhao, Minchen Li, Yixin Hu | cs.GR | [PDF](http://arxiv.org/pdf/2503.00868v1){: .btn .btn-green } |

**Abstract**: The generation of 3D content from single-view images has been extensively
studied, but 3D dynamic scene generation with physical consistency from videos
remains in its early stages. We propose a novel framework leveraging generative
3D Gaussian Splatting (3DGS) models to extract 3D dynamic fluid objects from
single-view videos. The fluid geometry represented by 3DGS is initially
generated from single-frame images, then denoised, densified, and aligned
across frames. We estimate the fluid surface velocity using optical flow and
compute the mainstream of the fluid to refine it. The 3D volumetric velocity
field is then derived from the enclosed surface. The velocity field is then
converted into a divergence-free, grid-based representation, enabling the
optimization of simulation parameters through its differentiability across
frames. This process results in simulation-ready fluid assets with physical
dynamics closely matching those observed in the source video. Our approach is
applicable to various fluid types, including gas, liquid, and viscous fluids,
and allows users to edit the output geometry or extend movement durations
seamlessly. Our automatic method for creating 3D dynamic fluid assets from
single-view videos, easily obtainable from the internet, shows great potential
for generating large-scale 3D fluid assets at a low cost.



---

## Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic  Pick-and-Place Setups

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Nicholas Pfaff, Evelyn Fu, Jeremy Binagia, Phillip Isola, Russ Tedrake | cs.RO | [PDF](http://arxiv.org/pdf/2503.00370v1){: .btn .btn-green } |

**Abstract**: Simulating object dynamics from real-world perception shows great promise for
digital twins and robotic manipulation but often demands labor-intensive
measurements and expertise. We present a fully automated Real2Sim pipeline that
generates simulation-ready assets for real-world objects through robotic
interaction. Using only a robot's joint torque sensors and an external camera,
the pipeline identifies visual geometry, collision geometry, and physical
properties such as inertial parameters. Our approach introduces a general
method for extracting high-quality, object-centric meshes from photometric
reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing
alpha-transparent training while explicitly distinguishing foreground
occlusions from background subtraction. We validate the full pipeline through
extensive experiments, demonstrating its effectiveness across diverse objects.
By eliminating the need for manual intervention or environment modifications,
our pipeline can be integrated directly into existing pick-and-place setups,
enabling scalable and efficient dataset creation.



---

## GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation  Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Runyi Li, Xuanyu Zhang, Chuhan Tong, Zhipei Xu, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2503.00531v1){: .btn .btn-green } |

**Abstract**: With the advancement of AIGC technologies, the modalities generated by models
have expanded from images and videos to 3D objects, leading to an increasing
number of works focused on 3D Gaussian Splatting (3DGS) generative models.
Existing research on copyright protection for generative models has primarily
concentrated on watermarking in image and text modalities, with little
exploration into the copyright protection of 3D object generative models. In
this paper, we propose the first bit watermarking framework for 3DGS generative
models, named GaussianSeal, to enable the decoding of bits as copyright
identifiers from the rendered outputs of generated 3DGS. By incorporating
adaptive bit modulation modules into the generative model and embedding them
into the network blocks in an adaptive way, we achieve high-precision bit
decoding with minimal training overhead while maintaining the fidelity of the
model's outputs. Experiments demonstrate that our method outperforms
post-processing watermarking approaches for 3DGS objects, achieving superior
performance of watermark decoding accuracy and preserving the quality of the
generated results.



---

## Abstract Rendering: Computing All that is Seen in Gaussian Splat Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yangge Li, Chenxi Ji, Xiangru Zhong, Huan Zhang, Sayan Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2503.00308v2){: .btn .btn-green } |

**Abstract**: We introduce abstract rendering, a method for computing a set of images by
rendering a scene from a continuously varying range of camera positions. The
resulting abstract image-which encodes an infinite collection of possible
renderings-is represented using constraints on the image matrix, enabling
rigorous uncertainty propagation through the rendering process. This capability
is particularly valuable for the formal verification of vision-based autonomous
systems and other safety-critical applications. Our approach operates on
Gaussian splat scenes, an emerging representation in computer vision and
robotics. We leverage efficient piecewise linear bound propagation to abstract
fundamental rendering operations, while addressing key challenges that arise in
matrix inversion and depth sorting-two operations not directly amenable to
standard approximations. To handle these, we develop novel linear relational
abstractions that maintain precision while ensuring computational efficiency.
These abstractions not only power our abstract rendering algorithm but also
provide broadly applicable tools for other rendering problems. Our
implementation, AbstractSplat, is optimized for scalability, handling up to
750k Gaussians while allowing users to balance memory and runtime through tile
and batch-based computation. Compared to the only existing abstract image
method for mesh-based scenes, AbstractSplat achieves 2-14x speedups while
preserving precision. Our results demonstrate that continuous camera motion,
rotations, and scene variations can be rigorously analyzed at scale, making
abstract rendering a powerful tool for uncertainty-aware vision applications.



---

## Seeing A 3D World in A Grain of Sand

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yufan Zhang, Yu Ji, Yu Guo, Jinwei Ye | cs.CV | [PDF](http://arxiv.org/pdf/2503.00260v1){: .btn .btn-green } |

**Abstract**: We present a snapshot imaging technique for recovering 3D surrounding views
of miniature scenes. Due to their intricacy, miniature scenes with objects
sized in millimeters are difficult to reconstruct, yet miniatures are common in
life and their 3D digitalization is desirable. We design a catadioptric imaging
system with a single camera and eight pairs of planar mirrors for snapshot 3D
reconstruction from a dollhouse perspective. We place paired mirrors on nested
pyramid surfaces for capturing surrounding multi-view images in a single shot.
Our mirror design is customizable based on the size of the scene for optimized
view coverage. We use the 3D Gaussian Splatting (3DGS) representation for scene
reconstruction and novel view synthesis. We overcome the challenge posed by our
sparse view input by integrating visual hull-derived depth constraint. Our
method demonstrates state-of-the-art performance on a variety of synthetic and
real miniature scenes.



---

## CAT-3DGS: A Context-Adaptive Triplane Approach to  Rate-Distortion-Optimized 3DGS Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-03-01 | Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, Wen-Hsiao Peng | cs.CV | [PDF](http://arxiv.org/pdf/2503.00357v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D
representation. Much research has been focused on reducing its storage
requirements and memory footprint. However, the needs to compress and transmit
the 3DGS representation to the remote side are overlooked. This new application
calls for rate-distortion-optimized 3DGS compression. How to quantize and
entropy encode sparse Gaussian primitives in the 3D space remains largely
unexplored. Few early attempts resort to the hyperprior framework from learned
image compression. But, they fail to utilize fully the inter and intra
correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work,
termed CAT-3DGS, introduces a context-adaptive triplane approach to their
rate-distortion-optimized coding. It features multi-scale triplanes, oriented
according to the principal axes of Gaussian primitives in the 3D space, to
capture their inter correlation (i.e. spatial correlation) for spatial
autoregressive coding in the projected 2D planes. With these triplanes serving
as the hyperprior, we further perform channel-wise autoregressive coding to
leverage the intra correlation within each individual Gaussian primitive. Our
CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively
skips from coding those Gaussian primitives that potentially have little impact
on the rendering quality. When trained end-to-end to strike a good
rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art
compression performance on the commonly used real-world datasets.

Comments:
- Accepted for Publication in International Conference on Learning
  Representations (ICLR)
