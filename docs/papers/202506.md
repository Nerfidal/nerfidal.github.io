---
layout: default
title: June 2025
parent: Papers
nav_order: 202506
---

<!---metadata--->


## DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian  Splatting and a Low-Rank Free-Form Deformation Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-27 | Yuliang Huang, Imraj Singh, Thomas Joyce, Kris Thielemans, Jamie R. McClelland | eess.IV | [PDF](http://arxiv.org/pdf/2506.22280v1){: .btn .btn-green } |

**Abstract**: 3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion
artifacts due to breathing. A common clinical approach mitigates this by
sorting projections into respiratory phases and reconstructing images per
phase, but this does not account for breathing variability. Dynamic CBCT
instead reconstructs images at each projection, capturing continuous motion
without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS)
offer powerful tools for modeling dynamic scenes, yet their application to
dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane,
use implicit motion representations, which are computationally expensive. While
explicit low-rank motion models have been proposed, they lack spatial
regularization, leading to inconsistencies in Gaussian motion. To address these
limitations, we introduce a free-form deformation (FFD)-based spatial basis
function and a deformation-informed framework that enforces consistency by
coupling the temporal evolution of Gaussian's mean position, scale, and
rotation under a unified deformation field. We evaluate our approach on six
CBCT datasets, demonstrating superior image quality with a 6x speedup over
HexPlane. These results highlight the potential of deformation-informed 4DGS
for efficient, motion-compensated CBCT reconstruction. The code is available at
https://github.com/Yuliang-Huang/DIGS.

Comments:
- Accepted by MICCAI 2025

---

## BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-27 | Zipei Ma, Junzhe Jiang, Yurui Chen, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2506.22099v1){: .btn .btn-green } |

**Abstract**: The realistic reconstruction of street scenes is critical for developing
real-world simulators in autonomous driving. Most existing methods rely on
object pose annotations, using these poses to reconstruct dynamic objects and
move them during the rendering process. This dependence on high-precision
object annotations limits large-scale and extensive scene reconstruction. To
address this challenge, we propose B\'ezier curve Gaussian splatting
(B\'ezierGS), which represents the motion trajectories of dynamic objects using
learnable B\'ezier curves. This approach fully leverages the temporal
information of dynamic objects and, through learnable curve modeling,
automatically corrects pose errors. By introducing additional supervision on
dynamic object rendering and inter-curve consistency constraints, we achieve
reasonable and accurate separation and reconstruction of scene elements.
Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark
demonstrate that B\'ezierGS outperforms state-of-the-art alternatives in both
dynamic and static scene components reconstruction and novel view synthesis.

Comments:
- Accepted at ICCV 2025, Project Page:
  https://github.com/fudan-zvg/BezierGS

---

## UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-27 | Fabian Perez, Sara Rojas, Carlos Hinojosa, Hoover Rueda-Chacón, Bernard Ghanem | eess.IV | [PDF](http://arxiv.org/pdf/2506.21884v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF)-based segmentation methods focus on object
semantics and rely solely on RGB data, lacking intrinsic material properties.
This limitation restricts accurate material perception, which is crucial for
robotics, augmented reality, simulation, and other applications. We introduce
UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling
joint hyperspectral novel view synthesis and unsupervised material
segmentation. Our method models spectral reflectance via diffuse and specular
components, where a learned dictionary of global endmembers represents pure
material signatures, and per-point abundances capture their distribution. For
material segmentation, we use spectral signature predictions along learned
endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF
enables scene editing by modifying learned endmember dictionaries for flexible
material-based appearance manipulation. Extensive experiments validate our
approach, demonstrating superior spectral reconstruction and material
segmentation to existing methods. Project page:
https://www.factral.co/UnMix-NeRF.

Comments:
- Paper accepted at ICCV 2025 main conference

---

## Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-26 | Zhirui Gao. Renjiao Yi, Yaqiao Dai, Xuening Zhu, Wei Chen, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2506.21401v1){: .btn .btn-green } |

**Abstract**: This paper presents an end-to-end framework for reconstructing 3D parametric
curves directly from multi-view edge maps. Contrasting with existing two-stage
methods that follow a sequential ``edge point cloud reconstruction and
parametric curve fitting'' pipeline, our one-stage approach optimizes 3D
parametric curves directly from 2D edge maps, eliminating error accumulation
caused by the inherent optimization gap between disconnected stages. However,
parametric curves inherently lack suitability for rendering-based multi-view
optimization, necessitating a complementary representation that preserves their
geometric properties while enabling differentiable rendering. We propose a
novel bi-directional coupling mechanism between parametric curves and
edge-oriented Gaussian components. This tight correspondence formulates a
curve-aware Gaussian representation, \textbf{CurveGaussian}, that enables
differentiable rendering of 3D curves, allowing direct optimization guided by
multi-view evidence. Furthermore, we introduce a dynamically adaptive topology
optimization framework during training to refine curve structures through
linearization, merging, splitting, and pruning operations. Comprehensive
evaluations on the ABC dataset and real-world benchmarks demonstrate our
one-stage method's superiority over two-stage alternatives, particularly in
producing cleaner and more robust reconstructions. Additionally, by directly
optimizing parametric curves, our method significantly reduces the parameter
count during training, achieving both higher efficiency and superior
performance compared to existing approaches.

Comments:
- Code: https://github.com/zhirui-gao/Curve-Gaussian Accepted by ICCV
  2025

---

## Geometry and Perception Guided Gaussians for Multiview-consistent 3D  Generation from a Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-26 | Pufan Li, Bi'an Du, Wei Hu | cs.CV | [PDF](http://arxiv.org/pdf/2506.21152v1){: .btn .btn-green } |

**Abstract**: Generating realistic 3D objects from single-view images requires natural
appearance, 3D consistency, and the ability to capture multiple plausible
interpretations of unseen regions. Existing approaches often rely on
fine-tuning pretrained 2D diffusion models or directly generating 3D
information through fast network inference or 3D Gaussian Splatting, but their
results generally suffer from poor multiview consistency and lack geometric
detail. To takle these issues, we present a novel method that seamlessly
integrates geometry and perception priors without requiring additional model
training to reconstruct detailed 3D objects from a single image. Specifically,
we train three different Gaussian branches initialized from the geometry prior,
perception prior and Gaussian noise, respectively. The geometry prior captures
the rough 3D shapes, while the perception prior utilizes the 2D pretrained
diffusion model to enhance multiview information. Subsequently, we refine 3D
Gaussian branches through mutual interaction between geometry and perception
priors, further enhanced by a reprojection-based strategy that enforces depth
consistency. Experiments demonstrate the higher-fidelity reconstruction results
of our method, outperforming existing methods on novel view synthesis and 3D
reconstruction, demonstrating robust and consistent 3D object generation.

Comments:
- 10 pages, 5 figures

---

## DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via  Sparse-Controlled Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-26 | Yeon-Ji Song, Jaein Kim, Byung-Ju Kim, Byoung-Tak Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2506.20998v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis is a task of generating scenes from unseen perspectives;
however, synthesizing dynamic scenes from blurry monocular videos remains an
unresolved challenge that has yet to be effectively addressed. Existing novel
view synthesis methods are often constrained by their reliance on
high-resolution images or strong assumptions about static geometry and rigid
scene priors. Consequently, their approaches lack robustness in real-world
environments with dynamic object and camera motion, leading to instability and
degraded visual fidelity. To address this, we propose Motion-aware Dynamic View
Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting
(DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular
videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry
videos and reconstructing detailed 3D geometry of the scene affected by dynamic
motion variations. Our model achieves robust performance in novel view
synthesis under dynamic blurry scenes and sets a new benchmark in realistic
novel view synthesis for blurry monocular video inputs.

Comments:
- CVPRW 2025, Neural Fields Beyond Conventional Cameras

---

## User-in-the-Loop View Sampling with Error Peaking Visualization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-26 | Ayaka Yasunaga, Hideo Saito, Shohei Mori | cs.CV | [PDF](http://arxiv.org/pdf/2506.21009v1){: .btn .btn-green } |

**Abstract**: Augmented reality (AR) provides ways to visualize missing view samples for
novel view synthesis. Existing approaches present 3D annotations for new view
samples and task users with taking images by aligning the AR display. This data
collection task is known to be mentally demanding and limits capture areas to
pre-defined small areas due to the ideal but restrictive underlying sampling
theory. To free users from 3D annotations and limited scene exploration, we
propose using locally reconstructed light fields and visualizing errors to be
removed by inserting new views. Our results show that the error-peaking
visualization is less invasive, reduces disappointment in final results, and is
satisfactory with fewer view samples in our mobile view synthesis system. We
also show that our approach can contribute to recent radiance field
reconstruction for larger scenes, such as 3D Gaussian splatting.

Comments:
- Accepted at IEEE ICIP 2025, Project Page:
  https://mediated-reality.github.io/projects/yasunaga_icip25/

---

## CL-Splats: Continual Learning of Gaussian Splatting with Local  Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-26 | Jan Ackermann, Jonas Kulhanek, Shengqu Cai, Haofei Xu, Marc Pollefeys, Gordon Wetzstein, Leonidas Guibas, Songyou Peng | cs.CV | [PDF](http://arxiv.org/pdf/2506.21117v1){: .btn .btn-green } |

**Abstract**: In dynamic 3D environments, accurately updating scene representations over
time is crucial for applications in robotics, mixed reality, and embodied AI.
As scenes evolve, efficient methods to incorporate changes are needed to
maintain up-to-date, high-quality reconstructions without the computational
overhead of re-optimizing the entire scene. This paper introduces CL-Splats,
which incrementally updates Gaussian splatting-based 3D representations from
sparse scene captures. CL-Splats integrates a robust change-detection module
that segments updated and static components within the scene, enabling focused,
local optimization that avoids unnecessary re-computation. Moreover, CL-Splats
supports storing and recovering previous scene states, facilitating temporal
segmentation and new scene-analysis applications. Our extensive experiments
demonstrate that CL-Splats achieves efficient updates with improved
reconstruction quality over the state-of-the-art. This establishes a robust
foundation for future real-time adaptation in 3D scene reconstruction tasks.

Comments:
- ICCV 2025, Project Page: https://cl-splats.github.io

---

## EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-26 | Taoyu Wu, Yiyi Miao, Zhuoxiao Li, Haocheng Zhao, Kang Dang, Jionglong Su, Limin Yu, Haoang Li | cs.CV | [PDF](http://arxiv.org/pdf/2506.21420v1){: .btn .btn-green } |

**Abstract**: Efficient three-dimensional reconstruction and real-time visualization are
critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian
Splatting (3DGS) has demonstrated remarkable performance in efficient 3D
reconstruction and rendering. Most 3DGS-based Simultaneous Localization and
Mapping (SLAM) methods only rely on the appearance constraints for optimizing
both 3DGS and camera poses. However, in endoscopic scenarios, the challenges
include photometric inconsistencies caused by non-Lambertian surfaces and
dynamic motion from breathing affects the performance of SLAM systems. To
address these issues, we additionally introduce optical flow loss as a
geometric constraint, which effectively constrains both the 3D structure of the
scene and the camera motion. Furthermore, we propose a depth regularisation
strategy to mitigate the problem of photometric inconsistencies and ensure the
validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve
scene representation in the SLAM system, we improve the 3DGS refinement
strategy by focusing on viewpoints corresponding to Keyframes with suboptimal
rendering quality frames, achieving better rendering results. Extensive
experiments on the C3VD static dataset and the StereoMIS dynamic dataset
demonstrate that our method outperforms existing state-of-the-art methods in
novel view synthesis and pose estimation, exhibiting high performance in both
static and dynamic surgical scenes. The source code will be publicly available
upon paper acceptance.



---

## PanSt3R: Multi-view Consistent Panoptic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-26 | Lojze Zust, Yohann Cabon, Juliette Marrie, Leonid Antsfeld, Boris Chidlovskii, Jerome Revaud, Gabriela Csurka | cs.CV | [PDF](http://arxiv.org/pdf/2506.21348v1){: .btn .btn-green } |

**Abstract**: Panoptic segmentation of 3D scenes, involving the segmentation and
classification of object instances in a dense 3D reconstruction of a scene, is
a challenging problem, especially when relying solely on unposed 2D images.
Existing approaches typically leverage off-the-shelf models to extract
per-frame 2D panoptic segmentations, before optimizing an implicit geometric
representation (often based on NeRF) to integrate and fuse the 2D predictions.
We argue that relying on 2D panoptic segmentation for a problem inherently 3D
and multi-view is likely suboptimal as it fails to leverage the full potential
of spatial relationships across views. In addition to requiring camera
parameters, these approaches also necessitate computationally expensive
test-time optimization for each scene. Instead, in this work, we propose a
unified and integrated approach PanSt3R, which eliminates the need for
test-time optimization by jointly predicting 3D geometry and multi-view
panoptic segmentation in a single forward pass. Our approach builds upon recent
advances in 3D reconstruction, specifically upon MUSt3R, a scalable multi-view
version of DUSt3R, and enhances it with semantic awareness and multi-view
panoptic segmentation capabilities. We additionally revisit the standard
post-processing mask merging procedure and introduce a more principled approach
for multi-view segmentation. We also introduce a simple method for generating
novel-view predictions based on the predictions of PanSt3R and vanilla 3DGS.
Overall, the proposed PanSt3R is conceptually simple, yet fast and scalable,
and achieves state-of-the-art performance on several benchmarks, while being
orders of magnitude faster than existing methods.

Comments:
- Accepted at ICCV 2025

---

## MADrive: Memory-Augmented Driving Scene Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-26 | Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, Dmitry Baranchuk | cs.CV | [PDF](http://arxiv.org/pdf/2506.21520v1){: .btn .btn-green } |

**Abstract**: Recent advances in scene reconstruction have pushed toward highly realistic
modeling of autonomous driving (AD) environments using 3D Gaussian splatting.
However, the resulting reconstructions remain closely tied to the original
observations and struggle to support photorealistic synthesis of significantly
altered or novel driving scenarios. This work introduces MADrive, a
memory-augmented reconstruction framework designed to extend the capabilities
of existing scene reconstruction methods by replacing observed vehicles with
visually similar 3D assets retrieved from a large-scale external memory bank.
Specifically, we release MAD-Cars, a curated dataset of ${\sim}70$K 360{\deg}
car videos captured in the wild and present a retrieval module that finds the
most similar car instances in the memory bank, reconstructs the corresponding
3D assets from video, and integrates them into the target scene through
orientation alignment and relighting. The resulting replacements provide
complete multi-view representations of vehicles in the scene, enabling
photorealistic synthesis of substantially altered configurations, as
demonstrated in our experiments. Project page:
https://yandex-research.github.io/madrive/



---

## SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian  Splatting Based on a Skinning Model


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-25 | Da Li, Donggang Jia, Markus Hadwiger, Ivan Viola | cs.GR | [PDF](http://arxiv.org/pdf/2506.21632v1){: .btn .btn-green } |

**Abstract**: Reconstructing an interactive human avatar and the background from a
monocular video of a dynamic human scene is highly challenging. In this work we
adopt a strategy of point cloud decoupling and joint optimization to achieve
the decoupled reconstruction of backgrounds and human bodies while preserving
the interactivity of human motion. We introduce a position texture to subdivide
the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human
point cloud. To capture fine details of human dynamics and deformations, we
incorporate a convolutional neural network structure to predict human body
point cloud features based on texture. This strategy makes our approach free of
hyperparameter tuning for densification and efficiently represents human points
with half the point cloud of HUGS. This approach ensures high-quality human
reconstruction and reduces GPU resource consumption during training. As a
result, our method surpasses the previous state-of-the-art HUGS in
reconstruction metrics while maintaining the ability to generalize to novel
poses and views. Furthermore, our technique achieves real-time rendering at
over 100 FPS, $\sim$6$\times$ the HUGS speed using only Linear Blend Skinning
(LBS) weights for human transformation. Additionally, this work demonstrates
that this framework can be extended to animal scene reconstruction when an
accurately-posed model of an animal is available.



---

## 3DGH: 3D Head Generation with Composable Hair and Face

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-25 | Chengan He, Junxuan Li, Tobias Kirschstein, Artem Sevastopolsky, Shunsuke Saito, Qingyang Tan, Javier Romero, Chen Cao, Holly Rushmeier, Giljoo Nam | cs.GR | [PDF](http://arxiv.org/pdf/2506.20875v1){: .btn .btn-green } |

**Abstract**: We present 3DGH, an unconditional generative model for 3D human heads with
composable hair and face components. Unlike previous work that entangles the
modeling of hair and face, we propose to separate them using a novel data
representation with template-based 3D Gaussian Splatting, in which deformable
hair geometry is introduced to capture the geometric variations across
different hairstyles. Based on this data representation, we design a 3D
GAN-based architecture with dual generators and employ a cross-attention
mechanism to model the inherent correlation between hair and face. The model is
trained on synthetic renderings using carefully designed objectives to
stabilize training and facilitate hair-face separation. We conduct extensive
experiments to validate the design choice of 3DGH, and evaluate it both
qualitatively and quantitatively by comparing with several state-of-the-art 3D
GAN methods, demonstrating its effectiveness in unconditional full-head image
synthesis and composable 3D hairstyle editing. More details will be available
on our project page: https://c-he.github.io/projects/3dgh/.

Comments:
- Accepted to SIGGRAPH 2025. Project page:
  https://c-he.github.io/projects/3dgh/

---

## Joint attitude estimation and 3D neural reconstruction of  non-cooperative space objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-25 | Clément Forray, Pauline Delporte, Nicolas Delaygue, Florence Genin, Dawa Derksen | cs.CV | [PDF](http://arxiv.org/pdf/2506.20638v1){: .btn .btn-green } |

**Abstract**: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

Comments:
- accepted for CVPR 2025 NFBCC workshop

---

## RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and  Rasterizer

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-25 | Da Li, Donggang Jia, Yousef Rajeh, Dominik Engel, Ivan Viola | cs.GR | [PDF](http://arxiv.org/pdf/2506.20202v1){: .btn .btn-green } |

**Abstract**: With the advancement of Gaussian Splatting techniques, a growing number of
datasets based on this representation have been developed. However, performing
accurate and efficient clipping for Gaussian Splatting remains a challenging
and unresolved problem, primarily due to the volumetric nature of Gaussian
primitives, which makes hard clipping incapable of precisely localizing their
pixel-level contributions. In this paper, we propose a hybrid rendering
framework that combines rasterization and ray tracing to achieve efficient and
high-fidelity clipping of Gaussian Splatting data. At the core of our method is
the RaRa strategy, which first leverages rasterization to quickly identify
Gaussians intersected by the clipping plane, followed by ray tracing to compute
attenuation weights based on their partial occlusion. These weights are then
used to accurately estimate each Gaussian's contribution to the final image,
enabling smooth and continuous clipping effects. We validate our approach on
diverse datasets, including general Gaussians, hair strand Gaussians, and
multi-layer Gaussians, and conduct user studies to evaluate both perceptual
quality and quantitative performance. Experimental results demonstrate that our
method delivers visually superior results while maintaining real-time rendering
performance and preserving high fidelity in the unclipped regions.



---

## SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-25 | Aobo Li, Zhengxin Lei, Jiangtao Wei, Feng Xu | cs.GR | [PDF](http://arxiv.org/pdf/2506.21633v1){: .btn .btn-green } |

**Abstract**: Three-dimensional target reconstruction from synthetic aperture radar (SAR)
imagery is crucial for interpreting complex scattering information in SAR data.
However, the intricate electromagnetic scattering mechanisms inherent to SAR
imaging pose significant reconstruction challenges. Inspired by the remarkable
success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this
paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR)
specifically designed for SAR target reconstruction. Our approach combines
Gaussian splatting with the Mapping and Projection Algorithm to compute
scattering intensities of Gaussian primitives and generate simulated SAR images
through SDGR. Subsequently, the loss function between the rendered image and
the ground truth image is computed to optimize the Gaussian primitive
parameters representing the scene, while a custom CUDA gradient flow is
employed to replace automatic differentiation for accelerated gradient
computation. Through experiments involving the rendering of simplified
architectural targets and SAR images of multiple vehicle targets, we validate
the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the
effectiveness of our method for target reconstruction is demonstrated on both
simulated and real-world datasets containing multiple vehicle targets, with
quantitative evaluations conducted to assess its reconstruction performance.
Experimental results indicate that our approach can effectively reconstruct the
geometric structures and scattering properties of targets, thereby providing a
novel solution for 3D reconstruction in the field of SAR imaging.



---

## ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded  Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-24 | Chenhao Zhang, Yezhi Shen, Fengqing Zhu | cs.GR | [PDF](http://arxiv.org/pdf/2506.21629v1){: .btn .btn-green } |

**Abstract**: In recent years, neural rendering methods such as NeRFs and 3D Gaussian
Splatting (3DGS) have made significant progress in scene reconstruction and
novel view synthesis. However, they heavily rely on preprocessed camera poses
and 3D structural priors from structure-from-motion (SfM), which are
challenging to obtain in outdoor scenarios. To address this challenge, we
propose to incorporate Iterative Closest Point (ICP) with optimization-based
refinement to achieve accurate camera pose estimation under large camera
movements. Additionally, we introduce a voxel-based scene densification
approach to guide the reconstruction in large-scale scenes. Experiments
demonstrate that our approach ICP-3DGS outperforms existing methods in both
camera pose estimation and novel view synthesis across indoor and outdoor
scenes of various scales. Source code is available at
https://github.com/Chenhao-Z/ICP-3DGS.

Comments:
- 6 pages, Source code is available at
  https://github.com/Chenhao-Z/ICP-3DGS. To appear at ICIP 2025

---

## Virtual Memory for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-24 | Jonathan Haberl, Philipp Fleck, Clemens Arth | cs.GR | [PDF](http://arxiv.org/pdf/2506.19415v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting represents a breakthrough in the field of novel view
synthesis. It establishes Gaussians as core rendering primitives for highly
accurate real-world environment reconstruction. Recent advances have
drastically increased the size of scenes that can be created. In this work, we
present a method for rendering large and complex 3D Gaussian Splatting scenes
using virtual memory. By leveraging well-established virtual memory and virtual
texturing techniques, our approach efficiently identifies visible Gaussians and
dynamically streams them to the GPU just in time for real-time rendering.
Selecting only the necessary Gaussians for both storage and rendering results
in reduced memory usage and effectively accelerates rendering, especially for
highly complex scenes. Furthermore, we demonstrate how level of detail can be
integrated into our proposed method to further enhance rendering speed for
large-scale scenes. With an optimized implementation, we highlight key
practical considerations and thoroughly evaluate the proposed technique and its
impact on desktop and mobile devices.

Comments:
- Based on the Master Thesis from Jonathan Haberl from 2024, Submitted
  to TVCG in Feb. 2025;

---

## NeRF-based CBCT Reconstruction needs Normalization and Initialization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-24 | Zhuowei Xu, Han Li, Dai Sun, Zhicheng Li, Yujia Li, Qingpeng Kong, Zhiwei Cheng, Nassir Navab, S. Kevin Zhou | eess.IV | [PDF](http://arxiv.org/pdf/2506.19742v1){: .btn .btn-green } |

**Abstract**: Cone Beam Computed Tomography (CBCT) is widely used in medical imaging.
However, the limited number and intensity of X-ray projections make
reconstruction an ill-posed problem with severe artifacts. NeRF-based methods
have achieved great success in this task. However, they suffer from a
local-global training mismatch between their two key components: the hash
encoder and the neural network. Specifically, in each training step, only a
subset of the hash encoder's parameters is used (local sparse), whereas all
parameters in the neural network participate (global dense). Consequently, hash
features generated in each step are highly misaligned, as they come from
different subsets of the hash encoder. These misalignments from different
training steps are then fed into the neural network, causing repeated
inconsistent global updates in training, which leads to unstable training,
slower convergence, and degraded reconstruction quality. Aiming to alleviate
the impact of this local-global optimization mismatch, we introduce a
Normalized Hash Encoder, which enhances feature consistency and mitigates the
mismatch. Additionally, we propose a Mapping Consistency Initialization(MCI)
strategy that initializes the neural network before training by leveraging the
global mapping property from a well-trained model. The initialized neural
network exhibits improved stability during early training, enabling faster
convergence and enhanced reconstruction performance. Our method is simple yet
effective, requiring only a few lines of code while substantially improving
training efficiency on 128 CT cases collected from 4 different datasets,
covering 7 distinct anatomical regions.



---

## HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-24 | Xiaoyuan Wang, Yizhou Zhao, Botao Ye, Xiaojun Shan, Weijie Lyu, Lu Qi, Kelvin C. K. Chan, Yinxiao Li, Ming-Hsuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2506.19291v1){: .btn .btn-green } |

**Abstract**: We propose HoliGS, a novel deformable Gaussian splatting framework that
addresses embodied view synthesis from long monocular RGB videos. Unlike prior
4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training
overhead in minute-long captures, our method leverages invertible Gaussian
Splatting deformation networks to reconstruct large-scale, dynamic environments
accurately. Specifically, we decompose each scene into a static background plus
time-varying objects, each represented by learned Gaussian primitives
undergoing global rigid transformations, skeleton-driven articulation, and
subtle non-rigid deformations via an invertible neural flow. This hierarchical
warping strategy enables robust free-viewpoint novel-view rendering from
various embodied camera trajectories by attaching Gaussians to a complete
canonical foreground shape (\eg, egocentric or third-person follow), which may
involve substantial viewpoint changes and interactions between multiple actors.
Our experiments demonstrate that \ourmethod~ achieves superior reconstruction
quality on challenging datasets while significantly reducing both training and
rendering time compared to state-of-the-art monocular deformable NeRFs. These
results highlight a practical and scalable solution for EVS in real-world
scenarios. The source code will be released.



---

## Self-Supervised Multimodal NeRF for Autonomous Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-24 | Gaurav Sharma, Ravi Kothari, Josef Schmid | cs.CV | [PDF](http://arxiv.org/pdf/2506.19615v2){: .btn .btn-green } |

**Abstract**: In this paper, we propose a Neural Radiance Fields (NeRF) based framework,
referred to as Novel View Synthesis Framework (NVSF). It jointly learns the
implicit neural representation of space and time-varying scene for both LiDAR
and Camera. We test this on a real-world autonomous driving scenario containing
both static and dynamic scenes. Compared to existing multimodal dynamic NeRFs,
our framework is self-supervised, thus eliminating the need for 3D labels. For
efficient training and faster convergence, we introduce heuristic-based image
pixel sampling to focus on pixels with rich information. To preserve the local
features of LiDAR points, a Double Gradient based mask is employed. Extensive
experiments on the KITTI-360 dataset show that, compared to the baseline
models, our framework has reported best performance on both LiDAR and Camera
domain. Code of the model is available at
https://github.com/gaurav00700/Selfsupervised-NVSF



---

## ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical  Gaussian World Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-24 | Tengbo Yu, Guanxing Lu, Zaijia Yang, Haoyuan Deng, Season Si Chen, Jiwen Lu, Wenbo Ding, Guoqiang Hu, Yansong Tang, Ziwei Wang | cs.RO | [PDF](http://arxiv.org/pdf/2506.19842v1){: .btn .btn-green } |

**Abstract**: Multi-task robotic bimanual manipulation is becoming increasingly popular as
it enables sophisticated tasks that require diverse dual-arm collaboration
patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to
understanding the multi-body spatiotemporal dynamics. An existing method
ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual
representation via Gaussian world model for single-arm settings, which ignores
the interaction of multiple embodiments for dual-arm systems with significant
performance drop. In this paper, we propose ManiGaussian++, an extension of
ManiGaussian framework that improves multi-task bimanual manipulation by
digesting multi-body scene dynamics through a hierarchical Gaussian world
model. To be specific, we first generate task-oriented Gaussian Splatting from
intermediate visual features, which aims to differentiate acting and
stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build
a hierarchical Gaussian world model with the leader-follower architecture,
where the multi-body spatiotemporal dynamics is mined for intermediate visual
representation via future scene prediction. The leader predicts Gaussian
Splatting deformation caused by motions of the stabilizing arm, through which
the follower generates the physical consequences resulted from the movement of
the acting arm. As a result, our method significantly outperforms the current
state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in
10 simulated tasks, and achieves 60% success rate on average in 9 challenging
real-world tasks. Our code is available at
https://github.com/April-Yz/ManiGaussian_Bimanual.



---

## ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-23 | Michal Nazarczuk, Sibi Catley-Chandar, Thomas Tanay, Zhensong Zhang, Gregory Slabaugh, Eduardo Pérez-Pellitero | cs.CV | [PDF](http://arxiv.org/pdf/2506.18792v1){: .btn .btn-green } |

**Abstract**: Dynamic Novel View Synthesis aims to generate photorealistic views of moving
subjects from arbitrary viewpoints. This task is particularly challenging when
relying on monocular video, where disentangling structure from motion is
ill-posed and supervision is scarce. We introduce Video Diffusion-Aware
Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages
personalised diffusion models to synthesise a pseudo multi-view supervision
signal for training a Gaussian splatting representation. By conditioning on
scene-specific features, ViDAR recovers fine-grained appearance details while
mitigating artefacts introduced by monocular ambiguity. To address the
spatio-temporal inconsistency of diffusion-based supervision, we propose a
diffusion-aware loss function and a camera pose optimisation strategy that
aligns synthetic views with the underlying scene geometry. Experiments on
DyCheck, a challenging benchmark with extreme viewpoint variation, show that
ViDAR outperforms all state-of-the-art baselines in visual quality and
geometric consistency. We further highlight ViDAR's strong improvement over
baselines on dynamic regions and provide a new benchmark to compare performance
in reconstructing motion-rich parts of the scene. Project page:
https://vidar-4d.github.io



---

## 3D Arena: An Open Platform for Generative 3D Evaluation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-23 | Dylan Ebert | cs.CV | [PDF](http://arxiv.org/pdf/2506.18787v1){: .btn .btn-green } |

**Abstract**: Evaluating Generative 3D models remains challenging due to misalignment
between automated metrics and human perception of quality. Current benchmarks
rely on image-based metrics that ignore 3D structure or geometric measures that
fail to capture perceptual appeal and real-world utility. To address this gap,
we present 3D Arena, an open platform for evaluating image-to-3D generation
models through large-scale human preference collection using pairwise
comparisons.
  Since launching in June 2024, the platform has collected 123,243 votes from
8,096 users across 19 state-of-the-art models, establishing the largest human
preference evaluation for Generative 3D. We contribute the iso3d dataset of 100
evaluation prompts and demonstrate quality control achieving 99.75% user
authenticity through statistical fraud detection. Our ELO-based ranking system
provides reliable model assessment, with the platform becoming an established
evaluation resource.
  Through analysis of this preference data, we present insights into human
preference patterns. Our findings reveal preferences for visual presentation
features, with Gaussian splat outputs achieving a 16.6 ELO advantage over
meshes and textured models receiving a 144.1 ELO advantage over untextured
models. We provide recommendations for improving evaluation methods, including
multi-criteria assessment, task-oriented evaluation, and format-aware
comparison. The platform's community engagement establishes 3D Arena as a
benchmark for the field while advancing understanding of human-centered
evaluation in Generative 3D.

Comments:
- 9 pages, 2 figures

---

## MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit  Neural Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-23 | Tianchen Deng, Guole Shen, Xun Chen, Shenghai Yuan, Hongming Shen, Guohao Peng, Zhenyu Wu, Jingchuan Wang, Lihua Xie, Danwei Wang, Hesheng Wang, Weidong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2506.18678v1){: .btn .btn-green } |

**Abstract**: Neural implicit scene representations have recently shown promising results
in dense visual SLAM. However, existing implicit SLAM algorithms are
constrained to single-agent scenarios, and fall difficulties in large-scale
scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks
cannot meet the constraints of communication bandwidth. To this end, we propose
the first distributed multi-agent collaborative neural SLAM framework with
hybrid scene representation, distributed camera tracking, intra-to-inter loop
closure, and online distillation for multiple submap fusion. A novel
triplane-grid joint scene representation method is proposed to improve scene
reconstruction. A novel intra-to-inter loop closure method is designed to
achieve local (single-agent) and global (multi-agent) consistency. We also
design a novel online distillation method to fuse the information of different
submaps to achieve global consistency. Furthermore, to the best of our
knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that
provides both continuous-time trajectories groundtruth and high-accuracy 3D
meshes groundtruth. To this end, we propose the first real-world Dense slam
(DES) dataset covering both single-agent and multi-agent scenarios, ranging
from small rooms to large-scale outdoor scenes, with high-accuracy ground truth
for both 3D mesh and continuous-time camera trajectory. This dataset can
advance the development of the research in both SLAM, 3D reconstruction, and
visual foundation model. Experiments on various datasets demonstrate the
superiority of the proposed method in both mapping, tracking, and
communication. The dataset and code will open-source on
https://github.com/dtc111111/mcnslam.



---

## Reconstructing Tornadoes in 3D with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-23 | Adam Yang, Nadula Kadawedduwa, Tianfu Wang, Maria Molina, Christopher Metzler | cs.CV | [PDF](http://arxiv.org/pdf/2506.18677v1){: .btn .btn-green } |

**Abstract**: Accurately reconstructing the 3D structure of tornadoes is critically
important for understanding and preparing for this highly destructive weather
phenomenon. While modern 3D scene reconstruction techniques, such as 3D
Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the
3D structure of tornados, at present we are critically lacking a controlled
tornado dataset with which to develop and validate these tools. In this work we
capture and release a novel multiview dataset of a small lab-based tornado. We
demonstrate one can effectively reconstruct and visualize the 3D structure of
this tornado using 3DGS.



---

## 2D Triangle Splatting for Direct Differentiable Mesh Training

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-23 | Kaifeng Sheng, Zheng Zhou, Yingliang Peng, Qianwei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2506.18575v2){: .btn .btn-green } |

**Abstract**: Differentiable rendering with 3D Gaussian primitives has emerged as a
powerful method for reconstructing high-fidelity 3D scenes from multi-view
images. While it offers improvements over NeRF-based methods, this
representation still encounters challenges with rendering speed and advanced
rendering effects, such as relighting and shadow rendering, compared to
mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a
novel method that replaces 3D Gaussian primitives with 2D triangle facelets.
This representation naturally forms a discrete mesh-like structure while
retaining the benefits of continuous volumetric modeling. By incorporating a
compactness parameter into the triangle primitives, we enable direct training
of photorealistic meshes. Our experimental results demonstrate that our
triangle-based method, in its vanilla version (without compactness tuning),
achieves higher fidelity compared to state-of-the-art Gaussian-based methods.
Furthermore, our approach produces reconstructed meshes with superior visual
quality compared to existing mesh reconstruction methods. Please visit our
project page at https://gaoderender.github.io/triangle-splatting.

Comments:
- 13 pages, 8 figures

---

## GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale  Multi-Agent Gaussian SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-23 | Annika Thomas, Aneesa Sonawalla, Alex Rose, Jonathan P. How | cs.RO | [PDF](http://arxiv.org/pdf/2506.18885v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting has emerged as an expressive scene representation for
RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor
environments remains unexplored. Multi-agent Gaussian SLAM is a promising
approach to rapid exploration and reconstruction of environments, offering
scalable environment representations, but existing approaches are limited to
small-scale, indoor environments. To that end, we propose Gaussian
Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative
Gaussian splatting SLAM method that integrates i) an implicit tracking module
based on local optimization over submaps and ii) an approach to inter- and
intra-robot loop closure integrated into a pose-graph optimization framework.
Experiments show that GRAND-SLAM provides state-of-the-art tracking performance
and 28% higher PSNR than existing methods on the Replica indoor dataset, as
well as 91% lower multi-agent tracking error and improved rendering over
existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.



---

## Limitations of NERF with pre-trained Vision Features for Few-Shot 3D  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-22 | Ankit Sanjyal | cs.CV | [PDF](http://arxiv.org/pdf/2506.18208v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction
from sparse image collections. Recent work has explored integrating pre-trained
vision features, particularly from DINO, to enhance few-shot reconstruction
capabilities. However, the effectiveness of such approaches remains unclear,
especially in extreme few-shot scenarios. In this paper, we present a
systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF,
frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.
Surprisingly, our experiments reveal that all DINO variants perform worse than
the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the
baseline's 14.71. This counterintuitive result suggests that pre-trained vision
features may not be beneficial for few-shot 3D reconstruction and may even
introduce harmful biases. We analyze potential causes including feature-task
mismatch, overfitting to limited data, and integration challenges. Our findings
challenge common assumptions in the field and suggest that simpler
architectures focusing on geometric consistency may be more effective for
few-shot scenarios.

Comments:
- 5 pages, 1 table, 2 figures. First submission. Code available at:
  \url{https://github.com/ANKITSANJYAL/nerf-few-shot-limitations}

---

## 3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in  Large-Scale Scene

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-21 | Shihan Chen, Zhaojin Li, Zeyu Chen, Qingsong Yan, Gaoyang Shen, Ran Duan | cs.GR | [PDF](http://arxiv.org/pdf/2506.17636v1){: .btn .btn-green } |

**Abstract**: Recent developments in 3D Gaussian Splatting have made significant advances
in surface reconstruction. However, scaling these methods to large-scale scenes
remains challenging due to high computational demands and the complex dynamic
appearances typical of outdoor environments. These challenges hinder the
application in aerial surveying and autonomous driving. This paper proposes a
novel solution to reconstruct large-scale surfaces with fine details,
supervised by full-sized images. Firstly, we introduce a coarse-to-fine
strategy to reconstruct a coarse model efficiently, followed by adaptive scene
partitioning and sub-scene refining from image segments. Additionally, we
integrate a decoupling appearance model to capture global appearance variations
and a transient mask model to mitigate interference from moving objects.
Finally, we expand the multi-view constraint and introduce a single-view
regularization for texture-less areas. Our experiments were conducted on the
publicly available dataset GauU-Scene V2, which was captured using unmanned
aerial vehicles. To the best of our knowledge, our method outperforms existing
NeRF-based and Gaussian-based methods, achieving high-fidelity visual results
and accurate surface from full-size image optimization. Open-source code will
be available on GitHub.

Comments:
- IROS 2025

---

## Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-20 | Tianjiao Yu, Vedant Shah, Muntasir Wahed, Ying Shen, Kiet A. Nguyen, Ismini Lourentzou | cs.CV | [PDF](http://arxiv.org/pdf/2506.17212v1){: .btn .btn-green } |

**Abstract**: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.



---

## Information-computation trade-offs in non-linear transforms

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-19 | Connor Ding, Abhiram Rao Gorle, Jiwon Jeong, Naomi Sagan, Tsachy Weissman | cs.IT | [PDF](http://arxiv.org/pdf/2506.15948v1){: .btn .btn-green } |

**Abstract**: In this work, we explore the interplay between information and computation in
non-linear transform-based compression for broad classes of modern
information-processing tasks. We first investigate two emerging nonlinear data
transformation frameworks for image compression: Implicit Neural
Representations (INRs) and 2D Gaussian Splatting (GS). We analyze their
representational properties, behavior under lossy compression, and convergence
dynamics. Our results highlight key trade-offs between INR's compact,
resolution-flexible neural field representations and GS's highly
parallelizable, spatially interpretable fitting, providing insights for future
hybrid and compression-aware frameworks. Next, we introduce the textual
transform that enables efficient compression at ultra-low bitrate regimes and
simultaneously enhances human perceptual satisfaction. When combined with the
concept of denoising via lossy compression, the textual transform becomes a
powerful tool for denoising tasks. Finally, we present a Lempel-Ziv (LZ78)
"transform", a universal method that, when applied to any member of a broad
compressor family, produces new compressors that retain the asymptotic
universality guarantees of the LZ78 algorithm. Collectively, these three
transforms illuminate the fundamental trade-offs between coding efficiency and
computational cost. We discuss how these insights extend beyond compression to
tasks such as classification, denoising, and generative AI, suggesting new
pathways for using non-linear transformations to balance resource constraints
and performance.

Comments:
- Authors listed in alphabetical order of last name

---

## R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement  for 3D Low-Level Vision

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-19 | Weeyoung Kwon, Jeahun Sung, Minkyu Jeon, Chanho Eom, Jihyong Oh | cs.CV | [PDF](http://arxiv.org/pdf/2506.16262v2){: .btn .btn-green } |

**Abstract**: Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have achieved significant progress in photorealistic
3D scene reconstruction and novel view synthesis. However, most existing models
assume clean and high-resolution (HR) multi-view inputs, which limits their
robustness under real-world degradations such as noise, blur, low-resolution
(LR), and weather-induced artifacts. To address these limitations, the emerging
field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision
tasks including super-resolution (SR), deblurring, weather degradation removal,
restoration, and enhancement into the 3D spatial domain. This survey, referred
to as R\textsuperscript{3}eVision, provides a comprehensive overview of robust
rendering, restoration, and enhancement for 3D LLV by formalizing the
degradation-aware rendering problem and identifying key challenges related to
spatio-temporal consistency and ill-posed optimization. Recent methods that
integrate LLV into neural rendering frameworks are categorized to illustrate
how they enable high-fidelity 3D reconstruction under adverse conditions.
Application domains such as autonomous driving, AR/VR, and robotics are also
discussed, where reliable 3D perception from degraded inputs is critical. By
reviewing representative methods, datasets, and evaluation protocols, this work
positions 3D LLV as a fundamental direction for robust 3D content generation
and scene-level reconstruction in real-world environments.

Comments:
- Please visit our project page at
  https://github.com/CMLab-Korea/Awesome-3D-Low-Level-Vision

---

## RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate  Camera Pose Estimation under Complex Trajectories

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-18 | Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, Fei Deng | cs.CV | [PDF](http://arxiv.org/pdf/2506.15242v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged
as powerful tools for 3D reconstruction and SLAM tasks. However, their
performance depends heavily on accurate camera pose priors. Existing approaches
attempt to address this issue by introducing external constraints but fall
short of achieving satisfactory accuracy, particularly when camera trajectories
are complex. In this paper, we propose a novel method, RA-NeRF, capable of
predicting highly accurate camera poses even with complex camera trajectories.
Following the incremental pipeline, RA-NeRF reconstructs the scene using NeRF
with photometric consistency and incorporates flow-driven pose regulation to
enhance robustness during initialization and localization. Additionally,
RA-NeRF employs an implicit pose filter to capture the camera movement pattern
and eliminate the noise for pose estimation. To validate our method, we conduct
extensive experiments on the Tanks\&Temple dataset for standard evaluation, as
well as the NeRFBuster dataset, which presents challenging camera pose
trajectories. On both datasets, RA-NeRF achieves state-of-the-art results in
both camera pose estimation and visual quality, demonstrating its effectiveness
and robustness in scene reconstruction under complex pose trajectories.

Comments:
- IROS 2025

---

## Particle-Grid Neural Dynamics for Learning Deformable Object Models from  RGB-D Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-18 | Kaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu Li | cs.RO | [PDF](http://arxiv.org/pdf/2506.15680v1){: .btn .btn-green } |

**Abstract**: Modeling the dynamics of deformable objects is challenging due to their
diverse physical properties and the difficulty of estimating states from
limited visual information. We address these challenges with a neural dynamics
framework that combines object particles and spatial grids in a hybrid
representation. Our particle-grid model captures global shape and motion
information while predicting dense particle movements, enabling the modeling of
objects with varied shapes and materials. Particles represent object shapes,
while the spatial grid discretizes the 3D space to ensure spatial continuity
and enhance learning efficiency. Coupled with Gaussian Splattings for visual
rendering, our framework achieves a fully learning-based digital twin of
deformable objects and generates 3D action-conditioned videos. Through
experiments, we demonstrate that our model learns the dynamics of diverse
objects -- such as ropes, cloths, stuffed animals, and paper bags -- from
sparse-view RGB-D recordings of robot-object interactions, while also
generalizing at the category level to unseen instances. Our approach
outperforms state-of-the-art learning-based and physics-based simulators,
particularly in scenarios with limited camera views. Furthermore, we showcase
the utility of our learned models in model-based planning, enabling
goal-conditioned object manipulation across a range of tasks. The project page
is available at https://kywind.github.io/pgnd .

Comments:
- Project page: https://kywind.github.io/pgnd

---

## SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads  Synthesis Using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-17 | Ziqiao Peng, Wentao Hu, Junyuan Ma, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Hui Tian, Jun He, Hongyan Liu, Zhaoxin Fan | cs.CV | [PDF](http://arxiv.org/pdf/2506.14742v1){: .btn .btn-green } |

**Abstract**: Achieving high synchronization in the synthesis of realistic, speech-driven
talking head videos presents a significant challenge. A lifelike talking head
requires synchronized coordination of subject identity, lip movements, facial
expressions, and head poses. The absence of these synchronizations is a
fundamental flaw, leading to unrealistic results. To address the critical issue
of synchronization, identified as the ''devil'' in creating realistic talking
heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with
Gaussian Splatting to ensure consistent subject identity preservation and a
Face-Sync Controller that aligns lip movements with speech while innovatively
using a 3D facial blendshape model to reconstruct accurate facial expressions.
To ensure natural head movements, we propose a Head-Sync Stabilizer, which
optimizes head poses for greater stability. Additionally, SyncTalk++ enhances
robustness to out-of-distribution (OOD) audio by incorporating an Expression
Generator and a Torso Restorer, which generate speech-matched facial
expressions and seamless torso regions. Our approach maintains consistency and
continuity in visual details across frames and significantly improves rendering
speed and quality, achieving up to 101 frames per second. Extensive experiments
and user studies demonstrate that SyncTalk++ outperforms state-of-the-art
methods in synchronization and realism. We recommend watching the supplementary
video: https://ziqiaopeng.github.io/synctalk++.



---

## 3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D  Gaussian-Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-17 | Yuke Xing, Jiarui Wang, Peizhi Niu, Wenjie Huang, Guangtao Zhai, Yiling Xu | cs.CV | [PDF](http://arxiv.org/pdf/2506.14642v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel
view synthesis, offering real-time rendering with high visual fidelity.
However, its substantial storage requirements present significant challenges
for practical applications. While recent state-of-the-art (SOTA) 3DGS methods
increasingly incorporate dedicated compression modules, there is a lack of a
comprehensive framework to evaluate their perceptual impact. Therefore we
present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA)
dataset specifically designed for compressed 3DGS representations. Our dataset
encompasses 15,200 images rendered from 10 real-world scenes through 6
representative 3DGS algorithms at 20 strategically selected viewpoints, with
different compression levels leading to various distortion effects. Through
controlled subjective experiments, we collect human perception data from 60
viewers. We validate dataset quality through scene diversity and MOS
distribution analysis, and establish a comprehensive benchmark with 30
representative IQA metrics covering diverse types. As the largest-scale 3DGS
quality assessment dataset to date, our work provides a foundation for
developing 3DGS specialized IQA metrics, and offers essential data for
investigating view-dependent quality distribution patterns unique to 3DGS. The
database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.



---

## Peering into the Unknown: Active View Selection with Neural Uncertainty  Maps for 3D Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-17 | Zhengquan Zhang, Feng Xu, Mengmi Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2506.14856v1){: .btn .btn-green } |

**Abstract**: Some perspectives naturally provide more information than others. How can an
AI system determine which viewpoint offers the most valuable insight for
accurate and efficient 3D object reconstruction? Active view selection (AVS)
for 3D reconstruction remains a fundamental challenge in computer vision. The
aim is to identify the minimal set of views that yields the most accurate 3D
reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian
Splatting, from a current observation and computing uncertainty for each
candidate viewpoint, we introduce a novel AVS approach guided by neural
uncertainty maps predicted by a lightweight feedforward deep neural network,
named UPNet. UPNet takes a single input image of a 3D object and outputs a
predicted uncertainty map, representing uncertainty values across all possible
candidate viewpoints. By leveraging heuristics derived from observing many
natural objects and their associated uncertainty patterns, we train UPNet to
learn a direct mapping from viewpoint appearance to uncertainty in the
underlying volumetric representations. Next, our approach aggregates all
previously predicted neural uncertainty maps to suppress redundant candidate
viewpoints and effectively select the most informative one. Using these
selected viewpoints, we train 3D neural rendering models and evaluate the
quality of novel view synthesis against other competitive AVS methods.
Remarkably, despite using half of the viewpoints than the upper bound, our
method achieves comparable reconstruction accuracy. In addition, it
significantly reduces computational overhead during AVS, achieving up to a 400
times speedup along with over 50\% reductions in CPU, RAM, and GPU usage
compared to baseline methods. Notably, our approach generalizes effectively to
AVS tasks involving novel object categories, without requiring any additional
training.

Comments:
- 9 pages, 3 figures in the main text. Under review for NeurIPS 2025

---

## GAF: Gaussian Action Field as a Dynamic World Model for Robotic  Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-17 | Ying Chai, Litao Deng, Ruizhi Shao, Jiajun Zhang, Liangjun Xing, Hongwen Zhang, Yebin Liu | cs.RO | [PDF](http://arxiv.org/pdf/2506.14135v2){: .btn .btn-green } |

**Abstract**: Accurate action inference is critical for vision-based robotic manipulation.
Existing approaches typically follow either a Vision-to-Action (V-A) paradigm,
predicting actions directly from visual inputs, or a Vision-to-3D-to-Action
(V-3D-A) paradigm, leveraging intermediate 3D representations. However, these
methods often struggle with action inaccuracies due to the complexity and
dynamic nature of manipulation scenes. In this paper, we propose a
Vision-to-4D-to-Action (V-4D-A) framework that enables direct action reasoning
from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF
extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion
attributes, allowing simultaneous modeling of dynamic scenes and manipulation
actions. To learn time-varying scene geometry and action-aware robot motion,
GAF supports three key query types: reconstruction of the current scene,
prediction of future frames, and estimation of initial action via robot motion.
Furthermore, the high-quality current and future frames generated by GAF
facilitate manipulation action refinement through a GAF-guided diffusion model.
Extensive experiments demonstrate significant improvements, with GAF achieving
+11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality,
while boosting the average success rate in robotic manipulation tasks by 10.33%
over state-of-the-art methods. Project page:
http://chaiying1.github.io/GAF.github.io/project_page/

Comments:
- http://chaiying1.github.io/GAF.github.io/project_page/

---

## HRGS: Hierarchical Gaussian Splatting for Memory-Efficient  High-Resolution 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-17 | Changbai Li, Haodong Zhu, Hanlin Chen, Juan Zhang, Tongfei Chen, Shuo Yang, Shuwei Shao, Wenhao Dong, Baochang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2506.14229v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D
scene reconstruction, but faces memory scalability issues in high-resolution
scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS),
a memory-efficient framework with hierarchical block-level optimization. First,
we generate a global, coarse Gaussian representation from low-resolution data.
Then, we partition the scene into multiple blocks, refining each block with
high-resolution data. The partitioning involves two steps: Gaussian
partitioning, where irregular scenes are normalized into a bounded cubic space
with a uniform grid for task distribution, and training data partitioning,
where only relevant observations are retained for each block. By guiding block
refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion
across adjacent blocks. To reduce computational demands, we introduce
Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for
each Gaussian and removes those with minimal contribution, speeding up
convergence and reducing memory usage. Additionally, we incorporate normal
priors from a pretrained model to enhance surface reconstruction quality. Our
method enables high-quality, high-resolution 3D scene reconstruction even under
memory constraints. Extensive experiments on three benchmarks show that HRGS
achieves state-of-the-art performance in high-resolution novel view synthesis
(NVS) and surface reconstruction tasks.



---

## GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with  Gaussian Radiance Fields and Differentiable Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-16 | Qianzhong Chen, Naixiang Gao, Suning Huang, JunEn Low, Timothy Chen, Jiankai Sun, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2506.14009v1){: .btn .btn-green } |

**Abstract**: Autonomous drones capable of interpreting and executing high-level language
instructions in unstructured environments remain a long-standing goal. Yet
existing approaches are constrained by their dependence on hand-crafted skills,
extensive parameter tuning, or computationally intensive models unsuitable for
onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action
(VLA) framework that runs fully onboard and follows natural-language commands
in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting
(3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling
efficient learning of low-level control from visual and linguistic inputs. At
its core is a Mixture-of-Experts (MoE) action head, which adaptively routes
computation to improve generalization while mitigating forgetting. In
multi-task generalization experiments, GRaD-Nav++ achieves a success rate of
83% on trained tasks and 75% on unseen tasks in simulation. When deployed on
real hardware, it attains 67% success on trained tasks and 50% on unseen ones.
In multi-environment adaptation experiments, GRaD-Nav++ achieves an average
success rate of 81% across diverse simulated environments and 67% across varied
real-world settings. These results establish a new benchmark for fully onboard
Vision-Language-Action (VLA) flight and demonstrate that compact, efficient
models can enable reliable, language-guided navigation without relying on
external infrastructure.



---

## Multiview Geometric Regularization of Gaussian Splatting for Accurate  Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-16 | Jungeon Kim, Geonsoo Park, Seungyong Lee | cs.CV | [PDF](http://arxiv.org/pdf/2506.13508v1){: .btn .btn-green } |

**Abstract**: Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields,
have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while
retaining its superior rendering quality. However, these approaches still
struggle to reconstruct smooth and reliable geometry, particularly in scenes
with significant color variation across viewpoints, due to their per-point
appearance modeling and single-view optimization constraints. In this paper, we
propose an effective multiview geometric regularization strategy that
integrates multiview stereo (MVS) depth, RGB, and normal constraints into
Gaussian Splatting initialization and optimization. Our key insight is the
complementary relationship between MVS-derived depth points and Gaussian
Splatting-optimized positions: MVS robustly estimates geometry in regions of
high color variation through local patch-based matching and epipolar
constraints, whereas Gaussian Splatting provides more reliable and less noisy
depth estimates near object boundaries and regions with lower color variation.
To leverage this insight, we introduce a median depth-based multiview relative
depth loss with uncertainty estimation, effectively integrating MVS depth
information into Gaussian Splatting optimization. We also propose an MVS-guided
Gaussian Splatting initialization to avoid Gaussians falling into suboptimal
positions. Extensive experiments validate that our approach successfully
combines these strengths, enhancing both geometric accuracy and rendering
quality across diverse indoor and outdoor scenes.

Comments:
- Accepted to Computer Graphics Forum (EGSR 2025)

---

## PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated  Human Images


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-16 | Lingteng Qiu, Peihao Li, Qi Zuo, Xiaodong Gu, Yuan Dong, Weihao Yuan, Siyu Zhu, Xiaoguang Han, Guanying Chen, Zilong Dong | cs.CV | [PDF](http://arxiv.org/pdf/2506.13766v1){: .btn .btn-green } |

**Abstract**: Reconstructing an animatable 3D human from casually captured images of an
articulated subject without camera or human pose information is a practical yet
challenging task due to view misalignment, occlusions, and the absence of
structural priors. While optimization-based methods can produce high-fidelity
results from monocular or multi-view videos, they require accurate pose
estimation and slow iterative optimization, limiting scalability in
unconstrained scenarios. Recent feed-forward approaches enable efficient
single-image reconstruction but struggle to effectively leverage multiple input
images to reduce ambiguity and improve reconstruction accuracy. To address
these challenges, we propose PF-LHM, a large human reconstruction model that
generates high-quality 3D avatars in seconds from one or multiple casually
captured pose-free images. Our approach introduces an efficient Encoder-Decoder
Point-Image Transformer architecture, which fuses hierarchical geometric point
features and multi-view image features through multimodal attention. The fused
features are decoded to recover detailed geometry and appearance, represented
using 3D Gaussian splats. Extensive experiments on both real and synthetic
datasets demonstrate that our method unifies single- and multi-image 3D human
reconstruction, achieving high-fidelity and animatable 3D human avatars without
requiring camera and human pose annotations. Code and models will be released
to the public.



---

## TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-16 | Mae Younes, Adnane Boukhayma | cs.GR | [PDF](http://arxiv.org/pdf/2506.13348v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting have demonstrated remarkable novel view synthesis
performance at high rendering frame rates. Optimization-based inverse rendering
within complex capture scenarios remains however a challenging problem. A
particular case is modelling complex surface light interactions for highly
reflective scenes, which results in intricate high frequency specular radiance
components. We hypothesize that such challenging settings can benefit from
increased representation power. We hence propose a method that tackles this
issue through a geometrically and physically grounded Gaussian Splatting borne
radiance field, where normals and material properties are spatially variable in
the primitive's local space. Using per-primitive texture maps for this purpose,
we also propose to harness the GPU hardware to accelerate rendering at test
time via unified material texture atlas.

Comments:
- Code will be available at https://github.com/maeyounes/TextureSplat

---

## Micro-macro Gaussian Splatting with Enhanced Scalability for  Unconstrained Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-16 | Yihui Li, Chengxin Lv, Hongyu Yang, Di Huang | cs.CV | [PDF](http://arxiv.org/pdf/2506.13516v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D scenes from unconstrained image collections poses
significant challenges due to variations in appearance. In this paper, we
propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel
method that enhances 3D reconstruction across diverse scales by decomposing
scene representations into global, refined, and intrinsic components. SMW-GS
incorporates the following innovations: Micro-macro Projection, which enables
Gaussian points to sample multi-scale details with improved diversity; and
Wavelet-based Sampling, which refines feature representations using
frequency-domain information to better capture complex scene appearances. To
achieve scalability, we further propose a large-scale scene promotion strategy,
which optimally assigns camera views to scene partitions by maximizing their
contributions to Gaussian points, achieving consistent and high-quality
reconstructions even in expansive environments. Extensive experiments
demonstrate that SMW-GS significantly outperforms existing methods in both
reconstruction quality and scalability, particularly excelling in large-scale
urban environments with challenging illumination variations. Project is
available at https://github.com/Kidleyh/SMW-GS.



---

## GS-2DGS: Geometrically Supervised 2DGS for Reflective Object  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-16 | Jinguang Tong, Xuesong li, Fahira Afzal Maken, Sundaram Muthu, Lars Petersson, Chuong Nguyen, Hongdong Li | cs.CV | [PDF](http://arxiv.org/pdf/2506.13110v1){: .btn .btn-green } |

**Abstract**: 3D modeling of highly reflective objects remains challenging due to strong
view-dependent appearances. While previous SDF-based methods can recover
high-quality meshes, they are often time-consuming and tend to produce
over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the
advantage of high speed and detailed real-time rendering, but extracting
surfaces from the Gaussians can be noisy due to the lack of geometric
constraints. To bridge the gap between these approaches, we propose a novel
reconstruction method called GS-2DGS for reflective objects based on 2D
Gaussian Splatting (2DGS). Our approach combines the rapid rendering
capabilities of Gaussian Splatting with additional geometric information from
foundation models. Experimental results on synthetic and real datasets
demonstrate that our method significantly outperforms Gaussian-based techniques
in terms of reconstruction and relighting and achieves performance comparable
to SDF-based methods while being an order of magnitude faster. Code is
available at https://github.com/hirotong/GS2DGS

Comments:
- Accepted by CVPR2025

---

## Metropolis-Hastings Sampling for 3D Gaussian Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-15 | Hyunjin Kim, Haebeom Jung, Jaesik Park | cs.CV | [PDF](http://arxiv.org/pdf/2506.12945v1){: .btn .btn-green } |

**Abstract**: We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS)
that leverages comprehensive multi-view photometric error signals within a
unified Metropolis-Hastings approach. Traditional 3DGS methods heavily rely on
heuristic-based density-control mechanisms (e.g., cloning, splitting, and
pruning), which can lead to redundant computations or the premature removal of
beneficial Gaussians. Our framework overcomes these limitations by
reformulating densification and pruning as a probabilistic sampling process,
dynamically inserting and relocating Gaussians based on aggregated multi-view
errors and opacity scores. Guided by Bayesian acceptance tests derived from
these error-based importance scores, our method substantially reduces reliance
on heuristics, offers greater flexibility, and adaptively infers Gaussian
distributions without requiring predefined scene complexity. Experiments on
benchmark datasets, including Mip-NeRF360, Tanks and Temples, and Deep
Blending, show that our approach reduces the number of Gaussians needed,
enhancing computational efficiency while matching or modestly surpassing the
view-synthesis quality of state-of-the-art models.

Comments:
- Project Page: https://hjhyunjinkim.github.io/MH-3DGS

---

## Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-15 | Wen-Hsuan Chu, Lei Ke, Jianmeng Liu, Mingxiao Huo, Pavel Tokmakov, Katerina Fragkiadaki | cs.CV | [PDF](http://arxiv.org/pdf/2506.12716v1){: .btn .btn-green } |

**Abstract**: We tackle the challenge of generating dynamic 4D scenes from monocular,
multi-object videos with heavy occlusions, and introduce GenMOJO, a novel
approach that integrates rendering-based deformable 3D Gaussian optimization
with generative priors for view synthesis. While existing models perform well
on novel view synthesis for isolated objects, they struggle to generalize to
complex, cluttered scenes. To address this, GenMOJO decomposes the scene into
individual objects, optimizing a differentiable set of deformable Gaussians per
object. This object-wise decomposition allows leveraging object-centric
diffusion models to infer unobserved regions in novel viewpoints. It performs
joint Gaussian splatting to render the full scene, capturing cross-object
occlusions, and enabling occlusion-aware supervision. To bridge the gap between
object-centric priors and the global frame-centric coordinate system of videos,
GenMOJO uses differentiable transformations that align generative and rendering
constraints within a unified framework. The resulting model generates 4D object
reconstructions over space and time, and produces accurate 2D and 3D point
tracks from monocular input. Quantitative evaluations and perceptual human
studies confirm that GenMOJO generates more realistic novel views of scenes and
produces more accurate point tracks compared to existing approaches.

Comments:
- This is an updated and extended version of our CVPR paper "Robust
  Multi-Object 4D Generation in Complex Video Scenarios"

---

## Efficient multi-view training for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-15 | Minhyuk Choi, Injae Kim, Hyunwoo J. Kim | cs.CV | [PDF](http://arxiv.org/pdf/2506.12727v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside
Neural Radiance Fields (NeRF) in inverse rendering due to its superior
rendering speed. Currently, the common approach in 3DGS is to utilize
"single-view" mini-batch training, where only one image is processed per
iteration, in contrast to NeRF's "multi-view" mini-batch training, which
leverages multiple images. We observe that such single-view training can lead
to suboptimal optimization due to increased variance in mini-batch stochastic
gradients, highlighting the necessity for multi-view training. However,
implementing multi-view training in 3DGS poses challenges. Simply rendering
multiple images per iteration incurs considerable overhead and may result in
suboptimal Gaussian densification due to its reliance on single-view
assumptions. To address these issues, we modify the rasterization process to
minimize the overhead associated with multi-view training and propose a 3D
distance-aware D-SSIM loss and multi-view adaptive density control that better
suits multi-view scenarios. Our experiments demonstrate that the proposed
methods significantly enhance the performance of 3DGS and its variants, freeing
3DGS from the constraints of single-view training.



---

## Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-15 | Mufan Liu, Cixiao Zhang, Qi Yang, Yujie Cao, Yiling Xu, Yin Xu, Shu Sun, Mingzeng Dai, Yunfeng Guan | cs.CV | [PDF](http://arxiv.org/pdf/2506.12787v2){: .btn .btn-green } |

**Abstract**: Modeling the wireless radiance field (WRF) is fundamental to modern
communication systems, enabling key tasks such as localization, sensing, and
channel estimation. Traditional approaches, which rely on empirical formulas or
physical simulations, often suffer from limited accuracy or require strong
scene priors. Recent neural radiance field (NeRF-based) methods improve
reconstruction fidelity through differentiable volumetric rendering, but their
reliance on computationally expensive multilayer perceptron (MLP) queries
hinders real-time deployment. To overcome these challenges, we introduce
Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in
modeling optical radiance fields to enable compact and accurate WRF
reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian
splatting framework that synthesizes WRF spectra at arbitrary positions under
single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated
rasterization to render spectra at over 100000 fps and uses a lightweight MLP
to model the deformation of 2D Gaussians, effectively capturing
mobility-induced WRF variations. In addition to novel spectrum synthesis, the
efficacy of SwiftWRF is further underscored in its applications in
angle-of-arrival (AoA) and received signal strength indicator (RSSI)
prediction. Experiments conducted on both real-world and synthetic indoor
scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster
than existing state-of-the-art methods, while significantly enhancing its
signal quality. The project page is https://evan-sudo.github.io/swiftwrf/.



---

## Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-14 | Hongbi Zhou, Zhangkai Ni | cs.CV | [PDF](http://arxiv.org/pdf/2506.12400v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel
view synthesis. However, existing methods struggle to adaptively optimize the
distribution of Gaussian primitives based on scene characteristics, making it
challenging to balance reconstruction quality and efficiency. Inspired by human
perception, we propose scene-adaptive perceptual densification for Gaussian
Splatting (Perceptual-GS), a novel framework that integrates perceptual
sensitivity into the 3DGS training process to address this challenge. We first
introduce a perception-aware representation that models human visual
sensitivity while constraining the number of Gaussian primitives. Building on
this foundation, we develop a perceptual sensitivity-adaptive distribution to
allocate finer Gaussian granularity to visually critical regions, enhancing
reconstruction quality and robustness. Extensive evaluations on multiple
datasets, including BungeeNeRF for large-scale scenes, demonstrate that
Perceptual-GS achieves state-of-the-art performance in reconstruction quality,
efficiency, and robustness. The code is publicly available at:
https://github.com/eezkni/Perceptual-GS

Comments:
- Accepted to International Conference on Machine Learning (ICML) 2025

---

## SPLATART: Articulated Gaussian Splatting with Estimated Object Structure

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-13 | Stanley Lewis, Vishal Chandra, Tom Gao, Odest Chadwicke Jenkins | cs.RO | [PDF](http://arxiv.org/pdf/2506.12184v1){: .btn .btn-green } |

**Abstract**: Representing articulated objects remains a difficult problem within the field
of robotics. Objects such as pliers, clamps, or cabinets require
representations that capture not only geometry and color information, but also
part seperation, connectivity, and joint parametrization. Furthermore, learning
these representations becomes even more difficult with each additional degree
of freedom. Complex articulated objects such as robot arms may have seven or
more degrees of freedom, and the depth of their kinematic tree may be notably
greater than the tools, drawers, and cabinets that are the typical subjects of
articulated object research. To address these concerns, we introduce SPLATART -
a pipeline for learning Gaussian splat representations of articulated objects
from posed images, of which a subset contains image space part segmentations.
SPLATART disentangles the part separation task from the articulation estimation
task, allowing for post-facto determination of joint estimation and
representation of articulated objects with deeper kinematic trees than
previously exhibited. In this work, we present data on the SPLATART pipeline as
applied to the syntheic Paris dataset objects, and qualitative results on a
real-world object under spare segmentation supervision. We additionally present
on articulated serial chain manipulators to demonstrate usage on deeper
kinematic tree structures.

Comments:
- 7 pages, Accepted to the 2025 RSS Workshop on Gaussian
  Representations for Robot Autonomy. Contact: Stanley Lewis, stanlew@umich.edu

---

## Anti-Aliased 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-12 | Mae Younes, Adnane Boukhayma | cs.GR | [PDF](http://arxiv.org/pdf/2506.11252v1){: .btn .btn-green } |

**Abstract**: 2D Gaussian Splatting (2DGS) has recently emerged as a promising method for
novel view synthesis and surface reconstruction, offering better
view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS
suffers from severe aliasing artifacts when rendering at different sampling
rates than those used during training, limiting its practical applications in
scenarios requiring camera zoom or varying fields of view. We identify that
these artifacts stem from two key limitations: the lack of frequency
constraints in the representation and an ineffective screen-space clamping
approach. To address these issues, we present AA-2DGS, an antialiased
formulation of 2D Gaussian Splatting that maintains its geometric benefits
while significantly enhancing rendering quality across different scales. Our
method introduces a world space flat smoothing kernel that constrains the
frequency content of 2D Gaussian primitives based on the maximal sampling
frequency from training views, effectively eliminating high-frequency artifacts
when zooming in. Additionally, we derive a novel object space Mip filter by
leveraging an affine approximation of the ray-splat intersection mapping, which
allows us to efficiently apply proper anti-aliasing directly in the local space
of each splat.

Comments:
- Code will be available at https://github.com/maeyounes/AA-2DGS

---

## PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-12 | Lintao Xiang, Hongpei Zheng, Yating Huang, Qijun Yang, Hujun Yin | cs.CV | [PDF](http://arxiv.org/pdf/2506.10335v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) is an innovative rendering technique that
surpasses the neural radiance field (NeRF) in both rendering speed and visual
quality by leveraging an explicit 3D scene representation. Existing 3DGS
approaches require a large number of calibrated views to generate a consistent
and complete scene representation. When input views are limited, 3DGS tends to
overfit the training views, leading to noticeable degradation in rendering
quality. To address this limitation, we propose a Point-wise Feature-Aware
Gaussian Splatting framework that enables real-time, high-quality rendering
from sparse training views. Specifically, we first employ the latest stereo
foundation model to estimate accurate camera poses and reconstruct a dense
point cloud for Gaussian initialization. We then encode the colour attributes
of each 3D Gaussian by sampling and aggregating multiscale 2D appearance
features from sparse inputs. To enhance point-wise appearance representation,
we design a point interaction network based on a self-attention mechanism,
allowing each Gaussian point to interact with its nearest neighbors. These
enriched features are subsequently decoded into Gaussian parameters through two
lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive
experiments on diverse benchmarks demonstrate that our method significantly
outperforms NeRF-based approaches and achieves competitive performance under
few-shot settings compared to the state-of-the-art 3DGS methods.



---

## Self-Supervised Multi-Part Articulated Objects Modeling via Deformable  Gaussian Splatting and Progressive Primitive Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Haowen Wang, Xiaoping Yuan, Zhao Jin, Zhen Zhao, Zhengping Che, Yousong Xue, Jin Tian, Yakun Huang, Jian Tang | cs.CV | [PDF](http://arxiv.org/pdf/2506.09663v1){: .btn .btn-green } |

**Abstract**: Articulated objects are ubiquitous in everyday life, and accurate 3D
representations of their geometry and motion are critical for numerous
applications. However, in the absence of human annotation, existing approaches
still struggle to build a unified representation for objects that contain
multiple movable parts. We introduce DeGSS, a unified framework that encodes
articulated objects as deformable 3D Gaussian fields, embedding geometry,
appearance, and motion in one compact representation. Each interaction state is
modeled as a smooth deformation of a shared field, and the resulting
deformation trajectories guide a progressive coarse-to-fine part segmentation
that identifies distinct rigid components, all in an unsupervised manner. The
refined field provides a spatially continuous, fully decoupled description of
every part, supporting part-level reconstruction and precise modeling of their
kinematic relationships. To evaluate generalization and realism, we enlarge the
synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset
that pairs RGB captures with accurately reverse-engineered 3D models. Extensive
experiments demonstrate that our method outperforms existing methods in both
accuracy and stability.



---

## UniForward: Unified 3D Scene and Semantic Field Reconstruction via  Feed-Forward Gaussian Splatting from Only Sparse-View Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Qijian Tian, Xin Tan, Jingyu Gong, Yuan Xie, Lizhuang Ma | cs.CV | [PDF](http://arxiv.org/pdf/2506.09378v1){: .btn .btn-green } |

**Abstract**: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and
semantic field reconstruction. Combining 3D scenes with semantic fields
facilitates the perception and understanding of the surrounding environment.
However, key challenges include embedding semantics into 3D representations,
achieving generalizable real-time reconstruction, and ensuring practical
applicability by using only images as input without camera parameters or ground
truth depth. To this end, we propose UniForward, a feed-forward model to
predict 3D Gaussians with anisotropic semantic features from only uncalibrated
and unposed sparse-view images. To enable the unified representation of the 3D
scene and semantic field, we embed semantic features into 3D Gaussians and
predict them through a dual-branch decoupled decoder. During training, we
propose a loss-guided view sampler to sample views from easy to hard,
eliminating the need for ground truth depth or masks required by previous
methods and stabilizing the training process. The whole model can be trained
end-to-end using a photometric loss and a distillation loss that leverages
semantic features from a pre-trained 2D semantic model. At the inference stage,
our UniForward can reconstruct 3D scenes and the corresponding semantic fields
in real time from only sparse-view images. The reconstructed 3D scenes achieve
high-quality rendering, and the reconstructed 3D semantic field enables the
rendering of view-consistent semantic features from arbitrary views, which can
be further decoded into dense segmentation masks in an open-vocabulary manner.
Experiments on novel view synthesis and novel view segmentation demonstrate
that our method achieves state-of-the-art performances for unifying 3D scene
and semantic field reconstruction.



---

## DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion  Decomposition for Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Junli Deng, Ping Shi, Qipei Li, Jinyang Guo | cs.CV | [PDF](http://arxiv.org/pdf/2506.09836v1){: .btn .btn-green } |

**Abstract**: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.



---

## TinySplat: Feedforward Approach for Generating Compact 3D Scene  Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Zetian Song, Jiaye Fu, Jiaqi Zhang, Xiaohan Lu, Chuanmin Jia, Siwei Ma, Wen Gao | cs.CV | [PDF](http://arxiv.org/pdf/2506.09479v1){: .btn .btn-green } |

**Abstract**: The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a
new paradigm to reconstruct 3D scenes. Using neural networks trained on
large-scale multi-view datasets, it can directly infer 3DGS representations
from sparse input views. Although the feedforward approach achieves high
reconstruction speed, it still suffers from the substantial storage cost of 3D
Gaussians. Existing 3DGS compression methods relying on scene-wise optimization
are not applicable due to architectural incompatibilities. To overcome this
limitation, we propose TinySplat, a complete feedforward approach for
generating compact 3D scene representations. Built upon standard feedforward
3DGS methods, TinySplat integrates a training-free compression framework that
systematically eliminates key sources of redundancy. Specifically, we introduce
View-Projection Transformation (VPT) to reduce geometric redundancy by
projecting geometric parameters into a more compact space. We further present
Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy
by aligning feature energy along dominant viewing directions via basis
transformation. Lastly, spatial redundancy is addressed through an
off-the-shelf video codec. Comprehensive experimental results on multiple
benchmark datasets demonstrate that TinySplat achieves over 100x compression
for 3D Gaussian data generated by feedforward methods. Compared to the
state-of-the-art compression approach, we achieve comparable quality with only
6% of the storage size. Meanwhile, our compression framework requires only 25%
of the encoding time and 1% of the decoding time.



---

## Gaussian Herding across Pens: An Optimal Transport Perspective on Global  Gaussian Reduction for 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Tao Wang, Mengyu Li, Geduo Zeng, Cheng Meng, Qiong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2506.09534v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance
field rendering, but it typically requires millions of redundant Gaussian
primitives, overwhelming memory and rendering budgets. Existing compaction
approaches address this by pruning Gaussians based on heuristic importance
scores, without global fidelity guarantee. To bridge this gap, we propose a
novel optimal transport perspective that casts 3DGS compaction as global
Gaussian mixture reduction. Specifically, we first minimize the composite
transport divergence over a KD-tree partition to produce a compact geometric
representation, and then decouple appearance from geometry by fine-tuning color
and opacity attributes with far fewer Gaussian primitives. Experiments on
benchmark datasets show that our method (i) yields negligible loss in rendering
quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;
and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.
Notably, our method is applicable to any stage of vanilla or accelerated 3DGS
pipelines, providing an efficient and agnostic pathway to lightweight neural
rendering.

Comments:
- 18 pages, 8 figures

---

## The Less You Depend, The More You Learn: Synthesizing Novel Views from  Sparse, Unposed Images without Any 3D Knowledge

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Haoru Wang, Kai Ye, Yangyan Li, Wenzheng Chen, Baoquan Chen | cs.CV | [PDF](http://arxiv.org/pdf/2506.09885v1){: .btn .btn-green } |

**Abstract**: We consider the problem of generalizable novel view synthesis (NVS), which
aims to generate photorealistic novel views from sparse or even unposed 2D
images without per-scene optimization. This task remains fundamentally
challenging, as it requires inferring 3D structure from incomplete and
ambiguous 2D observations. Early approaches typically rely on strong 3D
knowledge, including architectural 3D inductive biases (e.g., embedding
explicit 3D representations, such as NeRF or 3DGS, into network design) and
ground-truth camera poses for both input and target views. While recent efforts
have sought to reduce the 3D inductive bias or the dependence on known camera
poses of input views, critical questions regarding the role of 3D knowledge and
the necessity of circumventing its use remain under-explored. In this work, we
conduct a systematic analysis on the 3D knowledge and uncover a critical trend:
the performance of methods that requires less 3D knowledge accelerates more as
data scales, eventually achieving performance on par with their 3D
knowledge-driven counterparts, which highlights the increasing importance of
reducing dependence on 3D knowledge in the era of large-scale data. Motivated
by and following this trend, we propose a novel NVS framework that minimizes 3D
inductive bias and pose dependence for both input and target views. By
eliminating this 3D knowledge, our method fully leverages data scaling and
learns implicit 3D awareness directly from sparse 2D images, without any 3D
inductive bias or pose annotation during training. Extensive experiments
demonstrate that our model generates photorealistic and 3D-consistent novel
views, achieving even comparable performance with methods that rely on posed
inputs, thereby validating the feasibility and effectiveness of our
data-centric paradigm. Project page:
https://pku-vcl-geometry.github.io/Less3Depend/ .



---

## UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Ziyi Wang, Yanran Zhang, Jie Zhou, Jiwen Lu | cs.CV | [PDF](http://arxiv.org/pdf/2506.09952v1){: .btn .btn-green } |

**Abstract**: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

Comments:
- Accepted to CVPR 2025

---

## HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for  Dynamic Scene

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2506.09518v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D scenes from monocular videos remains a fundamental
challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time
rendering in static settings, extending it to dynamic scenes is challenging due
to the difficulty of learning structured and temporally consistent motion
representations. This challenge often manifests as three limitations in
existing methods: redundant Gaussian updates, insufficient motion supervision,
and weak modeling of complex non-rigid deformations. These issues collectively
hinder coherent and efficient dynamic reconstruction. To address these
limitations, we propose HAIF-GS, a unified framework that enables structured
and consistent dynamic modeling through sparse anchor-driven deformation. It
first identifies motion-relevant regions via an Anchor Filter to suppresses
redundant updates in static areas. A self-supervised Induced Flow-Guided
Deformation module induces anchor motion using multi-frame feature aggregation,
eliminating the need for explicit flow labels. To further handle fine-grained
deformations, a Hierarchical Anchor Propagation mechanism increases anchor
resolution based on motion complexity and propagates multi-level
transformations. Extensive experiments on synthetic and real-world benchmarks
validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in
rendering quality, temporal coherence, and reconstruction efficiency.



---

## DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular  Videos


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Chieh Hubert Lin, Zhaoyang Lv, Songyin Wu, Zhen Xu, Thu Nguyen-Phuoc, Hung-Yu Tseng, Julian Straub, Numair Khan, Lei Xiao, Ming-Hsuan Yang, Yuheng Ren, Richard Newcombe, Zhao Dong, Zhengqin Li | cs.GR | [PDF](http://arxiv.org/pdf/2506.09997v1){: .btn .btn-green } |

**Abstract**: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

Comments:
- Project page: https://hubert0527.github.io/dgslrm/

---

## ODG: Occupancy Prediction Using Dual Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-11 | Yunxiao Shi, Yinhao Zhu, Shizhong Han, Jisoo Jeong, Amin Ansari, Hong Cai, Fatih Porikli | cs.CV | [PDF](http://arxiv.org/pdf/2506.09417v2){: .btn .btn-green } |

**Abstract**: Occupancy prediction infers fine-grained 3D geometry and semantics from
camera images of the surrounding environment, making it a critical perception
task for autonomous driving. Existing methods either adopt dense grids as scene
representation, which is difficult to scale to high resolution, or learn the
entire scene using a single set of sparse queries, which is insufficient to
handle the various object characteristics. In this paper, we present ODG, a
hierarchical dual sparse Gaussian representation to effectively capture complex
scene dynamics. Building upon the observation that driving scenes can be
universally decomposed into static and dynamic counterparts, we define dual
Gaussian queries to better model the diverse scene objects. We utilize a
hierarchical Gaussian transformer to predict the occupied voxel centers and
semantic classes along with the Gaussian parameters. Leveraging the real-time
rendering capability of 3D Gaussian Splatting, we also impose rendering
supervision with available depth and semantic map annotations injecting
pixel-level alignment to boost occupancy learning. Extensive experiments on the
Occ3D-nuScenes and Occ3D-Waymo benchmarks demonstrate our proposed method sets
new state-of-the-art results while maintaining low inference cost.



---

## Gaussian2Scene: 3D Scene Representation Learning via Self-supervised  Learning with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-10 | Keyi Liu, Weidong Yang, Ben Fei, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2506.08777v2){: .btn .btn-green } |

**Abstract**: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.



---

## StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated  Video Streams


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-10 | Zike Wu, Qi Yan, Xuanyu Yi, Lele Wang, Renjie Liao | cs.CV | [PDF](http://arxiv.org/pdf/2506.08862v1){: .btn .btn-green } |

**Abstract**: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.



---

## A Probability-guided Sampler for Neural Implicit Surface Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-10 | Gonçalo Dias Pais, Valter Piedade, Moitreya Chatterjee, Marcus Greiff, Pedro Miraldo | cs.CV | [PDF](http://arxiv.org/pdf/2506.08619v1){: .btn .btn-green } |

**Abstract**: Several variants of Neural Radiance Fields (NeRFs) have significantly
improved the accuracy of synthesized images and surface reconstruction of 3D
scenes/objects. In all of these methods, a key characteristic is that none can
train the neural network with every possible input data, specifically, every
pixel and potential 3D point along the projection rays due to scalability
issues. While vanilla NeRFs uniformly sample both the image pixels and 3D
points along the projection rays, some variants focus only on guiding the
sampling of the 3D points along the projection rays. In this paper, we leverage
the implicit surface representation of the foreground scene and model a
probability density function in a 3D image projection space to achieve a more
targeted sampling of the rays toward regions of interest, resulting in improved
rendering. Additionally, a new surface reconstruction loss is proposed for
improved performance. This new loss fully explores the proposed 3D image
projection space model and incorporates near-to-surface and empty space
components. By integrating our novel sampling strategy and novel loss into
current state-of-the-art neural implicit surface renderers, we achieve more
accurate and detailed 3D reconstructions and improved image rendering,
especially for the regions of interest in any given scene.

Comments:
- Accepted in ECCV 2024

---

## TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary  Large-Scale Scene Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-10 | Xiaohan Zhang, Sitong Wang, Yushen Yan, Yi Yang, Mingda Xu, Qi Liu | cs.CV | [PDF](http://arxiv.org/pdf/2506.08704v1){: .btn .btn-green } |

**Abstract**: High-quality novel view synthesis for large-scale scenes presents a
challenging dilemma in 3D computer vision. Existing methods typically partition
large scenes into multiple regions, reconstruct a 3D representation using
Gaussian splatting for each region, and eventually merge them for novel view
rendering. They can accurately render specific scenes, yet they do not
generalize effectively for two reasons: (1) rigid spatial partition techniques
struggle with arbitrary camera trajectories, and (2) the merging of regions
results in Gaussian overlap to distort texture details. To address these
challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable
high-precision rendering for arbitrarily large-scale scenes. We present a
spatial partitioning method for large-scale scenes based on graphs, which
incorporates a regularization constraint to enhance the rendering of textures
and distant objects, as well as a progressive rendering strategy to mitigate
artifacts caused by Gaussian overlap. Experimental results demonstrate its
superior performance both on four aerial and four ground datasets and highlight
its remarkable efficiency: our method achieves an average improvement of 1.86
dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to
state-of-the-art approaches.



---

## SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-10 | Mengjiao Ma, Qi Ma, Yue Li, Jiahuan Cheng, Runyi Yang, Bin Ren, Nikola Popovic, Mingqiang Wei, Nicu Sebe, Luc Van Gool, Theo Gevers, Martin R. Oswald, Danda Pani Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2506.08710v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient
encoding of scene geometry, appearance, and semantics. Moreover, grounding
language in 3D scenes has proven to be an effective strategy for 3D scene
understanding. Current Language Gaussian Splatting line of work fall into three
main groups: (i) per-scene optimization-based, (ii) per-scene
optimization-free, and (iii) generalizable approach. However, most of them are
evaluated only on rendered 2D views of a handful of scenes and viewpoints close
to the training views, limiting ability and insight into holistic 3D
understanding. To address this gap, we propose the first large-scale benchmark
that systematically assesses these three groups of methods directly in 3D
space, evaluating on 1060 scenes across three indoor datasets and one outdoor
dataset. Benchmark results demonstrate a clear advantage of the generalizable
paradigm, particularly in relaxing the scene-specific limitation, enabling fast
feed-forward inference on novel scenes, and achieving superior segmentation
performance. We further introduce GaussianWorld-49K a carefully curated 3DGS
dataset comprising around 49K diverse indoor and outdoor scenes obtained from
multiple sources, with which we demonstrate the generalizable approach could
harness strong data priors. Our codes, benchmark, and datasets will be made
public to accelerate research in generalizable 3DGS scene understanding.

Comments:
- 15 pages, codes, data and benchmark will be released

---

## Complex-Valued Holographic Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-10 | Yicheng Zhan, Dong-Ha Shin, Seung-Hwan Baek, Kaan Akşit | cs.GR | [PDF](http://arxiv.org/pdf/2506.08350v1){: .btn .btn-green } |

**Abstract**: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

Comments:
- 28 pages, 21 figures

---

## GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for  High-Fidelity Super-Resolution

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | Shuja Khalid, Mohamed Ibrahim, Yang Liu | cs.GR | [PDF](http://arxiv.org/pdf/2506.07897v1){: .btn .btn-green } |

**Abstract**: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.



---

## Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression  of Dynamic Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | Allen Tu, Haiyang Ying, Alex Hanson, Yonghan Lee, Tom Goldstein, Matthias Zwicker | cs.GR | [PDF](http://arxiv.org/pdf/2506.07917v1){: .btn .btn-green } |

**Abstract**: Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve
high-quality novel view synthesis by using neural networks to predict the
time-varying deformation of each Gaussian. However, performing per-Gaussian
neural inference at every frame poses a significant bottleneck, limiting
rendering speed and increasing memory and compute requirements. In this paper,
we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general
pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS
representations by reducing neural inference through two complementary
techniques. First, we propose a temporal sensitivity pruning score that
identifies and removes Gaussians with low contribution to the dynamic scene
reconstruction. We also introduce an annealing smooth pruning mechanism that
improves pruning robustness in real-world scenes with imprecise camera poses.
Second, we propose GroupFlow, a motion analysis technique that clusters
Gaussians by trajectory similarity and predicts a single rigid transformation
per group instead of separate deformations for each Gaussian. Together, our
techniques accelerate rendering by $10.37\times$, reduce model size by
$7.71\times$, and shorten training time by $2.71\times$ on the NeRF-DS dataset.
SpeeDe3DGS also improves rendering speed by $4.20\times$ and $58.23\times$ on
the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be
integrated into any deformable 3DGS or 4DGS framework.

Comments:
- Project Page: https://speede3dgs.github.io/

---

## Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal  Navigation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | Yijie Deng, Shuaihang Yuan, Geeta Chandra Raju Bethala, Anthony Tzes, Yu-Shen Liu, Yi Fang | cs.CV | [PDF](http://arxiv.org/pdf/2506.07338v1){: .btn .btn-green } |

**Abstract**: Instance Image-Goal Navigation (IIN) requires autonomous agents to identify
and navigate to a target object or location depicted in a reference image
captured from any viewpoint. While recent methods leverage powerful novel view
synthesis (NVS) techniques, such as three-dimensional Gaussian splatting
(3DGS), they typically rely on randomly sampling multiple viewpoints or
trajectories to ensure comprehensive coverage of discriminative visual cues.
This approach, however, creates significant redundancy through overlapping
image samples and lacks principled view selection, substantially increasing
both rendering and comparison overhead. In this paper, we introduce a novel IIN
framework with a hierarchical scoring paradigm that estimates optimal
viewpoints for target matching. Our approach integrates cross-level semantic
scoring, utilizing CLIP-derived relevancy fields to identify regions with high
semantic similarity to the target object class, with fine-grained local
geometric scoring that performs precise pose estimation within promising
regions. Extensive evaluations demonstrate that our method achieves
state-of-the-art performance on simulated IIN benchmarks and real-world
applicability.



---

## PIG: Physically-based Multi-Material Interaction with 3D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | Zeyu Xiao, Zhenyi Wu, Mingyang Sun, Qipeng Yan, Yufan Guo, Zhuoer Liang, Lihua Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2506.07657v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has achieved remarkable success in reconstructing both
static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian
primitives, interactions between objects suffer from inaccurate 3D
segmentation, imprecise deformation among different materials, and severe
rendering artifacts. To address these challenges, we introduce PIG:
Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach
that combines 3D object segmentation with the simulation of interacting objects
in high precision. Firstly, our method facilitates fast and accurate mapping
from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.
Secondly, we assign unique physical properties to correspondingly segmented
objects within the scene for multi-material coupled interactions. Finally, we
have successfully embedded constraint scales into deformation gradients,
specifically clamping the scaling and rotation properties of the Gaussian
primitives to eliminate artifacts and achieve geometric fidelity and visual
consistency. Experimental results demonstrate that our method not only
outperforms the state-of-the-art (SOTA) in terms of visual quality, but also
opens up new directions and pipelines for the field of physically realistic
scene generation.



---

## ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline  Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | Xiaohan Lu, Jiaye Fu, Jiaqi Zhang, Zetian Song, Chuanmin Jia, Siwei Ma | cs.CV | [PDF](http://arxiv.org/pdf/2506.07670v1){: .btn .btn-green } |

**Abstract**: Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising
results for novel view synthesis (NVS) from sparse input views, particularly
under narrow-baseline conditions. However, its performance significantly
degrades in wide-baseline scenarios due to limited texture details and
geometric inconsistencies across views. To address these challenges, in this
paper, we propose ProSplat, a two-stage feed-forward framework designed for
high-fidelity rendering under wide-baseline conditions. The first stage
involves generating 3D Gaussian primitives via a 3DGS generator. In the second
stage, rendered views from these primitives are enhanced through an improvement
model. Specifically, this improvement model is based on a one-step diffusion
model, further optimized by our proposed Maximum Overlap Reference view
Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI
supplements missing texture and color by strategically selecting a reference
view with maximum viewpoint overlap, while DWEA enforces geometric consistency
using epipolar constraints. Additionally, we introduce a divide-and-conquer
training strategy that aligns data distributions between the two stages through
joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K
datasets under wide-baseline settings. Experimental results demonstrate that
ProSplat achieves an average improvement of 1 dB in PSNR compared to recent
SOTA methods.



---

## OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | Jens Piekenbrinck, Christian Schmidt, Alexander Hermans, Narunas Vaskevicius, Timm Linder, Bastian Leibe | cs.CV | [PDF](http://arxiv.org/pdf/2506.07697v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for
neural scene reconstruction, offering high-quality novel view synthesis while
maintaining computational efficiency. In this paper, we extend the capabilities
of 3DGS beyond pure scene representation by introducing an approach for
open-vocabulary 3D instance segmentation without requiring manual labeling,
termed OpenSplat3D. Our method leverages feature-splatting techniques to
associate semantic information with individual Gaussians, enabling fine-grained
scene understanding. We incorporate Segment Anything Model instance masks with
a contrastive loss formulation as guidance for the instance features to achieve
accurate instance-level segmentation. Furthermore, we utilize language
embeddings of a vision-language model, allowing for flexible, text-driven
instance identification. This combination enables our system to identify and
segment arbitrary objects in 3D scenes based on natural language descriptions.
We show results on LERF-mask and LERF-OVS as well as the full ScanNet++
validation set, demonstrating the effectiveness of our approach.



---

## STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory  Optimization and Architectural Support

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | Chenqi Zhang, Yu Feng, Jieru Zhao, Guangda Liu, Wenchao Ding, Chentao Wu, Minyi Guo | cs.GR | [PDF](http://arxiv.org/pdf/2506.09070v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.



---

## R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving  Simulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | William Ljungbergh, Bernardo Taveira, Wenzhao Zheng, Adam Tonderski, Chensheng Peng, Fredrik Kahl, Christoffer Petersson, Michael Felsberg, Kurt Keutzer, Masayoshi Tomizuka, Wei Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2506.07826v1){: .btn .btn-green } |

**Abstract**: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.



---

## Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and  Cross-Modal Consistency

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-09 | Xiangyu Guo, Zhanqian Wu, Kaixin Xiong, Ziyang Xu, Lijun Zhou, Gangwei Xu, Shaoqing Xu, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2506.07497v2){: .btn .btn-green } |

**Abstract**: We present Genesis, a unified framework for joint generation of multi-view
driving videos and LiDAR sequences with spatio-temporal and cross-modal
consistency. Genesis employs a two-stage architecture that integrates a
DiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR
generator with NeRF-based rendering and adaptive sampling. Both modalities are
directly coupled through a shared latent space, enabling coherent evolution
across visual and geometric domains. To guide the generation with structured
semantics, we introduce DataCrafter, a captioning module built on
vision-language models that provides scene-level and instance-level
supervision. Extensive experiments on the nuScenes benchmark demonstrate that
Genesis achieves state-of-the-art performance across video and LiDAR metrics
(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including
segmentation and 3D detection, validating the semantic fidelity and practical
utility of the generated data.



---

## Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented  Rasterization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-08 | Zhican Wang, Guanghui He, Dantong Liu, Lingjun Gao, Shell Xu Hu, Chen Zhang, Zhuoran Song, Nicholas Lane, Wayne Luk, Hongxiang Fan | cs.GR | [PDF](http://arxiv.org/pdf/2506.07069v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

Comments:
- Preprint. Under review

---

## Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-08 | Binxiao Huang, Zhihao Li, Shiyong Liu, Xiao Tang, Jiajun Tang, Jiaqi Lin, Yuxin Cheng, Zhenyu Chen, Xiaofei Wu, Ngai Wong | cs.CV | [PDF](http://arxiv.org/pdf/2506.06988v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has demonstrated exceptional performance in
image-based 3D reconstruction and real-time rendering. However, regions with
complex textures require numerous Gaussians to capture significant color
variations accurately, leading to inefficiencies in rendering speed. To address
this challenge, we introduce a hybrid representation for indoor scenes that
combines 3DGS with textured meshes. Our approach uses textured meshes to handle
texture-rich flat areas, while retaining Gaussians to model intricate
geometries. The proposed method begins by pruning and refining the extracted
mesh to eliminate geometrically complex regions. We then employ a joint
optimization for 3DGS and mesh, incorporating a warm-up strategy and
transmittance-aware supervision to balance their contributions
seamlessly.Extensive experiments demonstrate that the hybrid representation
maintains comparable rendering quality and achieves superior frames per second
FPS with fewer Gaussian primitives.



---

## Gaussian Mapping for Evolving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-07 | Vladimir Yugay, Thies Kersten, Luca Carlone, Theo Gevers, Martin R. Oswald, Lukas Schmid | cs.CV | [PDF](http://arxiv.org/pdf/2506.06909v1){: .btn .btn-green } |

**Abstract**: Mapping systems with novel view synthesis (NVS) capabilities are widely used
in computer vision, with augmented reality, robotics, and autonomous driving
applications. Most notably, 3D Gaussian Splatting-based systems show high NVS
performance; however, many current approaches are limited to static scenes.
While recent works have started addressing short-term dynamics (motion within
the view of the camera), long-term dynamics (the scene evolving through changes
out of view) remain less explored. To overcome this limitation, we introduce a
dynamic scene adaptation mechanism that continuously updates the 3D
representation to reflect the latest changes. In addition, since maintaining
geometric and semantic consistency remains challenging due to stale
observations disrupting the reconstruction process, we propose a novel keyframe
management mechanism that discards outdated observations while preserving as
much information as possible. We evaluate Gaussian Mapping for Evolving Scenes
(GaME) on both synthetic and real-world datasets and find it to be more
accurate than the state of the art.



---

## SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-07 | Sumit Sharma, Gopi Raju Matta, Kaushik Mitra | eess.IV | [PDF](http://arxiv.org/pdf/2506.06890v1){: .btn .btn-green } |

**Abstract**: Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging
technology, capable of detecting individual photons with remarkable timing
precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable
image capture at exceptionally high speeds under both low and high
illumination. Enabling 3D reconstruction and radiance field recovery from such
SPC data holds significant promise. However, the binary nature of SPC images
leads to severe information loss, particularly in texture and color, making
traditional 3D synthesis techniques ineffective. To address this challenge, we
propose a modular two-stage framework that converts binary SPC images into
high-quality colorized novel views. The first stage performs image-to-image
(I2I) translation using generative models such as Pix2PixHD, converting binary
SPC inputs into plausible RGB representations. The second stage employs 3D
scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian
Splatting (3DGS) to generate novel views. We validate our two-stage pipeline
(Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative
experiments, demonstrating significant improvements in perceptual quality and
geometric consistency over the alternative baseline.

Comments:
- Accepted for publication at ICIP 2025

---

## Parametric Gaussian Human Model: Generalizable Prior for Efficient and  Realistic Human Avatar Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-07 | Cheng Peng, Jingxiang Sun, Yushuo Chen, Zhaoqi Su, Zhuo Su, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2506.06645v1){: .btn .btn-green } |

**Abstract**: Photorealistic and animatable human avatars are a key enabler for
virtual/augmented reality, telepresence, and digital entertainment. While
recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering
quality and efficiency, existing methods still face fundamental challenges,
including time-consuming per-subject optimization and poor generalization under
sparse monocular inputs. In this work, we present the Parametric Gaussian Human
Model (PGHM), a generalizable and efficient framework that integrates human
priors into 3DGS for fast and high-fidelity avatar reconstruction from
monocular videos. PGHM introduces two core components: (1) a UV-aligned latent
identity map that compactly encodes subject-specific geometry and appearance
into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that
predicts Gaussian attributes by decomposing static, pose-dependent, and
view-dependent components via conditioned decoders. This design enables robust
rendering quality under challenging poses and viewpoints, while allowing
efficient subject adaptation without requiring multi-view capture or long
optimization time. Experiments show that PGHM is significantly more efficient
than optimization-from-scratch methods, requiring only approximately 20 minutes
per subject to produce avatars with comparable visual quality, thereby
demonstrating its practical applicability for real-world monocular avatar
creation.

Comments:
- Project Page: https://pengc02.github.io/pghm/

---

## Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-07 | Yangkai Lin, Jiabao Lei, Kui jia | cs.CV | [PDF](http://arxiv.org/pdf/2506.06846v1){: .btn .btn-green } |

**Abstract**: In recent years, there has been a growing demand to stylize a given 3D scene
to align with the artistic style of reference images for creative purposes.
While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method
for realistic 3D scene modeling, there remains a challenge in adapting it to
stylize 3D GS to match with multiple styles through automatic local style
transfer or manual designation, while maintaining memory efficiency for
stylization training. In this paper, we introduce a novel 3D GS stylization
solution termed Multi-StyleGS to tackle these challenges. In particular, we
employ a bipartite matching mechanism to au tomatically identify
correspondences between the style images and the local regions of the rendered
images. To facilitate local style transfer, we introduce a novel semantic style
loss function that employs a segmentation network to apply distinct styles to
various objects of the scene and propose a local-global feature matching to
enhance the multi-view consistency. Furthermore, this technique can achieve
memory efficient training, more texture details and better color match. To
better assign a robust semantic label to each Gaussian, we propose several
techniques to regularize the segmentation network. As demonstrated by our
comprehensive experiments, our approach outperforms existing ones in producing
plausible stylization results and offering flexible editing.

Comments:
- AAAI 2025

---

## Hi-LSplat: Hierarchical 3D Language Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-07 | Chenlu Zhan, Yufei Zhang, Gaoang Wang, Hongwei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2506.06822v1){: .btn .btn-green } |

**Abstract**: Modeling 3D language fields with Gaussian Splatting for open-ended language
queries has recently garnered increasing attention. However, recent 3DGS-based
models leverage view-dependent 2D foundation models to refine 3D semantics but
lack a unified 3D representation, leading to view inconsistencies.
Additionally, inherent open-vocabulary challenges cause inconsistencies in
object and relational descriptions, impeding hierarchical semantic
understanding. In this paper, we propose Hi-LSplat, a view-consistent
Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.
To achieve view-consistent 3D hierarchical semantics, we first lift 2D features
to 3D features by constructing a 3D hierarchical semantic tree with layered
instance clustering, which addresses the view inconsistency issue caused by 2D
semantic features. Besides, we introduce instance-wise and part-wise
contrastive losses to capture all-sided hierarchical semantic representations.
Notably, we construct two hierarchical semantic datasets to better assess the
model's ability to distinguish different semantic levels. Extensive experiments
highlight our method's superiority in 3D open-vocabulary segmentation and
localization. Its strong performance on hierarchical semantic datasets
underscores its ability to capture complex hierarchical semantics within 3D
scenes.



---

## Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational  Redundancy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Yu Feng, Weikai Lin, Yuge Cheng, Zihan Liu, Jingwen Leng, Minyi Guo, Chen Chen, Shixuan Sun, Yuhao Zhu | cs.AR | [PDF](http://arxiv.org/pdf/2506.05682v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural
rendering, but it remains computationally demanding on today's mobile SoCs. To
address this challenge, we propose Lumina, a hardware-algorithm co-designed
system, which integrates two principal optimizations: a novel algorithm, S^2,
and a radiance caching mechanism, RC, to improve the efficiency of neural
rendering. S2 algorithm exploits temporal coherence in rendering to reduce the
computational overhead, while RC leverages the color integration process of
3DGS to decrease the frequency of intensive rasterization computations. Coupled
with these techniques, we propose an accelerator architecture, LuminCore, to
further accelerate cache lookup and address the fundamental inefficiencies in
Rasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy
reduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB
peak signal-to-noise ratio reduction) across synthetic and real-world datasets.



---

## GS4: Generalizable Sparse Splatting Semantic SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Mingqi Jiang, Chanho Kim, Chen Ziwen, Li Fuxin | cs.CV | [PDF](http://arxiv.org/pdf/2506.06517v1){: .btn .btn-green } |

**Abstract**: Traditional SLAM algorithms are excellent at camera tracking but might
generate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting
(GS) approaches have emerged as an option for SLAM with accurate, dense 3D map
building. However, existing GS-based SLAM methods rely on per-scene
optimization which is time-consuming and does not generalize to diverse scenes
well. In this work, we introduce the first generalizable GS-based semantic SLAM
algorithm that incrementally builds and updates a 3D scene representation from
an RGB-D video stream using a learned generalizable network. Our approach
starts from an RGB-D image recognition backbone to predict the Gaussian
parameters from every downsampled and backprojected image location.
Additionally, we seamlessly integrate 3D semantic segmentation into our GS
framework, bridging 3D mapping and recognition through a shared backbone. To
correct localization drifting and floaters, we propose to optimize the GS for
only 1 iteration following global localization. We demonstrate state-of-the-art
semantic SLAM performance on the real-world benchmark ScanNet with an order of
magnitude fewer Gaussians compared to other recent GS-based methods, and
showcase our model's generalization capability through zero-shot transfer to
the NYUv2 and TUM RGB-D datasets.

Comments:
- 13 pages, 6 figures

---

## SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for  Surgical Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Yuchao Zheng, Jianing Zhang, Guochen Ning, Hongen Liao | cs.GR | [PDF](http://arxiv.org/pdf/2506.05935v1){: .btn .btn-green } |

**Abstract**: Intraoperative navigation relies heavily on precise 3D reconstruction to
ensure accuracy and safety during surgical procedures. However, endoscopic
scenarios present unique challenges, including sparse features and inconsistent
lighting, which render many existing Structure-from-Motion (SfM)-based methods
inadequate and prone to reconstruction failure. To mitigate these constraints,
we propose SurGSplat, a novel paradigm designed to progressively refine 3D
Gaussian Splatting (3DGS) through the integration of geometric constraints. By
enabling the detailed reconstruction of vascular structures and other critical
features, SurGSplat provides surgeons with enhanced visual clarity,
facilitating precise intraoperative decision-making. Experimental evaluations
demonstrate that SurGSplat achieves superior performance in both novel view
synthesis (NVS) and pose estimation accuracy, establishing it as a
high-fidelity and efficient solution for surgical scene reconstruction. More
information and results can be found on the page https://surgsplat.github.io/.



---

## Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic  Environments

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Mingrui Li, Yiming Zhou, Hongxing Zhou, Xinggang Hu, Florian Roemer, Hongyu Wang, Ahmad Osman | cs.CV | [PDF](http://arxiv.org/pdf/2506.05965v1){: .btn .btn-green } |

**Abstract**: Current Simultaneous Localization and Mapping (SLAM) methods based on Neural
Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static
3D scenes but struggle with tracking and reconstruction in dynamic
environments, such as real-world scenes with moving elements. Existing
NeRF-based SLAM approaches addressing dynamic challenges typically rely on
RGB-D inputs, with few methods accommodating pure RGB input. To overcome these
limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS)
SLAM method for dynamic scenes using monocular RGB input. To address dynamic
interference, we fuse optical flow masks and depth masks through a
probabilistic model to obtain a fused dynamic mask. With only a single network
iteration, this can constrain tracking scales and refine rendered geometry.
Based on the fused dynamic mask, we designed a novel motion loss to constrain
the pose estimation network for tracking. In mapping, we use the rendering loss
of dynamic pixels, color, and depth to eliminate transient interference and
occlusion caused by dynamic objects. Experimental results demonstrate that
Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic
environments, outperforming or matching existing RGB-D methods.



---

## Splat and Replace: 3D Reconstruction with Repetitive Elements

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Nicolás Violante, Andreas Meuleman, Alban Gauthier, Frédo Durand, Thibault Groueix, George Drettakis | cs.GR | [PDF](http://arxiv.org/pdf/2506.06462v1){: .btn .btn-green } |

**Abstract**: We leverage repetitive elements in 3D scenes to improve novel view synthesis.
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly
improved novel view synthesis but renderings of unseen and occluded parts
remain low-quality if the training views are not exhaustive enough. Our key
observation is that our environment is often full of repetitive elements. We
propose to leverage those repetitions to improve the reconstruction of
low-quality parts of the scene due to poor coverage and occlusions. We propose
a method that segments each repeated instance in a 3DGS reconstruction,
registers them together, and allows information to be shared among instances.
Our method improves the geometry while also accounting for appearance
variations across instances. We demonstrate our method on a variety of
synthetic and real scenes with typical repetitive elements, leading to a
substantial improvement in the quality of novel view synthesis.

Comments:
- SIGGRAPH Conference Papers 2025. Project site:
  https://repo-sam.inria.fr/nerphys/splat-and-replace/

---

## NeurNCD: Novel Class Discovery via Implicit Neural Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Junming Wang, Yi Shi | cs.LG | [PDF](http://arxiv.org/pdf/2506.06412v1){: .btn .btn-green } |

**Abstract**: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

Comments:
- Accepted by ICMR 2024

---

## ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Daniel Wang, Patrick Rim, Tian Tian, Alex Wong, Ganesh Sundaramoorthi | cs.GR | [PDF](http://arxiv.org/pdf/2506.05480v1){: .btn .btn-green } |

**Abstract**: We present ODE-GS, a novel method that unifies 3D Gaussian Splatting with
latent neural ordinary differential equations (ODEs) to forecast dynamic 3D
scenes far beyond the time span seen during training. Existing neural rendering
systems - whether NeRF- or 3DGS-based - embed time directly in a deformation
network and therefore excel at interpolation but collapse when asked to predict
the future, where timestamps are strictly out-of-distribution. ODE-GS
eliminates this dependency: after learning a high-fidelity, time-conditioned
deformation model for the training window, we freeze it and train a Transformer
encoder that summarizes past Gaussian trajectories into a latent state whose
continuous evolution is governed by a neural ODE. Numerical integration of this
latent flow yields smooth, physically plausible Gaussian trajectories that can
be queried at any future instant and rendered in real time. Coupled with a
variational objective and a lightweight second-derivative regularizer, ODE-GS
attains state-of-the-art extrapolation on D-NeRF and NVFI benchmarks, improving
PSNR by up to 10 dB and halving perceptual error (LPIPS) relative to the
strongest baselines. Our results demonstrate that continuous-time latent
dynamics are a powerful, practical route to photorealistic prediction of
complex 3D scenes.



---

## Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and  Expert Knowledge Transfer

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Filip Slezak, Magnus K. Gjerde, Joakim B. Haurum, Ivan Nikolov, Morten S. Laursen, Thomas B. Moeslund | cs.CV | [PDF](http://arxiv.org/pdf/2506.04908v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce a 3D Gaussian Splatting (3DGS)-based pipeline for
stereo dataset generation, offering an efficient alternative to Neural Radiance
Fields (NeRF)-based methods. To obtain useful geometry estimates, we explore
utilizing the reconstructed geometry from the explicit 3D representations as
well as depth estimates from the FoundationStereo model in an expert knowledge
transfer setup. We find that when fine-tuning stereo models on 3DGS-generated
datasets, we demonstrate competitive performance in zero-shot generalization
benchmarks. When using the reconstructed geometry directly, we observe that it
is often noisy and contains artifacts, which propagate noise to the trained
model. In contrast, we find that the disparity estimates from FoundationStereo
are cleaner and consequently result in a better performance on the zero-shot
generalization benchmarks. Our method highlights the potential for low-cost,
high-fidelity dataset creation and fast fine-tuning for deep stereo models.
Moreover, we also reveal that while the latest Gaussian Splatting based methods
have achieved superior performance on established benchmarks, their robustness
falls short in challenging in-the-wild settings warranting further exploration.



---

## VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for  Occupancy and Flow Prediction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Ziyue Zhu, Shenlong Wang, Jin Xie, Jiang-jiang Liu, Jingdong Wang, Jian Yang | cs.CV | [PDF](http://arxiv.org/pdf/2506.05563v1){: .btn .btn-green } |

**Abstract**: Recent advancements in camera-based occupancy prediction have focused on the
simultaneous prediction of 3D semantics and scene flow, a task that presents
significant challenges due to specific difficulties, e.g., occlusions and
unbalanced dynamic environments. In this paper, we analyze these challenges and
their underlying causes. To address them, we propose a novel regularization
framework called VoxelSplat. This framework leverages recent developments in 3D
Gaussian Splatting to enhance model performance in two key ways: (i) Enhanced
Semantics Supervision through 2D Projection: During training, our method
decodes sparse semantic 3D Gaussians from 3D representations and projects them
onto the 2D camera view. This provides additional supervision signals in the
camera-visible space, allowing 2D labels to improve the learning of 3D
semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene
flow to model the motion of Gaussians, and is thus able to learn the scene flow
of moving objects in a self-supervised manner using the labels of adjacent
frames. Our method can be seamlessly integrated into various existing occupancy
models, enhancing performance without increasing inference time. Extensive
experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat
in improving the accuracy of both semantic occupancy and scene flow estimation.
The project page and codes are available at
https://zzy816.github.io/VoxelSplat-Demo/.

Comments:
- Accepted by CVPR 2025 Project Page:
  https://zzy816.github.io/VoxelSplat-Demo/

---

## Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Alfred T. Christiansen, Andreas H. Højrup, Morten K. Stephansen, Md Ibtihaj A. Sakib, Taman S. Poojary, Filip Slezak, Morten S. Laursen, Thomas B. Moeslund, Joakim B. Haurum | cs.CV | [PDF](http://arxiv.org/pdf/2506.05009v1){: .btn .btn-green } |

**Abstract**: Training neural networks for tasks such as 3D point cloud semantic
segmentation demands extensive datasets, yet obtaining and annotating
real-world point clouds is costly and labor-intensive. This work aims to
introduce a novel pipeline for generating realistic synthetic data, by
leveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to
generate 3D assets of multiple different agricultural vehicles instead of using
generic models. These assets are placed in a simulated environment, where the
point clouds are generated using a simulated LiDAR. This is a flexible approach
that allows changing the LiDAR specifications without incurring additional
costs. We evaluated the impact of synthetic data on segmentation models such as
PointNet++, Point Transformer V3, and OACNN, by training and validating the
models only on synthetic data. Remarkably, the PTv3 model had an mIoU of
91.35\%, a noteworthy result given that the model had neither been trained nor
validated on any real data. Further studies even suggested that in certain
scenarios the models trained only on synthetically generated data performed
better than models trained on real-world data. Finally, experiments
demonstrated that the models can generalize across semantic classes, enabling
accurate predictions on mesh models they were never trained on.



---

## Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Duochao Shi, Weijie Wang, Donny Y. Chen, Zeyu Zhang, Jia-Wang Bian, Bohan Zhuang, Chunhua Shen | cs.CV | [PDF](http://arxiv.org/pdf/2506.05327v1){: .btn .btn-green } |

**Abstract**: Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS)
pipelines by unprojecting them into 3D point clouds for novel view synthesis.
This approach offers advantages such as efficient training, the use of known
camera poses, and accurate geometry estimation. However, depth discontinuities
at object boundaries often lead to fragmented or sparse point clouds, degrading
rendering quality -- a well-known limitation of depth-based representations. To
tackle this issue, we introduce PM-Loss, a novel regularization loss based on a
pointmap predicted by a pre-trained transformer. Although the pointmap itself
may be less accurate than the depth map, it effectively enforces geometric
smoothness, especially around object boundaries. With the improved depth map,
our method significantly improves the feed-forward 3DGS across various
architectures and scenes, delivering consistently better rendering results. Our
project page: https://aim-uofa.github.io/PMLoss

Comments:
- Project page: https://aim-uofa.github.io/PMLoss

---

## Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Nan Wang, Yuantao Chen, Lixing Xiao, Weiqing Xiao, Bohan Li, Zhaoxi Chen, Chongjie Ye, Shaocong Xu, Saining Zhang, Ziyang Yan, Pierre Merriaux, Lei Lei, Tianfan Xue, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2506.05280v2){: .btn .btn-green } |

**Abstract**: Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely
on photometric consistency to produce high-quality reconstructions. However, in
real-world scenarios, it is challenging to guarantee perfect photometric
consistency in acquired images. Appearance codes have been widely used to
address this issue, but their modeling capability is limited, as a single code
is applied to the entire image. Recently, the bilateral grid was introduced to
perform pixel-wise color mapping, but it is difficult to optimize and constrain
effectively. In this paper, we propose a novel multi-scale bilateral grid that
unifies appearance codes and bilateral grids. We demonstrate that this approach
significantly improves geometric accuracy in dynamic, decoupled autonomous
driving scene reconstruction, outperforming both appearance codes and bilateral
grids. This is crucial for autonomous driving, where accurate geometry is
important for obstacle avoidance and control. Our method shows strong results
across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further
demonstrate that the improvement in geometry is driven by the multi-scale
bilateral grid, which effectively reduces floaters caused by photometric
inconsistency.

Comments:
- Project page: https://bigcileng.github.io/bilateral-driving ; Code:
  https://github.com/BigCiLeng/bilateral-driving

---

## Synthetic Dataset Generation for Autonomous Mobile Robots Using 3D  Gaussian Splatting for Vision Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Aneesh Deogan, Wout Beks, Peter Teurlings, Koen de Vos, Mark van den Brand, Rene van de Molengraft | cs.RO | [PDF](http://arxiv.org/pdf/2506.05092v1){: .btn .btn-green } |

**Abstract**: Annotated datasets are critical for training neural networks for object
detection, yet their manual creation is time- and labour-intensive, subjective
to human error, and often limited in diversity. This challenge is particularly
pronounced in the domain of robotics, where diverse and dynamic scenarios
further complicate the creation of representative datasets. To address this, we
propose a novel method for automatically generating annotated synthetic data in
Unreal Engine. Our approach leverages photorealistic 3D Gaussian splats for
rapid synthetic data generation. We demonstrate that synthetic datasets can
achieve performance comparable to that of real-world datasets while
significantly reducing the time required to generate and annotate data.
Additionally, combining real-world and synthetic data significantly increases
object detection performance by leveraging the quality of real-world images
with the easier scalability of synthetic data. To our knowledge, this is the
first application of synthetic data for training object detection algorithms in
the highly dynamic and varied environment of robot soccer. Validation
experiments reveal that a detector trained on synthetic images performs on par
with one trained on manually annotated real-world images when tested on robot
soccer match scenarios. Our method offers a scalable and comprehensive
alternative to traditional dataset creation, eliminating the labour-intensive
error-prone manual annotation process. By generating datasets in a simulator
where all elements are intrinsically known, we ensure accurate annotations
while significantly reducing manual effort, which makes it particularly
valuable for robotics applications requiring diverse and scalable training
data.



---

## UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Jaehoon Choi, Dongki Jung, Christopher Maxey, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2506.05011v1){: .btn .btn-green } |

**Abstract**: Despite significant advancements in dynamic neural rendering, existing
methods fail to address the unique challenges posed by UAV-captured scenarios,
particularly those involving monocular camera setups, top-down perspective, and
multiple small, moving humans, which are not adequately represented in existing
datasets. In this work, we introduce UAV4D, a framework for enabling
photorealistic rendering for dynamic real-world scenes captured by UAVs.
Specifically, we address the challenge of reconstructing dynamic scenes with
multiple moving pedestrians from monocular video data without the need for
additional sensors. We use a combination of a 3D foundation model and a human
mesh reconstruction model to reconstruct both the scene background and humans.
We propose a novel approach to resolve the scene scale ambiguity and place both
humans and the scene in world coordinates by identifying human-scene contact
points. Additionally, we exploit the SMPL model and background mesh to
initialize Gaussian splats, enabling holistic scene rendering. We evaluated our
method on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and
Okutama-Action, each with distinct characteristics and 10~50 humans. Our
results demonstrate the benefits of our approach over existing methods in novel
view synthesis, achieving a 1.5 dB PSNR improvement and superior visual
sharpness.



---

## On-the-fly Reconstruction for Large-Scale Novel View Synthesis from  Unposed Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Andreas Meuleman, Ishaan Shah, Alexandre Lanvin, Bernhard Kerbl, George Drettakis | cs.CV | [PDF](http://arxiv.org/pdf/2506.05558v1){: .btn .btn-green } |

**Abstract**: Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy
reconstruction from photos, enabling free-viewpoint navigation. Nonetheless,
pose estimation using Structure from Motion and 3DGS optimization can still
each take between minutes and hours of computation after capture is complete.
SLAM methods combined with 3DGS are fast but struggle with wide camera
baselines and large scenes. We present an on-the-fly method to produce camera
poses and a trained 3DGS immediately after capture. Our method can handle dense
and wide-baseline captures of ordered photo sequences and large-scale scenes.
To do this, we first introduce fast initial pose estimation, exploiting learned
features and a GPU-friendly mini bundle adjustment. We then introduce direct
sampling of Gaussian primitive positions and shapes, incrementally spawning
primitives where required, significantly accelerating training. These two
efficient steps allow fast and robust joint optimization of poses and Gaussian
primitives. Our incremental approach handles large-scale scenes by introducing
scalable radiance field construction, progressively clustering 3DGS primitives,
storing them in anchors, and offloading them from the GPU. Clustered primitives
are progressively merged, keeping the required scale of 3DGS at any viewpoint.
We evaluate our solution on a variety of datasets and show that our solution
can provide on-the-fly processing of all the capture scenarios and scene sizes
we target while remaining competitive with other methods that only handle
specific capture styles or scene sizes in speed, image quality, or both.



---

## ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics  Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Daniel Rho, Jun Myeong Choi, Biswadip Dey, Roni Sengupta | cs.CV | [PDF](http://arxiv.org/pdf/2506.05317v2){: .btn .btn-green } |

**Abstract**: Neural rendering has made significant strides in 3D reconstruction and novel
view synthesis. With the integration with physics, it opens up new
applications. The inverse problem of estimating physics from visual data,
however, still remains challenging, limiting its effectiveness for applications
like physically accurate digital twin creation in robotics and XR. Existing
methods that incorporate physics into neural rendering frameworks typically
require dense multi-view videos as input, making them impractical for scalable,
real-world use. When presented with sparse multi-view videos, the sequential
optimization strategy used by existing approaches introduces significant error
accumulation, e.g., poor initial 3D reconstruction leads to bad material
parameter estimation in subsequent stages. Instead of sequential optimization,
directly optimizing all parameters at the same time also fails due to the
highly non-convex and often non-differentiable nature of the problem. We
propose ProJo4D, a progressive joint optimization framework that gradually
increases the set of jointly optimized parameters guided by their sensitivity,
leading to fully joint optimization over geometry, appearance, physical state,
and material property. Evaluations on PAC-NeRF and Spring-Gaus datasets show
that ProJo4D outperforms prior work in 4D future state prediction, novel view
rendering of future state, and material parameter estimation, demonstrating its
effectiveness in physically grounded 4D scene understanding. For demos, please
visit the project webpage: https://daniel03c1.github.io/ProJo4D/



---

## Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Gaia Di Lorenzo, Federico Tombari, Marc Pollefeys, Daniel Barath | cs.CV | [PDF](http://arxiv.org/pdf/2506.04789v1){: .btn .btn-green } |

**Abstract**: Learning effective multi-modal 3D representations of objects is essential for
numerous applications, such as augmented reality and robotics. Existing methods
often rely on task-specific embeddings that are tailored either for semantic
understanding or geometric reconstruction. As a result, these embeddings
typically cannot be decoded into explicit geometry and simultaneously reused
across tasks. In this paper, we propose Object-X, a versatile multi-modal
object representation framework capable of encoding rich object embeddings
(e.g. images, point cloud, text) and decoding them back into detailed geometric
and visual reconstructions. Object-X operates by geometrically grounding the
captured modalities in a 3D voxel grid and learning an unstructured embedding
fusing the information from the voxels with the object attributes. The learned
embedding enables 3D Gaussian Splatting-based object reconstruction, while also
supporting a range of downstream tasks, including scene alignment, single-image
3D object reconstruction, and localization. Evaluations on two challenging
real-world datasets demonstrate that Object-X produces high-fidelity novel-view
synthesis comparable to standard 3D Gaussian Splatting, while significantly
improving geometric accuracy. Moreover, Object-X achieves competitive
performance with specialized methods in scene alignment and localization.
Critically, our object-centric descriptors require 3-4 orders of magnitude less
storage compared to traditional image- or point cloud-based approaches,
establishing Object-X as a scalable and highly practical solution for
multi-modal 3D scene representation.



---

## FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Hengyu Liu, Yuehao Wang, Chenxin Li, Ruisi Cai, Kevin Wang, Wuyang Li, Pavlo Molchanov, Peihao Wang, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2506.04174v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has enabled various applications in 3D scene
representation and novel view synthesis due to its efficient rendering
capabilities. However, 3DGS demands relatively significant GPU memory, limiting
its use on devices with restricted computational resources. Previous approaches
have focused on pruning less important Gaussians, effectively compressing 3DGS
but often requiring a fine-tuning stage and lacking adaptability for the
specific memory needs of different devices. In this work, we present an elastic
inference method for 3DGS. Given an input for the desired model size, our
method selects and transforms a subset of Gaussians, achieving substantial
rendering performance without additional fine-tuning. We introduce a tiny
learnable module that controls Gaussian selection based on the input
percentage, along with a transformation module that adjusts the selected
Gaussians to complement the performance of the reduced model. Comprehensive
experiments on ZipNeRF, MipNeRF and Tanks\&Temples scenes demonstrate the
effectiveness of our approach. Code is available at https://flexgs.github.io.

Comments:
- CVPR 2025; Project Page: https://flexgs.github.io

---

## JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Yang Xiao, Guoan Xu, Qiang Wu, Wenjing Jia | cs.CV | [PDF](http://arxiv.org/pdf/2506.03872v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge
with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view
reconstruction methods provide an efficient solution for real-time novel view
synthesis by leveraging geometric priors learned from large-scale multi-view
datasets and computing 3D Gaussian centers via back-projection. Despite
offering strong geometric cues, both feed-forward multi-view depth estimation
and flow-depth joint estimation face key limitations: the former suffers from
mislocation and artifact issues in low-texture or repetitive regions, while the
latter is prone to local noise and global inconsistency due to unreliable
matches when ground-truth flow supervision is unavailable. To overcome this, we
propose JointSplat, a unified framework that leverages the complementarity
between optical flow and depth via a novel probabilistic optimization
mechanism. Specifically, this pixel-level mechanism scales the information
fusion between depth and flow based on the matching probability of optical flow
during training. Building upon the above mechanism, we further propose a novel
multi-view depth-consistency loss to leverage the reliability of supervision
while suppressing misleading gradients in uncertain areas. Evaluated on
RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art
(SOTA) methods, demonstrating the effectiveness and robustness of our proposed
probabilistic joint flow-depth optimization approach for high-fidelity
sparse-view 3D reconstruction.



---

## HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2506.04351v1){: .btn .btn-green } |

**Abstract**: 3D human generation is an important problem with a wide range of applications
in computer vision and graphics. Despite recent progress in generative AI such
as diffusion models or rendering methods like Neural Radiance Fields or
Gaussian Splatting, controlling the generation of accurate 3D humans from text
prompts remains an open challenge. Current methods struggle with fine detail,
accurate rendering of hands and faces, human realism, and controlability over
appearance. The lack of diversity, realism, and annotation in human image data
also remains a challenge, hindering the development of a foundational 3D human
model. We present a weakly supervised pipeline that tries to address these
challenges. In the first step, we generate a photorealistic human image dataset
with controllable attributes such as appearance, race, gender, etc using a
state-of-the-art image diffusion model. Next, we propose an efficient mapping
approach from image features to 3D point clouds using a transformer-based
architecture. Finally, we close the loop by training a point-cloud diffusion
model that is conditioned on the same text prompts used to generate the
original samples. We demonstrate orders-of-magnitude speed-ups in 3D human
generation compared to the state-of-the-art approaches, along with
significantly improved text-prompt alignment, realism, and rendering quality.
We will make the code and dataset available.



---

## Pseudo-Simulation for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta | cs.RO | [PDF](http://arxiv.org/pdf/2506.04218v1){: .btn .btn-green } |

**Abstract**: Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world evaluation is often challenging due to safety concerns
and a lack of reproducibility, whereas closed-loop simulation can face
insufficient realism or high computational costs. Open-loop evaluation, while
being efficient and data-driven, relies on metrics that generally overlook
compounding errors. In this paper, we propose pseudo-simulation, a novel
paradigm that addresses these limitations. Pseudo-simulation operates on real
datasets, similar to open-loop evaluation, but augments them with synthetic
observations generated prior to evaluation using 3D Gaussian Splatting. Our key
idea is to approximate potential future states the AV might encounter by
generating a diverse set of observations that vary in position, heading, and
speed. Our method then assigns a higher importance to synthetic observations
that best match the AV's likely behavior using a novel proximity-based
weighting scheme. This enables evaluating error recovery and the mitigation of
causal confusion, as in closed-loop benchmarks, without requiring sequential
interactive simulation. We show that pseudo-simulation is better correlated
with closed-loop simulations (R^2=0.8) than the best existing open-loop
approach (R^2=0.7). We also establish a public leaderboard for the community to
benchmark new methodologies with pseudo-simulation. Our code is available at
https://github.com/autonomousvision/navsim.



---

## Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot  Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Ben Moran, Mauro Comi, Steven Bohez, Tom Erez, Zhibin Li, Leonard Hasenclever | cs.RO | [PDF](http://arxiv.org/pdf/2506.04120v1){: .btn .btn-green } |

**Abstract**: Creating accurate, physical simulations directly from real-world robot motion
holds great value for safe, scalable, and affordable robot learning, yet
remains exceptionally challenging. Real robot data suffers from occlusions,
noisy camera poses, dynamic scene elements, which hinder the creation of
geometrically accurate and photorealistic digital twins of unseen objects. We
introduce a novel real-to-sim framework tackling all these challenges at once.
Our key insight is a hybrid scene representation merging the photorealistic
rendering of 3D Gaussian Splatting with explicit object meshes suitable for
physics simulation within a single representation. We propose an end-to-end
optimization pipeline that leverages differentiable rendering and
differentiable physics within MuJoCo to jointly refine all scene components -
from object geometry and appearance to robot poses and physical parameters -
directly from raw and imprecise robot trajectories. This unified optimization
allows us to simultaneously achieve high-fidelity object mesh reconstruction,
generate photorealistic novel views, and perform annotation-free robot pose
calibration. We demonstrate the effectiveness of our approach both in
simulation and on challenging real-world sequences using an ALOHA 2 bi-manual
manipulator, enabling more practical and robust real-to-simulation pipelines.



---

## SplArt: Articulation Estimation and Part-Level Reconstruction with 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Shengjie Lin, Jiading Fang, Muhammad Zubair Irshad, Vitor Campagnolo Guizilini, Rares Andrei Ambrus, Greg Shakhnarovich, Matthew R. Walter | cs.GR | [PDF](http://arxiv.org/pdf/2506.03594v1){: .btn .btn-green } |

**Abstract**: Reconstructing articulated objects prevalent in daily environments is crucial
for applications in augmented/virtual reality and robotics. However, existing
methods face scalability limitations (requiring 3D supervision or costly
annotations), robustness issues (being susceptible to local optima), and
rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a
self-supervised, category-agnostic framework that leverages 3D Gaussian
Splatting (3DGS) to reconstruct articulated objects and infer kinematics from
two sets of posed RGB images captured at different articulation states,
enabling real-time photorealistic rendering for novel viewpoints and
articulations. SplArt augments 3DGS with a differentiable mobility parameter
per Gaussian, achieving refined part segmentation. A multi-stage optimization
strategy is employed to progressively handle reconstruction, part segmentation,
and articulation estimation, significantly enhancing robustness and accuracy.
SplArt exploits geometric self-supervision, effectively addressing challenging
scenarios without requiring 3D annotations or category-specific priors.
Evaluations on established and newly proposed benchmarks, along with
applications to real-world scenarios using a handheld RGB camera, demonstrate
SplArt's state-of-the-art performance and real-world practicality. Code is
publicly available at https://github.com/ripl/splart.

Comments:
- https://github.com/ripl/splart

---

## Photoreal Scene Reconstruction from an Egocentric Device

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Zhaoyang Lv, Maurizio Monge, Ka Chen, Yufeng Zhu, Michael Goesele, Jakob Engel, Zhao Dong, Richard Newcombe | cs.CV | [PDF](http://arxiv.org/pdf/2506.04444v1){: .btn .btn-green } |

**Abstract**: In this paper, we investigate the challenges associated with using egocentric
devices to photorealistic reconstruct the scene in high dynamic range. Existing
methodologies typically assume using frame-rate 6DoF pose estimated from the
device's visual-inertial odometry system, which may neglect crucial details
necessary for pixel-accurate reconstruction. This study presents two
significant findings. Firstly, in contrast to mainstream work treating RGB
camera as global shutter frame-rate camera, we emphasize the importance of
employing visual-inertial bundle adjustment (VIBA) to calibrate the precise
timestamps and movement of the rolling shutter RGB sensing camera in a high
frequency trajectory format, which ensures an accurate calibration of the
physical properties of the rolling-shutter camera. Secondly, we incorporate a
physical image formation model based into Gaussian Splatting, which effectively
addresses the sensor characteristics, including the rolling-shutter effect of
RGB cameras and the dynamic ranges measured by sensors. Our proposed
formulation is applicable to the widely-used variants of Gaussian Splats
representation. We conduct a comprehensive evaluation of our pipeline using the
open-source Project Aria device under diverse indoor and outdoor lighting
conditions, and further validate it on a Meta Quest3 device. Across all
experiments, we observe a consistent visual enhancement of +1 dB in PSNR by
incorporating VIBA, with an additional +1 dB achieved through our proposed
image formation model. Our complete implementation, evaluation datasets, and
recording profile are available at
http://www.projectaria.com/photoreal-reconstruction/

Comments:
- Paper accepted to SIGGRAPH Conference Paper 2025

---

## Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Chengqi Li, Zhihao Shi, Yangdi Lu, Wenbo He, Xiangyu Xu | cs.CV | [PDF](http://arxiv.org/pdf/2506.03538v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction from in-the-wild images remains a challenging task due to
inconsistent lighting conditions and transient distractors. Existing methods
typically rely on heuristic strategies to handle the low-quality training data,
which often struggle to produce stable and consistent reconstructions,
frequently resulting in visual artifacts. In this work, we propose Asymmetric
Dual 3DGS, a novel framework that leverages the stochastic nature of these
artifacts: they tend to vary across different training runs due to minor
randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)
models in parallel, enforcing a consistency constraint that encourages
convergence on reliable scene geometry while suppressing inconsistent
artifacts. To prevent the two models from collapsing into similar failure modes
due to confirmation bias, we introduce a divergent masking strategy that
applies two complementary masks: a multi-cue adaptive mask and a
self-supervised soft mask, which leads to an asymmetric training process of the
two models, reducing shared error modes. In addition, to improve the efficiency
of model training, we introduce a lightweight variant called Dynamic EMA Proxy,
which replaces one of the two models with a dynamically updated Exponential
Moving Average (EMA) proxy, and employs an alternating masking strategy to
preserve divergence. Extensive experiments on challenging real-world datasets
demonstrate that our method consistently outperforms existing approaches while
achieving high efficiency. Codes and trained models will be released.



---

## Large Processor Chip Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Kaiyan Chang, Mingzhi Chen, Yunji Chen, Zhirong Chen, Dongrui Fan, Junfeng Gong, Nan Guo, Yinhe Han, Qinfen Hao, Shuo Hou, Xuan Huang, Pengwei Jin, Changxin Ke, Cangyuan Li, Guangli Li, Huawei Li, Kuan Li, Naipeng Li, Shengwen Liang, Cheng Liu, Hongwei Liu, Jiahua Liu, Junliang Lv, Jianan Mu, Jin Qin, Bin Sun, Chenxi Wang, Duo Wang, Mingjun Wang, Ying Wang, Chenggang Wu, Peiyang Wu, Teng Wu, Xiao Xiao, Mengyao Xie, Chenwei Xiong, Ruiyuan Xu, Mingyu Yan, Xiaochun Ye, Kuai Yu, Rui Zhang, Shuoming Zhang, Jiacheng Zhao | cs.AR | [PDF](http://arxiv.org/pdf/2506.02929v1){: .btn .btn-green } |

**Abstract**: Computer System Architecture serves as a crucial bridge between software
applications and the underlying hardware, encompassing components like
compilers, CPUs, coprocessors, and RTL designs. Its development, from early
mainframes to modern domain-specific architectures, has been driven by rising
computational demands and advancements in semiconductor technology. However,
traditional paradigms in computer system architecture design are confronting
significant challenges, including a reliance on manual expertise, fragmented
optimization across software and hardware layers, and high costs associated
with exploring expansive design spaces. While automated methods leveraging
optimization algorithms and machine learning have improved efficiency, they
remain constrained by a single-stage focus, limited data availability, and a
lack of comprehensive human domain knowledge. The emergence of large language
models offers transformative opportunities for the design of computer system
architecture. By leveraging the capabilities of LLMs in areas such as code
generation, data analysis, and performance modeling, the traditional manual
design process can be transitioned to a machine-based automated design
approach. To harness this potential, we present the Large Processor Chip Model
(LPCM), an LLM-driven framework aimed at achieving end-to-end automated
computer architecture design. The LPCM is structured into three levels:
Human-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D
Gaussian Splatting as a representative workload and employs the concept of
software-hardware collaborative design to examine the implementation of the
LPCM at Level 1, demonstrating the effectiveness of the proposed approach.
Furthermore, this paper provides an in-depth discussion on the pathway to
implementing Level 2 and Level 3 of the LPCM, along with an analysis of the
existing challenges.



---

## Voyager: Real-Time Splatting City-Scale 3D Gaussians on Your Phone

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Zheng Liu, He Zhu, Xinyang Li, Yirun Wang, Yujiao Shi, Wei Li, Jingwen Leng, Minyi Guo, Yu Feng | cs.GR | [PDF](http://arxiv.org/pdf/2506.02774v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is an emerging technique for photorealistic 3D
scene rendering. However, rendering city-scale 3DGS scenes on mobile devices,
e.g., your smartphones, remains a significant challenge due to the limited
resources on mobile devices. A natural solution is to offload computation to
the cloud; however, naively streaming rendered frames from the cloud to the
client introduces high latency and requires bandwidth far beyond the capacity
of current wireless networks.
  In this paper, we propose an effective solution to enable city-scale 3DGS
rendering on mobile devices. Our key insight is that, under normal user motion,
the number of newly visible Gaussians per second remains roughly constant.
Leveraging this, we stream only the necessary Gaussians to the client.
Specifically, on the cloud side, we propose asynchronous level-of-detail search
to identify the necessary Gaussians for the client. On the client side, we
accelerate rendering via a lookup table-based rasterization. Combined with
holistic runtime optimizations, our system can deliver low-latency, city-scale
3DGS rendering on mobile devices. Compared to existing solutions, Voyager
achieves over 100$\times$ reduction on data transfer and up to 8.9$\times$
speedup while retaining comparable rendering quality.



---

## EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for  Real-World 3DGS Scenes in VR

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Zihao Ding, Cheng-Tse Lee, Mufeng Zhu, Tao Guan, Yuan-Chun Sun, Cheng-Hsin Hsu, Yao Liu | cs.MM | [PDF](http://arxiv.org/pdf/2506.02380v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is an emerging media representation that
reconstructs real-world 3D scenes in high fidelity, enabling
6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,
developing and evaluating 3DGS-enabled applications and optimizing their
rendering performance, require realistic user navigation data. Such data is
currently unavailable for photorealistic 3DGS reconstructions of real-world
scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available
6-DoF navigation dataset featuring traces from 46 participants exploring twelve
diverse, real-world 3DGS scenes. The dataset was collected at two sites, using
the Meta Quest Pro headsets, recording the head pose and eye gaze data for each
rendered frame during free world standing 6-DoF navigation. For each of the
twelve scenes, we performed careful scene initialization to correct for scene
tilt and scale, ensuring a perceptually-comfortable VR experience. We also
release our open-source SIBR viewer software fork with record-and-replay
functionalities and a suite of utility tools for data processing, conversion,
and visualization. The EyeNavGS dataset and its accompanying software tools
provide valuable resources for advancing research in 6-DoF viewport prediction,
adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The
EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.



---

## LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Roman Titkov, Egor Zubkov, Dmitry Yudin, Jaafar Mahmoud, Malik Mohrat, Gennady Sidorov | cs.CV | [PDF](http://arxiv.org/pdf/2506.03073v1){: .btn .btn-green } |

**Abstract**: Modern Gaussian Splatting methods have proven highly effective for real-time
photorealistic rendering of 3D scenes. However, integrating semantic
information into this representation remains a significant challenge,
especially in maintaining real-time performance for SLAM (Simultaneous
Localization and Mapping) applications. In this work, we introduce LEG-SLAM --
a novel approach that fuses an optimized Gaussian Splatting implementation with
visual-language feature extraction using DINOv2 followed by a learnable feature
compressor based on Principal Component Analysis, while enabling an online
dense SLAM. Our method simultaneously generates high-quality photorealistic
images and semantically labeled scene maps, achieving real-time scene
reconstruction with more than 10 fps on the Replica dataset and 18 fps on
ScanNet. Experimental results show that our approach significantly outperforms
state-of-the-art methods in reconstruction speed while achieving competitive
rendering quality. The proposed system eliminates the need for prior data
preparation such as camera's ego motion or pre-computed static semantic maps.
With its potential applications in autonomous robotics, augmented reality, and
other interactive domains, LEG-SLAM represents a significant step forward in
real-time semantic 3D Gaussian-based SLAM. Project page:
https://titrom025.github.io/LEG-SLAM/



---

## Multi-Spectral Gaussian Splatting with Neural Color Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Lukas Meyer, Josef Grün, Maximilian Weiherer, Bernhard Egger, Marc Stamminger, Linus Franke | cs.GR | [PDF](http://arxiv.org/pdf/2506.03407v1){: .btn .btn-green } |

**Abstract**: We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)
framework that is able to generate multi-view consistent novel views from
images of multiple, independent cameras with different spectral domains. In
contrast to previous approaches, our method does not require cross-modal camera
calibration and is versatile enough to model a variety of different spectra,
including thermal and near-infra red, without any algorithmic changes.
  Unlike existing 3DGS-based frameworks that treat each modality separately (by
optimizing per-channel spherical harmonics) and therefore fail to exploit the
underlying spectral and spatial correlations, our method leverages a novel
neural color representation that encodes multi-spectral information into a
learned, compact, per-splat feature embedding. A shallow multi-layer perceptron
(MLP) then decodes this embedding to obtain spectral color values, enabling
joint learning of all bands within a unified representation.
  Our experiments show that this simple yet effective strategy is able to
improve multi-spectral rendering quality, while also leading to improved
per-spectra rendering quality over state-of-the-art methods. We demonstrate the
effectiveness of this new technique in agricultural applications to render
vegetation indices, such as normalized difference vegetation index (NDVI).



---

## RobustSplat: Decoupling Densification and Dynamics for Transient-Free  3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Chuanyu Fu, Yuqi Zhang, Kunbin Yao, Guanying Chen, Yuan Xiong, Chuan Huang, Shuguang Cui, Xiaochun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2506.02751v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for its
real-time, photo-realistic rendering in novel-view synthesis and 3D modeling.
However, existing methods struggle with accurately modeling scenes affected by
transient objects, leading to artifacts in the rendered images. We identify
that the Gaussian densification process, while enhancing scene detail capture,
unintentionally contributes to these artifacts by growing additional Gaussians
that model transient disturbances. To address this, we propose RobustSplat, a
robust solution based on two critical designs. First, we introduce a delayed
Gaussian growth strategy that prioritizes optimizing static scene structure
before allowing Gaussian splitting/cloning, mitigating overfitting to transient
objects in early optimization. Second, we design a scale-cascaded mask
bootstrapping approach that first leverages lower-resolution feature similarity
supervision for reliable initial transient mask estimation, taking advantage of
its stronger semantic consistency and robustness to noise, and then progresses
to high-resolution supervision to achieve more precise mask prediction.
Extensive experiments on multiple challenging datasets show that our method
outperforms existing methods, clearly demonstrating the robustness and
effectiveness of our method. Our project page is
https://fcyycf.github.io/RobustSplat/.

Comments:
- Project page: https://fcyycf.github.io/RobustSplat/

---

## Gen4D: Synthesizing Humans and Scenes in the Wild


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Jerrin Bright, Zhibo Wang, Yuhao Chen, Sirisha Rambhatla, John Zelek, David Clausi | cs.GR | [PDF](http://arxiv.org/pdf/2506.05397v1){: .btn .btn-green } |

**Abstract**: Lack of input data for in-the-wild activities often results in low
performance across various computer vision tasks. This challenge is
particularly pronounced in uncommon human-centric domains like sports, where
real-world data collection is complex and impractical. While synthetic datasets
offer a promising alternative, existing approaches typically suffer from
limited diversity in human appearance, motion, and scene composition due to
their reliance on rigid asset libraries and hand-crafted rendering pipelines.
To address this, we introduce Gen4D, a fully automated pipeline for generating
diverse and photorealistic 4D human animations. Gen4D integrates expert-driven
motion encoding, prompt-guided avatar generation using diffusion-based Gaussian
splatting, and human-aware background synthesis to produce highly varied and
lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale
synthetic dataset spanning three sports: baseball, icehockey, and soccer.
Together, Gen4D and SportPAL provide a scalable foundation for constructing
synthetic datasets tailored to in-the-wild human-centric vision tasks, with no
need for manual 3D modeling or scene design.

Comments:
- Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops

---

## RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis  and 3D Reconstruction of Autonomous Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Pou-Chun Kung, Skanda Harisha, Ram Vasudevan, Aline Eid, Katherine A. Skinner | cs.CV | [PDF](http://arxiv.org/pdf/2506.01379v1){: .btn .btn-green } |

**Abstract**: High-Fidelity 3D scene reconstruction plays a crucial role in autonomous
driving by enabling novel data generation from existing datasets. This allows
simulating safety-critical scenarios and augmenting training datasets without
incurring further data collection costs. While recent advances in radiance
fields have demonstrated promising results in 3D reconstruction and sensor data
synthesis using cameras and LiDAR, their potential for radar remains largely
unexplored. Radar is crucial for autonomous driving due to its robustness in
adverse weather conditions like rain, fog, and snow, where optical sensors
often struggle. Although the state-of-the-art radar-based neural representation
shows promise for 3D driving scene reconstruction, it performs poorly in
scenarios with significant radar noise, including receiver saturation and
multipath reflection. Moreover, it is limited to synthesizing preprocessed,
noise-excluded radar images, failing to address realistic radar data synthesis.
To address these limitations, this paper proposes RadarSplat, which integrates
Gaussian Splatting with novel radar noise modeling to enable realistic radar
data synthesis and enhanced 3D reconstruction. Compared to the
state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR
/ 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy),
demonstrating its effectiveness in generating high-fidelity radar data and
scene reconstruction. A project page is available at
https://umautobots.github.io/radarsplat.



---

## WorldExplorer: Towards Generating Fully Navigable 3D Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Manuel-Andreas Schneider, Lukas Höllein, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2506.01799v1){: .btn .btn-green } |

**Abstract**: Generating 3D worlds from text is a highly anticipated goal in computer
vision. Existing works are limited by the degree of exploration they allow
inside of a scene, i.e., produce streched-out and noisy artifacts when moving
beyond central or panoramic perspectives. To this end, we propose
WorldExplorer, a novel method based on autoregressive video trajectory
generation, which builds fully navigable 3D scenes with consistent visual
quality across a wide range of viewpoints. We initialize our scenes by creating
multi-view consistent images corresponding to a 360 degree panorama. Then, we
expand it by leveraging video diffusion models in an iterative scene generation
pipeline. Concretely, we generate multiple videos along short, pre-defined
trajectories, that explore the scene in depth, including motion around objects.
Our novel scene memory conditions each video on the most relevant prior views,
while a collision-detection mechanism prevents degenerate results, like moving
into objects. Finally, we fuse all generated views into a unified 3D
representation via 3D Gaussian Splatting optimization. Compared to prior
approaches, WorldExplorer produces high-quality scenes that remain stable under
large camera motion, enabling for the first time realistic and unrestricted
exploration. We believe this marks a significant step toward generating
immersive and truly explorable virtual 3D environments.

Comments:
- project page: see https://the-world-explorer.github.io/, video: see
  https://youtu.be/c1lBnwJWNmE

---

## GSCodec Studio: A Modular Framework for Gaussian Splat Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Sicheng Li, Chengzhen Wu, Hao Li, Xiang Gao, Yiyi Liao, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2506.01822v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting and its extension to 4D dynamic scenes enable
photorealistic, real-time rendering from real-world captures, positioning
Gaussian Splats (GS) as a promising format for next-generation immersive media.
However, their high storage requirements pose significant challenges for
practical use in sharing, transmission, and storage. Despite various studies
exploring GS compression from different perspectives, these efforts remain
scattered across separate repositories, complicating benchmarking and the
integration of best practices. To address this gap, we present GSCodec Studio,
a unified and modular framework for GS reconstruction, compression, and
rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction
methods and GS compression techniques as modular components, facilitating
flexible combinations and comprehensive comparisons. By integrating best
practices from community research and our own explorations, GSCodec Studio
supports the development of compact representation and compression solutions
for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec,
achieving competitive rate-distortion performance in static and dynamic GS
compression. The code for our framework is publicly available at
https://github.com/JasonLSC/GSCodec_Studio , to advance the research on
Gaussian Splats compression.

Comments:
- Repository of the project: https://github.com/JasonLSC/GSCodec_Studio

---

## WoMAP: World Models For Embodied Open-Vocabulary Object Localization


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Tenny Yin, Zhiting Mei, Tao Sun, Lihan Zha, Emily Zhou, Jeremy Bao, Miyu Yamane, Ola Shorinwa, Anirudha Majumdar | cs.RO | [PDF](http://arxiv.org/pdf/2506.01600v1){: .btn .btn-green } |

**Abstract**: Language-instructed active object localization is a critical challenge for
robots, requiring efficient exploration of partially observable environments.
However, state-of-the-art approaches either struggle to generalize beyond
demonstration datasets (e.g., imitation learning methods) or fail to generate
physically grounded actions (e.g., VLMs). To address these limitations, we
introduce WoMAP (World Models for Active Perception): a recipe for training
open-vocabulary object localization policies that: (i) uses a Gaussian
Splatting-based real-to-sim-to-real pipeline for scalable data generation
without the need for expert demonstrations, (ii) distills dense rewards signals
from open-vocabulary object detectors, and (iii) leverages a latent world model
for dynamics and rewards prediction to ground high-level action proposals at
inference time. Rigorous simulation and hardware experiments demonstrate
WoMAP's superior performance in a broad range of zero-shot object localization
tasks, with more than 9x and 2x higher success rates compared to VLM and
diffusion policy baselines, respectively. Further, we show that WoMAP achieves
strong generalization and sim-to-real transfer on a TidyBot.



---

## Globally Consistent RGB-D SLAM with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-01 | Xingguang Zhong, Yue Pan, Liren Jin, Marija Popović, Jens Behley, Cyrill Stachniss | cs.RO | [PDF](http://arxiv.org/pdf/2506.00970v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian splatting-based RGB-D SLAM displays remarkable
performance of high-fidelity 3D reconstruction. However, the lack of depth
rendering consistency and efficient loop closure limits the quality of its
geometric reconstructions and its ability to perform globally consistent
mapping online. In this paper, we present 2DGS-SLAM, an RGB-D SLAM system using
2D Gaussian splatting as the map representation. By leveraging the
depth-consistent rendering property of the 2D variant, we propose an accurate
camera pose optimization method and achieve geometrically accurate 3D
reconstruction. In addition, we implement efficient loop detection and camera
relocalization by leveraging MASt3R, a 3D foundation model, and achieve
efficient map updates by maintaining a local active map. Experiments show that
our 2DGS-SLAM approach achieves superior tracking accuracy, higher surface
reconstruction quality, and more consistent global map reconstruction compared
to existing rendering-based SLAM methods, while maintaining high-fidelity image
rendering and improved computational efficiency.

Comments:
- 18 pages

---

## CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-01 | Fengze Li, Yangle Liu, Jieming Ma, Hai-Ning Liang, Yaochun Shen, Huangxiang Li, Zhijing Wu | cs.CV | [PDF](http://arxiv.org/pdf/2506.01109v1){: .btn .btn-green } |

**Abstract**: Accurate fruit counting in real-world agricultural environments is a
longstanding challenge due to visual occlusions, semantic ambiguity, and the
high computational demands of 3D reconstruction. Existing methods based on
neural radiance fields suffer from low inference speed, limited generalization,
and lack support for open-set semantic control. This paper presents
FruitLangGS, a real-time 3D fruit counting framework that addresses these
limitations through spatial reconstruction, semantic embedding, and
language-guided instance estimation. FruitLangGS first reconstructs
orchard-scale scenes using an adaptive Gaussian splatting pipeline with
radius-aware pruning and tile-based rasterization for efficient rendering. To
enable semantic control, each Gaussian encodes a compressed CLIP-aligned
language embedding, forming a compact and queryable 3D representation. At
inference time, prompt-based semantic filtering is applied directly in 3D
space, without relying on image-space segmentation or view-level fusion. The
selected Gaussians are then converted into dense point clouds via
distribution-aware sampling and clustered to estimate fruit counts.
Experimental results on real orchard data demonstrate that FruitLangGS achieves
higher rendering speed, semantic flexibility, and counting accuracy compared to
prior approaches, offering a new perspective for language-driven, real-time
neural rendering across open-world scenarios.


