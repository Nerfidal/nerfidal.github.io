---
layout: default
title: June 2025
parent: Papers
nav_order: 202506
---

<!---metadata--->


## Pseudo-Simulation for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta | cs.RO | [PDF](http://arxiv.org/pdf/2506.04218v1){: .btn .btn-green } |

**Abstract**: Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world evaluation is often challenging due to safety concerns
and a lack of reproducibility, whereas closed-loop simulation can face
insufficient realism or high computational costs. Open-loop evaluation, while
being efficient and data-driven, relies on metrics that generally overlook
compounding errors. In this paper, we propose pseudo-simulation, a novel
paradigm that addresses these limitations. Pseudo-simulation operates on real
datasets, similar to open-loop evaluation, but augments them with synthetic
observations generated prior to evaluation using 3D Gaussian Splatting. Our key
idea is to approximate potential future states the AV might encounter by
generating a diverse set of observations that vary in position, heading, and
speed. Our method then assigns a higher importance to synthetic observations
that best match the AV's likely behavior using a novel proximity-based
weighting scheme. This enables evaluating error recovery and the mitigation of
causal confusion, as in closed-loop benchmarks, without requiring sequential
interactive simulation. We show that pseudo-simulation is better correlated
with closed-loop simulations (R^2=0.8) than the best existing open-loop
approach (R^2=0.7). We also establish a public leaderboard for the community to
benchmark new methodologies with pseudo-simulation. Our code is available at
https://github.com/autonomousvision/navsim.



---

## Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Chengqi Li, Zhihao Shi, Yangdi Lu, Wenbo He, Xiangyu Xu | cs.CV | [PDF](http://arxiv.org/pdf/2506.03538v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction from in-the-wild images remains a challenging task due to
inconsistent lighting conditions and transient distractors. Existing methods
typically rely on heuristic strategies to handle the low-quality training data,
which often struggle to produce stable and consistent reconstructions,
frequently resulting in visual artifacts. In this work, we propose Asymmetric
Dual 3DGS, a novel framework that leverages the stochastic nature of these
artifacts: they tend to vary across different training runs due to minor
randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)
models in parallel, enforcing a consistency constraint that encourages
convergence on reliable scene geometry while suppressing inconsistent
artifacts. To prevent the two models from collapsing into similar failure modes
due to confirmation bias, we introduce a divergent masking strategy that
applies two complementary masks: a multi-cue adaptive mask and a
self-supervised soft mask, which leads to an asymmetric training process of the
two models, reducing shared error modes. In addition, to improve the efficiency
of model training, we introduce a lightweight variant called Dynamic EMA Proxy,
which replaces one of the two models with a dynamically updated Exponential
Moving Average (EMA) proxy, and employs an alternating masking strategy to
preserve divergence. Extensive experiments on challenging real-world datasets
demonstrate that our method consistently outperforms existing approaches while
achieving high efficiency. Codes and trained models will be released.



---

## FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Hengyu Liu, Yuehao Wang, Chenxin Li, Ruisi Cai, Kevin Wang, Wuyang Li, Pavlo Molchanov, Peihao Wang, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2506.04174v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has enabled various applications in 3D scene
representation and novel view synthesis due to its efficient rendering
capabilities. However, 3DGS demands relatively significant GPU memory, limiting
its use on devices with restricted computational resources. Previous approaches
have focused on pruning less important Gaussians, effectively compressing 3DGS
but often requiring a fine-tuning stage and lacking adaptability for the
specific memory needs of different devices. In this work, we present an elastic
inference method for 3DGS. Given an input for the desired model size, our
method selects and transforms a subset of Gaussians, achieving substantial
rendering performance without additional fine-tuning. We introduce a tiny
learnable module that controls Gaussian selection based on the input
percentage, along with a transformation module that adjusts the selected
Gaussians to complement the performance of the reduced model. Comprehensive
experiments on ZipNeRF, MipNeRF and Tanks\&Temples scenes demonstrate the
effectiveness of our approach. Code is available at https://flexgs.github.io.

Comments:
- CVPR 2025; Project Page: https://flexgs.github.io

---

## Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot  Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Ben Moran, Mauro Comi, Steven Bohez, Tom Erez, Zhibin Li, Leonard Hasenclever | cs.RO | [PDF](http://arxiv.org/pdf/2506.04120v1){: .btn .btn-green } |

**Abstract**: Creating accurate, physical simulations directly from real-world robot motion
holds great value for safe, scalable, and affordable robot learning, yet
remains exceptionally challenging. Real robot data suffers from occlusions,
noisy camera poses, dynamic scene elements, which hinder the creation of
geometrically accurate and photorealistic digital twins of unseen objects. We
introduce a novel real-to-sim framework tackling all these challenges at once.
Our key insight is a hybrid scene representation merging the photorealistic
rendering of 3D Gaussian Splatting with explicit object meshes suitable for
physics simulation within a single representation. We propose an end-to-end
optimization pipeline that leverages differentiable rendering and
differentiable physics within MuJoCo to jointly refine all scene components -
from object geometry and appearance to robot poses and physical parameters -
directly from raw and imprecise robot trajectories. This unified optimization
allows us to simultaneously achieve high-fidelity object mesh reconstruction,
generate photorealistic novel views, and perform annotation-free robot pose
calibration. We demonstrate the effectiveness of our approach both in
simulation and on challenging real-world sequences using an ALOHA 2 bi-manual
manipulator, enabling more practical and robust real-to-simulation pipelines.



---

## SplArt: Articulation Estimation and Part-Level Reconstruction with 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Shengjie Lin, Jiading Fang, Muhammad Zubair Irshad, Vitor Campagnolo Guizilini, Rares Andrei Ambrus, Greg Shakhnarovich, Matthew R. Walter | cs.GR | [PDF](http://arxiv.org/pdf/2506.03594v1){: .btn .btn-green } |

**Abstract**: Reconstructing articulated objects prevalent in daily environments is crucial
for applications in augmented/virtual reality and robotics. However, existing
methods face scalability limitations (requiring 3D supervision or costly
annotations), robustness issues (being susceptible to local optima), and
rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a
self-supervised, category-agnostic framework that leverages 3D Gaussian
Splatting (3DGS) to reconstruct articulated objects and infer kinematics from
two sets of posed RGB images captured at different articulation states,
enabling real-time photorealistic rendering for novel viewpoints and
articulations. SplArt augments 3DGS with a differentiable mobility parameter
per Gaussian, achieving refined part segmentation. A multi-stage optimization
strategy is employed to progressively handle reconstruction, part segmentation,
and articulation estimation, significantly enhancing robustness and accuracy.
SplArt exploits geometric self-supervision, effectively addressing challenging
scenarios without requiring 3D annotations or category-specific priors.
Evaluations on established and newly proposed benchmarks, along with
applications to real-world scenarios using a handheld RGB camera, demonstrate
SplArt's state-of-the-art performance and real-world practicality. Code is
publicly available at https://github.com/ripl/splart.

Comments:
- https://github.com/ripl/splart

---

## JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Yang Xiao, Guoan Xu, Qiang Wu, Wenjing Jia | cs.CV | [PDF](http://arxiv.org/pdf/2506.03872v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge
with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view
reconstruction methods provide an efficient solution for real-time novel view
synthesis by leveraging geometric priors learned from large-scale multi-view
datasets and computing 3D Gaussian centers via back-projection. Despite
offering strong geometric cues, both feed-forward multi-view depth estimation
and flow-depth joint estimation face key limitations: the former suffers from
mislocation and artifact issues in low-texture or repetitive regions, while the
latter is prone to local noise and global inconsistency due to unreliable
matches when ground-truth flow supervision is unavailable. To overcome this, we
propose JointSplat, a unified framework that leverages the complementarity
between optical flow and depth via a novel probabilistic optimization
mechanism. Specifically, this pixel-level mechanism scales the information
fusion between depth and flow based on the matching probability of optical flow
during training. Building upon the above mechanism, we further propose a novel
multi-view depth-consistency loss to leverage the reliability of supervision
while suppressing misleading gradients in uncertain areas. Evaluated on
RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art
(SOTA) methods, demonstrating the effectiveness and robustness of our proposed
probabilistic joint flow-depth optimization approach for high-fidelity
sparse-view 3D reconstruction.



---

## Multi-Spectral Gaussian Splatting with Neural Color Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Lukas Meyer, Josef Grün, Maximilian Weiherer, Bernhard Egger, Marc Stamminger, Linus Franke | cs.GR | [PDF](http://arxiv.org/pdf/2506.03407v1){: .btn .btn-green } |

**Abstract**: We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)
framework that is able to generate multi-view consistent novel views from
images of multiple, independent cameras with different spectral domains. In
contrast to previous approaches, our method does not require cross-modal camera
calibration and is versatile enough to model a variety of different spectra,
including thermal and near-infra red, without any algorithmic changes.
  Unlike existing 3DGS-based frameworks that treat each modality separately (by
optimizing per-channel spherical harmonics) and therefore fail to exploit the
underlying spectral and spatial correlations, our method leverages a novel
neural color representation that encodes multi-spectral information into a
learned, compact, per-splat feature embedding. A shallow multi-layer perceptron
(MLP) then decodes this embedding to obtain spectral color values, enabling
joint learning of all bands within a unified representation.
  Our experiments show that this simple yet effective strategy is able to
improve multi-spectral rendering quality, while also leading to improved
per-spectra rendering quality over state-of-the-art methods. We demonstrate the
effectiveness of this new technique in agricultural applications to render
vegetation indices, such as normalized difference vegetation index (NDVI).



---

## LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Roman Titkov, Egor Zubkov, Dmitry Yudin, Jaafar Mahmoud, Malik Mohrat, Gennady Sidorov | cs.CV | [PDF](http://arxiv.org/pdf/2506.03073v1){: .btn .btn-green } |

**Abstract**: Modern Gaussian Splatting methods have proven highly effective for real-time
photorealistic rendering of 3D scenes. However, integrating semantic
information into this representation remains a significant challenge,
especially in maintaining real-time performance for SLAM (Simultaneous
Localization and Mapping) applications. In this work, we introduce LEG-SLAM --
a novel approach that fuses an optimized Gaussian Splatting implementation with
visual-language feature extraction using DINOv2 followed by a learnable feature
compressor based on Principal Component Analysis, while enabling an online
dense SLAM. Our method simultaneously generates high-quality photorealistic
images and semantically labeled scene maps, achieving real-time scene
reconstruction with more than 10 fps on the Replica dataset and 18 fps on
ScanNet. Experimental results show that our approach significantly outperforms
state-of-the-art methods in reconstruction speed while achieving competitive
rendering quality. The proposed system eliminates the need for prior data
preparation such as camera's ego motion or pre-computed static semantic maps.
With its potential applications in autonomous robotics, augmented reality, and
other interactive domains, LEG-SLAM represents a significant step forward in
real-time semantic 3D Gaussian-based SLAM. Project page:
https://titrom025.github.io/LEG-SLAM/



---

## Large Processor Chip Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Kaiyan Chang, Mingzhi Chen, Yunji Chen, Zhirong Chen, Dongrui Fan, Junfeng Gong, Nan Guo, Yinhe Han, Qinfen Hao, Shuo Hou, Xuan Huang, Pengwei Jin, Changxin Ke, Cangyuan Li, Guangli Li, Huawei Li, Kuan Li, Naipeng Li, Shengwen Liang, Cheng Liu, Hongwei Liu, Jiahua Liu, Junliang Lv, Jianan Mu, Jin Qin, Bin Sun, Chenxi Wang, Duo Wang, Mingjun Wang, Ying Wang, Chenggang Wu, Peiyang Wu, Teng Wu, Xiao Xiao, Mengyao Xie, Chenwei Xiong, Ruiyuan Xu, Mingyu Yan, Xiaochun Ye, Kuai Yu, Rui Zhang, Shuoming Zhang, Jiacheng Zhao | cs.AR | [PDF](http://arxiv.org/pdf/2506.02929v1){: .btn .btn-green } |

**Abstract**: Computer System Architecture serves as a crucial bridge between software
applications and the underlying hardware, encompassing components like
compilers, CPUs, coprocessors, and RTL designs. Its development, from early
mainframes to modern domain-specific architectures, has been driven by rising
computational demands and advancements in semiconductor technology. However,
traditional paradigms in computer system architecture design are confronting
significant challenges, including a reliance on manual expertise, fragmented
optimization across software and hardware layers, and high costs associated
with exploring expansive design spaces. While automated methods leveraging
optimization algorithms and machine learning have improved efficiency, they
remain constrained by a single-stage focus, limited data availability, and a
lack of comprehensive human domain knowledge. The emergence of large language
models offers transformative opportunities for the design of computer system
architecture. By leveraging the capabilities of LLMs in areas such as code
generation, data analysis, and performance modeling, the traditional manual
design process can be transitioned to a machine-based automated design
approach. To harness this potential, we present the Large Processor Chip Model
(LPCM), an LLM-driven framework aimed at achieving end-to-end automated
computer architecture design. The LPCM is structured into three levels:
Human-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D
Gaussian Splatting as a representative workload and employs the concept of
software-hardware collaborative design to examine the implementation of the
LPCM at Level 1, demonstrating the effectiveness of the proposed approach.
Furthermore, this paper provides an in-depth discussion on the pathway to
implementing Level 2 and Level 3 of the LPCM, along with an analysis of the
existing challenges.



---

## Voyager: Real-Time Splatting City-Scale 3D Gaussians on Your Phone

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Zheng Liu, He Zhu, Xinyang Li, Yirun Wang, Yujiao Shi, Wei Li, Jingwen Leng, Minyi Guo, Yu Feng | cs.GR | [PDF](http://arxiv.org/pdf/2506.02774v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is an emerging technique for photorealistic 3D
scene rendering. However, rendering city-scale 3DGS scenes on mobile devices,
e.g., your smartphones, remains a significant challenge due to the limited
resources on mobile devices. A natural solution is to offload computation to
the cloud; however, naively streaming rendered frames from the cloud to the
client introduces high latency and requires bandwidth far beyond the capacity
of current wireless networks.
  In this paper, we propose an effective solution to enable city-scale 3DGS
rendering on mobile devices. Our key insight is that, under normal user motion,
the number of newly visible Gaussians per second remains roughly constant.
Leveraging this, we stream only the necessary Gaussians to the client.
Specifically, on the cloud side, we propose asynchronous level-of-detail search
to identify the necessary Gaussians for the client. On the client side, we
accelerate rendering via a lookup table-based rasterization. Combined with
holistic runtime optimizations, our system can deliver low-latency, city-scale
3DGS rendering on mobile devices. Compared to existing solutions, Voyager
achieves over 100$\times$ reduction on data transfer and up to 8.9$\times$
speedup while retaining comparable rendering quality.



---

## RobustSplat: Decoupling Densification and Dynamics for Transient-Free  3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Chuanyu Fu, Yuqi Zhang, Kunbin Yao, Guanying Chen, Yuan Xiong, Chuan Huang, Shuguang Cui, Xiaochun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2506.02751v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for its
real-time, photo-realistic rendering in novel-view synthesis and 3D modeling.
However, existing methods struggle with accurately modeling scenes affected by
transient objects, leading to artifacts in the rendered images. We identify
that the Gaussian densification process, while enhancing scene detail capture,
unintentionally contributes to these artifacts by growing additional Gaussians
that model transient disturbances. To address this, we propose RobustSplat, a
robust solution based on two critical designs. First, we introduce a delayed
Gaussian growth strategy that prioritizes optimizing static scene structure
before allowing Gaussian splitting/cloning, mitigating overfitting to transient
objects in early optimization. Second, we design a scale-cascaded mask
bootstrapping approach that first leverages lower-resolution feature similarity
supervision for reliable initial transient mask estimation, taking advantage of
its stronger semantic consistency and robustness to noise, and then progresses
to high-resolution supervision to achieve more precise mask prediction.
Extensive experiments on multiple challenging datasets show that our method
outperforms existing methods, clearly demonstrating the robustness and
effectiveness of our method. Our project page is
https://fcyycf.github.io/RobustSplat/.

Comments:
- Project page: https://fcyycf.github.io/RobustSplat/

---

## EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for  Real-World 3DGS Scenes in VR

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Zihao Ding, Cheng-Tse Lee, Mufeng Zhu, Tao Guan, Yuan-Chun Sun, Cheng-Hsin Hsu, Yao Liu | cs.MM | [PDF](http://arxiv.org/pdf/2506.02380v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is an emerging media representation that
reconstructs real-world 3D scenes in high fidelity, enabling
6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,
developing and evaluating 3DGS-enabled applications and optimizing their
rendering performance, require realistic user navigation data. Such data is
currently unavailable for photorealistic 3DGS reconstructions of real-world
scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available
6-DoF navigation dataset featuring traces from 46 participants exploring twelve
diverse, real-world 3DGS scenes. The dataset was collected at two sites, using
the Meta Quest Pro headsets, recording the head pose and eye gaze data for each
rendered frame during free world standing 6-DoF navigation. For each of the
twelve scenes, we performed careful scene initialization to correct for scene
tilt and scale, ensuring a perceptually-comfortable VR experience. We also
release our open-source SIBR viewer software fork with record-and-replay
functionalities and a suite of utility tools for data processing, conversion,
and visualization. The EyeNavGS dataset and its accompanying software tools
provide valuable resources for advancing research in 6-DoF viewport prediction,
adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The
EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.



---

## WorldExplorer: Towards Generating Fully Navigable 3D Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Manuel-Andreas Schneider, Lukas Höllein, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2506.01799v1){: .btn .btn-green } |

**Abstract**: Generating 3D worlds from text is a highly anticipated goal in computer
vision. Existing works are limited by the degree of exploration they allow
inside of a scene, i.e., produce streched-out and noisy artifacts when moving
beyond central or panoramic perspectives. To this end, we propose
WorldExplorer, a novel method based on autoregressive video trajectory
generation, which builds fully navigable 3D scenes with consistent visual
quality across a wide range of viewpoints. We initialize our scenes by creating
multi-view consistent images corresponding to a 360 degree panorama. Then, we
expand it by leveraging video diffusion models in an iterative scene generation
pipeline. Concretely, we generate multiple videos along short, pre-defined
trajectories, that explore the scene in depth, including motion around objects.
Our novel scene memory conditions each video on the most relevant prior views,
while a collision-detection mechanism prevents degenerate results, like moving
into objects. Finally, we fuse all generated views into a unified 3D
representation via 3D Gaussian Splatting optimization. Compared to prior
approaches, WorldExplorer produces high-quality scenes that remain stable under
large camera motion, enabling for the first time realistic and unrestricted
exploration. We believe this marks a significant step toward generating
immersive and truly explorable virtual 3D environments.

Comments:
- project page: see https://the-world-explorer.github.io/, video: see
  https://youtu.be/c1lBnwJWNmE

---

## GSCodec Studio: A Modular Framework for Gaussian Splat Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Sicheng Li, Chengzhen Wu, Hao Li, Xiang Gao, Yiyi Liao, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2506.01822v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting and its extension to 4D dynamic scenes enable
photorealistic, real-time rendering from real-world captures, positioning
Gaussian Splats (GS) as a promising format for next-generation immersive media.
However, their high storage requirements pose significant challenges for
practical use in sharing, transmission, and storage. Despite various studies
exploring GS compression from different perspectives, these efforts remain
scattered across separate repositories, complicating benchmarking and the
integration of best practices. To address this gap, we present GSCodec Studio,
a unified and modular framework for GS reconstruction, compression, and
rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction
methods and GS compression techniques as modular components, facilitating
flexible combinations and comprehensive comparisons. By integrating best
practices from community research and our own explorations, GSCodec Studio
supports the development of compact representation and compression solutions
for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec,
achieving competitive rate-distortion performance in static and dynamic GS
compression. The code for our framework is publicly available at
https://github.com/JasonLSC/GSCodec_Studio , to advance the research on
Gaussian Splats compression.

Comments:
- Repository of the project: https://github.com/JasonLSC/GSCodec_Studio

---

## RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis  and 3D Reconstruction of Autonomous Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Pou-Chun Kung, Skanda Harisha, Ram Vasudevan, Aline Eid, Katherine A. Skinner | cs.CV | [PDF](http://arxiv.org/pdf/2506.01379v1){: .btn .btn-green } |

**Abstract**: High-Fidelity 3D scene reconstruction plays a crucial role in autonomous
driving by enabling novel data generation from existing datasets. This allows
simulating safety-critical scenarios and augmenting training datasets without
incurring further data collection costs. While recent advances in radiance
fields have demonstrated promising results in 3D reconstruction and sensor data
synthesis using cameras and LiDAR, their potential for radar remains largely
unexplored. Radar is crucial for autonomous driving due to its robustness in
adverse weather conditions like rain, fog, and snow, where optical sensors
often struggle. Although the state-of-the-art radar-based neural representation
shows promise for 3D driving scene reconstruction, it performs poorly in
scenarios with significant radar noise, including receiver saturation and
multipath reflection. Moreover, it is limited to synthesizing preprocessed,
noise-excluded radar images, failing to address realistic radar data synthesis.
To address these limitations, this paper proposes RadarSplat, which integrates
Gaussian Splatting with novel radar noise modeling to enable realistic radar
data synthesis and enhanced 3D reconstruction. Compared to the
state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR
/ 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy),
demonstrating its effectiveness in generating high-fidelity radar data and
scene reconstruction. A project page is available at
https://umautobots.github.io/radarsplat.



---

## WoMAP: World Models For Embodied Open-Vocabulary Object Localization


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Tenny Yin, Zhiting Mei, Tao Sun, Lihan Zha, Emily Zhou, Jeremy Bao, Miyu Yamane, Ola Shorinwa, Anirudha Majumdar | cs.RO | [PDF](http://arxiv.org/pdf/2506.01600v1){: .btn .btn-green } |

**Abstract**: Language-instructed active object localization is a critical challenge for
robots, requiring efficient exploration of partially observable environments.
However, state-of-the-art approaches either struggle to generalize beyond
demonstration datasets (e.g., imitation learning methods) or fail to generate
physically grounded actions (e.g., VLMs). To address these limitations, we
introduce WoMAP (World Models for Active Perception): a recipe for training
open-vocabulary object localization policies that: (i) uses a Gaussian
Splatting-based real-to-sim-to-real pipeline for scalable data generation
without the need for expert demonstrations, (ii) distills dense rewards signals
from open-vocabulary object detectors, and (iii) leverages a latent world model
for dynamics and rewards prediction to ground high-level action proposals at
inference time. Rigorous simulation and hardware experiments demonstrate
WoMAP's superior performance in a broad range of zero-shot object localization
tasks, with more than 9x and 2x higher success rates compared to VLM and
diffusion policy baselines, respectively. Further, we show that WoMAP achieves
strong generalization and sim-to-real transfer on a TidyBot.



---

## CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-01 | Fengze Li, Yangle Liu, Jieming Ma, Hai-Ning Liang, Yaochun Shen, Huangxiang Li, Zhijing Wu | cs.CV | [PDF](http://arxiv.org/pdf/2506.01109v1){: .btn .btn-green } |

**Abstract**: Accurate fruit counting in real-world agricultural environments is a
longstanding challenge due to visual occlusions, semantic ambiguity, and the
high computational demands of 3D reconstruction. Existing methods based on
neural radiance fields suffer from low inference speed, limited generalization,
and lack support for open-set semantic control. This paper presents
FruitLangGS, a real-time 3D fruit counting framework that addresses these
limitations through spatial reconstruction, semantic embedding, and
language-guided instance estimation. FruitLangGS first reconstructs
orchard-scale scenes using an adaptive Gaussian splatting pipeline with
radius-aware pruning and tile-based rasterization for efficient rendering. To
enable semantic control, each Gaussian encodes a compressed CLIP-aligned
language embedding, forming a compact and queryable 3D representation. At
inference time, prompt-based semantic filtering is applied directly in 3D
space, without relying on image-space segmentation or view-level fusion. The
selected Gaussians are then converted into dense point clouds via
distribution-aware sampling and clustered to estimate fruit counts.
Experimental results on real orchard data demonstrate that FruitLangGS achieves
higher rendering speed, semantic flexibility, and counting accuracy compared to
prior approaches, offering a new perspective for language-driven, real-time
neural rendering across open-world scenarios.



---

## Globally Consistent RGB-D SLAM with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-01 | Xingguang Zhong, Yue Pan, Liren Jin, Marija Popović, Jens Behley, Cyrill Stachniss | cs.RO | [PDF](http://arxiv.org/pdf/2506.00970v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian splatting-based RGB-D SLAM displays remarkable
performance of high-fidelity 3D reconstruction. However, the lack of depth
rendering consistency and efficient loop closure limits the quality of its
geometric reconstructions and its ability to perform globally consistent
mapping online. In this paper, we present 2DGS-SLAM, an RGB-D SLAM system using
2D Gaussian splatting as the map representation. By leveraging the
depth-consistent rendering property of the 2D variant, we propose an accurate
camera pose optimization method and achieve geometrically accurate 3D
reconstruction. In addition, we implement efficient loop detection and camera
relocalization by leveraging MASt3R, a 3D foundation model, and achieve
efficient map updates by maintaining a local active map. Experiments show that
our 2DGS-SLAM approach achieves superior tracking accuracy, higher surface
reconstruction quality, and more consistent global map reconstruction compared
to existing rendering-based SLAM methods, while maintaining high-fidelity image
rendering and improved computational efficiency.

Comments:
- 18 pages
