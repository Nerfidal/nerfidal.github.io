---
layout: default
title: June 2025
parent: Papers
nav_order: 202506
---

<!---metadata--->


## Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic  Environments

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Mingrui Li, Yiming Zhou, Hongxing Zhou, Xinggang Hu, Florian Roemer, Hongyu Wang, Ahmad Osman | cs.CV | [PDF](http://arxiv.org/pdf/2506.05965v1){: .btn .btn-green } |

**Abstract**: Current Simultaneous Localization and Mapping (SLAM) methods based on Neural
Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static
3D scenes but struggle with tracking and reconstruction in dynamic
environments, such as real-world scenes with moving elements. Existing
NeRF-based SLAM approaches addressing dynamic challenges typically rely on
RGB-D inputs, with few methods accommodating pure RGB input. To overcome these
limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS)
SLAM method for dynamic scenes using monocular RGB input. To address dynamic
interference, we fuse optical flow masks and depth masks through a
probabilistic model to obtain a fused dynamic mask. With only a single network
iteration, this can constrain tracking scales and refine rendered geometry.
Based on the fused dynamic mask, we designed a novel motion loss to constrain
the pose estimation network for tracking. In mapping, we use the rendering loss
of dynamic pixels, color, and depth to eliminate transient interference and
occlusion caused by dynamic objects. Experimental results demonstrate that
Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic
environments, outperforming or matching existing RGB-D methods.



---

## SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for  Surgical Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Yuchao Zheng, Jianing Zhang, Guochen Ning, Hongen Liao | cs.GR | [PDF](http://arxiv.org/pdf/2506.05935v1){: .btn .btn-green } |

**Abstract**: Intraoperative navigation relies heavily on precise 3D reconstruction to
ensure accuracy and safety during surgical procedures. However, endoscopic
scenarios present unique challenges, including sparse features and inconsistent
lighting, which render many existing Structure-from-Motion (SfM)-based methods
inadequate and prone to reconstruction failure. To mitigate these constraints,
we propose SurGSplat, a novel paradigm designed to progressively refine 3D
Gaussian Splatting (3DGS) through the integration of geometric constraints. By
enabling the detailed reconstruction of vascular structures and other critical
features, SurGSplat provides surgeons with enhanced visual clarity,
facilitating precise intraoperative decision-making. Experimental evaluations
demonstrate that SurGSplat achieves superior performance in both novel view
synthesis (NVS) and pose estimation accuracy, establishing it as a
high-fidelity and efficient solution for surgical scene reconstruction. More
information and results can be found on the page https://surgsplat.github.io/.



---

## Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational  Redundancy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-06 | Yu Feng, Weikai Lin, Yuge Cheng, Zihan Liu, Jingwen Leng, Minyi Guo, Chen Chen, Shixuan Sun, Yuhao Zhu | cs.AR | [PDF](http://arxiv.org/pdf/2506.05682v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural
rendering, but it remains computationally demanding on today's mobile SoCs. To
address this challenge, we propose Lumina, a hardware-algorithm co-designed
system, which integrates two principal optimizations: a novel algorithm, S^2,
and a radiance caching mechanism, RC, to improve the efficiency of neural
rendering. S2 algorithm exploits temporal coherence in rendering to reduce the
computational overhead, while RC leverages the color integration process of
3DGS to decrease the frequency of intensive rasterization computations. Coupled
with these techniques, we propose an accelerator architecture, LuminCore, to
further accelerate cache lookup and address the fundamental inefficiencies in
Rasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy
reduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB
peak signal-to-noise ratio reduction) across synthetic and real-world datasets.



---

## Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Nan Wang, Yuantao Chen, Lixing Xiao, Weiqing Xiao, Bohan Li, Zhaoxi Chen, Chongjie Ye, Shaocong Xu, Saining Zhang, Ziyang Yan, Pierre Merriaux, Lei Lei, Tianfan Xue, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2506.05280v2){: .btn .btn-green } |

**Abstract**: Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely
on photometric consistency to produce high-quality reconstructions. However, in
real-world scenarios, it is challenging to guarantee perfect photometric
consistency in acquired images. Appearance codes have been widely used to
address this issue, but their modeling capability is limited, as a single code
is applied to the entire image. Recently, the bilateral grid was introduced to
perform pixel-wise color mapping, but it is difficult to optimize and constrain
effectively. In this paper, we propose a novel multi-scale bilateral grid that
unifies appearance codes and bilateral grids. We demonstrate that this approach
significantly improves geometric accuracy in dynamic, decoupled autonomous
driving scene reconstruction, outperforming both appearance codes and bilateral
grids. This is crucial for autonomous driving, where accurate geometry is
important for obstacle avoidance and control. Our method shows strong results
across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further
demonstrate that the improvement in geometry is driven by the multi-scale
bilateral grid, which effectively reduces floaters caused by photometric
inconsistency.

Comments:
- Project page: https://bigcileng.github.io/bilateral-driving ; Code:
  https://github.com/BigCiLeng/bilateral-driving

---

## On-the-fly Reconstruction for Large-Scale Novel View Synthesis from  Unposed Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Andreas Meuleman, Ishaan Shah, Alexandre Lanvin, Bernhard Kerbl, George Drettakis | cs.CV | [PDF](http://arxiv.org/pdf/2506.05558v1){: .btn .btn-green } |

**Abstract**: Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy
reconstruction from photos, enabling free-viewpoint navigation. Nonetheless,
pose estimation using Structure from Motion and 3DGS optimization can still
each take between minutes and hours of computation after capture is complete.
SLAM methods combined with 3DGS are fast but struggle with wide camera
baselines and large scenes. We present an on-the-fly method to produce camera
poses and a trained 3DGS immediately after capture. Our method can handle dense
and wide-baseline captures of ordered photo sequences and large-scale scenes.
To do this, we first introduce fast initial pose estimation, exploiting learned
features and a GPU-friendly mini bundle adjustment. We then introduce direct
sampling of Gaussian primitive positions and shapes, incrementally spawning
primitives where required, significantly accelerating training. These two
efficient steps allow fast and robust joint optimization of poses and Gaussian
primitives. Our incremental approach handles large-scale scenes by introducing
scalable radiance field construction, progressively clustering 3DGS primitives,
storing them in anchors, and offloading them from the GPU. Clustered primitives
are progressively merged, keeping the required scale of 3DGS at any viewpoint.
We evaluate our solution on a variety of datasets and show that our solution
can provide on-the-fly processing of all the capture scenarios and scene sizes
we target while remaining competitive with other methods that only handle
specific capture styles or scene sizes in speed, image quality, or both.



---

## VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for  Occupancy and Flow Prediction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Ziyue Zhu, Shenlong Wang, Jin Xie, Jiang-jiang Liu, Jingdong Wang, Jian Yang | cs.CV | [PDF](http://arxiv.org/pdf/2506.05563v1){: .btn .btn-green } |

**Abstract**: Recent advancements in camera-based occupancy prediction have focused on the
simultaneous prediction of 3D semantics and scene flow, a task that presents
significant challenges due to specific difficulties, e.g., occlusions and
unbalanced dynamic environments. In this paper, we analyze these challenges and
their underlying causes. To address them, we propose a novel regularization
framework called VoxelSplat. This framework leverages recent developments in 3D
Gaussian Splatting to enhance model performance in two key ways: (i) Enhanced
Semantics Supervision through 2D Projection: During training, our method
decodes sparse semantic 3D Gaussians from 3D representations and projects them
onto the 2D camera view. This provides additional supervision signals in the
camera-visible space, allowing 2D labels to improve the learning of 3D
semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene
flow to model the motion of Gaussians, and is thus able to learn the scene flow
of moving objects in a self-supervised manner using the labels of adjacent
frames. Our method can be seamlessly integrated into various existing occupancy
models, enhancing performance without increasing inference time. Extensive
experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat
in improving the accuracy of both semantic occupancy and scene flow estimation.
The project page and codes are available at
https://zzy816.github.io/VoxelSplat-Demo/.

Comments:
- Accepted by CVPR 2025 Project Page:
  https://zzy816.github.io/VoxelSplat-Demo/

---

## ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Daniel Wang, Patrick Rim, Tian Tian, Alex Wong, Ganesh Sundaramoorthi | cs.GR | [PDF](http://arxiv.org/pdf/2506.05480v1){: .btn .btn-green } |

**Abstract**: We present ODE-GS, a novel method that unifies 3D Gaussian Splatting with
latent neural ordinary differential equations (ODEs) to forecast dynamic 3D
scenes far beyond the time span seen during training. Existing neural rendering
systems - whether NeRF- or 3DGS-based - embed time directly in a deformation
network and therefore excel at interpolation but collapse when asked to predict
the future, where timestamps are strictly out-of-distribution. ODE-GS
eliminates this dependency: after learning a high-fidelity, time-conditioned
deformation model for the training window, we freeze it and train a Transformer
encoder that summarizes past Gaussian trajectories into a latent state whose
continuous evolution is governed by a neural ODE. Numerical integration of this
latent flow yields smooth, physically plausible Gaussian trajectories that can
be queried at any future instant and rendered in real time. Coupled with a
variational objective and a lightweight second-derivative regularizer, ODE-GS
attains state-of-the-art extrapolation on D-NeRF and NVFI benchmarks, improving
PSNR by up to 10 dB and halving perceptual error (LPIPS) relative to the
strongest baselines. Our results demonstrate that continuous-time latent
dynamics are a powerful, practical route to photorealistic prediction of
complex 3D scenes.



---

## Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Duochao Shi, Weijie Wang, Donny Y. Chen, Zeyu Zhang, Jia-Wang Bian, Bohan Zhuang, Chunhua Shen | cs.CV | [PDF](http://arxiv.org/pdf/2506.05327v1){: .btn .btn-green } |

**Abstract**: Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS)
pipelines by unprojecting them into 3D point clouds for novel view synthesis.
This approach offers advantages such as efficient training, the use of known
camera poses, and accurate geometry estimation. However, depth discontinuities
at object boundaries often lead to fragmented or sparse point clouds, degrading
rendering quality -- a well-known limitation of depth-based representations. To
tackle this issue, we introduce PM-Loss, a novel regularization loss based on a
pointmap predicted by a pre-trained transformer. Although the pointmap itself
may be less accurate than the depth map, it effectively enforces geometric
smoothness, especially around object boundaries. With the improved depth map,
our method significantly improves the feed-forward 3DGS across various
architectures and scenes, delivering consistently better rendering results. Our
project page: https://aim-uofa.github.io/PMLoss

Comments:
- Project page: https://aim-uofa.github.io/PMLoss

---

## Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and  Expert Knowledge Transfer

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Filip Slezak, Magnus K. Gjerde, Joakim B. Haurum, Ivan Nikolov, Morten S. Laursen, Thomas B. Moeslund | cs.CV | [PDF](http://arxiv.org/pdf/2506.04908v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce a 3D Gaussian Splatting (3DGS)-based pipeline for
stereo dataset generation, offering an efficient alternative to Neural Radiance
Fields (NeRF)-based methods. To obtain useful geometry estimates, we explore
utilizing the reconstructed geometry from the explicit 3D representations as
well as depth estimates from the FoundationStereo model in an expert knowledge
transfer setup. We find that when fine-tuning stereo models on 3DGS-generated
datasets, we demonstrate competitive performance in zero-shot generalization
benchmarks. When using the reconstructed geometry directly, we observe that it
is often noisy and contains artifacts, which propagate noise to the trained
model. In contrast, we find that the disparity estimates from FoundationStereo
are cleaner and consequently result in a better performance on the zero-shot
generalization benchmarks. Our method highlights the potential for low-cost,
high-fidelity dataset creation and fast fine-tuning for deep stereo models.
Moreover, we also reveal that while the latest Gaussian Splatting based methods
have achieved superior performance on established benchmarks, their robustness
falls short in challenging in-the-wild settings warranting further exploration.



---

## ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics  Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Daniel Rho, Jun Myeong Choi, Biswadip Dey, Roni Sengupta | cs.CV | [PDF](http://arxiv.org/pdf/2506.05317v1){: .btn .btn-green } |

**Abstract**: Neural rendering has made significant strides in 3D reconstruction and novel
view synthesis. With the integration with physics, it opens up new
applications. The inverse problem of estimating physics from visual data,
however, still remains challenging, limiting its effectiveness for applications
like physically accurate digital twin creation in robotics and XR. Existing
methods that incorporate physics into neural rendering frameworks typically
require dense multi-view videos as input, making them impractical for scalable,
real-world use. When presented with sparse multi-view videos, the sequential
optimization strategy used by existing approaches introduces significant error
accumulation, e.g., poor initial 3D reconstruction leads to bad material
parameter estimation in subsequent stages. Instead of sequential optimization,
directly optimizing all parameters at the same time also fails due to the
highly non-convex and often non-differentiable nature of the problem. We
propose ProJo4D, a progressive joint optimization framework that gradually
increases the set of jointly optimized parameters guided by their sensitivity,
leading to fully joint optimization over geometry, appearance, physical state,
and material property. Evaluations on PAC-NeRF and Spring-Gaus datasets show
that ProJo4D outperforms prior work in 4D future state prediction, novel view
rendering of future state, and material parameter estimation, demonstrating its
effectiveness in physically grounded 4D scene understanding. For demos, please
visit the project webpage: https://daniel03c1.github.io/ProJo4D/



---

## Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Gaia Di Lorenzo, Federico Tombari, Marc Pollefeys, Daniel Barath | cs.CV | [PDF](http://arxiv.org/pdf/2506.04789v1){: .btn .btn-green } |

**Abstract**: Learning effective multi-modal 3D representations of objects is essential for
numerous applications, such as augmented reality and robotics. Existing methods
often rely on task-specific embeddings that are tailored either for semantic
understanding or geometric reconstruction. As a result, these embeddings
typically cannot be decoded into explicit geometry and simultaneously reused
across tasks. In this paper, we propose Object-X, a versatile multi-modal
object representation framework capable of encoding rich object embeddings
(e.g. images, point cloud, text) and decoding them back into detailed geometric
and visual reconstructions. Object-X operates by geometrically grounding the
captured modalities in a 3D voxel grid and learning an unstructured embedding
fusing the information from the voxels with the object attributes. The learned
embedding enables 3D Gaussian Splatting-based object reconstruction, while also
supporting a range of downstream tasks, including scene alignment, single-image
3D object reconstruction, and localization. Evaluations on two challenging
real-world datasets demonstrate that Object-X produces high-fidelity novel-view
synthesis comparable to standard 3D Gaussian Splatting, while significantly
improving geometric accuracy. Moreover, Object-X achieves competitive
performance with specialized methods in scene alignment and localization.
Critically, our object-centric descriptors require 3-4 orders of magnitude less
storage compared to traditional image- or point cloud-based approaches,
establishing Object-X as a scalable and highly practical solution for
multi-modal 3D scene representation.



---

## Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Alfred T. Christiansen, Andreas H. Højrup, Morten K. Stephansen, Md Ibtihaj A. Sakib, Taman S. Poojary, Filip Slezak, Morten S. Laursen, Thomas B. Moeslund, Joakim B. Haurum | cs.CV | [PDF](http://arxiv.org/pdf/2506.05009v1){: .btn .btn-green } |

**Abstract**: Training neural networks for tasks such as 3D point cloud semantic
segmentation demands extensive datasets, yet obtaining and annotating
real-world point clouds is costly and labor-intensive. This work aims to
introduce a novel pipeline for generating realistic synthetic data, by
leveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to
generate 3D assets of multiple different agricultural vehicles instead of using
generic models. These assets are placed in a simulated environment, where the
point clouds are generated using a simulated LiDAR. This is a flexible approach
that allows changing the LiDAR specifications without incurring additional
costs. We evaluated the impact of synthetic data on segmentation models such as
PointNet++, Point Transformer V3, and OACNN, by training and validating the
models only on synthetic data. Remarkably, the PTv3 model had an mIoU of
91.35\%, a noteworthy result given that the model had neither been trained nor
validated on any real data. Further studies even suggested that in certain
scenarios the models trained only on synthetically generated data performed
better than models trained on real-world data. Finally, experiments
demonstrated that the models can generalize across semantic classes, enabling
accurate predictions on mesh models they were never trained on.



---

## Synthetic Dataset Generation for Autonomous Mobile Robots Using 3D  Gaussian Splatting for Vision Training

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Aneesh Deogan, Wout Beks, Peter Teurlings, Koen de Vos, Mark van den Brand, Rene van de Molengraft | cs.RO | [PDF](http://arxiv.org/pdf/2506.05092v1){: .btn .btn-green } |

**Abstract**: Annotated datasets are critical for training neural networks for object
detection, yet their manual creation is time- and labour-intensive, subjective
to human error, and often limited in diversity. This challenge is particularly
pronounced in the domain of robotics, where diverse and dynamic scenarios
further complicate the creation of representative datasets. To address this, we
propose a novel method for automatically generating annotated synthetic data in
Unreal Engine. Our approach leverages photorealistic 3D Gaussian splats for
rapid synthetic data generation. We demonstrate that synthetic datasets can
achieve performance comparable to that of real-world datasets while
significantly reducing the time required to generate and annotate data.
Additionally, combining real-world and synthetic data significantly increases
object detection performance by leveraging the quality of real-world images
with the easier scalability of synthetic data. To our knowledge, this is the
first application of synthetic data for training object detection algorithms in
the highly dynamic and varied environment of robot soccer. Validation
experiments reveal that a detector trained on synthetic images performs on par
with one trained on manually annotated real-world images when tested on robot
soccer match scenarios. Our method offers a scalable and comprehensive
alternative to traditional dataset creation, eliminating the labour-intensive
error-prone manual annotation process. By generating datasets in a simulator
where all elements are intrinsically known, we ensure accurate annotations
while significantly reducing manual effort, which makes it particularly
valuable for robotics applications requiring diverse and scalable training
data.



---

## UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-05 | Jaehoon Choi, Dongki Jung, Christopher Maxey, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2506.05011v1){: .btn .btn-green } |

**Abstract**: Despite significant advancements in dynamic neural rendering, existing
methods fail to address the unique challenges posed by UAV-captured scenarios,
particularly those involving monocular camera setups, top-down perspective, and
multiple small, moving humans, which are not adequately represented in existing
datasets. In this work, we introduce UAV4D, a framework for enabling
photorealistic rendering for dynamic real-world scenes captured by UAVs.
Specifically, we address the challenge of reconstructing dynamic scenes with
multiple moving pedestrians from monocular video data without the need for
additional sensors. We use a combination of a 3D foundation model and a human
mesh reconstruction model to reconstruct both the scene background and humans.
We propose a novel approach to resolve the scene scale ambiguity and place both
humans and the scene in world coordinates by identifying human-scene contact
points. Additionally, we exploit the SMPL model and background mesh to
initialize Gaussian splats, enabling holistic scene rendering. We evaluated our
method on three complex UAV-captured datasets: VisDrone, Manipal-UAV, and
Okutama-Action, each with distinct characteristics and 10~50 humans. Our
results demonstrate the benefits of our approach over existing methods in novel
view synthesis, achieving a 1.5 dB PSNR improvement and superior visual
sharpness.



---

## FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Hengyu Liu, Yuehao Wang, Chenxin Li, Ruisi Cai, Kevin Wang, Wuyang Li, Pavlo Molchanov, Peihao Wang, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2506.04174v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3DGS) has enabled various applications in 3D scene
representation and novel view synthesis due to its efficient rendering
capabilities. However, 3DGS demands relatively significant GPU memory, limiting
its use on devices with restricted computational resources. Previous approaches
have focused on pruning less important Gaussians, effectively compressing 3DGS
but often requiring a fine-tuning stage and lacking adaptability for the
specific memory needs of different devices. In this work, we present an elastic
inference method for 3DGS. Given an input for the desired model size, our
method selects and transforms a subset of Gaussians, achieving substantial
rendering performance without additional fine-tuning. We introduce a tiny
learnable module that controls Gaussian selection based on the input
percentage, along with a transformation module that adjusts the selected
Gaussians to complement the performance of the reduced model. Comprehensive
experiments on ZipNeRF, MipNeRF and Tanks\&Temples scenes demonstrate the
effectiveness of our approach. Code is available at https://flexgs.github.io.

Comments:
- CVPR 2025; Project Page: https://flexgs.github.io

---

## JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Yang Xiao, Guoan Xu, Qiang Wu, Wenjing Jia | cs.CV | [PDF](http://arxiv.org/pdf/2506.03872v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge
with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view
reconstruction methods provide an efficient solution for real-time novel view
synthesis by leveraging geometric priors learned from large-scale multi-view
datasets and computing 3D Gaussian centers via back-projection. Despite
offering strong geometric cues, both feed-forward multi-view depth estimation
and flow-depth joint estimation face key limitations: the former suffers from
mislocation and artifact issues in low-texture or repetitive regions, while the
latter is prone to local noise and global inconsistency due to unreliable
matches when ground-truth flow supervision is unavailable. To overcome this, we
propose JointSplat, a unified framework that leverages the complementarity
between optical flow and depth via a novel probabilistic optimization
mechanism. Specifically, this pixel-level mechanism scales the information
fusion between depth and flow based on the matching probability of optical flow
during training. Building upon the above mechanism, we further propose a novel
multi-view depth-consistency loss to leverage the reliability of supervision
while suppressing misleading gradients in uncertain areas. Evaluated on
RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art
(SOTA) methods, demonstrating the effectiveness and robustness of our proposed
probabilistic joint flow-depth optimization approach for high-fidelity
sparse-view 3D reconstruction.



---

## SplArt: Articulation Estimation and Part-Level Reconstruction with 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Shengjie Lin, Jiading Fang, Muhammad Zubair Irshad, Vitor Campagnolo Guizilini, Rares Andrei Ambrus, Greg Shakhnarovich, Matthew R. Walter | cs.GR | [PDF](http://arxiv.org/pdf/2506.03594v1){: .btn .btn-green } |

**Abstract**: Reconstructing articulated objects prevalent in daily environments is crucial
for applications in augmented/virtual reality and robotics. However, existing
methods face scalability limitations (requiring 3D supervision or costly
annotations), robustness issues (being susceptible to local optima), and
rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a
self-supervised, category-agnostic framework that leverages 3D Gaussian
Splatting (3DGS) to reconstruct articulated objects and infer kinematics from
two sets of posed RGB images captured at different articulation states,
enabling real-time photorealistic rendering for novel viewpoints and
articulations. SplArt augments 3DGS with a differentiable mobility parameter
per Gaussian, achieving refined part segmentation. A multi-stage optimization
strategy is employed to progressively handle reconstruction, part segmentation,
and articulation estimation, significantly enhancing robustness and accuracy.
SplArt exploits geometric self-supervision, effectively addressing challenging
scenarios without requiring 3D annotations or category-specific priors.
Evaluations on established and newly proposed benchmarks, along with
applications to real-world scenarios using a handheld RGB camera, demonstrate
SplArt's state-of-the-art performance and real-world practicality. Code is
publicly available at https://github.com/ripl/splart.

Comments:
- https://github.com/ripl/splart

---

## Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot  Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Ben Moran, Mauro Comi, Steven Bohez, Tom Erez, Zhibin Li, Leonard Hasenclever | cs.RO | [PDF](http://arxiv.org/pdf/2506.04120v1){: .btn .btn-green } |

**Abstract**: Creating accurate, physical simulations directly from real-world robot motion
holds great value for safe, scalable, and affordable robot learning, yet
remains exceptionally challenging. Real robot data suffers from occlusions,
noisy camera poses, dynamic scene elements, which hinder the creation of
geometrically accurate and photorealistic digital twins of unseen objects. We
introduce a novel real-to-sim framework tackling all these challenges at once.
Our key insight is a hybrid scene representation merging the photorealistic
rendering of 3D Gaussian Splatting with explicit object meshes suitable for
physics simulation within a single representation. We propose an end-to-end
optimization pipeline that leverages differentiable rendering and
differentiable physics within MuJoCo to jointly refine all scene components -
from object geometry and appearance to robot poses and physical parameters -
directly from raw and imprecise robot trajectories. This unified optimization
allows us to simultaneously achieve high-fidelity object mesh reconstruction,
generate photorealistic novel views, and perform annotation-free robot pose
calibration. We demonstrate the effectiveness of our approach both in
simulation and on challenging real-world sequences using an ALOHA 2 bi-manual
manipulator, enabling more practical and robust real-to-simulation pipelines.



---

## Pseudo-Simulation for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta | cs.RO | [PDF](http://arxiv.org/pdf/2506.04218v1){: .btn .btn-green } |

**Abstract**: Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world evaluation is often challenging due to safety concerns
and a lack of reproducibility, whereas closed-loop simulation can face
insufficient realism or high computational costs. Open-loop evaluation, while
being efficient and data-driven, relies on metrics that generally overlook
compounding errors. In this paper, we propose pseudo-simulation, a novel
paradigm that addresses these limitations. Pseudo-simulation operates on real
datasets, similar to open-loop evaluation, but augments them with synthetic
observations generated prior to evaluation using 3D Gaussian Splatting. Our key
idea is to approximate potential future states the AV might encounter by
generating a diverse set of observations that vary in position, heading, and
speed. Our method then assigns a higher importance to synthetic observations
that best match the AV's likely behavior using a novel proximity-based
weighting scheme. This enables evaluating error recovery and the mitigation of
causal confusion, as in closed-loop benchmarks, without requiring sequential
interactive simulation. We show that pseudo-simulation is better correlated
with closed-loop simulations (R^2=0.8) than the best existing open-loop
approach (R^2=0.7). We also establish a public leaderboard for the community to
benchmark new methodologies with pseudo-simulation. Our code is available at
https://github.com/autonomousvision/navsim.



---

## Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Chengqi Li, Zhihao Shi, Yangdi Lu, Wenbo He, Xiangyu Xu | cs.CV | [PDF](http://arxiv.org/pdf/2506.03538v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction from in-the-wild images remains a challenging task due to
inconsistent lighting conditions and transient distractors. Existing methods
typically rely on heuristic strategies to handle the low-quality training data,
which often struggle to produce stable and consistent reconstructions,
frequently resulting in visual artifacts. In this work, we propose Asymmetric
Dual 3DGS, a novel framework that leverages the stochastic nature of these
artifacts: they tend to vary across different training runs due to minor
randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)
models in parallel, enforcing a consistency constraint that encourages
convergence on reliable scene geometry while suppressing inconsistent
artifacts. To prevent the two models from collapsing into similar failure modes
due to confirmation bias, we introduce a divergent masking strategy that
applies two complementary masks: a multi-cue adaptive mask and a
self-supervised soft mask, which leads to an asymmetric training process of the
two models, reducing shared error modes. In addition, to improve the efficiency
of model training, we introduce a lightweight variant called Dynamic EMA Proxy,
which replaces one of the two models with a dynamically updated Exponential
Moving Average (EMA) proxy, and employs an alternating masking strategy to
preserve divergence. Extensive experiments on challenging real-world datasets
demonstrate that our method consistently outperforms existing approaches while
achieving high efficiency. Codes and trained models will be released.



---

## Photoreal Scene Reconstruction from an Egocentric Device

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Zhaoyang Lv, Maurizio Monge, Ka Chen, Yufeng Zhu, Michael Goesele, Jakob Engel, Zhao Dong, Richard Newcombe | cs.CV | [PDF](http://arxiv.org/pdf/2506.04444v1){: .btn .btn-green } |

**Abstract**: In this paper, we investigate the challenges associated with using egocentric
devices to photorealistic reconstruct the scene in high dynamic range. Existing
methodologies typically assume using frame-rate 6DoF pose estimated from the
device's visual-inertial odometry system, which may neglect crucial details
necessary for pixel-accurate reconstruction. This study presents two
significant findings. Firstly, in contrast to mainstream work treating RGB
camera as global shutter frame-rate camera, we emphasize the importance of
employing visual-inertial bundle adjustment (VIBA) to calibrate the precise
timestamps and movement of the rolling shutter RGB sensing camera in a high
frequency trajectory format, which ensures an accurate calibration of the
physical properties of the rolling-shutter camera. Secondly, we incorporate a
physical image formation model based into Gaussian Splatting, which effectively
addresses the sensor characteristics, including the rolling-shutter effect of
RGB cameras and the dynamic ranges measured by sensors. Our proposed
formulation is applicable to the widely-used variants of Gaussian Splats
representation. We conduct a comprehensive evaluation of our pipeline using the
open-source Project Aria device under diverse indoor and outdoor lighting
conditions, and further validate it on a Meta Quest3 device. Across all
experiments, we observe a consistent visual enhancement of +1 dB in PSNR by
incorporating VIBA, with an additional +1 dB achieved through our proposed
image formation model. Our complete implementation, evaluation datasets, and
recording profile are available at
http://www.projectaria.com/photoreal-reconstruction/

Comments:
- Paper accepted to SIGGRAPH Conference Paper 2025

---

## HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-04 | Maksym Ivashechkin, Oscar Mendez, Richard Bowden | cs.CV | [PDF](http://arxiv.org/pdf/2506.04351v1){: .btn .btn-green } |

**Abstract**: 3D human generation is an important problem with a wide range of applications
in computer vision and graphics. Despite recent progress in generative AI such
as diffusion models or rendering methods like Neural Radiance Fields or
Gaussian Splatting, controlling the generation of accurate 3D humans from text
prompts remains an open challenge. Current methods struggle with fine detail,
accurate rendering of hands and faces, human realism, and controlability over
appearance. The lack of diversity, realism, and annotation in human image data
also remains a challenge, hindering the development of a foundational 3D human
model. We present a weakly supervised pipeline that tries to address these
challenges. In the first step, we generate a photorealistic human image dataset
with controllable attributes such as appearance, race, gender, etc using a
state-of-the-art image diffusion model. Next, we propose an efficient mapping
approach from image features to 3D point clouds using a transformer-based
architecture. Finally, we close the loop by training a point-cloud diffusion
model that is conditioned on the same text prompts used to generate the
original samples. We demonstrate orders-of-magnitude speed-ups in 3D human
generation compared to the state-of-the-art approaches, along with
significantly improved text-prompt alignment, realism, and rendering quality.
We will make the code and dataset available.



---

## Voyager: Real-Time Splatting City-Scale 3D Gaussians on Your Phone

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Zheng Liu, He Zhu, Xinyang Li, Yirun Wang, Yujiao Shi, Wei Li, Jingwen Leng, Minyi Guo, Yu Feng | cs.GR | [PDF](http://arxiv.org/pdf/2506.02774v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is an emerging technique for photorealistic 3D
scene rendering. However, rendering city-scale 3DGS scenes on mobile devices,
e.g., your smartphones, remains a significant challenge due to the limited
resources on mobile devices. A natural solution is to offload computation to
the cloud; however, naively streaming rendered frames from the cloud to the
client introduces high latency and requires bandwidth far beyond the capacity
of current wireless networks.
  In this paper, we propose an effective solution to enable city-scale 3DGS
rendering on mobile devices. Our key insight is that, under normal user motion,
the number of newly visible Gaussians per second remains roughly constant.
Leveraging this, we stream only the necessary Gaussians to the client.
Specifically, on the cloud side, we propose asynchronous level-of-detail search
to identify the necessary Gaussians for the client. On the client side, we
accelerate rendering via a lookup table-based rasterization. Combined with
holistic runtime optimizations, our system can deliver low-latency, city-scale
3DGS rendering on mobile devices. Compared to existing solutions, Voyager
achieves over 100$\times$ reduction on data transfer and up to 8.9$\times$
speedup while retaining comparable rendering quality.



---

## EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for  Real-World 3DGS Scenes in VR

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Zihao Ding, Cheng-Tse Lee, Mufeng Zhu, Tao Guan, Yuan-Chun Sun, Cheng-Hsin Hsu, Yao Liu | cs.MM | [PDF](http://arxiv.org/pdf/2506.02380v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is an emerging media representation that
reconstructs real-world 3D scenes in high fidelity, enabling
6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,
developing and evaluating 3DGS-enabled applications and optimizing their
rendering performance, require realistic user navigation data. Such data is
currently unavailable for photorealistic 3DGS reconstructions of real-world
scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available
6-DoF navigation dataset featuring traces from 46 participants exploring twelve
diverse, real-world 3DGS scenes. The dataset was collected at two sites, using
the Meta Quest Pro headsets, recording the head pose and eye gaze data for each
rendered frame during free world standing 6-DoF navigation. For each of the
twelve scenes, we performed careful scene initialization to correct for scene
tilt and scale, ensuring a perceptually-comfortable VR experience. We also
release our open-source SIBR viewer software fork with record-and-replay
functionalities and a suite of utility tools for data processing, conversion,
and visualization. The EyeNavGS dataset and its accompanying software tools
provide valuable resources for advancing research in 6-DoF viewport prediction,
adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The
EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.



---

## RobustSplat: Decoupling Densification and Dynamics for Transient-Free  3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Chuanyu Fu, Yuqi Zhang, Kunbin Yao, Guanying Chen, Yuan Xiong, Chuan Huang, Shuguang Cui, Xiaochun Cao | cs.CV | [PDF](http://arxiv.org/pdf/2506.02751v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has gained significant attention for its
real-time, photo-realistic rendering in novel-view synthesis and 3D modeling.
However, existing methods struggle with accurately modeling scenes affected by
transient objects, leading to artifacts in the rendered images. We identify
that the Gaussian densification process, while enhancing scene detail capture,
unintentionally contributes to these artifacts by growing additional Gaussians
that model transient disturbances. To address this, we propose RobustSplat, a
robust solution based on two critical designs. First, we introduce a delayed
Gaussian growth strategy that prioritizes optimizing static scene structure
before allowing Gaussian splitting/cloning, mitigating overfitting to transient
objects in early optimization. Second, we design a scale-cascaded mask
bootstrapping approach that first leverages lower-resolution feature similarity
supervision for reliable initial transient mask estimation, taking advantage of
its stronger semantic consistency and robustness to noise, and then progresses
to high-resolution supervision to achieve more precise mask prediction.
Extensive experiments on multiple challenging datasets show that our method
outperforms existing methods, clearly demonstrating the robustness and
effectiveness of our method. Our project page is
https://fcyycf.github.io/RobustSplat/.

Comments:
- Project page: https://fcyycf.github.io/RobustSplat/

---

## Large Processor Chip Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Kaiyan Chang, Mingzhi Chen, Yunji Chen, Zhirong Chen, Dongrui Fan, Junfeng Gong, Nan Guo, Yinhe Han, Qinfen Hao, Shuo Hou, Xuan Huang, Pengwei Jin, Changxin Ke, Cangyuan Li, Guangli Li, Huawei Li, Kuan Li, Naipeng Li, Shengwen Liang, Cheng Liu, Hongwei Liu, Jiahua Liu, Junliang Lv, Jianan Mu, Jin Qin, Bin Sun, Chenxi Wang, Duo Wang, Mingjun Wang, Ying Wang, Chenggang Wu, Peiyang Wu, Teng Wu, Xiao Xiao, Mengyao Xie, Chenwei Xiong, Ruiyuan Xu, Mingyu Yan, Xiaochun Ye, Kuai Yu, Rui Zhang, Shuoming Zhang, Jiacheng Zhao | cs.AR | [PDF](http://arxiv.org/pdf/2506.02929v1){: .btn .btn-green } |

**Abstract**: Computer System Architecture serves as a crucial bridge between software
applications and the underlying hardware, encompassing components like
compilers, CPUs, coprocessors, and RTL designs. Its development, from early
mainframes to modern domain-specific architectures, has been driven by rising
computational demands and advancements in semiconductor technology. However,
traditional paradigms in computer system architecture design are confronting
significant challenges, including a reliance on manual expertise, fragmented
optimization across software and hardware layers, and high costs associated
with exploring expansive design spaces. While automated methods leveraging
optimization algorithms and machine learning have improved efficiency, they
remain constrained by a single-stage focus, limited data availability, and a
lack of comprehensive human domain knowledge. The emergence of large language
models offers transformative opportunities for the design of computer system
architecture. By leveraging the capabilities of LLMs in areas such as code
generation, data analysis, and performance modeling, the traditional manual
design process can be transitioned to a machine-based automated design
approach. To harness this potential, we present the Large Processor Chip Model
(LPCM), an LLM-driven framework aimed at achieving end-to-end automated
computer architecture design. The LPCM is structured into three levels:
Human-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D
Gaussian Splatting as a representative workload and employs the concept of
software-hardware collaborative design to examine the implementation of the
LPCM at Level 1, demonstrating the effectiveness of the proposed approach.
Furthermore, this paper provides an in-depth discussion on the pathway to
implementing Level 2 and Level 3 of the LPCM, along with an analysis of the
existing challenges.



---

## LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Roman Titkov, Egor Zubkov, Dmitry Yudin, Jaafar Mahmoud, Malik Mohrat, Gennady Sidorov | cs.CV | [PDF](http://arxiv.org/pdf/2506.03073v1){: .btn .btn-green } |

**Abstract**: Modern Gaussian Splatting methods have proven highly effective for real-time
photorealistic rendering of 3D scenes. However, integrating semantic
information into this representation remains a significant challenge,
especially in maintaining real-time performance for SLAM (Simultaneous
Localization and Mapping) applications. In this work, we introduce LEG-SLAM --
a novel approach that fuses an optimized Gaussian Splatting implementation with
visual-language feature extraction using DINOv2 followed by a learnable feature
compressor based on Principal Component Analysis, while enabling an online
dense SLAM. Our method simultaneously generates high-quality photorealistic
images and semantically labeled scene maps, achieving real-time scene
reconstruction with more than 10 fps on the Replica dataset and 18 fps on
ScanNet. Experimental results show that our approach significantly outperforms
state-of-the-art methods in reconstruction speed while achieving competitive
rendering quality. The proposed system eliminates the need for prior data
preparation such as camera's ego motion or pre-computed static semantic maps.
With its potential applications in autonomous robotics, augmented reality, and
other interactive domains, LEG-SLAM represents a significant step forward in
real-time semantic 3D Gaussian-based SLAM. Project page:
https://titrom025.github.io/LEG-SLAM/



---

## Multi-Spectral Gaussian Splatting with Neural Color Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Lukas Meyer, Josef Grün, Maximilian Weiherer, Bernhard Egger, Marc Stamminger, Linus Franke | cs.GR | [PDF](http://arxiv.org/pdf/2506.03407v1){: .btn .btn-green } |

**Abstract**: We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)
framework that is able to generate multi-view consistent novel views from
images of multiple, independent cameras with different spectral domains. In
contrast to previous approaches, our method does not require cross-modal camera
calibration and is versatile enough to model a variety of different spectra,
including thermal and near-infra red, without any algorithmic changes.
  Unlike existing 3DGS-based frameworks that treat each modality separately (by
optimizing per-channel spherical harmonics) and therefore fail to exploit the
underlying spectral and spatial correlations, our method leverages a novel
neural color representation that encodes multi-spectral information into a
learned, compact, per-splat feature embedding. A shallow multi-layer perceptron
(MLP) then decodes this embedding to obtain spectral color values, enabling
joint learning of all bands within a unified representation.
  Our experiments show that this simple yet effective strategy is able to
improve multi-spectral rendering quality, while also leading to improved
per-spectra rendering quality over state-of-the-art methods. We demonstrate the
effectiveness of this new technique in agricultural applications to render
vegetation indices, such as normalized difference vegetation index (NDVI).



---

## Gen4D: Synthesizing Humans and Scenes in the Wild


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-03 | Jerrin Bright, Zhibo Wang, Yuhao Chen, Sirisha Rambhatla, John Zelek, David Clausi | cs.GR | [PDF](http://arxiv.org/pdf/2506.05397v1){: .btn .btn-green } |

**Abstract**: Lack of input data for in-the-wild activities often results in low
performance across various computer vision tasks. This challenge is
particularly pronounced in uncommon human-centric domains like sports, where
real-world data collection is complex and impractical. While synthetic datasets
offer a promising alternative, existing approaches typically suffer from
limited diversity in human appearance, motion, and scene composition due to
their reliance on rigid asset libraries and hand-crafted rendering pipelines.
To address this, we introduce Gen4D, a fully automated pipeline for generating
diverse and photorealistic 4D human animations. Gen4D integrates expert-driven
motion encoding, prompt-guided avatar generation using diffusion-based Gaussian
splatting, and human-aware background synthesis to produce highly varied and
lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale
synthetic dataset spanning three sports: baseball, icehockey, and soccer.
Together, Gen4D and SportPAL provide a scalable foundation for constructing
synthetic datasets tailored to in-the-wild human-centric vision tasks, with no
need for manual 3D modeling or scene design.

Comments:
- Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops

---

## RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis  and 3D Reconstruction of Autonomous Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Pou-Chun Kung, Skanda Harisha, Ram Vasudevan, Aline Eid, Katherine A. Skinner | cs.CV | [PDF](http://arxiv.org/pdf/2506.01379v1){: .btn .btn-green } |

**Abstract**: High-Fidelity 3D scene reconstruction plays a crucial role in autonomous
driving by enabling novel data generation from existing datasets. This allows
simulating safety-critical scenarios and augmenting training datasets without
incurring further data collection costs. While recent advances in radiance
fields have demonstrated promising results in 3D reconstruction and sensor data
synthesis using cameras and LiDAR, their potential for radar remains largely
unexplored. Radar is crucial for autonomous driving due to its robustness in
adverse weather conditions like rain, fog, and snow, where optical sensors
often struggle. Although the state-of-the-art radar-based neural representation
shows promise for 3D driving scene reconstruction, it performs poorly in
scenarios with significant radar noise, including receiver saturation and
multipath reflection. Moreover, it is limited to synthesizing preprocessed,
noise-excluded radar images, failing to address realistic radar data synthesis.
To address these limitations, this paper proposes RadarSplat, which integrates
Gaussian Splatting with novel radar noise modeling to enable realistic radar
data synthesis and enhanced 3D reconstruction. Compared to the
state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR
/ 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy),
demonstrating its effectiveness in generating high-fidelity radar data and
scene reconstruction. A project page is available at
https://umautobots.github.io/radarsplat.



---

## WoMAP: World Models For Embodied Open-Vocabulary Object Localization


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Tenny Yin, Zhiting Mei, Tao Sun, Lihan Zha, Emily Zhou, Jeremy Bao, Miyu Yamane, Ola Shorinwa, Anirudha Majumdar | cs.RO | [PDF](http://arxiv.org/pdf/2506.01600v1){: .btn .btn-green } |

**Abstract**: Language-instructed active object localization is a critical challenge for
robots, requiring efficient exploration of partially observable environments.
However, state-of-the-art approaches either struggle to generalize beyond
demonstration datasets (e.g., imitation learning methods) or fail to generate
physically grounded actions (e.g., VLMs). To address these limitations, we
introduce WoMAP (World Models for Active Perception): a recipe for training
open-vocabulary object localization policies that: (i) uses a Gaussian
Splatting-based real-to-sim-to-real pipeline for scalable data generation
without the need for expert demonstrations, (ii) distills dense rewards signals
from open-vocabulary object detectors, and (iii) leverages a latent world model
for dynamics and rewards prediction to ground high-level action proposals at
inference time. Rigorous simulation and hardware experiments demonstrate
WoMAP's superior performance in a broad range of zero-shot object localization
tasks, with more than 9x and 2x higher success rates compared to VLM and
diffusion policy baselines, respectively. Further, we show that WoMAP achieves
strong generalization and sim-to-real transfer on a TidyBot.



---

## WorldExplorer: Towards Generating Fully Navigable 3D Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Manuel-Andreas Schneider, Lukas Höllein, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2506.01799v1){: .btn .btn-green } |

**Abstract**: Generating 3D worlds from text is a highly anticipated goal in computer
vision. Existing works are limited by the degree of exploration they allow
inside of a scene, i.e., produce streched-out and noisy artifacts when moving
beyond central or panoramic perspectives. To this end, we propose
WorldExplorer, a novel method based on autoregressive video trajectory
generation, which builds fully navigable 3D scenes with consistent visual
quality across a wide range of viewpoints. We initialize our scenes by creating
multi-view consistent images corresponding to a 360 degree panorama. Then, we
expand it by leveraging video diffusion models in an iterative scene generation
pipeline. Concretely, we generate multiple videos along short, pre-defined
trajectories, that explore the scene in depth, including motion around objects.
Our novel scene memory conditions each video on the most relevant prior views,
while a collision-detection mechanism prevents degenerate results, like moving
into objects. Finally, we fuse all generated views into a unified 3D
representation via 3D Gaussian Splatting optimization. Compared to prior
approaches, WorldExplorer produces high-quality scenes that remain stable under
large camera motion, enabling for the first time realistic and unrestricted
exploration. We believe this marks a significant step toward generating
immersive and truly explorable virtual 3D environments.

Comments:
- project page: see https://the-world-explorer.github.io/, video: see
  https://youtu.be/c1lBnwJWNmE

---

## GSCodec Studio: A Modular Framework for Gaussian Splat Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-02 | Sicheng Li, Chengzhen Wu, Hao Li, Xiang Gao, Yiyi Liao, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2506.01822v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting and its extension to 4D dynamic scenes enable
photorealistic, real-time rendering from real-world captures, positioning
Gaussian Splats (GS) as a promising format for next-generation immersive media.
However, their high storage requirements pose significant challenges for
practical use in sharing, transmission, and storage. Despite various studies
exploring GS compression from different perspectives, these efforts remain
scattered across separate repositories, complicating benchmarking and the
integration of best practices. To address this gap, we present GSCodec Studio,
a unified and modular framework for GS reconstruction, compression, and
rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction
methods and GS compression techniques as modular components, facilitating
flexible combinations and comprehensive comparisons. By integrating best
practices from community research and our own explorations, GSCodec Studio
supports the development of compact representation and compression solutions
for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec,
achieving competitive rate-distortion performance in static and dynamic GS
compression. The code for our framework is publicly available at
https://github.com/JasonLSC/GSCodec_Studio , to advance the research on
Gaussian Splats compression.

Comments:
- Repository of the project: https://github.com/JasonLSC/GSCodec_Studio

---

## Globally Consistent RGB-D SLAM with 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-01 | Xingguang Zhong, Yue Pan, Liren Jin, Marija Popović, Jens Behley, Cyrill Stachniss | cs.RO | [PDF](http://arxiv.org/pdf/2506.00970v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian splatting-based RGB-D SLAM displays remarkable
performance of high-fidelity 3D reconstruction. However, the lack of depth
rendering consistency and efficient loop closure limits the quality of its
geometric reconstructions and its ability to perform globally consistent
mapping online. In this paper, we present 2DGS-SLAM, an RGB-D SLAM system using
2D Gaussian splatting as the map representation. By leveraging the
depth-consistent rendering property of the 2D variant, we propose an accurate
camera pose optimization method and achieve geometrically accurate 3D
reconstruction. In addition, we implement efficient loop detection and camera
relocalization by leveraging MASt3R, a 3D foundation model, and achieve
efficient map updates by maintaining a local active map. Experiments show that
our 2DGS-SLAM approach achieves superior tracking accuracy, higher surface
reconstruction quality, and more consistent global map reconstruction compared
to existing rendering-based SLAM methods, while maintaining high-fidelity image
rendering and improved computational efficiency.

Comments:
- 18 pages

---

## CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-06-01 | Fengze Li, Yangle Liu, Jieming Ma, Hai-Ning Liang, Yaochun Shen, Huangxiang Li, Zhijing Wu | cs.CV | [PDF](http://arxiv.org/pdf/2506.01109v1){: .btn .btn-green } |

**Abstract**: Accurate fruit counting in real-world agricultural environments is a
longstanding challenge due to visual occlusions, semantic ambiguity, and the
high computational demands of 3D reconstruction. Existing methods based on
neural radiance fields suffer from low inference speed, limited generalization,
and lack support for open-set semantic control. This paper presents
FruitLangGS, a real-time 3D fruit counting framework that addresses these
limitations through spatial reconstruction, semantic embedding, and
language-guided instance estimation. FruitLangGS first reconstructs
orchard-scale scenes using an adaptive Gaussian splatting pipeline with
radius-aware pruning and tile-based rasterization for efficient rendering. To
enable semantic control, each Gaussian encodes a compressed CLIP-aligned
language embedding, forming a compact and queryable 3D representation. At
inference time, prompt-based semantic filtering is applied directly in 3D
space, without relying on image-space segmentation or view-level fusion. The
selected Gaussians are then converted into dense point clouds via
distribution-aware sampling and clustered to estimate fruit counts.
Experimental results on real orchard data demonstrate that FruitLangGS achieves
higher rendering speed, semantic flexibility, and counting accuracy compared to
prior approaches, offering a new perspective for language-driven, real-time
neural rendering across open-world scenarios.


