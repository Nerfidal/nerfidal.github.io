---
layout: default
title: 2021
nav_order: 2
---
<!---metadata--->

### InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-31 by Mijeong Kim | Seonguk Seo | Bohyung Han in cs.CV, [PDF](http://arxiv.org/pdf/2112.15399v2)

**Abstract**: We present an information-theoretic regularization technique for few-shot
novel view synthesis based on neural implicit representation. The proposed
approach minimizes potential reconstruction inconsistency that happens due to
insufficient viewpoints by imposing the entropy constraint of the density in
each ray. In addition, to alleviate the potential degenerate issue when all
training images are acquired from almost redundant viewpoints, we further
incorporate the spatially smoothness constraint into the estimated images by
restricting information gains from a pair of rays with slightly different
viewpoints. The main idea of our algorithm is to make reconstructed scenes
compact along individual rays and consistent across rays in the neighborhood.
The proposed regularizers can be plugged into most of existing neural volume
rendering techniques based on NeRF in a straightforward way. Despite its
simplicity, we achieve consistently improved performance compared to existing
neural view synthesis methods by large margins on multiple standard benchmarks.

Comments:
- CVPR 2022, Website: http://cv.snu.ac.kr/research/InfoNeRF

---

### Learning Implicit Body Representations from Double Diffusion Based  Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-23 by Guangming Yao | Hongzhi Wu | Yi Yuan | Lincheng Li | Kun Zhou | Xin Yu in cs.CV, [PDF](http://arxiv.org/pdf/2112.12390v2)

**Abstract**: In this paper, we present a novel double diffusion based neural radiance
field, dubbed DD-NeRF, to reconstruct human body geometry and render the human
body appearance in novel views from a sparse set of images. We first propose a
double diffusion mechanism to achieve expressive representations of input
images by fully exploiting human body priors and image appearance details at
two levels. At the coarse level, we first model the coarse human body poses and
shapes via an unclothed 3D deformable vertex model as guidance. At the fine
level, we present a multi-view sampling network to capture subtle geometric
deformations and image detailed appearances, such as clothing and hair, from
multiple input views. Considering the sparsity of the two level features, we
diffuse them into feature volumes in the canonical space to construct neural
radiance fields. Then, we present a signed distance function (SDF) regression
network to construct body surfaces from the diffused features. Thanks to our
double diffused representations, our method can even synthesize novel views of
unseen subjects. Experiments on various datasets demonstrate that our approach
outperforms the state-of-the-art in both geometric reconstruction and novel
view synthesis.

Comments:
- 6 pages, 5 figures

---

### BANMo: Building Animatable 3D Neural Models from Many Casual Videos

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-23 by Gengshan Yang | Minh Vo | Natalia Neverova | Deva Ramanan | Andrea Vedaldi | Hanbyul Joo in cs.CV, [PDF](http://arxiv.org/pdf/2112.12761v3)

**Abstract**: Prior work for articulated 3D shape reconstruction often relies on
specialized sensors (e.g., synchronized multi-camera systems), or pre-built 3D
deformable models (e.g., SMAL or SMPL). Such methods are not able to scale to
diverse sets of objects in the wild. We present BANMo, a method that requires
neither a specialized sensor nor a pre-defined template shape. BANMo builds
high-fidelity, articulated 3D models (including shape and animatable skinning
weights) from many monocular casual videos in a differentiable rendering
framework. While the use of many videos provides more coverage of camera views
and object articulations, they introduce significant challenges in establishing
correspondence across scenes with different backgrounds, illumination
conditions, etc. Our key insight is to merge three schools of thought; (1)
classic deformable shape models that make use of articulated bones and blend
skinning, (2) volumetric neural radiance fields (NeRFs) that are amenable to
gradient-based optimization, and (3) canonical embeddings that generate
correspondences between pixels and an articulated model. We introduce neural
blend skinning models that allow for differentiable and invertible articulated
deformations. When combined with canonical embeddings, such models allow us to
establish dense correspondences across videos that can be self-supervised with
cycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity
3D reconstructions than prior works for humans and animals, with the ability to
render realistic images from novel viewpoints and poses. Project webpage:
banmo-www.github.io .

Comments:
- CVPR 2022 camera-ready version (last update: May 2022)

---

### 3D-aware Image Synthesis via Learning Structural and Textural  Representations

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-20 by Yinghao Xu | Sida Peng | Ceyuan Yang | Yujun Shen | Bolei Zhou in cs.CV, [PDF](http://arxiv.org/pdf/2112.10759v2)

**Abstract**: Making generative models 3D-aware bridges the 2D image space and the 3D
physical world yet remains challenging. Recent attempts equip a Generative
Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D
coordinates to pixel values, as a 3D prior. However, the implicit function in
NeRF has a very local receptive field, making the generator hard to become
aware of the global structure. Meanwhile, NeRF is built on volume rendering
which can be too costly to produce high-resolution results, increasing the
optimization difficulty. To alleviate these two problems, we propose a novel
framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis,
through explicitly learning a structural representation and a textural
representation. We first learn a feature volume to represent the underlying
structure, which is then converted to a feature field using a NeRF-like model.
The feature field is further accumulated into a 2D feature map as the textural
representation, followed by a neural renderer for appearance synthesis. Such a
design enables independent control of the shape and the appearance. Extensive
experiments on a wide range of datasets show that our approach achieves
sufficiently higher image quality and better 3D control than the previous
methods.

Comments:
- CVPR 2022 camera-ready, Project page:
  https://genforce.github.io/volumegan/

---

### Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual  Fly-Throughs

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-20 by Haithem Turki | Deva Ramanan | Mahadev Satyanarayanan in cs.CV, [PDF](http://arxiv.org/pdf/2112.10703v2)

**Abstract**: We use neural radiance fields (NeRFs) to build interactive 3D environments
from large-scale visual captures spanning buildings or even multiple city
blocks collected primarily from drones. In contrast to single object scenes (on
which NeRFs are traditionally evaluated), our scale poses multiple challenges
including (1) the need to model thousands of images with varying lighting
conditions, each of which capture only a small subset of the scene, (2)
prohibitively large model capacities that make it infeasible to train on a
single GPU, and (3) significant challenges for fast rendering that would enable
interactive fly-throughs.
  To address these challenges, we begin by analyzing visibility statistics for
large-scale scenes, motivating a sparse network structure where parameters are
specialized to different regions of the scene. We introduce a simple geometric
clustering algorithm for data parallelism that partitions training images (or
rather pixels) into different NeRF submodules that can be trained in parallel.
  We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as
well as against our own drone footage, improving training speed by 3x and PSNR
by 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and
introduce a novel method that exploits temporal coherence. Our technique
achieves a 40x speedup over conventional NeRF rendering while remaining within
0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.

Comments:
- CVPR 2022 Project page: https://meganerf.cmusatyalab.org GitHub:
  https://github.com/cmusatyalab/mega-nerf

---

### HVTR: Hybrid Volumetric-Textural Rendering for Human Avatars

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-19 by Tao Hu | Tao Yu | Zerong Zheng | He Zhang | Yebin Liu | Matthias Zwicker in cs.CV, [PDF](http://arxiv.org/pdf/2112.10203v2)

**Abstract**: We propose a novel neural rendering pipeline, Hybrid Volumetric-Textural
Rendering (HVTR), which synthesizes virtual human avatars from arbitrary poses
efficiently and at high quality. First, we learn to encode articulated human
motions on a dense UV manifold of the human body surface. To handle complicated
motions (e.g., self-occlusions), we then leverage the encoded information on
the UV manifold to construct a 3D volumetric representation based on a dynamic
pose-conditioned neural radiance field. While this allows us to represent 3D
geometry with changing topology, volumetric rendering is computationally heavy.
Hence we employ only a rough volumetric representation using a pose-conditioned
downsampled neural radiance field (PD-NeRF), which we can render efficiently at
low resolutions. In addition, we learn 2D textural features that are fused with
rendered volumetric features in image space. The key advantage of our approach
is that we can then convert the fused features into a high-resolution,
high-quality avatar by a fast GAN-based textural renderer. We demonstrate that
hybrid rendering enables HVTR to handle complicated motions, render
high-quality avatars under user-controlled poses/shapes and even loose
clothing, and most importantly, be efficient at inference time. Our
experimental results also demonstrate state-of-the-art quantitative results.

Comments:
- Accepted to 3DV 2022. See more results at
  https://www.cs.umd.edu/~taohu/hvtr/ Demo:
  https://www.youtube.com/watch?v=LE0-YpbLlkY

---

### Solving Inverse Problems with NerfGANs

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-16 by Giannis Daras | Wen-Sheng Chu | Abhishek Kumar | Dmitry Lagun | Alexandros G. Dimakis in cs.CV, [PDF](http://arxiv.org/pdf/2112.09061v1)

**Abstract**: We introduce a novel framework for solving inverse problems using NeRF-style
generative models. We are interested in the problem of 3-D scene reconstruction
given a single 2-D image and known camera parameters. We show that naively
optimizing the latent space leads to artifacts and poor novel view rendering.
We attribute this problem to volume obstructions that are clear in the 3-D
geometry and become visible in the renderings of novel views. We propose a
novel radiance field regularization method to obtain better 3-D surfaces and
improved novel views given single view observations. Our method naturally
extends to general inverse problems including inpainting where one observes
only partially a single view. We experimentally evaluate our method, achieving
visual improvements and performance boosts over the baselines in a wide range
of tasks. Our method achieves $30-40\%$ MSE reduction and $15-25\%$ reduction
in LPIPS loss compared to the previous state of the art.

Comments:
- 16 pages, 18 figures

---

### GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-16 by Yu Deng | Jiaolong Yang | Jianfeng Xiang | Xin Tong in cs.CV, [PDF](http://arxiv.org/pdf/2112.08867v3)

**Abstract**: 3D-aware image generative modeling aims to generate 3D-consistent images with
explicitly controllable camera poses. Recent works have shown promising results
by training neural radiance field (NeRF) generators on unstructured 2D images,
but still can not generate highly-realistic images with fine details. A
critical reason is that the high memory and computation cost of volumetric
representation learning greatly restricts the number of point samples for
radiance integration during training. Deficient sampling not only limits the
expressive power of the generator to handle fine details but also impedes
effective GAN training due to the noise caused by unstable Monte Carlo
sampling. We propose a novel approach that regulates point sampling and
radiance field learning on 2D manifolds, embodied as a set of learned implicit
surfaces in the 3D volume. For each viewing ray, we calculate ray-surface
intersections and accumulate their radiance generated by the network. By
training and rendering such radiance manifolds, our generator can produce high
quality images with realistic fine details and strong visual 3D consistency.

Comments:
- CVPR2022 Oral. Project page: https://yudeng.github.io/GRAM/

---

### HeadNeRF: A Real-time NeRF-based Parametric Head Model

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-10 by Yang Hong | Bo Peng | Haiyao Xiao | Ligang Liu | Juyong Zhang in cs.CV, [PDF](http://arxiv.org/pdf/2112.05637v3)

**Abstract**: In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model
that integrates the neural radiance field to the parametric representation of
the human head. It can render high fidelity head images in real-time on modern
GPUs, and supports directly controlling the generated images' rendering pose
and various semantic attributes. Different from existing related parametric
models, we use the neural radiance fields as a novel 3D proxy instead of the
traditional 3D textured mesh, which makes that HeadNeRF is able to generate
high fidelity images. However, the computationally expensive rendering process
of the original NeRF hinders the construction of the parametric NeRF model. To
address this issue, we adopt the strategy of integrating 2D neural rendering to
the rendering process of NeRF and design novel loss terms. As a result, the
rendering speed of HeadNeRF can be significantly accelerated, and the rendering
time of one frame is reduced from 5s to 25ms. The well designed loss terms also
improve the rendering accuracy, and the fine-level details of the human head,
such as the gaps between teeth, wrinkles, and beards, can be represented and
synthesized by HeadNeRF. Extensive experimental results and several
applications demonstrate its effectiveness. The trained parametric model is
available at https://github.com/CrisHY1995/headnerf.

Comments:
- Accepted by CVPR2022. Project page:
  https://crishy1995.github.io/HeadNeRF-Project/

---

### BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale  Scene Rendering

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-10 by Yuanbo Xiangli | Linning Xu | Xingang Pan | Nanxuan Zhao | Anyi Rao | Christian Theobalt | Bo Dai | Dahua Lin in cs.CV, [PDF](http://arxiv.org/pdf/2112.05504v4)

**Abstract**: Neural radiance fields (NeRF) has achieved outstanding performance in
modeling 3D objects and controlled scenes, usually under a single scale. In
this work, we focus on multi-scale cases where large changes in imagery are
observed at drastically different scales. This scenario vastly exists in
real-world 3D environments, such as city scenes, with views ranging from
satellite level that captures the overview of a city, to ground level imagery
showing complex details of an architecture; and can also be commonly identified
in landscape and delicate minecraft 3D models. The wide span of viewing
positions within these scenes yields multi-scale renderings with very different
levels of detail, which poses great challenges to neural radiance field and
biases it towards compromised results. To address these issues, we introduce
BungeeNeRF, a progressive neural radiance field that achieves level-of-detail
rendering across drastically varied scales. Starting from fitting distant views
with a shallow base block, as training progresses, new blocks are appended to
accommodate the emerging details in the increasingly closer views. The strategy
progressively activates high-frequency channels in NeRF's positional encoding
inputs and successively unfolds more complex details as the training proceeds.
We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale
scenes with drastically varying views on multiple data sources (city models,
synthetic, and drone captured data) and its support for high-quality rendering
in different levels of detail.

Comments:
- Accepted to ECCV22; Previous version: CityNeRF: Building NeRF at City
  Scale; Project page can be found in https://city-super.github.io/citynerf

---

### Plenoxels: Radiance Fields without Neural Networks



Published 2021-12-09 by Alex Yu | Sara Fridovich-Keil | Matthew Tancik | Qinhong Chen | Benjamin Recht | Angjoo Kanazawa in cs.CV, [PDF](http://arxiv.org/pdf/2112.05131v1)

**Abstract**: We introduce Plenoxels (plenoptic voxels), a system for photorealistic view
synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical
harmonics. This representation can be optimized from calibrated images via
gradient methods and regularization without any neural components. On standard,
benchmark tasks, Plenoxels are optimized two orders of magnitude faster than
Neural Radiance Fields with no loss in visual quality.

Comments:
- For video and code, please see https://alexyu.net/plenoxels

---

### Deep Visual Constraints: Neural Implicit Models for Manipulation  Planning from Visual Input

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-09 by Jung-Su Ha | Danny Driess | Marc Toussaint in cs.RO, [PDF](http://arxiv.org/pdf/2112.04812v3)

**Abstract**: Manipulation planning is the problem of finding a sequence of robot
configurations that involves interactions with objects in the scene, e.g.,
grasping and placing an object, or more general tool-use. To achieve such
interactions, traditional approaches require hand-engineering of object
representations and interaction constraints, which easily becomes tedious when
complex objects/interactions are considered. Inspired by recent advances in 3D
modeling, e.g. NeRF, we propose a method to represent objects as continuous
functions upon which constraint features are defined and jointly trained. In
particular, the proposed pixel-aligned representation is directly inferred from
images with known camera geometry and naturally acts as a perception component
in the whole manipulation pipeline, thereby enabling long-horizon planning only
from visual input. Project page:
https://sites.google.com/view/deep-visual-constraints

Comments:
- IEEE Robotics and Automation Letters (RA-L) 2022

---

### CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-09 by Can Wang | Menglei Chai | Mingming He | Dongdong Chen | Jing Liao in cs.CV, [PDF](http://arxiv.org/pdf/2112.05139v3)

**Abstract**: We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural
radiance fields (NeRF). By leveraging the joint language-image embedding space
of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose
a unified framework that allows manipulating NeRF in a user-friendly way, using
either a short text prompt or an exemplar image. Specifically, to combine the
novel view synthesis capability of NeRF and the controllable manipulation
ability of latent representations from generative models, we introduce a
disentangled conditional NeRF architecture that allows individual control over
both shape and appearance. This is achieved by performing the shape
conditioning via applying a learned deformation field to the positional
encoding and deferring color conditioning to the volumetric rendering stage. To
bridge this disentangled latent representation to the CLIP embedding, we design
two code mappers that take a CLIP embedding as input and update the latent
codes to reflect the targeted editing. The mappers are trained with a
CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we
propose an inverse optimization method that accurately projects an input image
to the latent codes for manipulation to enable editing on real images. We
evaluate our approach by extensive experiments on a variety of text prompts and
exemplar images and also provide an intuitive interface for interactive
editing. Our implementation is available at
https://cassiepython.github.io/clipnerf/

Comments:
- To Appear at CVPR 2022

---

### NeRF for Outdoor Scene Relighting

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-09 by Viktor Rudnev | Mohamed Elgharib | William Smith | Lingjie Liu | Vladislav Golyanik | Christian Theobalt in cs.CV, [PDF](http://arxiv.org/pdf/2112.05140v2)

**Abstract**: Photorealistic editing of outdoor scenes from photographs requires a profound
understanding of the image formation process and an accurate estimation of the
scene geometry, reflectance and illumination. A delicate manipulation of the
lighting can then be performed while keeping the scene albedo and geometry
unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene
relighting based on neural radiance fields. In contrast to the prior art, our
technique allows simultaneous editing of both scene illumination and camera
viewpoint using only a collection of outdoor photos shot in uncontrolled
settings. Moreover, it enables direct control over the scene illumination, as
defined through a spherical harmonics model. For evaluation, we collect a new
benchmark dataset of several outdoor sites photographed from multiple
viewpoints and at different times. For each time, a 360 degree environment map
is captured together with a colour-calibration chequerboard to allow accurate
numerical evaluations on real data against ground truth. Comparisons against
SoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at
higher quality and with realistic self-shadowing reproduction. Our method and
the dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.

Comments:
- 22 pages, 10 figures, 2 tables; ECCV 2022; project web page:
  https://4dqv.mpi-inf.mpg.de/NeRF-OSR/

---

### Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural  Human Rendering

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-08 by Mingfei Chen | Jianfeng Zhang | Xiangyu Xu | Lijuan Liu | Yujun Cai | Jiashi Feng | Shuicheng Yan in cs.CV, [PDF](http://arxiv.org/pdf/2112.04312v3)

**Abstract**: In this work we develop a generalizable and efficient Neural Radiance Field
(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under
settings with sparse camera views. Though existing NeRF-based methods can
synthesize rather realistic details for human body, they tend to produce poor
results when the input has self-occlusion, especially for unseen humans under
sparse views. Moreover, these methods often require a large number of sampling
points for rendering, which leads to low efficiency and limits their real-world
applicability. To address these challenges, we propose a Geometry-guided
Progressive NeRF (GP-NeRF). In particular, to better tackle self-occlusion, we
devise a geometry-guided multi-view feature integration approach that utilizes
the estimated geometry prior to integrate the incomplete information from input
views and construct a complete geometry volume for the target human body.
Meanwhile, for achieving higher rendering efficiency, we introduce a
progressive rendering pipeline through geometry guidance, which leverages the
geometric feature volume and the predicted density values to progressively
reduce the number of sampling points and speed up the rendering process.
Experiments on the ZJU-MoCap and THUman datasets show that our method
outperforms the state-of-the-arts significantly across multiple generalization
settings, while the time cost is reduced > 70% via applying our efficient
progressive rendering pipeline.

---

### Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance  Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-07 by Dor Verbin | Peter Hedman | Ben Mildenhall | Todd Zickler | Jonathan T. Barron | Pratul P. Srinivasan in cs.CV, [PDF](http://arxiv.org/pdf/2112.03907v1)

**Abstract**: Neural Radiance Fields (NeRF) is a popular view synthesis technique that
represents a scene as a continuous volumetric function, parameterized by
multilayer perceptrons that provide the volume density and view-dependent
emitted radiance at each location. While NeRF-based techniques excel at
representing fine geometric structures with smoothly varying view-dependent
appearance, they often fail to accurately capture and reproduce the appearance
of glossy surfaces. We address this limitation by introducing Ref-NeRF, which
replaces NeRF's parameterization of view-dependent outgoing radiance with a
representation of reflected radiance and structures this function using a
collection of spatially-varying scene properties. We show that together with a
regularizer on normal vectors, our model significantly improves the realism and
accuracy of specular reflections. Furthermore, we show that our model's
internal representation of outgoing radiance is interpretable and useful for
scene editing.

Comments:
- Project page: https://dorverbin.github.io/refnerf/

---

### CG-NeRF: Conditional Generative Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-07 by Kyungmin Jo | Gyumin Shim | Sanghun Jung | Soyoung Yang | Jaegul Choo in cs.CV, [PDF](http://arxiv.org/pdf/2112.03517v1)

**Abstract**: While recent NeRF-based generative models achieve the generation of diverse
3D-aware images, these approaches have limitations when generating images that
contain user-specified characteristics. In this paper, we propose a novel
model, referred to as the conditional generative neural radiance fields
(CG-NeRF), which can generate multi-view images reflecting extra input
conditions such as images or texts. While preserving the common characteristics
of a given input condition, the proposed model generates diverse images in fine
detail. We propose: 1) a novel unified architecture which disentangles the
shape and appearance from a condition given in various forms and 2) the
pose-consistent diversity loss for generating multimodal outputs while
maintaining consistency of the view. Experimental results show that the
proposed method maintains consistent image quality on various condition types
and achieves superior fidelity and diversity compared to existing NeRF-based
generative models.

---

### Dense Depth Priors for Neural Radiance Fields from Sparse Input Views

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-06 by Barbara Roessle | Jonathan T. Barron | Ben Mildenhall | Pratul P. Srinivasan | Matthias Nießner in cs.CV, [PDF](http://arxiv.org/pdf/2112.03288v2)

**Abstract**: Neural radiance fields (NeRF) encode a scene into a neural representation
that enables photo-realistic rendering of novel views. However, a successful
reconstruction from RGB images requires a large number of input views taken
under static conditions - typically up to a few hundred images for room-size
scenes. Our method aims to synthesize novel views of whole rooms from an order
of magnitude fewer images. To this end, we leverage dense depth priors in order
to constrain the NeRF optimization. First, we take advantage of the sparse
depth data that is freely available from the structure from motion (SfM)
preprocessing step used to estimate camera poses. Second, we use depth
completion to convert these sparse points into dense depth maps and uncertainty
estimates, which are used to guide NeRF optimization. Our method enables
data-efficient novel view synthesis on challenging indoor scenes, using as few
as 18 images for an entire scene.

Comments:
- CVPR 2022, project page:
  https://barbararoessle.github.io/dense_depth_priors_nerf/ , video:
  https://youtu.be/zzkvvdcvksc

---

### HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-06 by Fuqiang Zhao | Wei Yang | Jiakai Zhang | Pei Lin | Yingliang Zhang | Jingyi Yu | Lan Xu in cs.CV, [PDF](http://arxiv.org/pdf/2112.02789v3)

**Abstract**: Recent neural human representations can produce high-quality multi-view
rendering but require using dense multi-view inputs and costly training. They
are hence largely limited to static models as training each frame is
infeasible. We present HumanNeRF - a generalizable neural representation - for
high-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet
assists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated
pixel-alignment feature across multi-view inputs along with a pose embedded
non-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can
already produce reasonable rendering on sparse video inputs of unseen subjects
and camera settings. To further improve the rendering quality, we augment our
solution with an appearance blending module for combining the benefits of both
neural volumetric rendering and neural texture blending. Extensive experiments
on various multi-view dynamic human datasets demonstrate the generalizability
and effectiveness of our approach in synthesizing photo-realistic free-view
humans under challenging motions and with very sparse camera view inputs.

Comments:
- https://zhaofuq.github.io/humannerf/

---

### MoFaNeRF: Morphable Facial Neural Radiance Field

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-04 by Yiyu Zhuang | Hao Zhu | Xusen Sun | Xun Cao in cs.CV, [PDF](http://arxiv.org/pdf/2112.02308v2)

**Abstract**: We propose a parametric model that maps free-view images into a vector space
of coded facial shape, expression and appearance with a neural radiance field,
namely Morphable Facial NeRF. Specifically, MoFaNeRF takes the coded facial
shape, expression and appearance along with space coordinate and view direction
as input to an MLP, and outputs the radiance of the space point for
photo-realistic image synthesis. Compared with conventional 3D morphable models
(3DMM), MoFaNeRF shows superiority in directly synthesizing photo-realistic
facial details even for eyes, mouths, and beards. Also, continuous face
morphing can be easily achieved by interpolating the input shape, expression
and appearance codes. By introducing identity-specific modulation and texture
encoder, our model synthesizes accurate photometric details and shows strong
representation ability. Our model shows strong ability on multiple applications
including image-based fitting, random generation, face rigging, face editing,
and novel view synthesis. Experiments show that our method achieves higher
representation ability than previous parametric models, and achieves
competitive performance in several applications. To the best of our knowledge,
our work is the first facial parametric model built upon a neural radiance
field that can be used in fitting, generation and manipulation. The code and
data is available at https://github.com/zhuhao-nju/mofanerf.

Comments:
- accepted to ECCV2022; code available at
  http://github.com/zhuhao-nju/mofanerf

---

### NeRF-SR: High-Quality Neural Radiance Fields using Supersampling

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-03 by Chen Wang | Xian Wu | Yuan-Chen Guo | Song-Hai Zhang | Yu-Wing Tai | Shi-Min Hu in cs.CV, [PDF](http://arxiv.org/pdf/2112.01759v3)

**Abstract**: We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis
with mostly low-resolution (LR) inputs. Our method is built upon Neural
Radiance Fields (NeRF) that predicts per-point density and color with a
multi-layer perceptron. While producing images at arbitrary scales, NeRF
struggles with resolutions that go beyond observed images. Our key insight is
that NeRF benefits from 3D consistency, which means an observed pixel absorbs
information from nearby views. We first exploit it by a supersampling strategy
that shoots multiple rays at each image pixel, which further enforces
multi-view constraint at a sub-pixel level. Then, we show that NeRF-SR can
further boost the performance of supersampling by a refinement network that
leverages the estimated depth at hand to hallucinate details from related
patches on only one HR reference image. Experiment results demonstrate that
NeRF-SR generates high-quality results for novel view synthesis at HR on both
synthetic and real-world datasets without any external information.

Comments:
- Accepted to MM 2022. Project Page:
  https://cwchenwang.github.io/NeRF-SR

---

### CoNeRF: Controllable Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-03 by Kacper Kania | Kwang Moo Yi | Marek Kowalski | Tomasz Trzciński | Andrea Tagliasacchi in cs.CV, [PDF](http://arxiv.org/pdf/2112.01983v2)

**Abstract**: We extend neural 3D representations to allow for intuitive and interpretable
user control beyond novel view rendering (i.e. camera control). We allow the
user to annotate which part of the scene one wishes to control with just a
small number of mask annotations in the training images. Our key idea is to
treat the attributes as latent variables that are regressed by the neural
network given the scene encoding. This leads to a few-shot learning framework,
where attributes are discovered automatically by the framework, when
annotations are not provided. We apply our method to various scenes with
different types of controllable attributes (e.g. expression control on human
faces, or state control in movement of inanimate objects). Overall, we
demonstrate, to the best of our knowledge, for the first time novel view and
novel attribute re-rendering of scenes from a single video.

Comments:
- Project page: https://conerf.github.io/

---

### Efficient Neural Radiance Fields for Interactive Free-viewpoint Video

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-02 by Haotong Lin | Sida Peng | Zhen Xu | Yunzhi Yan | Qing Shuai | Hujun Bao | Xiaowei Zhou in cs.CV, [PDF](http://arxiv.org/pdf/2112.01517v3)

**Abstract**: This paper aims to tackle the challenge of efficiently producing interactive
free-viewpoint videos. Some recent works equip neural radiance fields with
image encoders, enabling them to generalize across scenes. When processing
dynamic scenes, they can simply treat each video frame as an individual scene
and perform novel view synthesis to generate free-viewpoint videos. However,
their rendering process is slow and cannot support interactive applications. A
major factor is that they sample lots of points in empty space when inferring
radiance fields. We propose a novel scene representation, called ENeRF, for the
fast creation of interactive free-viewpoint videos. Specifically, given
multi-view images at one frame, we first build the cascade cost volume to
predict the coarse geometry of the scene. The coarse geometry allows us to
sample few points near the scene surface, thereby significantly improving the
rendering speed. This process is fully differentiable, enabling us to jointly
learn the depth prediction and radiance field networks from RGB images.
Experiments on multiple benchmarks show that our approach exhibits competitive
performance while being at least 60 times faster than previous generalizable
radiance field methods.

Comments:
- SIGGRAPH Asia 2022; Project page: https://zju3dv.github.io/enerf/

---

### Zero-Shot Text-Guided Object Generation with Dream Fields



Published 2021-12-02 by Ajay Jain | Ben Mildenhall | Jonathan T. Barron | Pieter Abbeel | Ben Poole in cs.CV, [PDF](http://arxiv.org/pdf/2112.01455v2)

**Abstract**: We combine neural rendering with multi-modal image and text representations
to synthesize diverse 3D objects solely from natural language descriptions. Our
method, Dream Fields, can generate the geometry and color of a wide range of
objects without 3D supervision. Due to the scarcity of diverse, captioned 3D
data, prior methods only generate objects from a handful of categories, such as
ShapeNet. Instead, we guide generation with image-text models pre-trained on
large datasets of captioned images from the web. Our method optimizes a Neural
Radiance Field from many camera views so that rendered images score highly with
a target caption according to a pre-trained CLIP model. To improve fidelity and
visual quality, we introduce simple geometric priors, including
sparsity-inducing transmittance regularization, scene bounds, and new MLP
architectures. In experiments, Dream Fields produce realistic, multi-view
consistent object geometry and color from a variety of natural language
captions.

Comments:
- CVPR 2022. 13 pages. Website: https://ajayj.com/dreamfields

---

### 3D-Aware Semantic-Guided Generative Model for Human Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-02 by Jichao Zhang | Enver Sangineto | Hao Tang | Aliaksandr Siarohin | Zhun Zhong | Nicu Sebe | Wei Wang in cs.CV, [PDF](http://arxiv.org/pdf/2112.01422v2)

**Abstract**: Generative Neural Radiance Field (GNeRF) models, which extract implicit 3D
representations from 2D images, have recently been shown to produce realistic
images representing rigid/semi-rigid objects, such as human faces or cars.
However, they usually struggle to generate high-quality images representing
non-rigid objects, such as the human body, which is of a great interest for
many computer graphics applications. This paper proposes a 3D-aware
Semantic-Guided Generative Model (3D-SGAN) for human image synthesis, which
combines a GNeRF with a texture generator. The former learns an implicit 3D
representation of the human body and outputs a set of 2D semantic segmentation
masks. The latter transforms these semantic masks into a real image, adding a
realistic texture to the human appearance. Without requiring additional 3D
information, our model can learn 3D human representations with a
photo-realistic, controllable generation. Our experiments on the DeepFashion
dataset show that 3D-SGAN significantly outperforms the most recent baselines.
The code is available at https://github.com/zhangqianhui/3DSGAN

Comments:
- ECCV 2022. 29 pages

---

### Learning Neural Light Fields with Ray-Space Embedding Networks

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-02 by Benjamin Attal | Jia-Bin Huang | Michael Zollhoefer | Johannes Kopf | Changil Kim in cs.CV, [PDF](http://arxiv.org/pdf/2112.01523v3)

**Abstract**: Neural radiance fields (NeRFs) produce state-of-the-art view synthesis
results. However, they are slow to render, requiring hundreds of network
evaluations per pixel to approximate a volume rendering integral. Baking NeRFs
into explicit data structures enables efficient rendering, but results in a
large increase in memory footprint and, in many cases, a quality reduction. In
this paper, we propose a novel neural light field representation that, in
contrast, is compact and directly predicts integrated radiance along rays. Our
method supports rendering with a single network evaluation per pixel for small
baseline light field datasets and can also be applied to larger baselines with
only a few evaluations per pixel. At the core of our approach is a ray-space
embedding network that maps the 4D ray-space manifold into an intermediate,
interpolable latent space. Our method achieves state-of-the-art quality on
dense forward-facing datasets such as the Stanford Light Field dataset. In
addition, for forward-facing scenes with sparser inputs we achieve results that
are competitive with NeRF-based approaches in terms of quality while providing
a better speed/quality/memory trade-off with far fewer network evaluations.

Comments:
- CVPR 2022 camera ready revision. Major changes include: 1. Additional
  comparison to NeX on Stanford, RealFF, Shiny datasets 2. Experiment on 360
  degree lego bulldozer scene in the appendix, using Pluecker parameterization
  3. Moving student-teacher results to the appendix 4. Clarity edits -- in
  particular, making it clear that our Stanford evaluation *does not* use
  subdivision

---

### RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from  Sparse Inputs

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-12-01 by Michael Niemeyer | Jonathan T. Barron | Ben Mildenhall | Mehdi S. M. Sajjadi | Andreas Geiger | Noha Radwan in cs.CV, [PDF](http://arxiv.org/pdf/2112.00724v1)

**Abstract**: Neural Radiance Fields (NeRF) have emerged as a powerful representation for
the task of novel view synthesis due to their simplicity and state-of-the-art
performance. Though NeRF can produce photorealistic renderings of unseen
viewpoints when many input views are available, its performance drops
significantly when this number is reduced. We observe that the majority of
artifacts in sparse input scenarios are caused by errors in the estimated scene
geometry, and by divergent behavior at the start of training. We address this
by regularizing the geometry and appearance of patches rendered from unobserved
viewpoints, and annealing the ray sampling space during training. We
additionally use a normalizing flow model to regularize the color of unobserved
viewpoints. Our model outperforms not only other methods that optimize over a
single scene, but in many cases also conditional models that are extensively
pre-trained on large multi-view datasets.

Comments:
- Project page available at
  https://m-niemeyer.github.io/regnerf/index.html

---

### Hallucinated Neural Radiance Fields in the Wild

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-30 by Xingyu Chen | Qi Zhang | Xiaoyu Li | Yue Chen | Ying Feng | Xuan Wang | Jue Wang in cs.CV, [PDF](http://arxiv.org/pdf/2111.15246v3)

**Abstract**: Neural Radiance Fields (NeRF) has recently gained popularity for its
impressive novel view synthesis ability. This paper studies the problem of
hallucinated NeRF: i.e., recovering a realistic NeRF at a different time of day
from a group of tourism images. Existing solutions adopt NeRF with a
controllable appearance embedding to render novel views under various
conditions, but they cannot render view-consistent images with an unseen
appearance. To solve this problem, we present an end-to-end framework for
constructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose
an appearance hallucination module to handle time-varying appearances and
transfer them to novel views. Considering the complex occlusions of tourism
images, we introduce an anti-occlusion module to decompose the static subjects
for visibility accurately. Experimental results on synthetic data and real
tourism photo collections demonstrate that our method can hallucinate the
desired appearances and render occlusion-free images from different views. The
project and supplementary materials are available at
https://rover-xingyu.github.io/Ha-NeRF/.

Comments:
- Accepted by CVPR 2022. Project website:
  https://rover-xingyu.github.io/Ha-NeRF/

---

### NeRFReN: Neural Radiance Fields with Reflections

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-30 by Yuan-Chen Guo | Di Kang | Linchao Bao | Yu He | Song-Hai Zhang in cs.CV, [PDF](http://arxiv.org/pdf/2111.15234v2)

**Abstract**: Neural Radiance Fields (NeRF) has achieved unprecedented view synthesis
quality using coordinate-based neural scene representations. However, NeRF's
view dependency can only handle simple reflections like highlights but cannot
deal with complex reflections such as those from glass and mirrors. In these
scenarios, NeRF models the virtual image as real geometries which leads to
inaccurate depth estimation, and produces blurry renderings when the multi-view
consistency is violated as the reflected objects may only be seen under some of
the viewpoints. To overcome these issues, we introduce NeRFReN, which is built
upon NeRF to model scenes with reflections. Specifically, we propose to split a
scene into transmitted and reflected components, and model the two components
with separate neural radiance fields. Considering that this decomposition is
highly under-constrained, we exploit geometric priors and apply
carefully-designed training strategies to achieve reasonable decomposition
results. Experiments on various self-captured scenes show that our method
achieves high-quality novel view synthesis and physically sound depth
estimation results while enabling scene editing applications.

Comments:
- Accepted to CVPR 2022. Project page:
  https://bennyguo.github.io/nerfren/

---

### FENeRF: Face Editing in Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-30 by Jingxiang Sun | Xuan Wang | Yong Zhang | Xiaoyu Li | Qi Zhang | Yebin Liu | Jue Wang in cs.CV, [PDF](http://arxiv.org/pdf/2111.15490v2)

**Abstract**: Previous portrait image generation methods roughly fall into two categories:
2D GANs and 3D-aware GANs. 2D GANs can generate high fidelity portraits but
with low view consistency. 3D-aware GAN methods can maintain view consistency
but their generated images are not locally editable. To overcome these
limitations, we propose FENeRF, a 3D-aware generator that can produce
view-consistent and locally-editable portrait images. Our method uses two
decoupled latent codes to generate corresponding facial semantics and texture
in a spatial aligned 3D volume with shared geometry. Benefiting from such
underlying 3D representation, FENeRF can jointly render the boundary-aligned
image and semantic mask and use the semantic mask to edit the 3D volume via GAN
inversion. We further show such 3D representation can be learned from widely
available monocular image and semantic mask pairs. Moreover, we reveal that
joint learning semantics and texture helps to generate finer geometry. Our
experiments demonstrate that FENeRF outperforms state-of-the-art methods in
various face editing tasks.

Comments:
- Accepted to CVPR 2022. Project: https://mrtornado24.github.io/FENeRF/

---

### NeuSample: Neural Sample Field for Efficient View Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-30 by Jiemin Fang | Lingxi Xie | Xinggang Wang | Xiaopeng Zhang | Wenyu Liu | Qi Tian in cs.CV, [PDF](http://arxiv.org/pdf/2111.15552v1)

**Abstract**: Neural radiance fields (NeRF) have shown great potentials in representing 3D
scenes and synthesizing novel views, but the computational overhead of NeRF at
the inference stage is still heavy. To alleviate the burden, we delve into the
coarse-to-fine, hierarchical sampling procedure of NeRF and point out that the
coarse stage can be replaced by a lightweight module which we name a neural
sample field. The proposed sample field maps rays into sample distributions,
which can be transformed into point coordinates and fed into radiance fields
for volume rendering. The overall framework is named as NeuSample. We perform
experiments on Realistic Synthetic 360$^{\circ}$ and Real Forward-Facing, two
popular 3D scene sets, and show that NeuSample achieves better rendering
quality than NeRF while enjoying a faster inference speed. NeuSample is further
compressed with a proposed sample field extraction method towards a better
trade-off between quality and speed.

Comments:
- Project page: https://jaminfong.cn/neusample/

---

### Urban Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-29 by Konstantinos Rematas | Andrew Liu | Pratul P. Srinivasan | Jonathan T. Barron | Andrea Tagliasacchi | Thomas Funkhouser | Vittorio Ferrari in cs.CV, [PDF](http://arxiv.org/pdf/2111.14643v1)

**Abstract**: The goal of this work is to perform 3D reconstruction and novel view
synthesis from data captured by scanning platforms commonly deployed for world
mapping in urban outdoor environments (e.g., Street View). Given a sequence of
posed RGB images and lidar sweeps acquired by cameras and scanners moving
through an outdoor scene, we produce a model from which 3D surfaces can be
extracted and novel RGB images can be synthesized. Our approach extends Neural
Radiance Fields, which has been demonstrated to synthesize realistic novel
images for small scenes in controlled settings, with new methods for leveraging
asynchronously captured lidar data, for addressing exposure variation between
captured images, and for leveraging predicted image segmentations to supervise
densities on rays pointing at the sky. Each of these three extensions provides
significant performance improvements in experiments on Street View data. Our
system produces state-of-the-art 3D surface reconstructions and synthesizes
higher quality novel views in comparison to both traditional methods
(e.g.~COLMAP) and recent neural representations (e.g.~Mip-NeRF).

Comments:
- Project: https://urban-radiance-fields.github.io/

---

### HDR-NeRF: High Dynamic Range Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-29 by Xin Huang | Qi Zhang | Ying Feng | Hongdong Li | Xuan Wang | Qing Wang in cs.CV, [PDF](http://arxiv.org/pdf/2111.14451v4)

**Abstract**: We present High Dynamic Range Neural Radiance Fields (HDR-NeRF) to recover an
HDR radiance field from a set of low dynamic range (LDR) views with different
exposures. Using the HDR-NeRF, we are able to generate both novel HDR views and
novel LDR views under different exposures. The key to our method is to model
the physical imaging process, which dictates that the radiance of a scene point
transforms to a pixel value in the LDR image with two implicit functions: a
radiance field and a tone mapper. The radiance field encodes the scene radiance
(values vary from 0 to +infty), which outputs the density and radiance of a ray
by giving corresponding ray origin and ray direction. The tone mapper models
the mapping process that a ray hitting on the camera sensor becomes a pixel
value. The color of the ray is predicted by feeding the radiance and the
corresponding exposure time into the tone mapper. We use the classic volume
rendering technique to project the output radiance, colors, and densities into
HDR and LDR images, while only the input LDR images are used as the
supervision. We collect a new forward-facing HDR dataset to evaluate the
proposed method. Experimental results on synthetic and real-world scenes
validate that our method can not only accurately control the exposures of
synthesized views but also render views with a high dynamic range.

Comments:
- Accepted to CVPR 2022. Project page:
  https://xhuangcv.github.io/hdr-nerf/

---

### Deblur-NeRF: Neural Radiance Fields from Blurry Images

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-29 by Li Ma | Xiaoyu Li | Jing Liao | Qi Zhang | Xuan Wang | Jue Wang | Pedro V. Sander in cs.CV, [PDF](http://arxiv.org/pdf/2111.14292v2)

**Abstract**: Neural Radiance Field (NeRF) has gained considerable attention recently for
3D scene reconstruction and novel view synthesis due to its remarkable
synthesis quality. However, image blurriness caused by defocus or motion, which
often occurs when capturing scenes in the wild, significantly degrades its
reconstruction quality. To address this problem, We propose Deblur-NeRF, the
first method that can recover a sharp NeRF from blurry input. We adopt an
analysis-by-synthesis approach that reconstructs blurry views by simulating the
blurring process, thus making NeRF robust to blurry inputs. The core of this
simulation is a novel Deformable Sparse Kernel (DSK) module that models
spatially-varying blur kernels by deforming a canonical sparse kernel at each
spatial location. The ray origin of each kernel point is jointly optimized,
inspired by the physical blurring process. This module is parameterized as an
MLP that has the ability to be generalized to various blur types. Jointly
optimizing the NeRF and the DSK module allows us to restore a sharp NeRF. We
demonstrate that our method can be used on both camera motion blur and defocus
blur: the two most common types of blur in real scenes. Evaluation results on
both synthetic and real-world data show that our method outperforms several
baselines. The synthetic and real datasets along with the source code is
publicly available at https://limacv.github.io/deblurnerf/

Comments:
- accepted in CVPR2022

---

### NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw  Images

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-26 by Ben Mildenhall | Peter Hedman | Ricardo Martin-Brualla | Pratul Srinivasan | Jonathan T. Barron in cs.CV, [PDF](http://arxiv.org/pdf/2111.13679v1)

**Abstract**: Neural Radiance Fields (NeRF) is a technique for high quality novel view
synthesis from a collection of posed input images. Like most view synthesis
methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images
have been processed by a lossy camera pipeline that smooths detail, clips
highlights, and distorts the simple noise distribution of raw sensor data. We
modify NeRF to instead train directly on linear raw images, preserving the
scene's full dynamic range. By rendering raw output images from the resulting
NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In
addition to changing the camera viewpoint, we can manipulate focus, exposure,
and tonemapping after the fact. Although a single raw image appears
significantly more noisy than a postprocessed one, we show that NeRF is highly
robust to the zero-mean distribution of raw noise. When optimized over many
noisy raw inputs (25-200), NeRF produces a scene representation so accurate
that its rendered novel views outperform dedicated single and multi-image deep
raw denoisers run on the same wide baseline input images. As a result, our
method, which we call RawNeRF, can reconstruct scenes from extremely noisy
images captured in near-darkness.

Comments:
- Project page: https://bmild.github.io/rawnerf/

---

### GeoNeRF: Generalizing NeRF with Geometry Priors

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-26 by Mohammad Mahdi Johari | Yann Lepoittevin | François Fleuret in cs.CV, [PDF](http://arxiv.org/pdf/2111.13539v2)

**Abstract**: We present GeoNeRF, a generalizable photorealistic novel view synthesis
method based on neural radiance fields. Our approach consists of two main
stages: a geometry reasoner and a renderer. To render a novel view, the
geometry reasoner first constructs cascaded cost volumes for each nearby source
view. Then, using a Transformer-based attention mechanism and the cascaded cost
volumes, the renderer infers geometry and appearance, and renders detailed
images via classical volume rendering techniques. This architecture, in
particular, allows sophisticated occlusion reasoning, gathering information
from consistent source views. Moreover, our method can easily be fine-tuned on
a single scene, and renders competitive results with per-scene optimized neural
rendering methods with a fraction of computational cost. Experiments show that
GeoNeRF outperforms state-of-the-art generalizable neural rendering models on
various synthetic and real datasets. Lastly, with a slight modification to the
geometry reasoner, we also propose an alternative model that adapts to RGBD
images. This model directly exploits the depth information often available
thanks to depth sensors. The implementation code is available at
https://www.idiap.ch/paper/geonerf.

Comments:
- CVPR2022

---

### VaxNeRF: Revisiting the Classic for Voxel-Accelerated Neural Radiance  Field

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-25 by Naruya Kondo | Yuya Ikeda | Andrea Tagliasacchi | Yutaka Matsuo | Yoichi Ochiai | Shixiang Shane Gu in cs.CV, [PDF](http://arxiv.org/pdf/2111.13112v1)

**Abstract**: Neural Radiance Field (NeRF) is a popular method in data-driven 3D
reconstruction. Given its simplicity and high quality rendering, many NeRF
applications are being developed. However, NeRF's big limitation is its slow
speed. Many attempts are made to speeding up NeRF training and inference,
including intricate code-level optimization and caching, use of sophisticated
data structures, and amortization through multi-task and meta learning. In this
work, we revisit the basic building blocks of NeRF through the lens of classic
techniques before NeRF. We propose Voxel-Accelearated NeRF (VaxNeRF),
integrating NeRF with visual hull, a classic 3D reconstruction technique only
requiring binary foreground-background pixel labels per image. Visual hull,
which can be optimized in about 10 seconds, can provide coarse in-out field
separation to omit substantial amounts of network evaluations in NeRF. We
provide a clean fully-pythonic, JAX-based implementation on the popular JaxNeRF
codebase, consisting of only about 30 lines of code changes and a modular
visual hull subroutine, and achieve about 2-8x faster learning on top of the
highly-performative JaxNeRF baseline with zero degradation in rendering
quality. With sufficient compute, this effectively brings down full NeRF
training from hours to 30 minutes. We hope VaxNeRF -- a careful combination of
a classic technique with a deep method (that arguably replaced it) -- can
empower and accelerate new NeRF extensions and applications, with its
simplicity, portability, and reliable performance gains. Codes are available at
https://github.com/naruya/VaxNeRF .

---

### Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-23 by Jonathan T. Barron | Ben Mildenhall | Dor Verbin | Pratul P. Srinivasan | Peter Hedman in cs.CV, [PDF](http://arxiv.org/pdf/2111.12077v3)

**Abstract**: Though neural radiance fields (NeRF) have demonstrated impressive view
synthesis results on objects and small bounded regions of space, they struggle
on "unbounded" scenes, where the camera may point in any direction and content
may exist at any distance. In this setting, existing NeRF-like models often
produce blurry or low-resolution renderings (due to the unbalanced detail and
scale of nearby and distant objects), are slow to train, and may exhibit
artifacts due to the inherent ambiguity of the task of reconstructing a large
scene from a small set of images. We present an extension of mip-NeRF (a NeRF
variant that addresses sampling and aliasing) that uses a non-linear scene
parameterization, online distillation, and a novel distortion-based regularizer
to overcome the challenges presented by unbounded scenes. Our model, which we
dub "mip-NeRF 360" as we target scenes in which the camera rotates 360 degrees
around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is
able to produce realistic synthesized views and detailed depth maps for highly
intricate, unbounded real-world scenes.

Comments:
- https://jonbarron.info/mipnerf360/

---

### Direct Voxel Grid Optimization: Super-fast Convergence for Radiance  Fields Reconstruction

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-22 by Cheng Sun | Min Sun | Hwann-Tzong Chen in cs.CV, [PDF](http://arxiv.org/pdf/2111.11215v2)

**Abstract**: We present a super-fast convergence approach to reconstructing the per-scene
radiance field from a set of images that capture the scene with known poses.
This task, which is often applied to novel view synthesis, is recently
revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality
and flexibility. However, NeRF and its variants require a lengthy training time
ranging from hours to days for a single scene. In contrast, our approach
achieves NeRF-comparable quality and converges rapidly from scratch in less
than 15 minutes with a single GPU. We adopt a representation consisting of a
density voxel grid for scene geometry and a feature voxel grid with a shallow
network for complex view-dependent appearance. Modeling with explicit and
discretized volume representations is not new, but we propose two simple yet
non-trivial techniques that contribute to fast convergence speed and
high-quality output. First, we introduce the post-activation interpolation on
voxel density, which is capable of producing sharp surfaces in lower grid
resolution. Second, direct voxel density optimization is prone to suboptimal
geometry solutions, so we robustify the optimization process by imposing
several priors. Finally, evaluation on five inward-facing benchmarks shows that
our method matches, if not surpasses, NeRF's quality, yet it only takes about
15 minutes to train from scratch for a new scene.

Comments:
- Project page at https://sunset1995.github.io/dvgo/ ; Code at
  https://github.com/sunset1995/DirectVoxGO

---

### LOLNeRF: Learn from One Look

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-19 by Daniel Rebain | Mark Matthews | Kwang Moo Yi | Dmitry Lagun | Andrea Tagliasacchi in cs.CV, [PDF](http://arxiv.org/pdf/2111.09996v2)

**Abstract**: We present a method for learning a generative 3D model based on neural
radiance fields, trained solely from data with only single views of each
object. While generating realistic images is no longer a difficult task,
producing the corresponding 3D structure such that they can be rendered from
different views is non-trivial. We show that, unlike existing methods, one does
not need multi-view data to achieve this goal. Specifically, we show that by
reconstructing many images aligned to an approximate canonical pose with a
single network conditioned on a shared latent space, you can learn a space of
radiance fields that models shape and appearance for a class of objects. We
demonstrate this by training models to reconstruct object categories using
datasets that contain only one view of each subject without depth or geometry
information. Our experiments show that we achieve state-of-the-art results in
novel view synthesis and high-quality results for monocular depth prediction.

Comments:
- See https://lolnerf.github.io for additional results

---

### DIVeR: Real-time and Accurate Neural Radiance Fields with Deterministic  Integration for Volume Rendering

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-19 by Liwen Wu | Jae Yong Lee | Anand Bhattad | Yuxiong Wang | David Forsyth in cs.CV, [PDF](http://arxiv.org/pdf/2111.10427v2)

**Abstract**: DIVeR builds on the key ideas of NeRF and its variants -- density models and
volume rendering -- to learn 3D object models that can be rendered
realistically from small numbers of images. In contrast to all previous NeRF
methods, DIVeR uses deterministic rather than stochastic estimates of the
volume rendering integral. DIVeR's representation is a voxel based field of
features. To compute the volume rendering integral, a ray is broken into
intervals, one per voxel; components of the volume rendering integral are
estimated from the features for each interval using an MLP, and the components
are aggregated. As a result, DIVeR can render thin translucent structures that
are missed by other integrators. Furthermore, DIVeR's representation has
semantics that is relatively exposed compared to other such methods -- moving
feature vectors around in the voxel space results in natural edits. Extensive
qualitative and quantitative comparisons to current state-of-the-art methods
show that DIVeR produces models that (1) render at or above state-of-the-art
quality, (2) are very small without being baked, (3) render very fast without
being baked, and (4) can be edited in natural ways.

---

### LVAC: Learned Volumetric Attribute Compression for Point Clouds using  Coordinate Based Networks



Published 2021-11-17 by Berivan Isik | Philip A. Chou | Sung Jin Hwang | Nick Johnston | George Toderici in cs.GR, [PDF](http://arxiv.org/pdf/2111.08988v1)

**Abstract**: We consider the attributes of a point cloud as samples of a vector-valued
volumetric function at discrete positions. To compress the attributes given the
positions, we compress the parameters of the volumetric function. We model the
volumetric function by tiling space into blocks, and representing the function
over each block by shifts of a coordinate-based, or implicit, neural network.
Inputs to the network include both spatial coordinates and a latent vector per
block. We represent the latent vectors using coefficients of the
region-adaptive hierarchical transform (RAHT) used in the MPEG geometry-based
point cloud codec G-PCC. The coefficients, which are highly compressible, are
rate-distortion optimized by back-propagation through a rate-distortion
Lagrangian loss in an auto-decoder configuration. The result outperforms RAHT
by 2--4 dB. This is the first work to compress volumetric functions represented
by local coordinate-based neural networks. As such, we expect it to be
applicable beyond point clouds, for example to compression of high-resolution
neural radiance fields.

Comments:
- 30 pages, 29 figures

---

### Template NeRF: Towards Modeling Dense Shape Correspondences from  Category-Specific Object Images

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-11-08 by Jianfei Guo | Zhiyuan Yang | Xi Lin | Qingfu Zhang in cs.CV, [PDF](http://arxiv.org/pdf/2111.04237v1)

**Abstract**: We present neural radiance fields (NeRF) with templates, dubbed
Template-NeRF, for modeling appearance and geometry and generating dense shape
correspondences simultaneously among objects of the same category from only
multi-view posed images, without the need of either 3D supervision or
ground-truth correspondence knowledge. The learned dense correspondences can be
readily used for various image-based tasks such as keypoint detection, part
segmentation, and texture transfer that previously require specific model
designs. Our method can also accommodate annotation transfer in a one or
few-shot manner, given only one or a few instances of the category. Using
periodic activation and feature-wise linear modulation (FiLM) conditioning, we
introduce deep implicit templates on 3D data into the 3D-aware image synthesis
pipeline NeRF. By representing object instances within the same category as
shape and appearance variation of a shared NeRF template, our proposed method
can achieve dense shape correspondences reasoning on images for a wide range of
object classes. We demonstrate the results and applications on both synthetic
and real-world data with competitive results compared with other methods based
on 3D information.

Comments:
- 10 pages, 8 figures

---

### Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-10-27 by Mark Boss | Varun Jampani | Raphael Braun | Ce Liu | Jonathan T. Barron | Hendrik P. A. Lensch in cs.CV, [PDF](http://arxiv.org/pdf/2110.14373v1)

**Abstract**: Decomposing a scene into its shape, reflectance and illumination is a
fundamental problem in computer vision and graphics. Neural approaches such as
NeRF have achieved remarkable success in view synthesis, but do not explicitly
perform decomposition and instead operate exclusively on radiance (the product
of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform
decomposition but struggle to accurately recover detailed illumination, thereby
significantly limiting realism. We propose a novel reflectance decomposition
network that can estimate shape, BRDF, and per-image illumination given a set
of object images captured under varying illumination. Our key technique is a
novel illumination integration network called Neural-PIL that replaces a costly
illumination integral operation in the rendering with a simple network query.
In addition, we also learn deep low-dimensional priors on BRDF and illumination
representations using novel smooth manifold auto-encoders. Our decompositions
can result in considerably better BRDF and light estimates enabling more
accurate novel view-synthesis and relighting compared to prior art. Project
page: https://markboss.me/publication/2021-neural-pil/

Comments:
- Project page: https://markboss.me/publication/2021-neural-pil/ Video:
  https://youtu.be/AsdAR5u3vQ8 - Accepted at NeurIPS 2021

---

### Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-10-27 by Jeffrey Ichnowski | Yahav Avigal | Justin Kerr | Ken Goldberg in cs.RO, [PDF](http://arxiv.org/pdf/2110.14217v1)

**Abstract**: The ability to grasp and manipulate transparent objects is a major challenge
for robots. Existing depth cameras have difficulty detecting, localizing, and
inferring the geometry of such objects. We propose using neural radiance fields
(NeRF) to detect, localize, and infer the geometry of transparent objects with
sufficient accuracy to find and grasp them securely. We leverage NeRF's
view-independent learned density, place lights to increase specular
reflections, and perform a transparency-aware depth-rendering that we feed into
the Dex-Net grasp planner. We show how additional lights create specular
reflections that improve the quality of the depth map, and test a setup for a
robot workcell equipped with an array of cameras to perform transparent object
manipulation. We also create synthetic and real datasets of transparent objects
in real-world settings, including singulated objects, cluttered tables, and the
top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are
able to reliably compute robust grasps on transparent objects, achieving 90%
and 100% grasp success rates in physical experiments on an ABB YuMi, on objects
where baseline methods fail.

Comments:
- 11 pages, 9 figures, to be published in the Conference on Robot
  Learning (CoRL) 2021

---

### H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction  of Humans in Motion

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-10-26 by Hongyi Xu | Thiemo Alldieck | Cristian Sminchisescu in cs.CV, [PDF](http://arxiv.org/pdf/2110.13746v2)

**Abstract**: We present neural radiance fields for rendering and temporal (4D)
reconstruction of humans in motion (H-NeRF), as captured by a sparse set of
cameras or even from a monocular video. Our approach combines ideas from neural
scene representation, novel-view synthesis, and implicit statistical geometric
human representations, coupled using novel loss functions. Instead of learning
a radiance field with a uniform occupancy prior, we constrain it by a
structured implicit human body model, represented using signed distance
functions. This allows us to robustly fuse information from sparse views and
generalize well beyond the poses or views observed in training. Moreover, we
apply geometric constraints to co-learn the structure of the observed subject
-- including both body and clothing -- and to regularize the radiance field to
geometrically plausible solutions. Extensive experiments on multiple datasets
demonstrate the robustness and the accuracy of our approach, its generalization
capabilities significantly outside a small training set of poses and views, and
statistical extrapolation beyond the observed shape.

---

### Neural Relightable Participating Media Rendering



Published 2021-10-25 by Quan Zheng | Gurprit Singh | Hans-Peter Seidel in cs.CV, [PDF](http://arxiv.org/pdf/2110.12993v1)

**Abstract**: Learning neural radiance fields of a scene has recently allowed realistic
novel view synthesis of the scene, but they are limited to synthesize images
under the original fixed lighting condition. Therefore, they are not flexible
for the eagerly desired tasks like relighting, scene editing and scene
composition. To tackle this problem, several recent methods propose to
disentangle reflectance and illumination from the radiance field. These methods
can cope with solid objects with opaque surfaces but participating media are
neglected. Also, they take into account only direct illumination or at most
one-bounce indirect illumination, thus suffer from energy loss due to ignoring
the high-order indirect illumination. We propose to learn neural
representations for participating media with a complete simulation of global
illumination. We estimate direct illumination via ray tracing and compute
indirect illumination with spherical harmonics. Our approach avoids computing
the lengthy indirect bounces and does not suffer from energy loss. Our
experiments on multiple scenes show that our approach achieves superior visual
quality and numerical performance compared to state-of-the-art methods, and it
can generalize to deal with solid objects with opaque surfaces as well.

Comments:
- Accepted to NeurIPS 2021

---

### CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent  Pixel Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-10-19 by Peng Zhou | Lingxi Xie | Bingbing Ni | Qi Tian in cs.CV, [PDF](http://arxiv.org/pdf/2110.09788v1)

**Abstract**: The style-based GAN (StyleGAN) architecture achieved state-of-the-art results
for generating high-quality images, but it lacks explicit and precise control
over camera poses. The recently proposed NeRF-based GANs made great progress
towards 3D-aware generators, but they are unable to generate high-quality
images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that
is composed of a shallow NeRF network and a deep implicit neural representation
(INR) network. The generator synthesizes each pixel value independently without
any spatial convolution or upsampling operation. In addition, we diagnose the
problem of mirror symmetry that implies a suboptimal solution and solve it by
introducing an auxiliary discriminator. Trained on raw, single-view images,
CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of
6.97 for images at the $256\times256$ resolution on FFHQ. We also demonstrate
several interesting directions for CIPS-3D such as transfer learning and
3D-aware face stylization. The synthesis results are best viewed as videos, so
we recommend the readers to check our github project at
https://github.com/PeterouZh/CIPS-3D

Comments:
- 3D-aware GANs based on NeRF, https://github.com/PeterouZh/CIPS-3D

---

### StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image  Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-10-18 by Jiatao Gu | Lingjie Liu | Peng Wang | Christian Theobalt in cs.CV, [PDF](http://arxiv.org/pdf/2110.08985v1)

**Abstract**: We propose StyleNeRF, a 3D-aware generative model for photo-realistic
high-resolution image synthesis with high multi-view consistency, which can be
trained on unstructured 2D images. Existing approaches either cannot synthesize
high-resolution images with fine details or yield noticeable 3D-inconsistent
artifacts. In addition, many of them lack control over style attributes and
explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF)
into a style-based generator to tackle the aforementioned challenges, i.e.,
improving rendering efficiency and 3D consistency for high-resolution image
generation. We perform volume rendering only to produce a low-resolution
feature map and progressively apply upsampling in 2D to address the first
issue. To mitigate the inconsistencies caused by 2D upsampling, we propose
multiple designs, including a better upsampler and a new regularization loss.
With these designs, StyleNeRF can synthesize high-resolution images at
interactive rates while preserving 3D consistency at high quality. StyleNeRF
also enables control of camera poses and different levels of styles, which can
generalize to unseen views. It also supports challenging tasks, including
zoom-in and-out, style mixing, inversion, and semantic editing.

Comments:
- 24 pages, 19 figures. Project page: http://jiataogu.me/style_nerf/

---

### NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in  the Wild

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-10-14 by Jason Y. Zhang | Gengshan Yang | Shubham Tulsiani | Deva Ramanan in cs.CV, [PDF](http://arxiv.org/pdf/2110.07604v3)

**Abstract**: Recent history has seen a tremendous growth of work exploring implicit
representations of geometry and radiance, popularized through Neural Radiance
Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric
representation of occupancy, allowing them to model diverse scene structure
including translucent objects and atmospheric obscurants. But because the vast
majority of real-world scenes are composed of well-defined surfaces, we
introduce a surface analog of such implicit models called Neural Reflectance
Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface
that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions.
Even more importantly, surface parameterizations allow NeRS to learn (neural)
bidirectional surface reflectance functions (BRDFs) that factorize
view-dependent appearance into environmental illumination, diffuse color
(albedo), and specular "shininess." Finally, rather than illustrating our
results on synthetic scenes or controlled in-the-lab capture, we assemble a
novel dataset of multi-view images from online marketplaces for selling goods.
Such "in-the-wild" multi-view image sets pose a number of challenges, including
a small number of views with unknown/rough camera estimates. We demonstrate
that surface-based neural reconstructions enable learning from such data,
outperforming volumetric neural rendering-based reconstructions. We hope that
NeRS serves as a first step toward building scalable, high-quality libraries of
real-world shape, materials, and illumination. The project page with code and
video visualizations can be found at https://jasonyzhang.com/ners.

Comments:
- In NeurIPS 2021. v2-3: Fixed minor typos

---

### LENS: Localization enhanced by NeRF synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-10-13 by Arthur Moreau | Nathan Piasco | Dzmitry Tsishkou | Bogdan Stanciulescu | Arnaud de La Fortelle in cs.CV, [PDF](http://arxiv.org/pdf/2110.06558v1)

**Abstract**: Neural Radiance Fields (NeRF) have recently demonstrated photo-realistic
results for the task of novel view synthesis. In this paper, we propose to
apply novel view synthesis to the robot relocalization problem: we demonstrate
improvement of camera pose regression thanks to an additional synthetic dataset
rendered by the NeRF class of algorithm. To avoid spawning novel views in
irrelevant places we selected virtual camera locations from NeRF internal
representation of the 3D geometry of the scene. We further improved
localization accuracy of pose regressors using synthesized realistic and
geometry consistent images as data augmentation during training. At the time of
publication, our approach improved state of the art with a 60% lower error on
Cambridge Landmarks and 7-scenes datasets. Hence, the resulting accuracy
becomes comparable to structure-based methods, without any architecture
modification or domain adaptation constraints. Since our method allows almost
infinite generation of training data, we investigated limitations of camera
pose regression depending on size and distribution of data used for training on
public benchmarks. We concluded that pose regression accuracy is mostly bounded
by relatively small and biased datasets rather than capacity of the pose
regression model to solve the localization task.

Comments:
- Accepted at CoRL 2021

---

### Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo



Published 2021-10-11 by Berk Kaya | Suryansh Kumar | Francesco Sarno | Vittorio Ferrari | Luc Van Gool in cs.CV, [PDF](http://arxiv.org/pdf/2110.05594v1)

**Abstract**: We present a modern solution to the multi-view photometric stereo problem
(MVPS). Our work suitably exploits the image formation model in a MVPS
experimental setup to recover the dense 3D reconstruction of an object from
images. We procure the surface orientation using a photometric stereo (PS)
image formation model and blend it with a multi-view neural radiance field
representation to recover the object's surface geometry. Contrary to the
previous multi-staged framework to MVPS, where the position, iso-depth
contours, or orientation measurements are estimated independently and then
fused later, our method is simple to implement and realize. Our method performs
neural rendering of multi-view images while utilizing surface normals estimated
by a deep photometric stereo network. We render the MVPS images by considering
the object's surface normals for each 3D sample point along the viewing
direction rather than explicitly using the density gradient in the volume space
via 3D occupancy information. We optimize the proposed neural radiance field
representation for the MVPS setup efficiently using a fully connected deep
network to recover the 3D geometry of an object. Extensive evaluation on the
DiLiGenT-MV benchmark dataset shows that our method performs better than the
approaches that perform only PS or only multi-view stereo (MVS) and provides
comparable results against the state-of-the-art multi-stage fusion methods.

Comments:
- Accepted for publication at IEEE/CVF WACV 2022. 18 pages

---

### TyXe: Pyro-based Bayesian neural nets for Pytorch



Published 2021-10-01 by Hippolyt Ritter | Theofanis Karaletsos in stat.ML, [PDF](http://arxiv.org/pdf/2110.00276v1)

**Abstract**: We introduce TyXe, a Bayesian neural network library built on top of Pytorch
and Pyro. Our leading design principle is to cleanly separate architecture,
prior, inference and likelihood specification, allowing for a flexible workflow
where users can quickly iterate over combinations of these components. In
contrast to existing packages TyXe does not implement any layer classes, and
instead relies on architectures defined in generic Pytorch code. TyXe then
provides modular choices for canonical priors, variational guides, inference
techniques, and layer selections for a Bayesian treatment of the specified
architecture. Sampling tricks for variance reduction, such as local
reparameterization or flipout, are implemented as effect handlers, which can be
applied independently of other specifications. We showcase the ease of use of
TyXe to explore Bayesian versions of popular models from various libraries: toy
regression with a pure Pytorch neural network; large-scale image classification
with torchvision ResNets; graph neural networks based on DGL; and Neural
Radiance Fields built on top of Pytorch3D. Finally, we provide convenient
abstractions for variational continual learning. In all cases the change from a
deterministic to a Bayesian neural network comes with minimal modifications to
existing code, offering a broad range of researchers and practitioners alike
practical access to uncertainty estimation techniques. The library is available
at https://github.com/TyXe-BDL/TyXe.

Comments:
- Previously presented at PROBPROG 2020

---

### Vision-Only Robot Navigation in a Neural Radiance World

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-10-01 by Michal Adamkiewicz | Timothy Chen | Adam Caccavale | Rachel Gardner | Preston Culbertson | Jeannette Bohg | Mac Schwager in cs.RO, [PDF](http://arxiv.org/pdf/2110.00168v2)

**Abstract**: Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm
for the representation of natural, complex 3D scenes. NeRFs represent
continuous volumetric density and RGB values in a neural network, and generate
photo-realistic images from unseen camera viewpoints through ray tracing. We
propose an algorithm for navigating a robot through a 3D environment
represented as a NeRF using only an on-board RGB camera for localization. We
assume the NeRF for the scene has been pre-trained offline, and the robot's
objective is to navigate through unoccupied space in the NeRF to reach a goal
pose. We introduce a trajectory optimization algorithm that avoids collisions
with high-density regions in the NeRF based on a discrete time version of
differential flatness that is amenable to constraining the robot's full pose
and control inputs. We also introduce an optimization based filtering method to
estimate 6DoF pose and velocities for the robot in the NeRF given only an
onboard RGB camera. We combine the trajectory planner with the pose filter in
an online replanning loop to give a vision-based robot navigation pipeline. We
present simulation results with a quadrotor robot navigating through a jungle
gym environment, the inside of a church, and Stonehenge using only an RGB
camera. We also demonstrate an omnidirectional ground robot navigating through
the church, requiring it to reorient to fit through the narrow gap. Videos of
this work can be found at https://mikh3x4.github.io/nerf-navigation/ .

---

### TöRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-09-30 by Benjamin Attal | Eliot Laidlaw | Aaron Gokaslan | Changil Kim | Christian Richardt | James Tompkin | Matthew O'Toole in cs.CV, [PDF](http://arxiv.org/pdf/2109.15271v2)

**Abstract**: Neural networks can represent and accurately reconstruct radiance fields for
static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes
captured with monocular video, with promising performance. However, the
monocular setting is known to be an under-constrained problem, and so methods
rely on data-driven priors for reconstructing dynamic content. We replace these
priors with measurements from a time-of-flight (ToF) camera, and introduce a
neural representation based on an image formation model for continuous-wave ToF
cameras. Instead of working with processed depth maps, we model the raw ToF
sensor measurements to improve reconstruction quality and avoid issues with low
reflectance regions, multi-path interference, and a sensor's limited
unambiguous depth range. We show that this approach improves robustness of
dynamic scene reconstruction to erroneous calibration and large motions, and
discuss the benefits and limitations of integrating RGB+ToF sensors that are
now available on modern smartphones.

Comments:
- Accepted to NeurIPS 2021. Web page: https://imaging.cs.cmu.edu/torf/
  NeurIPS camera ready updates -- added quantitative comparisons to new
  methods, visual side-by-side comparisons performed on larger baseline camera
  sequences

---

### Neural Human Performer: Learning Generalizable Radiance Fields for Human  Performance Rendering

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-09-15 by Youngjoong Kwon | Dahun Kim | Duygu Ceylan | Henry Fuchs in cs.CV, [PDF](http://arxiv.org/pdf/2109.07448v1)

**Abstract**: In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary
human performance using sparse multi-view cameras. Recently, several works have
addressed this problem by learning person-specific neural radiance fields
(NeRF) to capture the appearance of a particular human. In parallel, some work
proposed to use pixel-aligned features to generalize radiance fields to
arbitrary new scenes and objects. Adopting such generalization approaches to
humans, however, is highly challenging due to the heavy occlusions and dynamic
articulations of body parts. To tackle this, we propose Neural Human Performer,
a novel approach that learns generalizable neural radiance fields based on a
parametric human body model for robust performance capture. Specifically, we
first introduce a temporal transformer that aggregates tracked visual features
based on the skeletal body motion over time. Moreover, a multi-view transformer
is proposed to perform cross-attention between the temporally-fused features
and the pixel-aligned features at each time step to integrate observations on
the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets
show that our method significantly outperforms recent generalizable NeRF
methods on unseen identities and poses. The video results and code are
available at https://youngjoongunc.github.io/nhp.

---

### Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit  3D Representations

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-09-05 by Jianxiong Shen | Adria Ruiz | Antonio Agudo | Francesc Moreno-Noguer in cs.CV, [PDF](http://arxiv.org/pdf/2109.02123v3)

**Abstract**: Neural Radiance Fields (NeRF) has become a popular framework for learning
implicit 3D representations and addressing different tasks such as novel-view
synthesis or depth-map estimation. However, in downstream applications where
decisions need to be made based on automatic predictions, it is critical to
leverage the confidence associated with the model estimations. Whereas
uncertainty quantification is a long-standing problem in Machine Learning, it
has been largely overlooked in the recent NeRF literature. In this context, we
propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of
standard NeRF that learns a probability distribution over all the possible
radiance fields modeling the scene. This distribution allows to quantify the
uncertainty associated with the scene information provided by the model. S-NeRF
optimization is posed as a Bayesian learning problem which is efficiently
addressed using the Variational Inference framework. Exhaustive experiments
over benchmark datasets demonstrate that S-NeRF is able to provide more
reliable predictions and confidence values than generic approaches previously
proposed for uncertainty estimation in other domains.

---

### Learning Object-Compositional Neural Radiance Field for Editable Scene  Rendering



Published 2021-09-04 by Bangbang Yang | Yinda Zhang | Yinghao Xu | Yijin Li | Han Zhou | Hujun Bao | Guofeng Zhang | Zhaopeng Cui in cs.CV, [PDF](http://arxiv.org/pdf/2109.01847v1)

**Abstract**: Implicit neural rendering techniques have shown promising results for novel
view synthesis. However, existing methods usually encode the entire scene as a
whole, which is generally not aware of the object identity and limits the
ability to the high-level editing tasks such as moving or adding furniture. In
this paper, we present a novel neural scene rendering system, which learns an
object-compositional neural radiance field and produces realistic rendering
with editing capability for a clustered and real-world scene. Specifically, we
design a novel two-pathway architecture, in which the scene branch encodes the
scene geometry and appearance, and the object branch encodes each standalone
object conditioned on learnable object activation codes. To survive the
training in heavily cluttered scenes, we propose a scene-guided training
strategy to solve the 3D space ambiguity in the occluded regions and learn
sharp boundaries for each object. Extensive experiments demonstrate that our
system not only achieves competitive performance for static scene novel-view
synthesis, but also produces realistic rendering for object-level editing.

Comments:
- Accepted to ICCV 2021. Project Page:
  https://zju3dv.github.io/object_nerf

---

### CodeNeRF: Disentangled Neural Radiance Fields for Object Categories

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-09-03 by Wonbong Jang | Lourdes Agapito in cs.GR, [PDF](http://arxiv.org/pdf/2109.01750v1)

**Abstract**: CodeNeRF is an implicit 3D neural representation that learns the variation of
object shapes and textures across a category and can be trained, from a set of
posed images, to synthesize novel views of unseen objects. Unlike the original
NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture
by learning separate embeddings. At test time, given a single unposed image of
an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and
appearance codes via optimization. Unseen objects can be reconstructed from a
single image, and then rendered from new viewpoints or their shape and texture
edited by varying the latent codes. We conduct experiments on the SRN
benchmark, which show that CodeNeRF generalises well to unseen objects and
achieves on-par performance with methods that require known camera pose at test
time. Our results on real-world images demonstrate that CodeNeRF can bridge the
sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}

Comments:
- 10 pages, 15 figures, ICCV 2021

---

### NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor  Multi-view Stereo

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-09-02 by Yi Wei | Shaohui Liu | Yongming Rao | Wang Zhao | Jiwen Lu | Jie Zhou in cs.CV, [PDF](http://arxiv.org/pdf/2109.01129v3)

**Abstract**: In this work, we present a new multi-view depth estimation method that
utilizes both conventional reconstruction and learning-based priors over the
recently proposed neural radiance fields (NeRF). Unlike existing neural network
based optimization method that relies on estimated correspondences, our method
directly optimizes over implicit volumes, eliminating the challenging step of
matching pixels in indoor scenes. The key to our approach is to utilize the
learning-based priors to guide the optimization process of NeRF. Our system
firstly adapts a monocular depth network over the target scene by finetuning on
its sparse SfM+MVS reconstruction from COLMAP. Then, we show that the
shape-radiance ambiguity of NeRF still exists in indoor environments and
propose to address the issue by employing the adapted depth priors to monitor
the sampling process of volume rendering. Finally, a per-pixel confidence map
acquired by error computation on the rendered image can be used to further
improve the depth quality. Experiments show that our proposed framework
significantly outperforms state-of-the-art methods on indoor scenes, with
surprising findings presented on the effectiveness of correspondence-based
optimization and NeRF-based optimization over the adapted depth priors. In
addition, we show that the guided optimization scheme does not sacrifice the
original synthesis capability of neural radiance fields, improving the
rendering quality on both seen and novel views. Code is available at
https://github.com/weiyithu/NerfingMVS.

Comments:
- To appear in ICCV 2021 (Oral). Project page:
  https://weiyithu.github.io/NerfingMVS/

---

### Self-Calibrating Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-08-31 by Yoonwoo Jeong | Seokjun Ahn | Christopher Choy | Animashree Anandkumar | Minsu Cho | Jaesik Park in cs.CV, [PDF](http://arxiv.org/pdf/2108.13826v2)

**Abstract**: In this work, we propose a camera self-calibration algorithm for generic
cameras with arbitrary non-linear distortions. We jointly learn the geometry of
the scene and the accurate camera parameters without any calibration objects.
Our camera model consists of a pinhole model, a fourth order radial distortion,
and a generic noise model that can learn arbitrary non-linear camera
distortions. While traditional self-calibration algorithms mostly rely on
geometric constraints, we additionally incorporate photometric consistency.
This requires learning the geometry of the scene, and we use Neural Radiance
Fields (NeRF). We also propose a new geometric loss function, viz., projected
ray distance loss, to incorporate geometric consistency for complex non-linear
camera models. We validate our approach on standard real image datasets and
demonstrate that our model can learn the camera intrinsics and extrinsics
(pose) from scratch without COLMAP initialization. Also, we show that learning
accurate camera models in a differentiable manner allows us to improve PSNR
over baselines. Our module is an easy-to-use plugin that can be applied to NeRF
variants to improve performance. The code and data are currently available at
https://github.com/POSTECH-CVLab/SCNeRF.

Comments:
- Accepted in ICCV21, Project Page:
  https://postech-cvlab.github.io/SCNeRF/

---

### iButter: Neural Interactive Bullet Time Generator for Human  Free-viewpoint Rendering

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-08-12 by Liao Wang | Ziyu Wang | Pei Lin | Yuheng Jiang | Xin Suo | Minye Wu | Lan Xu | Jingyi Yu in cs.CV, [PDF](http://arxiv.org/pdf/2108.05577v1)

**Abstract**: Generating ``bullet-time'' effects of human free-viewpoint videos is critical
for immersive visual effects and VR/AR experience. Recent neural advances still
lack the controllable and interactive bullet-time design ability for human
free-viewpoint rendering, especially under the real-time, dynamic and general
setting for our trajectory-aware task. To fill this gap, in this paper we
propose a neural interactive bullet-time generator (iButter) for
photo-realistic human free-viewpoint rendering from dense RGB streams, which
enables flexible and interactive design for human bullet-time visual effects.
Our iButter approach consists of a real-time preview and design stage as well
as a trajectory-aware refinement stage. During preview, we propose an
interactive bullet-time design approach by extending the NeRF rendering to a
real-time and dynamic setting and getting rid of the tedious per-scene
training. To this end, our bullet-time design stage utilizes a hybrid training
set, light-weight network design and an efficient silhouette-based sampling
strategy. During refinement, we introduce an efficient trajectory-aware scheme
within 20 minutes, which jointly encodes the spatial, temporal consistency and
semantic cues along the designed trajectory, achieving photo-realistic
bullet-time viewing experience of human activities. Extensive experiments
demonstrate the effectiveness of our approach for convenient interactive
bullet-time design and photo-realistic human free-viewpoint video generation.

Comments:
- Accepted by ACM MM 2021

---

### FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face  Animation

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-08-10 by ShahRukh Athar | Zhixin Shu | Dimitris Samaras in cs.CV, [PDF](http://arxiv.org/pdf/2108.04913v1)

**Abstract**: This paper presents a neural rendering method for controllable portrait video
synthesis. Recent advances in volumetric neural rendering, such as neural
radiance fields (NeRF), has enabled the photorealistic novel view synthesis of
static scenes with impressive results. However, modeling dynamic and
controllable objects as part of a scene with such scene representations is
still challenging. In this work, we design a system that enables both novel
view synthesis for portrait video, including the human subject and the scene
background, and explicit control of the facial expressions through a
low-dimensional expression representation. We leverage the expression space of
a 3D morphable face model (3DMM) to represent the distribution of human facial
expressions, and use it to condition the NeRF volumetric function. Furthermore,
we impose a spatial prior brought by 3DMM fitting to guide the network to learn
disentangled control for scene appearance and facial actions. We demonstrate
the effectiveness of our method on free view synthesis of portrait videos with
expression controls. To train a scene, our method only requires a short video
of a subject captured by a mobile device.

Comments:
- version 1.0.0

---

### Differentiable Surface Rendering via Non-Differentiable Sampling

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-08-10 by Forrester Cole | Kyle Genova | Avneesh Sud | Daniel Vlasic | Zhoutong Zhang in cs.GR, [PDF](http://arxiv.org/pdf/2108.04886v1)

**Abstract**: We present a method for differentiable rendering of 3D surfaces that supports
both explicit and implicit representations, provides derivatives at occlusion
boundaries, and is fast and simple to implement. The method first samples the
surface using non-differentiable rasterization, then applies differentiable,
depth-aware point splatting to produce the final image. Our approach requires
no differentiable meshing or rasterization steps, making it efficient for large
3D models and applicable to isosurfaces extracted from implicit surface
definitions. We demonstrate the effectiveness of our method for implicit-,
mesh-, and parametric-surface-based inverse rendering and neural-network
training applications. In particular, we show for the first time efficient,
differentiable rendering of an isosurface extracted from a neural radiance
field (NeRF), and demonstrate surface-based, rather than volume-based,
rendering of a NeRF.

Comments:
- Accepted to ICCV 2021

---

### NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-08-09 by Radu Alexandru Rosu | Sven Behnke in cs.CV, [PDF](http://arxiv.org/pdf/2108.03880v2)

**Abstract**: Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge
of novel deep learning methods, learned MVS has surpassed the accuracy of
classical approaches, but still relies on building a memory intensive dense
cost volume. Novel View Synthesis (NVS) is a parallel line of research and has
recently seen an increase in popularity with Neural Radiance Field (NeRF)
models, which optimize a per scene radiance field. However, NeRF methods do not
generalize to novel scenes and are slow to train and test. We propose to bridge
the gap between these two methodologies with a novel network that can recover
3D scene geometry as a distance function, together with high-resolution color
images. Our method uses only a sparse set of images as input and can generalize
well to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing
approach in order to significantly increase speed. We show on various datasets
that our method reaches comparable accuracy to per-scene optimized methods
while being able to generalize and running significantly faster. We provide the
source code at https://github.com/AIS-Bonn/neural_mvs

Comments:
- Accepted for International Joint Conference on Neural Networks
  (IJCNN) 2022. Code available at https://github.com/AIS-Bonn/neural_mvs

---

### A Deep Signed Directional Distance Function for Object Shape  Representation



Published 2021-07-23 by Ehsan Zobeidi | Nikolay Atanasov in cs.CV, [PDF](http://arxiv.org/pdf/2107.11024v2)

**Abstract**: Neural networks that map 3D coordinates to signed distance function (SDF) or
occupancy values have enabled high-fidelity implicit representations of object
shape. This paper develops a new shape model that allows synthesizing novel
distance views by optimizing a continuous signed directional distance function
(SDDF). Similar to deep SDF models, our SDDF formulation can represent whole
categories of shapes and complete or interpolate across shapes from partial
input data. Unlike an SDF, which measures distance to the nearest surface in
any direction, an SDDF measures distance in a given direction. This allows
training an SDDF model without 3D shape supervision, using only distance
measurements, readily available from depth camera or Lidar sensors. Our model
also removes post-processing steps like surface extraction or rendering by
directly predicting distance at arbitrary locations and viewing directions.
Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which
train high-capacity black-box models, our model encodes by construction the
property that SDDF values decrease linearly along the viewing direction. This
structure constraint not only results in dimensionality reduction but also
provides analytical confidence about the accuracy of SDDF predictions,
regardless of the distance to the object surface.

---

### 3D Neural Scene Representations for Visuomotor Control

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-07-08 by Yunzhu Li | Shuang Li | Vincent Sitzmann | Pulkit Agrawal | Antonio Torralba in cs.RO, [PDF](http://arxiv.org/pdf/2107.04004v2)

**Abstract**: Humans have a strong intuitive understanding of the 3D environment around us.
The mental model of the physics in our brain applies to objects of different
materials and enables us to perform a wide range of manipulation tasks that are
far beyond the reach of current robots. In this work, we desire to learn models
for dynamic 3D scenes purely from 2D visual observations. Our model combines
Neural Radiance Fields (NeRF) and time contrastive learning with an
autoencoding framework, which learns viewpoint-invariant 3D-aware scene
representations. We show that a dynamics model, constructed over the learned
representation space, enables visuomotor control for challenging manipulation
tasks involving both rigid bodies and fluids, where the target is specified in
a viewpoint different from what the robot operates on. When coupled with an
auto-decoding framework, it can even support goal specification from camera
viewpoints that are outside the training distribution. We further demonstrate
the richness of the learned 3D dynamics model by performing future prediction
and novel view synthesis. Finally, we provide detailed ablation studies
regarding different system designs and qualitative analysis of the learned
representations.

Comments:
- Accepted to Conference on Robot Learning (CoRL 2021) as Oral
  Presentation. The first two authors contributed equally. Project Page:
  https://3d-representation-learning.github.io/nerf-dy/

---

### Depth-supervised NeRF: Fewer Views and Faster Training for Free

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-07-06 by Kangle Deng | Andrew Liu | Jun-Yan Zhu | Deva Ramanan in cs.CV, [PDF](http://arxiv.org/pdf/2107.02791v2)

**Abstract**: A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting
incorrect geometries when given an insufficient number of input views. One
potential reason is that standard volumetric rendering does not enforce the
constraint that most of a scene's geometry consist of empty space and opaque
surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised
Neural Radiance Fields), a loss for learning radiance fields that takes
advantage of readily-available depth supervision. We leverage the fact that
current NeRF pipelines require images with known camera poses that are
typically estimated by running structure-from-motion (SFM). Crucially, SFM also
produces sparse 3D points that can be used as "free" depth supervision during
training: we add a loss to encourage the distribution of a ray's terminating
depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can
render better images given fewer training views while training 2-3x faster.
Further, we show that our loss is compatible with other recently proposed NeRF
methods, demonstrating that depth is a cheap and easily digestible supervisory
signal. And finally, we find that DS-NeRF can support other types of depth
supervision such as scanned depth sensors and RGB-D reconstruction outputs.

Comments:
- Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:
  https://github.com/dunbar12138/DSNeRF

---

### Animatable Neural Radiance Fields from Monocular RGB Videos

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-06-25 by Jianchuan Chen | Ying Zhang | Di Kang | Xuefei Zhe | Linchao Bao | Xu Jia | Huchuan Lu in cs.CV, [PDF](http://arxiv.org/pdf/2106.13629v2)

**Abstract**: We present animatable neural radiance fields (animatable NeRF) for detailed
human avatar creation from monocular videos. Our approach extends neural
radiance fields (NeRF) to the dynamic scenes with human movements via
introducing explicit pose-guided deformation while learning the scene
representation network. In particular, we estimate the human pose for each
frame and learn a constant canonical space for the detailed human template,
which enables natural shape deformation from the observation space to the
canonical space under the explicit control of the pose parameters. To
compensate for inaccurate pose estimation, we introduce the pose refinement
strategy that updates the initial pose during the learning process, which not
only helps to learn more accurate human reconstruction but also accelerates the
convergence. In experiments we show that the proposed approach achieves 1)
implicit human geometry and appearance reconstruction with high-quality
details, 2) photo-realistic rendering of the human from novel views, and 3)
animation of the human with novel poses.

Comments:
- 12 pages, 12 figures

---

### Scene Uncertainty and the Wellington Posterior of Deterministic Image  Classifiers



Published 2021-06-25 by Stephanie Tsuei | Aditya Golatkar | Stefano Soatto in cs.CV, [PDF](http://arxiv.org/pdf/2106.13870v2)

**Abstract**: We propose a method to estimate the uncertainty of the outcome of an image
classifier on a given input datum. Deep neural networks commonly used for image
classification are deterministic maps from an input image to an output class.
As such, their outcome on a given datum involves no uncertainty, so we must
specify what variability we are referring to when defining, measuring and
interpreting uncertainty, and attributing "confidence" to the outcome. To this
end, we introduce the Wellington Posterior, which is the distribution of
outcomes that would have been obtained in response to data that could have been
generated by the same scene that produced the given image. Since there are
infinitely many scenes that could have generated any given image, the
Wellington Posterior involves inductive transfer from scenes other than the one
portrayed. We explore the use of data augmentation, dropout, ensembling,
single-view reconstruction, and model linearization to compute a Wellington
Posterior. Additional methods include the use of conditional generative models
such as generative adversarial networks, neural radiance fields, and
conditional prior networks. We test these methods against the empirical
posterior obtained by performing inference on multiple images of the same
underlying scene. These developments are only a small step towards assessing
the reliability of deep network classifiers in a manner that is compatible with
safety-critical applications and human interpretation.

---

### HyperNeRF: A Higher-Dimensional Representation for Topologically Varying  Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-06-24 by Keunhong Park | Utkarsh Sinha | Peter Hedman | Jonathan T. Barron | Sofien Bouaziz | Dan B Goldman | Ricardo Martin-Brualla | Steven M. Seitz in cs.CV, [PDF](http://arxiv.org/pdf/2106.13228v2)

**Abstract**: Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this "hyper-space". Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between "moments",
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for
interpolation and 8.6% for novel-view synthesis, as measured by LPIPS.
Additional videos, results, and visualizations are available at
https://hypernerf.github.io.

Comments:
- SIGGRAPH Asia 2021, Project page: https://hypernerf.github.io/

---

### Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single  Panorama

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-06-21 by Ching-Yu Hsu | Cheng Sun | Hwann-Tzong Chen in cs.CV, [PDF](http://arxiv.org/pdf/2106.10859v1)

**Abstract**: We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first
method to the application of parallax-enabled novel panoramic view synthesis.
Recent works for novel view synthesis focus on perspective images with limited
field-of-view and require sufficient pictures captured in a specific condition.
Conversely, OmniNeRF can generate panorama images for unknown viewpoints given
a single equirectangular image as training data. To this end, we propose to
augment the single RGB-D panorama by projecting back and forth between a 3D
world and different 2D panoramic coordinates at different virtual camera
positions. By doing so, we are able to optimize an Omnidirectional Neural
Radiance Field with visible pixels collecting from omnidirectional viewing
angles at a fixed center for the estimation of new viewing angles from varying
camera positions. As a result, the proposed OmniNeRF achieves convincing
renderings of novel panoramic views that exhibit the parallax effect. We
showcase the effectiveness of each of our proposals on both synthetic and
real-world datasets.

---

### NeuS: Learning Neural Implicit Surfaces by Volume Rendering for  Multi-view Reconstruction

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-06-20 by Peng Wang | Lingjie Liu | Yuan Liu | Christian Theobalt | Taku Komura | Wenping Wang in cs.CV, [PDF](http://arxiv.org/pdf/2106.10689v3)

**Abstract**: We present a novel neural surface reconstruction method, called NeuS, for
reconstructing objects and scenes with high fidelity from 2D image inputs.
Existing neural surface reconstruction approaches, such as DVR and IDR, require
foreground mask as supervision, easily get trapped in local minima, and
therefore struggle with the reconstruction of objects with severe
self-occlusion or thin structures. Meanwhile, recent neural methods for novel
view synthesis, such as NeRF and its variants, use volume rendering to produce
a neural scene representation with robustness of optimization, even for highly
complex objects. However, extracting high-quality surfaces from this learned
implicit representation is difficult because there are not sufficient surface
constraints in the representation. In NeuS, we propose to represent a surface
as the zero-level set of a signed distance function (SDF) and develop a new
volume rendering method to train a neural SDF representation. We observe that
the conventional volume rendering method causes inherent geometric errors (i.e.
bias) for surface reconstruction, and therefore propose a new formulation that
is free of bias in the first order of approximation, thus leading to more
accurate surface reconstruction even without the mask supervision. Experiments
on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the
state-of-the-arts in high-quality surface reconstruction, especially for
objects and scenes with complex structures and self-occlusion.

Comments:
- 23 pages

---

### NeRF in detail: Learning to sample for view synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-06-09 by Relja Arandjelović | Andrew Zisserman in cs.CV, [PDF](http://arxiv.org/pdf/2106.05264v1)

**Abstract**: Neural radiance fields (NeRF) methods have demonstrated impressive novel view
synthesis performance. The core approach is to render individual rays by
querying a neural network at points sampled along the ray to obtain the density
and colour of the sampled points, and integrating this information using the
rendering equation. Since dense sampling is computationally prohibitive, a
common solution is to perform coarse-to-fine sampling.
  In this work we address a clear limitation of the vanilla coarse-to-fine
approach -- that it is based on a heuristic and not trained end-to-end for the
task at hand. We introduce a differentiable module that learns to propose
samples and their importance for the fine network, and consider and compare
multiple alternatives for its neural architecture. Training the proposal module
from scratch can be unstable due to lack of supervision, so an effective
pre-training strategy is also put forward. The approach, named `NeRF in detail'
(NeRF-ID), achieves superior view synthesis quality over NeRF and the
state-of-the-art on the synthetic Blender benchmark and on par or better
performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the
predicted sample importance, a 25% saving in computation can be achieved
without significantly sacrificing the rendering quality.

---

### Neural Actor: Neural Free-view Synthesis of Human Actors with Pose  Control



Published 2021-06-03 by Lingjie Liu | Marc Habermann | Viktor Rudnev | Kripasindhu Sarkar | Jiatao Gu | Christian Theobalt in cs.CV, [PDF](http://arxiv.org/pdf/2106.02019v2)

**Abstract**: We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.

---

### NeRFactor: Neural Factorization of Shape and Reflectance Under an  Unknown Illumination

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-06-03 by Xiuming Zhang | Pratul P. Srinivasan | Boyang Deng | Paul Debevec | William T. Freeman | Jonathan T. Barron in cs.CV, [PDF](http://arxiv.org/pdf/2106.01970v2)

**Abstract**: We address the problem of recovering the shape and spatially-varying
reflectance of an object from multi-view images (and their camera poses) of an
object illuminated by one unknown lighting condition. This enables the
rendering of novel views of the object under arbitrary environment lighting and
editing of the object's material properties. The key to our approach, which we
call Neural Radiance Factorization (NeRFactor), is to distill the volumetric
geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020]
representation of the object into a surface representation and then jointly
refine the geometry while solving for the spatially-varying reflectance and
environment lighting. Specifically, NeRFactor recovers 3D neural fields of
surface normals, light visibility, albedo, and Bidirectional Reflectance
Distribution Functions (BRDFs) without any supervision, using only a
re-rendering loss, simple smoothness priors, and a data-driven BRDF prior
learned from real-world BRDF measurements. By explicitly modeling light
visibility, NeRFactor is able to separate shadows from albedo and synthesize
realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor
is able to recover convincing 3D models for free-viewpoint relighting in this
challenging and underconstrained capture setup for both synthetic and real
scenes. Qualitative and quantitative experiments show that NeRFactor
outperforms classic and deep learning-based state of the art across various
tasks. Our videos, code, and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.

Comments:
- Camera-ready version for SIGGRAPH Asia 2021. Project Page:
  https://people.csail.mit.edu/xiuming/projects/nerfactor/

---

### Stylizing 3D Scene via Implicit Representation and HyperNetwork

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-05-27 by Pei-Ze Chiang | Meng-Shiun Tsai | Hung-Yu Tseng | Wei-sheng Lai | Wei-Chen Chiu in cs.CV, [PDF](http://arxiv.org/pdf/2105.13016v3)

**Abstract**: In this work, we aim to address the 3D scene stylization problem - generating
stylized images of the scene at arbitrary novel view angles. A straightforward
solution is to combine existing novel view synthesis and image/video style
transfer approaches, which often leads to blurry results or inconsistent
appearance. Inspired by the high-quality results of the neural radiance fields
(NeRF) method, we propose a joint framework to directly render novel views with
the desired style. Our framework consists of two components: an implicit
representation of the 3D scene with the neural radiance fields model, and a
hypernetwork to transfer the style information into the scene representation.
In particular, our implicit representation model disentangles the scene into
the geometry and appearance branches, and the hypernetwork learns to predict
the parameters of the appearance branch from the reference style image. To
alleviate the training difficulties and memory burden, we propose a two-stage
training procedure and a patch sub-sampling approach to optimize the style and
content losses with the neural radiance fields model. After optimization, our
model is able to render consistent novel views at arbitrary view angles with
arbitrary style. Both quantitative evaluation and human subject study have
demonstrated that the proposed method generates faithful stylization results
with consistent appearance across different views.

Comments:
- Accepted to WACV2022; Project page:
  https://ztex08010518.github.io/3dstyletransfer/

---

### Recursive-NeRF: An Efficient and Dynamically Growing NeRF

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-05-19 by Guo-Wei Yang | Wen-Yang Zhou | Hao-Yang Peng | Dun Liang | Tai-Jiang Mu | Shi-Min Hu in cs.CV, [PDF](http://arxiv.org/pdf/2105.09103v1)

**Abstract**: View synthesis methods using implicit continuous shape representations
learned from a set of images, such as the Neural Radiance Field (NeRF) method,
have gained increasing attention due to their high quality imagery and
scalability to high resolution. However, the heavy computation required by its
volumetric approach prevents NeRF from being useful in practice; minutes are
taken to render a single image of a few megapixels. Now, an image of a scene
can be rendered in a level-of-detail manner, so we posit that a complicated
region of the scene should be represented by a large neural network while a
small neural network is capable of encoding a simple region, enabling a balance
between efficiency and quality. Recursive-NeRF is our embodiment of this idea,
providing an efficient and adaptive rendering and training approach for NeRF.
The core of Recursive-NeRF learns uncertainties for query coordinates,
representing the quality of the predicted color and volumetric intensity at
each level. Only query coordinates with high uncertainties are forwarded to the
next level to a bigger neural network with a more powerful representational
capability. The final rendered image is a composition of results from neural
networks of all levels. Our evaluation on three public datasets shows that
Recursive-NeRF is more efficient than NeRF while providing state-of-the-art
quality. The code will be available at https://github.com/Gword/Recursive-NeRF.

Comments:
- 11 pages, 12 figures

---

### Editing Conditional Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-05-13 by Steven Liu | Xiuming Zhang | Zhoutong Zhang | Richard Zhang | Jun-Yan Zhu | Bryan Russell in cs.CV, [PDF](http://arxiv.org/pdf/2105.06466v2)

**Abstract**: A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.

Comments:
- Code: https://github.com/stevliu/editnerf Website:
  http://editnerf.csail.mit.edu/, v2 updated figure 8 and included additional
  details

---

### Dynamic View Synthesis from Dynamic Monocular Video

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-05-13 by Chen Gao | Ayush Saraf | Johannes Kopf | Jia-Bin Huang in cs.CV, [PDF](http://arxiv.org/pdf/2105.06468v1)

**Abstract**: We present an algorithm for generating novel views at arbitrary viewpoints
and any input time step given a monocular video of a dynamic scene. Our work
builds upon recent advances in neural implicit representation and uses
continuous and differentiable functions for modeling the time-varying structure
and the appearance of the scene. We jointly train a time-invariant static NeRF
and a time-varying dynamic NeRF, and learn how to blend the results in an
unsupervised manner. However, learning this implicit function from a single
video is highly ill-posed (with infinitely many solutions that match the input
video). To resolve the ambiguity, we introduce regularization losses to
encourage a more physically plausible solution. We show extensive quantitative
and qualitative results of dynamic view synthesis from casually captured
videos.

Comments:
- Project webpage: https://free-view-video.github.io/

---

### Neural Trajectory Fields for Dynamic Novel View Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-05-12 by Chaoyang Wang | Ben Eckart | Simon Lucey | Orazio Gallo in cs.CV, [PDF](http://arxiv.org/pdf/2105.05994v1)

**Abstract**: Recent approaches to render photorealistic views from a limited set of
photographs have pushed the boundaries of our interactions with pictures of
static scenes. The ability to recreate moments, that is, time-varying
sequences, is perhaps an even more interesting scenario, but it remains largely
unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for
dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input
sequence for each point in space. This allows us to enforce consistency between
any two frames in the sequence, which results in high quality reconstruction,
particularly in dynamic regions.

---

### Vision-based Neural Scene Representations for Spacecraft

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-05-11 by Anne Mergy | Gurvan Lecuyer | Dawa Derksen | Dario Izzo in cs.CV, [PDF](http://arxiv.org/pdf/2105.06405v1)

**Abstract**: In advanced mission concepts with high levels of autonomy, spacecraft need to
internally model the pose and shape of nearby orbiting objects. Recent works in
neural scene representations show promising results for inferring generic
three-dimensional scenes from optical images. Neural Radiance Fields (NeRF)
have shown success in rendering highly specular surfaces using a large number
of images and their pose. More recently, Generative Radiance Fields (GRAF)
achieved full volumetric reconstruction of a scene from unposed images only,
thanks to the use of an adversarial framework to train a NeRF. In this paper,
we compare and evaluate the potential of NeRF and GRAF to render novel views
and extract the 3D shape of two different spacecraft, the Soil Moisture and
Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube
sat. Considering the best performances of both models, we observe that NeRF has
the ability to render more accurate images regarding the material specularity
of the spacecraft and its pose. For its part, GRAF generates precise novel
views with accurate details even when parts of the satellites are shadowed
while having the significant advantage of not needing any information about the
relative pose.

---

### Neural 3D Scene Compression via Model Compression



Published 2021-05-07 by Berivan Isik in cs.CV, [PDF](http://arxiv.org/pdf/2105.03120v1)

**Abstract**: Rendering 3D scenes requires access to arbitrary viewpoints from the scene.
Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken
from the 3D scene that can reconstruct the scene back through interpolations,
or (2) storing a representation of the 3D scene itself that already encodes
views from all directions. So far, traditional 3D compression methods have
focused on the first type of storage and compressed the original 2D images with
image compression techniques. With this approach, the user first decodes the
stored 2D images and then renders the 3D scene. However, this separated
procedure is inefficient since a large amount of 2D images have to be stored.
In this work, we take a different approach and compress a functional
representation of 3D scenes. In particular, we introduce a method to compress
3D scenes by compressing the neural networks that represent the scenes as
neural radiance fields. Our method provides more efficient storage of 3D scenes
since it does not store 2D images -- which are redundant when we render the
scene from the neural functional representation.

Comments:
- Stanford CS 231A Final Project, 2021. WiCV at CVPR 2021

---

### Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-05-06 by Sida Peng | Junting Dong | Qianqian Wang | Shangzhan Zhang | Qing Shuai | Xiaowei Zhou | Hujun Bao in cs.CV, [PDF](http://arxiv.org/pdf/2105.02872v2)

**Abstract**: This paper addresses the challenge of reconstructing an animatable human
model from a multi-view video. Some recent works have proposed to decompose a
non-rigidly deforming scene into a canonical neural radiance field and a set of
deformation fields that map observation-space points to the canonical space,
thereby enabling them to learn the dynamic scene from images. However, they
represent the deformation field as translational vector field or SE(3) field,
which makes the optimization highly under-constrained. Moreover, these
representations cannot be explicitly controlled by input motions. Instead, we
introduce neural blend weight fields to produce the deformation fields. Based
on the skeleton-driven deformation, blend weight fields are used with 3D human
skeletons to generate observation-to-canonical and canonical-to-observation
correspondences. Since 3D human skeletons are more observable, they can
regularize the learning of deformation fields. Moreover, the learned blend
weight fields can be combined with input skeletal motions to generate new
deformation fields to animate the human model. Experiments show that our
approach significantly outperforms recent human synthesis methods. The code and
supplementary materials are available at
https://zju3dv.github.io/animatable_nerf/.

Comments:
- Accepted to ICCV 2021. The first two authors contributed equally to
  this paper. Project page: https://zju3dv.github.io/animatable_nerf/

---

### Editable Free-viewpoint Video Using a Layered Neural Representation

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-30 by Jiakai Zhang | Xinhang Liu | Xinyi Ye | Fuqiang Zhao | Yanshun Zhang | Minye Wu | Yingliang Zhang | Lan Xu | Jingyi Yu in cs.CV, [PDF](http://arxiv.org/pdf/2104.14786v1)

**Abstract**: Generating free-viewpoint videos is critical for immersive VR/AR experience
but recent neural advances still lack the editing ability to manipulate the
visual perception for large dynamic scenes. To fill this gap, in this paper we
propose the first approach for editable photo-realistic free-viewpoint video
generation for large-scale dynamic scenes using only sparse 16 cameras. The
core of our approach is a new layered neural representation, where each dynamic
entity including the environment itself is formulated into a space-time
coherent neural layered radiance representation called ST-NeRF. Such layered
representation supports fully perception and realistic manipulation of the
dynamic scene whilst still supporting a free viewing experience in a wide
range. In our ST-NeRF, the dynamic entity/layer is represented as continuous
functions, which achieves the disentanglement of location, deformation as well
as the appearance of the dynamic entity in a continuous and self-supervised
manner. We propose a scene parsing 4D label map tracking to disentangle the
spatial information explicitly, and a continuous deform module to disentangle
the temporal motion implicitly. An object-aware volume rendering scheme is
further introduced for the re-assembling of all the neural layers. We adopt a
novel layered loss and motion-aware ray sampling strategy to enable efficient
training for a large dynamic scene with multiple performers, Our framework
further enables a variety of editing functions, i.e., manipulating the scale
and location, duplicating or retiming individual neural layers to create
numerous visual effects while preserving high realism. Extensive experiments
demonstrate the effectiveness of our approach to achieve high-quality,
photo-realistic, and editable free-viewpoint video generation for dynamic
scenes.

---

### UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for  Multi-View Reconstruction

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-20 by Michael Oechsle | Songyou Peng | Andreas Geiger in cs.CV, [PDF](http://arxiv.org/pdf/2104.10078v2)

**Abstract**: Neural implicit 3D representations have emerged as a powerful paradigm for
reconstructing surfaces from multi-view images and synthesizing novel views.
Unfortunately, existing methods such as DVR or IDR require accurate per-pixel
object masks as supervision. At the same time, neural radiance fields have
revolutionized novel view synthesis. However, NeRF's estimated volume density
does not admit accurate surface reconstruction. Our key insight is that
implicit surface models and radiance fields can be formulated in a unified way,
enabling both surface and volume rendering using the same model. This unified
perspective enables novel, more efficient sampling procedures and the ability
to reconstruct accurate surfaces without input masks. We compare our method on
the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments
demonstrate that we outperform NeRF in terms of reconstruction quality while
performing on par with IDR without requiring masks.

Comments:
- ICCV 2021 oral

---

### Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-20 by Dawa Derksen | Dario Izzo in cs.CV, [PDF](http://arxiv.org/pdf/2104.09877v1)

**Abstract**: We present a new generic method for shadow-aware multi-view satellite
photogrammetry of Earth Observation scenes. Our proposed method, the Shadow
Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric
representation learning. For each scene, we train S-NeRF using very high
spatial resolution optical images taken from known viewing angles. The learning
requires no labels or shape priors: it is self-supervised by an image
reconstruction loss. To accommodate for changing light source conditions both
from a directional light source (the Sun) and a diffuse light source (the sky),
we extend the NeRF approach in two ways. First, direct illumination from the
Sun is modeled via a local light source visibility field. Second, indirect
illumination from a diffuse light source is learned as a non-local color field
as a function of the position of the Sun. Quantitatively, the combination of
these factors reduces the altitude and color errors in shaded areas, compared
to NeRF. The S-NeRF methodology not only performs novel view synthesis and full
3D shape estimation, it also enables shadow detection, albedo synthesis, and
transient object filtering, without any explicit shape supervision.

Comments:
- Accepted to CVPR2021 - EarthVision

---

### FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category  Modelling

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-17 by Christopher Xie | Keunhong Park | Ricardo Martin-Brualla | Matthew Brown in cs.CV, [PDF](http://arxiv.org/pdf/2104.08418v1)

**Abstract**: We investigate the use of Neural Radiance Fields (NeRF) to learn high quality
3D object category models from collections of input images. In contrast to
previous work, we are able to do this whilst simultaneously separating
foreground objects from their varying backgrounds. We achieve this via a
2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a
geometrically constant background and a deformable foreground that represents
the object category. We show that this method can learn accurate 3D object
category models using only photometric supervision and casually captured images
of the objects. Additionally, our 2-part decomposition allows the model to
perform accurate and crisp amodal segmentation. We quantitatively evaluate our
method with view synthesis and image fidelity metrics, using synthetic,
lab-captured, and in-the-wild data. Our results demonstrate convincing 3D
object category modelling that exceed the performance of existing methods.

---

### Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views  of Novel Scenes

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-14 by Julian Chibane | Aayush Bansal | Verica Lazova | Gerard Pons-Moll in cs.CV, [PDF](http://arxiv.org/pdf/2104.06935v1)

**Abstract**: Recent neural view synthesis methods have achieved impressive quality and
realism, surpassing classical pipelines which rely on multi-view
reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a
single scene with a neural network and require dense multi-view inputs. Testing
on a new scene requires re-training from scratch, which takes 2-3 days. In this
work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis
approach that is trained end-to-end, generalizes to new scenes, and requires
only sparse views at test time. The core idea is a neural architecture inspired
by classical multi-view stereo methods, which estimates surface points by
finding similar image regions in stereo images. In SRF, we predict color and
density for each 3D point given an encoding of its stereo correspondence in the
input images. The encoding is implicitly learned by an ensemble of pair-wise
similarities -- emulating classical stereo. Experiments show that SRF learns
structure instead of overfitting on a scene. We train on multiple scenes of the
DTU dataset and generalize to new ones without re-training, requiring only 10
sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning
further improve the results, achieving significantly sharper, more detailed
results than scene-specific models. The code, model, and videos are available
at https://virtualhumans.mpi-inf.mpg.de/srf/.

Comments:
- IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2021

---

### BARF: Bundle-Adjusting Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-13 by Chen-Hsuan Lin | Wei-Chiu Ma | Antonio Torralba | Simon Lucey in cs.CV, [PDF](http://arxiv.org/pdf/2104.06405v2)

**Abstract**: Neural Radiance Fields (NeRF) have recently gained a surge of interest within
the computer vision community for its power to synthesize photorealistic novel
views of real-world scenes. One limitation of NeRF, however, is its requirement
of accurate camera poses to learn the scene representations. In this paper, we
propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from
imperfect (or even unknown) camera poses -- the joint problem of learning
neural 3D representations and registering camera frames. We establish a
theoretical connection to classical image alignment and show that
coarse-to-fine registration is also applicable to NeRF. Furthermore, we show
that na\"ively applying positional encoding in NeRF has a negative impact on
registration with a synthesis-based objective. Experiments on synthetic and
real-world data show that BARF can effectively optimize the neural scene
representations and resolve large camera pose misalignment at the same time.
This enables view synthesis and localization of video sequences from unknown
camera poses, opening up new avenues for visual localization systems (e.g.
SLAM) and potential applications for dense 3D mapping and reconstruction.

Comments:
- Accepted to ICCV 2021 as oral presentation (project page & code:
  https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF)

---

### Neural RGB-D Surface Reconstruction

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-09 by Dejan Azinović | Ricardo Martin-Brualla | Dan B Goldman | Matthias Nießner | Justus Thies in cs.CV, [PDF](http://arxiv.org/pdf/2104.04532v3)

**Abstract**: Obtaining high-quality 3D reconstructions of room-scale scenes is of
paramount importance for upcoming applications in AR or VR. These range from
mixed reality applications for teleconferencing, virtual measuring, virtual
room planing, to robotic applications. While current volume-based view
synthesis methods that use neural radiance fields (NeRFs) show promising
results in reproducing the appearance of an object or scene, they do not
reconstruct an actual surface. The volumetric representation of the surface
based on densities leads to artifacts when a surface is extracted using
Marching Cubes, since during optimization, densities are accumulated along the
ray and are not used at a single sample point in isolation. Instead of this
volumetric representation of the surface, we propose to represent the surface
using an implicit function (truncated signed distance function). We show how to
incorporate this representation in the NeRF framework, and extend it to use
depth measurements from a commodity RGB-D sensor, such as a Kinect. In
addition, we propose a pose and camera refinement technique which improves the
overall reconstruction quality. In contrast to concurrent work on integrating
depth priors in NeRF which concentrates on novel view synthesis, our approach
is able to reconstruct high-quality, metrical 3D reconstructions.

Comments:
- CVPR'22; Project page:
  https://dazinovic.github.io/neural-rgbd-surface-reconstruction/ Video:
  https://youtu.be/iWuSowPsC3g

---

### MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror  Catadioptric Imaging

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-06 by Ziyu Wang | Liao Wang | Fuqiang Zhao | Minye Wu | Lan Xu | Jingyi Yu in cs.CV, [PDF](http://arxiv.org/pdf/2104.02607v2)

**Abstract**: Photo-realistic neural reconstruction and rendering of the human portrait are
critical for numerous VR/AR applications. Still, existing solutions inherently
rely on multi-view capture settings, and the one-shot solution to get rid of
the tedious multi-view synchronization and calibration remains extremely
challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait
free-viewpoint rendering approach using a catadioptric imaging system with
multiple sphere mirrors and a single high-resolution digital camera, which is
the first to combine neural radiance field with catadioptric imaging so as to
enable one-shot photo-realistic human portrait reconstruction and rendering, in
a low-cost and casual capture setting. More specifically, we propose a
light-weight catadioptric system design with a sphere mirror array to enable
diverse ray sampling in the continuous 3D space as well as an effective online
calibration for the camera and the mirror array. Our catadioptric imaging
system can be easily deployed with a low budget and the casual capture ability
for convenient daily usages. We introduce a novel neural warping radiance field
representation to learn a continuous displacement field that implicitly
compensates for the misalignment due to our flexible system setting. We further
propose a density regularization scheme to leverage the inherent geometry
information from the catadioptric data in a self-supervision manner, which not
only improves the training efficiency but also provides more effective density
supervision for higher rendering quality. Extensive experiments demonstrate the
effectiveness and robustness of our scheme to achieve one-shot photo-realistic
and high-quality appearance free-viewpoint rendering for human portrait scenes.

---

### Convolutional Neural Opacity Radiance Fields



Published 2021-04-05 by Haimin Luo | Anpei Chen | Qixuan Zhang | Bai Pang | Minye Wu | Lan Xu | Jingyi Yu in cs.CV, [PDF](http://arxiv.org/pdf/2104.01772v1)

**Abstract**: Photo-realistic modeling and rendering of fuzzy objects with complex opacity
are critical for numerous immersive VR/AR applications, but it suffers from
strong view-dependent brightness, color. In this paper, we propose a novel
scheme to generate opacity radiance fields with a convolutional neural renderer
for fuzzy objects, which is the first to combine both explicit opacity
supervision and convolutional mechanism into the neural radiance field
framework so as to enable high-quality appearance and global consistent alpha
mattes generation in arbitrary novel views. More specifically, we propose an
efficient sampling strategy along with both the camera rays and image plane,
which enables efficient radiance field sampling and learning in a patch-wise
manner, as well as a novel volumetric feature integration scheme that generates
per-patch hybrid feature embeddings to reconstruct the view-consistent
fine-detailed appearance and opacity output. We further adopt a patch-wise
adversarial training scheme to preserve both high-frequency appearance and
opacity details in a self-supervised framework. We also introduce an effective
multi-view image capture system to capture high-quality color and alpha maps
for challenging fuzzy objects. Extensive experiments on existing and our new
challenging fuzzy object dataset demonstrate that our method achieves
photo-realistic, globally consistent, and fined detailed appearance and opacity
free-viewpoint rendering for various fuzzy objects.

---

### Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-02 by Karl Stelzner | Kristian Kersting | Adam R. Kosiorek in cs.CV, [PDF](http://arxiv.org/pdf/2104.01148v1)

**Abstract**: We present ObSuRF, a method which turns a single image of a scene into a 3D
model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF
corresponding to a different object. A single forward pass of an encoder
network outputs a set of latent vectors describing the objects in the scene.
These vectors are used independently to condition a NeRF decoder, defining the
geometry and appearance of each object. We make learning more computationally
efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs
without explicit ray marching. After confirming that the model performs equal
or better than state of the art on three 2D image segmentation benchmarks, we
apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a
novel dataset in which scenes are populated by ShapeNet models. We find that
after training ObSuRF on RGB-D views of training scenes, it is capable of not
only recovering the 3D geometry of a scene depicted in a single input image,
but also to segment it into objects, despite receiving no supervision in that
regard.

Comments:
- 15 pages, 3 figures. For project page with videos, see
  http://stelzner.github.io/obsurf/

---

### Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-01 by Ajay Jain | Matthew Tancik | Pieter Abbeel in cs.CV, [PDF](http://arxiv.org/pdf/2104.00677v1)

**Abstract**: We present DietNeRF, a 3D neural scene representation estimated from a few
images. Neural Radiance Fields (NeRF) learn a continuous volumetric
representation of a scene through multi-view consistency, and can be rendered
from novel viewpoints by ray casting. While NeRF has an impressive ability to
reconstruct geometry and fine details given many images, up to 100 for
challenging 360{\deg} scenes, it often finds a degenerate solution to its image
reconstruction objective when only a few input views are available. To improve
few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic
consistency loss that encourages realistic renderings at novel poses. DietNeRF
is trained on individual scenes to (1) correctly render given input views from
the same pose, and (2) match high-level semantic attributes across different,
random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary
poses. We extract these semantics using a pre-trained visual encoder such as
CLIP, a Vision Transformer trained on hundreds of millions of diverse
single-view, 2D photographs mined from the web with natural language
supervision. In experiments, DietNeRF improves the perceptual quality of
few-shot view synthesis when learned from scratch, can render novel views with
as few as one observed image when pre-trained on a multi-view dataset, and
produces plausible completions of completely unobserved regions.

Comments:
- Project website: https://www.ajayj.com/dietnerf

---

### NeRF-VAE: A Geometry Aware 3D Scene Generative Model

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-04-01 by Adam R. Kosiorek | Heiko Strathmann | Daniel Zoran | Pol Moreno | Rosalia Schneider | Soňa Mokrá | Danilo J. Rezende in stat.ML, [PDF](http://arxiv.org/pdf/2104.00587v1)

**Abstract**: We propose NeRF-VAE, a 3D scene generative model that incorporates geometric
structure via NeRF and differentiable volume rendering. In contrast to NeRF,
our model takes into account shared structure across scenes, and is able to
infer the structure of a novel scene -- without the need to re-train -- using
amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts
previous generative models with convolution-based rendering which lacks
geometric structure. Our model is a VAE that learns a distribution over
radiance fields by conditioning them on a latent scene representation. We show
that, once trained, NeRF-VAE is able to infer and render
geometrically-consistent scenes from previously unseen 3D environments using
very few input images. We further demonstrate that NeRF-VAE generalizes well to
out-of-distribution cameras, while convolutional models do not. Finally, we
introduce and study an attention-based conditioning mechanism of NeRF-VAE's
decoder, which improves model performance.

Comments:
- 17 pages, 15 figures, under review

---

### CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields



Published 2021-03-31 by Michael Niemeyer | Andreas Geiger in cs.CV, [PDF](http://arxiv.org/pdf/2103.17269v1)

**Abstract**: Tremendous progress in deep generative models has led to photorealistic image
synthesis. While achieving compelling results, most approaches operate in the
two-dimensional image domain, ignoring the three-dimensional nature of our
world. Several recent works therefore propose generative models which are
3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to
the image plane. This leads to impressive 3D consistency, but incorporating
such a bias comes at a price: the camera needs to be modeled as well. Current
approaches assume fixed intrinsics and a predefined prior over camera pose
ranges. As a result, parameter tuning is typically required for real-world
data, and results degrade if the data distribution is not matched. Our key
hypothesis is that learning a camera generator jointly with the image generator
leads to a more principled approach to 3D-aware image synthesis. Further, we
propose to decompose the scene into a background and foreground model, leading
to more efficient and disentangled scene representations. While training from
raw, unposed image collections, we learn a 3D- and camera-aware generative
model which faithfully recovers not only the image but also the camera data
distribution. At test time, our model generates images with explicit control
over the camera as well as the shape and appearance of the scene.

---

### FoV-NeRF: Foveated Neural Radiance Fields for Virtual Reality

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-30 by Nianchen Deng | Zhenyi He | Jiannan Ye | Budmonde Duinkharjav | Praneeth Chakravarthula | Xubo Yang | Qi Sun in cs.GR, [PDF](http://arxiv.org/pdf/2103.16365v2)

**Abstract**: Virtual Reality (VR) is becoming ubiquitous with the rise of consumer
displays and commercial VR platforms. Such displays require low latency and
high quality rendering of synthetic imagery with reduced compute overheads.
Recent advances in neural rendering showed promise of unlocking new
possibilities in 3D computer graphics via image-based representations of
virtual or physical environments. Specifically, the neural radiance fields
(NeRF) demonstrated that photo-realistic quality and continuous view changes of
3D scenes can be achieved without loss of view-dependent effects. While NeRF
can significantly benefit rendering for VR applications, it faces unique
challenges posed by high field-of-view, high resolution, and
stereoscopic/egocentric viewing, typically causing low quality and high latency
of the rendered images. In VR, this not only harms the interaction experience
but may also cause sickness. To tackle these problems toward
six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first
gaze-contingent 3D neural representation and view synthesis method. We
incorporate the human psychophysics of visual- and stereo-acuity into an
egocentric neural representation of 3D scenery. We then jointly optimize the
latency/performance and visual quality while mutually bridging human perception
and neural scene synthesis to achieve perceptually high-quality immersive
interaction. We conducted both objective analysis and subjective studies to
evaluate the effectiveness of our approach. We find that our method
significantly reduces latency (up to 99% time reduction compared with NeRF)
without loss of high-fidelity rendering (perceptually identical to
full-resolution ground truth). The presented approach may serve as the first
step toward future VR/AR systems that capture, teleport, and visualize remote
environments in real-time.

Comments:
- 9 pages

---

### In-Place Scene Labelling and Understanding with Implicit Scene  Representation

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-29 by Shuaifeng Zhi | Tristan Laidlow | Stefan Leutenegger | Andrew J. Davison in cs.CV, [PDF](http://arxiv.org/pdf/2103.15875v2)

**Abstract**: Semantic labelling is highly correlated with geometry and radiance
reconstruction, as scene entities with similar shape and appearance are more
likely to come from similar classes. Recent implicit neural reconstruction
techniques are appealing as they do not require prior training data, but the
same fully self-supervised approach is not possible for semantics because
labels are human-defined properties.
  We extend neural radiance fields (NeRF) to jointly encode semantics with
appearance and geometry, so that complete and accurate 2D semantic labels can
be achieved using a small amount of in-place annotations specific to the scene.
The intrinsic multi-view consistency and smoothness of NeRF benefit semantics
by enabling sparse labels to efficiently propagate. We show the benefit of this
approach when labels are either sparse or very noisy in room-scale scenes. We
demonstrate its advantageous properties in various interesting applications
such as an efficient scene labelling tool, novel semantic view synthesis, label
denoising, super-resolution, label interpolation and multi-view semantic label
fusion in visual semantic mapping systems.

Comments:
- Camera ready version. To be published in Proceedings of IEEE
  International Conference on Computer Vision (ICCV 2021) as Oral Presentation.
  Project page with more videos: https://shuaifengzhi.com/Semantic-NeRF/

---

### GNeRF: GAN-based Neural Radiance Field without Posed Camera

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-29 by Quan Meng | Anpei Chen | Haimin Luo | Minye Wu | Hao Su | Lan Xu | Xuming He | Jingyi Yu in cs.CV, [PDF](http://arxiv.org/pdf/2103.15606v3)

**Abstract**: We introduce GNeRF, a framework to marry Generative Adversarial Networks
(GAN) with Neural Radiance Field (NeRF) reconstruction for the complex
scenarios with unknown and even randomly initialized camera poses. Recent
NeRF-based advances have gained popularity for remarkable realistic novel view
synthesis. However, most of them heavily rely on accurate camera poses
estimation, while few recent methods can only optimize the unknown camera poses
in roughly forward-facing scenes with relatively short camera trajectories and
require rough camera poses initialization. Differently, our GNeRF only utilizes
randomly initialized poses for complex outside-in scenarios. We propose a novel
two-phases end-to-end framework. The first phase takes the use of GANs into the
new realm for optimizing coarse camera poses and radiance fields jointly, while
the second phase refines them with additional photometric loss. We overcome
local minima using a hybrid and iterative optimization scheme. Extensive
experiments on a variety of synthetic and natural scenes demonstrate the
effectiveness of GNeRF. More impressively, our approach outperforms the
baselines favorably in those scenes with repeated patterns or even low textures
that are regarded as extremely challenging before.

Comments:
- ICCV 2021 (Oral)

---

### MVSNeRF: Fast Generalizable Radiance Field Reconstruction from  Multi-View Stereo

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-29 by Anpei Chen | Zexiang Xu | Fuqiang Zhao | Xiaoshuai Zhang | Fanbo Xiang | Jingyi Yu | Hao Su in cs.CV, [PDF](http://arxiv.org/pdf/2103.15595v2)

**Abstract**: We present MVSNeRF, a novel neural rendering approach that can efficiently
reconstruct neural radiance fields for view synthesis. Unlike prior works on
neural radiance fields that consider per-scene optimization on densely captured
images, we propose a generic deep neural network that can reconstruct radiance
fields from only three nearby input views via fast network inference. Our
approach leverages plane-swept cost volumes (widely used in multi-view stereo)
for geometry-aware scene reasoning, and combines this with physically based
volume rendering for neural radiance field reconstruction. We train our network
on real objects in the DTU dataset, and test it on three different datasets to
evaluate its effectiveness and generalizability. Our approach can generalize
across scenes (even indoor scenes, completely different from our training
scenes of objects) and generate realistic view synthesis results using only
three input images, significantly outperforming concurrent works on
generalizable radiance field reconstruction. Moreover, if dense images are
captured, our estimated radiance field representation can be easily fine-tuned;
this leads to fast per-scene reconstruction with higher rendering quality and
substantially less optimization time than NeRF.

Comments:
- Project Page: https://apchenstu.github.io/mvsnerf/
  Code:https://github.com/apchenstu/mvsnerf

---

### MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-27 by Jiaxin Li | Zijian Feng | Qi She | Henghui Ding | Changhu Wang | Gim Hee Lee in cs.CV, [PDF](http://arxiv.org/pdf/2103.14910v3)

**Abstract**: In this paper, we propose MINE to perform novel view synthesis and depth
estimation via dense 3D reconstruction from a single image. Our approach is a
continuous depth generalization of the Multiplane Images (MPI) by introducing
the NEural radiance fields (NeRF). Given a single image as input, MINE predicts
a 4-channel image (RGB and volume density) at arbitrary depth values to jointly
reconstruct the camera frustum and fill in occluded contents. The reconstructed
and inpainted frustum can then be easily rendered into novel RGB or depth views
using differentiable rendering. Extensive experiments on RealEstate10K, KITTI
and Flowers Light Fields show that our MINE outperforms state-of-the-art by a
large margin in novel view synthesis. We also achieve competitive results in
depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our
source code is available at https://github.com/vincentfung13/MINE

Comments:
- ICCV 2021. Main paper and supplementary materials

---

### Baking Neural Radiance Fields for Real-Time View Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-26 by Peter Hedman | Pratul P. Srinivasan | Ben Mildenhall | Jonathan T. Barron | Paul Debevec in cs.CV, [PDF](http://arxiv.org/pdf/2103.14645v1)

**Abstract**: Neural volumetric representations such as Neural Radiance Fields (NeRF) have
emerged as a compelling technique for learning to represent 3D scenes from
images with the goal of rendering photorealistic images of the scene from
unobserved viewpoints. However, NeRF's computational requirements are
prohibitive for real-time applications: rendering views from a trained NeRF
requires querying a multilayer perceptron (MLP) hundreds of times per ray. We
present a method to train a NeRF, then precompute and store (i.e. "bake") it as
a novel representation called a Sparse Neural Radiance Grid (SNeRG) that
enables real-time rendering on commodity hardware. To achieve this, we
introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid
representation with learned feature vectors. The resulting scene representation
retains NeRF's ability to render fine geometric details and view-dependent
appearance, is compact (averaging less than 90 MB per scene), and can be
rendered in real-time (higher than 30 frames per second on a laptop GPU).
Actual screen captures are shown in our video.

Comments:
- Project page: https://nerf.live

---

### PlenOctrees for Real-time Rendering of Neural Radiance Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-25 by Alex Yu | Ruilong Li | Matthew Tancik | Hao Li | Ren Ng | Angjoo Kanazawa in cs.CV, [PDF](http://arxiv.org/pdf/2103.14024v2)

**Abstract**: We introduce a method to render Neural Radiance Fields (NeRFs) in real time
using PlenOctrees, an octree-based 3D representation which supports
view-dependent effects. Our method can render 800x800 images at more than 150
FPS, which is over 3000 times faster than conventional NeRFs. We do so without
sacrificing quality while preserving the ability of NeRFs to perform
free-viewpoint rendering of scenes with arbitrary geometry and view-dependent
effects. Real-time performance is achieved by pre-tabulating the NeRF into a
PlenOctree. In order to preserve view-dependent effects such as specularities,
we factorize the appearance via closed-form spherical basis functions.
Specifically, we show that it is possible to train NeRFs to predict a spherical
harmonic representation of radiance, removing the viewing direction as an input
to the neural network. Furthermore, we show that PlenOctrees can be directly
optimized to further minimize the reconstruction loss, which leads to equal or
better quality compared to competing methods. Moreover, this octree
optimization step can be used to reduce the training time, as we no longer need
to wait for the NeRF training to converge fully. Our real-time neural rendering
approach may potentially enable new applications such as 6-DOF industrial and
product visualizations, as well as next generation AR/VR systems. PlenOctrees
are amenable to in-browser rendering as well; please visit the project page for
the interactive online demo, as well as video and code:
https://alexyu.net/plenoctrees

Comments:
- ICCV 2021 (Oral)

---

### KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-25 by Christian Reiser | Songyou Peng | Yiyi Liao | Andreas Geiger in cs.CV, [PDF](http://arxiv.org/pdf/2103.13744v2)

**Abstract**: NeRF synthesizes novel views of a scene with unprecedented quality by fitting
a neural radiance field to RGB images. However, NeRF requires querying a deep
Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering
times, even on modern GPUs. In this paper, we demonstrate that real-time
rendering is possible by utilizing thousands of tiny MLPs instead of one single
large MLP. In our setting, each individual MLP only needs to represent parts of
the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining
this divide-and-conquer strategy with further optimizations, rendering is
accelerated by three orders of magnitude compared to the original NeRF model
without incurring high storage costs. Further, using teacher-student
distillation for training, we show that this speed-up can be achieved without
sacrificing visual quality.

Comments:
- ICCV 2021. Code, pretrained models and an interactive viewer are
  available at https://github.com/creiser/kilonerf/

---

### Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance  Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-24 by Jonathan T. Barron | Ben Mildenhall | Matthew Tancik | Peter Hedman | Ricardo Martin-Brualla | Pratul P. Srinivasan in cs.CV, [PDF](http://arxiv.org/pdf/2103.13415v3)

**Abstract**: The rendering procedure used by neural radiance fields (NeRF) samples a scene
with a single ray per pixel and may therefore produce renderings that are
excessively blurred or aliased when training or testing images observe scene
content at different resolutions. The straightforward solution of supersampling
by rendering with multiple rays per pixel is impractical for NeRF, because
rendering each ray requires querying a multilayer perceptron hundreds of times.
Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF to
represent the scene at a continuously-valued scale. By efficiently rendering
anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable
aliasing artifacts and significantly improves NeRF's ability to represent fine
details, while also being 7% faster than NeRF and half the size. Compared to
NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with
NeRF and by 60% on a challenging multiscale variant of that dataset that we
present. Mip-NeRF is also able to match the accuracy of a brute-force
supersampled NeRF on our multiscale dataset while being 22x faster.

---

### UltraSR: Spatial Encoding is a Missing Key for Implicit Image  Function-based Arbitrary-Scale Super-Resolution

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-23 by Xingqian Xu | Zhangyang Wang | Humphrey Shi in cs.CV, [PDF](http://arxiv.org/pdf/2103.12716v2)

**Abstract**: The recent success of NeRF and other related implicit neural representation
methods has opened a new path for continuous image representation, where pixel
values no longer need to be looked up from stored discrete 2D arrays but can be
inferred from neural network models on a continuous spatial domain. Although
the recent work LIIF has demonstrated that such novel approaches can achieve
good performance on the arbitrary-scale super-resolution task, their upscaled
images frequently show structural distortion due to the inaccurate prediction
of high-frequency textures. In this work, we propose UltraSR, a simple yet
effective new network design based on implicit image functions in which we
deeply integrated spatial coordinates and periodic encoding with the implicit
neural representation. Through extensive experiments and ablation studies, we
show that spatial encoding is a missing key toward the next-stage
high-performing implicit image function. Our UltraSR sets new state-of-the-art
performance on the DIV2K benchmark under all super-resolution scales compared
to previous state-of-the-art methods. UltraSR also achieves superior
performance on other standard benchmark datasets in which it outperforms prior
works in almost all experiments.

---

### AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-20 by Yudong Guo | Keyu Chen | Sen Liang | Yong-Jin Liu | Hujun Bao | Juyong Zhang in cs.CV, [PDF](http://arxiv.org/pdf/2103.11078v3)

**Abstract**: Generating high-fidelity talking head video by fitting with the input audio
sequence is a challenging problem that receives considerable attentions
recently. In this paper, we address this problem with the aid of neural scene
representation networks. Our method is completely different from existing
methods that rely on intermediate representations like 2D landmarks or 3D face
models to bridge the gap between audio input and video output. Specifically,
the feature of input audio signal is directly fed into a conditional implicit
function to generate a dynamic neural radiance field, from which a
high-fidelity talking-head video corresponding to the audio signal is
synthesized using volume rendering. Another advantage of our framework is that
not only the head (with hair) region is synthesized as previous methods did,
but also the upper body is generated via two individual neural radiance fields.
Experimental results demonstrate that our novel framework can (1) produce
high-fidelity and natural results, and (2) support free adjustment of audio
signals, viewing directions, and background images. Code is available at
https://github.com/YudongGuo/AD-NeRF.

Comments:
- Project: https://yudongguo.github.io/ADNeRF/ Code:
  https://github.com/YudongGuo/AD-NeRF

---

### FastNeRF: High-Fidelity Neural Rendering at 200FPS

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-18 by Stephan J. Garbin | Marek Kowalski | Matthew Johnson | Jamie Shotton | Julien Valentin in cs.CV, [PDF](http://arxiv.org/pdf/2103.10380v2)

**Abstract**: Recent work on Neural Radiance Fields (NeRF) showed how neural networks can
be used to encode complex 3D environments that can be rendered
photorealistically from novel viewpoints. Rendering these images is very
computationally demanding and recent improvements are still a long way from
enabling interactive rates, even on high-end hardware. Motivated by scenarios
on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based
system capable of rendering high fidelity photorealistic images at 200Hz on a
high-end consumer GPU. The core of our method is a graphics-inspired
factorization that allows for (i) compactly caching a deep radiance map at each
position in space, (ii) efficiently querying that map using ray directions to
estimate the pixel values in the rendered image. Extensive experiments show
that the proposed method is 3000 times faster than the original NeRF algorithm
and at least an order of magnitude faster than existing work on accelerating
NeRF, while maintaining visual quality and extensibility.

Comments:
- main paper: 10 pages, 6 figures; supplementary: 10 pages, 17 figures

---

### DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields  using Depth Oracle Networks

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-03-04 by Thomas Neff | Pascal Stadlbauer | Mathias Parger | Andreas Kurz | Joerg H. Mueller | Chakravarty R. Alla Chaitanya | Anton Kaplanyan | Markus Steinberger in cs.CV, [PDF](http://arxiv.org/pdf/2103.03231v4)

**Abstract**: The recent research explosion around implicit neural representations, such as
NeRF, shows that there is immense potential for implicitly storing high-quality
scene and lighting information in compact neural networks. However, one major
limitation preventing the use of NeRF in real-time rendering applications is
the prohibitive computational cost of excessive network evaluations along each
view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural
representations closer to practical rendering of synthetic content in real-time
applications, such as games and virtual reality. We show that the number of
samples required for each view ray can be significantly reduced when samples
are placed around surfaces in the scene without compromising image quality. To
this end, we propose a depth oracle network that predicts ray sample locations
for each view ray with a single network evaluation. We show that using a
classification network around logarithmically discretized and spherically
warped depth values is essential to encode surface locations rather than
directly estimating depth. The combination of these techniques leads to DONeRF,
our compact dual network design with a depth oracle network as its first step
and a locally sampled shading network for ray accumulation. With DONeRF, we
reduce the inference costs by up to 48x compared to NeRF when conditioning on
available ground truth depth information. Compared to concurrent acceleration
methods for raymarching-based neural representations, DONeRF does not require
additional memory for explicit caching or acceleration structures, and can
render interactively (20 frames per second) on a single GPU.

Comments:
- Accepted to EGSR 2021 in the CGF track; Project website:
  https://depthoraclenerf.github.io/

---

### Neural 3D Video Synthesis from Multi-view Video



Published 2021-03-03 by Tianye Li | Mira Slavcheva | Michael Zollhoefer | Simon Green | Christoph Lassner | Changil Kim | Tanner Schmidt | Steven Lovegrove | Michael Goesele | Richard Newcombe | Zhaoyang Lv in cs.CV, [PDF](http://arxiv.org/pdf/2103.02597v2)

**Abstract**: We propose a novel approach for 3D video synthesis that is able to represent
multi-view video recordings of a dynamic real-world scene in a compact, yet
expressive representation that enables high-quality view synthesis and motion
interpolation. Our approach takes the high quality and compactness of static
neural radiance fields in a new direction: to a model-free, dynamic setting. At
the core of our approach is a novel time-conditioned neural radiance field that
represents scene dynamics using a set of compact latent codes. We are able to
significantly boost the training speed and perceptual quality of the generated
imagery by a novel hierarchical training scheme in combination with ray
importance sampling. Our learned representation is highly compact and able to
represent a 10 second 30 FPS multiview video recording by 18 cameras with a
model size of only 28MB. We demonstrate that our method can render
high-fidelity wide-angle novel views at over 1K resolution, even for complex
and dynamic scenes. We perform an extensive qualitative and quantitative
evaluation that shows that our approach outperforms the state of the art.
Project website: https://neural-3d-video.github.io/.

Comments:
- Accepted as an oral presentation for CVPR 2022. Project website:
  https://neural-3d-video.github.io/

---

### Mixture of Volumetric Primitives for Efficient Neural Rendering



Published 2021-03-02 by Stephen Lombardi | Tomas Simon | Gabriel Schwartz | Michael Zollhoefer | Yaser Sheikh | Jason Saragih in cs.GR, [PDF](http://arxiv.org/pdf/2103.01954v2)

**Abstract**: Real-time rendering and animation of humans is a core function in games,
movies, and telepresence applications. Existing methods have a number of
drawbacks we aim to address with our work. Triangle meshes have difficulty
modeling thin structures like hair, volumetric representations like Neural
Volumes are too low-resolution given a reasonable memory budget, and
high-resolution implicit representations like Neural Radiance Fields are too
slow for use in real-time applications. We present Mixture of Volumetric
Primitives (MVP), a representation for rendering dynamic 3D content that
combines the completeness of volumetric representations with the efficiency of
primitive-based rendering, e.g., point-based or mesh-based methods. Our
approach achieves this by leveraging spatially shared computation with a
deconvolutional architecture and by minimizing computation in empty regions of
space with volumetric primitives that can move to cover only occupied regions.
Our parameterization supports the integration of correspondence and tracking
constraints, while being robust to areas where classical tracking fails, such
as around thin or translucent structures and areas with large topological
variability. MVP is a hybrid that generalizes both volumetric and
primitive-based representations. Through a series of extensive experiments we
demonstrate that it inherits the strengths of each, while avoiding many of
their limitations. We also compare our approach to several state-of-the-art
methods and demonstrate that MVP produces superior results in terms of quality
and runtime performance.

Comments:
- 13 pages; SIGGRAPH 2021

---

### NeRF--: Neural Radiance Fields Without Known Camera Parameters

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-02-14 by Zirui Wang | Shangzhe Wu | Weidi Xie | Min Chen | Victor Adrian Prisacariu in cs.CV, [PDF](http://arxiv.org/pdf/2102.07064v4)

**Abstract**: Considering the problem of novel view synthesis (NVS) from only a set of 2D
images, we simplify the training process of Neural Radiance Field (NeRF) on
forward-facing scenes by removing the requirement of known or pre-computed
camera parameters, including both intrinsics and 6DoF poses. To this end, we
propose NeRF$--$, with three contributions: First, we show that the camera
parameters can be jointly optimised as learnable parameters with NeRF training,
through a photometric reconstruction; Second, to benchmark the camera parameter
estimation and the quality of novel view renderings, we introduce a new dataset
of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset
(BLEFF); Third, we conduct extensive analyses to understand the training
behaviours under various camera motions, and show that in most scenarios, the
joint optimisation pipeline can recover accurate camera parameters and achieve
comparable novel view synthesis quality as those trained with COLMAP
pre-computed camera parameters. Our code and data are available at
https://nerfmm.active.vision.

Comments:
- Project page see https://nerfmm.active.vision. Add a break point
  analysis experiment and release a BLEFF dataset

---

### A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape,  Appearance, and Pose

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-02-11 by Shih-Yang Su | Frank Yu | Michael Zollhoefer | Helge Rhodin in cs.CV, [PDF](http://arxiv.org/pdf/2102.06199v3)

**Abstract**: While deep learning reshaped the classical motion capture pipeline with
feed-forward networks, generative models are required to recover fine alignment
via iterative refinement. Unfortunately, the existing models are usually
hand-crafted or learned in controlled conditions, only applicable to limited
domains. We propose a method to learn a generative neural body model from
unlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We
equip them with a skeleton to apply to time-varying and articulated motion. A
key insight is that implicit models require the inverse of the forward
kinematics used in explicit surface models. Our reparameterization defines
spatial latent variables relative to the pose of body parts and thereby
overcomes ill-posed inverse operations with an overparameterization. This
enables learning volumetric body shape and appearance from scratch while
jointly refining the articulated pose; all without ground truth labels for
appearance, pose, or 3D shape on the input videos. When used for
novel-view-synthesis and motion capture, our neural model improves accuracy on
diverse datasets. Project website: https://lemonatsu.github.io/anerf/ .

Comments:
- NeurIPS 2021. Project website: https://lemonatsu.github.io/anerf/

---

### Conditions de Kan sur les nerfs des $ω$-catégories

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-02-08 by Félix Loubaton in math.CT, [PDF](http://arxiv.org/pdf/2102.04281v3)

**Abstract**: We show that the Street nerve of a strict $\omega$-category $C$ is a Kan
complex (respectively a quasi-category) if and only if the $n$-cells of $C$ for
$n\geq 1$ (respectively $n> 1$) are weakly invertible. Moreover, we equip
$\mathcal{N}(C)$ with a structure of saturated complicial set where the
$n$-simplices correspond to morphisms from the $n^{th}$ oriental to $C$ sending
the unique non-trivial $n$-cell of the domain to a weakly invertible cell of
$C$.

Comments:
- 52 pages, in French

---

### PVA: Pixel-aligned Volumetric Avatars



Published 2021-01-07 by Amit Raj | Michael Zollhoefer | Tomas Simon | Jason Saragih | Shunsuke Saito | James Hays | Stephen Lombardi in cs.CV, [PDF](http://arxiv.org/pdf/2101.02697v1)

**Abstract**: Acquisition and rendering of photo-realistic human heads is a highly
challenging research problem of particular importance for virtual telepresence.
Currently, the highest quality is achieved by volumetric approaches trained in
a person specific manner on multi-view data. These models better represent fine
structure, such as hair, compared to simpler mesh-based models. Volumetric
models typically employ a global code to represent facial expressions, such
that they can be driven by a small set of animation parameters. While such
architectures achieve impressive rendering quality, they can not easily be
extended to the multi-identity setting. In this paper, we devise a novel
approach for predicting volumetric avatars of the human head given just a small
number of inputs. We enable generalization across identities by a novel
parameterization that combines neural radiance fields with local, pixel-aligned
features extracted directly from the inputs, thus sidestepping the need for
very deep or complex networks. Our approach is trained in an end-to-end manner
solely based on a photometric re-rendering loss without requiring explicit 3D
supervision.We demonstrate that our approach outperforms the existing state of
the art in terms of quality and is able to generate faithful facial expressions
in a multi-identity setting.

Comments:
- Project page located at https://volumetric-avatars.github.io/

---

### Non-line-of-Sight Imaging via Neural Transient Fields

<span for="heading" style="font-weight: bold; background-color: #3498db; color: white; padding: 5px 10px; border-radius: 10px; margin-right: 1em;">nerf</span>

Published 2021-01-02 by Siyuan Shen | Zi Wang | Ping Liu | Zhengqing Pan | Ruiqian Li | Tian Gao | Shiying Li | Jingyi Yu in eess.IV, [PDF](http://arxiv.org/pdf/2101.00373v3)

**Abstract**: We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging.
Previous solutions have sought to explicitly recover the 3D geometry (e.g., as
point clouds) or voxel density (e.g., within a pre-defined volume) of the
hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF)
approach, we use a multi-layer perceptron (MLP) to represent the neural
transient field or NeTF. However, NeTF measures the transient over spherical
wavefronts rather than the radiance along lines. We therefore formulate a
spherical volume NeTF reconstruction pipeline, applicable to both confocal and
non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of
viewpoints (scanning spots) and the sampling is highly uneven. We thus
introduce a Monte Carlo technique to improve the robustness in the
reconstruction. Comprehensive experiments on synthetic and real datasets
demonstrate NeTF provides higher quality reconstruction and preserves fine
details largely missing in the state-of-the-art.